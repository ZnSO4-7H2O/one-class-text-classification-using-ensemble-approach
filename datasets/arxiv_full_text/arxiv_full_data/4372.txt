{"title": "Real-Time Single Image and Video Super-Resolution Using an Efficient  Sub-Pixel Convolutional Neural Network", "tag": ["cs.CV", "stat.ML"], "abstract": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution. In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction. This means that the super-resolution (SR) operation is performed in HR space. We demonstrate that this is sub-optimal and adds computational complexity. In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU. To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space. In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output. By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation. We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.", "text": "recently several models based deep neural networks achieved great success terms reconstruction accuracy computational performance single image super-resolution. methods resolution input image upscaled high resolution space using single ﬁlter commonly bicubic interpolation reconstruction. means super-resolution operation performed space. demonstrate sub-optimal adds computational complexity. paper present ﬁrst convolutional neural network capable real-time videos single gpu. achieve this propose novel architecture feature maps extracted space. addition introduce efﬁcient sub-pixel convolution layer learns array upscaling ﬁlters upscale ﬁnal feature maps output. effectively replace handcrafted bicubic ﬁlter pipeline complex upscaling ﬁlters speciﬁcally trained feature whilst also reducing computational complexity overall operation. evaluate proposed approach using images videos publicly available datasets show performs signiﬁcantly better order magnitude faster previous cnn-based methods. recovery high resolution image video resolution counter part topic great interest digital image processing. task referred super-resolution ﬁnds direct applications many areas hdtv medical imaging satellite imaging face recognition surveillance global problem assumes data low-pass ﬁltered downsampled noisy version data. highly ill-posed problem loss high-frequency information occurs during non-invertible low-pass ﬁltering subsampling operations. furthermore operation effectively one-to-many mapping space multiple solutions determining correct solution non-trivial. assumption underlies many techniques much high-frequency data redundant thus accurately reconstructed frequency components. therefore inference problem thus relies model statistics images question. many methods assume multiple images available instances scene different perspectives i.e. unique prior afﬁne transformations. categorised multi-image methods exploit explicit redundancy constraining ill-posed problem additional information attempting invert downsampling process. however methods usually require computationally complex image registration fusion stages accuracy directly impacts quality result. alternative family methods single image super-resolution techniques techniques seek learn implicit redundancy present natural data recover missing information single instance. usually arises form local spatial correlations images additional temporal correlations videos. case prior information form reconstruction constraints needed restrict solution space reconstruction. related work figure proposed efﬁcient sub-pixel convolutional neural network convolution layers feature maps extraction sub-pixel convolution layer aggregates feature maps space builds image single step. based patch-based methods. detailed review generic sisr methods found family approaches recently thrived tackling sisr problem sparsity-based techniques. sparse coding effective mechanism assumes natural image sparsely represented transform domain. transform domain usually dictionary image atoms learnt training process tries discover correspondence patches. dictionary able embed prior knowledge necessary constrain ill-posed problem super-resolving unseen data. approach proposed methods drawback sparsity-based techniques introducing sparsity constraint nonlinear reconstruction generally computationally expensive. image representations derived neural networks recently also shown promise sisr. methods employ back-propagation algorithm train large image databases imagenet order learn nonlinear mappings image patches. stacked collaborative local auto-encoders used super-resolve image layer layer. osendorfer suggested method sisr based extension predictive convolutional sparse coding framework multiple layer convolutional neural network inspired sparse-coding methods proposed chen proposed multi-stage trainable nonlinear reaction diffusion alternative weights nonlinearity trainable. wang trained cascaded sparse coding network inspired lista fully exploit natural sparsity images. network structure limited neural networks example random forest also successfully used sisr. figure plot trade-off accuracy speed different methods performing upscaling scale factor results presents mean psnr run-time images single core clocked ghz. development efﬁciency algorithms especially computational memory cost gains importance ﬂexibility deep network models learn nonlinear relationships shown attain superior reconstruction accuracy compared previously hand-crafted models super-resolve image space necessary increase resolution image match image point. image resolution increased middle network gradually. another popular approach increase resolution ﬁrst layer network however approach number drawbacks. firstly increasing resolution images image enhancement step increases computational complexity. especially problematic convolutional networks processing speed directly depends input image resolution. secondly interpolation methods typically used accomplish task bicubic interpolation bring additional information solve ill-posed reconstruction problem. learning upscaling ﬁlters brieﬂy suggested footnote dong et.al. however importance integrating part operation fully recognised option explored. additionally noted dong efﬁcient implementations convolution layer whose output size larger input size well-optimized implementations convnet trivially allow behaviour. paper contrary previous works propose increase resolution network super-resolve data feature maps. eliminates need perform operation larger resolution. purpose propose efﬁcient sub-pixel convolution layer learn upscaling operation image video super-resolution. advantages contributions fold network upscaling handled last layer network. means image directly network feature extraction occurs nonlinear convolutions space. reduced input resolution effectively smaller ﬁlter size integrate information maintaining given contextual area. resolution ﬁlter size reduction lower computational memory complexity substantially enough allow super-resolution high deﬁnition videos realtime shown sec. network layers learn upscaling ﬁlters feature maps opposed upscaling ﬁlter input image. addition using explicit interpolation ﬁlter means network implicitly learns processing necessary thus network capable learning better complex mapping compared single ﬁxed ﬁlter upscaling ﬁrst layer. results additional gains reconstruction accuracy model shown sec. sec. validate proposed approach using images videos publicly available benchmarks datasets compared performance previous works including show proposed model achieves state-of-art performance nearly order magnitude faster previously published methods images videos. task sisr estimate image given image downscaled corresponding original image ihr. downsampling operation deterministic known produce ﬁrst convolve using gaussian ﬁlter thus simulating camera’s point spread function downsample image factor refer upscaling ratio. general colour channels thus represented real-valued tensors size respectively. solve sisr problem srcnn proposed recovers upscaled interpolated version instead ilr. recover layer convolutional network used. section propose novel network architecture illustrated fig. avoid upscaling feeding network. architecture ﬁrst apply layer convolutional neural network directly image apply sub-pixel convolution layer upscales feature maps produce isr. learnable network weights biases respectively. convolution tensor size nl−× number features layer ﬁlter size layer biases vectors length nonlinearity function applied element-wise ﬁxed. last layer convert feature maps image isr. addition deconvolution layer popular choice recovering resolution max-pooling image down-sampling layers. approach successfully used visualizing layer activations generating semantic segmentations using high level features network trivial show bicubic interpolation used srcnn special case deconvolution layer suggested already deconvolution layer proposed seen multiplication input pixel ﬁlter element-wise stride sums resulting output windows also known backwards convolution upscale image convolution fractional stride space mentioned naively implemented interpolation perforate un-pooling space space followed convolution stride space. implementations increase computational cost factor since convolution happens space. space ﬁlter size weight spacing would activate different parts convolution. weights fall pixels simply activated need calculated. number activation patterns exactly activation pattern according location activated. patterns periodically activated convolution ﬁlter across image depending different sub-pixel location output pixel coordinates space. paper propose effective implement operation periodic shufﬂing operator rearranges elements tensor tensor shape effects operation illustrated fig. mathematically operation described following psxyc tx/ry/rc·r·mod+c·mod+c convolution operator thus shape note apply nonlinearity outputs convolution last layer. easy equivalent sub-pixel convolution space ﬁlter refer layer sub-pixel convolution layer network efﬁcient sub-pixel convolutional neural network last layer produces image feature maps directly upscaling given training consisting image examples generate corresponding calculate pixel-wise mean images squared error reconstruction objective function train network noticeable implementation periodic shufﬂing avoided training time. instead shufﬂing output part layer preshufﬂe training data match output layer thus proposed layer logr times faster compared deconvolution layer training times faster compared implementations using various forms upscaling convolution. detailed report quantitative evaluation including original data including images videos downsampled data super-resolved data overall individual scores run-times provided supplemental material. evaluation used publicly available benchmark datasets including timofte dataset widely used sisr papers provides source code multiple methods training images test datasets provides images; berkeley segmentation dataset provides images testing super texture dataset provides texture images. ﬁnal models randomly selected images imagenet training. following previous works consider luminance channel ycbcr colour space section humans sensitive luminance changes upscaling factor train speciﬁc network. video experiments videos publicly available xiph database used report video results previous methods database contains collection videos approximately seconds length width height addition also ultra video figure last-layer ﬁlters trained imagenet upscaling factor shows weights srcnn model shows weights espcn model weights operation applied channels. ﬁlters default ordering. espcn evaluations. choice parameter inspired srcnn’s layer model equations sec. training phase pixel sub-images extracted training ground truth images upscaling factor. synthesize low-resolution samples blur using gaussian ﬁlter sub-sample upscaling factor. sub-images extracted original images pixels original image appear ground truth training data. choose tanh instead relu activation function ﬁnal model motivated experimental results. training stops improvement cost initial learning function observed epochs. rate ﬁnal learning rate updated gradually improvement cost function smaller threshold ﬁnal layer learns times slower training takes roughly three hours images seven days images imagenet upscaling factor psnr performance metric evaluate models. psnr srcnn chen’s models extended benchmark calculated based matlab code models provided section demonstrate positive effect subpixel convolution layer well tanh activation function. ﬁrst evaluate power sub-pixel convolution layer comparing srcnn’s standard model here follow approach using relu activation function models experiment training models images another images imagenet. results shown tab. espcn relu trained imagenet images achieved statistically signiﬁcantly better performance compared srcnn models. noticeable espcn performs similar srcnn training images using espcn signiﬁcant impact psnr compared srcnn similar number parameters make visual comparison model sub-pixel convolution layer srcnn visualized weights espcn model srcnn imagenet model fig. fig. weights ﬁrst last layer ﬁlters strong similarity designed features including log-gabor ﬁlters wavelets haar features noticeable despite ﬁlter independent space independent ﬁlters actually smooth space compared srcnn’s last layer ﬁlters ﬁnal layer ﬁlters complex patterns different feature maps also much richer meaningful representations. also evaluated effect tanh activation function based model trained images imagenet images. results tab. suggests tanh function performs better sisr compared relu. results imagenet images tanh activation shown tab. section show espcn trained imagenet compared results srcnn tnrd currently best performing approach published. simplicity show results known worse interested reader results previous methods found choose compare best srcnn imagenet model section results calculated based stages model. results shown tab. signiﬁcantly better srcnn imagenet model whilst close cases out-performing tnrd although tnrd uses single bicubic interpolation upscale input image space possibly beneﬁts trainable nonlinearity function. trainable nonlinearity function exclusive network interesting explore future. visual comparison superresolved images given fig. fig. methods create much sharper higher contrast images espcn provides noticeably improvement srcnn. section compare espcn trained models single frame bicubic interpolation srcnn popular video benchmarks. advantage network speed. makes ideal candidate video allows super-resolve videos frame frame. results shown tab. tab. better srcnn imagenet model. improvement signiﬁcant results image data maybe differences datasets. similar disparity observed different categories image benchmark supertexture. section evaluated best model’s time upscale factor evaluate time methods matlab codes provided methods convolutions including python/theano implementation used improve efﬁciency based matlab codes provided results presented fig. model runs magnitude faster fastest methods published far. compared srcnn imagenet model number convolution required super-resolve image times smaller number total parameters model times smaller. total complexity noted results outperform algorithms accuracy larger datasets. however single core selected order allow straight-forward comparison results previous published results super-resolution operation thus times lower. achieved stunning average speed super-resolving single image gpu. utilising amazing speed network interesting explore ensemble prediction using independently trained models discussed achieve better performance future. also evaluated time video superresolution using videos xiph ultra video group database. upscale factor srcnn imagenet model takes frame whilst espcn model takes frame. upscale factor srcnn imagenet model takes frame whilst espcn model takes frame. table mean psnr different methods evaluated extended benchmark set. srcnn stands srcnn imagenet model tnrd stands trainable nonlinear reaction diffusion model espcn stands imagenet model tanh activation. best results category shown bold. signiﬁcant difference psnrs proposed method srcnn dataset sunflower station pedestrianarea snowmnt aspen oldtowncross duckstakeoff crowdrun average sunflower station pedestrianarea snowmnt aspen oldtowncross duckstakeoff crowdrun average table results videos xiph database. srcnn stands srcnn imagenet model espcn stands imagenet model tanh activation. best results category shown bold. signiﬁcant difference psnrs proposed method srcnn paper demonstrate non-adaptive upscaling ﬁrst layer provides worse results adaptive upscaling sisr requires computational complexity. address problem propose perform feature extraction stages space instead space. propose novel sub-pixel convolution layer capable super-resolving data space little additional computational dataset bosphorus readysetgo beauty yachtride shakendry honeybee jockey average bosphorus readysetgo beauty yachtride shakendry honeybee jockey average table results videos ultra video group database. srcnn stands srcnn imagenet model espcn stands imagenet model tanh activation. best results category shown bold. signiﬁcant difference psnrs proposed method srcnn cost compared deconvolution layer training time. evaluation performed extended bench mark data upscaling factor shows signiﬁcant speed performance boost compared previous approach parameters makes model ﬁrst model capable videos real time single gpu. reasonable assumption processing video information scene’s content shared neighbouring video frames. exceptions assumption scene changes objects sporadically appearing disappearing scene. creates additional dataimplicit redundancy exploited video superresolution shown spatio-temporal networks popular fully utilise temporal information videos human action recognition future investigate extending espcn network spatio-temporal network super-resolve frame multiple neighbouring frames using convolutions. chang d.-y. yeung xiong. super-resolution ieee computer society conference neighbor embedding. computer vision pattern recognition volume pages i–i. ieee image super-resolution using deep convolutional networks. ieee transactions pattern analysis machine intelligence dong zhang image deblurring superresolution adaptive sparse domain selection adaptive regularization. ieee transactions image processing efrat glasner apartsin nadler levin. accurate blur models image priors single image super-resolution. ieee international conference computer vision pages ieee fernandez-granda candes. super-resolution transforminvariant group-sparse regularization. ieee international conference computer vision pages ieee image super-resolution ieee transactions image denker henderson howard hubbard jackel. handwritten digit recognition backpropagation network. advances neural information processing systems. citeseer martin fowlkes malik. database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics. proc. int’l conf. computer vision volume pages july image superresolution fast approximate convolutional sparse coding. neural information processing pages springer peled yeshurun. superresolution application human white matter ﬁber tract visualization diffusion tensor imaging. magnetic resonance medicine ofﬁcial journal society magnetic resonance medicine society magnetic resonance medicine caballero ledig zhuang bhatia marvao dawes oregan rueckert. cardiac image super-resolution global correspondence using multi-atlas patchmatch. mori sakuma sato barillot navab editors medical image computing computer assisted intervention volume lncs pages simonyan zisserman. deep convolutional networks large-scale image recognition. arxiv preprint arxiv. thornton atkinson holland. sub-pixel mapping rural land cover objects spatial resolution satellite sensor imagery using super-resolution pixel-swapping. international journal remote sensing viola jones. rapid object detection using boosted cascade simple features. ieee conference computer vision pattern recognition volume pages ieee wang zhang liang pan. semi-coupled dictionary learning applications image super-resolution photoieee conference computer vision sketch synthesis. pattern recognition pages ieee zeiler taylor fergus. adaptive deconvolutional ieee internetworks high level feature learning. national conference computer vision pages ieee", "year": 2016}