{"title": "Bayesian Deep Convolutional Encoder-Decoder Networks for Surrogate  Modeling and Uncertainty Quantification", "tag": ["physics.comp-ph", "cs.CV", "cs.LG", "stat.ML"], "abstract": "We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder-decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification benchmark problems including flow in heterogeneous media defined in terms of limited data-driven permeability realizations. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to $4,225$ where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.", "text": "interested development surrogate models uncertainty quantiﬁcation propagation problems governed stochastic pdes using deep convolutional encoder-decoder network similar fashion approaches considered deep learning image-to-image regression tasks. since normal neural networks data intensive cannot provide predictive uncertainty propose bayesian approach convolutional neural nets. recently introduced variational gradient descent algorithm based stein’s method scaled deep convolutional networks perform approximate bayesian inference millions uncertain network parameters. approach achieves state performance terms predictive accuracy uncertainty quantiﬁcation comparison approaches bayesian neural networks well techniques include gaussian processes ensemble methods even training data size relatively small. evaluate performance approach consider standard uncertainty quantiﬁcation benchmark problems including heterogeneous media deﬁned terms limited data-driven permeability realizations. performance surrogate model developed good even though underlying structure shared input output ﬁelds often case image-to-image regression models used computer vision problems. studies performed underlying stochastic input dimensionality uncertainty quantiﬁcation methods fail. uncertainty propagation tasks considered predictive output bayesian tion unknown/incomplete material properties boundary conditions forcing terms other. uncertainty propagation takes place reformulating problem interest system stochastic partial diﬀerential equations solution problems often needs rely solution deterministic problem ﬁnite number realizations random input using monte carlo sampling collocation methods. considering computational cost solving complex multiscale/multiphysics deterministic problems often relies bayesian surrogate models trained small number deterministic solution runs time capable capturing epistemic uncertainty introduced limited training data realistic problems science engineering access limited number deterministic simulation runs. vanilla monte carlo uncertainty propagation thus hopeless. dominant solution train surrogate model using limited simulation-based training data perform prediction uncertainty propagation tasks using surrogate instead solving actual pdes. unfortunately existing surrogate models diﬃculty scaling high-dimensional problems ones based gaussian processes generalized polynomial chaos expansions high dimensionality often arises discretization properties small correlation lengths random distributed sources force input ﬁelds multiple scales alleviate curse stochastic input dimensionality usually assume given input data embedded non-linear manifold within higher dimensional space. intrinsic dimensionality captured dimensionality reduction techniques karhunenlo`eve expansion t-sne auto-encoders probabilistic methods like variational auto-encoders gaussian process latent variable models many more. dimensionality reduction models unsupervised learning problems explicitly take regression task account. thus classical approach uncertainty quantiﬁcation ﬁrst reduce dimensionality input obtain low-dimensional latent representation input ﬁeld built regression model latent representation output. approach certainly eﬃcient particular latent representation physical space input data available. used dimensionality reduction permeability ﬁeld performed independent task bayesian regression. approach taken step probabilistic mappings input latent space latent space output modeled generalized linear models trained simultaneously end-to-end using stochastic variational inference instead performing unsupervised supervised/regression tasks separately. essential upcoming approaches handling high-dimensional data learn latent input representation automatically supervision output regression tasks. central idea deep neural networks especially convolutional neural networks stack layers linear convolutions nonlinear activations automatically extract multi-scale features concepts highdimensional input thus alleviating hand-craft feature engineering searching right basis functions relying expert knowledge. however general perspective using deep neural networks context surrogate modeling physical problems uncertainty quantiﬁcation data problems thus suitable addressing deep learning approaches. however argue otherwise sense simulation generates large amount data potentially reveal essential characteristics underlying system. addition even relatively small dataset deep neural networks show unique generalization property typically over-parameterized models overﬁt i.e. test error grow network parameters increase. deep learning explored competitive methodology across ﬁels ﬂuid mechanics hydrology bioinformatics high energy physics other. unique generalization behavior makes possible deep neural networks surrogate modeling. capable capture complex nonlinear mapping high-dimensional input output expressiveness small number data simulation runs. addition resurgence interest putting deep neural network formal bayesian framework. bayesian deep learning enables network express uncertainty predictions using small number training data. bayesian neural networks quantify predictive uncertainty treating network parameters random variables perform bayesian inference uncertain parameters conditioned limited observations. work mainly consider surrogate modeling physical systems governed stochastic partial diﬀerential equations high-dimensional stochastic input random porous media spatially discretized stochastic input ﬁeld corresponding output ﬁelds high-dimensional. adopt end-to-end image-to-image regression approach challenging surrogate modeling problem. speciﬁcally fully convolutional encoder-decoder network designed capture complex mapping directly high-dimensional input ﬁeld output ﬁelds without using explicit intermediate dimensionality reduction method. make model parameter eﬃcient compact densenet build feature extractor within encoder decoder paths intuitively encoder network extracts multi-scale features input data used decoder network reconstruct output ﬁelds. similarity problems computer vision treat input-output image-to-image map. account limited training data endow network uncertainty estimates treat convolutional encoder-decoder network bayesian scale recently proposed approximate inference method called stein variational gradient descent modern deep convolutional networks. show methodology learn bayesian surrogate problem intrinsic dimensionality achieving promising results predictive accuracy uncertainty estimates using training data. importantly develop surrogate case dimensionality using training data. also show uncertainty estimates well-calibrated using reliability diagram. believe bayesian neural networks strong candidates surrogate modeling uncertainty propagation high-dimensional problems limited training data. remaining paper organized follows section present problem setup surrogate modeling high-dimensional input proposed approach treating image regression problem. introduce cnns encoder-decoder network used model. section present bayesian formulation neural networks non-parametric variational method underlying challenging approximate inference task. section provide implementation details show performed experiments porous media problem. ﬁnally conclude discuss various unexplored research directions section sample space. formulation allows multiple input channels even though interest input property represented vector random ﬁeld. random ﬁeld appears coeﬃcients spdes used model material properties permeability porosity ﬁelds geological media ﬂows. assume computer simulation physical systems performed given spatial grid locations {s··· sns} case random ﬁeld discretized ﬁxed grids thus equivalent high-dimensional random vector denoted rdxns. corresponding response solved thus represented vector rdyns. discretization described assuming simplicity ﬁxed boundary initial conditions source terms appropriate consider computation simulation black-box mapping form order tackle limitations using deterministic computationally expensive simulator uncertainty propagation surrogate model approximate ‘ground-truth’ simulation-induced function model parameters number simulation runs ﬁelds discretized grids. number dimensions input output location. treated herein number channels input output images similar channels natural images. easy generalize spatial distinction image classiﬁcation problem requires imagewise prediction image regression problem concerned pixel-wise predictions e.g. predicting depth pixel image physical problem predicting output ﬁelds grid point. problems intensively studied within computer vision community leveraging rapid recent progress convolutional neural networks alexnet inception resnet densenet many more. common model design pattern semantic segmentation depth regression encoder-decoder architecture. intuition behind regression high-dimensional objects coarse-reﬁne process i.e. reduce spatial dimension input image high-level coarse features using encoder recover spatial dimension reﬁning coarse features decoder. characteristics shared vision tasks input output images share underlying structure diﬀerent renderings underlying structure however surrogate modeling tasks input output images appear quite diﬀerent complex physical inﬂuences random input ﬁeld forcing terms boundary conditions system response. reason pursuing training surrogate model avoids repeated solution pdes diﬀerent input realizations. surprisingly discuss later paper encoder-decoder network still works well. remark training data surrogate model interest include realizations random input ﬁeld corresponding multioutput obtained simulation. interest address problems limited training data sets considering high-computational cost simulation run. however note tasks include ability predict system response conﬁdence using input realizations consistent given training data also computation statistics response induced random input. tasks require availability high-number input data sets problem generating input realizations using training dataset solution generative model problem. signiﬁcant progress recent years topic generative adversarial networks ever exploding variants variational auto-encoders autoregressive models like pixelcnn pixelrnn other. however note work focus image-to-image mapping performance uncertainty quantiﬁcation tasks. thus assume enough input samples provided testing output statistics calculation even though small dataset used training. examples section synthetic log-permeability datasets generated sampling gaussian random ﬁeld exponential kernel. output permeability sample generated using deterministic simulator. subsection brieﬂy introduce state-of-the-art architecture called densenet fully convolutional encoder-decoder networks developed computer vision present utilize advances build baseline network surrogate modeling uncertainty quantiﬁcation. densenet recently proposed architecture extends ideas resnet highway networks create dense connections layers improve information network better parameter eﬃciency. output layer. traditionally cnns pass output layer input next layer i.e. denotes nonlinear function hidden layer. current cnns commonly deﬁned composition batch normalization rectiﬁed linear unit convolution transposed convolution resnets create additional identity mapping bypasses nonlinear layer i.e. hl+xl−. nonlinear layer needs learn residual function facilitates training deeper networks. image regression based encoder-decoder networks downsampling upsampling required change size feature maps makes concatenation feature maps unfeasible. dense blocks transition layers introduced solve problem modularize network design. dense block contains multiple densely connected layers whose input output feature maps size. contains design parameters namely number layers within growth rate layer. illustration dense block fig. figure dense block contains layers growth rate second layer dense block output feature map. notice input third layer concatenation output input features i.e. often case layer composed batch normalization rectiﬁed linear unit convolution convolution kernel kernel size stride zero padding keep size feature maps input. transition layers used change size feature maps reduce number dense blocks. speciﬁcally encoding layer typically halfs size feature maps decoding layer doubles feature size. layers reduce number feature maps. illustrated fig. figure encoding layer decoding layer contain convolutions. ﬁrst convolution reduces number feature maps keeps size using kernel parameters second convolution changes size feature maps number using kernel main diﬀerence type second convolution layer conv convt respectively downsampling upsampling. note pooling used transition layers maintaining location information. colored feature maps used independent feature maps color shown ﬁgures. fully convolutional networks extensions cnns pixel-wise prediction e.g. semantic segmentation. fcns replace fully connected layers cnns convolution layers upsampling layers recover input spatial resolution introduce skip connections feature maps downsampling upsampling path recover ﬁner information lost downsampling path. recent work focuses improving upsampling path increase connectivity within upsampling downsampling paths. u-nets extend upsampling path symmetric downsampling path skip connections size feature maps downsampling upsampling paths. within segnets decoder uses pooling indices computed max-pooling step corresponding encoder perform non-linear upsampling. fully convolutional densenets extend densenets fcns closest network design several diﬀerences. keep feature maps dense block concatenated passing transition layers keep output feature maps last convolution layer within dense block. feature maps explosion problem addressed ﬁrst convolution layer within transition layer. besides skip connections encoding decoding paths weak correspondence input output images. also max-pooling encoding layers instead convolution stride follow fully convolutional networks image segmentation without using fully connected layers encode-decoder architecture similar u-net segnet without concatenation feature maps encoder paths decoder paths. furthermore adapt densenet structure encoder decoder networks. extensive hyperparameter architecture search arrived baseline dense convolutional encoder-decoder network called denseed similar network proposed noticeable diﬀerences stated shown fig. encoding path input ﬁeld realizations ﬁrst convolution layer large kernel size stride zero padding extracted feature maps passed alternative cascade dense blocks encoding layers introduced figs. dense block last encoding layer outputs high-level coarse feature maps extracted input shown purple right network fig. subsequently decoder path. decoding network consists alternation dense blocks decoding layers last decoding layer directly leading prediction output ﬁelds. network architecture engineering hyperparameter search among main challenges source innovations deep learning mostly problem-speciﬁc empirical. general network architecture introduced section based recent development neural network design image segmentation. image regression problem main design considerations include following downsampling layers convolution pooling; upsampling layers bilinear upsampling transposed convolution; smallest spatial dimensions feature maps determined kernel convolution layers kernel size stride zero padding number layers growth rate within dense block; regularizations weight decay batch normalization dropout etc; optimizer stochastic gradient descent algorithms variants details architecture search hyperparameter selection particular problem considered presented appendix also report various experiments using denseed surrogate modeling limited training data. overﬁtting observed calculations obtained results quite good. intriguing active research topic deep learning community pixels channels output image. denotes parameters network. case includes kernel weights convolution transposed convolution layers scale shift parameters batch normalization layers. section example fully convolutional encoder-decoder remark encoder-decoder network batch normalization layer used convolutional layer also considered eﬀective regularizer. commonly adopted nowadays deep convolutional networks replacing dropout. consider deterministic neural input output parameters including weights biases. bayesian neural networks treat parameters random variables instead deterministic unknowns account epistemic uncertainty induced lack training data. besides that usually additive noise introduced model aleatoric uncertainty reduced observations also make probabilistic model explicit likelihood depending noise distribution i.e. https//openreview.net/forum?id=bjlrsmbaz&noteid=bjlrsmbaz https//github.com/pytorch/vision/tree/master/torchvision/models https//www.reddit.com/r/machinelearning/comments/lfc/d_what_ pression neural lower memory computation cost calls sparsity promoting priors. assume fully factorized gaussian prior zero mean gamma-distributed precision parameters numerical results discussed section concentrate output-wise channel-wise cases above. treat noise precision random variable conjugate prior gamma. generated training data priori e.g. thus values provide good initial guess output system becomes pixel-wise case help capture large variations example near discontinuous regions output. practice apply softplus transformation approximate inference bayesian deep neural network daunting task large number uncertain parameters e.g. tens hundreds millions modern deep networks. surrogate problem task high-dimensional posterior distribution millions random variables using less hundreds thousands training data. reviewed section variational inference methods restrict approximate posterior within certain parametric variational family sampling-based methods slow diﬃcult converge. adopt recently proposed non-parametric variational inference method called stochastic variational gradient descent similar standard gradient descent maintaining eﬃciency particle methods. prescribed probabilistic model likelihood function prior interested bayesian inference uncertain posterior distribution parameters i.e. denote i.i.d. observations i.e. yi}n bnns homoescedastic gaussian noise case variational inference aims approximate target posterior distribution variational distribution lies restricted distributions minimizing divergence i.e. variational family considered distributions obtained smooth transforms initial tractable distribution represented terms particles. transforms applied particle take following form problem direction maximally decrease divergence variational approximation target distribution i.e. solve following functional optimization problem step model gradient joint likelihood computed automatic diﬀerentiation tool pytorch. ﬁrst compute joint likelihood feeding forward data yi}n note gradient stored pytorch module associated weights interest classical problem computation posterior predictive distribution well computation output response averaged input probability distribution. particular interested computing following vector ones dimension square operator applied element-wise vectors. compute statistics conditional statistics uncertainty surrogate i.e. variances output dimension conditional predictive mean variance. study two-dimensional single phase steady-state random permeability ﬁeld following case study section consider random permeability ﬁeld unit square spatial domain gaussian random ﬁeld exponential kernel onedimensional case corresponds ornstein-uhlenbeck process mean-square continuous mean-square diﬀerentiable. thus discretize ﬁeld grid ﬁeld value jumps move pixel pixel. ﬁeld become smoother ﬁner grid ﬁxed spatial domain. high variability creates signiﬁcant challenge data-driven models capture i.e. intrinsic dimensionality discretized random ﬁeld total number pixels e.g. however common assumption natural images underlying dimensionality actually small despite complex appearance. evaluate generality eﬀectiveness methodology control intrinsic dimensionality permeability dataset. evaluated model using datasets produced increasing dimensionality notice number terms permeability ﬁeld directly sampled exponential gaussian ﬁeld without dimensionality reduction. intrinsic dimensionality dataset hidden model i.e. model built terms system output. instead models end-to-end mapping input ﬁelds element formulation implemented fenics third-order raviartthomas elements velocity fourth-order discontinuous elements pressure. sample input permeability ﬁeld computed output pressure velocity ﬁelds three datasets shown fig. available data limited common practice crossvalidation evaluate model. since dataset synthetically generated access number training test data computing constraints solve darcy equations. current data includes four sets training validation test uncertainty propagation set. training sampled using simplest design experiment method latin hypercube sampling. speciﬁcally log-permeability ﬁeld eigenvalues eigenfunctions exponential covariance function gaussian ﬁeld speciﬁed eqs. zk’s i.i.d. standard normal number coeﬃcients maintained expansion. maximum number used figure sample permeability obtained dataset dataset dataset corresponding velocity components pressure obtained simulator. ﬁgures shown using pixels reveal high variability input output ﬁelds. log-permeability ﬁelds three sets reconstructed directly sampled standard normal. validation test contains input permeability ﬁelds dataset uncertainty propagation contains realizations. datasets organized images discussed section predictive output non-bayesian surrogate i.e. test target mean test target total number test data. metric enables comparison diﬀerent datasets since error normalized score closer corresponding better regression. metric used evaluating nonbayesian surrogate following metrics additional metrics evaluating bayesian surrogate. note metric also used tracking performance training process thus evaluated training test data sets. metric evaluates likelihood observed data. used assess quality predictive model. predictive uncertainty propagated uncertainty metrics introduced section estimated distributions include histograms kernel density estimates output ﬁelds certain locations physical domain. reliability diagram given trained bayesian surrogate test data compute predictive interval test data point based gaussian quantiles using predictive mean variance compute frequency test targets fall within predictive interval. well-calibrated regression model observed frequency hyperparameters search include parameters determine network architecture ones specify training process aﬀect model performance. hyperband algorithm optimize hyperparameters constraint number model parameters less million. details experiments given appendix network conﬁguration highest rscore hyperband ﬁnds shown fig. details provided table conﬁguration referred denseed-c. nd–th columns table show number output feature maps spatial resolution output feature maps number parameters figure denseed-c blocks growth rate initial feature maps ﬁrst convolution layer total layers parameters network. number network parameters optimized based generalization error analysis reported appendix contains downsampling layers thus smallest spatial dimension feature maps convolution kernel ksp. last transposed conv kernel decoding layer ksp. number output feature maps decoding layers output padding conv kernels dense blocks encoding decoding layers described section network denseed-c trained adam variant stochastic gradient descent loss function regularized implemented weight decay modern neural frameworks pytorch tensorflow. loss functions achieve better results smoothed loss conditional loss requires investigations considered future publication. initial learning rate weight decay batch size also learning rate scheduler drops times plateau rooted mse. model trained epochs. train model dataset introduced section training deterministic neural networks regularized equivalent ﬁnding maximum posterior uncertain parameters bayesian neural networks whose prior independent normal. typical training process shown fig. train network diﬀerent number training data kle. validation r-score shown fig. shows that training data r-score closer intrinsic dimensionality smaller r-score higher training data dimensionality. note score reasonably small size training data three cases dimensionality shows eﬀectiveness network denseed-c low-dimensional high-dimensional problems. feeding test input permeability ﬁeld trained network i.e. show prediction test input shown fig. using denseed-c trained three datasets figs. respectively. predictions quite good even case input output ﬁelds vary rapidly certain regions domain. experiments consider homoscedastic noise model bayesian neural networks i.e. output-wise gaussian noise gamma prior precision student’s t-prior uncertain parameters denoted apply implementation corresponds diﬀerent initializations deterministic denseed noise precision update parameters denseed’s corresponding noise precision using svgd algorithm algorithm kernel chosen expx median heuristic choice kernel bandwidth /log median pairwise distances current samples {θi}s typically figure prediction input realization shown fig. dataset using denseed-c trained datasets sizes respectively. subﬁgures ﬁrst shows three test target ﬁelds i.e. pressure velocity second shows corresponding model predictions third shows error. figure prediction input realization shown fig. dataset using denseed-c trained datasets sizes respectively. subﬁgures ﬁrst shows three test target ﬁelds i.e. pressure velocity second shows corresponding model predictions third shows error. figure prediction input realization shown fig. dataset using denseed-c trained datasets sizes respectively. subﬁgures ﬁrst shows three test target ﬁelds i.e. pressure velocity second shows corresponding model predictions third shows error. using smaller batch size helps lower training test errors time training. adam update using gradient instead vanilla stochastic gradient descent epochs learning rate learning rate scheduler decreases times training rmse plateau. training epochs training data size varies training time depends heavily training mini-batch size cases. potential ways speed signiﬁcantly training process include increasing mini-batch size implementing svgd parallel using multi-gpus. python source code become available upon publication https//github.com/bmmi/bayesnn. report next r-score computed similar non-bayesian case except predicted output mean used compare test target. scores shown fig. bayesian surrogate improves r-score signiﬁcantly non-bayesian version. r-score gives general estimate well regression performs. given input permeability ﬁeld bayesian neural network predict mean corresponding output ﬁelds also gives uncertainty estimate represented predictive variance spatial location unavailable deterministic models desirable training data small. figs. show predictions test input shown fig. training data respectively. predictive accuracy improves size training dataset increases predictive uncertainty drops. mean error standard deviation predictive output distribution pixel var. three columns left right correspond pressure ﬁeld velocity ﬁelds respectively. mean error standard deviation predictive output distribution pixel var. three columns left right correspond pressure ﬁeld velocity ﬁelds respectively. mean error standard deviation predictive output distribution pixel var. three columns left right correspond pressure ﬁeld velocity ﬁelds respectively. surrogate input realizations sampled gaussian ﬁeld calculating output statistics section figs. show uncertainty propagation results compare monte carlo using data bayesian surrogate trained datasets kle. show estimate pressure velocity components locations unit square training data fig. respectively. obtained kernel density estimation using predictive mean. density estimate close monte carlo result even training dataset small becomes closer training dataset increases. fig. observe predictions velocity ﬁelds better pressure ﬁeld especially locations away diagonal unit square domain general case current network architecture three output ﬁelds treated same. order access quality computed uncertainty adopt reliability diagram expresses discrepancy predictive probability frequency falling predictive interval test data. diagram shown fig. overall models wellcalibrated since quite close ideal diagonal especially case training dataset size shown fig. general model turns over-conﬁdent training data small gradually becomes prudent training data increases. main reason observation predictive uncertainty dominated variation seen training data small small data observed. initial learning rates scheduling scheme network parameters noise precision also play roles since uncertainty determined optimum stochastic optimization obtained. empirically show using samples bayesian neural nets svgd algorithm suﬃcient problem show fig. convergence test training rmse vary number samples. figure test training error using svgd train bayesian training data kle. increase number posterior samples i.e. number deterministic neural networks training test errors drop become steady. plot empirically supports reason choose model instances/samples svgd. work explore alternative bayesian surrogate i.e. bayesian neural network predict output ﬁelds systems governed stochastic partial diﬀerential equations high-dimensional stochastic input. approach built highly expressive deep convolutional encoder-decoder networks. end-to-end image-to-image regression avoids usual linear dimensionality reduction step achieves promising results terms predictive performance uncertainty modeling even limited training data. show experiments network works well across problems diﬀerent intrinsic dimensionality. believe performance small dataset domain deep neural networks unique generalization property eﬀectively says overparameterized deep neural nets lack over-ﬁtting. bayesian model provides uncertainty estimates predictions thus accounting epistemic uncertainty training small datasets. show bayesian inference based svgd works well training bayesian neural network presented results uncertainty propagation tasks. uncertainty trained model well-calibrated investigating reliability diagrams. inﬂuences pressure ﬁeld velocity ﬁelds end. address observation prediction performance velocity ﬁelds better pressure ﬁeld using current network architecture denseed-c. bayesian neural network inference task genuinely diﬃcult since asked posterior millions random variables based hundreds training data. exploring priors network weights using recent advances variational inference bayesian neural networks deﬁnitely valuable directions pursue. image-to-image regression approach used handle prediction systems diﬀerent source terms adding source ﬁeld another channel input besides material property ﬁeld. implementation needs increase number input channels leave everything else unchanged. current network utilize information physics governing equations constraints three output ﬁelds. incorporating physics information deep neural networks principled ways improve model performance. computational science early developments supported computer science mathematics division ornl darpa equips program. n.z. thanks technische universit¨at m¨unchen institute advanced study support hans fisher senior fellowship funded german excellence initiative european union seventh framework programme grant agreement authors acknowledge steven atkinson cics generating providing darcy datasets. optimize network hyperparameters following general architecture shown fig. encoding layer fig. down-samples input feature maps using convolution stride ﬁrst convolution layer. decoding layer fig. up-samples feature maps using transposed convolution instead pooling layers since location information critical regression. number down-sampling up-sampling layers same. datasets described section general network architecture works well across diﬀerent datasets bayesian non-bayesian network models. experiment spatial dimension feature maps coarsest scale determined many down-sampling layers used. shrinking spatial dimension feature maps extract high-level coarse information input permeability ﬁeld subsequently used predict output pressure velocity ﬁelds. design choice related central concept cnns called receptive ﬁeld unit within certain layer region input image aﬀects unit feedforward region output image aﬀects unit back-propagating gradients. dense prediction tasks surrogate modeling problems important pixel code feature maps suitable receptive ﬁeld input output images. observe dataset generated fewer terms input output velocity ﬁelds smoother output stronger correlation across pixels fig. figs. ﬁelds vary rapidly pixel pixel still weak long-range correlation pixel values. experimental setup used skip connections convolution kernels ﬁxed ﬁrst conv layer layers within dense block encoding decoding layers last convtt layer. dropout used growth rate dense blocks taken loss function taken regularized validation metric r-score adam optimizer used training epochs learning rate plateau scheduler test rmse. batch size always smaller number training data weight decay vary dense block conﬁgurations i.e. list integers specifying number layers within dense block. list contains number integers symmetry encoding decoding paths. example blocks specify network architecture fig. a.a. second dense block means layers within dense blocks growth rate layer case code network denseed-c work well dataset since code dimension small. enforces output ﬁeld smooth still favorable dataset. also clearly note network denseed-c work well even though performs well dataset. predictions learning curve selected network design denseed-c presented section experiment hyperparameter optimization hyperband hyperparameters select include ones specifying network architecture training process. separately optimize sets hyperparameters ﬁxed. hyperband bandit random search algorithm several rounds successive halving. input algorithm i.e. maximum iterations hysearch space network architecture search space speciﬁed table constraint number parameters less million gives network architecture presented table several numerical experiments conducted show generalization behavior network denseed-c. network trained diﬀerent block conﬁgurations keeping number training data number parameters ranged training test rmse shown fig. clearly model overparameterized still shows overﬁtting behavior i.e. training error become smaller test error higher increase number parameters. number parameters less plenty room improve training loss test loss. conﬁgure number parameters baseline network denseed-c fig. parameters favorable according generalization curve fig.", "year": 2018}