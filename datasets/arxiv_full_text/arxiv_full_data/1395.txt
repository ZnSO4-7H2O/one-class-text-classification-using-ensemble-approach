{"title": "Early Methods for Detecting Adversarial Images", "tag": ["cs.LG", "cs.CR", "cs.CV", "cs.NE"], "abstract": "Many machine learning classifiers are vulnerable to adversarial perturbations. An adversarial perturbation modifies an input to change a classifier's prediction without causing the input to seem substantially different to human perception. We deploy three methods to detect adversarial images. Adversaries trying to bypass our detectors must make the adversarial image less pathological or they will fail trying. Our best detection method reveals that adversarial images place abnormal emphasis on the lower-ranked principal components from PCA. Other detectors and a colorful saliency map are in an appendix.", "text": "many machine learning classiﬁers vulnerable adversarial perturbations. adversarial perturbation modiﬁes input change classiﬁer’s prediction withcausing input seem substantially different human perception. deploy three methods detect adversarial images. adversaries trying bypass detectors must make adversarial image less pathological fail trying. best detection method reveals adversarial images place abnormal emphasis lower-ranked principal components pca. detectors colorful saliency appendix. images undergo slight pathological modiﬁcations causing machine learning systems misclassify humans barely notice perturbations. types manipulated images adversarial images existence demonstrates frailties machine learning classiﬁers disconnect human computer vision. unexpected divide allow attackers complete leverage deep learning systems. example adversarial images could cause deep learning classiﬁer mistake handwritten digits thereby fooling classiﬁer misread amount check fooling data could evade malware detectors spam ﬁlters deep learning backend worse generating adversarial images requires exact knowledge deep learning system allowing attackers achieve consistent control various classiﬁcation systems paper make progress detecting adversarial images contribute saliency make network classiﬁcation decisions understandable. start presenting ﬁrst detection method reveals adversarial images place abnormally strong emphasis principal components account little variance data. second methods uses observation hendrycks gimpel states correctly classiﬁed many outof-distribution examples tend different softmax distributions apply adversarial images. third detection method show adversarial image reconstructions worse clean image reconstructions decoder uses classiﬁcation information. attempts adversary bypass detector either fails compels adversary make larger modiﬁcations clean image. show ﬁrst three adversarial image detection methods. ﬁrst detector need whiten sphere input. this must center training data zero compute covariance matrix centered data perform whitening taking input example computing σ−/u giving whitened input. whitened vector ﬁrst entry coefﬁcient eigenvector/principal component largest eigenvalue. later entries coefﬁcients eigenvectors smaller eigenvalues. adversarial images different coefﬁcients low-ranked principal components clean images. figure adversarial images abnormally emphasize coefﬁcients low-ranked principal components. plot corresponds coefﬁcients randomly chosen clean adversarial image pair. concreteness principal component coefﬁcient entry σ−/u input image examples randomly chosen. adversarial image coefﬁcients later principal components consistently greater variance shown figure fact coefﬁcient variance sole feature detecting adversarial images. experiment using variance detect whether tiny-imagenet cifar- mnist images adversarial perturbations applied. reduce chance bugs experiment used pretrained tiny-imagenet classiﬁer popular pre-existing adversarial image generator github addition implementation adversarial image generator cifar- mnist. case tiny-imagenet test images pixel range subtract mean. images create fast gradient sign iteratively generated adversarial images. cifar- mnist test images fast gradient sign adversarial images step size iteratively generated images step size regularization strength step sizes small since data domain width instead adversarial examples clipped descent step ensure adversarial perturbations within normal image bounds. finally keep adversarial image softmax prediction probability randomly chosen target class. compute table variance entries whitened adversarial images whitened clean images differ enough allow reliable detection. baseline values perfect detectors correspond entries percentages. detector values computing variance coefﬁcients clean adversarial images. variance values subsection whitened input. speciﬁcally select ﬁnal entry whitened tiny-imagenet input compute variance value vector subsection. cifar- whitened image compute variance entries starting entry. mnist compute variance entries starting position. variances frequently larger adversarial images generated fast gradient sign iterative methods demonstrated table also making adversarial images typical variances seems fail. case fast gradient sign adversarial images image generated large step image’s coefﬁcient variance explodes large modiﬁcation. consider aforementioned coefﬁcient subsections whitened tiny-imagenet cifar- mnist examples. fast gradient sign adversarial images coefﬁcient variance beyond standard deviations mean coefﬁcient variance clean images. remarkably case mnist fast gradient sign images coefﬁcient’s variance beyond billion standard deviations mean coefﬁcient variance clean images. similarly iteratively generated adversarial images fail obtain typical variances. mnist repeat iterative generation procedure described earlier bound search procedure. show bound search establish notation. vexample var) array subsection ﬁnal entry. clean examples clean clean variances vclean vclean variances group standard deviation σclean std) mean µclean mean). desire adversarial example coefﬁcient variance vadversarial within standard deviation σclean mean coefﬁcient variance clean examples µclean. requirement established adding logarithmic barriers loss. constraints search procedure could generate adversarial mnist digit. converting constraints tiny-imagenet popular codebase taken github substantially different mainstream libraries. however convert constraints cifar- images randomly chosen clean images could converted adversarial image satisfactory coefﬁcient variance. encouraging typical coefﬁcient variances appears difﬁcult fast gradient sign iterative adversarial images. would like thank nicolas papernot chris olah helpful feedback early drafts paper. thank greg shakhnarovich numerous insightful suggestions. would also like thank nvidia corporation donating gpus used research. sander dieleman schlter colin raffel eben olson sren kaae snderby daniel nouri daniel maturana martin thoma eric battenberg jack kelly jeffrey fauw michael heilman brian mcfee diogo. lasagne first release. hendrycks kevin gimpel. baseline detecting misclassiﬁed out-of-distribution examples neural networks. international conference learning representations christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. international conference learning representations yuting zhang kibok honglak lee. augmenting supervised neural networks unsupervised objectives large-scale image classiﬁcation. international conference machine learning justify three methods detecting adversarial images need establish detector evaluation metrics. detection classes detector outputs score positive negative class. order evaluate detectors report detection accuracy since accuracy threshold-dependent chosen threshold vary depending practitioner’s trade-off false negatives false positives faced issue employ area receiver operating characteristic curve metric threshold-independent performance evaluation curve graph showing true positive rate false positive rate other. moreover auroc interpreted probability adversarial example greater detector score/value clean example consequently random positive example detector corresponds auroc perfect classiﬁer corresponds auroc sidesteps issue selecting threshold area precisionrecall curve sometimes deemed informative reason auroc ideal positive class negative class greatly differing base rates happen practice. curve plots precision recall other. baseline detector aupr equal precision perfect classiﬁer aupr show classiﬁer behaves respect precision recall also show aupr. hendrycks gimpel show many out-of-distribution examples tend different softmax output vectors in-distribution examples. example in-distribution examples tend greater maximum probability softmax vectors out-of-distribution examples. similarly divergence softmax distribution uniform distribution larger clean examples out-of-distribution examples. therefore adversarial examples stop modiﬁed predicted class probability around often easily distinguishable clean examples since maximum softmax probability clean examples tend larger across many tasks. easily detect whether image adversarial considering resulting softmax distribution adversarial images. softmax distributions metrics gamed done adversarial image becomes less pathological. show this create adversarial image softmax distribution similar clean examples constraining adversarial image generation procedure. experiment take correctly classiﬁed cifar- test images calculate standard deviation divergence softmax output distribution uniform distribution elements u]). create adversarial image iteratively taking step size regularization strength clipping image step. bound search procedure radius proportional σklu] mean divergence u]). bounding accomplished adding logarithmic barriers loss. bounds generator must construct adversarial image within steps typical divergence. succeeds creation success. table observe constraining search procedure ﬁnite radii greatly increases distance. therefore typical softmax distribution adversarial image quality must decline. ﬁnal detection method comes comparing inputs reconstructions. experiment train mnist classiﬁer attach auxiliary decoder reconstruct input. auxiliary decoders sometimes known increase classiﬁcation performance hidden layer size adam optimizer suggested parameters gelu nonlinearity autoencoder bottleneck neurons. crucially also feed logits viz. input softmax bottleneck layer. training create adversarial images iterative procedure like before except increase regularization strength -fold evident figure reconstructions adversarial images look atypical. observation build ﬁnal detector. detector’s input score mean difference input image reconstruction. difference greater adversarial images reconstruction clean examples detect adversarial example clean example auroc aupr baseline values thus differences inputs reconstructions allow adversarial image detection. stop considering adversarial images turn different safety goal. goal make neural networks interpretable. common understand network classiﬁcations saliency maps. simple make saliency computing gradient network’s logits respect input backpropagating error signal without modifying weights. error signal traversed backward network display resulting gradient image shows salient parts image. recently proposed technique improve saliency maps guided backpropagation springenberg understand guided backpropagation establish notation. normally training improve resulting saliency signiﬁcantly instead leading saliency map. demonstrate results figure using pretrained vgg- model positive saliency consists positive gradient values. ﬁrst column network classiﬁes desert scene lakeside. saliency reveals coloring clouds bluer would increase logits would making sunlit orange. surmise clouds serve lakeside water shore. next construed water snake saliency background resembles water ﬁsh’s scales greener articulated like water snake. also saliency white regions become orange would increase logits. meanwhile guided backpropagation shows ﬁsh’s eye. last saliency makes evident bluer contrasting darker lower region would increase logits suggesting blue sky’s role image’s seashore misclassiﬁcation. overall saliency allows visualizing clearer sources saliency. although showed diverse detectors future ad-hoc attacks completely bypass these. indeed softmax distribution detector already bypassed. adversarial image must contort evade detector therefore reduce level pathology. extend idea conclude better line attack stopping adversarial images. instead seeking statistic architecture trick blockade adversarial attacks might combination detectors effectively lessen image’s ability fool. narrow metric gamed necessarily ensemble several strong imperfect predictors. therefore different avenue attack adversarial images amassing combination detectors. safeguard adversarial images forcing adversarial perturbation conspicuous. work showcased techniques detecting adversarial images visualizing salient regions image. future research could onto body detectors since ensemble detectors feasible avenue defending adversarial images. possible future detector could attention. adversarial image made picture street classiﬁed tank. since adversarial image attacks seem localized attention tank’s turret scattered uniform across image thus abnormal detectable. work could effective nearly harmless preprocessing hinder adversarial perturbations. example preliminary experiment shows cifar- image square pixels apply slight gaussian blur take square root result image often longer tricks network allowing detect adversarial attack. adversarial generator anticipates preprocessing optimization norm adversarial image must increase average. preprocessing techniques could studied onto ensemble defenses. finally general work out-of-distribution detection error detection transfer directly adversarial setting. future research bear mind adversarial attacks invisible human know perturbations greatly affect statistics underlying image reconstruction softmax distribution thereby enabling detection.", "year": 2016}