{"title": "Learning Discrete Bayesian Networks from Continuous Data", "tag": ["cs.AI", "cs.LG"], "abstract": "Real data often contains a mixture of discrete and continuous variables, but many Bayesian network structure learning and inference algorithms assume all random variables are discrete. Continuous variables are often discretized, but the choice of discretization policy has significant impact on the accuracy, speed, and interpretability of the resulting models. This paper introduces a principled Bayesian discretization method for continuous variables in Bayesian networks with quadratic complexity instead of the cubic complexity of other standard techniques. Empirical demonstrations show that the proposed method is superior to the state of the art. In addition, this paper shows how to incorporate existing methods into the structure learning process to discretize all continuous variables and simultaneously learn Bayesian network structures.", "text": "real data often contains mixture discrete continuous variables many bayesian network structure learning inference algorithms assume random variables discrete. continuous variables often discretized choice discretization policy signiﬁcant impact accuracy speed interpretability resulting models. paper introduces principled bayesian discretization method continuous variables bayesian networks quadratic complexity instead cubic complexity standard techniques. empirical demonstrations show proposed method superior state art. addition paper shows incorporate existing methods structure learning process discretize continuous variables simultaneously learn bayesian network structures. bayesian networks often used model uncertainty causality applications ranging decision-making systems medical diagnosis bayesian networks provide eﬃcient factorization joint probability distribution random variables. common assume random variables bayesian network discrete since many bayesian network learning inference algorithms unable eﬃciently handle continuous variables. addition many commonly used bayesian network software packages netica smilearn bnlearn geared towards discrete variables. however many applications require continuous variables position velocity dynamic systems three common approaches extending bayesian networks continuous variables. ﬁrst model conditional probability density continuous variable using speciﬁc families parametric distributions redesign bayesian network learning inference algorithms based parameterizations. example parametric continuous distributions bayesian networks gaussian graphical model second approach nonparametric distributions particle representations gaussian processes unlike parametric methods nonparametric methods often underlying probability distribution given suﬃcient data. parametric models ﬁxed number parameters whereas number parameters nonparametric model grow amount training data. third approach discretization. automated discretization methods studied machine learning statistics many years primarily classiﬁcation problems. methods search best discretization policy continuous attribute considering interaction class variable. common discretize continuous variables learning bayesian network structure avoid consider variable interactions interactions dependencies variables bayesian networks introduce complexity. prior work exists discretizing continuous variables naive bayesian networks tree-augmented networks discretization methods general bayesian networks proposed common discretization method bayesian networks split continuous variables uniform-width intervals using ﬁeld-speciﬁc expertise. principled method minimum description length principle discretization principle proposed rissanen states best model dataset minimizes amount information needed describe methods trade goodness-of-ﬁt model complexity reduce generalization error. context bayesian networks friedman goldszmidt applied principle determine optimal number discretization intervals continuous variables optimal positions discretization edges. approach selects discretization policy minimizes description length discretized bayesian network information necessary recovering continuous values discretized data. optimal discretization policy single continuous variable found using dynamic programming cubic runtime number data instances. bayesian networks multiple continuous variables discretization iteratively applied continuous variable. variable treated continuous time continuous variables treated discretized based initial discretization policy discretization result previous iteration. iterative approach allows incorporated structure learning process. simultaneous structure learning discretization alternates traditional discrete structure learning optimal discretization. starting preliminary discretization policy ﬁrst applies structure learning algorithm identify locally optimal graph structure. reﬁnes discretization policy based learned network. cycle repeated convergence. results work suggest method suﬀers sensitivity discretization edge locations returns discretization intervals continuous variables. caused mdl’s mutual information measure quality discretization edges. mutual information composed empirical probabilities computed using event count ratios varies less signiﬁcantly positions discretization edges method suggest article. modl bayesian method discretizing continuous feature according class variable selects model maximum probability given data. modl method uses dynamic programming optimal discretization runtime number class variable instantiations. lustgarten suggest several formulations prior models. asymptotic equivalence between modl single-variable single-class problem examined vit´anyi paper describes bayesian discretization method continuous variables bayesian networks extending prior work single-variable discretization methods boull´e lustgarten proposed method optimizes discretization policy relative network takes parents children spouse variables account. optimal single-variable discretization method derived sacriﬁcing optimality. section covers bayesian networks multiple continuous variables section covers discretization simultaneously learning network structure. paper concludes comparison existing minimumdescription length method real-world datasets section given dataset sorted ascending order unique values index last occurrence denoted discretization edges mark boundaries discretization intervals. paper modl restricted midpoints unique ascending instances thus edge equals j+)/ useful integer representations written bayesian network random variables deﬁned directed acyclic graph whose nodes random variables conditional probability distribution node given parents. edges bayesian network represent probabilistic dependencies among nodes encode markov property node independent non-descendants given parents paxi children node denoted chxi. concept behind factorization also motivation behind forward sampling bayesian network. parents independent children given parents necessarily individually independent thus parental term factors right hand side come second principle distributions values parents given interval equiprobable. fourth principle distributions interval independent factors multiplied together. evaluation number instantiations child number instantiations spouse number instances discretization interval given instantiation instantiation follows factors right hand side come third principle distribution values given interval given value equiprobable. according fourth prior distributions independent other optimization problem equation solved. dynamic programming procedure shown algorithm takes three inputs continuous variable; joint data instances variables sorted ascending order according network structure. runtime algorithm also single-variable discretization method extended bayesian networks multiple continuous variables iteratively discretizing individual variables. discretization process single variable requires variables discrete. iterative approach uses initial discretization policy order start process assigns equal-width intervals continuous variable largest number intervals initially discrete variables network. initial discretization one-variable discretization method iteratively applied continuous variable reverse topoligical order leaves root. reverse topological order advantage relying fewer initial discretizations continuous variables ﬁrst pass. example network figure discrete variable discretization involves whereas discretization involves algorithm terminated number discretization intervals associated edges converge variables maximum number complete passes enforced prevent inﬁnite iterations convergence occur. algorithm typically converges within passes tested real-world data. pseudocode multi-variable discretization procedure shown algorithm requires four inputs dataset samples joint distribution; ﬁxed network structure; continuous variables reverse topological order ˆncycle upper bound number complete passes. often necessary infer structure bayesian network data. three common approaches bayesian network structure learning constraint-based scorebased bayesian model averaging work uses structure learning algorithm frequently used score-based structure learning method. score-based structure learning bayesian network discrete discretized variables represents number instantiations represents number instantiations parents parents bayesian score space acyclic graphs superexponential number nodes; common rely heuristic search strategies algorithm assumes given topological ordering variables greedily adds parents nodes maximally increase bayesian score. ﬁxed ordering ensures acyclicity guarantee globally optimal network structure. typically multiple times diﬀerent topological orderings network highest likelihood retained. traditional bayesian structure learning algorithms require discretized data whereas proposed discretization algorithm requires known network structure. discretization methods combined structure learning algorithm simultaneously perform bayesian structure learning discretization continuous variables. proposed algorithm alternates structure learning discretization. dataset initially discretized described section obtain initial network structure. aﬀected continuous variables rediscretized every time edge added resulting discretization policies used update discretized dataset next step algorithm executed. cycle repeated algorithm converges. procedure given algorithm takes inputs dataset samples joint distribution; continuous variables; order permutation variables ˆnparent upper bound number parents node; ˆncycle upper bound number complete passes. upper bound number parents common practice used prevent computing conditional distributions excessively large parameter sets. function algorithm computes component bayesian score common practice multiple times diﬀerent variable permutations choose structure highest score. such algorithm multiple times diﬀerent variable permutations discretized bayesian network highest score retained. section describes experiments conducted evaluate bayesian discretization method. experiments datasets publically available university california irvine machine learning repository variables labeled alphabetically order given dataset information webpage. ﬁgures follow shaded nodes correspond initially discrete variables subscripts indicate number discrete instantiations. experiments conducted dataset. ﬁrst experiment compares performance bayesian discretization methods known bayesian network structure. structure obtained discretizing continuous variable uniform-width intervals median number instantiations discrete variables using structure highest bayesian score runs algorithm random topological orderings. second experiment compares methods applied simultaneously discretizing learning network structure. number variables bayesian network max{v discretization policies dataset discretized according initially edgeless graph structure discretized version maxi). mean cross-validated lognote likelihood mean log-likelihood witheld dataset among cross-validation folds acts estimate generalization error. folds used experiment. cretization intervals fourth term cumulative intervals. hence optimal discretization policy intervals optimal discretization policy corresponding subprobnote preliminary discretization discretization order variables arbitrary discretization method stated original work order make fair comparison bayesian method preliminary discretization discretization order follows procedure section auto dataset contains variables related fuel consumption automobiles urban driving. dataset samples eight variables including instances missing data. three variables discrete instantiations respectively. bayesian discretization methods tested auto data using network shown figure structure obtained initially discretizing continuous variable uniform-width intervals median cardinality discrete variables taking structure highest likelihood runs table lists discretization edges mean log-likelihoods -fold cross validation discrete bayesian network resulting discretization methods. method produce discretization edges assigning continuous interval continuous variable produces result lower likelihood. reason produces fewer discretization edges discussed table discretization result auto dataset ﬁxed structure figure ﬁrst rows list discretization edges last lists mean cross-validated log-likelihood; positive values better. figure shows marginal probability density variables bayesian discretization policy overlaid original auto data. resulting probability density good match original data. figure comparison bayesian discretization policy variables original auto data learned ﬁxed network. marginal probability density closely matches data. experiment network structure ﬁxed advance learned simultaneously discretization policies. figure shows learned bayesian network structure corresponding numbers intervals discretization continuous variable. result obtained running algorithm ﬁfty times using bayesian method choosing structure highest score figure compares bayesian discretizaton policy variables learned network original auto data. color discretized region indicates marginal probability density sample drawn region. although fewer discretization edges learned network marginal distribution still captured. discretization policy vary network structure changes still produces high-quality discretizations. wine dataset contains variables related chemical analysis wines three diﬀerent italian cultivars. dataset samples fourteen variables. variable discrete variable three instantiations. bayesian discretization methods tested wine data using network shown figure structure obtained initially discretizing continuous variable three uniform-width intervals three median cardinality discrete variables taking structure highest likelihood runs table lists discretization edges mean log-likelihoods -fold cross validation bayesian network resulting discretization method. bayesian method outperforms method likelihood signiﬁcant margin. figure comparison bayesian discretization policy variables original auto data learned simultaneously network structure. although number discretization edges less figure probability distribution still closely matches original data. discretization edges appear three discretization methods variable variable indicates suﬃciently indeed important discretization edges sensitive edges. figure compares bayesian discretizaton policies variables original wine data. discretization edges appear plots. method enough intervals discretization. relative sensitivities method input data discussed section figure discrete-valued bayesian network learned wine dataset obtained running algorithm ﬁfty times. comparison figures show bayesian network learned discretization process edges network learned initially discretized data. network learned along discretization algorithm freedom adjust structure discretization policy simultaneously identify useful correlations produce denser structure. figure shows discretization policy variables obtained bayesian method. discretization edge also appears discretization policies ﬁxed network suggests discretization edges robust network structure. furthermore discretization edge ﬁxed-structure case missing learned structure. caused twice many parents learned network structure. parents variable fewer discretization intervals support number suﬃcient statistics required deﬁne resulting distribution increases exponentially number parents. housing dataset contains variables related values houses boston suburbs. dataset samples fourteen variables. variables discrete-valued instantiations respectively. despite continuous several variables housing dataset possess many repeated values. following experiments conducted maximum three parents variable prevent running memory running mdl. bayesian network structure figure obtained initially discretizing continuous variable uniform-width intervals running algorithm times choosing network highest likelihood. table shows numbers interval discretization continuous variable log-likelihood dataset based discretization method. method produce discretization edges variables. relative weighting method’s objective function discussed section figures show discretization policy learned using bayesian approach ﬁxed network shown figure scatter points figure jittered show quantity repeated values variables repeated point forms single discrete region thereby encouraging discretization. contrast samples well spread resulting fewer discretization regions larger discretization intervals. table discretization policy summary housing dataset based ﬁxed network structure shown. ﬁrst twelve rows show numbers discretization intervals last mean cross-validated log-likelihood. figure discretization policies variables learned using bayesian approach ﬁxed network. scatter points jittered reveal repeated values dataset. values perfectly repeated lead high correlation concentrated densities marginal distribution. figure shows learned bayesian network structure corresponding numbers intervals discretization continuous variable. variable neighborless learned ﬁxed networks. continuous variables connected many discretization intervals. typically discretizing variable expected number discretization intervals close highest cardinality among variables markov blanket. naturally leads clusters variables many discretization intervals. figures show discretization result variables network shown figure again variable many discretization edges repeated values. although number intervals discretization less number figure still captures distribution data along discretization edges figure discretization policies variables learned using optimal bayesian approach ﬁxed network shown figure values variable repeated thus producing fewer discretization intervals figure discretization policy variables learned network figure large number discretization edges repeated values regions color-saturated high concentration repeated values. figure discretization policy variables learned network figure fewer discretization intervals ﬁxed network structure figure discretization result still closely models data. penalty terms tend increase number discretization intervals; edge position terms vary position number discretization edges. policy larger number discretization edges optimal edge position term varies enough respect penalty term order produce local minimum suﬃciently value. value edge position term primarily determined mutual information varies less severely corresponding terms bayesian method. term uses empirical probability distributions based ratios counts whereas bayesian method uses factorial terms thus method varies less. method therefore less sensitive discretization edges. sensitivity gives rise relative performance methods experiments conducted above. paper introduced principled discretization method continuous variables bayesian networks quadratic complexity instead cubic complexity standard techniques. empirical demonstrations show proposed method superior state art. addition paper shows incorporate existing methods structure learning process discretize continuous variables simultaneously learn bayesian network structures. proposed method incorporated superior performance empirically demonstrated. future work investigate edge positions locations midpoints samples extend approach cluster categorical variables many levels. software publicly available github.com/sisl/learndiscretebayesnets.jl.", "year": 2015}