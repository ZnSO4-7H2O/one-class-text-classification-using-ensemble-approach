{"title": "Towards Dropout Training for Convolutional Neural Networks", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Recently, dropout has seen increasing use in deep learning. For deep convolutional neural networks, dropout is known to work well in fully-connected layers. However, its effect in convolutional and pooling layers is still not clear. This paper demonstrates that max-pooling dropout is equivalent to randomly picking activation based on a multinomial distribution at training time. In light of this insight, we advocate employing our proposed probabilistic weighted pooling, instead of commonly used max-pooling, to act as model averaging at test time. Empirical evidence validates the superiority of probabilistic weighted pooling. We also empirically show that the effect of convolutional dropout is not trivial, despite the dramatically reduced possibility of over-fitting due to the convolutional architecture. Elaborately designing dropout training simultaneously in max-pooling and fully-connected layers, we achieve state-of-the-art performance on MNIST, and very competitive results on CIFAR-10 and CIFAR-100, relative to other approaches without data augmentation. Finally, we compare max-pooling dropout and stochastic pooling, both of which introduce stochasticity based on multinomial distributions at pooling stage.", "text": "recently dropout seen increasing deep learning. deep convolutional neural networks dropout known work well fully-connected layers. however effect convolutional pooling layers still clear. paper demonstrates max-pooling dropout equivalent randomly picking activation based multinomial distribution training time. light insight advocate employing proposed probabilistic weighted pooling instead commonly used max-pooling model averaging test time. empirical evidence validates superiority probabilistic weighted pooling. also empirically show effect convolutional dropout trivial despite dramatically reduced possibility over-fitting convolutional architecture. elaborately designing dropout training simultaneously max-pooling fully-connected layers achieve state-of-the-art performance mnist competitive results cifar- cifar- relative approaches without data augmentation. finally compare max-pooling dropout stochastic pooling introduce stochasticity based multinomial distributions pooling stage. words deep learning; convolutional neural networks; max-pooling dropout deep convolutional neural networks recently told many success stories visual recognition tasks record holders many challenging datasets. standard consists alternating convolutional pooling layers fully-connected layers top. compared regular feed-forward networks similarly-sized layers cnns much fewer connections parameters local-connectivity shared-filter architecture convolutional layers less prone over-fitting. another nice property cnns pooling operation provides form translation invariance thus benefits generalization. despite attractive qualities despite fact cnns much easier train regular deep feed-forward neural networks cnns millions billions parameters still easily overfit relatively small training data. dropout recently proposed regularizer fight over-fitting. regularization method stochastically sets zero activations hidden units training case training time. breaks co-adaptions feature detectors since dropped-out units cannot influence retained units. another interpret dropout yields efficient form model averaging number trained models exponential units models share parameters. dropout also inspired stochastic model averaging methods stochastic pooling dropconnect although dropout known work well fully-connected layers convolutional neural nets effect convolutional pooling layers however well studied. paper shows using max-pooling dropout training time equivalent sampling activation based multinomial distribution distribution tunable parameter light this probabilistic weighted pooling proposed employed test time efficiently average possibly max-pooling dropout trained networks. empirical evidence confirms superiority probabilistic weighted pooling max-pooling. like fully-connected dropout number possible max-pooling dropout models also grows exponentially increase number hidden units pooling layers decreases increase pooling region’s size. also empirically show effect convolutional dropout trivial despite dramatically reduced possibility over-fitting convolutional architecture. carefully designing dropout training simultaneously max-pooling fully-connected layers report state-of-the-art results mnist competitive results cifar- cifar- comparisons approaches without data augmentation. stochastic pooling max-pooling dropout randomly sample activation based multinomial distributions pooling stage becomes interesting compare performance. experimental results show stochastic pooling performs max-pooling dropout different retaining probabilities max-pooling dropout typical retaining probabilities often outperforms stochastic pooling large margin. vincent cnns regained attentions machine learning computer vision community. like deep models many issues arise deep cnns naively trained. main combat over-fitting wide range regularization techniques developed. simple effective method adding penalty network weights. common forms regularization include early stopping bayesian fitting weight elimination data augmentation. practice employing techniques training neural bagging model independent parameters members would trained explicitly. case dropout training exponentially many possibly trained models models share arithmetic mean obvious exponentially many models trained dropout. fortunately average prediction exponentially many sub-models approximately computed simply running whole network weights scaled retaining probability. approximation mathematically characterized linear sigmoidal networks applied fully-connected layers. reason provided convolutional shared-filter architecture drastic reduction number parameters thus reduced possibility overfit convolutional layers. wonderful work krizhevsky trained convolutional neural million parameters classify million high-resolution images imagenet different categories. primary methods used reduce over-fitting experiments. first data augmentation easiest commonly used approach reduce over-fitting image data. dropout exactly second one. also used fully-connected layers. ilsvrc- competition deep convolutional neural yielded top- test error rate better second-best entry achieved shallow learning hand-craft feature engineering. considered breakthrough computer vision. community believes deep convolutional nets perform best simple hand-written digits also really work complex natural images. compared original work dropout provided exhaustive experimental results. experiments cifar- using dropout fully-connected layers reduced test error adding dropout convolutional layers reduced error revealing applying dropout convolutional layers aided generalization. similar performance gains observed cifar- svhn. still explore max-pooling dropout. stochastic pooling dropout-inspired regularization method. authors replaced conventional deterministic pooling operations stochastic procedure. instead always capturing strongest activity within pooling region max-pooling does stochastic pooling randomly picks activations according multinomial distribution. test time probability weighting used estimate average possible models. interestingly stochastic pooling resembles case using dropout max-pooling layers worth comparing them. feed-forward nets. instead setting zero activations sets randomly picked subset weights within network zero probability words fully-connected layer dropconnect becomes sparsely connected layer connections chosen stochastically training. unit thus receives input random subset units previous layer. dropconnect resembles dropout involves stochasticity within model differs stochasticity weights rather output vectors layer. results several visual recognition datasets showed dropconnect often outperformed dropout. maxout network another model inspired dropout. maxout unit picks maximum value within group linear pieces activation. type nonlinearity generalization rectified activation function capable approximating arbitrary convex function. combining dropout maxout networks shown achieve best results mnist cifar- cifar- svhn. however authors train maxout networks without dropout. besides train rectified counterparts dropout directly compare maxout networks. therefore clear factor contributed remarkable results. demonstrate max-pooling dropout equivalent sampling activation according multinomial distribution training time. basing interpretation propose probabilistic weighted pooling test time. also describe convolutional dropout. illustrating example showing procedure max-pooling dropout. activation pooling region respectively. without dropout strongest activation always selected output. dropout unit pooling region could possibly dropped out. example retained pooled output. size feature layer size filters number convolved features convolved feature scalar product filter’s weights local region’s activations number possibly different similarly dropout turned test time. whole network containing units layer used filter’s weights scaled retaining probability. efficiently gets estimate averaging possibly trained dropout networks. expect convolutional dropout helps generalization reducing over-fitting. however less advantageous since shared-filter local-connectivity architecture convolutional layers drastic reduction number parameters already reduces possibility overfit empirical results confirm point improve generalization test data often inferior max-pooling fully-connected dropout. purpose experiments threefold provide empirical evidence probabilistic weighted pooling accurate approximation averaging possibly max-pooling dropout trained models max-pooling scaled max-pooling explore dropout training different layers compare max-pooling dropout stochastic pooling. rectified linear function convolutional fully-connected layers softmax activation function output layer. commonly used sigmoidal tanh nonlinearities adopted gradient vanishing problem them. vanishing gradient effect causes slow optimization convergence training deep model already computationally expensive. local response normalization applied applying rectified non-linearity certain layers train models using stochastic mini-batch gradient descent batch size momentum learning rate minimize cross entropy loss. weights layers initialized zero-mean gaussian distribution standard deviation constant neuron biases layers. heuristic follow reduce learning rate twice factor terminating training. retain unit probability default. specially first fully-connected layer. experiments conducted three widely used datasets mnist cifar- cifar-. fig. displays example images. architecture experiments denoted following xx-c-pc-p-n-n represents input image size convolutional layer feature maps filters pooling layer pooling region stride convolutional initially conduct experiments using mnist widely used benchmark dataset computer vision. consists pixel grayscale images containing digit training test examples. perform pre-processing except scaling pixel values fig. mnist training test errors different pooling methods test time. max-pooling dropout used training. max-pooling without dropout presented baseline. illustrate smaller architecture xx-c-pc-p-n-n. illustrate training test errors produced bigger xx-c-p-c-p-n-n. investigate effect using dropout different layers first train different models separately using dropout input convolutional max-pooling fully-connected layers. models trained using architectures xx-c-p-c-p-n-n fig. mnist training test errors separately using convolutional max-pooling fully-connected dropout. case without dropout also presented comparison’s purpose. architecture xx-c-p-c-p-n-n. illustrate training test errors produced bigger xx-c-p-c-p-n-n. xx-c-p-c-p-n-n. case without dropout used baseline. max-pooling dropout probabilistic weighted pooling used test time superiority max-pooling scaled max-pooling. able mnist test errors xx-c-p-c-p-n-n trained dropout various types layers compared current state-of-the-art models excluding methods augment training data. observed separately using convolutional max-pooling fully-connected dropout reduces over-fitting improves test performance. achieve better results using dropout simultaneously input different layers. max-pooling dropout probabilistic weighted pooling used test time. training epochs using architecture xx-c-p-c-p-n-n. comparison’s purpose results separately using dropout different layers also presented. table records models’ test errors well fig. compares different pooling methods test time max-pooling dropout trained models cifar-. retaining probability respectively. again probabilistic weighted pooling shows clear superiority max-pooling scaled max-pooling. train various models separately simultaneously using dropout different layers cifar-. max-pooling dropout trained models probabilistic weighted pooling used test time. shown table improves generalization test data using dropout separately convolutional max-pooling fully-connected layers. again dropout employed improperly performance would decrease simultaneously using convolutional maxpooling dropout. best result achieved using max-pooling fully-connected dropout. similar max-pooling dropout stochastic pooling also randomly picks activation according multinomial distribution training time also involves probabilistic weighting test time. concretely training time first computes probability unit within pooling region layer normalizing activations fig. mnist training test errors stochastic pooling max-pooling dropout different retaining probabilities. test errors produced xx-c-p-c-p-n-n. illustrate training test errors produced xx-c-p-c-p-n-n. ooling falls max-pooling dropout architectures. test data stochastic pooling performs worst smaller architecture. bigger test performance stochastic pooling max-pooling dropout clear comparison train models different retaining probabilities mnist cifar- cifar-. max-pooling dropout trained models probabilistic weighted pooling used test time. fig. compares test performances max-pooling dropout different retaining probabilities stochastic pooling. observed relation fig. test errors max-pooling dropout different retaining probabilities stochastic pooling. architecture xx-c-p-c-p-n-n mnist xx-cp-c-p-c-p-n-n-n cifar- xx-c-p-cp-c-p-n-n-n cifar-. performance max-pooling dropout retaining probability u-shape. small large max-pooling dropout performs poorer stochastic pooling. max-pooling dropout typical outperforms stochastic pooling large margin. therefore although stochastic pooling hyper-parameter free saves tuning retaining probability performance often inferior max-pooling dropout.", "year": 2015}