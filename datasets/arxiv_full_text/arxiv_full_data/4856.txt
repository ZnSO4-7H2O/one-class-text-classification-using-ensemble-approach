{"title": "Can Active Learning Experience Be Transferred?", "tag": ["cs.LG", "cs.AI"], "abstract": "Active learning is an important machine learning problem in reducing the human labeling effort. Current active learning strategies are designed from human knowledge, and are applied on each dataset in an immutable manner. In other words, experience about the usefulness of strategies cannot be updated and transferred to improve active learning on other datasets. This paper initiates a pioneering study on whether active learning experience can be transferred. We first propose a novel active learning model that linearly aggregates existing strategies. The linear weights can then be used to represent the active learning experience. We equip the model with the popular linear upper- confidence-bound (LinUCB) algorithm for contextual bandit to update the weights. Finally, we extend our model to transfer the experience across datasets with the technique of biased regularization. Empirical studies demonstrate that the learned experience not only is competitive with existing strategies on most single datasets, but also can be transferred across datasets to improve the performance on future learning tasks.", "text": "setups machine learning study experience transferred future tasks. simplest setup transfer learning inductive transfer. transfer learning accumulating experience several source tasks applying experience related target task. several attempts made previous studies improve performance active learning transfer learning however algorithms proposed studies transfer experience supervised semi-supervised learning source tasks target task transfer experience active learning furthermore algorithms assume shared feature space different tasks experience transfer heterogeneous active learning tasks studied. related setups include never-ending learning lifelong learning. never-ending learning rather general setup deﬁnes machines learn like humans transfer experience different tasks self-supervised manner realized system accumulating beliefs reading continuously life-long learning hand considers feeding machines sequence tasks hope improving performance next task sequence. setup similar thought realized sentiment classiﬁcation tasks best knowledge neither never-ending lifelong learning carried active learning tasks. fact allowing machine mimic humans life-long active learning highly non-trivial experience accumulated transferred heterogeneous active learning tasks well-deﬁned mention applying past experience future learning tasks. paper introducing cross-dataset active learning problem section ﬁrst propose notion machine experience transferred across active learning tasks section iii. notion based encoding human knowledge active learning scoring functions existing active learning algorithms representing machine experience linear weights combine human knowledge. notion existing active learning algorithms simply viewed taking special immutable weights combine knowledge. active learning important machine learning problem reducing human labeling effort. current active learning strategies designed human knowledge applied dataset immutable manner. words experience usefulness strategies cannot updated transferred improve active learning datasets. paper initiates pioneering study whether active learning experience transferred. ﬁrst propose novel active learning model linearly aggregates existing strategies. linear weights used represent active learning experience. equip model popular linear upperconﬁdence-bound algorithm contextual bandit update weights. finally extend model transfer experience across datasets technique biased regularization. empirical studies demonstrate learned experience competitive existing strategies single datasets also transferred across datasets improve performance future learning tasks. many machine learning applications high-quality labels costly obtain active learning machine learning scenario tries reduce labeling cost still maintaining performance learned models asking labeling questions current active learning algorithms based human knowledge questions knowledge applied immutably every dataset conducting active learning. recent work argued single active learning algorithm based immutable human knowledge unlikely perform well datasets hence proposed adaptively learn probabilistic blending human-designed active learning algorithms. blending learned within single dataset connecting multi-armed bandit learning. given possibility learn decent blending different pieces human knowledge within single dataset thought learned experience transferred datasets improve performance active learning? thought related human beings learn questions real life. learn questions within single learning task; instead accumulate experience question-asking past current learning tasks transfer experience future learning tasks. weights active learning process. inspired aforementioned work connect problem updating linear weights contextual bandit learning. based connection apply state-of-the-art contextual bandit algorithm linear upper-conﬁdence-bound update weights. resulting approach effectively blends existing active learning algorithms towards better performance. extend proposed approach allow learned experience transferred across datasets section transferring extension based idea biased regularization restricts adaptive weights close past experience. simple formulation biased regularization seamlessly coupled linucb algorithm form transferring extension. empirical studies section demonstrate approach competitive existing active learning algorithms. results also indicate transferring extension effectively improves learning performance approach experience learned heterogeneous homogeneous tasks thus demonstrating usefulness learned experience. finally conclude possibility transferring active learning experience section work focus popular active learning setup called pool-based active learning binary classiﬁcation. setup active learning algorithm presented labeled pool unlabeled pool initially. denote labeled pool ...} unlabeled pool ˜xnu} {+−}. general algorithm access small beginning size relatively large. initial algorithm calls base model learn classiﬁer then given budget iteration .... algorithm allowed query label given labeling oracle. instance-label pair moved base model called enlarged learn classiﬁer goal algorithm make performance good possible performance measured test accuracy separate test work. also study active learning experience accumulated across datasets. setup cross-dataset active learning algorithm active learning present sequence datasets hope improving active learning performance along sequence like life-long learning speciﬁcally hope experience accumulated exploited conducting active learning iteration scoring function instance subject current classiﬁer ht−. algorithm shall denote classic intuitive strategy called uncertainty sampling queries instance classiﬁer uncertain with. realizes uncertainty sampling strategy scoring function computes inverse distance hyperplane learned support vector machine works argue uncertainty sampling works well close enough ideal boundary result unsatisfactory performance good enough representative sampling family strategies based different scoring function tries improve uncertainty sampling. example applies kmeans clustering takes inverse distance cluster center scoring function representativeness modulated whether resides inside margin classiﬁer ht−. equips gaussian distributions k-means clustering calculate representativeness proposes scoring function multiplies uncertainty representativeness. optimizes scoring function based estimating label assignments min-max view argues optimized scoring function covers uncertainty representativeness. strategies embed human knowledge labeling questions scoring functions. several works also consider selecting strategies adaptively better performance motivated fact human-designed scoring functions cannot always match dataset characteristics thus adaptive selection necessary. state-of-the-art approach active learning learning performs adaptive strategy selection connecting selection problem bandit learning designs learning-performance-based reward function guide bandit learner selecting reasonable strategies probabilistically. internal probability strategy gets selected reﬂects goodness strategy updated within single dataset. recall accumulate active learning experience across datasets. human-designed scoring functions cannot help generally immutable cannot adaptively change experience. na¨ıve extending current adaptive-selection approaches accumulating active learning experience deﬁne experience internal probability distribution selections transfer distribution next active learning task. nevertheless shall section unstable nature probabilistic choices makes distribution volatile serve robust active learning experience practice. section shall ﬁrst introduce notion active learning experience. propose novel active learning approach linear strategy aggregation queries unlaintroduced section scoring functions humandesigned active learning algorithms represent pieces human knowledge labeling questions. proper combine different pieces human knowledge namely different scoring functions naturally viewed experience active learning. speciﬁcally consider combining blending human-designed scoring functions scoring function better performance deﬁne blending parameters experience. note current adaptive-selection approaches cannot fully match novel deﬁnition blend recommended queries scoring functions instead blending scoring functions directly. take initiative deﬁnition consider simplest model scoring functions blended linearly leave possibility using sophisticated models future directions. particular given scoring functions different humandesigned strategies aggregated scoring function wmsm. weight vector contains blending parameters serves experience transferred. notion experience established introduce proposed approach linear strategy aggregation solves task adaptively updating experience querying unlabeled instance maximize active learning performance. motivated previous adaptive selection approaches design connection task well-known adaptive learning problem contextual bandit ﬁrst discuss details contextual bandit problem. setup contextual bandit problem follows player presented actions budget iteration context vector action provided player required perform action action performed corresponding reward rktt revealed. objective player maximize cumulative reward. maximize cumulative reward player typically required balance exploration exploitation many algorithms contextual bandit problem studied literature family estimates reward action linear model corresponding context stateof-the-art algorithm family called linear upperconﬁdence-bound carries strong theoretical guarantees also performs well realworld tasks next take closer look linucb ﬁrst term corresponds estimated reward action iteration second term represents uncertainty action context vector. parameter controls preference exploration exploitation follow pioneer blending approach active learning connect active learning contextual bandit linucb. particular treat action |du|}. then performing action iteration linucb equivalent querying corresponding ˜xkt lsa. remaining issues specify context vectors ˜zkt rewards rktt calculated. ﬁrst discuss choice context vectors achieve experience updating illustrate design rewards represents active learning performance section iii-c. algorithm linear strategy aggregation parameters linucb balancing parameter ridge regression parameter minimum goodness parameter number iterations input labeled pool unlabeled pool scoring functions s··· sm}; labeling oracle then vector linucb corresponds evolving experience calculated ridge regression; inner ﬁrst term corresponds product aggregated scoring function made current experience human knowledge {sm}m queries unlabeled instance contains well exploration term introduced linucb updates experience recall goal ridge regression within linucb provide good estimate context vector reward. apply trick improve quality estimate. particular another element element constant value previous reward rewards deﬁned section iii-c. according added constant affect choice allows ridge regression utilize previous reward estimating current reward. words value provides shared context active learning performance assist linear model. empirically observe trick indeed improves quality estimate stability lsa. issue left properly designed reward represents active learning performance namely test accuracy work. state-of-the-art reward function proposed called importance-weighted accuracy used active learning learning approach iw-acc weighs instance inverse probability instance queried calculates weighted accuracy reward. importance weighting allows iw-acc unbiased estimator test accuracy. nevertheless unlike albl deterministic algorithm based linucb. thus pktt iw-acc cannot directly taken reward. thus propose reward scheme mimics idea iw-acc. proposed scheme instance ˜xkt queried iteration weighted recall maximizes decide instance queried. reﬂects goodness unlabeled instance ˜xkt. using inverse uktt weights proposed scheme effectively meets idea importance weighting behind iw-acc avoiding need probabilistic queries. small constant guards rare edge cases uktt proposed rewards another serving rkt−t− technically previous reward simplest choice would taking representing random-guessing accuracy. work heuristically take training accuracy learning initial order provide better shared context performance. proposed scheme ﬁnal piece complete. iteration simply runs linucb query unlabeled instance ˜xkt using updates experience context vector zktt well proposed reward rktt details listed algorithm able adaptively update experience within single dataset. next goal achieve experience transfer across datasets hope improving active learning performance. thus design extension called transfer takes learned experience reference conducting active learning current dataset. design motivated earlier work focuses personalized handwriting recognition main idea work ﬁrst learn generic handwriting recognizer wgen large amount handwriting data people. personalized handwriting recognizer learned small amount individual data biased regularization brsvm replaces objective function regularization term w−wgen enforce biased regularization term personalized close generic wgen. brsvm personalized handwriting recognizer allows learning prior knowledge wgen reference point. cross-dataset active learning problem intend take wprev experience learned datasets reference point. simplicity ﬁrst assume wprev comes experience active learning previous dataset. wprev learned recall ridge-regression solution then borrow idea brsvm wprev regularization replace term. biased regularization simply achieved solving notice difference term λwprev thus easily achieve biased regularization t-lsa replacing instead weight vector updated online wprev means zero experience biased regularization falls back usual regularization t-lsa falls back lsa. consider full setup cross-dataset active learning deﬁned section sequence datasets presented. experience learned learning using wprev reference point ﬁrst term wprev allows information earlier experience somewhat preserved second term allows experience accumulated. thus learned contains experience ﬁrst second datasets. natural learn wprev generally learn wprev simple wprev completes design full t-lsa algorithm listed algorithm simplicity overload denote algorithm help biased regularization t-lsa achieves cross-dataset active learning. experience helpful possibly happens transferring experience related datasets t-lsa utilizes experience speedup exploration wild. experience helpful mean negative transfer terminology transfer learning second term ztw−rt allows experience adaptively accumulated. section empirically study different kinds experience affect performance t-lsa. uncertain uncertainty sampling represent representative sampling based k-mean clustering uncertainty part essentially uncertain take scoring function representativeness blending. take logistic regression base classiﬁcation model -regularized logistic regression solver liblinear default parameters learn classiﬁer model. conduct experiments sets benchmark datasets. ﬁrst commonly used validate pool-based active learning approaches binary classiﬁcation taken validate competitiveness versus approaches also examine potential t-lsa crossdataset active learning heterogeneous datasets. ﬁrst benchmark include following eight datasets repository austra breast diabetes german heart lettermvsn liver wdbc dataset lettermvsn constructed multi-class dataset letter. second contains datasets handwritten digit recognition usps mnist used several previous studies multi-task learning take second examine potential t-lsa cross-dataset active learning homogeneous datasets. follow reduce feature dimensions usps mnist respectively principal component analysis. larger datasets lettermvsn usps mnist randomly keep examples make experiments sufﬁciently efﬁcient. then split dataset randomly training testing take training unlabeled pool test reporting active learning performance. randomly select instances unlabeled pool initial labeled pool experiments dataset averaged times. ﬁrst compare four underlying active learning algorithms state-of-the-art albl approach blending algorithms single datasets. then compare t-lsa albl cross-dataset setting understand effectiveness experience transfer. fairness also na¨ıvely extend albl t-albl illustrated section take talbl comparison. particular t-albl initializes internal probability distribution previously learned distribution achieve experience transfer. parameter tuning active learning known hard experiments approaches several parameter combinations report result best combination. practically existing blending approaches like albl comb combinations adaptively approximate best result. speciﬁcally experiments single datasets experiments cross-dataset active learning t-lsa respectively. parameters algorithms follow recommended parameters provided paper/codes authors. unlike albl maintains internal probabilistic distribution active learning algorithms comb maintains distribution unlabeled instances. non-trivial transfer distribution experience datasets different number instances. ﬁrst compare four underlying active learning algorithms ﬁrst eight benchmark datasets plot test accuracy different percentages queries fig. results observe usually close best curves four algorithms querying unlabeled instances. results demonstrate effective terms blending human knowledge towards decent query decisions. less-strong performance ﬁrst queries hints need using experience guide exploration instead starting zero experience. results fig. supported table ttests signiﬁcance level. tests compare underlying algorithms different ranks. table indicates often yields competitive performance best underlying algorithm always no-worse second best. results fig. table conﬁrm decent adaptive blending approach active learning like ancestors albl comb note deterministic approach albl comb probabilistic. understand effectiveness blending approach compare albl. space limits plot test accuracy along standard deviation four datasets austra breast heart wdbc fig. also compare albl t-tests conﬁdence level datasets summarize results table results fig. table indicate competitive sometimes even slightly better albl. furthermore according fig. observe variation curve decreases rapidly albl curve also generally smaller ﬁrst exploration queries. observation indicates albl probabilistic blending approach generally less stable matches conjecture section distribution albl volatile serve robust active learning experience practice. next move experiments cross-dataset active introduce experiment setting learning. ﬁrst proceed discuss details experiment results. experiment setting follows target dataset ﬁrst picked random sequence consists datasets generated. transferring algorithms including t-lsa t-albl ﬁrst datasets sequence accumulate experience. previous experience active learning performance transferring algorithms experiments active learning across datasets conducted different scenarios homogeneous heterogeneous tasks considered respectively. speciﬁcally homogeneous tasks consists datasets share similar learning targets feature space constructed benchmark datasets multi-task learning. heterogeneous tasks hand involves datasets different learning targets feature space simulated eight benchmark datasets active learning. ﬁrst discuss experiments homogeneous tasks algorithms exploit transferred experience expected perform better. experiments heterogeneous tasks general challenging scenario discussed. experiments scenario ﬁrst compare tlsa t-albl using experience different number previous datasets non-transferring predecessors namely albl evaluate effectiveness experience transfer active learning. then directly compare t-lsa t-albl albl using speciﬁc understand absolute performance difference t-lsa competitors. experiments homogeneous tasks experiments learning across homogeneous tasks conducted benchmark datasets hand-written digit recognition usps mnist multi-task learning. split usps mnist binary classiﬁcation datasets namely construct homogeneous learning tasks. since active learning performance usps mnist converges quickly compare results respect queries ﬁrst unlabeled data better illustrate difference. dataset presented here. fig. observe t-lsa generally outperforms tasks usps mnist. hand fig. indicates t-albl performs similarly even worse albl task usps mnist. task improvement t-albl regard albl rather minor mnist obvious negative transfer observed usps. compare t-lsa t-albl albl directly using experience previous dataset examine absolute performance difference t-lsa competitors. results illustrated fig. competitors compared tasks datasets based t-test conﬁdence level results summarized table iii. fig. observe performance inferior especially ﬁrst queries. t-lsa hand often performs best among four competitors. results table indicates slight improvement t-lsa initial stage learning shows competitive performance t-lsa competitors. observations usps mnist demonstrate t-lsa successfully improves active learning performance transferring active learning experience proposed linear weights expected scenario active learning across homogeneous tasks. t-albl however often performs inferior albl conﬁrming experience transfer probability distribution albl lead negative impact. experiments heterogeneous tasks next shall discuss experiments learning across heterogeneous tasks. experiments conducted eight benchmark datasets active learning. feature spaces learning targets vary others different active learning ﬁrst compare t-lsa t-albl albl present results fig. fig. respectively. owing space limits readability selected results austra breast diabetes wdbc presented. according fig. t-lsa improves performance datasets austra breast wdbc. dataset diabetes t-lsa inferior initial stage quickly catch even outperform lsa. hand observe fig. t-albl improves albl datasets breast wdbc inferior datasets austra diabetes. compare t-lsa directly t-albl albl transferring algorithms exploit experience previous datasets illustrate results fig. also compare algorithms based t-test conﬁdence level summarize results table fig. t-lsa reaches best performance among four competitors datasets austra breast wdbc catch best competitor querying unlabeled data dataset diabetes. results table conﬁrm t-lsa often outperform competitors. aforementioned observations demonstrate experience transfer proposed linear weights superior probabilistic distribution albl following advantages better improvement experience transfer ability recover quickly transferred experience negative performance. addition t-lsa shown improve providing better starting point exploration initial stage active learning. propose novel approach accomplishes mission transferring active learning experience across datasets. approach based uniﬁed representation human knowledge environment status active learning linear model representation. model allows taking linear weights experience updated linucb algorithm contextual bandit learning novel reward function. experience learned model transferred active learning tasks biased regularization. empirical studies conﬁrm competitiveness proposed approach also conﬁrm beneﬁcial transfer experience across active learning tasks either homogeneous heterogeneous better performance. mitchell cohen talukdar betteridge carlson mishra gardner kisiel krishnamurthy mazaitis mohamed nakashole platanios ritter samadi settles wang wijaya gupta chen saparov greaves welling never-ending learning proceedings twenty-ninth aaai conference artiﬁcial intelligence chen lifelong learning sentiment classiﬁcation proceedings annual meeting association computational linguistics international joint conference natural language processing asian federation natural language processing lewis gale sequential algorithm training text classiﬁers proceedings annual international acm-sigir conference research development information retrieval. tresp wang representative sampling text classiﬁcation using support vector machines proceedings european conference information retrieval research vol. beygelzimer langford reyzin schapire contextual bandit algorithms supervised learning guarantees proceedings international conference artiﬁcial intelligence statistics auer using conﬁdence bounds exploitation-exploration tradeoffs journal machine learning research vol. agrawal goyal thompson sampling contextual bandits linear payoffs proceedings international conference machine learning lichman machine learning repository kang grauman learning share multi-task feature learning proceedings international conference machine learning", "year": 2016}