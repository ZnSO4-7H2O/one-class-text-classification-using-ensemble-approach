{"title": "Efficient Training of Very Deep Neural Networks for Supervised Hashing", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In this paper, we propose training very deep neural networks (DNNs) for supervised learning of hash codes. Existing methods in this context train relatively \"shallow\" networks limited by the issues arising in back propagation (e.e. vanishing gradients) as well as computational efficiency. We propose a novel and efficient training algorithm inspired by alternating direction method of multipliers (ADMM) that overcomes some of these limitations. Our method decomposes the training process into independent layer-wise local updates through auxiliary variables. Empirically we observe that our training algorithm always converges and its computational complexity is linearly proportional to the number of edges in the networks. Empirically we manage to train DNNs with 64 hidden layers and 1024 nodes per layer for supervised hashing in about 3 hours using a single GPU. Our proposed very deep supervised hashing (VDSH) method significantly outperforms the state-of-the-art on several benchmark datasets.", "text": "nevertheless efﬁcacy deep learning applications hashing hinges ability efﬁciently train deep models back propagation currently widely-used training method deep learning simplicity. backprop known suffer called vanishing gradient issue gradients front layers n-layer network decrease exponentially directly impacts computational efﬁciency turn limits size networks trained. instance training vgg’s deep features ilsvrc convolutional layers takes approximately month using gpus. contributions propose deep supervised hashing algorithm training deep neural networks hashing. method take form vector input image intensities traditional features like gist even features given training data class labels network learns data representation tailored hashing outputs binary hash codes varying lengths. vdsh easily train large deep networks within hours single gpu. learning objective generate optimal hash codes linear classiﬁcation. minimize least square weighted encoding features label vectors regularization model parameters prevent overﬁtting. rather using backprop propose novel computationally efﬁcient training algorithm vdsh inspired alternating direction method multipliers represent features recursive introducing auxiliary variable model output hidden layer data sample. apply augmented lagrangian incorporate learning objective equality constraints another auxiliary variables introduced store network weights every pair adjacent layers locally efﬁcient update. paper propose training deep neural networks supervised learning hash codes. existing methods context train relatively shallow networks limited issues arising back propagation well computational efﬁciency. propose novel efﬁcient training algorithm inspired alternating direction method multipliers overcomes limitations. method decomposes training process independent layer-wise local updates auxiliary variables. empirically observe training algorithm always converges computational complexity linearly proportional number edges networks. empirically manage train dnns hidden layers nodes layer supervised hashing hours using single gpu. proposed deep supervised hashing method signiﬁcantly outperforms state-of-theart several benchmark datasets. supervised hashing techniques learn compact similarity-preserving binary representations labeled data similar inputs mapped nearby binary hash codes hamming space information retrieval efﬁciently effectively done large-scale databases. large category methods seek learn hyperplanes linear hash functions iterative quantization supervised minimal loss hashing semi-supervised hashing fasthash several kernel-based hashing methods like binary reconstructive embedding kernelbased supervised hashing also proposed. well recognized deep models able learn powerful image representations latent space samples different properties well separated. context convolutional neural networks based hashing schemes developed method based achieved state-of-the-art supervised hashing. carreira-perpin´an raziperchikolaei proposed learning binary autoencoders hashing well using mac. contrast admm-based method suitable efﬁcient solving regularized loss minimization shown block-splitting algorithm admm solves optimization problems equality constraints decomposing objective several disjoint sub-objectives using auxiliary variables original objective optimized iteratively using coordinate descent. small additional computational cost circumvent need relaxation penalty related parameters required context problem setup closely mirrors given collection samples {xi}n rd×n goal learn collection k-bit binary codes }k×n i-th column denotes binary code i-th sample learn codes consider parameterized family models parameterized arbitrary element hash code particular model described obtained taking sign namely supervised hashing also provided class labels samples goal context ensure binary codes samples corresponding class similar. adopt perspective binary codes learned context linear classiﬁcation good hashing codes namely preserve semantic similarity data samples. encode ground truth classes c-dim binary vectors entry belongs class hypothesis suggests collection linear classiﬁer functions predicted outcbi]t closely denotes matrix transpose operator. words seek hash codes linear classiﬁers approximation error measured terms loss function leads following optimization problem computational efﬁciency vdsh. training complexity linearly proportional number connections between nodes network. train dnns hidden layers nodes layer supervised learning hash codes within hours single titan achieving state-of-the-art results several benchmark datasets. related work supervised hashing deep models learning highlevel feature representations building deep hierarchical models shown great potential various applications. researchers adopting deep models jointly learn image representations hash codes data. kang proposed deep multi-view hashing algorithm learn hash codes multiple data representations. proposed learning image representations supervised hashing approximating data afﬁnity matrix features. zhao proposed deep semantic ranking hashing method preserve multilevel semantic similarity multilabel images. erin liong proposed deep hashing method explore nonlinear relationships among data. zhang proposed deep regularized similarity comparison hashing method allow length output bits scalable. works learn hash functions deep architecture. contrast vdsh built arbitrary vector representations. features used method viewed ﬁne-tuning networks supervised hashing. besides scale depth dnns much larger previous methods pose harder challenges training. dnns literature many different architectures alexnet googlenet vgg-vd weighting structures circulant structure low-rank approximation proposed. several techniques proposed improve generalization networks dropout dropconnet viewed better regularization. techniques speeding-up training proposed well distributed training batch normalization architectures methods however trained using backprop suffering issues vanishing gradients. ongoing efforts overcome issues backprop include variational bayesian autoencoder auto-encoding target propagation difference target propagation carreira-perpin´an wang recently proposed method training deeply nested systems. method auxiliary coordinates breaks dependency nested systems equality constraints quadratic penalty method utilized efﬁcient solver. shen proposed supervised discrete figure schematics vdsh training algorithm. blue color represents network structures green dashed rectangles represent two-layer substructures. fm+) denotes output m-th -th) hidden layers data sample data sample introduce types auxiliary variables represent outputs hidden layer data samples local copies network weights substructures. learning network weights decomposes independent local learning weights leading efﬁciency feasibility deep learning entry-wise maximum operator. note possible method incorporate complex functions deﬁne complex operations hidden nodes involved well e.g. maxout dropout dropconnet batch normalization network pruning discussion scope paper consider future work. backprop option training vdsh used learning hash codes suffers well-known vanishing gradient problem gradients front layers n-layer network decrease exponentially directly impacts computational efﬁciency turn limits size networks trained. overcome problem explicitly introduce auxiliary variables {zim} every every layer represent network circumvent long-term dependencies training issue still left dependency between loss function regularizer circumvent issue introduce auxiliary ∀i∀m motivated block variables splitting algorithm position update network weights locally independently across different layers leads improved computational much difﬁculty arises need deal sign function. number researchers proposed various techniques deal problem. include approximating sign function using sigmoids penalizing deviations relaxing binary constraint continuous i.e. adopt approach ﬁrst learn continuous embeddings threshold later binary codes. leads objective training vdsh follows suggests lead sub-optimal performance. experiments potential suboptimality offset training deep models resulting signiﬁcantly better performance relative simplicity choose squared loss functions penalties norm penalty etc. possible). speciﬁcally square loss function. denotes joint regularizer denote norm frobenius norm respectively regularization parameters. {θ}m denotes weights entire network rdm×dm− denotes weights m-th hidden layers rdm− denotes nonlinear function maps outputs lower layers outputs upper layers ﬁnal layer vdsh utilize relu activation function particular analyze behavior vdsh training algorithm alg. small hidden layers nodes layer mnist dataset. simplicity training parameters beforehand. subproblem alg. optimized subgradient descent. empirical convergence theoretically vdsh guaranteed converge local minima. nevertheless empirically admm works well even objectives nonconvex observed note lagrangian dual variables uim) {uim} {vim} denote lagrangian related parameters predeﬁned dual update steps. note lagrangian dual variables computed using βuim γvim ∀i∀m. propose novel algorithm listed alg. denotes total number training samples denotes identity matrix. denote ∀i∀mgim alternatively optimize variable time.in iteration using auxiliary variables classiﬁcation error ﬁrst propagated last hidden layer sequentially propagated rest hidden layers. next given updated local copies network weights updated independently. later leads updates entire network weights finally classiﬁer updated minimize total regularized loss ﬁxing rest parameters. repeat updating algorithm satisﬁes convergence condition. note since original pixel features layer- output features figure t-sne visualization different features mnist training samples different colors denote different classes. clearly network output features hidden layers better separated i.e. layer- output features best. figure actual training time comparison using training hidden layer dnns different number nodes layer training dnns various hidden layers nodes layer. vim)∀m) converge ∀i∀m holds respecf tively. motivates plot mean norm lagrangian dual variables demonstrate empirical convergence behavior vdsh. fig. depicts empirical convergence behavior hidden layer. intuition suggests small dual update steps lead slow convergence empirically slow change terms mean value. hand large steps lead zigzag behavior around local optimum. appropriate step size smooth convergence layers. interestingly four different dual steps eight layers tend show similar convergence rates. instance fig. curves tend relatively iteration implies larger changes front layers small changes ﬁnal layers network leading faster convergence. turn implies training algorithm vdsh potential overcome vanishing gradient issue backprop. similar behavior observed visualize output features different layers iterations using t-sne fig. number layers increases data evidently forms clearer clusters indicating vdsh encodes data effectively also converges layer. computational complexity computational comm= dmdm+n denotes input dimension denotes output dimension number training samples. follows fact computational complexity training vdsh proportional training individual two-layer substructure account admm-style decomposition. since information goes substructure back forth subgradient descent updates computational complexity substructure data sample corresponding layers grows depict speed training using un-optimized matlab implementation fig. training parameters default. used comparison i-mxghz quadro respectively. timing behavior using either plots supports computational complexity analysis above timing roughly quadratic number nodes timing roughly linear number hidden layers. also compare method backprop terms computational time. train shallow model hidden layers nodes layer training speed times faster backprop achieving similar performance. however train deeper model hidden layers nodes layer training algorithm converges within hour backprop converged within weeks. section compare vdsh state-ofthe-art supervised hashing methods including cca-itq fasthash dsrh dsch drsch image retrieval tasks. following evaluation protocols used previous supervised hashing methods dataset split large retrieval database small query set. entire retrieval database used train hashing models unless otherwise speciﬁed. lengths output hash codes vary bits. retrieval performance query evaluated using mean averfigure network evaluation using default features mnist cifar-. precision code-length curve w.r.t. varying number layers dimensions. code-length curve w.r.t. varying number layers dimensions. precision precision within hamming radius data samples normalized unit length. simplicity networks number nodes hidden layer. tune network architectures well training parameters using cross validation training data report performance query data using best networks. experiments xeon single titan un-optimized matlab implementation. datasets setup test vdsh mainly three benchmark datasets image retrieval tasks learned hash functions mnist cifar- nus-wide method learns mapping function image features hash codes equivalent learning image pixels implicitly composition functions. mnist contains gray-scale handwritten digit images pixels following randomly sample images class form image query rest images training retrieval database. default image represented -dim vector consisting pixel intensities. cifar- contains color images resolution pixels object classes images class. following randomly sample images class query rest images training retrieval set. default features image presented -dim gist feature vector. nus-wide contains images collected multi-label dataset image web. associated semantic concepts. image represented -dim bag-of-words feature vector provided dataset. following consider frequent concept labels randomly sample images label form query set. remaining images used training retrieval set. images considered true match share least common label. network evaluation varying depth hidden layers dimension nodes layer report areaunder-curve precision varying code lengths fig. mnist cifar-. note metrics plots datasets behave similarly best networks dataset different. general larger networks hidden layers nodes layer lead better hash codes better performance. performance appears saturate beyond certain network size turn demonstrates utility regularization preventing overly complicated models. addition also number nodes/layers increases obtain better retrieval performance. intuitively makes sense numbers control amount information passing layer other. performance comparisons compare vdsh supervised hashing methods detail mnist cifar- respectively. ﬁnal models train network hidden layers nodes layer mnist network hidden layers nodes layer cifar-. training time mnist minutes milliseconds sample testing including hash code generation retrieve k-sample database. cifar- takes around hour training milliseconds sample retrieve k-sample database. comparison default features shown fig. note unable full training huge memory requirements hence image subset randomly sampled methods. clearly vdsh signiﬁcantly outputs competitors large margins. also vdsh robust others maintaining stable performance across increasing code lengths. order compare vdsh fairly deep hashing methods learn features jointly hash codes utilize pre-trained vgg-f model extract features mnist cifar- directly without ﬁne-tuning. apply vdsh ccaitq fasthash features generate hash codes. compared fully optimized deep hashing methods drsch two-stage scheme optimized retrieval. pre-learned agnostic hash codes intended generated. report precision comparison fig. experimental settings features. note reported results bits curves incomplete here. surprisingly vdsh work signiﬁcantly better competitors. vdsh consistently best delivering robust performance across code lengths. fasthash tends good performance however precision within hamming radius drops drastically longer hash codes indicative inability form compact clusters hash code space. evidently robust behavior suggests hash codes generated vdsh testing sufﬁciently well clustered data samples class mapped nearby hash codes. verify conjecture comparing vdsh fasthash cifar- visualize hash codes bits test images using t-sne fig. directly report precision recall w.r.t. different code lengths hamming radius equal respectively fig. fig. different features vdsh forms cleaner clusters relative suggesting good retrieval performance. visual observation implies that vdsh testing query image typically falls near cluster belonging ground-truth class. leads hamming distance relatively small archival data within class methods. next plot performance decreasing hamming radius fig. vdsh appears robust suffer performance degradation decreasing radius. contrast performance fasthash varies signiﬁcantly achieve best result within hamming radius ﬁnding strengthens view vdsh capable learning compactly clustered hash codes across different code lengths sample hash code generation retrieve ksample database. performance comparisons depicted fig. cca-itq vdsh entire retrieval database used training. methods huge memory requirements limit randomly sample images training. vdsh consistently achieves best. performance vdsh signiﬁcant fig. hypothesize could fact multi-label dataset. since deﬁne images neighbors share common label percent image pairs dataset deﬁned neighbors compared around percent single label dataset scale. feature spaces different classes thus tend large overlap. vdsh network could confused training samples belong different classes thus unable generate effective hash codes. another possibility performance using provided bag-of-words features already saturated. addition compare method others ilsvrc randomly select images classes training dataset ilsvrc create k-image training train different hashing methods utilize entire k-image validation dataset ilsvrc query set. ﬁrst extract -dim features using vgg-f model. compare vdsh fasthash based hash codes within hamming radius results clearly paper propose deep supervised hashing algorithm learn hash codes training deep neural networks. vdsh utilizes outputs dnns generate hash codes rounding. computational efﬁciency formulate training vdsh norm regularized least square problem propose novel admm based training algorithm overcome issues vanishing gradients traditional backprop algorithm decomposing network-wide training multiple independent layer-wise local updates. discuss empirical convergence computational complexity training algorithm illustrate weights learned networks. conduct comprehensive experiments compare vdsh supervised hashing methods three benchmark datasets vdsh outperforms state-of-theart signiﬁcantly. thank anonymous reviewers useful comments. material based upon work supported part u.s. department homeland security science technology directorate ofﬁce university programs grant award -st--ed grant contract fa--c-. views conclusions contained document authors interpreted necessarily representing social policies either expressed implied u.s.", "year": 2015}