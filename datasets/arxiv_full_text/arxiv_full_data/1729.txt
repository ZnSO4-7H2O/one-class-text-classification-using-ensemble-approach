{"title": "Modeling Compositionality with Multiplicative Recurrent Neural Networks", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "We present the multiplicative recurrent neural network as a general model for compositional meaning in language, and evaluate it on the task of fine-grained sentiment analysis. We establish a connection to the previously investigated matrix-space models for compositionality, and show they are special cases of the multiplicative recurrent net. Our experiments show that these models perform comparably or better than Elman-type additive recurrent neural networks and outperform matrix-space models on a standard fine-grained sentiment analysis corpus. Furthermore, they yield comparable results to structural deep models on the recently published Stanford Sentiment Treebank without the need for generating parse trees.", "text": "present multiplicative recurrent neural network general model compositional meaning language evaluate task ﬁne-grained sentiment analysis. establish connection previously investigated matrixspace models compositionality show special cases multiplicative recurrent net. experiments show models perform comparably better elman-type additive recurrent neural networks outperform matrix-space models standard ﬁne-grained sentiment analysis corpus. furthermore yield comparable results structural deep models recently published stanford sentiment treebank without need generating parse trees. recent advancements neural networks deep learning provided fruitful applications natural language processing tasks. important advancement invention word embeddings represent single word dense low-dimensional vector meaning space numerous problems beneﬁted natural next question then properly larger phrases dense representations tasks require properly capturing meaning. existing methods take compositional approach deﬁning function composes multiple word vector representations phrase representation socher yessenalina cardie compositional matrix-space models example represent phrase-level meanings vector space represent words matrices vector space. therefore matrix assigned word capture transforms meaning space meaning representations longer phrases simply computed multiplication word matrices sequential order representational power however accompanied large number parameters matrix every word vocabulary. thus learning difﬁcult. sequential composition words phrases mechanism tackling semantic composition. recursive neural networks example employ structural approach compositionality composition function phrase operates children binary parse tree sentence. single words represented vector-space. different ways deﬁning composition function lead different variants recursive neural network. socher simple additive afﬁne function additional nonlinearity used. matrix-vector recursive neural network socher extends assigning additional matrix word similar aforementioned matrix-space models; composition function involves matrix-vector multiplication sibling representations. recently socher deﬁnes bilinear tensor multiplication composition function capture multiplicative interactions siblings. hand recurrent neural networks neural network architecture sequential prediction capabilities implicitly model compositionality applied natural language sentences. representation phrase conceptualized nonlinear function acts network’s hidden layer results repeated function composition hidden layer next word phrase/sentence unfortunately possible conventional additive recurrent networks powerful enough accommodate complex effects language suggested previous work recursive neural networks speciﬁcally even though additive models theoretically model arbitrary functions combined nonlinearity might require large number hidden units learnability large parameter sets data might pose issue. investigate multiplicative recurrent neural network model compositional semantic effects language. previously type multiplicative sequential approach applied character-level text generation task work investigate capacity recognizing sentiment sentence phrase represented sequence dense word vectors. like matrix-space models multiplicative rnns sequential models language; type recurrent implicitly model compositionality. like successful multiplicative recursive neural networks multiplicative rnns capture types sibling interactions much simpler. particular parse trees required sequential computations replace associated recursive computations performance depend accuracy parser. also show connection multiplicative compositional matrix-space models also applied sentiment analysis particular matrix-space models effectively special case multiplicative rnns word represented large one-hot vector instead dense small one. thus networks carry idea matrix-space models one-hot sparse representation dense word vectors. directly employ word vector representations makes better suited semi-supervised learning given plethora word vector training schemes. multiplicative recurrent networks considered unify views distributed language processing operator semantics view matrix-space models word interpreted operator acting meaning representation sequential memory processing view recurrent neural networks. experiments show multiplicative rnns provide comparable better performance conventional additive recurrent nets matrix-space models terms ﬁne-grained sentiment detection accuracy. furthermore although absence parse tree information puts additional learning burden multiplicative rnns reach comparable performance recursive neural network variants require parse tree annotations sentence. vector space models. natural language processing common representing single token vector one-hot vector token dimensionality vocabulary size. results high dimensional sparse representation. additionally every word equal distance another disregarding syntactic semantic similarities. alternatively distributed representation maps token real-valued dense vector smaller size generally representations learned unsupervised manner large corpus e.g. wikipedia. various architectures explored learn embeddings might different generalization capabilities depending task geometry induced word vector space might interesting semantic properties work employ word vector representations initial input representation training neural networks. matrix space models. alternative approach embed words matrix space assigning matrices words. intuitively matrix embedding word desired order capture operator semantics embedding model word transforms meaning applied context. baroni zamparelli partially apply idea model adjectives matrices noun vectors. theoretical work rudolph giesbrecht deﬁne proper matrix space model assigning every word matrix; representations longer phrases computed matrix multiplication. show matrix space models generalize vector space models argue neurologically psychologically plausible. yessenalina cardie apply model ﬁne-grained sentiment detection. socher structural approach every word assigned matrix-vector pair vector captures meaning word isolation matrix captures transforms meaning applied vector. compositionality vector matrix spaces. commutative vector operations addition element-wise multiplication along negation provide simple composition schemes even though ignore order words might prove effective depending length phrases task models compositional distributional semantics emulate formal semantics representing functions tensors arguments vectors generalise tensor-learning approach complex non-commutative composition functions modeled sequential structural models sentence. particular compositionality recurrent neural networks considered tranformations memory applied successive word vectors order. recursive neural networks employ structural setting compositions smaller phrases larger ones determined parent-children relationship associated binary parse tree matrix space models compositionality naturally modeled function composition sequence sentiment analysis. sentiment analysis active area among researchers various granularities word- phrase- sentencedocument-level besides preexisting work tried formulate problem binary classiﬁcation recently ﬁnegrained approaches explored ultimately vast majority approaches tackle task compositionally addition bag-of-words features incorporate engineered features account negators intensiﬁers contextual valence shifters matrix-space model models single word square matrix transforms meaning vector another vector meaning space. intuitively word viewed function operator acts meaning representation. therefore phrase represented successive application individual operators inside phrase. sequence words length rm×m denote matrix representation word vocabulary. then representation simply yields another linear transformation space. observe representation respects word order note even though modeled linear operator meaning space function {mwi}i=..t linear since constitutes multiplications terms. applying representation task simply applying function initial empty meaning vector results transformed ﬁnal meaning vector used make decision phrase case sentiment detection sentiment score assigned follows figure vector tensor sliced along dimension left. dense word vector computes weighted base matrices square matrix used transform meaning vector. right. one-hot word vector computation equivalent selecting base matrices falls back matrix-space model. recurrent neural network class neural network recurrent connections allow form memory. makes applicable sequential prediction tasks arbitrary spatio-temporal dimension. model conditional distribution output variables given input sequence. work focus attention elman-type networks elman-type network hidden layer time step computed nonlinear transformation current input layer previous hidden layer ht−. then ﬁnal output computed using hidden layer interpret intermediate representation summarizing past far. formally given sequence vectors {xt}t=..t elman-type operates computing following memory output sequences nonlinearity element-wise sigmoid function output nonlinearity softmax function weight matrices input hidden layer among hidden units respectively output weight matrix bias vectors connected hidden output units respectively. scalar sigmoid function simply probability positive label conditioned {xτ}τ =..t. tasks requiring single label sequence discard intermediate outputs {yt}t=.. output last time step length sequence. also means training external error incurred ﬁnal time step. general supervision applied intermediate time step whenever labels available dataset even intermediate time step labels used testing phase since makes training easier. property recurrent neural networks input layer activations hidden layer activations previous time step interact additively make activations hidden layers current time step. might rather restrictive applications difﬁcult learn modeling complex input interactions. hand multiplicative interaction layers might provide better representation semantic analysis tasks. sentiment detection example might considered negation sentiment comes might effectively modeled multiplicative interactions. investigate multiplicative recurrent neural network sentiment analysis task main focus paper tensor bilinear operation deﬁnes another vector ay)i right-hand side represents standard vector matrix multiplications single slice tensor means single entry linear combination entries ht−k also includes multiplicative terms form jkxtjht−k. simplify equation adding bias units since ﬁne-grained sentiment labels denote intensity addition polarity class labels ordinal nature. therefore ordinal regression scheme neural networks described cheng intuitively sentiment class denotes threshold instances belonging class sentiment values less equal instance belongs class automatically belongs lower order classes well. therefore target vector instance otherwise. consider output vector cumulative probability distribution classes. class labels deﬁned output response subject normalization. therefore output layer nonlinearity case elementwise sigmoid function instead softmax function mapped ﬁnally assign corresponding integer label. figure hidden layer vectors reduced dimensions various phrases. left. recurrent neural network. right. purely multiplicative recurrent neural tensor network. mrnn handling negation nonlinear correctly shifts sentiment. matrix space model individual associated matrices corresponding words therefore view mrnns simpliﬁcation matrixspace models tensor extract matrix word associated word vector rather associating matrix every word. viewed learning matrix-space model parameter sharing. reduces number parameters greatly instead matrix every word vocabulary vector word tensor extract matrices. another interpretation following instead learning individual linear operator word matrix-space models mrnn learns number base linear operators. mrnn then represents word weighted base operators note one-hot vector representation word instead dense word embedding matrices base operators simply selects matrices essentially falling back exact matrix-space model therefore mrnns provide natural transition matrix-space model one-hot sparse word representation dimensional dense word embedding. besides reduction number parameters another potential advantage mrnns matrix-space models matrix-space model task-dependent task learn matrix word whole vocabulary. hand mrnns make task-independent word vectors parameters network would task-dependent. allows easier extension multitask learning transfer learning settings. intensity. scheme yessenalina cardie preprocess extract individual phrases annotated documents convert annotations integer ordinal label denoting sentiment score negative positive. preprocessing phrases total average length training-validation-test partitions provided authors apply -fold report average performance folds. additionally recently published stanford sentiment treebank includes labels phrases parse trees sentences average sentence length similarly real-valued sentiment labels converted integer ordinal label simple thresholding. single training-validation-test partition provided authors. make parse trees treebank since approach structural; however include phrase-level supervised labels labels partial sentences. problem formulation. experiments mpqa corpus employ ordinal regression setting. experiments employ simple multiclass classiﬁcation setting make models directly comparable previous work. classiﬁcation setting output nonlinearity softmax function output vector valued response class probabilities. ordinal regression setting described section evaluation metrics. experiments using mpqa corpus ranking loss yesse|yi predicted true scores nalina cardie deﬁned socher respectively. experiments using accuracy word vectors. experiment randomly initialized word vectors pretrained word vector representations pretrained word vectors publicly available dimensional word vectors mikolov trained part google news dataset using pretrained word vectors ﬁnetune reduce degree freedom models. observe mrnn slightly better approximately number parameters suggests multiplicative interactions improve model additive interactions. even though difference signiﬁcant test signiﬁcant development set. partially attribute effect test variance. also suggests multiplicative models indeed powerful require careful regularization early stopping high model variance might tend overﬁt development set. randomly initialized mrnn outperforms equivalent randomly initialized matrix-space model suggests compact representations shared parameters learned mrnn indeed generalize better. mrnn pretrained word vectors best results suggests importance good pretraining schemes especially supervised data limited. also conﬁrmed preliminary experiments using word vector training methods embeddings hlbl yielded signiﬁcant difference ranking loss. test effect different nonlinearities experiment identity rectiﬁer tanh functions mrnns. experiments show small consistent improvement rectiﬁer tanh using extra nonlinearity. differences rectiﬁer identity tanh rectiﬁer signiﬁcant; however difference tanh identity signiﬁcant suggesting performance boost using nonlinear squashing function. nonetheless using nonlinearity marginally worse. possible explanation since squashing function source nonlinearity mrnns crucial. mrnn outperform conventional naive bayes baselines. observe close performance recursive neural network considered structural counterpart. mrnn improves performs better recursive worse matrix-vector recursive net. note none rnn-based methods employ parse trees sentences unlike recursive neural network variants. work explore multiplicative recurrent neural networks model compositional interpretation language. evaluate task ﬁne-grained sentiment analysis ordinal regression setting show mrnns outperform previous work mpqa comparable results previous work stanford sentiment treebank without using parse trees. also describe mrnns effectively generalize matrix-space models sparse -hot word vector representation distributed dense representation. beneﬁt mrnns matrix-space models separation task-independent word representations task-dependent classiﬁers making easy extend semi-supervised learning transfer learning settings. slices tensor interpreted base matrices simpliﬁed matrix-space model. intuitively every meaning factor word separate operator acting meaning representation combine operator word itself. parameter sharing perspective mrnns provide better models. matrix-space models update sentence affects word matrices occur particular sentence. hand mrnn update sentence affects global tensor well. update network alters operation similar words towards similar direction. drawback mrnns conventional additive rnns increased model variance resulting multiplicative interactions. tackled stricter regularization. another future direction explore sparsity constraints word vectors would mean every word would select base operators meaning representation. work supported part grant iis- darpa deft grant fa--. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied darpa u.s. government. bengio yoshua ducharme rjean vincent pascal jauvin christian hofmann thomas poggio tomaso shawe-taylor john. neural probabilistic language model. advances neural information processing systems cheng jianlin wang zheng pollastri gianluca. neural network approach ordinal regression. neural networks ijcnn ieee international joint conference ieee collobert ronan weston jason. uniﬁed architecture natural language processing deep neural networks multitask learning. proceedings international conference machine learning collobert ronan weston jason bottou l´eon karlen michael kavukcuoglu koray kuksa pavel. natural language processing scratch. mach. learn. res. november issn http//dl.acm.org/citation.cfm?id=.. grefenstette dinu zhang sadrzadeh baroni multi-step regression learning compositional distributional semantics. proceedings international conference computational semantics long papers potsdam germany march association computational linguistics. http//www.aclweb.org/anthology/w-. mikolov tomas sutskever ilya chen corrado greg dean jeff. distributed representations words phrases compositionality. advances neural information processing systems rudolph sebastian giesbrecht eugenie. compositional matrix-space models language. proceedings annual meeting association computational linguistics association computational linguistics shaikh mostafa masum prendinger helmut mitsuru ishizuka. assessing sentiment text semantic dependency contextual valence analysis. affective computing intelligent interaction springer socher richard cliff andrew manning chris. parsing natural scenes natural language recursive neural networks. proceedings international conference machine learning socher richard huval brody manning christopher andrew semantic compositionality recursive matrix-vector spaces. proceedings joint conference empirical methods natural language processing computational natural language learning association computational linguistics socher richard perelygin alex jean chuang jason manning christopher andrew potts christopher. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing emnlp sutskever ilya martens james hinton geoffrey generating text recurrent neural networks. proceedings international conference machine learning turian joseph ratinov bengio yoshua. word representations simple general method semi-supervised learning. proceedings annual meeting association computational linguistics association computational linguistics widdows dominic. orthogonal negation vector spaces modelling word-meanings document retrieval. proceedings annual meeting association computational linguistics sapporo japan july association computational linguistics. ./.. http//www.aclweb.org/anthology/p-. wilson theresa wiebe janyce hoffmann paul. recognizing contextual polarity phrase-level sentiment analysis. proceedings conference human language technology empirical methods natural language processing association computational linguistics yessenalina ainur cardie claire. compositional matrix-space models sentiment analysis. proceedings conference empirical methods natural language processing association computational linguistics zanzotto fabio massimo korkontzelos ioannis fallucchi francesca manandhar suresh. estimating proceedings international conlinear models compositional distributional semantics. ference computational linguistics beijing china august coling organizing committee. http//www.aclweb.org/anthology/c-.", "year": 2014}