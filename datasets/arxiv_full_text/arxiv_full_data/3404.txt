{"title": "Active Deep Learning for Classification of Hyperspectral Images", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Active deep learning classification of hyperspectral images is considered in this paper. Deep learning has achieved success in many applications, but good-quality labeled samples are needed to construct a deep learning network. It is expensive getting good labeled samples in hyperspectral images for remote sensing applications. An active learning algorithm based on a weighted incremental dictionary learning is proposed for such applications. The proposed algorithm selects training samples that maximize two selection criteria, namely representative and uncertainty. This algorithm trains a deep network efficiently by actively selecting training samples at each iteration. The proposed algorithm is applied for the classification of hyperspectral images, and compared with other classification algorithms employing active learning. It is shown that the proposed algorithm is efficient and effective in classifying hyperspectral images.", "text": "abstract—active deep learning classiﬁcation hyperspectral images considered paper. deep learning achieved success many applications good-quality labeled samples needed construct deep learning network. expensive getting good labeled samples hyperspectral images remote sensing applications. active learning algorithm based weighted incremental dictionary learning proposed applications. proposed algorithm selects training samples maximize selection criteria namely representative uncertainty. algorithm trains deep network efﬁciently actively selecting training samples iteration. proposed algorithm applied classiﬁcation hyperspectral images compared classiﬁcation algorithms employing active learning. shown proposed algorithm efﬁcient effective classifying hyperspectral images. learning introduced remote sensing data classiﬁcation considered extension artiﬁcial neural network effective classiﬁcation complex problems. however training deep network quite expensive requires large number training samples. application deep network hyperspectral image classiﬁcation practical limited number training samples available dimension feature space large. active learning iterative procedure selecting informative examples subset unlabeled samples. choice based ranking scores computed model outcome. chosen candidates added training classiﬁer trained training samples. training done actively selected samples efﬁcient done randomly selected samples uses samples suitable training. therefore active learning method train deep network faster manuscript received february revised april july july accepted august work supported part nsfc grant grant part radi director youth foundation. least three different kinds approaches selecting training samples active learning. ﬁrst based uncertainty unlabeled samples uncertain sampling query-by-committee second based inﬂuence model unlabeled samples length gradients fisher information ratio third based intrinsic distribution structure unlabeled samples manifold learning kullback–leibler divergence similarity gaussian similarity cluster also mixed methods employ criteria selecting training samples active learning. example density-weighting method employs uncertainty distribution unlabeled samples. methods using multiple metrics potential achieve higher efﬁciency active learning. uncertainty distribution utilized selecting training samples active learning algorithm proposed paper. active learning methods widely studied remote sensing applications. research active learning combined special classiﬁer special remote sensing application. examples include kernel-based method active learning method combined support vector machine logistic regression gaussian process regression survey active learning remote sensing found although active learning applied many applications remote sensing approaches closely connected speciﬁc type speciﬁc structure classiﬁer. examples random sampling maximum uncertainty sampling query-by-committee essentially deep belief network without tuning done active learning. applicable classiﬁers unsupervised feature learning supervised tuning employed training dbn. samples candidate data randomly classiﬁcation accuracy usually low. however fast simple convenient. queries uncertain instance active learner entropy used uncertainty measure. applied trains committee members current labeled set. different members committee represent different hypotheses classiﬁcation problem. selects candidate samples showing maximal disagreement different members committee. paper propose active learning scheme information unsupervised supervised stages utilized. proposed active learning applied deep learning structure efﬁcacy classifying remotely sensed ﬁrst term denotes expectation respect data distribution second tern denotes expectation respect distribution deﬁned model. direct connections hidden units easy unbiased sample ﬁrst term. however getting unbiased sample second term much difﬁcult. done starting random state visible unit performing alternating gibbs sampling long time. much faster learning procedure called constrastive divergence method proposed performs gibbs sampling uses gradient descent procedure update increments parameters. hyperspectral images demonstrated experiments. paper organized follows section basic idea discussed shown unsupervised supervised stages could provide useful information. section criteria active learning discussed criteria uncertainty representativeness proposed criteria active learning. section object function constructed combining representativeness uncertainty samples optimization algorithm solving object function presented. experimental results presented section integers indexing pixels hyperspectral image composed feature vectors corresponding m-dimensional pixel. label pixels c-dimensional label vector represents labeling process considered mapping process image label solved dbn. architecture used research shown fig. fully interconnected belief network input layer hidden layers output layer input layer units corresponding feature vector output layer units corresponding label vector architecture transforms high-dimensional data low-dimensional code using adaptive multilayered encoder network. popular method constructing deep architecture greedy layer-wise restricted boltzmann machine particular form log-linear markov random ﬁeld. consider layer takes output previous layer input generates output next layer notational convenience visible hidden variables denoted respectively. research. example research density-weighting method information density framework described settles craven simultaneously considers underlying structure information data explicit class labels samples. also proposed informative instances uncertain also representative underlying distribution therefore informative sample selected maximizing information density rbms stacked trained greedy manner form illustrated fig. dbns viewed graphical models learn extract deep hierarchical representation training data. complete training includes stages unsupervised features learning stage supervised tuning stage. unsupervised features learning stage rbms learn layer time method. efﬁcient greedy learning scheme. supervised tuning stage initial stacked discriminative tuning performed back propagation algorithm. many applications deep learning shows better performance classical methods svm. however deep learning needs initialize parameters svm. complete training randomly selected training samples requires large number training samples despite efﬁciency algorithm. therefore practical apply problems limited size training samples large feature spaces. select training dataset becomes important training deep network. improve training efﬁciency algorithm proposed paper considers stages training unsupervised feature learning supervised tuning. unsupervised learning stage provides condition estimation representativeness data supervised learning stage provides condition uncertainty estimation data. proposed active learning method metrics representativeness uncertainty integrated object function. weighted incremental dictionary learning algorithm illustrated fig. proposed optimize object function metrics. optimization samples ranked informative samples selected. experiments hyperspectral images conﬁrmed effectiveness proposed algorithm. candidate sample arbitrary sample unlabeled dataset function measures similarity sample function measures uncertainty sample. success information density approach depends ﬁnding good uncertainty function good similarity function information density framework advantages classiﬁcation utilizes information traditional active learning methods. inspired information density method similarity function uncertainty function deep learning architectures proposed paper. also important combine criteria together searching informative samples training. algorithm called wi-dl developed next section. algorithm ranks samples importance criteria wi-dl algorithm selects informative samples maximize aforementioned criteria training data. cosine similarity function used similarity function many similarity measures divergence similarity gaussian similarity local manifold similarity used active learning. active learning good similarity measure important ﬁnding representative samples. words good representative model needed efﬁcient representation distribution structure. recently sparse representation method represents signal basis become popular machine learning ﬁeld. basis also called dictionary. unlike decompositions using predeﬁned analytic basis variants signal represented using overcomplete dictionary without analytic form. basic assumption behind dictionary learning approach structure complex incoherent characteristics extracted directly data rather using mathematical description. atoms dictionary characterize entire dataset samples searched active learning representative. furthermore subset samples used dictionary high efﬁciency representative samples active learning problem. belongs class. uncertainty function directly related structures classiﬁer easy implement. next section uncertainty measure combined sparse representation develop active learning algorithm. last section deﬁned representativeness measurement based sparse representation uncertainty measurement based entropy. section construct discriminate function used search informative samples employ representativeness uncertainty samples. deep network. represent features input data concisely efﬁciently nonlinear dimension reduction. therefore samples labeled searching process done one-to-one relationship appropriate feature vectors corresponding samples labeled training dataset. initial training data projected deep network trained unsupervised learning stage explained section projected data used initial dictionary active learning dictionary dictionary next iteration obtained. selection atoms done similarly batch operation shown optimization done incrementally. object function incremental learning given norm norm dictionary previous iteration atoms appended form dictionary. ﬁrst term residual error adding training samples. coefﬁcient vectors data associated dictionaries respectively. since unsupervised coding stage deep learning could viewed feature learning dimension reduction reasonable select training samples based information feature data projected dbn. therefore atoms selected last hidden layer dictionary assumed redundant number nonzero coefﬁcients representation denoted expected small. implies feature vector viewed linear combination columns dictionary also traditional methods solving k-singular value decomposition nonparametric bayesian dictionary learning etc. directly applicable active learning deep network uncertainty also needs considered addition representativeness. uncertainty sampling commonly used query framework active learning. framework active learner queries instances least certain. entropy often used uncertainty measure. contains uncertain samples good enough used dictionary updated one. using idea information density representativeness uncertainty considered. convenience notation symbol used without index. representativeness measured square however importance term different because difference uncertainty. uncertainty sample increases contributes sum. result uncertainty measure introduced object function entropy atom deﬁned uncertain atom larger means that feature vector mainly composed uncertain atoms increase punishments object function. furthermore large coefﬁcients exacerbate inﬂuence uncertainties atoms. cost function jointly convex convex respect sets ﬁxed. extensive research focuses good dictionary atoms represent dataset sparsely. usually stages dictionary leaning algorithms sparse coding stage searching optimal solution dictionary updating stage solution sparse coding stage variety atom selection schemes many greedy-based algorithms orthogonal matching pursuit compressive sampling matched pursuit stageomp however none traditional sparse coding methods consider uncertainty atoms classiﬁcation problems based active learning considered earlier research. atoms dictionary selected unlabeled dataset elements ˆdb} eigenvectors matrix eigenvector corresponding largest eigenvalue. sample selected projection weighted uncertainty largest. selection process written fig. classiﬁcation results different active learning methods. proposed wi-dl result shows better results compared ground truth fig. accuracies wi-dl respectively. wi-dl qbc. small speedy unsupervised learning brings initial conﬁguration. complete training requires large training computationally expensive. additional dictionary atoms selected iteration active learning trains classiﬁer much efﬁciently randomly selected training samples additional dictionary atoms best samples training. experiment atoms added dictionary iteration. iteration active learning classiﬁer trained training updated additional atoms. process repeated classiﬁer completely trained details summarized experimental results. validate proposed method three hyperspectral datasets paviac paviau botswana used experiment. proposed algorithm wi-dl compared test datasets three algorithms namely dbns used paper four hidden layers. computational efﬁciency considered selection number layers. initial weights dbns randomly selected between layer dbns based rbm. rbms stacked trained greedy manner form architecture illustrated fig. unsupervised features learning stage rbms learn layer time method supervised ﬁne-tuning stage algorithm applied. compared traditional dictionary learning algorithm proposed wi-dl distinctive characteristics. first different object function traditional introduced weight parameters furthermore uncertainty metric considered samples sorting selecting procedures. second traditional dictionary learning applied signal vector data wi-dl applied current residual data active learning incremental learning problem. overall wi-dl considered representativeness uncertainty selecting samples atoms. ﬁrst experiment done pavia center dataset. acquired reﬂective optics system imaging spectrometer sensor widely used earlier research. number bands original dataset spatial resolution covers spectral range hyperspectral bands. original paviac dataset bands selected removing signalto-noise ratio bands. test images segmented dataset without bands size test image fig. shows ground truth detailed view lower left corner. shows classes different colors names class labels shown right side ﬁgure. nine classes interest selected labeled dataset. four algorithms applied three sets data constructed ground-truth data. contains three classes randomly selected data training candidates testing data different percentages. table shows number samples class dataset. class name class number given ﬁrst second columns third column shows total number samples rest columns show numbers samples training candidates test sets. four hidden layers input nodes corresponding hyperspectral bands nine output nodes corresponding nine classes interest created. training data used conﬁgure parameters classiﬁer preparation active learning. then active learning algorithm presented section applied ﬁne-tune classiﬁer actively selecting atoms candidate set. training data used initial conﬁguration used initial dictionary samples actively selected candidates iteration active learning. newly selected dictionary samples labeled added existing dictionary classiﬁer tuned updated dictionary. total iterations performed wi-dl result fig. shows samples correctly classiﬁed compared results algorithms. observed wi-dl algorithm performs better approaches especially enlarged area lower left corner. demonstrate effectiveness active learning classiﬁcation accuracy measured varying amount training samples. result experiments shown fig. observed performance wi-dl better algorithms classiﬁcation accuracy wi-dl improves faster algorithms samples added. performance three algorithms ranked order experiments. experiments performed windows computer -bit intel core running algorithms implemented matlab elapsed times classiﬁcation three datasets four different algorithms measured summarized table observed fastest random selection requires computation. slower close needs compute entropy candidate sample. proposed wi-dl relatively slow. complexity wi-dl mainly sparse coding complexity mainly training different committee members. wi-dl usually faster greedy-based algorithm sparse coding used. test dataset used test performance ﬁnetuned classiﬁer. fig. shows classiﬁcation result obtained proposed wi-dl algorithm boxed area center enlarged lower left corner show classiﬁcation details. comparison image fig. seen classiﬁcation result matches reasonably well ground truth. three active-learning classiﬁcation algorithms namely methods applied test dataset compare classiﬁcation performances. classiﬁcation results shown fig. water hippo grass floodplain grasses floodplain grasses reeds riparian firescar island interior acacia woodlands acacia shrublands acacia grasslands total data used experiment airborne data rosis optical sensor collected hysens project sponsored european union. images acquired area university pavia northern italy july number bands rosis sensor spectral coverage ranging hyperspectral channels used classiﬁcation removal noisy bands. spatial resolution pixel. data atmospherically corrected original image shown false color fig. fig. shows ground truth detailed view lower left corner. shows classes different colors names class labels shown right side. nine classes interest selected labeled dataset. four algorithms applied three sets data constructed ground truth data. contains three classes randomly selected data training candidates testing data different percentages. table shows number samples class dataset. fig. shows results classiﬁcation done wi-dl methods. details boxed area middle enlarged lower left corner fig. result proposed wi-dl algorithm shown fig. matches reasonably well ground truth shown fig. also classiﬁcation result wi-dl algorithm fig. better results obtained algorithms fig. seen enlarged area. fig. shows changes classiﬁcation accuracies number training samples increases. observed performance wi-dl better algorithms classiﬁcation accuracy wi-dl improves faster algorithms samples added. times classiﬁcation three datasets four different algorithms measured summarized table hardware software environments experiments dataset used experiment acquired nasa satellite okavango delta botswana hyperion sensor acquires data pixel resolution .-km strip bands covering spectrum ranging preprocessing performed university texas—center space research. uncalibrated noisy bands cover water absorption features removed bands remained dataset. data analyzed study acquired consist observations identiﬁed classes. fig. shows test image false color fig. shows ground truth detailed view lower left corner. shows classes different colors names class labels shown right side. classes interest selected labeled dataset summarized table four algorithms applied three sets data constructed ground-truth data. contains three classes randomly selected data training candidates testing data different percentages. table shows number samples class dataset. class name class number given ﬁrst second columns third column shows total number samples rest columns show numbers samples training candidates test sets. fig. shows results classiﬁcation done wi-dl methods. details boxed area middle zoomed lower left corner fig. result proposed wi-dl algorithm shown fig. matches reasonably well ground truth shown fig. also classiﬁcation result wi-dl algorithm fig. better results obtained algorithms fig. seen enlarged area. fig. shows changes classiﬁcation accuracies number training samples increases. observed performance wi-dl better algorithms classiﬁcation accuracy wi-dl improves faster algorithms samples added. performance three algorithms ranked order experiments. elapsed times classiﬁcation three datasets four different algorithms measured summarized table hardware software environments experiments paper proposed classiﬁcation algorithm based active learning deep networks. active learning additional samples training selected using representativeness uncertainty potential samples. achieved integrating criteria object function. active learning algorithm wi-dl algorithm suitable searching atoms developed minimizing object function criteria. performance wi-dl algorithm compared three methods qbc. proposed wi-dl performed well classiﬁcation experiment remotely sensed hyperspectral images. demonstrated proposed algorithm achieves higher accuracy fewer training samples actively selecting training samples. donoho tsaig drori jean-luc starck sparse solution underdetermined systems linear equations stagewise orthogonal matching pursuit ieee trans. inf. theory vol. feb. chen zhao spectral-spatial classiﬁcation hyperspectral data based deep belief network ieee sel. topics appl. earth observ. remote sens. vol. jun. tang batch-mode active learning algorithm using region-partitioning diversity classiﬁer ieee sel. topics appl. earth observ. remote sens. vol. apr. bioucas-dias plaza semisupervised hyperspectral image segmentation using multinomial logistic regression active learning ieee trans. geosci. remote sens. vol. nov. tuia volpi copa kanevski survey active learning algorithms supervised remote sensing image classiﬁcation ieee sel. topics appl. earth observ. remote sens. vol. jun. currently associate professor institute remote sensing digital earth chinese academy sciences. department electrical computer engineering george washington university washington visiting scholar. research focused data sparse representation compressive sensing deep learning applidr. also reviewer applied remote sensing ieee journal selected topics applied earth observations remote sensing neurocomputing signal processing etc. zhang received b.s. degree major computer science henan normal university m.s. degree pattern recognition intelligent system ph.d. degree computer science chinese academic science beijing china respectively. currently visiting fellow laboratory brain cognition national institute mental health bethesda usa. current research interests include using machine learning method data analysis. assistant professor syracuse university syracuse joining george washington university washington currently professor electrical computer engineering. also visiting professor center automation research university maryland research modeling automatic target recognition speech signal processing. research sponsored national science foundation ofﬁce naval research national security agency nasa niagara mohawk power corporation. consulted enerlog corporation niagara mohawk power corporation u.s. patent ofﬁce.", "year": 2016}