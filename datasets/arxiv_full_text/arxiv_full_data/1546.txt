{"title": "Google's Neural Machine Translation System: Bridging the Gap between  Human and Machine Translation", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.", "text": "wolfgang macherey maxim krikun yuan klaus macherey klingner apurva shah melvin johnson xiaobing łukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens neural machine translation end-to-end learning approach automated translation potential overcome many weaknesses conventional phrase-based translation systems. unfortunately systems known computationally expensive training translation inference sometimes prohibitively case large data sets large models. several authors also charged systems lack robustness particularly input sentences contain rare words. issues hindered nmt’s practical deployments services accuracy speed essential. work present gnmt google’s neural machine translation system attempts address many issues. model consists deep lstm network encoder decoder layers using residual connections well attention connections decoder network encoder. improve parallelism therefore decrease training time attention mechanism connects bottom layer decoder layer encoder. accelerate ﬁnal translation speed employ low-precision arithmetic inference computations. improve handling rare words divide words limited common sub-word units input output. method provides good balance ﬂexibility character-delimited models eﬃciency word-delimited models naturally handles translation rare words ultimately improves overall accuracy system. beam search technique employs length-normalization procedure uses coverage penalty encourages generation output sentence likely cover words source sentence. directly optimize translation bleu scores consider reﬁning models using reinforcement learning found improvement bleu scores reﬂect human evaluation. wmt’ english-to-french english-to-german benchmarks gnmt achieves competitive results state-of-the-art. using human side-by-side evaluation isolated simple sentences reduces translation errors average compared google’s phrase-based production system. neural machine translation recently introduced promising approach potential addressing many shortcomings traditional machine translation systems. strength lies ability learn directly end-to-end fashion mapping input text associated output text. architecture typically consists recurrent neural networks consume input text sequence generate translated output text. often accompanied attention mechanism helps cope eﬀectively long input sequences. advantage neural machine translation sidesteps many brittle design choices traditional phrase-based machine translation practice however systems used worse accuracy phrase-based translation systems especially training large-scale datasets used best publicly available translation systems. three inherent weaknesses neural machine translation responsible slower training inference speed ineﬀectiveness dealing rare words sometimes failure translate words source sentence. firstly generally takes considerable amount time computational resources train system large-scale translation dataset thus slowing rate experimental turnaround time innovation. inference generally much slower phrase-based systems large number parameters used. secondly lacks robustness translating rare words. though addressed principle training copy model mimic traditional alignment model using attention mechanism copy rare words approaches unreliable scale since quality alignments varies across languages latent alignments produced attention mechanism unstable network deep. also simple copying always best strategy cope rare words example transliteration appropriate. finally systems sometimes produce output sentences translate parts input sentence words fail completely cover input result surprising translations. work presents design implementation gnmt production system google aims provide solutions problems. implementation recurrent networks long short-term memory rnns lstm rnns layers residual connections layers encourage gradient parallelism connect attention bottom layer decoder network layer encoder network. improve inference time employ low-precision arithmetic inference accelerated special hardware eﬀectively deal rare words sub-word units inputs outputs system. using wordpieces gives good balance ﬂexibility single characters eﬃciency full words decoding also sidesteps need special treatment unknown words. beam search technique includes length normalization procedure deal eﬃciently problem comparing hypotheses diﬀerent lengths decoding coverage penalty encourage model translate provided input. implementation robust performs well range datasets across many pairs languages without need language-speciﬁc adjustments. using implementation able achieve results comparable better previous state-of-the-art systems standard benchmarks delivering great improvements google’s phrase-based production translation system. speciﬁcally wmt’ english-to-french single model scores bleu improvement bleu single model without external alignment model reported improvement bleu single model without external alignment model reported single model also comparable single model making alignment model used likewise wmt’ english-to-german single model scores bleu bleu better previous competitive baseline production data implementation even eﬀective. human evaluations show gnmt reduced translation errors compared previous phrase-based system many pairs languages english french english spanish english chinese. additional experiments suggest quality resulting translation system gets closer average human translators. related work statistical machine translation dominant translation paradigm decades practical implementations generally phrase-based systems translate sequences words phrases lengths diﬀer even prior advent direct neural machine translation neural networks used component within systems success. perhaps notable attempts involved joint language model learn phrase representations yielded impressive improvement combined phrase-based translation. approach however still makes phrase-based translation systems core therefore inherits shortcomings. proposed approaches learning phrase representations learning end-to-end translation neural networks oﬀered encouraging hints ultimately delivered worse overall accuracy compared standard phrase-based systems. limited success. following seminal papers area translation quality crept closer level phrase-based translation systems common research benchmarks. perhaps ﬁrst successful attempt surpassing phrase-based translation described wmt’ english-to-french system achieved bleu improvement compared state-of-the-art phrase-based system. since then many novel techniques proposed improve using attention mechanism deal rare words mechanism model translation coverage multi-task semi-supervised training incorporate data character decoder character encoder subword units also deal rare word outputs diﬀerent kinds attention mechanisms sentence-level loss minimization translation accuracy systems encouraging systematic comparison large scale production quality phrase-based translation systems lacking. model architecture model follows common sequence-to-sequence learning framework attention three components encoder network decoder network attention network. encoder transforms source sentence list vectors vector input symbol. given list vectors decoder produces symbol time special end-of-sentence symbol produced. encoder decoder connected attention module allows decoder focus diﬀerent regions source sentence course decoding. notation bold lower case denote vectors bold upper case represent matrices cursive upper case represent sets capital letters represent sequences lower case represent individual symbols sequence equation list ﬁxed size vectors. number members list number symbols source sentence using chain rule conditional probability sequence decomposed decoder implemented combination network softmax layer. decoder network produces hidden state next symbol predicted goes softmax layer generate probability distribution candidate output symbols. experiments found systems achieve good accuracy encoder decoder rnns deep enough capture subtle irregularities source target languages. observation similar previous observations deep lstms signiﬁcantly outperform shallow lstms work additional layer reduced perplexity nearly similar deep stacked long short term memory network encoder decoder rnn. figure model architecture gnmt google’s neural machine translation system. left encoder network right decoder network middle attention module. bottom encoder layer bi-directional pink nodes gather information left right green nodes gather information right left. layers encoder uni-directional. residual connections start layer third bottom encoder decoder. model partitioned multiple gpus speed training. setup encoder lstm layers decoder layers. setting model replica partitioned -ways placed diﬀerent gpus typically belonging host machine. training bottom bi-directional encoder layers compute parallel ﬁrst. ﬁnish uni-directional encoder layers start computing separate gpu. retain much parallelism possible running decoder layers bottom decoder layer output obtaining recurrent attention context sent directly remaining decoder layers. softmax layer also partitioned placed multiple gpus. depending output vocabulary size either gpus encoder decoder networks separate dedicated gpus. residual connections mentioned above deep stacked lstms often give better accuracy shallower models. however simply stacking layers lstm works certain number layers beyond network becomes slow diﬃcult train likely exploding vanishing gradient problems experience large-scale translation tasks simple stacked lstm layers work well layers barely layers poorly beyond layers. figure diﬀerence normal stacked lstm stacked lstm residual connections. left simple stacked lstm layers right implementation stacked lstm layers residual connections. residual connections input bottom lstm layer element-wise added output bottom layer lstm layer input. motivated idea modeling diﬀerences intermediate layer’s output targets shown work well many projects past introduce residual connections among lstm layers stack concretely lstmi lstmi+ i-th lstm layers stack whose parameters respectively. t-th time step stacked lstm without residual connections have residual connections greatly improve gradient backward pass allows train deep encoder decoder networks. experiments lstm layers encoder decoder though residual connections allow train substantially deeper networks bi-directional encoder first layer translation systems information required translate certain words output side appear anywhere source side. often source side information approximately left-to-right similar best possible context point encoder network makes sense bi-directional encoder also used allow maximum possible parallelization computation bi-directional connections used bottom encoder layer encoder layers uni-directional. figure illustrates bi-directional lstms bottom encoder layer. layer lstmf processes source sentence left right layer lstmb processes source sentence right left. outputs lstmf lstmb ﬁrst concatenated next layer lstm. figure structure bi-directional connections ﬁrst layer encoder. lstm layer lstmf processes information left right lstm layer lstmb processes information right left. output lstmf lstmb ﬁrst concatenated next lstm layer lstm. model parallelism complexity model make model parallelism data parallelism speed training. data parallelism straightforward train model replicas concurrently using downpour algorithm replicas share copy model parameters replica asynchronously updating parameters using combination adam algorithms. experiments often around replica works mini-batch sentence pairs time often experiments. addition data parallelism model parallelism used improve speed gradient computation replica. encoder decoder networks partitioned along depth dimension placed multiple gpus eﬀectively running layer diﬀerent gpu. since ﬁrst encoder layer uni-directional layer start computation layer fully ﬁnished improves training speed. softmax layer also partitioned partition responsible subset symbols output vocabulary. figure shows details partitioning done. model parallelism places certain constraints model architectures use. example cannot aﬀord bi-directional lstm layers encoder layers since would reduce parallelism among subsequent layers layer would wait forward backward directions previous layer ﬁnished. would eﬀectively constrain make gpus parallel attention portion model chose align bottom decoder output encoder output maximize parallelism running decoder network. aligned decoder layer encoder layer would removed parallelism decoder network would beneﬁt using decoding. segmentation approaches neural machine translation models often operate ﬁxed word vocabularies even though translation fundamentally open vocabulary problem broad categories approaches address translation out-of-vocabulary words. approach simply copy rare words source target either based attention model using external alignment model even using complicated special purpose pointing network another broad category approaches sub-word units e.g. chararacters mixed word/characters intelligent sub-words wordpiece model successful approach falls second category adopt wordpiece model implementation initially developed solve japanese/korean segmentation problem google speech recognition system approach completely data-driven guaranteed generate deterministic segmentation possible sequence characters. similar method used deal rare words neural machine translation. processing arbitrary words ﬁrst break words wordpieces given trained wordpiece model. special word boundary symbols added training model original word sequence recovered wordpiece sequence without ambiguity. decoding time model ﬁrst produces wordpiece sequence converted corresponding word sequence. example word sequence corresponding wordpiece sequence word makers feud seat width orders stake wordpieces _makers _over _seat _width _with _big _orders _stake example word broken wordpieces word feud broken wordpieces words remain single wordpieces. special character added mark beginning word. wordpiece model generated using data-driven approach maximize language-model likelihood training data given evolving word deﬁnition. given training corpus number desired tokens optimization problem select wordpieces resulting corpus minimal number wordpieces segmented according chosen wordpiece model. greedy algorithm optimization problem similar described detail compared original implementation used special symbol beginning words ends. also number basic characters manageable number depending data rest special unknown character avoid polluting given wordpiece vocabulary rare characters. using total vocabulary wordpieces achieves good accuracy fast decoding speed across pairs language pairs tried. mentioned above translation often makes sense copy rare entity names numbers directly source target. facilitate type direct copying always shared wordpiece model source language target language. using approach guaranteed string source target sentence segmented exactly making easier system learn copy tokens. wordpieces achieve balance ﬂexibility characters eﬃciency words. also models better overall bleu scores using wordpieces possibly fact models deal eﬃciently essentially inﬁnite vocabulary without resorting characters only. mixed word/character model second approach mixed word/character model. word model keep ﬁxed-size word vocabulary. however unlike conventional word model words collapsed single symbol convert words sequence constituent characters. special preﬁxes prepended characters show location characters word distinguish normal in-vocabulary characters. three preﬁxes <b><m> indicating beginning word middle word word respectively. example let’s assume word miki vocabulary. preprocessed sequence special tokens <b>m <m>i <m>k <e>i. process done source target sentences. decoding output also contain sequences special tokens. preﬁxes trivial reverse tokenization original words part post-processing step. main problem objective reﬂect task reward function measured bleu score translation. further objective explicitly encourage ranking among incorrect output sequences outputs higher bleu scores still obtain higher probabilities model since incorrect outputs never observed training. words using maximum-likelihood training only model learn robust errors made decoding since never observed quite mismatch training testing procedure. several recent papers considered diﬀerent ways incorporating task reward optimization neural sequence-to-sequence models. work also attempt reﬁne model pretrained maximum likelihood objective directly optimize task reward. show that even large datasets reﬁnement state-of-the-art maximum-likelihood models using task reward improves results considerably. bleu score undesirable properties used single sentences designed corpus measure. therefore slightly diﬀerent score experiments call gleu score. gleu score record sub-sequences tokens output target sequence compute recall ratio number matching n-grams number total n-grams target sequence precision ratio number matching n-grams number total n-grams generated output sequence. gleu score simply minimum recall precision. gleu score’s range always symmetrical switching output target. according experiments gleu score correlates quite well bleu metric corpus level drawbacks sentence reward objective. common practice reinforcement learning subtract mean reward equation mean estimated sample mean sequences drawn independently distribution pθ). implementation stabilize training optimize linear combination objectives follows setup ﬁrst train model using maximum likelihood objective convergence. reﬁne model using mixed maximum likelihood expected reward objective bleu score development longer improving. second step optional. quantizable model quantized inference main challenges deploying neural machine translation model interactive production translation service computationally intensive inference making latency translation diﬃcult high volume deployment computationally expensive. quantized inference using reduced precision arithmetic technique signiﬁcantly reduce cost inference models often providing eﬃciency improvements computational devices. example demonstrated convolutional neural network model sped factor minimal loss classiﬁcation accuracy ilsvrc- benchmark. demonstrated neural network model weights quantized three states many previous studies however mostly focus models relatively layers. deep lstms long sequences pose novel challenge quantization errors signiﬁcantly ampliﬁed many unrolled steps going deep lstm stack. section present approach speed inference quantized arithmetic. solution tailored towards hardware options available google. reduce quantization errors additional constraints added model training quantizable minimal impact output model. model trained additional constraints subsequently quantized without loss translation quality. experimental results suggest additional constraints hurt model convergence quality model converged. recall equation lstm stack residual connections accumulators along time axis along depth axis. theory accumulators unbounded practice noticed values remain quite small. quantized inference explicitly constrain values accumulators within guarantee certain range used quantization later. forward computation lstm stack residual connections modiﬁed following quantized inference replace ﬂoating point operations equations ﬁxed-point integer operations either -bit -bit resolution. weight matrix represented using -bit integer matrix ﬂoat vector shown below represented using -bit integers representing range accumulator values equation done using -bit integer multiplication accumulated larger accumulators. operations including activations elementwise operations done using -bit integer operations. equation weight matrix linear layer number rows number symbols target vocabulary corresponding unique target symbol. represents logits ﬁrst clipped normalized probability vector input guaranteed quantization scheme applied decoder rnn. clipping range logits determined empirically case quantized inference weight matrix quantized bits equation matrix multiplication done using arithmetic. calculations within tmax function attention model quantized inference. worth emphasizing training model full-precision ﬂoating point numbers. constraints model training clipping accumulator values softmax logits ﬁxed value gradually annealed generous bound beginning training rather stringent bound towards training. inference time ﬁxed additional constraints degrade model convergence decoding quality model converged. figure compare loss steps unconstrained model constrained model wmt’ english-to-french. loss constrained model slightly better possibly regularization roles constraints play. solution strikes good balance eﬃciency accuracy. since computationally expensive operations done using -bit integer operations quantized inference quite eﬃcient. also since error-sensitive accumulator values stored using -bit integers solution accurate robust quantization errors. figure perplexity steps normal training quantization-aware training wmt’ english french maximum likelihood training. notice training losses similar quantization-aware loss slightly better. conjecture quantization-aware training slightly better clipping constraints additional regularization improves model quality. google’s tensor processing unit respectively. model used comparison trained quantization constraints objective model decoded quantized operations done using full-precision ﬂoats. decoded certain operations embedding lookup attention module remain quantized operations oﬀ-loaded tpu. cases decoding done single machine intel haswell cpus consists total cores machine equipped nvidia experiment single google experiment tpu. table shows decoding using reduced precision arithmetics suﬀers minimal loss perplexity loss bleu all. result matches previous work reporting quantizing convolutional neural network models retain model quality. table also shows decoding model actually times faster gpu. firstly dual-cpus host machine oﬀers theoretical peak flop performance thirds gpu. secondly beam search algorithm forces decoder incur non-trivial amount data transfer host every decoding step. hence current decoder implementation table model inference tpu. model used comparison trained objective quantization constraints. results obtained decoding en→fr development respectively. unless otherwise noted always train evaluate quantized models experiments. little diﬀerence quality perspective model decoded cpus decoded tpus cpus decode model evaluation training experimentation tpus serve production traﬃc. decoder beam search decoding sequence maximizes score function given trained model. introduce important reﬁnements pure max-probability based beam search algorithm coverage penalty length normalization. length normalization account fact compare hypotheses diﬀerent length. without form length-normalization regular beam search favor shorter results longer ones average since negative log-probability added step yielding lower scores longer sentences. ﬁrst tried simply divide length normalize. improved original heuristic dividing lengthα optimized development usually found best). eventually designed empirically-better scoring function below also includes coverage penalty favor translations fully cover source sentence according attention module. attention probability j-th target word i-th source word construction equal parameters control strength length normalization coverage penalty. decoder falls back pure beam search probability. beam search typically keep hypotheses using fewer slight negative eﬀects bleu scores. besides pruning number considered hypotheses forms pruning used. firstly step consider tokens local scores beamsize best token step. secondly normalized best score found according equation prune hypotheses beamsize best normalized score far. latter type pruning applies full hypotheses compares scores normalized space available hypothesis ends. latter form pruning also eﬀect quickly hypotheses generated suﬃciently good hypothesis found search quickly. pruning speeds search cpus improve throughput decoding many sentences similar length batch decode parallel make available hardware optimized parallel computations. case beam search ﬁnishes hypotheses sentences batch beam slightly less eﬃcient theoretically practice negligible additional computational cost. table wmt’ en→fr bleu score respect diﬀerent values model experiment trained using without reﬁnement. single en→fr model achieves bleu score development beam search scoring function purely based sequence probability slightly larger values improve bleu score wide range values giving results close best bleu scores. table shows impact bleu score decoding wmt’ english-to-french development set. model used experiments trained using objective seen results length normalization coverage penalty improves bleu score considerably length normalization coverage penalty less eﬀective models reﬁnement. table summarizes results. understandable reﬁnement models already learn attention full source sentence under-translate over-translate would result penalty bleu scores. table en→fr bleu score respect diﬀerent values model used trained using reﬁned compared results table coverage penalty length normalization appear less eﬀective models rl-based model reﬁnements. results obtained development set. experiments results section present experimental results publicly available corpora used extensively benchmarks neural machine translation systems wmt’ english-to-french english-to-german datasets benchmark gnmt models word-based character-based wordpiece-based vocabularies. also present improved accuracy models ﬁne-tuning model ensembling. main objective datasets show contributions various components implementation particular wordpiece model model reﬁnement model ensembling. addition testing publicly available corpora also test gnmt google’s translation production corpora three decimal orders magnitudes bigger corpora given language pair. compare accuracy model human accuracy best phrase-based machine translation production system google translate. experiments models consist encoder layers decoder layers. attention network simple feedforward network hidden layer nodes. models lstm nodes encoder decoder layers. datasets evaluate model en→fr dataset en→de dataset well many googleinternal production datasets. en→fr training contains sentence pairs. en→de training contains sentence pairs. cases newstest test sets compare previous work combination newstest newstest used development set. addition also evaluate model google-internal datasets representing wider spectrum languages distinct linguistic properties english french english spanish english chinese. evaluation metrics evaluate models using standard bleu score metric. comparable previous work report tokenized bleu score computed multi-bleu.pl script downloaded public implementation moses also used well-known bleu score fully capture quality translation. reason also carry side-by-side evaluations human raters evaluate compare quality translations presented side side given source sentence. side-by-side scores range score meaning completely nonsense translation score meaning perfect translation meaning translation completely consistent source grammar correct. translation given score sentence retains meaning source sentence grammar mistakes translation given score sentence preserves meaning source sentence misses signiﬁcant parts. scores generated human raters ﬂuent languages hence often capture translation quality better bleu scores. training procedure models trained system implemented using tensorflow. training setup follows classic data parallelism paradigm. replicas running concurrently separate machines. every replica updates shared parameters asynchronously. initialize trainable parameters uniformly common wisdom training models apply gradient clipping gradients uniformly scaled norm modiﬁed gradients larger ﬁxed constant case. norm original gradients already smaller equal given threshold gradients changed. ﬁrst stage maximum likelihood training combination adam simple learning algorithms provided tensorflow runtime system. adam ﬁrst steps switch simple sgd. step training mini-batch examples. figure perplexity steps adam adam-then-sgd en→fr maximum likelihood training. adam converges much faster beginning. towards however adam-then-sgd gradually better. notice bump curve around steps switch adam sgd. suspect bump occurs diﬀerent optimization trajectories adam sgd. switch adam model ﬁrst suﬀers little able quickly recover afterwards. part learning rate important also anneal learning rate certain number total steps. en→fr dataset begin anneal learning rate steps halve learning rate every steps additional steps. en→fr takes around days train basic model using nvidia gpus. model fully converged using objective switch based model reﬁnement i.e. optimize objective function equation reﬁne model bleu score change much development set. model reﬁnement phase simply optimization algorithm. number steps needed reﬁne model varies dataset dataset. en→fr takes around days complete steps. prevent overﬁtting apply dropout training scheme similar en→fr en→de datasets dropout probability respectively. various technical reasons dropout applied training phase reﬁnement phase. exact hyper-parameters vary dataset dataset model model. en→de dataset since signiﬁcantly smaller en→fr dataset higher dropout evaluation maximum likelihood training models experiments word-based character-based mixed word-character-based several wordpiece models varying vocabulary sizes. word model selected frequent source words source vocabulary popular target words target vocabulary. words source vocabulary target vocabulary converted special <first_char>_unk_<last_char> symbols. note case attention mechanism copy corresponding word source replace unknown words decoding mixed word-character model similar word model except out-of-vocabulary words converted sequences characters special delimiters around described section detail. experiments vocabulary size mixed word-character model pure character model simply split words constituent characters resulting typically hundred basic characters wordpiece models train diﬀerent models vocabulary sizes table summarizes results en→fr dataset. table also compare strong baselines without model ensembling. seen table wpm-k wordpiece model shared source target vocabulary wordpieces performs well dataset achieves best quality well fastest inference speed. pure character model works surprisingly well task much worse best wordpiece models bleu score. however models rather slow train slow sequences much longer. best model wpm-k achieves bleu score note bleu score represents averaged score models trained. maximum bleu score models higher point models completely self-contained opposed previous models reported depend external alignment models achieve best results. also note test numbers achieved picking optimal model development used decode test set. note timing numbers section obtained cpus tpus. machine described above decoder batchsize sentences parallel maximum concurrent hypotheses time sentence. time sentence total decoding time divided number respective sentences test set. en→de considered diﬃcult task en→fr much less training data german morphologically rich language needs huge vocabulary word models. thus advantageous wordpiece mixed word/character models provide gain bleu points word model bleu points previously reported results best model wpm-k achieves bleu score averaged runs. consistently production corpora wordpiece models tend better models terms speed accuracy. evaluation rl-reﬁned models models trained previous section optimized log-likelihood next step prediction correlate well translation quality discussed section training ﬁne-tune sentence bleu scores normal maximum-likelihood training. results ﬁne-tuning best en→fr en→de models presented table show ﬁne-tuning models improve bleu scores. en→fr model reﬁnement improves bleu score close point. en→de rl-reﬁnement slightly hurts test performance even though observe bleu points improvement development set. results presented table average independent models. also note overlap wins reﬁnement decoder ﬁne-tuning less ﬁne-tuned decoder would bigger model ensemble human evaluation ensemble rl-reﬁned models obtain state-of-the-art result bleu points en→fr dataset. results reported table en→de dataset. results reported table finally better understand quality models eﬀect reﬁnement carried four-way side-by-side human evaluation compare translations reference translations best phrase-based statistical machine translations. side-by-side comparison humans asked rate four translations given source sentence. four translations best phrasebased translations downloaded http//matrix.statmt.org/systems/show/ ensemble ml-trained models ensemble ml-trained rl-reﬁned models reference human translations taken directly newstest results presented table results show even though reﬁnement achieve better bleu scores barely improves human impression translation quality. could combination factors including relatively small sample size experiment improvement bleu score relatively small model ensembling scale human side-by-side evaluations insensitive possible mismatch bleu metric real translation quality perceived human raters. table contains example translations pbmt \"nmt \"human\" along side-by-side scores human raters assigned translation results production data carried extensive experiments many google-internal production data sets. experiments cast doubt whether improves real translation quality simply bleu metric rl-based model reﬁnement used experiments. given larger volume training data available google corpora dropout also needed experiments. section describe experiments human perception translation quality. asked human raters rate translations three-way side-by-side comparison. three sides from translations production phrase-based statistical translation system used google translations gnmt system translations humans ﬂuent languages. reported table averaged rated scores english french english spanish english chinese. gnmt models wordpiece models without model ensembling shared source target vocabulary wordpieces. pair languages evaluation data consist randomly sampled sentences wikipedia news websites corresponding human translations target language. results show model reduces translation errors compared pbmt model major pairs languages. typical distribution side-by-side scores shown figure figure histogram side-by-side scores sampled sentences wikipedia news websites typical language pair english spanish seen wide distribution scores even human translation rated humans shows ambiguous task clear gnmt much accurate pbmt. expected metric gnmt system improves also compared pbmt system. cases human gnmt translations nearly indistinguishable relatively simplistic isolated sentences sampled wikipedia news articles experiment. note observed human raters even though ﬂuent languages necessarily fully understand randomly sampled sentence suﬃciently hence cannot necessarily generate best possible translation rate given translation accurately. also note that although scale scores goes human translations imperfect score around table shows possible ambiguities translations also possibly non-calibrated raters translators varying level proﬁciency. conclusion paper describe detail implementation google’s neural machine translation system including techniques critical accuracy speed robustness. public wmt’ translation benchmark system’s translation quality approaches surpasses currently published results. importantly also show approach carries much larger production data sets several orders magnitude data deliver high quality translations. ﬁndings wordpiece modeling eﬀectively handles open vocabularies challenge morphologically rich languages translation quality inference speed combination model data parallelism used eﬃciently train state-of-the-art sequence-to-sequence models roughly week model quantization drastically accelerates translation inference allowing large models deployed production environment many additional details like length-normalization coverage penalties similar essential making systems work well real data. using human-rated side-by-side comparison metric show gnmt system approaches accuracy achieved average bilingual human translators test sets. particular compared previous phrase-based production system gnmt system delivers roughly reduction translation errors several popular language pairs. references abadi barham chen chen davis dean devin ghemawat irving isard kudlur levenberg monga moore murray steiner tucker vasudevan warden wicke zheng tensorﬂow system large-scale machine learning. tech. rep. google brain arxiv preprint. bahdanau bengio neural machine translation jointly learning align brown cocke pietra pietra jelinek mercer roossin statistical approach language translation. proceedings conference computational linguistics volume coling association computational linguistics brown pietra pietra mercer mathematics statistical machine translation parameter estimation. comput. linguist. buck heafield ooyen n-gram counts language models common merrienboer gülçehre bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. conference empirical methods natural language processing durrani haddow koehn heafield edinburgh’s phrase-based machine translation systems wmt-. proceedings ninth workshop statistical machine translation association computational linguistics baltimore luong m.-t. sutskever vinyals zaremba addressing rare word problem neural machine translation. proceedings annual meeting association computational linguistics international joint conference natural language processing norouzi bengio chen jaitly schuster schuurmans reward augmented maximum likelihood neural structured prediction. neural information processing systems sébastien kyunghyun memisevic bengio using large target vocabulary neural machine translation. proceedings annual meeting association computational linguistics international joint conference natural language processing sennrich haddow birch neural machine translation rare words subword units. proceedings annual meeting association computational linguistics shen cheng minimum risk training proceedings annual meeting association table example translations pbmt gnmt system human. source target sentences public benchmark en→fr data set. right-hand column shows human ratings scale disagree human ratings e.g. translation elle repéré trois jours plus tard promeneur chien piégé dans carrière contains grammatical mistakes changes semantics still scored present illustrate potential problems scoring process. \"the reason boeing cram seats make plane competitive products\" said kevin keniston head passenger comfort europe’s airbus. raison pour laquelle boeing sont train faire c’est concentrer davantage sièges pour prendre leur avion plus compétitive avec produits\" déclaré kevin keniston chef confort passagers l’airbus l’europe. raison pour laquelle boeing fait cela créer plus sièges pour rendre avion plus compétitif avec produits\" déclaré kevin keniston chef confort passagers chez airbus. \"boeing fait pour pouvoir caser plus sièges rendre avions plus compétitifs rapports produits\" déclaré kevin keniston directeur confort passager chez l’avionneur européen airbus. asked this oﬃcial american administration replied \"the united states conducting electronic surveillance aimed oﬃces world bank washington.\" interrogé sujet responsable l’administration américaine répondu \"les etats-unis n’est eﬀectuer surveillance électronique destiné bureaux banque mondiale washington\". interrogé sujet fonctionnaire l’administration américaine répondu \"les états-unis n’eﬀectuent surveillance électronique l’intention bureaux banque mondiale washington\". interrogé sujet responsable l’administration américaine répondu \"les etats-unis mènent surveillance électronique visant sièges banque mondiale washington\". source martin told asked daley whether then-boss knew potential pbmt martin déclaré qu’il demandé daley patron l’époque connaissaient gnmt martin qu’il avait demandé daley patron d’alors était courant human martin qu’il avait demandé daley patron d’alors était source pbmt elle repéré trois jours plus tard promeneur chien piégé dans carrière gnmt elle repérée trois jours plus tard traîneau chiens piégé dans carrière. human elle repérée trois jours plus tard personne promenait chien source analysts believe country unlikely slide back full-blown conﬂict recent pbmt analystes estiment pays chances retomber dans conﬂit total mais événements récents inquiété investisseurs étrangers locaux. selon analystes probable pays retombe dans conﬂit généralisé mais événements récents attiré investisseurs étrangers habitants locaux.", "year": 2016}