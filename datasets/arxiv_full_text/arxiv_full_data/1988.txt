{"title": "A Method for Restoring the Training Set Distribution in an Image  Classifier", "tag": ["stat.ML", "cs.AI", "cs.CV"], "abstract": "Convolutional Neural Networks are a well-known staple of modern image classification. However, it can be difficult to assess the quality and robustness of such models. Deep models are known to perform well on a given training and estimation set, but can easily be fooled by data that is specifically generated for the purpose. It has been shown that one can produce an artificial example that does not represent the desired class, but activates the network in the desired way. This paper describes a new way of reconstructing a sample from the training set distribution of an image classifier without deep knowledge about the underlying distribution. This enables access to the elements of images that most influence the decision of a convolutional network and to extract meaningful information about the training distribution.", "text": "alexey chaplygin joshua chacksfield data science research group department analytics business intelligence corp. {alexey.chapligin chacksfieldj}gmail.com convolutional neural networks well-known staple modern image classification. however difficult assess quality robustness models. deep models known perform well given training estimation easily fooled data specifically generated purpose. shown produce artificial example represent desired class activates network desired way. paper describes reconstructing sample training distribution image classifier without deep knowledge underlying distribution. enables access elements images influence decision convolutional network extract meaningful training distribution. introduction convolutional neural networks shown great performance image classification tasks deep networks generally seen black boxes believed hard analyse training. recent years number techniques derived adversarial examples invented capable fooling image classifiers. creating adversarial example easy task produce comically wrong results text captioning. realm image classification shown images generated classified high confidence unrecognisable humans miss categorisation images cause serious issues commercially available classifiers leaving open attacks potential vulnerabilities. question features influence output convolutional network often posed difficult several techniques developed provide insights internals trained models. approaches range simply slicing convolutional layers network visualising neurons constructions called grad cams guided grad cams however techniques require samples distribution upon model trained show limits trained classifier. acquiring sample training data difficult cases nature classifier barely known trained standard open dataset imagenet. multiple cases nature classifier might unknown example digital locks iris retina facial recognition. another potential described method determine distribution data upon image classifier trained access whether classifier could applied particular problem. related work research conducted independent article aimed provide view training distribution. anonymous authors detailed investigation titled classifier-to-generator attack estimation training data distribution classifier generally coincides findings paper training distribution reconstructed classifier. however main difference classifier-togenerator attack approach approach described approach require structural similarity substrate distribution restore auxiliary dataset. aforementioned paper used similar datasets both implies good note paper result project executed europe business need generate body-wear prints/patterns definite association defined category repeat exact full details object. interesting prints remind objects nature created blank piece clothes substrate. methodology example network used pre-trained classes method could applied image classifier. classes filtered classes reconstructed represented target vector generate images conditional used. generator constructed that convolution layer input padded filters size applied stride size lastly applied improve model fitting deconvolution layers constructed similar manner using filters size applied stride size also applying last layer function used. figure adversarial examples crafted show-and-fool using targeted caption method examples showing alterations images cause gross ‚Äòmisclassification‚Äô images. pair shows original image correctly generated text. bottom pair contains crafted example results highly different caption generated. understanding distribution training classifier. another difference conditional paper opposed classic gan. order substrate image starting point reconstruction. using classic model unable restore distribution unknown nature wasn‚Äôt applicable case pre-trained classifier used penalize output following way. square crops size pixels image generated three distinct crops selected randomly equal probability. figure generated image loss applied determine whether image. classification performed different ways firstly whole image resized secondly three random crops. classifications used loss ùë≥ùëΩùëÆùëÆ. construct deconvolutional layers along u-net structure feature maps convolutional part also concatenated deconvolutional layers outputs next layer reconstruct image size training weight normalization applied together parametric leaky relu faster convergence improved stability multiple loss functions used construct objective function project. four parts loss function used firstly canonical loss defined observed image noise component output image passed network. adam optimizer used minimize target function learning rate beta batch size images number steps training accomplished within hours nvidia titan gpu. final loss constraint classifiers category present generated image constructed parts positive classes appearing entire picture negative classes random crops. simple summation linear logarithmic term applied vector calculated generators using architecture described paper able restore categories categories mixed together once. using higher number categories generator failed effectively reconstruct categories objects became unrecognizable mixing details category another. extension method described above approach similar gradcam used mask regions activated classifier most. alteration provided visually better separation substrate image generated picture desired category. however applicable scope original project smooth merge substrate generated category required. masking definitely advised achieving better results reconstructing training distribution. conclusion applying architectures classifier sample reconstructed. approach opens host possible vulnerabilities image recognition models serious consequences. examples include system used authorisation purposes could exploited model trained sensitive private data could opened public access. time described approach used visualize capabilities image classifiers analyse redundancy within models. samples clearly seen activated artefacts direct relation predicted category. labrador category also reproduced trees horsecart category grass details around horse observed. shows redundancy classifier and/or optimal training set. believe model able reconstruct training distribution using auxiliary dataset training distribution fact conditional used. conditional determines whether generated image looks like photo actual object real world. however also restriction approach auxiliary dataset consist images containing style would like inherit generated object. known trained photos possible reconstruct samples training using auxiliary dataset also containing photos. investigation auxiliary datasets different distributions using different classifiers avenue research could bring greater understanding problem. supplementary material architecture extended able output images size optimised objective function imagenet category clearly seen generated picture. however confidence image classifier required balance loss resulted visually pleasing patterns. also required random crop contain wanted category forcing model generate pattern-like outputs rather picture certain object.", "year": 2018}