{"title": "On the Behavior of Convolutional Nets for Feature Extraction", "tag": ["cs.NE", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions of images, without requiring an expensive training phase. Contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all CNN features from a single layer close to the output and testing their performance by feeding them to a classifier. This approach has provided consistent results, although its relevance is limited to classification tasks. In a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep CNN, when used for characterizing every class of 11 datasets. We seek to provide new insights into the behavior of CNN features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. Our results confirm that low and middle level features may behave differently to high level features, but only under certain conditions. We find that all CNN features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single CNN feature may provide. We also study how much noise these features may include, and propose a thresholding approach to discard most of it. All these insights have a direct application to the generation of CNN embedding spaces.", "text": "dario garcia-gasulla ferran par´es armand vilalta jonatan moreno barcelona supercomputing center omega building jordi girona barcelona catalonia deep neural networks representation learning techniques. training deep capable generating descriptive language unprecedented size detail machine learning. extracting descriptive language coded within trained model reusing purposes ﬁeld interest provides access visual descriptors previously learnt processing millions images without requiring expensive training phase. contributions ﬁeld purely empirical extracting features single layer close output testing performance feeding classiﬁer. approach provided consistent results although relevance limited classiﬁcation tasks. completely diﬀerent approach paper statistically measure discriminative power every single feature found within deep used characterizing every class datasets. seek provide insights behavior features particularly ones convolutional layers relevant application knowledge representation reasoning. results conﬁrm middle level features behave diﬀerently high level features certain conditions. features used knowledge representation purposes presence absence doubling information single feature provide. also study much noise features include propose thresholding approach discard insights direct application generation embedding spaces. image classiﬁcation among successful applications deep learning. performance deep learning networks challenges like imagenet large scale visual recognition competition based capabilities models building exceptionally rich representation language given dataset. language used solving problems like classiﬁcation detection achieving state-of-the-art performance coherently deep learning models frequently deﬁned representation learning techniques unfortunately build representation language deep networks require data computational eﬀort reduces number problems models directly applied within deep learning ﬁeld research commonly known transfer learning tries reuse representation language learnt problem solve another. formalize notation introduced yang notation main using features learnt source problem network optimized later ﬁne-tuning process target problem signiﬁcantly approach shown produce better results training network data available alternative transfer learning neural network trained feature extractor order machine data language learnt task enabling pre-trained deep network representations datasets lack size required train methods according yang case would example feature representation transfer notice approach transfer learning used tackle unsupervised learning problems clustering would also enable features obtained unsupervised learning task context convolutional neural networks attempts feature representation transfer focused reusing activations obtained layers close output compared lower-level layers high-level layers provide better results used out-of-the-box feed classiﬁers clustering algorithms regardless results convolutional layers within encode large amount visual knowledge varying complexity knowledge successfully exploited far. since general purpose feature representation transfer maximize representativeness hypothesize optimal representation include degree information larger variety layers therefore relevant particularly knowledge representation reasoning ﬁelds understand diﬀerences convolutional layers fully connected layers learnt properly exploited. behavior individual features domain deﬁned dataset. first section introduces previous contributions transfer learning ﬁeld focusing feature extraction. section introduces datasets model used experiments well image embedding build feature extraction process. basis study statistical distance methods review section section introduces behavior statistical distances current problem distributions compose. distributions analyzed section impact noise measures discussed section consistency ﬁndings given diﬀerent source task shown section finally conclusions drawn study summarized section models deﬁned large number parameters requiring lots data instances optimization. release large visual datasets handmade features produced best results vision tasks. nowadays cnns trained using datasets imagenet learning powerful visual descriptors allows outperform previously competitive solutions improved fisher vectors many visual tasks ﬁrst fully connected layer outperform features lower layers task separating concepts according wordnet hierarchy. authors evaluate features extracted last convolutional layer ﬁrst fully connected layers various datasets. results indicate using features ﬁrst fully connected layers train support vector machine achieve state-of-the-art results various related tasks. mostly ﬁrst fully connected layer performing data augmentation increase quality features component-wise power transformation. applying l-normalization resultant vectors trained cub...) achieving competitive results them. tasks features various layers evaluated separately using ﬁrst fully connected layer obtaining best results. rather diﬀerent approach depicted yosinski goal study transferability features purpose tuning deep neural network target task dataset. regard authors distance source target tasks strongly related depth optimal layer used transfer learning process. azizpour empirically evaluated several parameters aﬀect transfer learning process feature extraction. among parameters considered related architecture training initial related transfer learning process parameters evaluated visual recognition tasks identifying good parameters depending distance source task target task. regarding representation layer authors ﬁrst second fully connected layer produces best results cases feeding classiﬁcation. deep residual networks evolution traditional include branching paths. unlike cnns stack layers sequentially resnets implement shortcut connections eases convergence training process allowing training networks layers mahmood explore resnets feature extraction particularly solve three image classiﬁcation problems. results indicate resnets competitive alternative classic architectures also context feature extraction. long proposed deep adaptation network architecture solve problem domain adaptation convolutional neural networks. architecture ﬁrst convolutional layers parameters reused without modiﬁcation weights last convolutional layers ﬁne-tuned task. fully connected weights tailored speciﬁc tasks multiple kernel maximum mean discrepancies however approach consider problem solving diﬀerent target task impact reusability pre-trained features. target problem include enough labeled data train representation language. context problem solved using features crafted diﬀerent problem although quality resultant embedding representation strongly depend similar are. language learnt lacks vocabulary properly characterize particularities mind typically chosen capture range visual patterns broad possible language likely include features relevant many diﬀerent target tasks capable characterizing wide variety image domains. regard trained imagenet dataset good candidate parameters). many popular architectures various used feature extraction since goal explore behavior convolutional layers feature extraction process architecture follows canonical scheme layers time wish model capable learning rich representation language various levels combination requirements leads architecture source features composed convolutional layers fully-connected layers exception figures obtained using architecture used illustrative purposes. architecture authors detailed paper. diﬀers extra convolutional layers conv conv conv results obtained architecture consistent ones obtained experiments. models publicly available authors page. indoor scene recognition dataset consists diﬀerent indoor scenes categories. main challenge resides class dependence global spatial properties relative presence objects. oulu knots dataset contains knot images spruce wood classiﬁed according nordic standards. dataset industrial application considered challenging even human experts. dataset sizes number classes number images class speciﬁed table experiments train models using datasets means require provided train test splits. instead merge splits make data available. modify construction embedding space. parameters explored azizpour case main parameters consider coherent study. first image representation built result processing crops image averaging resulting activations. frequently used methodology feature extraction provides robustness resultant representations. second perform spatial average pooling convolutional layer obtain single value ﬁlter. transformation reduces number features embedding well relative spatial information maintaining descriptive power spatial pooling methodology also recurrent solution ﬁeld since wish explore behavior convolutional layers embedding contain convolutional layers available comparison purposes also extract fully connected layers contrast behavior convolutional fully connected features. notice spatial pooling performed convolutional layers cannot applied layers. components resultant embedding composed values shown table remaining document mentions embedding refer representation. previous studies usefulness convolutional layers feature extraction transfer learning purely empirical based performance speciﬁc classiﬁers using features extracted single layer approach shown provide consistent results limited classiﬁcation strongly inﬂuenced choice classiﬁer paper propose diﬀerent approach evaluate behavior features. instead evaluating performance speciﬁc machine learning algorithm embedding measure descriptive power features statistically studying behavior diﬀerent classes composing dataset. goal learn descriptive nature features knowledge representation reasoning methodologies adapted accordingly. detail approach consists evaluating characteristic feature embedding target classes considered datasets. words want evaluate descriptive power group features analyze discriminative power single feature. neurons crisp behavior w.r.t. classes even original training task. instead neuron provides fuzzy piece information class. contextualize information provided inner/outer class behaviour visualized histograms feature activations statistically speaking rescaled histograms density estimations approximating true probability density function although sophisticated methods available histogram sake computational simplicity. study behavior given feature given class compare corresponding inner/outer density estimations. ﬁrst statistical distance consider using purpose well-known kullback-leibler divergence. although histograms approximations pdfs possible histogram. however inconvenient since histogram diﬀerent features diﬀerent pdfs features properly ﬁtted pdf. figure left inner/outer class density estimations feature layer conv class pink-yellow dahlia ﬂowers dataset. right accumulated distributions kolmogorov-smirnov statistic data. right line indicates maximum distance corresponding kolmogorov-smirnov statistic. diﬀerence distributions grows. experiments analogous belonging class thus represents inner-class distribution represents outer-class distribution. approximation bhattacharyya distance alternative measure distance discrete probability distributions. analogously measured density estimations following equation discrete points domain estimation below. analysis interest know inner-class behaviour higher outer-class behavior vice versa since situations provide diﬀerent insights. kolmogorov-smirnov statistic measures distance empirical distribution functions point domain evaluates distance obtains maximum. formally deﬁned equation graphically displayed right plot figure reduce computational cost evaluating discretize domain bins thus decreasing domain resolution notice that since cumulative distribution directly computed values contrast bhattacharyya distance dks’s mathematical range signed variant sign indicates point diﬀer most. variant extends range allows diﬀerentiate inner class behavior outer class behavior vice versa means distributions identical means distributions intersect point. statistic distance analysis based inner/outer class dks. section introduce behavior values distribution values layer-wise computing given datasets. following section contains detailed study distributions various perspectives. images belonging class almost identical distribution values feature images belong class feature values images tend higher rest images implies visual elements represented feature commonly found images images. similarly feature values general lower implies elements represented feature rare within images compared rest dataset. illustrate behavior explore features highest values dataset figure shows values diﬀerent layers indicating class large value occurs. show particular feature encoding plot image crops imagenet validation producing highest activation value feature. images dataset provide better feature characterizations since features originally trained classes. example figure features high value greenhouse class either showing plants grass ﬁelds regardless layer depth. feature high value buﬀet class identiﬁes food plates feature high value cloister identiﬁes gothic arches. case feature conv presents high value florist class crops producing high activations correspond colorful patterns contrast surroundings. analogous study positive values figure consider lowest values initially could expect features lowest given class would identifying elements never appear images example hypothetical class whale could expected extremely negative feature identifying wheel. however since values computed context dataset assumption incomplete. matter fact features lowest given class identifying elements appear images rarely compared frequency rest images. example dataset composed classes whale clownﬁsh features lowest values class whale would correspond figure embedding features high dataset architecture. class producing high shown feature. feature corresponds speciﬁc neuron ﬁlter original model. illustrate captured visual patterns feature show cropped images imagenet validation producing highest activation values neuron ﬁlter. images cropped match neuron receptive ﬁeld. highest values class clownﬁsh likely features identifying orange related patterns. hand features identifying uncommon patterns classes would value close zero classes inner/outer class distributions would similar. illustrate behavior extremely negative values figure shows feature extremely negative values four diﬀerent classes dataset. particular feature apparently specialized recognize ﬂying animals shown images imagenet validation produce maximum feature activation deeper analysis feature based methods used yosinski indicates central colorful ﬁgure cluttered background inﬂuential feature activation. nevertheless according study feature produces negative values several classes birds. explanation behind lies particularities classes feature produces extremely negative values four classes correspond birds live water coastal environment dull colors feature hand apparently specialized identifying colorful ﬂying animals lying branches. case extremely negative values neuron would analogous identifying ﬂying animals dull colors absence visual features. another example behavior ﬂowers dataset shown figure again features produce negative values seem representative whole dataset speciﬁc classes. feature encodes visual patterns corresponding radial orange patterns focuses wide petals clearly classes ﬂowers highly negative values properties. hence abnormal absence features roughly characterizing ﬂowers without radial pistils wide petals. figure example ﬁlter extremely negative value four diﬀerent classes dataset architecture. first illustrates visual pattern captured feature using method figure second third contain sample images dataset four diﬀerent classes name classes. figure example ﬁlters extremely negative value diﬀerent classes ﬂowers dataset architecture. first illustrates visual pattern captured feature using method figure second third contain sample images ﬂowers dataset diﬀerent classes name classes. examples illustrate lack feature activations convey relevant information. notice behavior negative values depend context provided dataset extremely negative values classes happen features consistently high value rest dataset. statistically extremely negative values feature happen small classes since classes grew inner/outer class disparity would decrease making closer zero. capability extracting knowledge lack data novel particularly relevant feature representation transfer features originally designed target task. setting presence absence visual patterns provide relevant information characterization images. also discuss relevance behavior ﬁne-grained datasets containing classes belonging small rather similar family entities. since extremely negative values identify infrequently feature activations needed feature frequent dataset often happen ﬁne-grained datasets many common features data. however broad datasets include wider visual variety classes much fewer features frequent classes infrequent few. hence much harder obtain extremely negative values. introducing essential behavior positive negative consider overall distribution values dataset. plotting values produced dataset obtain clear bimodal distribution separating positive negative values. figure example. modality resembles log-norm distribution. represent distribution values layers datasets single plot figure ﬂattens distribution displays corresponding modes error bars. discussing resultant distributions deﬁne terms following sections. highly discriminative capable separating associated labels. categorization made features highly descriptive ones help build rich representation domain highly discriminative ones help solve classiﬁcation task. context study discriminativeness feature w.r.t. class shown close either unfortunately descriptiveness feature cannot illustrated terms values descriptiveness originates domain task labels references discriminativeness feature refer deﬁnition. figure inner/outer class distribution layer diﬀerent datasets embedding. since distribution similar log-normal distribution represent mode central point error bars enclosing accumulated discuss distributions values shown figure regardless layer depth features discriminative tasks values close zero. separation decreases deeper layers particularly tasks diﬀer source task. since deeper features specialized source task features turn irrelevant task producing similar activation values turn results values closer zero. indeed distance decrease datasets essentially subset source task imagenet caltech caltech. behavior distributions fully-connected layers discussed section.. investigate variable behavior distributions based layer depth figure shows distributions separated plots features convolutional layers another features fully connected layers. according plot figure almost features convolutional layers equally discriminative datasets even tasks direct subset source task furthermore number degree positively discriminative features almost symmetrical number degree negatively discriminative features. indicates convolutional features contain similar amount information exploited modalities. behavior distributions convolutional layers discussed section.. insights coherent ﬁndings state-of-the-art indicating features high-level layer speciﬁc discriminant particularly target tasks close source task however results indicate features low-level layers general discriminant originally considered. opens door knowledge representations purposes related problems unsupervised learning. getting detailed analysis consider characterizes useful feature perspective distributions. help motivate conclusions draw consequent analysis. mentioned before features high absolute value given task discriminative classes task. considering features layer together figure desirable distribution becomes density concentrated much possible extremes axis. plots figure show convolutional features average separated irrelevancy however bibliography plenty experiments fully-connected layer shown outperform convolutional layer classiﬁcation. explanation phenomenon correlation values discriminativeness linear discriminativeness grows rapidly approaches result features good single feature higher discrimative power fully-connected features reported bibliography certain datasets thus supported distribution values which datasets slightly higher longer tail plus side distribution convolutional features datasets convolutional features discrimative power fully-connected features. figure distance distribution convolutional fully connected layers diﬀerent datasets embedding. axis indicates values feature class axis indicates occurrence percentage corresponding total features/class pairs section discuss observations make plots introduced previous section. following sections separately analyze behavior convolutional layers fully-connected layers. figures datasets ordered average number images class. shown figure clear correlation number values obtained convolutional layers correlation decreases layer depth non-existent fully-connected layers. start analysis distributions convolutional layers shown figure focusing unusually long bars wood certain degree also ﬂowers datasets. behaviour obvious ﬁrst convolutional layers modality becomes attenuated later layers symmetric modality. beyond relatively images class ﬂowers dataset specially wood dataset composed classes diﬀer small texture-like characteristics. level convolutional layers known learn ﬁlters similar gabor ﬁlters color blobs appropriate solve sort problems. factors explain features disproportionally discriminative datasets. textures dataset display behavior dataset speciﬁc textural patterns answer lies composition dataset. addition images class images textures dataset display textures image level looking pixels image impossible identify texture coherently discriminative features dataset ones found within middle upper convolutional layers. example behavior level convolutional layers figure shows features layers conv conv produce high values speciﬁc class ﬂowers dataset. particular features correspond horizontal gradients vertical gradients edge detectors features appear infrequently often images class compared rest ﬂowers dataset. beyond behavior wood ﬂowers datasets ﬁrst convolutional layers distribution values rather stable general. plot figure shows low-level convolutional features behave similarly datasets even though features optimized classiﬁcation imagenet classes seems roughly discriminative imagenet rest datasets. provides evidence transfer learning tuning produces good results also indicates features layers could used almost ubiquitously knowledge representation. dataset behaving clearly diﬀerently extremes values low-level layers features wood dataset reasons previously discussed detailed classes small sample size. ﬂowers caltech also tails height average datasets also include properties consider distributions values fully connected layers bottom plot figure dataset food distinct distribution large spike values close values close behaviour likely directly related variability domain well number images class something inconsistent food large number samples lead diﬀerent activations within class high value given class ﬂowers dataset architecture. ﬁrst contains sample image corresponding class. second contain feature visualizations imagenet obtained process figure include many diﬀerent ingredients presented many diﬀerent ways). variations lead indistinguishable inner-class outer-class behaviors turn results values close zero. results indicate similarity tasks relevant property behavior fully-connected features. supported distributions corresponding caltech caltech datasets. diﬀerences average number instances class |ic| total size number classes distributions almost identical figure explore consideration next categorize datasets groups based degree overlap imagenet figure distribution values features fully connected layer. plot show subset datasets grouped similarity imagenet. axis indicates values axis indicates occurrence percentage total. figure shows distribution values fully-connected features plotted separately three groups. group group distribution values gets close zero axis three datasets group. implies that three datasets single feature-class pair identical inner outer-class distribution. words datasets fully-connected features least somewhat discriminative classes. three datasets showing behavior imagenet caltech caltech. wide spectrum datasets directly contained within source task imagenet. signiﬁcantly happens regardless number classes hand datasets group happen limited certain domain even though datasets subsets source task still fully-connected features discriminant class. irrelevant feature class pairs likely correspond features used characterize type elements found imagenet restricted domains datasets indiscriminant features prevent distribution reach zero point. impact including wide spectrum classes w.r.t. indiscriminant features supported dataset fourth lowest percentage feature/class pairs close textures dataset figure which although apparently little common source imagenet task includes wide variety textures coming plants animals man-made objects etc. general bimodal distributions groups imbalanced part distribution accounts signiﬁcantly larger proportion total area. hand distribution values group closer distribution values convolutional features modalities symmetrical. imbalanced behavior groups explained nature fully-connected features optimized original training strongly activate small subset classes inhibited vast majority. also results higher tail side. hand balanced behavior group indicates cases instead activating strongly classes features activate moderately larger amount classes. particularly interesting indicates fully-connected features could treated convolutional features target task completely diﬀerent source task. section discussed distribution values dataset level assuming values evenly distributed among classes compose dataset. however case subset classes composing dataset large relevant features another subset classes under-represented features characterizing them. answer question figure plot accumulated distribution values class. black lines represents single class dataset. graph accumulative showing many features value greater axis value class. thus plotting number features meet class. figure positive distance accumulated distribution diﬀerent datasets embedding. line corresponds diﬀerent class dataset. results dataset randomized labels shown red. dashed line marks threshold introduced section figure shows certain variance among classes dataset given threshold. implies classes richly characterized embedding others suspected. although class reaches axis around reach remarkable check features implies figure positive distance accumulated distribution diﬀerent datasets embedding. line corresponds diﬀerent class dataset. results dataset randomized labels shown red. dashed line marks threshold introduced section class characterized embedding ﬁgure show behavior dataset randomized labels obtained assigning image random label keeping total number instances class unmodiﬁed notice process keeps unchanged properties like number classes imbalance number instances class. randomizing labels observe characterization embedding produces purely noisy classes characteristics target task. shown datasets figure randomized classes drop features black lines allows assert classes represented meaningfully certain point. also triggers question portion curve could pruned maximize discriminativeness minimizing noise. equivalent minimum value consider relevant choosing features characterize class. main goals paper study viability using convolutional features feature representation transfer. however unspeciﬁcity many convolutional features generate noise sense provide information related target labels. plots figure showing inner outer class distributions randomized classes provides ﬁrst insight actual magnitude noise. maximizes distance datasets corresponding version randomized labels deﬁne distance using average number features range analogous compute every point along axis subﬁgures figure average axis values black/red lines. average black lines give behavior regular dataset average lines give behavior randomized version. obtaining diﬀerence values obtain average distance dataset randomized version. formally average distance value table thresholds deﬁned maximum average distance eleven datasets explored. regions computed separately. third ﬁfth column shows maximum davg distance dataset randomized version. clear correlation number samples class |ic|. indeed logarithmic curve ﬁtted respect |ic| obtaining coeﬃcients determination respectively. indicates number images class good indicator level noise expected. factor overshadows relevant aspects level similarity tasks important tasks exactly feature-class pairs evenly distributed among convolutional fully-connected layers. indicates noise found throughout embedding datasets. datasets groups pruned feature-class pairs fully-connected layers signiﬁcantly larger pruned pairs convolutional layers. caused higher speciﬁcity high level features frequently irrelevant characterizing classes diﬀer source task. results could useful given target task determining features layers used building embedding. analogous figure figure shows distribution values layer using embedding created places dataset. overall distribution quite similar obtained imagenet embedding. focusing convolutional layers conﬁrm particular behaviours wood ﬂowers also present. moreover convolutional layers conv conv look practically datasets. similarity reinforces hypothesis generalist nature convolutional features regardless source target tasks. case fully-connected features behavior analogous group imagenet embedding closest task source task target tasks intersection source task stanforddogs catsdogs display completely opposite activity. however remarkable diﬀerence embeddings second-to-last fully-connected layer divergences contract signiﬁcantly datasets although clear explanation phenomenon hypothesize diﬀerent objects characteristics needed classify holistic classes places cause features layer extremely speciﬁc source task. feature representation transfer studied past performance classiﬁer contributions measure layer performs discriminating classes task originally trained for. contributions know that considered together features composing fully connected layer deﬁne discriminant embedding spaces. contrast contributions purpose paper analyze behavior features layers individually measure relevance knowledge representation. exploring inner/outer class activations feature classes several datasets. conclusions draw study coherent current state-of-the-art novel. next outline features also used describe classes absence thus providing diﬀerent type information modality. particularly relevant ﬁne-grained datasets many common features characteristic certain classes absence novel contribution could useful knowledge representation reasoning purposes either characteristic class irrelevant features rest convolutional layers convey variate information characteristic class presence absence. motivates distinct knowledge extraction approaches depending layer depth criminating corresponding labels. overall results indicate features low-level layers general discriminant originally considered opens door knowledge representations purposes related problems unsupervised learning trained rest target datasets. indicates features layers could used knowledge representation wide variety datasets without ﬁne-tuning. something previously proposed bibliography strongly related similarity task original problem network trained for. however wide spectrum tasks also factor. wide spectrum domains similar source task guarantee indiscriminant fully-connected features eleven datasets evaluated. means that knowledge representation setting class would become indescribable showcasing richness representation language built every layer class distances accounted noise. conservative threshold little variance deﬁned across datasets applying threshold half features embedding remain relevant signiﬁcance feature activations depends dataset used reference. representation data using neural network embeddings consider context able exploit possible modalities information. beyond conclusions work provides methodology identifying relevant features throughout deep cnn. applying approach presented here deﬁne full-network embedding outperforms traditional single-layer embeddings classiﬁcation tasks improves performance multimodal pipelines image caption image retrieval tasks work partially supported joint study agreement ibm/bsc deep learning center agreement spanish government programa severo ochoa spanish ministry science technology tin--p project generalitat catalunya core research evolutional science technology program japan science technology agency", "year": 2017}