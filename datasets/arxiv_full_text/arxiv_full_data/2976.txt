{"title": "Unsupervised Learning of Visual Structure using Predictive Generative  Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "q-bio.NC"], "abstract": "The ability to predict future states of the environment is a central pillar of intelligence. At its core, effective prediction requires an internal model of the world and an understanding of the rules by which the world changes. Here, we explore the internal models developed by deep neural networks trained using a loss based on predicting future frames in synthetic video sequences, using a CNN-LSTM-deCNN framework. We first show that this architecture can achieve excellent performance in visual sequence prediction tasks, including state-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever et al., 2009). Using a weighted mean-squared error and adversarial loss (Goodfellow et al., 2014), the same architecture successfully extrapolates out-of-the-plane rotations of computer-generated faces. Furthermore, despite being trained end-to-end to predict only pixel-level information, our Predictive Generative Networks learn a representation of the latent structure of the underlying three-dimensional objects themselves. Importantly, we find that this representation is naturally tolerant to object transformations, and generalizes well to new tasks, such as classification of static images. Similar models trained solely with a reconstruction loss fail to generalize as effectively. We argue that prediction can serve as a powerful unsupervised loss for learning rich internal representations of high-level object features.", "text": "ability predict future states environment central pillar intelligence. core effective prediction requires internal model world understanding rules world changes. here explore internal models developed deep neural networks trained using loss based predicting future frames synthetic video sequences using cnnlstm-decnn framework. ﬁrst show architecture achieve excellent performance visual sequence prediction tasks including state-of-theart performance standard bouncing balls dataset using weighted mean-squared error adversarial loss architecture successfully extrapolates out-of-the-plane rotations computer-generated faces. furthermore despite trained end-to-end predict pixel-level information predictive generative networks learn representation latent structure underlying three-dimensional objects themselves. importantly representation naturally tolerant object transformations generalizes well tasks classiﬁcation static images. similar models trained solely reconstruction loss fail generalize effectively. argue prediction serve powerful unsupervised loss learning rich internal representations high-level object features. rich literature neuroscience concerning predictive coding idea neuronal systems predict future states world primarily encode deviations predictions predicting future sensory inputs intrinsically useful explore idea prediction might also serve powerful framework unsupervised learning. prediction errors provide source constant feedback successful prediction fundamentally requires robust internal model world. problem prediction estimating conditional distribution given recent data estimate probability future states. much recent success domain within natural language processing relatively low-dimensional real-valued problems motion capture generating realistic samples high dimensional images particularly predicting next frames videos proven much difﬁcult. recently ranzato used close analogy models discretizing image patches dictionary prediction posed predicting index within future time points. approach chosen innate difﬁculty using traditional losses video prediction. pixel space loss functions mean-squared error unstable slight deformations fail capture intuitions image similarity. illustrated srivastava predictive models trained tend react uncertainty blurring. srivastava propose long short-term memory encoder-decoder model next frame prediction despite blurry predictions natural image patches results point potential prediction promising alternative adversarial loss generative adversarial network framework involves training generator discriminator minimax fashion. successful extensions including conditional laplacian pyramid gans show promise useful model generating images. build upon recent advances generative models well classical ideas predictive coding unsupervised temporal learning investigate deep neural networks trained predictive loss. model consisting convolutional neural network lstm deconvolutional neural network falling class encoder-recurrent-decoder architectures model trained end-to-end combine feature representation learning learning temporal dynamics. addition implement adversarial loss demonstrate effectiveness architecture trained standard bouncing balls experiment applying architecture dataset computer-generated faces undergoing rotations. dataset appropriate intermediate step examples full-scale natural images fully study representational learning process. weighted combination leads predictions simultaneously consistent previous frames visually convincing. furthermore course training model becomes better representing latent variables underlying generative model. test generality representation face identiﬁcation task requiring transformation tolerance. classiﬁcation static images model trained predictive loss dynamic stimuli strongly outperforms comparable models trained reconstruction loss static images. thus illustrate promise prediction unsupervised model learning video. addition work already cited current proposal strong roots idea learning temporal continuity. early efforts ﬁeld demonstrated invariances particular transformations learned temporal exposure related algorithms slow feature analysis take advantage persistence latent causes world learn representations robust noisy quickly-varying sensory input. recent work explicitly implemented temporal coherence cost function deep learning architectures enforcing networks develop representation feature vectors consecutive video frames closer together non-consecutive frames related ideas recent paper proposed training models linearize transformations observed sequences frames natural video also related approach especially context rotating objects ﬁeld relational feature learning posits modeling timeseries data learning representations transformations take frame next. recently michalski proposed predictive training scheme transformation ﬁrst inferred frames applied obtain prediction third frame. reported evidence beneﬁt using predictive training versus traditional reconstruction training. finally using variations autoencoders unsupervised learning pre-training certainly fact palm coined term predictive encoder refer autoencoder trained predict future input instead reconstructing current stimuli. preliminary experiments shown model could learn gabor-like ﬁlters training scenarios traditional autoencoders failed learn useful representations. schematic framework shown figure generative model ﬁrst embeds sequence frames successively lower-dimensional feature space using cnn. consisting layers convolution rectiﬁcation max-pooling. output passed lstm network brieﬂy lstm units particular type hidden unit improve upon vanishing gradient problem common training rnns lstm unit contains cell thought memory state. access cell controlled input gate forget gate ﬁnal output lstm unit function cell state output gate version lstm following update equations input lstm network weight matrices elementwise logistic sigmoid function. lstm units bouncing balls dataset rotating faces. models implemented using software package keras upon processing last output lstm hidden state outputted decnn produces predicted image. parameters network chosen predicted image size input. rotating faces dataset decnn consists fullyconnected layer followed layers nearest-neighbor upsampling convolution rectiﬁcation. last layer also contains saturating non-linearity maximum pixel value. lower dimensional size layer decnn omitted model trained bouncing balls dataset. adversarial loss also considered concatenating lstm hidden state random vector passed decnn would allow sampling previous conditional gans however case sampling ended highly peaked different samples input sequence nearly indistinguishable possibly sequences deterministic random vector later discarded. adversarial loss predicted frame generator passed discriminator network also conditioned original input sequence. similar generator input sequence passed lstm. experimented sharing lstm weights discriminator generator ultimately best results network separate parameters. processing last input frame lstm hidden state discriminator concatenated encoding proposed next frame sequence either true frame generator’s output. encoding consists layer resulting feature vector size lstm output. concatenation vectors passed multi-layer perceptron consisting three layers sigmoidal read-out. original formulation adversarial loss function discriminator outputs probability proposed frame came ground truth data. trained maximize probability frame came true distribution minimize produced generator. generator trained fool discriminator. input sequence frames generator evaluated predictive generative networks datasets synthetic video sequences. baseline compare architectures ﬁrst report performance standard bouncing balls paradigm proceed dataset containing out-of-the-plane rotations computer-generated faces thoroughly analyze learned representations. bouncing balls dataset common test models generate high dimensional sequences. consists simulations three balls bouncing box. followed standard procedure create training videos testing videos used additional videos validation. networks trained take variable number frames input selected randomly epoch range output prediction next timestep. training effective dataset used. models optimized using rmsprop learning rate table report average squared one-step-ahead prediction error frame. model compares favorably recently introduced deep temporal sigmoid belief network restricted boltzmann machine variants recurrent temporal structured rtrbm example prediction sequence shown figure prediction made step ahead using previous frames input. rotating faces dataset video consists unique randomly generated face rotating vertical axis random speed initial angle. speed sampled uniformly rad/frame initial angle sampled corresponds frontal generative models often evaluated using parzen window estimate log-likelihood deﬁciencies approach high dimensional images chose values weighting parameter based qualitative assessment. adversarial models notoriously difﬁcult train empirically found beneﬁts giving generator discriminator warm start. generator corresponded initializing solution trained solely mse. analogous increasing value training thus ensuring models learn frequency components ﬁrst high frequency components akin lapgan approach discriminator used pre-training scheme ﬁrst trained generator high value exposes discriminator wide variety stimuli pixel space early training helps quickly discriminate real generated images subsequently paired mse-initialized generator. data shown paper initialization schemes used generator optimized using rmsprop learning rate discriminator trained stochastic gradient descent learning rate momentum example predictions shown figure compare results training weighted al/mse model. predictions faces seen training. models successfully estimate angle basic shape face well. however model produces blurred low-passed versions expected whereas al/mse model generates images high detail. notably al/mse model learned faces contain conspicuous eyes ears largely omitted model. al/mse model make mistakes it’s often generating faces notably look realistic seem slightly inconsistent identity face preceding frames. seen second right panel figure weighting higher exaggerates effect. would hope discriminator would able discern identity changed proposed rotated view interestingly even humans struggle task beyond generating realistic predictions interested understanding representations learned predictive models especially relation underlying generative model. faces created according principal component analysis face space estimated real-world faces. addition principal components remaining latent variables initial angle rotation speed. decoding analysis performed l-regularized regression used estimate latent variables lstm representation. decoded hidden unit responses time steps i.e. last time step hidden representation outputted decnn produce predicted image. regression validated tested using different dataset used train model. baseline compare decoding predictive models model architecture trained precisely stimulus reconstruction loss. here input sequence frames model trained reconstruct last. note model cannot simply copy input must learn dimensional representation lstm dimension size much less input e.g. common autoencoder scenario. decoding results initial angle rotation speed ﬁrst four principal components contained table although produce visually distinct predictions al/mse pgns show similar decoding performance. surprising since dictate shape face model estimates well. nevertheless predictive models strongly outperform autoencoder. sophisticated ways train autoencoders including denoising criteria show that ﬁxed training predictive loss lead better representation underlying generative model reconstruction loss. gain insight learning dynamics show decoding performance hidden state cell state function training epoch model figure epoch corresponds random initial weights latent variables already decoded fairly well expected given empirical evidence theoretical justiﬁcations success random weights neural networks still clear ability estimate latent variables increases training. model quickly peaks ability linearly encode speed initial angle. learned slowly decoding accuracy actually ﬁrst decreasing speed angle rapidly learned. sequence model learns reminiscent theoretical work supporting notion modes dataset learned coarse-to-ﬁne fashion understand representational changes accompanying increase decoding performance provide visualizations hidden unit feature space training figures figure contains multidimensional-scaling plot initial random weights weights epoch trained mse. points colored value rotation speed. although regression feature space epoch produces apparent structure space changes training. clear understanding changes linearized feature space dimensions axes pointing direction regression coefﬁcients decoding rotation speed. points held-out projected axes plotted figure show evolution projection space regression coefﬁcients calculated separately epoch. training points become spread manifold. overall increase feature vector length increase training. thus training variance feature vectors become aligned latent variables generative model. previous analyses suggest pgns learn dimensional linear representation face space. illustrated figure here feature representation given seed face calculated feature vector perturbed direction principal component axis estimated decoding analysis. feature vector passed pre-trained decnn produce image. generated image compared changing value directly face generation software. figure shows extrapolations produce realistic images especially al/mse model correlate underlying model. dimensions precisely semantic meanings differences especially noticed cheeks lines. linear extrapolations feature space generally match changes features demonstrating models learned representation latent variables linear. generation frame-by-frame future predictions useful especially interested extent prediction could used unsupervised loss learning representations suited tasks. tested hypothesis task completely orthogonal original loss function namely classiﬁcation static images. control speciﬁcally isolate effect loss itself trained comparable models using reconstruction loss either dynamic static stimuli. ﬁrst control carried latent variable decoding analysis architecture training trained reconstruction loss fig. next model architecture reconstruction loss trained static videos video created unique frame original training set. last models lstm replaced fully-connected layer equal number weights equal number units lstm. trained traditional autoencoder fashion simply reconstruct single frames using every frame original video set. control models trained since sensitive hyperparameters. classiﬁcation dataset consisted randomly generated faces equally-spaced angles support vector machine feature representations model. models containing lstm layer feature representation ﬁfth time step chosen. test transformation tolerance training testing done separate sets angles. classiﬁcation performance curves shown figure models show improvement compared random initial weights predictive models outperform controls. highest score size training data. al/mse performs moderately worse still better control models. likely because previously mentioned al/mse model tend produce realistic faces somewhat unfaithful underlying identity. relatively simple task compared modern machine learning datasets provides proof-of-principle model trained unsupervised predictive loss dynamic sequences learn interesting structure even useful tasks. extending ideas predictive coding learning temporal continuity modern deep learning architectures shown unsupervised predictive loss result rich internal representation visual objects. cnn-lstm-decnn models trained loss function successfully learn predict future image frames several contexts ranging physics simulated bouncing balls out-of-plane rotations previously unseen computer-generated faces. however importantly models trained predictive unsupervised loss also well-suited tasks beyond domain video sequences. instance representations trained predictive loss outperform models comparable complexity supervised classiﬁcation problem static images. effect particularly pronounced regime classiﬁer must operate example views object taken together results support idea prediction serve powerful framework developing transformationtolerant object representations sort needed support onefew-shot learning. experiments presented done context highly-simpliﬁed artiﬁcial worlds underlying generative model stimuli known number degrees freedom data few. extending experiments real world imagery obvious future priority nonetheless argue experiments highly controlled stimuli hold potential yield powerful guiding insights. understanding scale predictive generative models form encompass transformation degrees freedom found real-world objects area great interest future research. would like thank chuan-yung tsai fruitful discussions. work supported grant national science foundation center brains minds machines funded award ccf-. pinto nicolas doukhan david dicarlo james david high-throughput screening approach discovering good forms biologically inspired visual representation. plos comput biol ranzato marc’aurelio szlam arthur bruna joan mathieu micha¨el collobert ronan chopra sumit. video modeling baseline generative models natural videos. corr abs/. saxe andrew bhand maneesh chen zhenghao pang suresh bipin andrew random weights unsupervised feature learning. workshop deep learning unsupervised feature learning summerﬁeld christopher egner tobias greene matthew koechlin etienne mangels jennifer hirsch joy. predictive codes forthcoming perception frontal cortex. science taylor graham hinton geoffrey. factored conditional restricted boltzmann machines modeling motion style. proceedings international conference machine learning zhao mingmin zhuang chengxu wang yizhou sing. predictive encoding contextual relationships perceptual inference interpolation prediction. corr abs/.", "year": 2015}