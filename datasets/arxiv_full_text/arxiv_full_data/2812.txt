{"title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With  Expert Demonstrations", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Pretraining with expert demonstrations have been found useful in speeding up the training process of deep reinforcement learning algorithms since less online simulation data is required. Some people use supervised learning to speed up the process of feature learning, others pretrain the policies by imitating expert demonstrations. However, these methods are unstable and not suitable for actor-critic reinforcement learning algorithms. Also, some existing methods rely on the global optimum assumption, which is not true in most scenarios. In this paper, we employ expert demonstrations in a actor-critic reinforcement learning framework, and meanwhile ensure that the performance is not affected by the fact that expert demonstrations are not global optimal. We theoretically derive a method for computing policy gradients and value estimators with only expert demonstrations. Our method is theoretically plausible for actor-critic reinforcement learning algorithms that pretrains both policy and value functions. We apply our method to two of the typical actor-critic reinforcement learning algorithms, DDPG and ACER, and demonstrate with experiments that our method not only outperforms the RL algorithms without pretraining process, but also is more simulation efficient.", "text": "however enormous number online simulation data required deep reinforcement learning. hence attempt learn expert demonstrations decrease amount online data required deep reinforcement learning algorithms. representative method learning expert demonstrations inverse reinforcement learning. proposed ﬁrst inverse reinforcement learning algorithm recovers reward function based assumption expert policy global optimal policy. recovered reward function abbeel able propose apprenticeship learning train policy expert demonstrations simulation environment output reward. apprenticeship learning inspired many similar algorithms proposed imitation learning method merges inverse reinforcement learning reinforcement learning hence imitate expert demonstrations generative adversarial networks algorithms proved successful solving mdp\\r however mdp\\r different original since mdp\\r environments output task based reward data. reason inverse reinforcement based algorithms attempt assume expert demonstrations global optimal imitate expert demonstrations. order learn expert demonstrations alongside state-of-the-art reinforcement learning algorithms different frameworks required. prior work attempt make expert demonstrations reinforcement learning algorithms. lakshminarayanan proposed training method based assumption expert demonstrations global optimal thus pretrain state-action value function estimators. cruz focused feature extracting high dimensional especially image based simulation environments proposed framework discrete control problems pretrains neural networks classiﬁcation tasks using supervised learning. purpose pretraining process speed training process trypretraining expert demonstrations found useful speeding training process deep reinforcement learning algorithms since less online simulation data required. people supervised learning speed process feature learning others pretrain policies imitating expert demonstrations. however methods unstable suitable actor-critic reinforcement learning algorithms. also existing methods rely global optimum assumption true scenarios. paper employ expert demonstrations actor-critic reinforcement learning framework meanwhile ensure performance affected fact expert demonstrations global optimal. theoretically derive method computing policy gradients value estimators expert demonstrations. method theoretically plausible actor-critic reinforcement learning algorithms pretrains policy value functions. apply method typical actor-critic reinforcement learning algorithms ddpg acer demonstrate experiments method outperforms algorithms without pretraining process also simulation efﬁcient. introduction deep reinforcement learning general method successful solving complex control problems. mnih combined learning deep neural networks proved successful image based atari games. policy gradient methods proved signiﬁcantly efﬁcient continuous control problems discrete control problems among policy gradient methods actor-critic algorithms heart many signiﬁcant advances reinforcement learning algorithms estimate extract features high dimensional states. however work suitable image based discrete action environments ignored fact expert demonstrations perform better current learned policies. ﬁrst published version alphago important work pretrains neural networks human expert demonstrations. work policy network value network used. value network trained on-policy reinforcement learning policy network pretrained expert demonstrations using supervised learning trained policy gradient. work quite similar role expert demonstrations speed feature extraction give policy warm start. fact expert demonstrations perform better fully used framework extensive enough problems reinforcement learning algorithms. paper propose extensive framework pretrains actor-critic reinforcement learning algorithms expert demonstrations expert demonstrations policy functions value estimators. theoretically derive method computing policy gradient value estimators expert demonstrations. experiments show method improves performance baseline algorithms continuous control environments high-dimensionalstate discrete control environments. background preliminaries paper deal inﬁnite-horizon discounted markov decision process deﬁned tuple tuple ﬁnite states ﬁnite actions transition probability distribution reward function probability distribution initial state discount factor. stochastic policy returns probability distribution actions based states deterministic policy returns action based states. paper deal stochastic policies deterministic policies means respectively. thus state-action value function qπis goal actor-critic reinforcement learning algorithms maximize discounted reward obtain optimal policy parameterized policy estimating based simulated samples many algorithms state-action value estimator estimate state-value function policy function another off-policy algorithm estimator policy acer optimizes stochastic policy. algorithm maximizes off-policy deterministic discounted reward well modiﬁes off-policy policy gradient ˆgacer ∇θηβ deﬁnition perform better based fact goal actor-critic algorithms maximize expert policy different imitation learning since optimum policy mdps. pairs {}t=... sampled actor-critic algorithms tend optimize target. thus pretraining procedures algorithms need estimate optimization target using expert demonstrations. also deﬁnition need estimate well. however demonstrations expert policy black-box simulation environment cannot directly estimated. hence introduce theorem theorem policies many actor-critic algorithms like ddpg acer policy optimization based accurate estimations state-action value functions value functions learned policy typically algorithms data sampled {}t=... estimate estimating processes usually need large amount simulations accurate enough. result links state-action value functions expert demonstrations allowing apply constraint training state-action value functions. constraint value estimators like value estimators accurate enough constraint would satisﬁed. hence algorithm update value estimators constraint estimators would accurate result improve policy optimizing process. another pretraining process policy optimization using expert demonstrations. like actor-critic algorithms suppose advantage function already known conducting policy optimization. estimate update step expert demonstrations estimations value functions. recently people like propose sample efﬁcient algorithms like acer q-prop since algorithms need large amount simulation time training. expert demonstrations since reward data cannot conduct sample efﬁcient policy optimization processes. however update policies simulation time needed. call situation simulation efﬁcient means algorithms need large amount data need simulation data training. note sample efﬁcient algorithms simulation efﬁcient algorithms methods intend decrease simulation time. paper evaluate method simulation efﬁcient section found pretraining methods actorcritic algorithms namely based theorem theorem connects policy discounted reward expert demonstration data requiring reward data expert trajectories. equation gives constraint value function estimators based deﬁnition perform better equation provides offpolicy method optimize policy function regardless expert demonstrations perform. algorithms expert demonstrations theorem provides satisfy constraint update policies demonstrations expert policy need reward data sampled section organize results section piratical apply pretraining methods typical actorcritic algorithms ddpg acer. pretraining processes based theorem need estimator advantage function policy based parameterized policy state-action value function estimator obtain advantage function estimator considering training processes ddpg acer beginning processes policies nearly random estimators accurate since little data simulation. therefore exist expert demonstrations perform better initial policies introduce data using constraint order obtain accurate estimator accurate enough fact performs better. hence update estimator expert demonstrations following gradient otherwise zero equation optimize policy expert demonstrations. since expert demonstrations contain reward data update policy parameters simple policy gradient reason optimal policy mdps train expert demonstrations limited period time beginning training process guarantee performs better hence call process pretraining. acer gradients algorithms original gradients baseline actorcritic algorithms andgpre pretraining gradients estimator parameterized policy function respectively pretraining. introduce expert demonstrations base algorithms instead replacing them since state-action value functions estimated baseline algorithms gradient makes satisfy constraint ddpg representative off-policy actor-critic deterministic algorithm. algorithm continuous action space mdps optimizes policy using off-policy policy gradient. neural networks used ddpg time. named critic network state-action value function estimator named actor network parameterized policy since algorithm deterministic control input actor network state mdps output corresponding action. acer off-policy actor-critic stochastic algorithm modiﬁes policy gradient make process sample efﬁcient. acer solves discrete control problems continuous control problems. discrete control problems double-output convolutional neural work used acer. output softmax policy values. although share parameters updated separately different gradients. stochastic control problems structure named stochastic dueling networks used value function estimation. network outputs deterministic value estimation stochastic state-action value estimation ˙a∼πθ. hence equation becomes hopper walkerd vertical dashed black lines represent points pretraining horizontal dashed brown lines represent average episode reward expert demonstrations. transparent blue lines original training results opaque lines smoothed lines sliding windows. experiments test algorithms based ddpg acer various environments order investigate simulation efﬁcient pretraining methods are. baselines ddpg acer without pretraining. ddpg baseline apply algorithm dimensional simulation environments using mujoco physics engine test tasks action dimensionality halfcheetah hopper walkerd tasks illustrated figure setups ddpg baseline share network architecture compute policies estimate value functions referring adam used learning parameters learning rate actor network critic network respectively critic network weight decay used actor network critic network hidden layers units respectively. results pretraining method based ddpg illustrated figure ﬁgures horizontal dashed brown lines represent average episode reward expert demonstrations. obvious expert demonstrations global optimal demonstrations order guarantee expert policies perform better learned policies pretraining process stops early training steps simulation steps. shown figure obvious ddpg pretraining method outperforms initial ddpg. results halfcheetah representative clear pretraining process gives training warm start pretraining stops performance drops learning gradient. however pretraining ddpg learns faster baseline hence outperforms initial ddpg. although results ddpg unstable hopper walkerd smoothed results figure example screenshots atari simulation environments attend experiment acer baseline. tasks left right airraid breakout carnival crazyclimber gopher. acer baseline apply algorithm image based atari games. tested discrete control problems acer environments tested airraid breakout carnival crazyclimber gopher. environments illustrated figure experiment settings similar double-output network consists convolutional layer kernels stride convolutional layer kernels stride convolutional layer kernels stride followed fully connected layer units. network outputs softmax policy state-action value every action. limitation memory thread acer replay memory frames different setting entropy regularization weight also adopted discount factor importance weight truncation trust region updating used described settings trust region update remain same. acer without trust region update tested paper. results pretraining method based acer trust region update illustrated figure environments image based atari games. lines meaning figure obvious acer pretraining process outperforms initial acer. figure results pretraining based acer trust region update. similar figure vertical dashed black lines points pretraining horizontal dashed brown lines average episode reward expert demonstrations. transparent blue lines represent original training results opaque ones smoothed results sliding windows. stochastic discrete control random policy random state-action value estimator always satisﬁes constraint hence deﬁned policy gradient based expert demonstrations similar original baseline acer therefore performance learned policies fall pretraining. conclusion work propose extensive method pretrains actor-critic reinforcement learning methods. based theorem design method takes advantage expert demonstrations. method rely global optimal assumption expert demonstrations differences method algorithms. method pretrains policy function state-action value estimators simultaneously gradients experiments based ddpg acer demonstrate method outperforms algorithms. limitation framework estimate advantage function expert demonstrations framework suitable algorithms like trpo maintain value estimator hand fact expert demonstrations perform better considered pretraining policies left references pieter abbeel andrew apprenticeship learning inverse reinforcement learning. proceedings twenty-ﬁrst international conference machine learning. gabriel cruz yunshu matthew taylor. pre-training neural networks human demonstrations deep reinforcement learning. technical report september thomas degris martha white richard sutton. off-policy actor-critic.pdf. icml shixiang timothy lillicrap zoubin ghahramani richard turner sergey levine. qprop sample-efﬁcient policy gradient policy critic. iclr bilal piot matthieu geist olivier pietquin. boosted bellman residual minimization hanjoint european conferdling expert demonstrations. ence machine learning knowledge discovery databases pages springer david silver lever nicolas heess thomas degris daan wierstra martin riedmiller. deterministic policy gradient algorithms. proceedings international conference machine learning pages david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature richard sutton david mcallester satinder singh yishay mansour. policy gradient methods reinforcement learning function approximation. advances neural information processing systems pages emanuel todorov erez yuval tassa. mujoco physics engine model-based control. intelligent robots systems ieee/rsj international conference pages ieee ziyu wang victor bapst nicolas heess volodymyr mnih remi munos koray kavukcuoglu nando freitas. sample efﬁcient actor-critic experience replay. arxiv preprint arxiv. nicolas heess gregory wayne david silver lillicrap erez yuval tassa. learning continuous control policies stochastic value gradients. advances neural information processing systems pages todd hester matej vecerik olivier pietquin marc lanctot schaul bilal piot andrew sendonaris gabriel dulac-arnold osband john agapiou learning demonstrations real world reinforcement learning. arxiv preprint arxiv. sham kakade john langford. approximately optimal approximate reinforcement learning. proceedings international conference machine learning pages diederik kingma jimmy arxiv lakshminarayanan sherjil ozair yoshua bengio. reinforcement learning expert demonstrations. neural information processing systems workshop deep learning action interaction timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa continuous condavid silver daan wierstra. arxiv preprint trol deep reinforcement learning. arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning pages", "year": 2018}