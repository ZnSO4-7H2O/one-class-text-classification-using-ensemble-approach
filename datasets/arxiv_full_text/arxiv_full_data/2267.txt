{"title": "Markov Decision Processes with Continuous Side Information", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider a reinforcement learning (RL) setting in which the agent interacts with a sequence of episodic MDPs. At the start of each episode the agent has access to some side-information or context that determines the dynamics of the MDP for that episode. Our setting is motivated by applications in healthcare where baseline measurements of a patient at the start of a treatment episode form the context that may provide information about how the patient might respond to treatment decisions. We propose algorithms for learning in such Contextual Markov Decision Processes (CMDPs) under an assumption that the unobserved MDP parameters vary smoothly with the observed context. We also give lower and upper PAC bounds under the smoothness assumption. Because our lower bound has an exponential dependence on the dimension, we consider a tractable linear setting where the context is used to create linear combinations of a finite set of MDPs. For the linear setting, we give a PAC learning algorithm based on KWIK learning techniques.", "text": "consider reinforcement learning setting agent interacts sequence episodic mdps. start episode agent access side-information context determines dynamics episode. setting motivated applications healthcare baseline measurements patient start treatment episode form context provide information patient might respond treatment decisions. propose algorithms learning contextual markov decision processes assumption unobserved parameters vary smoothly observed context. also give lower upper bounds smoothness assumption. lower bound exponential dependence dimension consider tractable linear setting context used create linear combinations ﬁnite mdps. linear setting give learning algorithm based kwik learning techniques. consider basic sequential decision making problem healthcare namely learning treatment policy patients optimize health outcome interest. could model interaction every patient markov decision process precision personalized medicine want treatment personalized every patient. time amount data available given patient enough personalize well. means modeling patient diﬀerent result severely suboptimal treatment policies. extreme pooling patients’ data results data perhaps relevant patient currently want treat. therefore face trade-oﬀ large amount shared data learn single policy ﬁnding relevant policy patient. similar trade-oﬀ occurs applications involving humans agents environment online tutoring advertising. observation many personalized decision making scenarios side information available individuals might help designing personalized policies also help pool interaction data across right subsets individuals. examples data include laboratory data medical history patients healthcare user proﬁles history logs advertising student proﬁles historical scores online tutoring. access side information learn better policies even limited amount interaction individual users. refer side-information contexts adopt augmented model called contextual markov decision process proposed hallak assume contexts fully observed available interaction starts mdp. paper study sample complexity learning cmdps worst case. consider concrete settings learning cmdp continuous contexts. ﬁrst setting individual mdps vary arbitrary smooth manner contexts propose cover-rmax algorithm section bounds. innate hardness learning general case captured lower bound construction section show possible achieve signiﬁcantly better sample complexity structured cmdps consider another setting contexts used create linear combinations ﬁnite ﬁxed unknown mdps. kwik framework devise kwik lr-rmax algorithm section also provide upper bound algorithm. consider case ﬁxed horizon episodic mdps. denote policy’s action state timestep episode initial state observed according distribution afterwards agent chooses action according policy. reward next state according reward transition functions. policy deﬁne value follows throughout paper rewards bounded denote |s||a| respectively. also assume context space bounded norm upper bounded constant. consider online learning scenario following protocol make distributional assumptions context sequence. instead allow sequence chosen arbitrary potentially adversarial manner. natural criteria judging eﬃciency algorithm look number episodes performs sub-optimally. main analysis bound number episodes \u0001-optimal although give bounds coverrmax algorithm given below reader make note that made explicit attempts achieve tightest possible result. rmax algorithm base construction handle exploration-exploitation simplicity. approach also combined algorithms improved dependence section present cover-rmax algorithm provide bound smoothness assumption. motivation contextual setting sharing information among diﬀerent contexts helpful. therefore natural assume mdps corresponding similar contexts similar. formalized following smoothness assumption assume distance metric constants known. smoothness assumption allows minimally tweaked version rmax provide analysis smooth cmdps similar existing literature mdps. know transition dynamics expected reward functions state-action pair ﬁnite easily compute optimal policy. idea rmax distinguish state-action pairs known unknown state-action pair known visited enough number times empirical estimates reward transition probabilities near-accurate suﬃcient data. state becomes known actions pairs become known. rmax constructs auxiliary encourages optimistic behaviour assigning maximum reward remaining unknown states. according optimal policy auxiliary following must happen exploit information available achieve near-optimal value visit unknown states accumulate information eﬃciently. formally known states deﬁne induced following manner. denote number observations state-action pair transitions respectively. also denote total reward obtained state-action pair deﬁne values certainty equivalent policy computed induced perform balanced wandering unknown states. balanced wandering ensures actions tried equally fairly unknown states. assigning maximum reward unknown states pushes agent visit states provides necessary exploration impetus. generic template rmax given algorithm bias introduced ignoring diﬀerences among mdps ball. allows pool together data mdps ball avoid diﬃculty inﬁnite mdps instead deal ﬁnitely many them. size cover i.e. number balls measured notion covering numbers deﬁned instead learning model contextual separately combine data within ball. therefore take care things choose radius enough cover value number visits state becomes known ball. satisfying conditions lemma mdps within observe bound linear dependence covering number case d-dimensional euclidean metric space covering number however show section that dependence would prove lower bound number sub-optimal episodes learning algorithm smooth cmdp shows linear dependence covering number context space unavoidable. know existing constructing lower bounds continuous state spaces smoothness cannot simply augment state representation include context information. instead prove lower bound theorem builds upon work dann brunskill lower bounds episodic ﬁnite mdps slivkins lower bounds contextual bandits. proof. overall idea embed multiple learning problems cmdp agent learn optimal policy separately cannot generalize across them. show maximum number problems embedded scales states absorbing rewards respectively. states reward actions. state essentially acts hard bandit instance whose actions move randomly. action satisﬁes action satisﬁes discuss populate context space hard mdps. note figure that agent know action rewarding adversary choose element scenario would like allow adversary choose independently individual packing point yield lower bound linear packing number. however always possible smoothness assumption committing point restrict adversary’s choices another point. deal diﬃculty note pair hard mdps diﬀer related covering number radius chosen arbitrary choices hard instances diﬀerent packing points always satisfy smoothness assumption mdps specify follows state action choose context sequence given input repetitions arbitrary permutation construction learning diﬀerent points independent lower bound simply lower bound learning single multiplied cardinality using well known relation previous section clear contextual smoothness assumptions exponential dependence context dimension unavoidable. further computational requirements cover-rmax algorithm scales covering number context space. such section focus structured assumption mapping context space mdps show achieve substantially improved sample eﬃciency. shorthand vectors concatenate parameters diﬀerent base mdps parameters base mdps unknown need recovered data learning reward function vary context hence reward smoothness satisﬁed agent combination coeﬃcients directly available context vector itself. assumption motivated application scenario user/patient responds according characteristic distribution possible behavioural patterns. model estimation recall section treat mdps whose contexts fall small ball single estimate parameters using data local context ball. section however global structure parametric assumption implies data obtained context useful learning parameters another context away avoid exponential dependence need leverage structure generalize globally across entire context space. next-state snext drawn therefore indicator whether snext equal forms unbiased estimate pc|s i.e. esnext∼pc pc|s based observation construct feature-label pair whenever observe transition tuple context relationship governed linear prediction rule coeﬃcients. hence estimate data simply collect feature-label pairs correspond particular tuple linear regression recover coeﬃcients. observe however matrix ill-conditioned contexts subspace spanned previously observed contexts make accurate predictions despite inability recover model parameters. online linear regression procedure take care issue choose kwik procedure. original kwik deals scalar labels used decide whether need predict pc|s individual accurately estimate close true distribution error pair already provide tighter error bounds treating whole. introduce identifying known kwik kwik lr-rmax algorithm propose linear setting still uses rmax template exploration every episode build induced greedily according optimal policy balanced wandering. major diﬀerence cover-rmax lies known states identiﬁed constructed explain high level algorithm works following constructing query kwik procedure estimates every pair using redict. kwik procedure either returns returns estimates guaranteed accurate. returned consider unknown associate rmax reward exploration. optimistic exploration ensures signiﬁcant probability observing pairs predicted observe algorithm initialize matrices using initialize update time. design matrix episode context observed episode matrix inverse rules verify update rule line essentially yields value episode inverse empirical covariance matrix plays central role linear regression analysis. matrix accumulates outer product feature vector predetermined threshold recall inverse covariance matrix small implies estimate qtwt close along direction otherwise return predict kwik subroutine rewards similar hence omitted. ensure estimated transition probability valid project estimated vector onto done eﬃciently using existing techniques state kwik bound learning transition function; kwik bound learning rewards much smaller hence omitted here. kwik bound scalar linear regression walsh property multinomial samples kwik bound. kwik algorithm executed probability vectors min{b suitable constants number updates take place bounded o)}) probability least non-⊥ prediction returned ˆpct conceptually view algorithm running scalar linear regression simultaneously projects vector label scalar ﬁxed linear transformation require every scalar regressor kwik guarantee error guarantee vector label follows union bound. states ﬁxed number updates unknown pair parameters always known desired accuracy. number updates obtained setting desired accuracy transitions failure probability δ/sa theorem lemma instead updating counts number visits look number updates unknown pairs. applying union bound state action pairs using lemma easy sub-optimal episodes transfer latent contexts general deﬁnition cmdps captures problem transfer multi-task taylor stone lazaric surveys empirical results. recent papers also advanced theoretical understanding transfer instance brunskill hallak analyzed sample complexity cmdps element ﬁnite small mdps label treated latent context. mahmud consider problem transferring optimal policies large known mdps mdp. commonality papers label observed. hence methods initially explore every identify label requires episode length substantially longer planning horizon. problematic assumption motivating scenarios interact patient user student limited period time data single episode enough identifying underlying mdp. contrast prior work propose leverage observable context information perform direct transfer previous mdps algorithm works arbitrary episode length side information work leverages available side-information inspired contexts contextual bandits side information also found literature ammar developed multi-task policy gradient method context used transferring knowledge tasks; killian used parametric forms mdps develop models personalized medicine policies treatment. metric space smooth cmdps pool observations across similar contexts reduce problem learning policies ﬁnitely many mdps. alternative approach consider inﬁnite whose state representation augmented context apply pac-mdp methods metric state spaces however might increase sample computational complexity unnecessarily longer leverage structure particular component state namely context remains episode. concretely augmenting approach needs perform planning augmented states contexts makes computational/storage requirement worse addition allow context acteristics dependence context space. sequence chosen adversarial manner. corresponds adversarially chosen initial states mdps usually handled pac-mdp methods. kwik learning linear hypothesis classes linear combination setting provides instance parametric assumptions lead substantially improved bounds. build upon kwik-rmax learning framework developed previous work kwik linear regression sub-routine. resulting kwik lr-rmax algorithm sample complexity bound inherently depends kwik bound linear regression. well known even linear hypothesis classes kwik bound exponential input dimension agnostic case therefore success algorithm relies validity modelling assumption. abbasi-yadkori studied problem similar linear combination setting proposed no-regret algorithm combining ucrl conﬁdence techniques stochastic linear optimization literature work takes independent diﬀerent approach provide guarantee directly comparable regret bound. still observe dependence optimal whereas hand dependence optimal dependence counterpart analysis suboptimal. interesting future direction combine algorithmic ideas papers improve guarantees. paper present general setting using side information learning near-optimal policies large potentially inﬁnite number mdps. proposed cover-rmax algorithm model-based pac-exploration algorithm case mdps vary smoothly respect observed side information. lower bound construction indicates necessary exponential dependence algorithm context dimension smooth cmdp. also consider another instance parametric assumption using kwik linear regression procedure present kwik lr-rmax algorithm efﬁcient exploration linear combination mdps. analysis shows signiﬁcant improvement structural assumption. context based modelling multiple tasks rich application possibilities personalized recommendations healthcare treatment policies tutoring systems. believe setting possibly extended cover large space multi-task quite well ﬁnite/inﬁnite number mdps observed/latent contexts deterministic/noisy mapping context environment. hope work spurs research along directions. work supported part grant open philanthropy project center human-compatible part grant ambuj tewari acknowledges support grant career iis- sloan research fellowship. opinions ﬁndings conclusions recommendations expressed authors necessarily reﬂect views sponsors.", "year": 2017}