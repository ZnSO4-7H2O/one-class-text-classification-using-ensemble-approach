{"title": "Denoising autoencoder with modulated lateral connections learns  invariant representations of natural images", "tag": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "abstract": "Suitable lateral connections between encoder and decoder are shown to allow higher layers of a denoising autoencoder (dAE) to focus on invariant representations. In regular autoencoders, detailed information needs to be carried through the highest layers but lateral connections from encoder to decoder relieve this pressure. It is shown that abstract invariant features can be translated to detailed reconstructions when invariant features are allowed to modulate the strength of the lateral connection. Three dAE structures with modulated and additive lateral connections, and without lateral connections were compared in experiments using real-world images. The experiments verify that adding modulated lateral connections to the model 1) improves the accuracy of the probability model for inputs, as measured by denoising performance; 2) results in representations whose degree of invariance grows faster towards the higher layers; and 3) supports the formation of diverse invariant poolings.", "text": "suitable lateral connections encoder decoder shown allow higher layers denoising autoencoder focus invariant representations. regular autoencoders detailed information needs carried highest layers lateral connections encoder decoder relieve pressure. shown abstract invariant features translated detailed reconstructions invariant features allowed modulate strength lateral connection. three structures modulated additive lateral connections without lateral connections compared experiments using real-world images. experiments verify adding modulated lateral connections model improves accuracy probability model inputs measured denoising performance; results representations whose degree invariance grows faster towards higher layers; supports formation diverse invariant poolings. denoising autoencoders provide easily accessible method unsupervised learning representations since training based simple back-propagation quadratic error function. autoencoder built mappings encoder maps corrupted input data features decoder maps features back denoised data output. thus basic form autoencoders need store details input representation. deep learning used unsupervised pretraining recently purely supervised learning become dominant approach least cases large number labeled data available difﬁculty combining autoencoders supervised learning autoencoders retain information whereas supervised learning typically loses some. instance classiﬁcation images spatial pooling activations throws away location details retaining identity details. sense unsupervised supervised training pulling model different directions. theoretical perspective clear unsupervised learning must helpful least semi-supervised setting. indeed kingma obtained promising results variational autoencoders. raises hopes could achieved simpler daes. recently valpola proposed variant denoising autoencoder lose information. novelty lateral connections allow higher levels autoencoder focus invariant abstract features layer-wise cost function terms allow network learn deep figure examples two-hidden-layer models denoising autoencoder ladder network lateral connections. illustration ratio size size hidden layers sizes ratio corresponds hidden layers equal size hierarchies efﬁciently. valpola hypothesized modulated lateral connections support development invariant features provided initial results artiﬁcial data back idea. seen figure information input output alternative routes details longer need stored abstract representation. step closer compatible supervised learning select types invariances abstractions relevant task hand. paper focus investigating effects lateral connections. extend earlier results experiments using natural image data make comparisons regular denoising autoencoders without lateral connections section show following proposed structure attains better model data measured ability denoise. good reasons believe indicates network captured accurate probabilistic model data since denoising representing distributions degree invariance representations grows towards higher levels tested models much faster modulated lateral connections. particular higher levels model seem focus entirely invariant representations whereas higher levels regular autoencoder invariant features mixed large number details. modulated lateral connections guide layer learn various types poolings. pooled neurons participate several qualitatively different poolings selective invariant different aspects input. typically many sources variation irrelevant classiﬁcation. example object recognition images sources could include position orientation scale recognized object illumination conditions. order correctly classify samples sources variation disregarded retaining information needed discriminating different classes. words classiﬁcation needs invariant irrelevant transformations. simple naive achieve invariance would list possible realizations objects various transformations usually practical vast amounts possible realizations. also offer generalization objects. useful solution split generation invariance manageable parts representing inputs terms features invariant types transformations. since different kinds objects represented features approach makes possible generalize invariances unseen objects. rather listing possible realizations individual objects list possible realizations constituent features. invariance achieved retaining information whether possible realizations present discarding information exactly. operation known pooling various ways implementing case binary inputs operation natural choice. many ways generalize continuous variables including maximum operation summation followed concave function pooling achieves invariance irrelevant transformations features also need discriminative along relevant dimensions. selectivity often achieved coincidence detection i.e. detecting simultaneous presence multiple input features. binary case implemented operation. continuous case possibilities include extensions operation product summation followed convex function also lateral inhibition among feature detectors. operations typically produce sparse coding output features sensitive speciﬁc combinations input features. since type coding tends increase number features also known feature expansion. idea alternating pooling feature expansion dates least back hubel wiesel found early stages visual processing cerebral cortex alternating steps feature expansion implemented lateral competition among called simple cells invariance-generating poolings called complex cells. hierarchies alternating steps degree invariance grows towards higher levels. cortical processing also includes various normalizations feature also included models invariance design. instance invariance translation small deformations achieved pooling shifted versions feature similar pooling operations popular convolutional neural networks invariance hand-crafted transformations. transformations applied input samples pooling learned requiring output stay constant transformation. category includes supervised learning inputs deformed various transformations. invariance exploiting higher-order correlations within individual samples. supervised learning poolings target labels correlate nonlinearly inputs. also unsupervised methods same. example subspace complex-cell like poolings natural images focus last type exploiting higher-order correlations. assumptions made method general also possible combine approach supervised learning specialized ways generating invariances. autoencoder networks natural propensity conserve information therefore well suited feature expansion. autoencoder networks consist parameterized functions encoding decoding function maps input space feature space turn maps back input space producing reconstruction original input training criterion minimize reconstruction error. enables learning features unsupervised manner. denoising autoencoder variant traditional autoencoder input corrupted noise objective network reconstruct original uncorrupted input corrupted bengio show denoising autoencoders implicitly estimate data distribution asymptotic distribution markov chain alternates corruption denoising. interpretation provides solid probabilistic foundation them. consequently denoising teaching criterion enables learning over-complete representations property crucial adding lateral connections autoencoder. denoising autoencoders used build deep architectures either stacking several training greedy layer-wise manner chaining several encoding decoding functions training layers simultaneously. layers encoding functions encoding path would compose denote intermediate feature vectors corresponding decoded denoised vectors figure depicts structure encoding functions form tendency regular autoencoders preserve information seems odds development invariant features relies poolings selectively discard types information. goal investigate hypothesis suitable lateral connections allow autoencoders discard details higher layers retain abstract invariant features decoding functions recover discarded details encoder. compare three different denoising autoencoder structures basic denoising autoencoder variants lateral connections. experimented various model deﬁnitions prior deciding ones deﬁned section multiple ways merge lateral connections. lateral connections seen figure autoencoders trained without noise would short-circuit input output identity mapping. input contains noise pressure meaningful higher-level representations capture regularities allow denoising. note encoding function form element-wise product learnable parameter vectors along weights biases sigmoid function ensure modulation stays within reasonable bounds. function stays afﬁne functional form element-wise decoding motivated element-wise denoising functions used denoising source separation corresponds assuming elements independent priori. hypothesis autoencoder learn invariances efﬁciently decoder make good them. valpola proposed connecting top-down mapping inside sigmoid term choice motivated optimal denoising hierarchical variance models. contrast additive lateral connection signal abstract layer used modulate lateral connection top-down connection moved bias modulated connections used autoencoders rather different context. memisevic uses weight tensor connect inputs feature. connect component component keeping number additional parameters small. order compare models optimized model structure constraining million parameters million mini-batch updates best denoising performance lowest reconstruction cost better denoising performs better implicit probabilistic model tasks models comparison fair. two-layer models focus optimal size layers especially ratio size size models rectiﬁed linear unit activation function noise gaussian zero mean standard deviation scaled data. order best possible baseline comparison evaluated weight tying autoencoder without lateral connections noticed tying weights resulted faster convergence thus better denoising performance without weight tying. however combining weights tied beginning untied latter half training denoising performance improved slightly affect relative difference various models. since weight tying negligible impact results complicates training reproducibility designed experiments models tied weights counting tied weights separate parameters. results denoising performance described section performed analysis natural images invariances learned features easy visualize know beforehand invariances exist computer vision important application ﬁeld right. used patches image datasets cifar- natural images used olshausen field refer dataset o&f. training length limited million mini-batch updates mini-batch size learning rate adapted adadelta best variants model trained longer million updates analyzed determine invariance learned representations. described reported section supplementary material provides details data preprocessing division training validation sets training procedure hyperparameters used adadelta weights initialized. also tried stacked layer-wise training training layer updates followed ﬁnetuning phase another updates total number updates parameter equals million. also tried local cost function noise source layer using global cost stacked training beneﬁcial compared direct simultaneous training layers report paper. figure best validation cost element function ratio size size cifar- datasets. dotted line result linear denoising dashed lines represent denoising performance one-layer models without lateral connections according colors. note models identical scale horizontal axis linear logarithmic that. figure translation invariance measure neurons function signiﬁcance. color indicates average sign connected weights negative positive best viewed color. section details. results denoising performance models layers presented figure shows lowest reconstruction cost validation dataset datasets. conﬁguration trained times different random initialization conﬁdence bounds calculated corrected sample standard deviation lowest reconstruction cost. best performing no-lat model one-layer model shown dashed line. best two-layer no-lat models cifar- ratios αmin αmin respectively. since no-lat autoencoders need push information layer intuitive narrow bottlenecks model large small ratios perform poorly. study optimal ratio larger revealed lower layer smaller effective dimensionality terms principal components lower compared cifar-. second layer beneﬁcial no-lat model given parameter number constraint. model beneﬁts second layer works best ratio small namely αmin αmin cifar- respectively. second layer hurt beneﬁt model signiﬁcantly performance no-lat models. results also presented numbers table supplementary material. practically prior information poolings incorporated either model structure treatment training data. means invariances learned model must present higher-order correlations inputs explained section well known invariances developed natural images question well different model structures able represent learn features. test this generated sets rotated scaled translated images measured invariant activations type transformation separately. example translation contained translated images given transformation type calculated mean activation compared variances variance var{h} samples feature completely invariant respect deﬁnition follows transformation equals one. overall conclusions similar tested transformations main text concentrates translation invariance. results invariances reported supplementary material section average layer-wise invariance grows towards higher layers models much faster best models others i.e. cifar- best model γ... whereas best no-lat γ... values reported table supplementary material. illustrate this plotted figure invariance measures best variant model. plot dots correspond hidden neurons color reﬂects average sign encoder connection neuron horizontal axis signiﬁcance neuron measure much model uses hidden neuron deﬁned analyzed supplementary material section notable observations. first neurons model highly invariant whereas models invariant neurons vast majority neurons invariance. no-lat model invariance seems even smaller neurons model uses more. moreover tested second layer model stays highly invariant even layer size increased second invariant neurons tend stronger negative positive weights especially no-lat models. since nonlinearity layer rectiﬁed linear unit convex function saturates negative side negative tied weights mean network ﬂipped functions concave functions saturate positive side resembling operation. interpretation discussed supplementary material section modulated model used practically second layer neurons pooling. studying poolings found typically every layer neuron participates several qualitatively different poolings. seen figure layer neuron participates different kinds poolings sensitive particular features invariant types. example layer neuron selective orientation frequency color participates three different layer poolings ﬁrst selective color invariant orientation. second selective orientation invariant color. third responds high frequency orientation. details analysis available supplementary material. various measures invariance proposed invariance measure closely related autocorrelation used measure invariance e.g. dosovitskiy translated patches includes pairs various translations therefore measure weighted average autocorrelation function. wider autocorrelation larger measure measure fooled copying invariant feature many hidden neurons veriﬁed happening here slow feature analysis robust redundancy yields qualitatively results tested networks. figure various pooling groups neuron. represents three relevant pooling groups selected neuron depicted ﬁrst column belongs represents pooling neuron showing relevant neurons associated poolings found selecting layer neurons following strongest links layer identify poolings participate. consecutively layer neurons corresponding pooling group identiﬁed walking back strongest links layer neuron. best viewed color. procedure choosing features plot also depicted figure supplementary material. experiments showed invariance denoising autoencoders increased towards higher layers signiﬁcantly decoder suitable structure details could combined invariant features multiplicative interactions. lateral connections encoder decoder allowed models discard details higher levels details invariant features combined suitably. best models modulated lateral connections able learn large number poolings unsupervised manner. tested invariance nothing model biased learning direction observed invariance selective discrimination several different dimensions color orientation frequency. summary ﬁndings fully line earlier proposition unsupervised denoising autoencoder modulated lateral connections work tandem supervised learning because shown ﬁrst time higher layers model ability focus abstract representations unlike regular autoencoders therefore able discard details supervised learning deems irrelevant. becomes possible combine autoencoders popular supervised learning pipelines include in-built pooling operations multiple ways extend work including explicit bias towards invariances; sparse structure convolutional networks making much larger scale models deeper hierarchies feasible; dynamic models; semi-supervised learning. dosovitskiy springenberg riedmiller brox discriminative unsupervised feature learning convolutional neural networks. ghahramani welling cortes lawrence weinberger advances neural information processing systems kingma mohamed rezende welling semi-supervised learning deep generative models. ghahramani welling cortes lawrence weinberger advances neural information processing systems curran associates inc. table denoising performance translation invariance measure selected models. models exactly input layer data average invariance models e.g. cifar- preprocessing data patches size sampled randomly continuously training. separate test images aside testing generalization performance last samples cifar sixth image dataset. continuous sampling allows generation millions data samples alleviating overﬁtting problems. dataset already whitened preprocessing applied color patches cifar- whitened dimensionality reduced match dimensionality grayscale images o&f. despite dimensionality reduction variance retained. white additive gaussian noise used corrupting inputs scaled standard deviation training adadelta used adapt learning rate momentum weight vectors initialized normal distribution norm orthogonalized. order improve convergence speed models centered hidden unit activations following raiko auxiliary bias term applied immediately nonlinearity centers output zero mean. first turned different models different proportions invariant neurons wanted understand better going questions important roles different types features invariances formed. second level invariances could low-frequency features invariant already ﬁrst layer formed pooling layer neurons. ﬁrst question answered looking connections coming from. connections visualized figures a–c. neurons layer ordered respect invariance increases left right. connecting edges colored according sign connecting weight strength edge reﬂects signiﬁcance connection. signiﬁcance deﬁned proportion variance higher-level neuron generates lower-level neurons. initially tried visualizing simply magnitudes weights problem input neuron variance output neuron saturated large weight magnitude indicate connection important; would make much difference connection removed. visualizing ﬁrst took squares scaled input neuron’s variance assuming input neurons independent quantity reﬂects input variance var{h neuron receives. depending saturation neuron smaller greater proportion therefore scaled variance transmitted actual output variance var{h incoming variances named quantity signiﬁcance connection approximately measures output variance layer originates from. signiﬁcance also depicted figure coordinate output signiﬁcances turned invariant neurons tend stronger negative positive weights. visualized color figure blue signiﬁes negative positive weights. images connections translucent means equal number positive negative weights results purple color. striking feature plots invariant features tend negative weights. since nonlinearity layer rectiﬁed linear unit convex function saturates negative side negative tied weights mean network ﬂipped functions concave functions saturate positive side. therefore looks like network learned layer concave or-like invariance-generating poolings convex and-like coincidence detectors. forming convex using positive weights concave using negative weights demonstrated respectively truth tables table table geometric form illustrated figure figure also studied invariance second layer neurons scaling rotation transformations using invariance measure deﬁned section translation. formed sets samples scaling cifar- images zoom factors rotating images −◦−◦ scaling rotation invariance experiments respectively. results shown figure figure similar translation invariance figure figure illustrates impact increasing i.e. increasing size second layer. notably second layer neurons stay highly invariant even layer size increased. figure neurons ordered according increasing translation invariance left right. blue denotes negative positive weights strength connections depends signiﬁcance connection layer neurons model also visualized figure best viewed color. figure conceptual illustration linear rectiﬁer unit performing logical operations three-element binary input afﬁne transform operations positive weights convex form whereas operation negative weights concave functional form. figure various invariance measures neurons function signiﬁcance cifar. color indicates average sign connected weights negative positive best viewed color. section details. figure method selecting pooling groups including given neuron contains phases. first follow strongest links second layer identify pooling groups ﬁrst-layer neuron belongs second visualize pooling group identifying layer neurons strongest links back layer neuron example ﬁrst pooling group contains neurons marked green second pooling group third consist purple neurons. colors correspond rows figure phases performed", "year": 2014}