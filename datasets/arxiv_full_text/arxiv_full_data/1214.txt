{"title": "Learning Neural Network Architectures using Backpropagation", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.", "text": "deep neural networks millions parameters heart many state machine learning models today. however recent works shown models much smaller number parameters also perform well. work introduce problem architecture-learning i.e; learning architecture neural network along weights. start large neural network learn neurons prune. introduce trainable parameter called tri-state relu helps pruning unnecessary neurons. also propose smooth regularizer encourages total number neurons elimination small. resulting objective differentiable simple optimize. experimentally validate method small large networks show learn models considerably smaller number parameters without affecting prediction accuracy. large-scale tasks like image classiﬁcation general practice recent times train large networks many millions parameters looking models natural many parameters really needed good performance? words models simple smaller model advantage faster evaluate easier store crucial real-time embedded applications. work consider problem automatically building smaller networks achieve performance levels similar larger networks. regularizers often used encourage learning simpler models. usually restrict magnitude sparsity weights. however restrict computational complexity neural networks need regularizer restricts width depth network. here width layer refers number neurons layer depth simply corresponds total number layers. generally speaking greater width depth number neurons computationally complex model naturally would want restrict total number neurons means controlling computational complexity model. however number neurons integer making difﬁcult optimize over. work aims making problem easier solve. srinivas babu learning neural architectures propose novel trainable parameters used restrict total number neurons neural network model thus effectively selecting width depth general term ‘architecture’ neural network refer aspects network width depth however word simply mean width depth. given want reduce complexity model formally deﬁne notions complexity architecture. notation. inﬁnite-dimensional vector whose ﬁrst components positive integers rest zeros. represents m-layer neural network architecture neurons layer. call architecture neural network. vectors deﬁne associated norm corresponds notion architectural complexity neural network. notion complexity simply total number neurons network. true measure computational complexity neural network would total number weights parameters. however consider single layer neural network proportional number neurons hidden layer. even though equivalence breaks multi-layered neural networks nevertheless want simplicity. deﬁnition. complexity m-layer neural network architecture given denotes weights neural network architecture. denotes loss function depends underlying task solved. example squared-error loss functions generally used regression problems cross-entropy loss classiﬁcation. objective exists classical trade-off model complexity loss handled parameter. note learn weights well architecture problem. term algorithm solves problem architecture-learning algorithm. observe task deﬁned difﬁcult solve primarily integer. makes integer programming problem. hence cannot gradient-based techniques optimize this. main contribution work re-formulation optimization problem stochastic gradient descent back-propagation used. require strategy automatically select neural network’s architecture i.e; width layer depth network. select width layer introduce additional learnable parameters multiply every neuron’s output shown figure parameters restricted binary neurons zero-parameter simply removed. ﬁgure trainable parameters corresponding neurons values zero nullifying contribution. thus binary trainable parameters equal effective width network. convolutional layers feature outputs additional parameters select subset feature maps. single additional parameter multiplies entire feature either making zero preserving ﬁlters analogous neurons convolutional layers. figure strategy selecting width depth. left grey blobs denote neurons coloured blobs denote proposed additional trainable parameters. right purple bars denote weight-matrices. graph regularizer binarizing regularizer reduce complexity network also strive reduce network’s depth. well known neural network layers without non-linearity equivalent single layer whose parameters given matrix product weight matrices original layers. shown right figure therefore consider trainable non-linearity prefers ‘linearity’ ‘non-linearity’. wherever linearity selected corresponding layer combined next layer. hence total complexity neural network would number parameters layers non-linearity. reduces usual relu ﬁxed trainable turns parametric relu trainable. however restrict parameters take binary values. result three possible states exist function. function always returns zero. behaves similar relu reduces identity function. here parameter selects width layer decides depth. parameter different across channels layer parameter tied value across channels. combine layer next yield single layer. channel simply remove neuron well corresponding weights next layer. given deﬁnition tri-state relu above require method learn binary parameters regularizer given regularizer encourages binary values parameters constrained henceforth shall refer binarizing regularizer. murray showed regularizer indeed converge binary values given large number iterations. case function downward-facing parabola minima shown figure result weights fall convergence. contrast regularizer upward facing parabola minimum causes push weights close zero. note regularization constant width-limiting term depth-limiting term. objective solved using usual back-propagation algorithm. indicated earlier binarizing regularizer works guaranteed enforce same perform clipping parameter update. optimization even though ﬁnal parameters expected close binary still real numbers close parameter obtained optimization. tsrelu function uses binarized version variable srinivas babu learning neural architectures adding model complexity considered problem solving equation result objective function described necessarily select smaller models. correspond complexity layer. model complexity term given ﬁrst term equation limits complexity layer’s width second term limits network’s depth encouraging linearity. note ﬁrst term becomes zero non-linearity absent. also note indicator function ﬁrst term non-differentiable. result simply treat term constant respect many works look performing compression neural network. weight-pruning techniques popularized optimal brain damage optimal brain surgery recently proposed neuron pruning technique relied neuronal similarity. work hand performs neuron pruning based learning rather hand-crafted rules. learning objective thus seen performing pruning learning together unlike work perform operations alternately. learning neural network architecture also explored extent. cascadecorrelation proposed novel learning rule ‘grow’ neural network. however shown single layer network hence clear scale large deep networks. work inspired recent work kulkarni proposed learn width neural networks similar ours. speciﬁcally proposed learn diagonal matrix along neurons represents layer’s neurons. however instead imposing binary constraint learn real-weights impose -based sparsity-inducing regularizer encourage zeros. imposing binary constraint able directly regularize model complexity. recently bayesian optimization-based algorithms also proposed automatically learning hyper-parameters neural networks. however purpose selecting architecture typically require training multiple models different architectures method selects architecture single run. large number evolutionary algorithms also exist task ﬁnding neural network architectures. many methods proposed train models deep lower parameterisation conventional networks. collins kohli propose sparsity inducing regulariser backpropogation promotes many weights zero magnitude. achieve reduction memory consumption compared traditionally trained models. contrast method promotes neurons zero-magnitude. result overall objective function much simpler solve. denil demonstrate parameters model predicted given parameters. training time learn parameters predict rest. yang propose adaptive fastfood transform efﬁcient re-parametrization fully-connected layer weights. results reduction complexity weight storage computation. recent works also focussed using approximations weight matrices perform compression. jaderberg svd-based rank approximations weight matrix. gong clustering-based product quantization approach build indexing scheme reduces space occupied matrix disk. section perform experiments analyse behaviour method. ﬁrst experiments evaluate performance mnist dataset. later look case study ilsvrc dataset. experiments performed using theano deep learning framework evaluate method mnist dataset using lenet-like architecture. network consists convolutional layers ﬁlters fully connected layers neurons. architecture starting point learn smaller architectures. first learn using additional parameters regularizers. second remove neurons zero gate values collapse depth linearities wherever advantageous. example might advantageous remove depth bottleneck layer thus second part process human-guided. starting baseline architecture learn smaller architectures variations method. note max-pooling applied convolutional layers rules depth selection layers. compare proposed method baselines directly training neural network ﬁnal architecture method learning ﬁxed ﬁnal width various layers. table layers learnt column binary elements denotes whether width depth learnt layer baseline network. example second shows method width learnt ﬁrst layers depth also learnt third layer. table shows considered models large small perform less equally well terms accuracy. empirically shows small models discovered preserve accuracy. table architecture learning performance method lenet-like baseline. layers learnt column binary elements denotes whether width depth learnt layer baseline network. architecture learning neural network trained also compare compression performance method svd-based compression weight matrix table compress layer using svd. results show learning smaller network beneﬁcial learning large network performing svd-based compression. perform experiments analyse behaviour method. cases train ‘al’-like models consider third layer evaluation. start learning baseline architecture considered above. first look effects using different hyper-parameters figure observe increasing encourages method prune more decreasing encourages method learn architecture extended amount time. cases architecture stays more-or-less constant large enough number iterations. second look learnt architectures different amounts data complexity. intuitively simpler data lead smaller architectures. simple obtain data differing complexity simply vary number classes multi-class problem like mnist. hence vary number classes method case without changing hyper-parameters. seen figure almost monotonic increase architectural complexity error rate conﬁrms hypothesis. third look depth-selection capabilities method. used models various initial depths observed depths resultant models. used initial architecture layers width repeated obtain network desired depth. small changes initial depth ﬁnal learnt depth stays less constant. figure plot architecture learnt number iterations. affects convergence rate affects amount pruning. plot neurons learnt mnist various number classes. neuron count error rate increase increase number classes. architecture selection recent times bayesian optimization emerged compelling option hyperparameter optimization. experiments compare architecture-selection capabilities method particular spearmint-lite software package default parameters experiments. ﬁrst determine width last layer later width three layers comparison objective function architecture-learning. means externally compute cost every training figure shows typically needs multiple runs discover networks perform close performing multiple runs often prohibitive large networks. even small network like ours training took minutes titanx epochs. training change training time whereas using spent hours completing runs. further directly optimizes cost function opposed performs blackbox optimization. given perform architecture selection hyper-parameters need? notice need decide four quantities objective decide widths need decide quantities thus n-layer neural network able decide numbers based global hyper-parameters. appendix shall look heuristics setting hyperparameters. case study alexnet experiments follow alexnet-like model called caffenet provided caffe deep learning framework. similar alexnet except order max-pooling normalization interchanged. ilsvrc validation compute accuracies table unlike experiments performed previously start pre-trained model perform architecture learning learnt weights. method performs almost well state compression methods. means simply smaller neural network instead using weight re-parameterization techniques large network. further many compression methods formulated keeping fully-connected layers mind. tasks like semantic segmentation networks convolutional layers used results show proposed method successfully prune fully connected neurons convolutional ﬁlters. further among compression methods utilize dense matrix computations whereas methods require specialized kernels sparse matrix computations custom implementations diagonal matrix multiplication etc. conclusions presented method learn neural network’s architecture along weights. rather directly selecting width depth networks introduced small number real-valued hyper-parameters selected width depth also smaller architectures mnist imagenet datasets perform large architectures. method simple straightforward suitably applied neural network. also used tool explore dependence architecture optimization convergence neural networks. james bergstra olivier breuleux frédéric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math expression compiler. proceedings python scientiﬁc computing conference volume page austin misha denil babak shakibi laurent dinh nando freitas predicting parameters deep learning. advances neural information processing systems pages emily denton wojciech zaremba joan bruna yann lecun fergus. exploiting linear structure within convolutional networks efﬁcient evaluation. advances neural information processing systems pages babak hassibi david stork second order derivatives network pruning optimal brain surgeon. advances neural information processing systems pages kaiming xiangyu zhang shaoqing jian sun. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. arxiv preprint arxiv. yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. praveen kulkarni joaquin zepeda frederic jurie patrick pãl’rez louis chevallier. learning structure deep architectures using regularization. mark jones xianghua gary editors proceedings british machine vision conference pages .–.. bmva press september yann lecun john denker sara solla richard howard lawrence jackel. optimal brain damage. advances neural information processing systems volume pages jonathan long evan shelhamer trevor darrell. fully convolutional networks semantic segmentation. proceedings ieee conference computer vision pattern recognition pages olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision ./s---y. jasper snoek hugo larochelle ryan adams. practical bayesian optimization machine learning algorithms. advances neural information processing systems pages suraj srinivas venkatesh babu. data-free parameter pruning deep neural networks. mark jones xianghua gary editors proceedings british machine vision conference pages .–.. bmva press september isbn ---. ./c... https//dx. doi.org/./c... christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. ieee conference computer vision pattern recognition june intuitively observe properties would automatically hold ‘master’ property requires architecture weights globally optimal holds. given optimization objective neural networks highly non-convex global optimality cannot guaranteed. result restrict studying three properties listed. text follows provide statements hold method. obtained analysing widths layer neural network assuming depth never collapsed. words hold neural networks single hidden layer. proofs provided later section. important property forms main motivation architecture-learning. procedure replace node-pruning techniques used compress neural networks. proposition convergence loss proposed method train satisﬁes statement implies change architecture inversely proportional change loss. words architecture grows smaller loss must increase. isn’t strict relationship loss accuracy high loss generally indicates worse accuracy. proposed method learns architecture weights. would happen initialized neural network learnt architecture proceeded learn weights? property ensures cases fall local minimum architecture proposition loss train convergence obtained training neural network data ﬁxed architecture loss convergence neural network trained proposed method data results ﬁnal architecture then characterizing data-complexity traditionally hard. here consider following approach. proposition datasets produce train losses upon training ﬁxed architecture trained proposed method ﬁnal architectures satisfy relation convergence. here ‘harder’ dataset produces higher loss neural network architecture. result ‘harder’ dataset always produces larger ﬁnal architecture. provide proof statement. instead experimentally verify section proofs propositions +λbrb +λmrm total objective function binarizing regularizer model complexity term. convergence assume corresponding weights binary close binary. maximum step size first value based initial widths loss values. recall value multiplies number neurons cost function. network layer neurons hence multiplies divides regularizer value remains same. used mnist-network alexnet. given initial architecture large places emphasis getting small models reducing loss. second times using positive shifts curve fig. right. letting curve shifts extreme right peak hence", "year": 2015}