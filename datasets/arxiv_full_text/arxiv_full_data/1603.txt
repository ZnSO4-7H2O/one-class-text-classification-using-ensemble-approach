{"title": "Dependency-based Convolutional Neural Networks for Sentence Embedding", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To exploit both deep learning and linguistic structures, we propose a tree-based convolutional neural network model which exploit various long-distance relationships between words. Our model improves the sequential baselines on all three sentiment and question classification tasks, and achieves the highest published accuracy on TREC.", "text": "sentence modeling classiﬁcation convolutional neural network approaches recently achieved state-of-the-art results efforts process word vectors sequentially neglect long-distance dependencies. combine deep learning linguistic structures propose dependency-based convolution approach making tree-based n-grams rather surface ones thus utlizing nonlocal interactions words. model improves sequential baselines four sentiment question classiﬁcation tasks achieves highest published accuracy trec. convolutional neural networks originally invented computer vision recently attracted much attention natural language processing problems sequence labeling semantic parsing search query retrieval particular recent work cnn-based sentence modeling achieved excellent often state-of-the-art results various classiﬁcation tasks sentiment subjectivity question-type classiﬁcation. however despite celebrated success remains major limitation linguistics perspective cnns invented pixel matrices image processing consider sequential n-grams consecutive surface string neglect longdistance dependencies latter play important role many linguistic phenomena negation subordination wh-extraction might dully affect sentiment subjectivity categorization sentence. indeed sentiment analysis literature researchers incorporated long-distance information syntactic parse trees results somewhat inconsistent reported small improvements otherwise result syntactic features become popular sentiment analysis community. suspect reasons data sparsity problem largely alleviated recent advances word embedding. combine advantages worlds? propose simple dependency-based convolutional neural networks model similar sequential cnns word sequential context considers word parent grandparent great-grand-parent siblings dependency tree. incorporate longdistance information otherwise unavailable surface string. classiﬁcation tasks demonstrate superior performance dcnns baseline sequential cnns. particular accuracy trec dataset outperforms previously published results literature including heavy hand-engineered features. original ﬁrst proposed lecun applies convolution kernels series continuous areas given images adapted collobert following dimensional convolution operates convolution kernel sequential order equation represents dimensional word representation i-th word n-gram models feeds local information convolution operations. however setting capture long-distance relationships unless enlarge window indeﬁnitely would inevitably cause data sparsity problem. order capture long-distance dependencies propose dependency-based convolution model figure illustrates example movie reviews dataset sentiment sentence obviously positive quite difﬁcult sequential cnns many n-gram windows would include highly negative word shortcomings distance despite shortcomings quite long. dcnn however could capture tree-based bigram despite shortcomings thus ﬂipping sentiment tree-based trigram root moving stories highly positive. figure illustrates ancestor paths patterns various orders. always start convolution concatenate ancestors. root node reached root dummy ancestors max-over-tree pooling dropout ﬁlters convolve different word concatenation regarded pattern detection similar pattern words ﬁlter could return maximum activation. sequential cnns max-over-time pooling operates feature maximum activation representing entire feature map. dcnns also pool maximum activation feature detect strongest activation whole tree since tree longer deﬁnes sequential time direction refer pooling max-over-tree pooling. order capture enough variations randomly initialize ﬁlters detect different structure patterns. ﬁlter’s height number words considered width always equal dimensionality word representation. ﬁlter represented feature max-over-tree pooling. series convolution different ﬁlter different heights multiple features carry different structural information become ﬁnal representation input sentence. then sentence representation passed fully connected soft-max layer outputs distribution different labels. neural networks often suffer overtraining. following employ random dropout penultimate layer order prevent co-adaptation hidden units. experiments drop rate learning rate default. following training done stochastic gradient descent shufﬂed mini-batches adadelta update rule figure convolution patterns trees. word concatenation always starts denote parent grand parent great-grand parent etc. denotes words excluded convolution. convolution siblings ancestor paths alone enough capture many linguistic phenomena conjunction. inspired higher-order dependency parsing also incorporate siblings given word various ways. figure details. datasets ﬁrst obtain dependency parse tree stanford parser different window size different choice convolution shown figure dataset without development randomly choose training data indicate early stopping. order fare comparison baseline also window size. results generated efﬁciency however could potentially better results. implementation code released. combined model powerful structural information still fully cover sequential information. also parsing errors directly affect dcnn performance sequential n-grams always correctly observed. best exploit information want combine models. easiest combination concatenate representations together feed fully connected soft-max neural networks. cases combine different feature different type sources could stabilize performance. ﬁnal sentence representation thus number ancestor sibling sequential ﬁlters. practice ﬁlters template figure fully combined representation -dimensional contrast -dimensional sequential cnn. experiments table summarizes results context high-performing efforts literature. three benchmark datasets categories sentiment analysis movie review stanford sentiment treebank datasets question classiﬁcation trec sentiment analysis sentiment analysis datasets based movie reviews. differences mainly different numbers categories whether standard split given. sentences dataset. instance labeled positive negative cases contains sentence. since standard data split given following literature fold cross validation include every sentence training testing least once. concatenating sibling sequential information obviously improves dcnns ﬁnal model outperforms baseline sequential cnns ties stanford sentiment treebank annotates ﬁner-grained labels positive positive neutral negative negative extension dataset. sentences standard split. model achieves accuracy second irsoy cardie model dcnns ancestor dcnns ancestor+sibling dcnns ancestor+sibling+sequential cnns-non-static baseline cnns-multichannel deep cnns recursive autoencoder recursive neural tensor deep recursive lstm tree paragraph-vec svms table results movie review stanford sentiment treebank trec datasets. trec- trec grained labels. †results generated ∗results generated implementation. question classiﬁcation trec dataset entire dataset sentences classiﬁed following categories abbreviation entity description location numeric. experiment dcnns easily outperform methods even ancestor convolution only. dcnns sibling achieve best performance published literature. dcnns combined sibling sequential information might suffer overﬁtting training data based observation. thing note best result even exceeds svms hand-coded rules. trec dataset also provides subcategories numerictemperature numericdistance entityvehicle. make task realistic challenging also test proposed model respect subcategories. obvious improvements sequential cnns last column table like ours silva tree-based system uses constituency trees compared dependency trees. report higher ﬁne-grained accuracy parser trained questionbank used standard stanford parser trained penn treebank questionbank. moreover mentioned above approach rule-based automatically learned. discussions examples compared sentiment analysis advantage proposed model obviously substantial trec dataset. based error analysis conclude mainly difference parse tree quality tasks. sentiment analysis dataset collected rotten tomatoes website includes many irregular usage language. sentences even come languages english. errors parse trees inevitably affect classiﬁcation accuracy. however parser works substantially better trec dataset since questions formal written english training stanford parser already includes questionbank includes trec sentences. figure visualizes examples errs dcnn not. example labels location hawaii state long-distance backbone what ﬂower clearly asking entity. similarly dcnn captures obviously negative tree-based trigram nothing worth emailing. note model also works non-projective dependency trees last examples figure visualize cases dcnn outperforms baseline cnns ﬁne-grained trec. example word temperature second root word span earth. window size tree convolution every words span convolved temperature reason dcnn correct. figure showcases examples baseline cnns better results dcnns. example misclassiﬁed entity dcnn parsing/tagging error numerical labeled entity dcnn description cnn. part-of-speech tagging). word sentence verb instead noun hummingbirds relative clause modifying speed. sentences misclassiﬁed baseline dcnn. figure shows three examples. example classiﬁed numerical methods ambiguous meaning word point difﬁcult capture word embedding. word mean location opinion etc. apparently numerical aspect captured word embedding. example might annotation error. shortly submitting learned independently reported concurrent related efforts. constituency model based unpublished work programming languages performs convolution pretrained recursive node representations rather word embeddings thus baring little resemblance dependency-based model. dependency model related always includes node children variant sibling model always ﬂat. contrast ancestor model looks vertical path word ancestors linguistically motivated conclusions presented simple dependencybased convolution framework outperforms sequential baselines modeling sentences. kushal dave steve lawrence david pennock. mining peanut gallery opinion extraction semantic classiﬁcation product reviews. proceedings world wide web. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutimproving neural networks dinov. preventing co-adaptation feature detectors. journal machine learning research mohit iyyer jordan boyd-graber leonardo claudino richard socher daum´e iii. neural network factoid question answering paragraphs. proceedings emnlp. lecun jackel bottou brunot cortes denker drucker guyon mller sckinger simard vapnik. comparison learning algorithms handwritten digit recognition. int’l conf. artiﬁcial neural nets. christopher manning mihai surdeanu john bauer jenny finkel steven bethard david mcclosky. stanford corenlp natural language processing toolkit. proceedings demonstrations pages lili zhang wang. tbcnn tree-based convolutional neural network programming language processing. unpublished manuscript http//arxiv.org/ abs/.. lili peng zhang jin. discriminative neural sentence modeling tree-based convolution. unpublished manuscript http//arxiv.org/abs/. version dated june version dated yelong shen xiaodong jianfeng deng gregoire mesnil. learning semantic representations using convolutional neural networks search. proceedings www. richard socher jeffrey pennington eric huang andrew christopher manning. semi-supervised recursive autoencoders predicting sentiment distributions. proceedings emnlp richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts. recursive deep models semantic compositionality sentiment treebank. proceedings emnlp", "year": 2015}