{"title": "Machine Learning Approach to RF Transmitter Identification", "tag": ["eess.SP", "cs.LG", "cs.NE", "stat.ML"], "abstract": "With the development and widespread use of wireless devices in recent years (mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has become extremely crowded. In order to counter security threats posed by rogue or unknown transmitters, it is important to identify RF transmitters not by the data content of the transmissions but based on the intrinsic physical characteristics of the transmitters. RF waveforms represent a particular challenge because of the extremely high data rates involved and the potentially large number of transmitters present in a given location. These factors outline the need for rapid fingerprinting and identification methods that go beyond the traditional hand-engineered approaches. In this study, we investigate the use of machine learning (ML) strategies to the classification and identification problems, and the use of wavelets to reduce the amount of data required. Four different ML strategies are evaluated: deep neural nets (DNN), convolutional neural nets (CNN), support vector machines (SVM), and multi-stage training (MST) using accelerated Levenberg-Marquardt (A-LM) updates. The A-LM MST method preconditioned by wavelets was by far the most accurate, achieving 100% classification accuracy of transmitters, as tested using data originating from 12 different transmitters. We discuss strategies for extension of MST to a much larger number of transmitters.", "text": "youssef l.-s. bouchard k.z. haigh krovi silovsky c.p. vander valk department chemistry biochemistry bioengineering california nanosystems institute document contain technology technical data controlled either u.s. international traﬃc arms regulations u.s. export administration regulations. development widespread wireless devices recent years electromagnetic spectrum become extremely crowded. order counter security threats posed rogue unknown transmitters important identify transmitters data content transmissions based intrinsic physical characteristics transmitters. waveforms represent particular challenge extremely high data rates involved potentially large number transmitters present given location. factors outline need rapid ﬁngerprinting identiﬁcation methods beyond traditional hand-engineered approaches. study investigate machine learning strategies classiﬁcation identiﬁcation problems wavelets reduce amount data required. four diﬀerent strategies evaluated deep neural nets convolutional neural nets support vector machines multi-stage training using accelerated levenberg-marquardt updates. a-lm method preconditioned wavelets accurate achieving classiﬁcation accuracy transmitters tested using data originating diﬀerent transmitters. discuss strategies extension much larger number transmitters. large number electronic devices today’s environment spectrum crowded. examples devices commonly encountered emit radio waves include cordless phones cell phones microwave ovens wireless audio video transmitters motion detectors wlan cars. advent internet things even larger swath devices contributes emissions form physical devices vehicles items embedded electronics software sensors actuators network connectivity enable objects communicate transmitting receiving data. large number communication protocols currently operate diﬀerent bands many devices wildly insecure variety reasons thus imperative solve security vulnerabilities identify counter attacks. vulnerabilities broader sense include attacks devices also false impersonations devices example rogue transmitters. rapid identiﬁcation threats unknown signals paramount importance. another important motivation development transmitter identiﬁcation schemes mitigation problems associated interference. overlap diﬀerent bands strong number transmitters large often reduced interference. interference generated almost device emits electromagnetic signal cordless phones bluetooth headsets microwave ovens even smart meters. single biggest source wi-fi interference local wi-fi network wi-fi shared medium operates unlicensed bands within range. compounding problem fact transmitters tend operate independently. thus lack timing alignment results signiﬁcant interchannel interference. interference occurs packets lost must retransmitted. conventional techniques switching frequency bands insuﬃcient solving interference problem. methods accurately identify transmitting sources could lead better schemes signal separation crowded environment. identiﬁcation threats aircrafts radar warning receivers typically analyze frequency pulse width pulse-repetition frequency modulation pulses modulations antenna scan characteristics. modern systems computer determines closest parameters found threat identiﬁcation table force identiﬁcation. even modern systems identiﬁcation algorithms well beyond schemes based matching parameters. thus makes sense explore sophisticated techniques. recent years ﬁeld artiﬁcial intelligence rapidly grown popularity development modern techniques deep learning. applications image speech recognition everyday life situations increasingly common. contrast scarcely used domain little work done explore connection signal processing study extend approaches spectrum domain order develop practical applications emerging spectrum problems demand vastly improved discrimination performance today’s hand-engineered systems. applications particular challenges exist numerous transmission protocols large amounts data large bandwidths high data rates involved. calls development algorithms capable addressing challenges. many modern algorithms born desire mimic biological systems systems biological analogues tailored strategies needed. main task must implemented feature learning. naive application spectrum depends hand-engineered features expert selected based belief best describe signals pertinent speciﬁc task. hand application deep learning domains achieved excellent performance vision speech problems learning features similar learned brain sensory data. recently shown features potential transform spectrum problems similar domains. speciﬁcally schemes capable learning appropriate features describe signals associated properties training data. ultimately innovations result generation systems learn data.the development transmitter identiﬁcation schemes would help counter highest accuracy across broad range conditions including exposure limited training dataset achieved using a-lm method. validate methods experimental data diﬀerent ofdm transmitters. strategies extensions much larger numbers transmitters discussed including promising approach based preconditioning packets decomposition signal content wavelets ahead phase. sample data collected diﬀerent radios transmitters each total transmitters. radios share power supply reference oscillator rest chain diﬀers transmitter collected packets total packets transmitter. packets generated pseudorandom values injected debug interface modem; address identiﬁcation included signal. packets transmitted radio. used proprietary ofdm protocol baseband sample rate transmitter msps subcarrier spacing cyclic preﬁx length samples subcarriers qpsk modulation. captured packet represented time-domain complex-valued data points. reduce ambiguity describing data preparation handling denote time-domain data collection complex-valued vector processing real imaginary parts treated elements two-dimensional vector input network formed sequence vectors. handling signal case wavelet preconditioning described section explored eﬀects training diﬀerent techniques using diﬀerent amounts training testing data data used training testing values experiment denoted data used training testing values denoted dataset transmitters packets captured data corresponds packets data remaining packets. order demonstrate ability learn features signals create models identify distinguish diﬀerent known transmitters recognize unknown transmitters high degree accuracy four diﬀerent algorithms investigated mst. conﬁgurations each total diﬀerent analyses. methods implementations described below. used implementation found weka tested polykernel pearson universal kernel known eﬀective polykernel weka implementation extremely slow. reimplemented would operate eﬃciently embedded platform support vector regression.) used platt’s minimization optimization algorithm compute maximum-margin hyperplanes. baseline neural models used simple fully-connected hidden layers nodes. used rectiﬁed linear units non-linearity hidden layers sigmoid transfer function output layer. mini-batch size adam optimizer used training. volutional layer ﬁlters size followed max-pooling layer pool size. second convolutional layer ﬁlters size followed max-pooling layer pool size. fully conmst method ﬁrst developed handling large datasets limited computing resources image noise identiﬁcation removal applied identiﬁcation problem ﬁrst time. alternative method achieve deep learning fewer resources. present approach second-order training section compare case ﬁrst-order training section begin reviewing operational principle widespread methods. training performed multiple stages stage consists multi-layer perceptrons shown figure hierarchical strategy drastically increases eﬃciency training burden reaching global solution complex model perform well variations input data divided multiple simpler models simple model performs well part variations input data. using subsequent stages area specialization model gradually broadened. process reaching global minimum becomes much easier ﬁrst stage since models following stages search combinations partial solutions problem rather directly ﬁnding complete solution using data. fig. method employs groups organized hierarchical fashion. outputs mlps ﬁrst stage inputs mlps second stage. outputs mlps second stage inputs mlps third stage implemented work general outputs stage inputs higher stage front-end added process input prior reaching ﬁrst stage. fig. simplest incarnation perceptron trained time. neurons trained model ideal response using training samples consisting value input corresponding value target diﬀerent input ranges. number training samples increased neuron inputs partial solutions neuron instead value. neuron ﬁnds combination keeps best parts partial solutions neurons black curve target function; blue curve result. diversity areas specialization diﬀerent models. training done properly eﬃcient illustrated figures model. main idea divide training several smaller mlps. architecture computationally tractable training large drastically simpliﬁes approach deep learning. simple models ﬁrst stage trained batch consisting small part training dataset. example training dataset consisting training samples divided batches samples each noting batches common training samples. mlps ﬁrst stage mlps divided groups mlps group trained using batches. batch size progressively increased higher stages input dimension stage fig. complex functions single perceptrons replaced network perceptrons trained model ideal response using training samples consisting value input corresponding value target diﬀerent input ranges. number training samples range increased inputs partial solutions instead value. ﬁnds combination keeps best parts partial solutions black curve target function; blue curve result. typically decreased. example conﬁguration used herein stage mlps input size stage mlps input size number mlps ﬁrst stage. additionally systematically assigning speciﬁc stopping criteria stage gain level control fast overall system data resulting better overall performance. example designed stages small target error chosen ﬁrst stage drastically decreased successive stages. alternatively larger target error chosen slowly decreased stages depending complexity problem convergence properties training algorithm. shown uses second order methods’ ability yield optimal stopping criteria produce anns better generalizing ability advantages leveraged signal classiﬁcation identiﬁcation. feed-forward neural networks typically trained back-propagation whereby weights updated iteratively using ﬁrstsecond-order update rules. first-order updates generated using gradient descent method variant second-order methods based newton gauss-newton levenberg-marquardt update rules training yields better results faster ﬁrst-order methods however cannot used train large-scale anns even modern computers because complexity issues inversion hessian main computational challenge overcome problem used variant update rule termed accelerated overcomes computational challenges associated enables handle much larger networks converge faster apart computational complexity diﬀerences solution-quality result a-lm however close. hand performance second-order training clearly surpasses ﬁrst-order training. figure shows performance comparison ﬁrstsecond-order training second-order training converges hundred iterations simple illustrative curveﬁtting task whereas ﬁrst-order training converged even iterations. thus conclude second-order training ﬁnds better path good solution compared ﬁrst-order methods. unless stated otherwise -stage system following conﬁguration used experiments herein stage mlps hidden layers/mlp neurons/layer. stage mlps hidden layer/mlp neurons/layer. stage mlps hidden layer/mlp neurons/layer. details experiment provided table implemented matlab using tansig activation functions hidden layers purelin output layers regardless method used compute weights update alone oﬀers important advantages conventional algorithms reduced computational complexity arising mstbased training done. reduced complexity enables second-order training algorithms much larger scale typically possible. second-order training main bottleneck computation hessian matrix inverse. context improves computational eﬃciency ways. ﬁrst using multiple smaller matrices instead single large matrix operations involving jacobian hessian. consider system conﬁguration used herein signal identiﬁcation example input size samples total number parameters system parameters imagine single many parameters. second-order training single giant would require inversion hessian matrix size would exceedingly challenging computational standpoint. contrast requires inversion much smaller hessian matrices. present study requires hessian matrices elements hessians size hessians size uses best matrix inversion algorithm available complexity would times faster iteration i.e. ./=. second increases eﬃciency allowing parallel training mlps stage. example training would times faster training single giant number parameters given full parallel implementation i.e. ./=. drastic improvement computation time also accompanied drastic reduction storage memory requirements. non-parallel implementation example requires times less memory storing hessian i.e. parallel-processing implementation would consume times less memory i.e. section examine question performance multi-stage training strategy second-order training updates? back-propagation part training phase.] second-order order training algorithm known better results ﬁrst-order methods fewer iterations matlab documentation states trainlm often fastest back-propagation algorithm toolbox highly recommended ﬁrst-choice supervised algorithm although require memory algorithms. a-lm algorithm extends applicability second-order methods large scale anns. order demonstrate power second-order training compared performance conditions ﬁrstsecond-order training. results show performance secondorder training superior terms accuracy execution time also faster ﬁrstorder training. fact single iteration ﬁrst-order training faster second-order training iteration convergence requires substantially iterations. fig. performance single ﬁrstsecond-order training curve-ﬁtting task network architecture training data set. four diﬀerent functions ﬁtted. target function shown blue whereas result shown red. second-order a-lm updates middle bottom ﬁrst-order steepest-descent updates functions second-order method fully converges iterations. converges slowly seen bottom iterations still enough yield good result. architecture experiments four mlps ﬁrst stage second stage hidden layers neurons layer. function training data consisted randomly generated values given range inputs corresponding values targets. testing data consisted equally spaced values within given range required estimate corresponding value. number iterations stated total number iterations mlps. within trained equal number iterations case equal total number iterations divided possible achieve high performance under ﬁrst-order training. however order reach performance comparable second-order-trained system complexity needs increased signiﬁcantly point ﬁrst-order training loses computational eﬃciency advantage. conducted experiments demonstrate applicability method identify unknown transmitters using training subset available data twelve transmitters. results demonstrate ability classiﬁcation scalability recognition rogue/unknown signals. tal). given signal task consisted identifying transmitter belongs table figures compare methods data used training. remaining signals used training used testing. secondorder trained a-lm method performed better under conditions remained highly accurate even trained using less data table also includes comparison ﬁrstsecond-order trained performance. larger values ﬁrst-order training converge reasonable time using conﬁguration designed second-order training. hence separate conﬁguration optimized ﬁrst-order training used comparisons. conﬁguration takes account inferior convergence properties ﬁrst-order training. spreads desired cost function minimum goal stages achievable intermediate goals stage. section test ability accurately distinguish number known transmitters. training conducted using percentage signals twelve transmitters validation error improving otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise otherwise table method uses individual mlps trained algorithm. notation used naming individual mlps number stage number. mlps shown stage mlps shown stage mlps shown stage compactness. stopping criteria mlps stages include validation error improving consecutive iterations parameter update rule greater consecutive iterations mean square error reaching certain threshold speciﬁed separately stage. outputs shown stage show desired response diﬀerent transmitters. example trained input corresponds transmitter whereas groups mlps trained diﬀerent transmitters stages mlps stage trained give diﬀerent response corresponding transmitter number. consecutive iterations mean square error reaching certain number speciﬁed separately stage maximum number iterations reached large mean square error value speciﬁed ﬁrst stage goal slowly decreased stages order compensate slow convergence order training especially ﬁrst stage computationally demanding input size large mlps stages trained input corresponded speciﬁc transmitter groups mlps trained table iii. analysis performance ﬁrstsecondorder training. mst- refers standard conﬁguration used previous experiments mst- refers conﬁguration times number mlps stage. experiment data accuracy whereas accuracy dataset used here. training times given arbitrary units. diﬀerent transmitters. mlps stage trained give diﬀerent response corresponding transmitter number. result second-order trained a-lm method outperformed ﬁrst-order training under conditions. fig. comparison algorithms clearly outperforms methods particularly trained much smaller dataset. solid lines represent training dotted lines represent training. values appear table contradictory eﬀect segment length performance systems. former case performance decreases length segments grows opposite eﬀect observed latter. reasoning various artifacts aﬀect signal increasing number combinations growing length model robust enough account variability. model applies ﬁlters locally also incorporates pooling mechanism believe make robust. also longer length input segments device-speciﬁc patterns learned convolutional ﬁlters. finally model parameters requires data achieve good performance explains worse performance short segments. performance models degrade signiﬁcantly condition limited training data. contrastive experiments also show training limited amounts data much less stable terms resulting accuracy demonstrated accuracy drop model trained samples long segments fig. models using data segments data training. a-lm method outperformed methods. rather surprising given represents relatively high-information model models unable achieve high accuracy. contrast nearly perfect accuracy hence appearance identity matrix. figure shows confusion matrices models low-information mode using ﬁrst samples signal methods performed poorly maintained high accuracy spite limited training set. considered scenario extending trained deployed system ability recognize devices. task typically referred incremental learning. avoid building unique classiﬁer device enable low-shot learning devices used output nodes sigmoid activations used multi-label classiﬁcation. advantage structure front-end layers shared across device classiﬁers. fact device detector diﬀers rows weight matrix last layer output layer. thus adding device entails adding extra output node would simply require fig. confusion matrices represent labels transmitters classiﬁed algorithms represents relatively high-information state second-order achieves accuracy trained data. fig. confusion matrices show classiﬁcation accuracy transmitters classiﬁed algorithms note confusions likely pair transmitter radio likely radio another multiple transmitters radio share power supply reference oscillator. however appears largely insensitive characteristic range parameters investigated. note also easy identify even weaker-performing algorithms. experimental setup fully train original model devices. another devices added model described above. figure compares accuracy models. ﬁrst trained data devices. trained data devices another devices registered model means extending output layer. hidden layers remain unaltered model extension. contrastive experiments shown technique works better models models demonstrates former generalize better representation extracted front-end layers enough discriminative power distinguish unseen devices. another observation performance drop short segments emphasized test condition. attribute limited generalization device-speciﬁc patterns learned short segments. example previous experiment used mlps transmitter ﬁrst stage. thus increasing number transmitters require training mlps ﬁrst stage also increase size input second stage. designed following experiment test ability method features learned known transmitters build representations transmitters. classiﬁcation task experiment data transmitters used train ﬁrst stage. data transmitters introduced second third stages. remarkable system performance shown table even challenging case training establishes much better alternative cnn. ability recognize devices knowing devices suggest possess scalability property. scalability property critical expansion system larger number transmitters small portion transmitters would needed train ﬁrst stage. dramatically reduce complexity system. section examine question given goal identifying large number unknown transmitters performance improved keeping network size relatively small? propose method wavelet preconditioning address scalability problem. continuous wavelet transform tool choice time-frequency analysis signals. additional signal content created enables clear depiction various frequency components signal function time. feeding output module enhances classiﬁcation identiﬁcation tasks physical characteristics transmitter clearly separated two-dimensional representation. furthermore allows representing data much compact reduces number parameters mst. example time domain segment consisting samples requires approximately million parameters system given herein eﬃciently represented method computational complexity system largely determined number mlps ﬁrst stage reasons. size input vector typically larger number mlps stage whereas increasing number transmitters require increasing size input vector. thus ﬁrst stage mlps typically highest number parameters. number mlps ﬁrst stage determines number inputs second stage. samples using cwt. reduces number parameters drastic reduction number parameters reduces system complexity increasing convergence speed training algorithm. matlab implementation command produces wavelet scalograms carefully designed wavelet preconditioning front-end wavelet scalograms self-organizing pooling layers used front system extract features wavelet transform reduce dimensionality input. performed variance analysis wavelet transform scalogram across various packets gain insight information content packet. figure plot variance scalogram pixel intensities taken across multiple transmitters signal packets. regions high variance indicate scalogram components involved representation signal features. ofdm waveforms data points. skipped ﬁrst points stored next points vector i.e. fi+. vector constructed elements taking either absolute value cosine phase angle real part imaginary parts complex-valued components i.e. fig. wavelet preconditioning front-end used compress data ofdm packet parameters machine learner. ﬁgure illustrates architecture module used signal feature extraction. output module sent stages classiﬁcation. self-organizing unsupervised method used selecting ﬁlters weights convolution layers. fig. wavelet scalograms constructed taking real imaginary phase magnitude time-domain signal. magnitude shows least amount features sparse representation yielded best performance classiﬁcation. fig. diﬀerence scalogram signal function transmitter index transmitters left right txyv txrv txrv txrv txyv txyv txyv txyv bottom row. seen scalograms diﬀerent features identiﬁed transmitter. providing transmitter ﬁngerprint thanks spatial correlations introduced transform. correlations signal much diﬃcult observe time-domain representation. next evaluated performance wavelet feature learning method feeding waveletpreconditioned signals system comparing time-domain methods. figure shows confusion matrix results obtained three diﬀerent scenarios. ﬁrst scenario training done time domain signal using method real imaginary components initial segment signal following onset concatenated sent mst. scenario leads confusion matrix second scenario training done timedomain signal similar method time absolute used instead concatenating real imaginary parts. labeled time domain confusion matrix third scenario training done features extracted wavelet transform described previously. wavelet preconditioning mst. results shown wavelet confusion matrix seen wavelet preconditioning outperforms time-domain methods achieving nearly accuracy case transmitters. note results samples onset used fair comparison. table wavelet preconditioning time-domain method wavelet method outperforms time-domain method terms accuracy speed. single mst; table results mst.) training times given arbitrary units. scaling large numbers transmitters critical appropriate feature extraction method distill essential features unique transmitter. show addition wavelet preconditioning leads higher accuracy compared time-domain method. compare wavelet time-domain methods calculated average result runs single hidden layers/ neurons layer ﬁrst order training. results presented table show average performance average convergence time. fig. time-domain initial part diﬀerence signal signal broadcast across diﬀerent transmitters. timedomain representation much diﬃcult identify features unique transmitter four variance maps shown figure seen variance absolute value plot shows excellent sparsity compared plots suggesting potentially useful representation signal feature extraction handful small regions map. hand variance spread uniformly across remaining cases. question want address next whether scalogram better transmitter identiﬁcation timedomain signal superiority wavelet transform representation application illustrated comparing scalogram time-domain signal identical signal sent across diﬀerent transmitters. figure show scalograms signal sent across diﬀerent transmitters. highlight ﬂuctuations relative convenient mean analogous comparison time-domain signal shown figure time-domains signal broadcast across diﬀerent transmitters compared. fitm denotes magnitude complexvalued time-domain signal time index transmitter index signal number mean across signals transmitters fig. confusion matrices case wavelet preconditioning time-domain methods wavelet preconditioning method outperforms time-domain methods achieving nearly accuracy transmitters. confusion matrices plotted using matlab command plotconfusion rows columns always exactly data randomly selected pool signals transmitters little less signals diﬀerent runs. learning section larger number transmitters using wavelet preconditioning data preparation method. here data transmitters used train ﬁrst stage. data remaining transmitters introduced second third stages. results several training testing partitioning percentages diﬀerent ratios shown table vii. note even under extremely severe conditions system still maintains remarkably high performance. thus conclude wavelet preconditioning promising approach transmitter identiﬁcation classiﬁcation investigated date. results show strategy based second-order training well-suited transmitter identiﬁcation classiﬁcation problems. outperforms state-of-the-art algorithms applications. also found wavelet preconditioning enabled higher accuracy reduce complexity identifying large number unknown transmitters. anticipate scalability property enable identiﬁcation large number unknown transmitters assign unique identiﬁer each. note closing results promising study viewed proof-of-concept study extended challenging conditions encountered real busy environments. next obvious steps would involve increasing number transmitters testing robustness method varying packets noisy channels conditions overlapping transmissions interfering channels moving sources jamming channel eﬀects added. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision geoﬀrey hinton deng dong george dahl abdelrahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara sainath brian kingsbury. deep neural networks acoustic modeling speech recognition. signal processing magazine youssef l.-s. bouchard. training artiﬁcial neural networks reduced computational complexity filed june patent app. https//gtp.autm.net/ public/project//. mark hall eibe frank geoﬀrey holmes bernhard pfahringer peter reutemann i.h. witten. update. http//www.cs.waikato.ac.nz/˜eibe/pubs/weka update.pdf. platt. fast training support vector machines using sequential minimal optimization. schoelkopf burges smola editors advances kernel methods support vector learning. press cambridge http//research.microsoft.com/ enus/um/people/jplatt/smo-book.pdf. karen zita haigh allan mackay michael cook lin. machine learning embedded systems case study. technical report report technologies cambridge march haightechreport-embedded.pdf. karen zita haigh allan mackay michael cook lin. parallel learning decision making smart embedded communications platform. technical report report technologies august hiroshi dozono niina satoru araki. convolutional self organizing map. international conference computational science computational intelligence", "year": 2017}