{"title": "Online Transfer Learning in Reinforcement Learning Domains", "tag": ["cs.AI", "cs.LG", "I.2.11; I.2.6"], "abstract": "This paper proposes an online transfer framework to capture the interaction among agents and shows that current transfer learning in reinforcement learning is a special case of online transfer. Furthermore, this paper re-characterizes existing agents-teaching-agents methods as online transfer and analyze one such teaching method in three ways. First, the convergence of Q-learning and Sarsa with tabular representation with a finite budget is proven. Second, the convergence of Q-learning and Sarsa with linear function approximation is established. Third, the we show the asymptotic performance cannot be hurt through teaching. Additionally, all theoretical results are empirically validated.", "text": "paper proposes online transfer framework capture interaction among agents shows current transfer learning reinforcement learning special case online transfer. furthermore paper re-characterizes existing agents-teaching-agents methods online transfer analyze teaching method three ways. first convergence q-learning sarsa tabular representation ﬁnite budget proven. second convergence qlearning sarsa linear function approximation established. third show asymptotic performance cannot hurt teaching. additionally theoretical results empirically validated. agents autonomously learn master sequential decision tasks reinforcement learning sutton barto traditionally reinforcement learning agents trained used isolation. recently reinforcement learning community became interested interaction among agents improve learning. many possible methods assist agent’s learning erez smart taylor stone paper focuses action advice torrey taylor student agent practices teacher agent suggests actions take. method requires agreement action sets teachers students allowing different state representations different learning algorithms among teachers students. although advice method shown empirically provides multiple beneﬁts torrey taylor zimmer existing work provide formal understanding teaching advice. therefore paper proposes framework online transfer framework characterize interaction among agents aiming understand teaching advice transfer learning perspective. extend transfer learning framework reinforcement learning proposed lazaric online transfer learning capture online interaction agents. also show transfer learning special case online transfer framework framework similar active learning settles reinforcement learning setting. introducing novel framework used analyze existing advice methods action advice torrey taylor first prove convergence q-learning sarsa tabular representation ﬁnite amount advice. second convergence sarsa q-learning linear function approximation established ﬁnite advice. convergence means algorithms converge optimal q-value. third show non-inﬁnite amount advice cannot change student’s asymptotic performance. three results conﬁrmed empirically simple linear chain complex pac-man simulation. markov decision process markov decision process compact state ﬁnite action transition probability kernel. triplet probability transition state taking action state deﬁned borel-measurable subset bounded deterministic function assigns reward transition state state taking action discount factor expected total discounted reward γtr] time step denotes reward received taking action state time step according reward distribution. convenience omit state action denote reward received time step expected total discounted reward γtrt]. following relationship given policy ally means ﬁnding optimal policy maximizes expected return. optimal policy policies maxa∈a a))p representing expected total discounted reward received along optimal trajectory taking action state following optimal policy thereafter. maxa∈a notice although stochastic policy optimal always deterministic optimal policy least high expected value. q-learning important learning algorithm reinforcement learning. modelfree off-policy learning algorithm break-through reinforcement learning control. watkins introduced q-learning follows contrast off-policy algorithms on-policy algorithms sarsa analogy q-learning rummery niranjan given estimate policy difference q-learning sarsa temporal difference determined policy reward received time step notice action selection equation involves policy making on-policy. ﬁnite sets q-value function easily represented |s|×|a| matrix represented computer table. matrix representation also called tabular representation. case convergence q-learning sarsa related algorithms shown previous work peter watkins dayan singh however inﬁnite large infeasible tabular representation compact representation required paper focuses q-learning linear function approximation sarsa linear function approximation. linear approximation means state-value function represented linear combination features {φi}d feature number features. given state action action value time step deﬁned determined policy time combining equation equation obtain sarsa linear approximation. ﬁxed features goal learn parameter vector approximates optimal q-value online transfer transfer learning technique leverages past knowledge source tasks improve learning performance learning algorithm target task. therefore describe knowledge transferred different algorithms. standard reinforcement learning algorithm usually takes input knowledge task returns solution possible solutions. denote space possible input knowledge learning algorithms denote space hypotheses speciﬁcally refers necessary input information computing solution task e.g. samples features learning rate. general objective transfer learning reduce need samples target task taking advantage prior knowledge. online transfer learning algorithm deﬁned sequence transferring learning phases e.g. transferring knowledeg learning transferring based previous learning learning etc. knowledge collected target task time learn knowledge obtained learning algorithm time deﬁne time step one-step update learning algorithms batch update batch learning algorithms. thus algorithm transfer one-step knowledge one-episode knowledge even one-task multi-task knowledge learner depending setting. denote knowledge space respect time learn knowledge learning algorithm time hypothesis space time learn used input next time step online transfer equation then atransf generates transferred knowledge learn next time step practice initial knowledge learning phase learn empty default value. framework expect hypothesis space sequence become better better time criteria space hypothesis respect space possible solutions time figure illustration. example consider active relocation model mihalkova mooney setting expert learner treated transfer algorithm atransf learning algorithm alearn respectively. learner able relocate current state visited state learner become stuck sub-optimal state. thus expert able help learner relocate current state better state according expert’s knowledge. algorithm represented framework number samples expert collect source tasks learn learn equation settings impossible transfer algorithm learning algorithm explicitly access knowledge target tasks limited access example communication failure restrictions cause problems. subsets states actions rewards respectively. estimate q-value function time introduce index distinguish difference different time steps. example learning algorithm able reach states time step time step thus si+. figure standard learning process requires original knowledge target tasks. online transfer learning process transfer algorithm takes input knowledge source tasks target task learner time output transfer knowledge time leaning algorithm takes transfer knowledge time generate hypothesis time process repeat good hypothesis computed. discuss advice method previous work torrey taylor concrete implementation online transfer learning. suppose teacher learned effective policy given task. using ﬁxed policy teach students beginning learn task. student learns teacher observe state student encounters action student takes. budget advice teacher choose advise student states take correct action authors torrey taylor assumed teacher’s action advice always correct students required execute suggested actions. suppose reinforcement learning teacher agent trained task access learned q-value function then student agent begins training task able accept advice form actions teacher. notation denote advice model teacher agent student agent policy teacher provides advice student. following example illustrates characterize advice model context online transfer learning framework. example consider advice model using mistake correcting approach limited budget linear function approximation torrey taylor model teacher student treated transfer algorithm learning algorithm respectively. first transfer algorithm atransf collects samples source tasks. then return advice action learning algorithm alearn according current state action observed learning algorithm learning algorithm alearn takes advice action samples target task returns state action next step meanwhile alearn maintains function space spanned features {φj}n deﬁned domain expert. moreover teacher limited budget advising student time step therefore transf advice model propose paper important theoretical problem resolve convergence algorithms since guarantees correctness algorithms. next subsection discuss action advice interacts tabular versions q-learning sarsa. after corresponding algorithms linear function approximation discussed. notice conditions ensure inﬁnity visits action-state pairs. theorem given advice model student adopts q-learning algorithm conditions lemma hold convergence q-learning still holds advice model setting. proof. notice conditions veriﬁes state-action pair visited inﬁnitely many times. ﬁnite advice advice model. therefore assumptions still hold advice model setting. apply lemma convergence result follows. compared q-learning sarsa on-policy algorithm requires learning policy update values. singh prove sarsa glie policy converges. result prove convergence sarsa advice model. first need deﬁne glie policy. deﬁnition decaying policy called glie greedy limit inﬁnite exploration policy satisﬁes following conditions state-action pair visited inﬁnity many times; policy greedy respect q-value function probability proof. singh prove similar convergence result weaker assumption assume paper assume bounded pairs implies theorem given advice model student adopts sarsa algorithm conditions lemma hold convergence sarsa still holds advice model setting proof. notice glie policy guarantee state-action pair visited inﬁnitely many times. ﬁnite advice advice model. therefore assumptions still hold advice model setting. apply lemma convergence result follows. remark hand convergence results state-action pair visited inﬁnitely often. advice model ﬁnite budget invalidate inﬁnite visit assumption. therefore results follows previous convergence results hold. hand inﬁnite visit assumption sufﬁcient condition convergence result assumption hold convergence still hold. moreover algorithms converge even budget inﬁnite long student still able visit state-action pairs inﬁnitely many times. previous subsection discuss results regarding tabular representation learning algorithms require ﬁnite states actions state. however inﬁnite large state-action space practice important since characterize many realistic scenarios. convergence q-learning sarsa linear approximation standard setting proved melo provided relevant assumptions hold. approach inspired work assumes algorithm holds convergence conditions melo apply convergence theorems action advice model results follow. need deﬁne notations simplifying proofs. given compact state ﬁxed policy markov chain induced policy assume chain uniformly ergodic invariant probability measure policy satisﬁes non-zero measure. probability measure borel-measurable action algorithm q-learning linear approximation converges probability theorem given advice model markov chain induced also uniformly ergodic student adopts q-learning linear approximation conditions lemma hold convergence q-learning linear approximation still advice model setting. next analyze convergence sarsa linear approximation advice model. sarsa on-policy algorithm need different assumptions. policy \u0001-greedy respect q-value function ﬁxed chooses random action probability greedy action aθθθs state θθθ-dependent policy πθθθ satisﬁes πθθθ θθθ. consider policy πθθθt \u0001-greedy respect time step lipshitz continuous respect denotes lipshitz constant moreover assume induced markov chain uniformly ergodic. lemma theorem {φi}d deﬁned above. lipshcitz constant learning policy w.r.t. step-size sequence {αt} theorem given advice model θθθ-dependent \u0001-greedy w.r.t. ﬁxed θθθt time step student adopts sarsa linear approximation conditions lemma hold sarsa linear approximation still converges probability remark notice assume budget teacher ﬁnite implies ﬁnite policies affect convergence results long conditions lemma still hold. therefore student eventually converge even teacher sub-optimal. next investigate asymptotic behavior advice model. convergence results rely inﬁnite experience suitable practice ﬁrst redeﬁne concept convergence. small constant. therefore even advice sub-optimal student always optimal action according q-value updates ﬁnite advice affect asymptotic performance sense inﬁnite horizon. asymptotic performance determined algorithms student uses advice provided teacher. remark theorem indicates limitation advice model. generally intuitive methods improve performance student advice model higher amounts advice redistribution advice. theorem points that ﬁnite budget advice asymptotic performance still determined algorithm student adopts long algorithm converges. furthermore advice delay limited also convergence algorithm student uses. section introduce experimental results domains. goal experiments provide experimental support convergence proofs previous section well justify action advice improves learning. ﬁrst domain simple linear chain states linear chain. second pac-man well-known arcade game. apply q-learning tabular learning linear chain sarsa linear function approximation pac-man. ﬁrst experimental domain linear chain lagoudakis parr domain adopt q-learning tabular representation store q-values simplicity. figure details. paper states actions state left right. state start state state ﬁnal state episode terminated. agent receive reward step non-terminated states goal state. smooth variance student performance average independent trials student learning. linear chain teacher given advice budget reinforcement learning parameters students compare methods calculate area learning curve. apply one-way anova test difference settings result shows indicating reject null hypothesis test groups means. therefore experimental settings statistically different optimal teacher outperforms random teacher outperforms advice outperforms poor teacher. also provide ﬁnal reward standard deviation ﬁnal reward total reward standard deviation ﬁnal reward table pac-man famous arcade game player navigates maze trying earn points touching edible items trying avoid caught four ghosts. java implementation game provided pac-man ghosts league rohlfshagen lucas domain discrete large state space different position combination player ghosts linear function approximation used represent state. student agents learn task using sarsa state representation deﬁned features count objects range distances used torrey taylor experimental results figure settings converges episodes training despite different teacher performance. before one-way anova used test total reward accumulated four different teaching conditions. showing experimental settings statistically different correct teacher better advice better random teacher better poor teacher. also provide rewards table section brieﬂy outline related work transfer learning reinforcement domains online transfer learning supervised learning algorithmic teaching. transfer learning reinforcement domain studies recently taylor stone lazaric lazaric introduces transfer learning framework inspires develop online transfer learning framework classiﬁes transfer learning reinforcement domain three categories instance transfer representation transfer parameter transfer lazaric action advice model method instance transfer explicit action advice lazaric proposed instance-transfer method selectively transfers samples basis similarity source target tasks lazaric azar introduced model takes teacher/advice model input learning reinforcement learning algorithm able query input advice policy necessary. however model consider learning reinforcement learning algorithm behavior believe important online reinforcement learning. zhao propose online transfer learning framework supervised learning zhao aiming transfer useful knowledge source domain online learning task target domain. introduce framework solve transfer different settings. ﬁrst source tasks share domain target tasks second source domain target domain different domain. finally branch computational learning theory called algorithmic teaching tries understand teaching theoretical ways balbach zeugmann algorithmic learning theory teacher usually determines example sequence teach sequence learner. algorithmic teaching models teaching dimension goldman kearns teaching learners restricted mind changes balbach zeugmann however models still concentrate supervised learning. cakmak lopes developed teaching method based algorithm teaching work focuses one-time optimal teaching sequence computing lacks online setting. paper proposes online transfer learning framework. characterizes existing works addressing teaching reinforcement learning. theoretical analysis methods teachers provide action advice lead following conclusions. first q-learning sarsa converge optimal q-valuewhen ﬁnite amount advice. second linear function approximation q-learning sarsa converge optimal q-value assuming normal assumptions hold. thirdthere limit advice model teacher advice affect asymptotic performance algorithms converge. fourth results empirically justiﬁed linear chain pac-man. future sample complexity regret analysis advice model investigated convergence results established. additional models online transfer framework developed focus interaction machines also consider interaction machines humans finally consider reinforcement learning algorithms r-max study theoretical properties algorithms presence advice model.", "year": 2015}