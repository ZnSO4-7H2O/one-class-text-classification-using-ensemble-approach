{"title": "An Improved Admissible Heuristic for Learning Optimal Bayesian Networks", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Recently two search algorithms, A* and breadth-first branch and bound (BFBnB), were developed based on a simple admissible heuristic for learning Bayesian network structures that optimize a scoring function. The heuristic represents a relaxation of the learning problem such that each variable chooses optimal parents independently. As a result, the heuristic may contain many directed cycles and result in a loose bound. This paper introduces an improved admissible heuristic that tries to avoid directed cycles within small groups of variables. A sparse representation is also introduced to store only the unique optimal parent choices. Empirical results show that the new techniques significantly improved the efficiency and scalability of A* and BFBnB on most of datasets tested in this paper.", "text": "recently search algorithms breadthﬁrst branch bound developed based simple admissible heuristic learning bayesian network structures optimize scoring function. heuristic represents relaxation learning problem variable chooses optimal parents independently. result heuristic contain many directed cycles result loose bound. paper introduces improved admissible heuristic tries avoid directed cycles within small groups variables. sparse representation also introduced store unique optimal parent choices. empirical results show techniques signiﬁcantly improved efﬁciency scalability bfbnb datasets tested paper. bayesian networks often used represent relationships among variables domain. network structure unknown learn structure directly given dataset. several exact algorithms learning optimal bayesian networks developed based dynamic programming branch bound linear integer programming recently yuan proposed shortest-path ﬁnding formulation bayesian network structure learning problem shortest path found implicit search graph called order graph corresponds optimal network structure. search algorithm developed solve search problem. malone adopted formulation realized search performed layered breadth-ﬁrst order. external memory delayed duplicate detection used ensure completion regardless amount available ram. algorithm named breadth-ﬁrst branch bound shown similar runtimes scale many variables. simple admissible heuristic used bfbnb algorithms guide search. main idea relax acyclicity constraint bayesian networks variable freely choose optimal parents variables. heuristic provides optimistic estimation good solution hence admissible. however simple relaxation behind heuristic introduce many directed cycles result loose bound. paper introduces much improved admissible heuristic named k-cycle conﬂict heuristic based additive pattern database technique main idea avoid directed cycles within small groups variables called patterns compute heuristic values concatenating patterns. also exponential-size parent graphs created bfbnb algorithms retrieve optimal parent choices during search. introduce sparse representation storing unique optimal parent sets improve time space efﬁciency search. empirical results show techniques signiﬁcantly improved efﬁciency scalability bfbnb benchmark datasets tested. remainder paper structured follows. section provides overview bayesian network structure learning shortest-path ﬁnding formulation problem. section introduces improved heuristic. section introduces sparse representation optimal parent choices discusses adapt bfbnb algorithms techniques. section reports empirical results benchmark datasets. finally section concludes paper remarks. bayesian network directed acyclic graph vertices correspond random variables arcs lack represent dependence conditional independence relations variables. relations quantiﬁed using conditional probability distributions. consider problem learning network structure dataset instantiation variables scoring function used measure goodness network structure example minimum description length scoring function uses term reward structures entropy another penalize complex structures. task optimal structure minimizes score. decomposable i.e. score structure simply scores variable. algorithms describe assume scoring function decomposable. remainder paper assumes score method equally applicable decomposable scoring functions bde. yuan formulated structure learning problem shortest-path ﬁnding problem. figure shows implicit search graph four variables. top-most node empty start search node bottom-most node complete goal node. graph represents generating successor node adding variable existing variables cost equal cost selecting optimal parent computed considering subsets i.e. search graph thus speciﬁed path start node goal ordering variables order appearance. search graph also called order graph. variable selects optimal parents preceding variables putting together optimal parent choices particular ordering generates valid bayesian network optimal speciﬁc ordering. shortest path among possible paths corresponds global optimal bayesian network. structure called parent graph retrieving costs. parent graph variable consists subsets {x}. figure shows parent graph figure shows parent graph containing scores using subset parent figure shows optimal scores propagating best scores bottom graph. score looking parent graph variable node contains various search methods well dynamic programming applied solve shortest-path ﬁnding problem search algorithm proposed based following admissible heuristic function. deﬁnition node order graph heuristic value algorithm shown much efﬁcient existing dynamic programming algorithms. however requires search information including parent order graphs stored search makes algorithm easily memory large datasets. malone developed breadth-ﬁrst branch bound algorithm search order graph layered breadth-ﬁrst order. carefully coordinating parent order graphs search information stored disk processed incrementally read back necessary. bfbnb algorithm shown efﬁcient algorithm able scale much larger datasets. theoretically scalability bfbnb algorithm limited amount disk space available. figure sample parent graph variable scores parent sets. ﬁrst line node gives parent second line gives score using parents optimal scores candidate parent set. second line node gives optimal score using subset variables ﬁrst line parents unique optimal parent sets scores. pruned parent sets shown gray. parent pruned predecessors equal better score. heuristic function deﬁned equation based classic approach designing admissible heuristics. pearl pointed optimal solution relaxed problem used admissible bound original problem. structure learning original problem learn bayesian network acyclic directed graph equation relaxes problem completely ignoring acyclicity constraint directed graphs allowed. paper aims improve heuristic enforcing partial acyclicity. ﬁrst motivate approach using small example. describe speciﬁcs heuristic. according equation heuristic estimate start node order graph allows variable choose optimal parents variables. suppose optimal parents respectively. parent choices shown directed graph figure since acyclicity constraint ignored directed cycles introduced e.g. however know ﬁnal solution cannot cycles; three scenarios possible parent parent neither true. third case dominated cases following theorem. theorem candidate parent sets bestscore bestscore. tained larger parent candidates available choose from. third case cannot provide better value cases variables must fewer parents choose from. ﬁrst cases unclear better take minimum them. consider ﬁrst case ﬁrst delete rule parent rechoose optimal parents must check parent sets including deletion alone cannot produce bound. total bound computed summing together original bound bound call total bound second case handled similarly; call total bound joint heuristic must optimistic compute minimum effectively considered possible ways break cycle obtained improved heuristic value. heuristic clearly admissible still allow cycles among variables. overlap. overlapping cycles cannot broken independently. example suppose break cycle setting parents {x}. effectively breaks cycle well introduces cycle described detail shortly divide variables non-overlapping groups focus avoiding cycles within group. different groups allowed form cycle. idea generalized compute joint heuristics groups variables size avoiding cycles within group. call resulting technique k-cycle conﬂict heuristic. note equation special case heuristic simply contains heuristics individual variables heuristic application additive pattern database technique ﬁrst explain pattern database pattern database approach computing admissible heuristic problem solving relaxed problem. example sliding tile puzzle relaxed contain tiles removing tiles. relaxation multiple states original problem mapped state abstract state space relaxed problem. abstract state called pattern exact costs solving abstract states stored pattern database; cost retrieved admissible heuristic consistent state original state space. furthermore relax problem different ways obtain multiple pattern databases. solutions relaxed problems independent problems said interactions them. consider -puzzle also relax contain tiles relaxation solved independently previous share puzzle movements. costs pattern databases added together obtain admissible heuristic hence name additive pattern databases. learning problem pattern deﬁned variables cost joint heuristic variables involved. costs patterns sharing variables added together obtain admissible heuristic decomposability scoring function. need explicitly break cycles compute k-cycle conﬂict heuristic. following theorem offers straightforward approach computing heuristic. theorem cost pattern equal shortest distance node goal node order graph. ordering variables best joint score different paths goal correspond different orderings variables among shortest path thus corresponds optimal ordering. consider example figure joint heuristic pattern equal shortest distance node goal figure therefore heuristic computed ﬁnding shortest distances nodes last layers order graph goal. describe backward search algorithm computing heuristic section furthermore difference cost pattern simple heuristic indicates amount improvement brought avoiding cycles within pattern. differential cost thus used quality measure ordering patterns choosing patterns likely result tighter heuristic. also discard pattern introduce additional improvement subset patterns. pruning signiﬁcantly reduce size pattern database improve efﬁciency accessing database. k-cycle conﬂict heuristic computed calculate heuristic value node search. node need partition non-overlapping patterns costs together heuristic value. potentially many ways partition; ideally want highest total cost represents accurate heuristic value. problem ﬁnding optimal partition formulated maximum weighted matching problem deﬁne graph vertex represents variable edge variables representing pattern containing variables edge weight equal cost pattern. goal select edges graph edges share vertex total weight edges maximized. matching problem solved time number vertices. hyperedges matching graph connecting vertices represent larger patterns. goal becomes select edges hyperedges maximize total weight. however threedimensional higher-order maximum weighted matching problem np-hard means solve np-hard problem calculating heuristic value search node. alleviate potential inefﬁciency elect greedy method compute heuristic value. method sequentially chooses patterns based quality. consider node unsearched variables ﬁrst choose pattern highest differential cost patterns subsets repeat process choosing next pattern remaining unsearched variables variables covered. total cost chosen patterns used heuristic value node version k-cycle conﬂict heuristic introduced example dynamically partitioned pattern database patterns dynamically selected search algorithm. refer dynamic pattern database short. potential drawback dynamic pattern databases that even using greedy method computing heuristic values still expensive simple heuristic equation consequently running time longer even though tighter heuristic results pruning fewer expanded nodes. resort another version k-cycle conﬂict heuristic based statically partitioned pattern database technique idea statically divide variables several groups create separate pattern database group. consider problem variables simply divide variables equal-sized groups group create pattern database contains costs subsets store hash table. refer heuristic static pattern database short. much simpler static pattern databases compute heuristic value. consider node unsearched variables divide variables patterns according static grouping. simply look costs patterns pattern databases together heuristic value node. better every search step processes variable affects pattern computing heuristic value done incrementally. computing k-cycle conﬂict heuristic solving shortest-path ﬁnding problem requires search order graph. searches require parent graphs calculated advance search. section ﬁrst introduce sparse representation parent graphs. discuss search order graph backward compute k-cycle conﬂict heuristic forward solve shortest path-ﬁnding problem adapting bfbnb algorithms. table parentsx vectors line indicates corresponding parent includes variable indicates otherwise. note that after pruning none optimal parent sets include parent graph variable exhaustively enumerates optimal scores subsets {x}. naively approach requires storing scores parent sets. theorem however number unique optimal parent sets often smaller. example figure shows score shared several nodes parent graph. parent graph representation allocate space repetitive information resulting waste space. instead storing complete parent graphs propose sparse representation sorts unique parent scores variable list also maintain parallel list stores associated optimal parent sets. call sorted lists scoresx parentsx. table shows sorted lists parent graph figure essence allows store efﬁciently process scores figure create full parent graphs realizing scores pruned example following theorem prune scores even computing pruning duplicate scores sparse representation requires much less memory storing possible parent sets scores. long kscoresk also requires less memory bfbnb several orders magnitude. approach offers memory savings compared previous best approaches. addition sparse representation also much efﬁcient create pre-pruning. table result performing bitwise operation exclude parent sets include validx vector means parent include used selecting optimal parents. ﬁrst indicates best possible score parent set. table result performing bitwise operation exclude parent sets include either vector means parent validnew includes neither initial validx vecx retor already excluded ﬁnding validnew quired excluding parents variable candidate sparse representation simply scan list starting beginning. soon ﬁrst parent subset optimal parent score. however scanning lists inefﬁcient done properly. since scanning inefﬁciency large impact whole search algorithm. therefore propose following incremental approach. initially allow variable variables candidate parents ﬁrst element sorted score list must optimal. example ﬁrst score table must bestscore. suppose remove consideration candidate parent; scan list continuing last stopped parent include must bestscore remove continue scanning list ﬁnding parent includes neither bestscore improve efﬁciency propose following efﬁcient scanning technique. variable ﬁrst initialize incumbent vector length kscoresx called validx indicates parent scores scoresx usable; ﬁrst score list optimal score. then create vectors also length kscoresx variable {x}. vector variable denoted parentsx contains parent sets contain others. table shows vectors table then exclude variable candidate parent perform operation validnew validx parentsx validnew vector contains parent sets subsets ﬁrst corresponds bestscore. table shows example excluding possible parents ﬁrst vector corresponds bestscore. exclude vector resulting last step becomes incumbent vector similar operation applied validnew validx& parentsx ﬁrst result corresponds bestscore. table demonstrates operation. also important note exclude variable time. example excluding wanted exclude rather could take validnew versions k-cycle conﬂict heuristic dynamic static pattern databases. compute dynamic pattern database breadth-ﬁrst search backward search layers order graph. search starts goal node expands order graph backward layer layer. reverse u∪{x} cost equal bestscore. reverse cost updated whenever path lower cost found. breadth-ﬁrst search ensures node obtain optimal reverse cost whole layer processed. corresponding pattern pruned differential score equal subset pattern. otherwise added pattern database together pattern cost differential cost. static pattern databases calculated differently. static grouping need compute pattern database group basically full order graph containing subsets also backward breadth ﬁrst search create graph layer layer starting node however cost reverse order graph bestscore bfbnb algorithms utilize heuristic sparse parent graphs. originally algorithm ﬁrst creates full parent graphs expands order graph best-ﬁrst order starting top. improved version ﬁrst create unique score lists k-cycle conﬂict heuristic. search difference appears generalgorithm stores scores possible parent sets variables. bfbnb memory-efﬁcient dynamic programming store possible parent sets layer parent graphs variables size largest layer parent graphs indication space requirement. sparse representation stores unique optimal parent sets variables layers. figure shows memory savings sparse representation. number unique scores stored sparse representation typically several orders magnitude smaller number parent sets stored full representation. results agree quite well previously published results theorem increasing number data records increases maximum number candidate parents. therefore number unique candidate parent sets increases number records increases; however many parent sets pruned. number variables also affects number candidate parent sets. consequently number unique scores increases function number records number variables. amount pruning data-dependent though easily predictable. practice number records affect number unique scores number variables. scoring functions exhibit similar behavior. results also suggest savings increase number variables increases datasets. implies that variables necessarily increases number possible parent sets exponentially number unique optimal parent sets increases much slowly. intuitively even though parents small number good parents particular variable. heuristic sparse representation used improve bfbnb algorithms. beneﬁcial understanding much improvement technique contributes. also heuristic versions static dynamic pattern databases; parameterized different ways. applied various parameterizations techniques algorithms datasets autos flag. dynamic pattern database varied static pattern databases tried groupings autos dataset groupings flag dataset. results shown table sparse representation helped bfbnb algorithms achieve much better efﬁciency scalability. memory datasets using full parent graphs able solve autos achieved using sparse parent graphs. bfbnb algorithm affected similar way. originally works coordinating expansion order graph parent graphs layer layer. improved version unique score lists heuristic calculated ﬁrst. search part algorithm needs expand order graph generating successors works similarly improved algorithm. tested techniques bfbnb algorithms comparing original versions. experiments performed intel processor hard disk space running ubuntu used benchmark datasets machine learning repository test algorithms. datasets records missing values removed. variables discretized states around means. ﬁrst evaluated memory savings made possible using sparse representation comparison full parent graphs. particular compared maximum number scores stored variables algorithm. typical dynamic programming table comparison enhanced bfbnb algorithms various combinations parent graph representations heuristics size means number patterns stored; sparse means sparse parent graphs; full means full parent graphs; time means running time nodes means number nodes expanded algorithms; means algorithm fail ﬁnish within -hour time limit experiment; means algorithm used full included memory cases. heuristic) flag using sparse parent graphs. similarly bfbnb time flag dataset within hour time limit using full parent graphs able solve dataset using sparse representation autos sparse representation helped improve time efﬁciency bfbnb order magnitude. last note numbers expanded nodes bfbnb slightly different using representations; randomness local search method used compute initial upper bound solution bfbnb. static dynamic pattern databases helped bfbnb algorithms improve efﬁciency scalability. simple heuristic static pattern database grouping memory flag dataset. pattern database heuristics enabled ﬁnish successfully. dynamic pattern database helped reduce number expanded nodes signiﬁcantly algorithms datasets. setting helped even more. however increasing often resulted increased running time sometimes increased number expanded nodes well. believe larger always results better heuristic; occasional increase expanded nodes greedy strategy used choose patterns fully utilize larger pattern database. longer running time reasonable though less efﬁcient compute heuristic value larger pattern databases inefﬁciency gradually overtook beneﬁt brought better heuristic. therefore seems best parametrization dynamic pattern database general. static pattern databases able test much larger groups need enumerate groups certain size. results suggest larger groupings tend result tighter heuristic. sizes static pattern databases typically much larger dynamic pattern databases. however still negligible comparison number expanded search nodes cases. thus cost effective compute larger affordable-size static pattern databases achieve better search efﬁciency. results show best static pattern databases typically helped bfbnb achieve better efﬁciency best dynamic pattern database even number expanded nodes larger. reason calculating heuristic values efﬁcient using static pattern databases. since static pattern databases seem work better dynamic pattern databases cases tested bfbnb using static pattern database sparse representation datasets original algorithms. used simple static grouping datasets number variables. results shown table bfbnb algorithm improved version around times faster original version sometimes even orders magnitude faster reduction number expanded nodes dratable comparison number nodes expanded running time bfbnb algorithms enhanced static pattern database grouping number variables sparse representation parent scores original versions algorithms. total number variables number data points. matic however. main reason original bfbnb algorithm interleaves expanding order graph full parent graphs search improved version ﬁrst calculates sparse representation parent scores performs search. much efﬁcient compute sparse representation computing full parent graphs. however dataset sensor readings improved bfbnb algorithm runs slower original version. potential explanations. first particular dataset large number data points makes sparse representation truly sparse. second heuristic seems much tighter simple heuristic dataset numbers expanded nodes similar cases. beneﬁts techniques obvious applied datasets original algorithm able ﬁnish improved algorithm order magnitude faster; number expanded nodes also signiﬁcantly reduced. addition able solve three larger datasets sensor readings autos flag. running time datasets pretty short indicates memory consumption parent graphs reduced algorithm able memory order graph solved search problems rather easily. orthogonal directions research natural. development search algorithms learning optimal bayesian networks represented bfbnb algorithms developed contribution paper sparse representation parent graphs store unique optimal parent sets scores. method improves time space efﬁciency parent graph part search thus falls ﬁrst direction. second direction believe equally important development search heuristics. another contribution paper admissible heuristic called k-cycle conﬂict heuristic developed based additive pattern databases. tested bfbnb algorithms enhanced heuristic sparse representation machine learning datasets. results show techniques contributed signiﬁcant improvement efﬁciency scalability algorithms. therefore believe methods represent another signiﬁcant step forward exact bayesian network structure learning. future work plan investigate better approaches obtaining groupings static pattern databases. could based prior knowledge initial estimation correlation variables. groupings expected work better simple grouping tested paper. parviainen koivisto exact structure discovery bayesian networks less space. proceedings twenty-fifth conference uncertainty artiﬁcial intelligence. montreal quebec canada auai press. silander myllymaki simple approach ﬁnding globally optimal bayesian network structure. proceedings annual conference uncertainty artiﬁcial intelligence arlington virginia auai press. teyssier koller ordering-based search simple effective algorithm learning bayesian networks. proceedings twenty-first conference annual conference uncertainty artiﬁcial intelligence arlington virginia auai press. tian branch-and-bound algorithm learning bayesian networks. proceedings conference uncertainty artiﬁcial intelligence morgan kaufmann publishers inc.", "year": 2012}