{"title": "Learning Bayesian Networks from Incomplete Databases", "tag": ["cs.AI", "cs.LG"], "abstract": "Bayesian approaches to learn the graphical structure of Bayesian Belief Networks (BBNs) from databases share the assumption that the database is complete, that is, no entry is reported as unknown. Attempts to relax this assumption involve the use of expensive iterative methods to discriminate among different structures. This paper introduces a deterministic method to learn the graphical structure of a BBN from a possibly incomplete database. Experimental evaluations show a significant robustness of this method and a remarkable independence of its execution time from the number of missing data.", "text": "bayesian approaches learn graphical structure databases share database complete ported unknown. attempts relax assumption involve expensive erative methods discriminate ferent structures. deterministic structure database. experimental significant remarkable independence execution time bayesian belief direct acyclic graph nodes represent variables arcs represent among variables. child variable parent variables fined conditional able given configurations although original concept rely human experts provide graphical structure dur­ past years increasing number efforts meth­ addressed toward development able directly early results quest based bayesian approaches seminal cooper herskovitz gave rise stream research within bayesian framework along proach learning process involves main tasks induction methods perform first task known model selection procedure explore space possible graphical models scoring metric assess goodness-of­ particular exploit heuristics reduce search space scor­ metric drive search process. although database known task extracting np-hard general case certain assumptions meth­ able extract quite large bbns databases thousands cases. assumptions database complete entry database reported unknown. reason assumption step bayesian learning process computation database given graph­ marginal likelihood ical model. computation performed ficiently database complete using exact bayesian updating becomes intractable data missing. therefore mate marginal likelihood data used. current approaches exploit algorithm markov chain monte carlo methods gibbs sampling basic strategy underlying methods based missing information available information. replacing hood estimates extracted proceeds iteratively stability sampling generates conditional estimation state denote x;k> l\";j kij· suppose given wish select model conditional among variables approach database. prior belief particular model com­ information pute posterior ramoni sebastiani ministic ities defining rely missing method called bounding possible available collapses combination pending intuition behind database induces pattern single missing data source information information available show clearly pro­ timates data missing vided gibbs sampling random robust departure true pattern data. hand reduces bution cost exact convex combination paper describes marginal thus extending task learning task extracting database. incomplete structured follows nical background section section imental evaluation evant results. defined variables direct acyclic model conditional variables. able parent variables conditional given configurations shall consider discrete number states states greater herskovitz puted conditional probabilities regarded observer's bution represents data. joint probability written depends posterior bility induce model database. variables current thus empty root node. local contribution node parents joint probability algorithm computing include bution stops probability greedy search strategy increase shown extremely cost-effective number variables complete hyper-parameters number missing entries 'd;) approach creases returns database number missing jijk estimate creases prior probability method coherent data totally nii' instance child variable missing o.ijk jijknij a.;j although data missing limited used encode pattern instance erating pattern thermore tion available extreme lead lower bound minl{pz. 'd;)} there­ fore interval tains posterior tained denoting number possible comple­ cases rij· number tions incomplete missing entries increases estimate tends cases distributed accord­ prior belief parameters defining bbn. considered table models generated random sample cases applied algorithm induction struc­ order consistent ture assuming uniform prior distributions deleted parameters. sample random database empty. incomplete database system duce model data. algorithm takes input database together variables induction strategy replaces mate graphical model conditional method. method implemented com­ lisp experiments performed macintosh tables show models induced models databases generated estimates -logp different percent­ ages available seconds taken extract graphical mate parameters marginal probabilities bles initial cases models learned database generated correct ones entries database available model independence database. entries duced models differ generating link. times show remarkable percentage table gives estimates step algorithm. -log entries available log.§ model induced incomplete structure times likely generating eight possi­ assume prior distribution order models consistent uniform. strong evidence model used generate database fact generating becomes entries deleted small num­ entries cause imprecision mate -logp. conditional timated model selected dif­ timate marginal probability fers estimate obtained complete database entries avail­ able -log -log.§ model induced data times likely generating marginal probabilities network similar marginal probabili­ ties found model induced complete database thus choice slightly little effect predicting power network. table gives estimate -logp eight possible ordering estimates computed variables. values table adding relevant estimates retained. error estimate model induced erating models limited order pri­ equally compute four incomplete pute posterior posterior database probability models posterior probabilities near entries duced model structure. similar found models induced database generated models duced databases entries respectively likely structure. bilities plete database slightly soning process. accurate entries. source complexity performances number missing realize depend number missing number missing dure described effect limited independencies tion trees store counters number entries extra dependencies exception database generated available. learned however dif­ conditional probabilities slightly ferent ities extremely missing plications cur­ rent methods learn bbns incomplete rely iterative methods sampling marginal model fundamental step process graphical structure paper introduced provide graphical structure incomplete database. perimental method remarkable execution", "year": 2013}