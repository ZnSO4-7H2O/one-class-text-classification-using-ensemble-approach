{"title": "High Order Recurrent Neural Networks for Acoustic Modelling", "tag": ["cs.CL", "cs.AI", "eess.AS", "stat.ML"], "abstract": "Vanishing long-term gradients are a major issue in training standard recurrent neural networks (RNNs), which can be alleviated by long short-term memory (LSTM) models with memory cells. However, the extra parameters associated with the memory cells mean an LSTM layer has four times as many parameters as an RNN with the same hidden vector size. This paper addresses the vanishing gradient problem using a high order RNN (HORNN) which has additional connections from multiple previous time steps. Speech recognition experiments using British English multi-genre broadcast (MGB3) data showed that the proposed HORNN architectures for rectified linear unit and sigmoid activation functions reduced word error rates (WER) by 4.2% and 6.3% over the corresponding RNNs, and gave similar WERs to a (projected) LSTM while using only 20%--50% of the recurrent layer parameters and computation.", "text": "lstm recently become dominant type recurrent architecture. however lstms extra parameters associated gating four times parameters standard rnns hidden layer size signiﬁcantly increases storage computation training testing. paper propose another modiﬁcation high order alternative lstm. handles vanishing gradients adding connections hidden state values multiple previous time steps input. interpreting layer hidden vector continuous valued hidden state connections termed high order since introduce dependencies multiple previous hidden states. acoustic modelling using hornns investigated sigmoid relu activation functions. sigmoid case found additional high order connections beneﬁcial. furthermore analogous projected lstm linear recurrent projection layer used hornns reduce number parameters results projected hornn experimental results show hornn/hornnp similar word error rates lstm/lstmp models hidden vector size using fewer half parameters computation. furthermore hornns also found outperform rnns residual connections terms speed wer. paper organised follows. section reviews lstm models. markov property rnns described sec. leads hornns architectures sigmoid relu activation functions. experimental setup results given sec. sec. followed conclusions. weights bias activation function input activation value. general processed number layers obtain ﬁnal network output. well known sigmoid denoted rnns suffer vanishing gradient issue since vanishing long-term gradients major issue training standard recurrent neural networks alleviated long short-term memory models memory cells. however extra parameters associated memory cells mean lstm layer four times many parameters hidden vector size. paper addresses vanishing gradient problem using high order additional connections multiple previous time steps. speech recognition experiments using british english multi-genre broadcast data showed proposed hornn architectures rectiﬁed linear unit sigmoid activation functions reduced word error rates corresponding rnns gave similar wers lstm using recurrent layer parameters computation. recurrent neural network artiﬁcial neural network layer hidden layer outputs previous time step form part input used process current time step allows information preserved time well suited sequence processing problems acoustic language modelling automatic speech recognition however training rnns sigmoid activation functions gradient descent difﬁcult. issues exploding vanishing gradients i.e. long-term gradients back-propagated time either continually increase decrease zero. causes training either fail capture long-term temporal relations standard update steps parameters range. many methods proposed solve gradient exploding vanishing problems. simple gradient clipping found work well practice prevent gradient exploding circumventing vanishing gradients normally requires sophisticated strategies instance uses hessian-free training makes second-order derivative information. modifying recurrent layer structure another approach. rectiﬁed linear unit sigmoid activation functions trainable amplitudes proposed maintain magnitude long-term gradients gating technique used long short-term memory model additional parameters implement memory circuit remember longterm information recurrent layer model similar lstm gated recurrent unit recently additional residual highway connections proposed train deep feed-forward models allows gradients pass easily many layers. various similar ideas applied recurrent models among approaches paper gradient vanishing issue tackled relaxing ﬁrst-order markov conditional independence constraint. hence direct preceding state also previous states ht−n used calculating adds additional high order connections architecture results hornn. training perspective including high order states creates shortcuts backpropagation allow additional long-term information easily. speciﬁcally gradients w.r.t. general n-order obtained training criterion. eqn. sums multiple terms prevent gradient vanishing. inference perspective assumes sufﬁcient past temporal information embedded representation using ﬁxed sized means information distant long-term steps properly integrated short-term information. hornn architecture allows direct access past long-term information. many alternative ways using ht−n calculation hornn framework. paper assumes high order connections linked input step found sufﬁcient high order connection input i.e. ht−n viewed kind memory whose temporal resolution modiﬁed experiments structure eqn. allowed relu hornns give similar wers lstms. however using sigmoid hornns slightly different structure needed reach similar wer. extra high order connection ht−m sigmoid function input i.e. here ht−m directly added sigmoid input without impacting temporal resolution since ht−m previous sigmoid output. eqns. used relu sigmoid hornns throughout paper. comparing eqns. eqn. hornn increases number layer parameters sizes method reduce increase parameters project hidden state vectors dimension recurrent linear projection factorises eqns. upnp low-rank approximation. projected hornns relu sigmoid activations hence deﬁned step cell candidate created encode information current step. ﬁrst updated interpolating based forget gate input gate converted lstm hidden state transforming hyperbolic tangent scaling output gate procedure simulates memory circuit analogous logic gates speciﬁcally lstm layer step evaluated tanh tanh represents element-wise product matrices diagonal serve peephole. although lstms work well large variety tasks computationally expensive. representations temporal step extracted however additional cost ﬁnding requires three times computation parameter storage since need calculated. eqn. st-order markov conditional independence property means current state depends immediately preceding state current input property differs st-order markov property also conditioning note property also applies bidirectional rnns easy show deﬁning hidden state {hfwd hbid forward backward hidden states. projected lstm factorise reduces number lstm parameters dhdp next compare computational complexity lstms hornns. given multiplying matrix matrix requires multiply-adds ignoring element-wise operations testing complexity hornnp layer odh) whereas lstmp odh). shows hornnps less calculations lstmps. found hornnps often result speed lstmps current implementation independently developing hornn acoustic modelling found similar ideas previously applied rather different tasks however research focus model architectures different paper. particular model proposed equivalent eqn. without subsampling high order hidden vectors applied model timit phone recognition. furthermore previous studies didn’t discuss high order connections markov property framework. another related model recent residual memory network viewed unfolded hornn deﬁned eqn. zero distinct untied parameters unfolded layer positive integer. addition since highway networks viewed generalised form residual networks highway rnns lstms also related work note also possible combine residual highway ideas hornns increasing recurrent depth. proposed hornn models evaluated training systems multi-genre broadcast data speech recognition challenge task audio programmes covering range genres. hour full training selected episodes sub-titles phone matched error rate compared lightly supervised output used training supervision. hour subset sampled utterance level set. word vocabulary used trigram word level language model estimated acoustic transcripts separate million word subtitle archive. test devb contains hours audio data manually segmented utterances episodes shows. subset ofﬁcial full development data overlaps training test sets excluded. system outputs evaluated confusion network decoding well -best viterbi decoding. experiments conducted extended version lstm implemented following log-mel ﬁlter bank analysis used expanded vector coefﬁcients. data normalised utterance level mean show-segment level variance inputs recurrent model time step single frames delayed steps models trained using crossentropy criterion frame-level shufﬂing used. recurrent models unfolded time steps gradients shared parameters normalised dividing sharing counts maximum parameter changes constrained update value clipping threshold minibatch samples. decision tree clustered triphone tied-states along gmm-hmm/dnn-hmm system training alignments used training sets. hidden layer dimension added recurrent output layers models. newbob+ learning rate scheduler used train models setup previous systems initial learning rate used relu models initial rate used train models. since regularisation plays important role rnn/lstm training weight decay factors carefully tuned maximise performance system. initial experiments studied various hornn architectures order investigate suitable values relu model eqn. sigmoid model eqn. save computation subset used training. models recurrent layer size ﬁxed lstm standard created baselines parameters recurrent layers respectively. resrnn deﬁned eqn. also tested additional baseline using relu sigmoid functions. resrnns number parameters hornns. note rather standard case examined falls high order framework hornns tested; ﬁxed sigmoid hornns. results shown figure lstm gives lower wers standard relu resrnn similar lstm. relu hornns gave wers least lstm best relu resrnn systems. sigmoid hornns gave better wers sigmoid resrnns similar wers lstm. performance improved using p-sigmoid hornn activation function associates linear scaling factor recurrent layer output unit makes similar relu. addition hornns faster lstm resrnns. resrnns slightly slower hornns since second matrix multiplication depends ﬁrst recurrent step. rest experiments relu hornns used sigmoid hornns used next projected lstms projected hornns compared. first ﬁxed respectively single recurrent layer lstmp hornnp models. lstmp baseline parameters hornnp system parameters. table hornnps similar wers lstmp. reducing hornn systems reduced number parameters gave full set. increased number recurrent layer parameters better model full training set. table single recurrent layer recurrent layer architectures hornns still produced similar wers corresponding lstmps. validates previous ﬁnding larger data proposed hornn structures work well widely used lstms acoustic modelling using fewer parameters. addition along multi-layered structure hornns also applied kinds recurrent models replacing rnns lstms bidirectional grid structures etc. finally layer sigmoid system paper proposed hornns acoustic modelling address vanishing gradient problem training recurrent neural networks. different architectures proposed cover relu sigmoid activation functions. yielded reductions standard rnns activation function. furthermore additional structures investigated reducing number hornn parameters linear recurrent projected layer; adding another recurrent layer. cases compared projected lstms residual rnns shown hornns gave similar performance signiﬁcantly efﬁcient computation storage. savings parameter number computation used implement wider deeper recurrent layers hornns gave relative reduction comparable lstms values hornns increased respectively make overall number recurrent layer parameters closer lstm produced system lstmp also modiﬁed parameters. results table margin since representations embed accurate temporal information p-sigmoid function used hornnps since linear projection layer also scales finally lstmp hornnp compared stacking another recurrent layer. recurrent layers hornnp systems parameters still produced similar wers lstmp system results indicate rather spending calculations maintaining lstm memory cell effective hornns computational budget extracting better temporal representations using wider deeper recurrent layers. m.k. baskar karaﬁ´at burget vesel´y gr´ezl j.h. ˇcernock´y residual memory networks feed-forward approach learn long-term temporal dependencies proc. icassp orleans bell m.j.f. gales hain kilgour lanchantin mcparland renals wester p.c. woodland challenge evaluating multi-genre broadcast media transcription proc. asru scottsdale lanchantin m.j.f. gales karanasou qian wang p.c. woodland zhang selection multi-genre broadcast data training automatic speech recognition systems proc. interspeech francisco p.c. woodland qian zhang m.j.f. gales karanasou lanchantin wang cambridge university transcription systems multi-genre broadcast challenge proc. asru scottsdale", "year": 2018}