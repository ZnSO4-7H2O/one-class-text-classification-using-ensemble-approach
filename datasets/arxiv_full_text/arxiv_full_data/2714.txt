{"title": "Stochastic Optimization with Bandit Sampling", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Many stochastic optimization algorithms work by estimating the gradient of the cost function on the fly by sampling datapoints uniformly at random from a training set. However, the estimator might have a large variance, which inadvertently slows down the convergence rate of the algorithms. One way to reduce this variance is to sample the datapoints from a carefully selected non-uniform distribution. In this work, we propose a novel non-uniform sampling approach that uses the multi-armed bandit framework. Theoretically, we show that our algorithm asymptotically approximates the optimal variance within a factor of 3. Empirically, we show that using this datapoint-selection technique results in a significant reduction in the convergence time and variance of several stochastic optimization algorithms such as SGD, SVRG and SAGA. This approach for sampling datapoints is general, and can be used in conjunction with any algorithm that uses an unbiased gradient estimation -- we expect it to have broad applicability beyond the specific examples explored in this work.", "text": "many stochastic optimization algorithms work estimating gradient cost function sampling datapoints uniformly random training set. however estimator might large variance inadvertantly slows convergence rate algorithms. reduce variance sample datapoints carefully selected non-uniform distribution. work propose novel non-uniform sampling approach uses multiarmed bandit framework. theoretically show algorithm asymptotically approximates optimal variance within factor empirically show using datapoint-selection technique results signiﬁcant reduction convergence time variance several stochastic optimization algorithms saga. approach sampling datapoints general used conjunction algorithm uses unbiased gradient estimation expect broad applicability beyond speciﬁc examples explored work. coordinates learning parameters. ﬁrst term mean convex functions called sub-cost functions second product convex regularizer regularization parameter sub-cost function parameterized datapoint denotes feature vector label. examples common sub-cost functions include l-penalized logistic regression logxi l-penalized w)]+) ridge regression gradient descent variants form classic often effective methods solving however minimize using gradient descent iteration needs gradient calculations must computed which large prohibitively expensive stochastic gradient descent reduces computational complexity iteration sampling datapoint uniformly random time step computing gradient datapoint; ∇φit unbiased estimator however estimator large variance negatively affects convergence rate underlying optimization algorithm requires increased number iterations. classes stochastic optimization algorithms proximal reducing variance improves speed convergence optimal coordinate motivated development several techniques reduce variance using previous information reﬁne estimation gradient; e.g. occasionally calculating using full gradient reﬁne estimation using previous calculations another technique closely related work sample non-uniform distribution sampling datapoint time proportional ∇φi. example probability non-uniform sampling according update rule second depend denote term one’s control sufﬁces minimize minimum ∇φis similar magnitudes thus attained close uniform distribution. however magnitude datapoint comparatively large optimal distribution uniform; case optimal effective variance roughly times smaller effective variance using uniform distribution. optimal probabilities given gradients unknown? question approached minimizing upper bound results time-invariant distribution method known importance sampling however drawback method upper-bound loose hence optimal distribution. moreover requires computation upper-bound computationally expensive. contributions work inspired active learning methods adaptive approach deﬁne instead ﬁxing advance. datapoints selected ﬁrst iterations {it}≤t≤ refer corresponding gradients {∇φit}≤t≤ feedback deﬁne problem best deﬁne distribution given feedback falls framework multi-armed bandit problems. call approach multi-armed bandit sampling show approach gives distribution asymptotically close optimal. theorem argminp distribution optimizes effective variance iterations. distributions selected mabs. gradients bounded mabs approximates optimal work denote euclidian norm note d-dimensional random vector general hence strictly speaking variances entries. although simply called varaince term pseudo-variance distinguish term variance. emphasize mabs used conjunction algorithm uses unbiased gradient estimation reduce variance estimation sgd. includes saga svrg prox_sgd quasi_newton methods. present empirical performance methods paper summary main contributions recasting problem reducing variance stochastic optimization multi-armed bandit providing sampling algorithm analysis rate convergence optimal illustrating convergence rates stochastic optimization algorithms exhibiting signiﬁcant improvements practice yielded selecting datapoints using mabs goal mabs sampling distribution minimizes effective variance thus pseudo-variance stochastic optimization unbiased estimator effective algorithms ∇φit /npt eˆg]. however consider broader class stochastic optimization variance algorithms arms selecting time gives negative reward losses vary among arms. time algorithm updates sampling distribution based loss selected time arms setting update sampling distribution access losses computed sampled gradient ∇φi. probability selecting based time optimal distribution minimizes cumulated loss rounds. cost function time wants minimize. observe ﬁrst term right-hand side cost function adversarial ∇ivt arm/datapoint distribution time optimal sampling distribution given building analogy datapoint sampling propose mabs algorithm based mabs algorithm weights i}≤i≤n initialized distribution weighted t}≤i≤n time uniform distribution {/n}≤i≤n average distribution i.e. deviates uniform distribution. mabs updates weight selected datapoint time according updating rule remark difference variance-reduction problem rewards assumed upper bounded almost surely. however rewards might unbounded depending distribution occurs probability close making term using results lemma upper bound lower bound potential function although second term right-hand side increases effective variance note scales needed hence need compute supt{at required. computation gradient exactly whereas exact value supt{at requires computations computational overhead mabs insigniﬁcant small compared coordinate dimension case datasets table used evaluation section condition might prohibitive large. however relax condition expense slightly worse bound remark know supt{at idea that instead mixing distribution t}≤i≤n non-uniform distribution instead variant algorithm). combining mabs stochastic optimization algorithms section restate known convergence guarantees psgd order highlight impact effective variance them. upper-bounds convergence guarantees depend effective variance using sampling distribution given mabs effective variance reduced results improved convergence guarantees. recall algorithms unbiased estimator gradient ∇φit /npit hence eˆg]. known convergence rate restated terms effective variance follows. ﬁrst term increases second term n/e)] increases meaning dominant convergence guaranty large uniform much larger hence convergence guaranty poor comparing effective variance small good estimator) expect stable algorithm i.e. choose larger step size without diverging. assume φi/n i.e. reguralizer l-smooth. using smoothness property lwt+ γl∇φit taking expectations conditionally e|wt] −γ∇f guarantee cost function decreases need afford larger step size test stability mabs range section show signiﬁcant stability compared sgd. psgd psgd function µ-strongly convex l-smooth respect continuously differentiable function bregman divergence associated function psgd updates according figure study different sampling methods comparing convergence effective variance function observe lowest mabs used effect increases signiﬁcantly intuitively method works minimizing ﬁrst-order approximation function plus regularizer non-uniform version algorithm ∇φit replaced ∇φit/) assume σ-strongly convex µ-strongly convex l-smooth respect convex. then following inequality holds psgd similar holds distribution hence expect using mabs small effective variance psgd better convergence guarantee. study convergence guarantee psgd used mabs stability left future work. evaluate performance mabs conjunction several stochastic optimization algorithms address question much bandit-based sampling help? towards this compare performance several stochastic optimization algorithms mabs compared uniform importance sampling versions. must ﬁrst deﬁne appropriate unbiased estimator algorithm. particular compare following algorithms present necessary deﬁnitions mabs stochastic gradient descent proximal stochastic variance-reduced gradient −∇φit )/∇φi−∇φi/n deﬁned follows. time divided bins size updated beginning bin. wt/n details improved version algorithm). ∇φi−∇φi/n gradient sub-cost function last time datapoint chosen details). empirical results synthetic data discussed section beneﬁt mabs depend similar ∇φis are. smoothness parameter sub-cost functions li/n average-smoothness lm/¯l ratio. observed large expect non-uniform sampling dataset. datasets datapoints features. labels deﬁned coefﬁcient hyperplane generated gaussian distribution mean standard deviation gaussian noise mean variance features generated gaussian distribution whose mean variance generated randomly. order obtain different choose datapoint largest smoothness multiply entire feature vector number whereas labels features remain ﬁxed. increases hence sub-cost function used xiw−yi)/ i.e. ridge regression algorithms step size experiment iterations repeated times. report effective variance iteration difference values found three sampling versions value found gradient descent compare stochastic algorithms ideal gradient descent. results. figure observe mabs best performance three sampling methods value sgd_mabs closest additionally increases performance sgd_mabs improves conﬁrming intuition datapoint large gradient convergence mabs optimal sampling distribution faster. hand expected performance sgd_u degrades signiﬁcantly sgd_is appear affected advantage mabs strongest large figure depicts effective variance ﬁnal iteration function similar observations made. particular effective variance sgd_mabs lowest decreasing effective variance sgd_is sgd_u non-decreasing increasing respectively. empirical results real-world data consider classiﬁcation datasets ijcnn classes. prox-svrg saga compare effect different sampling methods. report value reached three sampling versions stochastic optimization algorithms above function number iterations cost function used l-penalized logistic regression i.e. logxi experiment iterations repeated times. experiments step sizes except experiments prox_svrg larger step size used. results depicted figure again stochastic optimization algorithms mabs consistently best among algorithms. comparing results datasets ijcnn mabs helpful conﬁrms intuition mabs improves convergence rate dataset larger figure results different sampling methods similar other might fact prox_svrg variance reduction technique efﬁcient non-uniform sampling technique. whereas figure mabs still efﬁcient improving convergence rate saga varaince reduction technique also tested mabs conjunction quasi_newton algorithm step size sgd_mabs times closer optimal value uniform sampling. quasi_newton algorithm step size quasi_newton_mabs closer optimal value quasi_newton. figure comparison three different stochastic optimization algorithms datasets using different sampling methods. mabs never suboptimal often signiﬁcantly outperforms sampling methods. figure comparison three different stochastic optimization algorithms using different sampling methods different step sizes mabs signiﬁcantly outperforms methods able optimal value even large stability following discussion section study robustness prox-svrg saga using large step size. particular consider dataset l-penalized logistic regression above. collect value ﬁnal iteration different stochastic optimization algorithms conjunction different sampling methods ﬁxed step size experiment repeated times. results depicted figure show mabs indeed robust sampling method; sgd_mabs able optimal coordinate whereas sgd_is diverge figure difference three sampling methods less still prox_svrg_mabs outperforms others. saga_mabs also robust saga sampling methods able optimal coordinate diverges training time brieﬂy note adding mabs cost much respect training time. example given high-dimensional data empirically sgd_mabs uses clock-time sgd. contrast sgd_is uses clock-time slow simulations terminate. conclusion future work work novel sampling method presented reduce variance gradient estimation. method inspired multi-armed bandit algorithms require preprocessing. first variance unbiased estimator gradient iteration deﬁned function sampling distribution gradients sub-cost functions ∇φi. next considering past information mabs minimizes cost function appropriately updating learns optimal distribution given selected datapoints {it}≤t≤t gradients {∇φit}≤t≤t shown natural assumption mabs asymptotically approximate optimal variance within factor moreover mabs combined three stochastic optimization algorithms tested real data. observe effectiveness variance reduction rate convergence optimization algorithms compared sampling approaches. furthermore mabs tested synthetic datasets effectiveness observed large range also observed sgd_mabs signiﬁcantly stable sampling methods. several important directions remain open. first would like improve constants bound theorem secondly although observe robustness ﬁnding optimal step size prox_svrg saga remains open. lastly could interest extend work stochastic optimization methods providing theoretical guarantees observing performance practice. omar besbes yonatan assaf zeevi. stochastic multi-armed-bandit problem non-stationary rewards. advances neural information processing systems pages aaron defazio francis bach simon lacoste-julien. saga fast incremental gradient method support non-strongly convex composite objectives. advances neural information processing systems pages mark schmidt reza babanezhad mohamed ahmed aaron defazio clifton anoop sarkar. non-uniform stochastic average gradient method training conditional random ﬁelds. artiﬁcial intelligence statistics pages proof proof uses approach proofs multiplicative-weight update algorithms adapt using lemma proof based upper bounding lower bounding potential function ﬁnal iteration reward then update rule datapoint unbiased estimator weight potential function computational complexity mabs similar mabs requires memory size sore weights iteration weight updated. want update probabilities iteration mabs needs computations expensive. however tree structure reduce computational complexity sampling updating mabs similar assume compute bounds supt{at exactly reﬁne algorithm improve results. idea that instead mixing distribution t}≤i≤n distribution remark note results derived case want approximation optimal solution ﬁxed optimal i.e. minp however additional assumptions improve results perform close optimal assumptions bounds variation algorithm parallels algorithm resetting phase reset weights number iterations. precisely time divided bins. beginning reset algorithm size chosen variation across large. hence know upper bounds effective variance mabs optimal effective variance. therefore compare effective variance mabs focus second term i.e. focus following term similar discussion section consider extreme scenarios. becomes compared others i.e. ai/a becomes beneﬁt mabs exceeds ratio number datapoints number iteration small bounds magnitude gradients greatly varying.", "year": 2017}