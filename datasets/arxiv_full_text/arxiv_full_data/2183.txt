{"title": "Recurrent Environment Simulators", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Models that can simulate how environments change in response to actions can be used by agents to plan and act efficiently. We improve on previous environment simulators from high-dimensional pixel observations by introducing recurrent neural networks that are able to make temporally and spatially coherent predictions for hundreds of time-steps into the future. We present an in-depth analysis of the factors affecting performance, providing the most extensive attempt to advance the understanding of the properties of these models. We address the issue of computationally inefficiency with a model that does not need to generate a high-dimensional image at each time-step. We show that our approach can be used to improve exploration and is adaptable to many diverse environments, namely 10 Atari games, a 3D car racing environment, and complex 3D mazes.", "text": "models simulate environments change response actions used agents plan efﬁciently. improve previous environment simulators high-dimensional pixel observations introducing recurrent neural networks able make temporally spatially coherent predictions hundreds time-steps future. present in-depth analysis factors affecting performance providing extensive attempt advance understanding properties models. address issue computationally inefﬁciency model need generate highdimensional image time-step. show approach used improve exploration adaptable many diverse environments namely atari games racing environment complex mazes. order plan effectively agent-based systems require ability anticipate consequences actions within environment often extended period future. agents equipped ability access models simulate environments changes response actions. need environment simulation widespread psychology model-based predictive abilities form sensorimotor contingencies seen essential perception neuroscience environment simulation forms part deliberative planning systems used brain reinforcement learning ability imagine future evolution environment needed form predictive state representations monte carlo planning simulating environment requires models temporal sequences must possess number properties useful models make predictions accurate temporally spatially coherent long time periods; allow ﬂexibility policies action sequences used. addition models general-purpose scalable able learn high-dimensional perceptual inputs diverse realistic environments. model achieves desiderata empower agent-based systems vast array abilities including counterfactual reasoning intuitive physical reasoning model-based exploration episodic control intrinsic motivation hierarchical control. deep neural networks recently enabled signiﬁcant advances simulating complex environments allowing models consider high-dimensional visual inputs across wide variety domains model represents state-of-the-art area demonstrating high long-term accuracy deterministic discrete-action environments. despite advances still several challenges open questions. firstly properties simulators terms generalisation sensitivity choices model structure training poorly understood. secondly accurate prediction long time periods future remains difﬁcult achieve. finally models computationally inefﬁcient since require prediction high-dimensional image time action executed unnecessary situations agent interested ﬁnal prediction taking several actions. paper advance state-of-the-art environment modelling. build work develop alternative architectures training schemes signiﬁcantly improve performance provide in-depth analysis advance understanding properties figure graphical model representing recurrent structure used recurrent structure. filled empty nodes indicate observed hidden variables respectively. models. also introduce simulator need predict visual inputs every action reducing computational burden model. test simulators three diverse challenging families environments namely atari games ﬁrst-person game agent moves randomly generated mazes racing environment; show used model-based exploration. recurrent environment simulators environment simulator model that given sequence actions corresponding observations environment able predict effect subsequent actions forming predictions state representations environment. starting point recurrent simulator state-of-the-art simulating deterministic environments visual observations discrete actions. simulator recurrent neural network following backbone structure equation hidden state representation environment non-linear deterministic state transition function. symbol indicates selection predicted frame ˆxt− real frame producing types state transition called prediction-dependent transition observation-dependent transition respectively. encoding function consisting series convolutions decoding function combines state action multiplicative interaction transforms using series full convolutions form predicted frame ˆxt. model trained minimise mean squared error observed time-series corresponding evolution environment prediction. probabilistic framework corresponds maximising log-likelihood graphical model depicted fig. graph link represents stochastic dependence formed adding gaussian noise term zero mean unit variance whilst remaining links represent deterministic dependences. dashed lines indicate links active depending whether state transition prediction-dependent observation-dependent. model trained using stochastic gradient descent mini-batch consists segments length randomly sub-sampled segment minibatch model uses ﬁrst observations evolve state forms predictions last observations only. training comprises three phases differing prediction-dependent observation-dependent transitions value prediction length ﬁrst phase model uses observation-dependent transitions predicts time-steps. second third phases model uses prediction-dependent transitions predicts time-steps respectively. evaluation usage model prediction-dependent transitions. action-dependent state transition strong feature model described actions inﬂuence state transitions indirectly predictions observations. allowing actions condition state transitions directly could potentially enable model incorporate action information effectively. therefore propose following backbone structure short-term versus long-term accuracy last phases training scheme described used address issue poor accuracy recurrent neural networks trained using observation-dependent transitions display asked predict several time-steps ahead. however paper analyse discuss alternative training schemes. principle highest accuracy obtained training model closely possible used therefore using number prediction-dependent transitions close possible number time-steps model asked predict for. however prediction-dependent transitions increase complexity objective function alternative schemes often used current training approaches guided belief using observation rather prediction ˆxt− form state effect reducing propagation errors made predictions higher earlier stages training enabling model correct mistakes made time-step example bengio introduce scheduled sampling approach time-step type state transition sampled bernoulli distribution parameter annealed initial value corresponding using observationdependent transitions ﬁnal value corresponding using prediction-dependent transitions according schedule selected validation. analysis different training schemes atari considered interplay among warm-up length prediction length number prediction-dependent transitions suggests that rather corrective effect observation-dependent transitions seen restricting time interval model considers predictive abilities therefore focuses resources. indeed found that higher number consecutive prediction-dependent transitions model encouraged focus learning global dynamics environment results higher long-term accuracy. highest long-term accuracy always obtained training scheme uses prediction-dependent transitions even early stages training. focussing learning global dynamics comes price shifting model resources away learning precise details frames leading decrease short-term accuracy. therefore complex games reasonable long-term accuracy cannot obtained training schemes prediction-dependent observation-dependent transitions preferable. follows analysis percentage consecutive prediction-dependent transitions rather percentage transitions considered designing training schemes. viewpoint poor results obtained bengio using predictiondependent transitions explained difference type tasks considered. indeed unlike case model tolerant degree error blurriness earlier predictions discrete problems considered bengio prediction error earlier time-steps severely affect predictions later time-steps model needs highly accurate short-term order perform reasonably longer-term. also bengio treated prediction used form ﬁxed quantity rather function therefore perform exact maximum likelihood. prediction-independent state transition addition potentially enabling model incorporate action information effectively allowing actions directly inﬂuence state dynamics another crucial advantage allows consider case state transition depend frame i.e. form corresponding removing dashed links ˆxt− fig. shall call model prediction-independent simulator referring ability evolve state without using prediction usage. prediction-independent state transitions high-dimensional observation problems also considered srivastava prediction-independent simulator dramatically increase computational efﬁciency situations agent interested effect sequence actions rather single action. indeed model need project lower dimensional state space higher dimensional observation space convolutions vice versa time-step. prediction-dependent simulators analyse simulators state transition form three families environments different characteristics challenges namely atari games arcade learning environment ﬁrst-person game agent moves randomly generated mazes racing environment called torcs evaluation protocols. ﬁrst model asked predict time-steps future using actions test data. second human uses model interactive simulator. ﬁrst protocol enables determine model performs within action policy training data whilst second protocol enables explore model generalises action policies. state transition used following action-conditioned long short-term memory denotes hadamard product logistic sigmoid function one-hot vector representation parameter matrices. eqs. lstm state cell forming model state input forget output gates respectively vectors dimension respectively. details encoding decoding functions three families environments found appendix used warm-up phase length backpropagate gradient phase. considered atari games freeway pacman qbert seaquest space invaders bowling breakout fishing derby pong riverraid. these ﬁrst analysed used comparison. remaining chosen better test ability model environments challenging characteristics scrolling backgrounds small/thin objects aspects game sparse-reward games require long-term predictions used training test datasets consisting million images respectively actions chosen trained agent according .-greedy policy. large number training frames ensured simulators strongly overﬁt training data warm-up prediction lengths regulate degree accuracy different ways. value determines past model access information case irrespectively type transition used although using prediction-dependent transitions information last time-steps environment would need inferred. accessing information back past necessary even model used perform one-step ahead prediction only. higher value number prediction-dependent transitions corresponding objective function encourages long-term accuracy. achieved guiding one-step ahead prediction error predictions strongly affected teaching model make information past. precise model performing one-step ahead prediction less noise guidance required. therefore models accurate convolutional transition structures need less encouragement. increasing percentage consecutive prediction-dependent transitions increases long-term accuracy often expense short-term accuracy. found using observationdependent transitions leads poor performance games. increasing number consecutive prediction-dependent transitions produces increase long-term accuracy also decrease short-term accuracy usually corresponding reduction sharpness. games complex although lowest long-term prediction error still achieved using predictiondependent transitions reasonable long-term accuracy cannot obtained training schemes prediction-dependent observation-dependent transitions therefore preferable. illustrate results compare following training schemes prediction length observation-dependent transitions. observation prediction-dependent transitions ﬁrst last time-steps respectively. %-%-% observation-dependent transitions ﬁrst parameter updates; observation-dependent transitions ﬁrst time-steps prediction-dependent transitions last time-steps subsequent parameters updates; observation-dependent transitions ﬁrst time-steps prediction-dependent transitions last time-steps remaining parameter updates alt. alternate observation-dependent prediction-dependent transitions time-step next. observation prediction-dependent transitions ﬁrst last time-steps respectively. observation prediction-dependent transitions ﬁrst last time-steps respectively. observation-dependent transitions ﬁrst parameter updates; prediction-dependent transitions subsequent parameter updates. prediction-dependent transitions. completeness also consider training scheme consists three phases parameter updates respectively. ﬁrst phase formed using observed frame whilst subsequent phases formed using predicted frame ˆxt−. figs. show prediction error averaged sequences games bowling fishing derby pong seaquest. speciﬁcally fig. shows error predicting time-steps ahead model seen million frames using actions warm-up frames test data whilst figs. show error time-steps versus number frames seen model. ﬁgures clearly show long-term accuracy generally improves increasing number consecutive prediction-dependent transitions. using alternating rather consecutive prediction-dependent transitions long-term accuracy worse effectively asking model predict time-steps ahead. also deﬁne prediction error game player given chances roll ball alley attempt knock many pins possible score updated knocked pins relocated. knocking every ﬁrst shot strike knocking every shots spare. player’s score determined number pins knocked down well number strikes spares acquired. figure prediction error averaged sequences bowling fishing derby different training schemes. color line code used ﬁgures. prediction error time-steps model seen million frames. prediction error number frames seen model time-steps using prediction-dependent transitions produces lower short-term accuracy and/or slower short-term convergence. finally ﬁgures show using training phase observationdependent transitions long detrimental models reaches best performance similar alt. training scheme often worse. looking predicted frames could notice that games containing balls paddles using observation-dependent transitions gives rise errors reproducing dynamics objects. errors decrease increasing prediction-dependent transitions. games using observation-dependent transitions causes model fail representing moving objects except agent cases. training schemes containing prediction-dependent transitions encourage model focus learning dynamics moving objects less details would increase short-term accuracy giving rise globally accurate less sharp predictions. finally games complex strong emphasis long-term accuracy produces predictions overall sufﬁciently good. speciﬁcally videos available pdtvsodt using observationdependent transitions detrimental effect long-term accuracy fishing derby pacman highlighted names like direct links folders containing videos. video consists randomly selected time-steps ahead predictions separated black frames shown qbert riverraid seaquest space invaders. salient features videos consistent inaccuracy predicting paddle ball breakout; reset life time-steps pacman; prediction background time-steps qbert; generation objects background riverraid; quick disappearance existing appearance sides frame seaquest. bowling freeway pong long-term accuracy generally good movement ball always correctly predicted bowling pong chicken sometimes disappears freeway. hand using predictiondependent transitions results good long-term accuracy bowling fishing derby freeway pong riverraid seaquest model accurately represents paddle ball dynamics bowling pong; chicken hardly disappears freeway objects background created often correctly positioned riverraid seaquest. trading-off long short-term accuracy using prediction-dependent transitions particularly evident videos seaquest higher number transitions better model learns dynamics game appearing right location often. however comes price reduced sharpness mostly representing ﬁsh. trade-off causes problems breakout pacman qbert space invaders schemes also observation-dependent transitions preferable games. example breakout model fails representing ball making predictions sufﬁciently good. notice prediction error misleading terms desired performance %pdt training scheme performs well mixing schemes long-term accuracy highlights difﬁculties evaluating performance models. increasing prediction length increases long-term accuracy using predictionfig. show effect using different prediction lengths dependent transitions. training schemes %pdt %pdt %pdt pong seaquest. pong %pdt training scheme using higher improves long-term accuracy game scheme gives reasonable accuracy model able beneﬁt longer history. however case seaquest hand %pdt training scheme using higher improves long-term accuracy games decreases short-term accuracy. similarly above reduced short-term accuracy corresponds reduced sharpness videos available example moving caught fishing derby seaquest ball pong less sharp higher truncated backpropagation still enables increase long-term accuracy. memory constraints could backpropagate gradients sequences length split prediction sequence subsequences performed parameter updates separately subsequence. example split prediction sequence successive subsequences length performed parameter updates ﬁrst subsequence initialised state second subsequence ﬁnal state ﬁrst subsequence performed parameter updates second subsequence. approach corresponds form truncated backpropagation time extreme strategy used zaremba fig. show effect using subsequences length btt) training schemes %pdt %pdt %pdt pong seaquest. %pdt %pdt training schemes display difference accuracy different values hand %pdt training scheme using subsequence improves long-term accuracy decreases short-term accuracy decrease accuracy subsequences drastic games. riverraid using subsequence %pdt %pdt training schemes improves long-term accuracy dramatically shown fig. enables correct prediction loss. interestingly %pdt training scheme using prediction length give amount gain using bptt even history length same. would seem suggest improvement bptt encouraging longer-term accuracy indicating achieved even fully backpropagating gradient. conclusion using higher truncated backpropagation improve performance. however schemes many prediction-dependent transitions high value lead poor predictions. figure prediction error number frames seen model riverraid using bptt bptt training schemes %pdt %pdt %pdt. black line obtained %pdt training scheme. whilst cannot expect simulators generalise structured sequences actions never chosen present training data moving agent alley bowling reasonable expect degree generalisation action-wise simple environments breakout freeway pong. tested three games humans using models interactive simulators. generally found models trained using prediction-dependent transitions fragile states environment experienced training humans able play games longer simulators trained mixing training schemes. seems indicate models higher long-term test accuracy higher risk overﬁtting training policy. fig. show salient frames game pong played human time-steps game starts score opponent scores times whilst human player scores twice. scoring updated correctly game dynamics accurate. fig. show salient fames game breakout played human time-steps pong scoring updated correctly game dynamics accurate. images demonstrate degree generalisation model human style play. appendix present extensive evaluation different action-dependent state transitions including convolutional transformations action fusion gate cell updates different ways incorporating action information. also present comparison action-dependent action-independent state transitions. action-dependent state transitions give better performance baseline games. example found increasing state dimension dimension convolved frame namely might preferable. interestingly increase number parameters gain obtained using convolutions gate cell updates. results seem suggest high-dimensional sparse state transition structures could promising direction improvement. regarding different ways incorporation action information found using local incorporation augmenting frame action information indirect action inﬂuence gives worse performance direct global action inﬂuence several ways incorporating action information directly globally give similar performance. torcs maze environments highlight need learn dynamics temporally spatially coherent torcs exposes need learn fast moving dynamics consistency motion whilst mazes partially-observed therefore require simulator build internal representation surrounding using memory well learn basic physics rotation momentum solid properties walls. torcs. data generated using artiﬁcial agent controlling fast without opponents using actions test simulator able produce accurate predictions several hundreds time-steps. moved around racing track simulator able predict appearance features background well model jerky motion caused choices random actions. finally instruments correctly displayed. simulator good enough used interactively several hundred frames using actions provided human. showed model learnt well deal hitting wall right side track. salient frames game shown fig. mazes. used environment consists randomly generated mazes containing textured surfaces occasional paintings walls mazes size using actions test simulator able reasonably predict frames even steps. fig. compare predicted frames real frames several time-steps wall layout better predicted walls closer agent corridors away-walls long lighting ceiling correct frames shown. using simulator interactively actions provided human could test simulator learnt consistent aspects maze walking walls model maintained position layout taking spins wall conﬁgurations previously generated regenerated afresh shown fig. coherence maze good nearby walls long-corridors. search exploration strategies better \u0001-greedy active area research. various solutions proposed density based optimistic exploration considered memory-based approach steers agent towards previously unobserved frames. section test simulators using similar approach select group actions rather single action time. furthermore rather ﬁxed environment consider challenging mazes environment. also enables present qualitative analysis exactly measure plot proportion maze visited time. quantitatively qualitatively better random exploration used maze simulator predict outcome sequences actions chosen hardcoded policy. algorithm monte-carlo simulations randomly selected sequences actions ﬁxed length time-step stored last observed frames episodic memory buffer compared predicted frames memory. method covered maze area time-steps random exploration. results obtained monte-carlo simulations sequences actions comparing typical paths chosen random explorer explorer explorer much smoother trajectories. good local exploration strategy leads faster movement corridors. transform good global exploration strategy explorer would augmented better memory order avoid going corridor twice. sorts smooth local exploration strategies could also useful navigation problems. prediction-independent simulator state transitions form therefore require high-dimensional predictions. atari environment example avoids project state space dimension observation space dimension decoding function vice versa encoding function used structure enables saving around million ﬂops time-step. state transition found working structure eqs. different parameters warm-up prediction phases. prediction-dependent simulator used warm-up phase length backpropagate gradient back time-step order learn encoding function analysis atari suggests prediction-independent simulator much sensitive changes state transition structure training scheme predictiondependent simulator. found using prediction length gave much worse long-term accuracy prediction-dependent simulator. problem could alleviated prediction length truncated backpropagation. fig. shows comparison prediction-dependent prediction-independent simulators using subsequences length even though prediction-independent simulator backpropagate gradient warm-up phase). looking videos available pi-simulators notice prediction-independent simulator tends give worse type long-term prediction. fishing derby example long-term model tends create smaller dimension addition present real frames. nevertheless difﬁcult games prediction-independent simulator achieves better performance prediction-dependent simulator. investigation alternative figure prediction error number frames seen model prediction-dependent prediction-independent simulators using bptt bowling freeway pong breakout fishing derby pacman qbert seaquest space invaders paper introduced approach simulate action-conditional dynamics demonstrated highly adaptable different environments ranging atari games racing environments mazes. showed state-of-the-art results atari demonstrated feasibility live human play three task families. system able capture complex long-term interactions displays sense spatial temporal coherence knowledge demonstrated high-dimensional time-series data these. presented in-deep analysis effect different training approaches short longterm prediction capabilities showed moving towards schemes simulator relies less past observations form future predictions effect focussing model resources learning global dynamics environment leading dramatic improvements long-term predictions. however requires distribution resources impacts short-term performance harmful overall performance model games. trade-off also causing model less robust states environment seen training. alleviate problem would require design sophisticated model architectures ones considered here. whilst also expected ad-hoc architectures would less sensitive different training approaches believe guiding noise well teaching model make past information objective function would still beneﬁcial improving long-term prediction. complex environments compositional structure independently moving objects phenomena rarely interact. order simulators better capture compositional structure need develop specialised functional forms memory stores better suited dealing independent representations interlinked interactions relationships. homogeneous deep network architectures presented clearly optimal domains seen atari environments pacman system trouble keeping track multiple independently moving ghosts. whilst lstm memory training scheme proven capture long-term dependencies alternative memory structures required order example learn spatial coherence global level displayed model mazes oder navigation. case action-conditional dynamics policy-induced data distribution cover state space might fact nonstationary agent lifetime. cause regions state space oversampled whereas regions might actually care around agent policy state distribution underrepresented. addition induces biases data ultimately enable model learn environment dynamics correctly. veriﬁed experiments paper live human play model-based exploration problem pressing might expected environments. however simulators displayed limitations faults speciﬁcities training data example predicting event based recognition particular sequence actions always co-occurring event training data rather recognition real causes. finally limitation approach that however capable might deterministic model designed deterministic environments. clearly real world environments involve noisy state transitions future work address extension techniques developed paper generative temporal models. authors would like thank david barber helping graphical model interpretation alex pritzel preparing data yori zwols frederic besse helping implementation model oriol vinyals whye junhyuk anonymous reviewers useful discussions feedback manuscript. beattie leibo teplyashin ward wainwright küttler lefrancq green valdés sadik schrittwieser anderson york cant cain bolton gaffney king hassabis legg petersen. deepmind lab. corr abs/. http//arxiv. org/abs/.. bengio vinyals jaitly shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems hochreiter schmidhuber. long short-term memory. neural computation lengyel dayan. hippocampal contributions control third way. advances neural mnih kavukcuoglu silver rusu veness bellemare graves riedmiller fidjeland ostrovski petersen beattie sadik antonoglou king kumaran wierstra legg hassabis. human-level control deep reinforcement learning. nature http//dx.doi.org/./nature. mnih puigdomènech badia mirza graves lillicrap harley silver kavukcuoglu. asynchronous methods deep reinforcement learning. proceedings international conference machine learning lewis singh. action-conditional video prediction using deep networks atari games. advances neural information processing systems http//arxiv.org/abs/.. sutton barto. reinforcement learning introduction. press talvitie. model regularization stable sample rollouts. proceedings thirtieth conference annual watter springenberg boedecker riedmiller. embed control locally linear latent dynamics model control images. advances neural information processing systems generating data selected action repeated time-steps frame recorded analysis. images preprocessed subtracting mean pixel values dividing pixel value stochastic gradient algorithm used centered rmsprop learning rate epsilon momentum decay mini-batch size model implemented torch using default initialization parameters. state initialized zero. used trained agent generate training test datasets consisting images respectively actions chosen according .-greedy policy. large number training frames necessary prevent simulators strongly overﬁtting training data. would case with example million training frames shown fig. ghosts frightened mode time-step returned chase mode time-step simulator able predict exact time return chase mode without sufﬁcient history suggests sequence memorized. encoding consisted convolutional layers ﬁlters size stride padding height width respectively. every layer followed randomized rectiﬁed linear function parameters output tensor convolutional layers dimension ﬂattened vector dimension decoding consisted fully-connected layer hidden units followed full convolutional layers inverse symmetric structure encoding transformation ﬁlters size stride padding full convolutional layer followed rrelu. bowling. bowling easiest games model. simulator trained using observationdependent transitions gives quite accurate predictions. however using prediction-dependent transitions reduces error updating score predicting ball direction. breakout. breakout difﬁcult game model. simulator trained predictiondependent transitions predicts paddle movement accurately almost always fails represent ball. simulator trained observation-dependent transitions struggles much less represent ball predict paddle ball positions accurately ball also often disappears hitting paddle. interestingly long-term prediction error %pdt training scheme lowest representing ball predicted frames look closer real frames representing ball incorrectly. improvement model ability represent ball could obtained pre-processing frames max-pooling done increases ball size. believe sophisticated convolutional structure would even effective succeed discovering structure. fishing derby. fishing derby long-term accuracy disastrous %pdt training scheme good %pdt training scheme. short-term accuracy better schemes using observation-dependent transitions %-%pdt training schemes especially numbers parameter updates. freeway. bowling freeway easiest games model parameter updates required convergence bowling. %pdt training scheme gives good accuracy although sometimes chicken disappears position incorrectly predicted happens extremely rarely %pdt training scheme. schemes score often wrongly updated warning phase. pacman. pacman difﬁcult game model accurate prediction obtained time-steps future. movement ghosts especially frightened mode regulated position pacman according complex rules. furthermore .-greedy policy enable agent explore certain regions state space. result simulator predict well movement pacman fails predict long-term movement ghosts frightened mode chase mode later episodes. pong. %pdt training scheme model often incorrectly predicts direction ball agent opponent. quite rarely ball disappears agent. %pdt training scheme direction ball much accurately predicted ball often disappears agent ball paddles generally less sharp. qbert. qbert game %pdt training scheme unable predict accurately beyond short-term frames background predicted. predictiondependent transitions used less sharply agent moving objects represented. riverraid. riverraid prediction %pdt training scheme poor scheme causes generation objects background. schemes model fails predict frames follow loss that’s prediction error increases sharply around time-step fig. long-term prediction error lower %pdt training scheme scheme simulator accurate before sometimes after loss. problem incorrect prediction loss disappears using bbtt prediction-dependent transitions. seaquest. seaquest %pdt training scheme existing disappears time-steps ever appears sides frame. higher number prediction-dependent transitions less sharply represented accurately dynamics appearance sides frame predicted. space invaders space invaders difﬁcult game model accurate prediction obtained time-steps future. %pdt training scheme unable predict accurately beyond short-term. %pdt training scheme struggles represent bullets. figs. show effect using different prediction lengths training schemes %pdt %pdt %pdt games. vectors dimension respectively parameters) alternatives using unconstrained convolutional transformations prediction length %-%pdt training scheme. ﬁgures transition clearly preferable baseline exception fishing derby transitions hidden dimensionality perform better converge earlier terms number parameter updates. denotes convolutions above. model around parameters. –cda different parameters gate –cda ht−–iz cell updates convolutions. models around parameters. ’ht−–iz figs. compare different ways incorporating action action-dependent state transitions using prediction length %-%pdt training scheme. speciﬁcally compare baseline structure following alternatives whht− waat− waat− multiplicative/additive interaction action i.e. whht− waat−. model around parameters. wsst− waat− multiplicative interaction action encoded frame i.e. indicates augmenting operation frame dimension augmented full-zero full-one matrices dimension producing tensor dimension output ﬁrst convolution written action-independent state transition generally gives worse performance games higher error. interesting disadvantage structure inability predict moving objects around agent seaquest. noticed videos seaquest show poor modelling ﬁsh. structure also makes difﬁcult correctly update score games seaquest fishing derby. fig. show results human playing freeway time-steps model able update score correctly point score starts ﬂashing change color warn resetting game. model able predict score correctly warning phase bias data ﬂashing starts right time resetting game. generated million images training testing respectively agent trained asynchronous advantage actor critic algorithm agent could choose among three actions accelerate straight accelerate left accelerate right according \u0001-greedy policy selected random independently episode. added nothing’ action generating actions random. smaller lead longer episodes larger lead shorter episodes could number convolutional layers ﬁlters kernel sizes atari padding. unlike atari torcs could rely agents random policies generate interesting sequences. agent could choose actions forward backward rotate left rotate right nothing. episode agent alternated random walk steps spinning steps encourages coherent learning predicted frames spin. random walk dithering meaning actions chosen probability every time-step. training test datasets made episodes respectively. episodes length frames resulting images training testing respectively. adapted encoding convolutions ﬁlters size stride padding decoding transformation adapted accordingly. observed increasing number monte-carlo simulations beyond made little difference probably possible actions number possible monte-carlo simulations increasing signiﬁcantly sequence length actions beyond lead large decrease performance. explain this observed steps average prediction error less half average prediction error steps since average minimum maximum distances vary signiﬁcantly deep simulations ended noise signal predictions decisions better random. prediction-independent simulators section compare different action-dependent state transitions prediction lengths prediction-independent simulator. vectors dimension respectively different matrices warm-up prediction phases following alternatives base–zt− remove action-independent transformation i.e. represents zero-vector different matrices warm-up prediction phases. model around parameters. –zt− substitute gate updates separate gating encoded frame i.e. ’base–zt− state transition performs quite poorly long-term prediction compared transitions. transition prediction-independent simulator performs much worse prediction-dependent simulator baseline state transition best performance obtained ’ht−–iz –zt− structure however large number parameters. figs. show effect using different prediction lengths structure ’base– ht−’. using longer prediction lengths dramatically improves long-term. overall best performance obtained using subsequences length figure prediction error prediction-independent simulator different action-dependent state transitions bowling freeway pong breakout fishing derby pacman qbert seaquest space invaders. number frames million excludes warm-up frames. figure prediction error prediction-independent simulator different prediction lengths bowling freeway pong breakout fishing derby pacman qbert seaquest space invaders. figure prediction error prediction-independent simulator bptt bptt bowling freeway pong breakout fishing derby pacman qbert seaquest space invaders.", "year": 2017}