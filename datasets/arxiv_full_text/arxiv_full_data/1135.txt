{"title": "ReNN: Rule-embedded Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "The artificial neural network shows powerful ability of inference, but it is still criticized for lack of interpretability and prerequisite needs of big dataset. This paper proposes the Rule-embedded Neural Network (ReNN) to overcome the shortages. ReNN first makes local-based inferences to detect local patterns, and then uses rules based on domain knowledge about the local patterns to generate rule-modulated map. After that, ReNN makes global-based inferences that synthesizes the local patterns and the rule-modulated map. To solve the optimization problem caused by rules, we use a two-stage optimization strategy to train the ReNN model. By introducing rules into ReNN, we can strengthen traditional neural networks with long-term dependencies which are difficult to learn with limited empirical dataset, thus improving inference accuracy. The complexity of neural networks can be reduced since long-term dependencies are not modeled with neural connections, and thus the amount of data needed to optimize the neural networks can be reduced. Besides, inferences from ReNN can be analyzed with both local patterns and rules, and thus have better interpretability. In this paper, ReNN has been validated with a time-series detection problem.", "text": "abstractâ€”the artificial neural network shows powerful ability inference still criticized lack interpretability prerequisite needs dataset. paper proposes ruleembedded neural network overcome shortages. renn first makes local-based inferences detect local patterns uses rules based domain knowledge local patterns generate rule-modulated map. that renn makes global-based inferences synthesizes local patterns rule-modulated map. solve optimization problem caused rules two-stage optimization strategy train renn model. introducing rules renn strengthen traditional neural networks long-term dependencies difficult learn limited empirical dataset thus improving inference accuracy. complexity neural networks reduced since long-term dependencies modeled neural connections thus amount data needed optimize neural networks reduced. besides inferences renn analyzed local patterns rules thus better interpretability. paper renn validated time-series detection problem. decades development artificial neural network shows powerful ability inference approaching even surpassing human level several specific scenarios image recognition chess games reason success could manage great number parameters deep networks learn highly complex functions. however interpretability often criticized since inferences difficult explained concise interaction among parameters network. several researches attempted open black interpret inferences. insight features generated automatically hidden layer issue opening box. schwartz-ziv tishby proposed visualize information plane revealed function efficient representation hidden layers zeiler investigated activations neural units feature layer highlighted regions input data responsible activations approach useful dissect models suggest ways improve szegedy analyzed models synthesizing samples lead high activations neural units found models easily hacked adding certain structured noise image space revealing supervised learning models might ignore common sense target e.g. mammals four legs generally. recently neural units feature layer interpreted human-interpretable semantic concepts help densely labeled dataset modeled explanatory graph reveals knowledge hierarchy hidden inside pre-trained model although techniques improved interpretability cannot interpret inference concisely human e.g. telling sample rather cat. main reason hard learn knowledge common sense single limited empirical dataset. compared human make inferences well interpret inferences concisely based foundations i.e. experiences knowledge. knowledge learned summarized development science technologies human make better inferences smaller empirical dataset. therefore vital teach models knowledge improve several aspects reducing model complexity cost learn problem adding interpretability knowledge increasing accuracy. knowledge representation used describe richness world computer systems understood artificial intelligence used reasoning inferring subsequently rule-based representation common formalisms knowledge representation. example expert systems based rules widely used computer-aided medical diagnosis data processing analyzing fault diagnosis etc. subsequent question knowledge representation could combined ann. knowledge-based answer designed topological structure according knowledge input variables represented knowledge dependency structures rules however answer appropriate deep whose input variables often high-dimensional data designers often little idea dependency structures high-dimensional data. paper proposes rule-embedded neural network makes knowledge improve performance ann. rules used represent domain knowledge common sense computable manner. renn disassembles black parts localbased inference global-based inference. local-based inference handles local patterns intuitively easily learned empirical datasets global-based inference introduces rules local patterns accumulated human long time. accordingly knowledge human teachers experiences empirical datasets combined contribute global-based inference together improve performance. besides differentiate contributions local patterns rules final inferences thus improve interpretability neural networks. example apply renn timeseries detection problem experiments. figure block design feature-mapping global-mapping architecture time-series data. consists convolution layers batch normalization layers pooling layers deconvolution layers concatenation layers ordered hollow arrows show. filter size last convolution layer others solid arrows show shallower features output batch normalization layers fused deeper layers concatenation layers. output feature designed reflect features targets. features could subcomponents targets. example task detect face occurrences twodimensional image feature could consist neural activations face organs i.e. eyes ears nose mouth features could also instances targets. example task detect r-peaks electrocardiograph seconds feature could consist neural activations multiple r-peaks. rules designed according domain knowledge activating nodes rule-modulated rulebased evidences could found activations feature map. scenario face detection relative positions face subcomponents utilized analyze candidate positions scales face occurrences. scenario r-peak detection periodicity variability heart rate used estimate possibility rpeak-occurrences candidate positions. rule-modulating plays important role global mapping renn. however rules make renn model nondifferentiable leading failure model training gradient-descend-based optimization methods. two-stage optimization strategy adopted overcome limitation. feature-mapping block firstly optimized training dataset design renn blocks consider characteristics specific task data dimension target scales. task r-peak detection onedimensional considered example following sections. consists time-series data electrical activity heart recorded noninvasive electrodes placed skin. r-peak time point procedure rapid depolarization right left ventricles. r-peaks useful need estimate exact time heartbeat e.g. calculating heart rate variability pulse wave transit time sampling rate paper scale r-peak morphology time-points architecture time series data shown fig. convolution layer used capture local patterns max-pooling layer used summarize local patterns enlarge receptive field neural note deeper layers. batch normalization layer used overcome vanishing gradient problem accelerate training deep neural networks. deconvolution layer used upsample pooled data pointwise classification achieved feedforward computation concatenation layer used reserve fine features shallower deeper layers features coarser pooling operations. know neural node layers convolution receptive filed five pooling deconvolution layers double receptive field. output node architecture receptive field time-points receptive field covers local information r-peak morphology i.e. p-q-r-s-t patterns electrical activity heartbeat. receptive field enlarged max-pooling layers. constraint supporting regions overlapped other determine many supporting regions could used voting. simplicity maximum number supporting regions six. number output channels convolution layers deconvolution layers important hyper-parameters control complexity feature layers. output channels layers experiments simplicity. rule-modulating block designed according knowledge r-peaks ecg. distinguish r-peaks noises human beings first model piece signals knowledge heart beats questions. example owner normal sinus rhythm arrhythmias? heart rate? normal sinus rhythm heart rate would used pick r-peaks noises according time distance neighboring r-peaks. knowledge represented rules. first step analyze knowledge-based concepts feature map. concepts heart rate standard deviation help local-based inference r-peaks prominent easy detect feature softmax function. refer time intervals r-peaks constraint seconds applied intervals according domain knowledge heart rhythm common users. assumption gaussian distribution eliminate abnormal intervals caused noises. that average standard deviation calculated sdnn. possibility r-peaks time point according votes supporting regions. supporting region defined according knowledge heart rhythm. know r-peak occurrences approximately-integral multiples away current time point used supporting evidences possibility r-peak occurrence current time point. therefore supporting regions consist time points approximately-integral multiples section sets experiments evaluate renn task r-peak detection ecg. used firsthand information analyze structure function heart years becoming popular smart wearable devices monitor status users however still challenging accurately detect r-peaks since several types noises affecting occurrences motion artifact muscular activation interference interference experiments dataset constructed lohas tech. data measured sampling rate single-lead device electrodes placed left right arms. dataset includes measurements users well labels r-peaks measurement. users healthy normal sinus rhythm. measurement seconds long filtered high-pass filter low-pass filter remove irrelative signal components. dataset includes labels r-peaks. labels first labeled algorithm suspicious labels carefully checked re-labeled human experts. standard deviation r-peak intervals used estimate heart rate variability measurement. measurements dataset sdnn measurements high sdnn dataset. according f-score renn achieves error rate number channels four localbased inference achieves comparable performance channels. however computational cost local-based inference channels several times renn. results reveal renn make rulemodulated enhance accuracy detect r-peaks. results also reveal renn potential reduce computational cost deep neural networks. fig. illustrates example measurements renn detection involving output feature-mapping block rule-modulating block global-mapping block. voltage range signal measurement weaker standard voltage range also noises might wrongly recognized r-peaks. intuitively hard distinguish r-peaks signal local patterns. nevertheless knowledge heart rhythm user. measured -year-old female healthy history heart diseases. accordingly premature beat abnormal beat measurement thus discriminate r-peaks noisy signal heart rate neighboring r-peaks. analyzing process human local patterns global patterns also reflected detection process renn. firstly feature-mapping block detects potential r-peaks according local patterns receptive fields. secondly rule-modulating block analyzes heart rhythm feature maps estimates distributions heartbeats based heart rhythm features neighboring heartbeats. finally global-mapping block synthesizes original rule-modulated refine detection. example shown fig. find noisy peak suppressed successfully distorted r-peak enhanced global-based inference. figure loss curves training process local based model renn model. horizontal axis represents number training epochs vertical axis represents value loss function. train renn model block adam optimizer weighted cross-entropy labels model output loss function. feed measurement labels r-peaks apply optimizer training step. measurements epochs thus measurement times. learning rate beginning decayed every steps exponentially rate total number training step million. value loss function analyze convergence model detection performance following metrics number rpeaks correctly detected model number r-peaks missed number noise samples wrongly detected r-peaks also calculate f-score harmonic mean precision sensitivity detection model. given exact location r-peak unstable neighboring sampling-points noises tolerance interval detection. correctly detected r-peak judged model outputs positive tolerance interval labeled r-peak. tolerance interval length sampling points i.e. fig. shows convergence loss function early training phase. loss curves local-based model renn model decrease rapidly first training epochs tend stable gradually. final values loss function local-based model renn model respectively. results reveal loss functions converged training dataset renn model achieve better fitting performance. figure renn detection measurement. line input time-series data measurement signals weak. three lines bottom outputs feature-mapping block rule-modulating block global-mapping block respectively. four lines aligned according time axis. solid circles anchored lines time points labeled r-peaks. downward triangle shows false positive local-based inference global-based inference reduces probability r-peak time point little support heart rhythm besides first r-peak front triangle distorted noise. detected higher probability support heart rhythm local patterns local morphologies r-peaks detected feature-mapping block. rules among r-peaks designed according sdnn. furthermore disassemble local morphologies details i.e. p-q-r-s-t peaks train featuremapping block details. then rules order time constraints peaks applied. line-o shows global-based inferences combine local patterns rules. line-o less jitters line-f indicating combination local patterns rules reduced uncertainty global-based inferences. check analyze conflicts localbased global-based inferences carefully. might possibilities. firstly indicate exceptions rules premature beat heart value diagnosis case. indications conflicts already diagnosed label user suppress similar conflicts take actions medication. secondly paper proposed renn overcome limitation current anns lack knowledge validated approach r-peak detection problem. introducing knowledge rule-embedding approach renn could improve detection accuracy well reduce model complexity. needs dataset model training mitigated model complexity reduced. besides renn improved interpretability neural-network-based technologies. global-based inference renn interpreted interaction local patterns rules among local patterns. model long-term dependencies difficult problem machine learning area since uncertainty grows rapidly input data become higher-dimensional. current popular solution train deep neural networks dataset lstm resnet argue rule-embedding approach could provide solution make long-term dependencies efficiently especially already accumulated knowledge long-term dependencies. might become human-computer interaction local-pattern-rules contents interaction. computers tell local patterns might question rules embedded rather placed ahead behind. rules placed ahead knowledge semantic features extracted data. hidden layers play role extract features. rules placed behind want model dependencies local patterns rules automatically make inferences synthetically. rules play role extract global features i.e. rule-modulated thus rules freely without worry conflicts rules combinatorial explosion multiple rules. besides rules renn independent optimization procedure human experts domain knowledge could design rules freely without constraints differentiability. makes combination artificial neural network existing knowledge much easier. furthermore rules could organized accumulated cross-validation latest probabilistic graphical model based reasoning systems leading rule base rules marked applicable tasks domains. rule base computers could gradually know apply rules. would helpful transfer learning samples used unsupervised learning labels used. localpattern-rule rule base might lead thinking artificial intelligence. author thanks prof. wang institute automation chinese academic sciences introducing philosophy structure average artificial intelligent inspired idea research. author thanks jidong shandong provincial hospital help build dataset thanks wang yudong lohas tech supports thanks zongbo zhang shuyu dongyang yuzhi shuai weekly discussion machine learning problems. polat gÃ¼neÅŸ expert system approach based principal component analysis adaptive neuro-fuzzy inference system diagnosis diabetes disease\" digital signal processing vol. jain srinivas jain novel based expert system architecture on-line off-line fault diagnosis control transformers\" tencon ieee region conference thayer yamamoto brosschot \"the relationship autonomic imbalance heart rate variability cardiovascular disease risk factors\" international journal cardiology vol. ahlstrom johansson uhlin lÃ¤nne \"noninvasive investigation blood pressure changes using pulse wave transit time novel approach monitoring hemodialysis patients\" journal artificial organs official journal japanese society artificial organs vol. zhou wang wang \"pulse waveform indicator baseline offset pulse transit time based blood pressure estimation\" ieee healthcare innovations point care technologies tompkins real-time detection algorithm\" ieee transactions biomedical engineering vol. bme- hochreiter schmidhuber \"long short-term memory\" neural", "year": 2018}