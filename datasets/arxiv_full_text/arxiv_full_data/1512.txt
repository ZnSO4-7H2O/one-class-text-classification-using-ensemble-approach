{"title": "Deep Multimodal Semantic Embeddings for Speech and Images", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "In this paper, we present a model which takes as input a corpus of images with relevant spoken captions and finds a correspondence between the two modalities. We employ a pair of convolutional neural networks to model visual objects and speech signals at the word level, and tie the networks together with an embedding and alignment model which learns a joint semantic space over both modalities. We evaluate our model using image search and annotation tasks on the Flickr8k dataset, which we augmented by collecting a corpus of 40,000 spoken captions using Amazon Mechanical Turk.", "text": "paper present model takes input corpus images relevant spoken captions ﬁnds correspondence modalities. employ pair convolutional neural networks model visual objects speech signals word level networks together embedding alignment model learns joint semantic space modalities. evaluate model using image search annotation tasks flickrk dataset augmented collecting corpus spoken captions using amazon mechanical turk. conventional automatic speech recognition systems utilize training data form speech audio parallel text transcriptions. paper investigate possible text transcripts replaced relevant visual images. given dataset comprised image scenes accompanying spoken audio captions segmented word level propose model capable learning associate spoken instances word images dogs name example. model relies pair convolutional neural networks images another speech along alignment embedding model. outputs networks provide ﬁxeddimensional representations variable-sized visual objects spoken words mapped shared semantic embedding space. allows align words captions objects refer image scene. large body research jointly modeling images text exists literature aware prior work models semantics images speech directly audio signal level. multimodal modeling images text extremely popular pursuit machine learning ﬁeld past decade many approaches focusing accurately annotating objects regions within images. example barnard relied pre-segmented labelled images estimate joint distributions words objects socher learned latent meaning space covering images words learned non-parallel data. approaches focused improving identiﬁcation visual objects pool predeﬁned classes research studied problem aligning text images videos describe. example kong took visual scenes high level captions parsed text detected visual objects aligned modalities markov random ﬁeld. aligned semantic graphs text queries relational graphs objects videos perform natural language video search. matuszek employed separate classiﬁers text visual objects shared label sets. related problem natural language caption generation. large number papers published subject recent efforts using recurrent deep neural networks made tremendous progress generated much interest ﬁeld. work paper generate captions images originally inspired text-to-image alignment models presented karpathy karpathy uses reﬁned version alignment model presented produce training exemplars caption-generating language model conditioned visual features. alignment process semantic embedding space containing images words learned. works also attempted learn multimodal semantic embedding spaces frome trained separate deep neural networks language modeling well visual object classiﬁcation. embedded object classes dense word vector space neural network language model ﬁne-tuned visual object network predict embedding vectors words corresponding object classes. paper shares much spirit prior work semantic embedding images words difference instead dealing text orthographic level learn model derive meaning directly spoken audio. single high dimensional vector space. example vector space want different spoken examples word neighbor another also neighbor image crops containing dogs. order this require means transform variable sized image crops well variable duration audio waveforms ﬁxed dimensional vector representations. further also require coaxing vectors taking property semantically similar images words neighbor another. achieve this employ separate neural network architectures images audio marry together embedding alignment model. order detect candidate regions image likely contain meaningful objects region convolutional neural network model rcnn object detector works ﬁrst using selective search build large list proposal regions typically numbering thousands given image. proposal region object classiﬁer used extract activations penultimate layer neurons network. activations form ﬁxed-dimensional well work) feature vector representation proposal region. one-versus-all support vector machines used calculate detection scores classes region highly overlapping regions similar classiﬁcation scores merged. finally remaining regions ranked order maximum classiﬁcation score across classes. work follow take detected regions along entire image frame resulting regions image. dimensional rcnn feature vectors represent region refer {vi|i previous efforts perform semantic alignment text objects image scenes beneﬁted fact text naturally segmented words instances word share orthography. hand segmenting continuous speech words nontrivial different spoken instances underlying word inevitably differ duration also acoustic feature representations inﬂuenced factors microphone speaker characteristics context word spoken. speech recognition system reasonable solution building spoken interface natural language image retrieval systems described work interested investigating potential neural networks learn meaningful semantic representations operate directly feature level. however tasking system also performing word segmentation audio stream signiﬁcantly complicates problem hand. choose take step back text-based framework pre-segmenting spoken caption sequence audio waveforms containing single ground-truth word throwing away word identity segment. authors trained isolated word recognizer utilized n-best recognition hypothesis re-ranking; here propose similar model spectrogram isolated word image captions. standard cnns expect inputs ﬁxed size order accommodate variable duration words follow choose embed spectrograms ﬁxed duration window applying zero-padding truncation necessary. found second window sufﬁcient capture duration words corpus case second long window long enough capture words appearing data. create spectrogram representing word begin performing forced-alignment audio ground truth text transcription order determine word boundary information. next apply standard millisecond window millisecond shift word utterance extracting energy ﬁlterbank features window using ﬁlterbanks spaced along scale. next subtract mean value apply variance normalization entire spectrogram. finally either zeros truncate equally sides force spectrogram width frames second. figure shows example input data network looks like instance word strategists. image vector h-dimensional semantic embedding space. word spectrogram vector embedding space nonlinear transform element-wise nonlinear function. experiments paper max. motivated assumption spoken caption given image contain words directly reference objects image karpathy’s objective function tries assign high similarity matching image-caption pairs grounding word vector image fragment vectors. inner product similarity given word embedding image fragment embedding used measure degree grounding word caption given score according maximum similarity across image fragments image overall image-caption similarity score computed summing scores words caption thresholded karpathy uses margin objective function forces matching image-caption pairs higher similarity score mismatched pairs margin. given denotes similarity matching imagesentence pair cost deﬁned recent works natural language image caption generation utilized number datasets contain images alongside human-generated text captions pascal flickrk flickrk mscoco however none datasets include speech data decided collect spoken audio experiments. manageable size ubiquitousness previous literature choose flickrk starting point data collection. flickrk contains approximately images captured flickr photo sharing website depicts actions involving people animals. image annotated text caption different people resulting total captions. collect captions authors turned amazon’s mechanical turk online service extract vector representations word image caption feed word’s spectrogram network discard softmax outputs retaining activations dimensional fully connected layer immediately classiﬁcation layer. given caption refer vectors {wj|j number words appearing caption. given image-caption pair corresponding object detection boxes word spectrograms task align word detection boxes found image. this adopt transform model objective function presented however replace text modelling side karpathy’s models word spectrogram enabling align image fragments directly segments speech audio. provide brief overview alignment model objective here. {vi|i di-dimensional vectors representing activations penultimate layer rcnn detected image region described section also {wj|j dimensional vectors representing similar activations spectrogram words spoken caption. alignment model vectors shared h-dimensional space semantically related words images high similarity. alignment model two-faceted separate transforms applied image vectors well word spectrogram vectors. afﬁne transform allows requesters post human intelligence tasks hits made available anonymous non-expert workers turkers choose complete tasks small amount money. turned mechanical turk collect spoken audio recordings captions flickrk dataset. spoke javascript framework basis audio collection hit. spoke ﬂexible framework creating speech-enabled websites acting wrapper around html getusermedia also supporting streaming audio client backend server socket.io library. spoke client-side framework also includes interface google’s speechrecognition service used provide near-instantaneous feedback turker. figure displays screenshot audio collection interface used hits. random captions displayed user click start/stop button record speech read caption loud. playback button allows turker listen recordings diagnose problems microphone environment. spoke pipes audio google recognizer checks recognition result prompt notiﬁes user speech could recognized accurately. turker given option re-record errorful caption. cannot submitted captions successfully recorded. experiments simple metric veriﬁcation caption words must appear recognition result regardless ordering. found lenient sufﬁcient users rarely complained system correctly recognizing speech collected utterances easily aligned caption text using kaldi forced alignment system. majority utterances ﬂagged unalignable either empty short believe client-server connection issues; problematic utterances recollected another round hits. paid turkers cents spoken caption resulting total cost including amazon’s service fee. collected speech unique turkers average worker completing captions. handful turkers completed average number captions highest number collected single worker verify integrity collected audio data split utterances utterance training utterance development utterance testing covering word vocabulary. splits correspond training validation testing splits given used kaldi build large vocabulary speech recognition system adapting standard wall street journal recipe gmm/hmm mllt system data. employed training train acoustic language models pronunciation lexicon development tune acoustic language model weights. ﬁnal word error rate system test providing another indication data relatively high quality. order preprocess flickrk data employ recognizer force align audio ground truth text transcripts segment audio word level. flickrk corpus contains small number images captions relative datasets imagenet follow example off-the-shelf rcnn provided trained imagenet extract -dimensional visual object embeddings. similarly employ supervised pretraining word spectrogram using wall street journal split contains roughly hours speech extracted instances words occuring least times data. gave total words covering vocabulary size split training testing sets. used data train word spectrogram using word vocabulary output targets. even though training supervised unique words appearing flickrk transcriptions appear training spectrogram cnn. embedding alignment model training epochs. training performed using standard image train flickrk data using accompanying captions. batch randomly choose captions associated image. tried several different settings dimension semantic embedding space found values seemed work well line also found necessary normalize vectors unit magnitude order prevent exploding gradients. evaluate alignment embedding model follow example model perform image retrieval annotation. image search deﬁned choosing caption test asking system image belongs caption. image annotation opposite problem choosing image test without caption asking system search captions test belongs image. report recall evaluation metric probability correct result found returned hits. table details results system well comparison replacing word spectrogram embeddings dimensional word vectors taken also compare socher karpathy text word vector system outperforms model similar karpathy’s reﬁnements made single layer word embedding network rather bidirectional recurrent neural network. reports high recalls flickrk data include results flickrk data. although spectrogram perform nearly well systems access ground truth text massively outperforms random ranking scheme. spite fact spectrogram system direct access ground truth word identity caption words also word embedding vectors dimension rather believe results quite promising training data expect substantial improvements. figure displays several alignments flickrk images captions inferred system. means perfect system reliably aligns salient objects images associated caption words. also trained several different word spectrogram cnns varying conﬁgurations. table displays top- top- accuracies networks. two-layer conventional units layer relu nonlinearities achieved classiﬁcation accuracy adding third layer brought number even lower speculate training large enough train network. however replacing ﬁrst fully connected layer -unit convolutional layer boosted accuracy also trained network convolutional layers fully connected layer achieved similar results network single convolutional layer. also explored varying size shapes convolutional ﬁlters pooling layers dimension fully connected layers network achieving accuracy reﬂects best performance. although networks show wide range top- accuracies interesting note top- accuracies excess figure displays ﬁlter responses ﬁrst layer network. paper presented ﬁrst efforts construct model learn joint semantic representation spoken words well visual objects. training time model requires weak labels form paired images natural language spoken captions. system aligns salient visual objects images associated caption words process building semantic representation across modalities. evaluate model flickrk image search annotation tasks compare several systems access ground truth text. many avenues would like take research next. deeper investigation performance speech embedding ground truth text systems logical ﬁrst step increasing amount training data shed light this. would also like incorporate word level segmentation alignment scheme alleviating need forced alignment making setting realistic. lastly neural networks used extract features visual objects spoken words pre-trained supervised fashion outside data believe large amount data possible train together along alignment embedding model. fig. examples inferred alignments flickrk data. words image’s caption stacked right image accompanied alignment scores. keep images free much clutter threshold scores displaying link word maximally associated object bounding score positive. note system actually text caption words spectrogram. replace spectrogram ﬁgures ground truth text sake clarity. connecting modalities semi-supervised segmentation annotation images using unaligned text corpora proceedings conference computer vision pattern recognition fidler kong urtasun visual semantic search retrieving videos complex textual queries proceedings conference computer vision pattern recognition matuszek fitzgerald zettlemoyer joint model language perception grounded attribute learning proceedings international conference machine learning frome corrado shlens bengio dean ranzato mikolov devise deep visualsemantic embedding model proceedings neural information processing society girshick donahue darrell malik rich feature hierarchies accurate object detection semantic segmentation proceedings conference computer vision pattern recognition cvpr rashtchian young hodosh hockenmaier collecting image annotations using amazon’s mechanical turk proceedings naacl workshop creating speech language data amazon’s mechanical turk young hodosh hockenmaier from image descriptions visual denotations similarity metrics semantic inference event descriptions transactions association computational linguistics povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsky stemmer vesely kaldi speech recognition toolkit ieee workshop automatic speech recognition understanding socher karpathy manning grounded compositional semantics ﬁnding describing images sentences transactions association computational linguistics", "year": 2015}