{"title": "Safe and Efficient Off-Policy Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace($\\lambda$), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to $Q^*$ without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q($\\lambda$), which was an open problem since 1989. We illustrate the benefits of Retrace($\\lambda$) on a standard suite of Atari 2600 games.", "text": "work take fresh look algorithms off-policy return-based reinforcement learning. expressing common form derive novel algorithm retrace three desired properties variance; safely uses samples collected behaviour policy whatever degree off-policyness; efﬁcient makes best samples collected near on-policy behaviour policies. analyze contractive nature related operator off-policy policy evaluation control settings derive online sample-based algorithms. believe ﬁrst return-based off-policy control algorithm converging a.s. without glie assumption corollary prove convergence watkins’ open problem since illustrate beneﬁts retrace standard suite atari games. fundamental trade-off reinforcement learning lies deﬁnition update target estimate monte carlo returns bootstrap existing q-function? return-based methγtrt) offer advantages value bootstrap methods better behaved combined function approximation quickly propagate fruits exploration hand value bootstrap methods readily applied off-policy data common case. paper show learning returns need cross-purposes off-policy learning. start recent work harutyunyan show naive off-policy policy evaluation without correcting off-policyness trajectory still converges desired value function provided behavior target policies apart algorithm learns trajectories generated simply summing discounted off-policy corrected rewards time step. unfortunately assumption close restrictive well difﬁcult uphold control case target policy greedy respect current q-function. sense algorithm safe handle case arbitrary off-policyness. alternatively tree-backup algorithm tolerates arbitrary target/behavior discrepancies scaling information future temporal differences product target policy probabilities. efﬁcient near on-policy case though traces prematurely blocking learning full returns. work express several off-policy return-based algorithms common form. derive improved algorithm retrace safe efﬁcient enjoying convergence guarantees off-policy policy evaluation importantly control setting. retrace learn full returns retrieved past policy data context experience replay returned favour advances deep reinforcement learning off-policy learning also desirable exploration since allows agent deviate target policy currently evaluation. best knowledge ﬁrst online return-based off-policy control algorithm require glie assumption addition provide corollary ﬁrst proof convergence watkins’ finally illustrate signiﬁcance retrace deep learning setting applying suite atari games provided arcade learning environment notation consider agent interacting markov decision process ﬁnite state space action space discount factor transition function mapping stateaction pairs distributions reward function. notational simplicity consider ﬁnite action space case inﬁnite possibly continuous action space handled retrace algorithm well. policy mapping distribution q-function maps state-action pair value particular reward q-function. policy deﬁne operator bellman residual policy examination shows also ﬁxed point policy evaluation operator estimated using monte carlo methods intermediate values trade estimation bias sample variance seek evaluate target policy using trajectories drawn behaviour policy on-policy; otherwise off-policy. consider trajectories form interested related off-policy learning problems. policy evaluation setting given ﬁxed policy whose value wish estimate sample trajectories drawn behaviour policy control setting consider sequence policies depend sequence q-functions seek approximate general operator consider comparing several return-based off-policy algorithms informally call coefﬁcients traces operator. importance sampling importance sampling simplest correct discrepancy learning off-policy returns off-policy correction uses product likelihood ratios notice deﬁned choice yields reduction technique well known estimates suffer large even possibly inﬁnite variance motivated variance reduction techniques off-policy recent alternative proposed harutyunyan introduces off-policy correction based q-baseline approach called policy evaluation control respectively corresponds choice offers advantage avoiding blow-up variance product ratios encountered interestingly operator contracts around provided sufﬁciently close other. deﬁning maxx level off-policyness authors prove operator deﬁned contraction mapping around around worst case unfortunately requires knowledge condition conservative. neither safe guarantee convergence arbitrary tree-backup algorithm precup corrects target/behaviour discrepancy multiplying term product target policy probabilities. corresponding operator deﬁnes contraction mapping policies makes safe algorithm. however algorithm efﬁcient near on-policy case unnecessarily cuts traces preventing make full returns indeed need discount stochastic on-policy transitions retrace contribution algorithm retrace takes best three previous algorithms. retrace uses importance sampling ratio truncated compared suffer variance explosion product ratios. similarly unlike traces on-policy case making possible beneﬁt full returns. off-policy case traces safely similarly consider ﬁxed target policy ease exposition consider ﬁxed behaviour policy noting result extends setting sequences behaviour policies ﬁrst result states γ-contraction operator deﬁned non-negative coefﬁcients assumption theorem operator deﬁned unique ﬁxed point furthermore thus sub-convex combination weighted non-negative coefﬁcients thus γ-contraction mapping around remark notice coefﬁcient proof theorem depends write control setting single target policy replaced sequence policies depend prior work focused strictly greedy policies consider larger class increasingly greedy sequences. make notion precise. deﬁnition sequence policies increasingly greedy w.r.t. sequence q-functions following property holds qk+. intuitively means least greedy previous policy qk+. many natural sequences policies increasingly greedy including εk-greedy policies softmax policies proofs appendix. assume markovian sense depends full past history. allows deﬁne -probability transition operator finally additional requirement convergence control case assume satisﬁes theorem consider arbitrary sequence behaviour policies sequence target policies increasingly greedy w.r.t. sequence online algorithms analyzed contraction properties expected operators. describe online algorithms learn sample trajectories. analyze algorithms every visit form practical generalization ﬁrst-visit form. section consider retrace algorithm deﬁned coefﬁcient min. rewrite operator write retrace operator focus control case noting similar result derived policy evaluation. theorem consider sequence trajectory generated following along trajectory time ﬁrst occurrence update εk-away greedy policies i.e. maxx πk−πqk assume πk∧µk asymptotically commute limk πk∧µk πk∧µk p{xt sample trajectories ﬁnite terms second moment lengths proof extends similar convergence proofs bertsekas tsitsiklis optimistic policy iteration tsitsiklis provided appendix. notice compared theorem assume here. however make additional assumption πk∧µk commute limit. satisﬁed example probability assigned behavior policy greedy action independent examples include ε-greedy policies generally mixtures greedy policy arbitrary distribution choice trace coefﬁcients theorems ensure convergence trace coefﬁcient however make best choice need consider speed convergence depends variance online estimate indicates many online updates required single iteration contraction coefﬁcient variance variance estimate strongly depends variance product trace easy quantity control general usually indeγtc γtvt ﬁnite thus important requirement numerically stable algorithm small possible certainly rules importance sampling larger reason choose contraction speed contraction coefﬁcient depends much traces small possible iterations obtain ε-approximation). smallest traces indeed traces beneﬁt learning full returns bellman operator reasonable trade-off variance high contraction speed given retrace provide convergence online algorithm. relax assumption trace markovian could trade trace time possibly largerthan- trace another time long product less possible choice could topics discussion glie assumption. crucial point theorem convergence occurs arbitrary behaviour policies. thus online result theorem require behaviour policies become greedy limit inﬁnite exploration believe theorem provides ﬁrst convergence result λ-return algorithm require assumption. proof watkins’ corollary theorem selecting target policies greedy w.r.t. deduce watkins’ converges a.s. believe ﬁrst proof. increasingly greedy policies assumption sequence target policies increasingly greedy w.r.t. sequence general considering greedy policies w.r.t. leads efﬁcient algorithms. indeed using nongreedy target policies speed convergence traces frequently. course order converge eventually need target policies become greedy limit comparison unlike retrace need know behaviour policy however fails converge retrace uses knowledge traces safely handle arbitrary policies comparison similarly need knowledge behaviour policy consequence able beneﬁt possible near on-policy situations cutting traces unnecessarily close. actually even lead better estimate analyzed continuous action space. mention theorems extend case continuous inﬁnite action spaces. trace coefﬁcients make densities instead probabilities min. possible removing technical assumption πk∧µk asympopen questions include totically commute relaxing markov assumption control case order allow trace coefﬁcients form validate theoretical results employ retrace experience replay setting sample transitions stored within large bounded replay memory subsequently replayed experience. naturally older data memory usually drawn policy differs current policy offering excellent point comparison algorithms presented section agent adapts architecture mnih replay short sequences memory instead single transitions. q-function target update sample sequence rt··· xt+k compare algorithms’ performance different atari games arcade learning environment using bellemare al.’s inter-algorithm score distribution. inter-algorithm scores normalized respectively correspond worst best score particular game within algorithms comparison. game inter-algorithm score algorithm score distribution function x}|/. roughly strictly higher curve corresponds better algorithm. across values performs best save obtains slightly superior performance. however highly sensitive choice retrace achieve dramatically higher performance q-learning early maintain advantage throughout. compared retrace offers narrower still marked advantage best performer games; claims remainder. per-game details given appendix. acknowledgments. authors thank daan wierstra nicolas heess hado hasselt ziyu wang david silver audrunas gr¯uslys georg ostrovski hubert soyer others google deepmind useful feedback work. references bellemare naddaf veness bowling arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research mahmood sutton off-policy learning based weighted importance sampling linear computational complexity. conference uncertainty artiﬁcial intelligence. recall deﬁnition increasingly greedy sequence policies. deﬁnition sequence policies increasingly greedy w.r.t. sequence functions following property holds obvious property holds policies greedy w.r.t. indeed case πk+qk+ πqk+ prove property holds εk-greedy policies well soft-max policies stated lemmas below. course policies satisfy property mina qk)). lemma non-increasing sequence. sequence policies εk-greedy w.r.t. sequence functions increasingly greedy w.r.t. sequence. used fact lemma non-decreasing sequence soft-max parameters. sequence policies soft-max w.r.t. sequence functions increasingly greedy w.r.t. sequence. assume centered fk-measurable noise term bounded variance bounded θkqk random sequence converges a.s. then assumptions theorem almost surely. proof. write prove result three steps. upper bound ﬁrst part proof similar proof matrix non-negative contraction mapping thus cannot directly apply proposition bertsekas tsitsiklis however seen proof theorem matrix γ-contraction mapping. relate using assumption πk∧µk commute asymptotically i.e. πk∧µk πk∧µk transition matrices apply proposition bertsekas tsitsiklis sequence matrices non-negative coefﬁcients bounded thus γ-contraction mappings ﬁxed point noise centered fk-measurable satisﬁes bounded variance assumption bounded thus limk straightforward indeed induction assume shown variance noise term bounded using fact reward function bounded. follows assumptions modiﬁed stepsize sequence satisﬁes conditions assumption second noise term measures difference online iterates corresponding ofﬂine values shown satisfy required assumption analogously argument proof prop. bertsekas tsitsiklis proof relies eligibility coefﬁcients rewards bounded trajectories ﬁnite conditions stepsizes satisﬁed. thus apply theorem conclude iterates proof. intuition asymptotically gets close deterministic policy πqk. case minimum distribution puts mass close greedy action mass actions thus gets close lemma applies indeed assumption ε-away have although experiments’ learning problem closely matches setting used mnih conducted trials multi-threaded cpu-based framework mnih obtaining ample result data affordable resources. differences follows. sixteen threads private environment instances train simultaneously; infers ﬁnds gradients w.r.t. local copy network parameters; gradients update master parameter local copies refreshed. target network parameters simply shared globally. thread private replay memory holding transitions optimizer unchanged shared rmsprop step size annealing environment frames exploration parameter behaviour differs slightly every frames threads switch randomly three schedules starting schedules intermediate positions left ones. experiments comprise atari games life loss treated episode termination. control minibatched one-step qlearning shows performance comparable multi-threaded setup. retrace runs minibatches four -step sequences current exploration policy target policy trials clamp rewards control q-function targets clamped prior gradient calculation; analogous quantities multi-step algorithms clamped scaled sequence length. coarse logarithmic parameter sweeps games asterix breakout enduro freeway h.e.r.o pong q*bert seaquest yielded step sizes rmsprop regularization parameters control multistep algorithms respectively. reported performance averages four trials different random seeds experimental conﬁguration. compared algorithms different values using score baseline. before compute inter-algorithm scores per-game basis. averaged interalgorithm scores across games produce table ﬁrst remark retrace always achieve score higher demonstrating efﬁcient sense section next note performs best small values begins fail values sense also safe. particularly problematic safe threshold likely problem-dependent. finally setting retrace performs particularly poorly; high values achieves close score games. retrace makes sense values trace cutting effect required off-policy learning taken care coefﬁcient. contrary relies value take care cutting traces off-policy data. table average inter-algorithm scores value scores ﬁxed across different corresponding inter-algorithm scores varies depending worst best performer within figure average inter-algorithm scores value scores ﬁxed across different corresponding inter-algorithm scores varies depending worst best performer within note average scores directly comparable across different values alien amidar assault asterix asteroids atlantis bank heist battle zone beam rider berzerk bowling boxing breakout carnival centipede chopper command crazy climber defender demon attack double dunk elevator action enduro fishing derby freeway frostbite gopher gravitar h.e.r.o. hockey james bond kangaroo krull kung-fu master montezuma’s revenge pac-man name game phoenix pitfall pooyan pong private q*bert river raid road runner robotank seaquest skiing solaris space invaders star gunner surround tennis time pilot tutankham venture video pinball wizard yar’s revenge zaxxon times best", "year": 2016}