{"title": "An Empirical Evaluation of Similarity Measures for Time Series  Classification", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Time series are ubiquitous, and a measure to assess their similarity is a core part of many computational systems. In particular, the similarity measure is the most essential ingredient of time series clustering and classification systems. Because of this importance, countless approaches to estimate time series similarity have been proposed. However, there is a lack of comparative studies using empirical, rigorous, quantitative, and large-scale assessment strategies. In this article, we provide an extensive evaluation of similarity measures for time series classification following the aforementioned principles. We consider 7 different measures coming from alternative measure `families', and 45 publicly-available time series data sets coming from a wide variety of scientific domains. We focus on out-of-sample classification accuracy, but in-sample accuracies and parameter choices are also discussed. Our work is based on rigorous evaluation methodologies and includes the use of powerful statistical significance tests to derive meaningful conclusions. The obtained results show the equivalence, in terms of accuracy, of a number of measures, but with one single candidate outperforming the rest. Such findings, together with the followed methodology, invite researchers on the field to adopt a more consistent evaluation criteria and a more informed decision regarding the baseline measures to which new developments should be compared.", "text": "time series ubiquitous measure assess similarity core part many computational systems. particular similarity measure essential ingredient time series clustering classiﬁcation systems. importance countless approaches estimate time series similarity proposed. however lack comparative studies using empirical rigorous quantitative large-scale assessment strategies. article provide extensive evaluation similarity measures time series classiﬁcation following aforementioned principles. consider diﬀerent measures coming alternative measure ‘families’ publicly-available time series data sets coming wide variety scientiﬁc domains. focus out-of-sample classiﬁcation accuracy in-sample accuracies parameter choices also discussed. work based rigorous evaluation methodologies includes powerful statistical signiﬁcance tests derive meaningful conclusions. obtained results show equivalence terms accuracy number measures single candidate outperforming rest. ﬁndings together followed methodology invite researchers ﬁeld adopt consistent evaluation criteria informed decision regarding baseline measures developments compared. usually represent valuable information subject analysis classiﬁcation indexing prediction interpretation real-world examples include ﬁnancial data medical data computer data motion data even object shapes handwriting eﬀectively transformed time series facilitating analysis retrieval core issue dealing time series determining pairwise similarity i.e. degree given time series resembles another. fact time series similarity measure central many mining retrieval clustering classiﬁcation tasks furthermore evidence simple approaches tasks exploiting generic time series similarity measures usually outperform elaborate sometimes speciﬁcally-targeted strategies. case instance time series classiﬁcation one-nearest neighbor approach using well-known time series similarity measure found outperform exhaustive list alternatives including decision trees multiscale histograms multi-layer perceptron neural networks order logic rules boosting multiple classiﬁer systems. deriving measure correctly reﬂects time series similarities straightforward. apart dealing high dimensionality calculation measures needs fast eﬃcient indeed better information gathering tools size time series data sets continue increase future. moreover need generic/multi-purpose similarity measures readily applied data whether application ﬁnal goal initial approach given task. last aspect highlights another desirable quality time series similarity measures robustness diﬀerent types data years several time series similarity measures proposed nevertheless quantitative comparisons made order evaluate eﬃcacy multiple-data framework apart interesting important task itself opposed clustering time series classiﬁcation oﬀers possibility straightforwardly assess merit time series similarity measures controlled objective quantitarecent study wang perform extensive comparison classiﬁcation accuracies measures across data sets coming various scientiﬁc domains. main conclusions study that even though newly proposed measures theoretically attractive eﬃcacy common well-established measures vast majority cases diﬃcult beat. speciﬁcally dynamic time warping found consistently superior studied measures addition authors emphasize euclidean distance remains quite accurate robust simple eﬃcient measuring similarity time series. finally looking detail results presented wang spot group time series similarity measures seems eﬃcacy comparable based edit distances. particular edit distance real sequences seems competitive slightly better dtw. interestingly none three measures initially targeted generic time series data introduced hindsight intuition behind euclidean distance relates spatial proximity initially devised speciﬁc task spoken word recognition edit distances introduced measuring dissimilarity strings study wang best knowledge comparative study dealing time series classiﬁcation using multiple similarity measures large collection data. general studies introducing measure compare measures usually using reduced data corpus furthermore lack agreement literature regarding evaluation methodologies. besides statistical signiﬁcance usually studied best improperly evaluated. inconvenient robust evaluation methodologies statistical signiﬁcance principal tools establish formal rigorous diﬀerences across considered measures addition optimal parameter values every measure rarely discussed. issues impact scientiﬁc development ﬁeld never sure e.g. measure used baseline future work perform empirical evaluation similarity measures time series classiﬁcation. follow initiative wang consider pool publicly-available time series data sets however instead additionally focusing representation methods computational/storage demands theoretical issues take pragmatic approach restrict classiﬁcation accuracy. believe important aspect considered ﬁrst stage that contrast aforementioned issues suﬃciently well-covered existing literature. considered measures decide include found generally achieve highest accuracies among measures compared wang apart choose euclidean distance plus diﬀerent measures considered study making total important contributions diﬀerentiate current work previous studies include extensive summary background considered measures basic formulations applications references formalization robust evaluation methodology exploiting standard out-of-sample cross-validation strategies rigorous statistical signiﬁcance tests order assess superiority given measure evaluation train test accuracies assessment optimal parameters measure data set. rest paper organized follows. firstly provide background time series similarity measures outline applications detail calculation next explain proposed evaluation methodology subsequently report obtained results conclusion section ends paper list approaches dealing time series similarity vast comprehensive enumeration beyond scope present work section present several representative examples diﬀerent ‘families’ time series similarity measures lock-step measures feature-based measures model-based measures elastic measures eﬀort made selecting standard measures group emphasizing approaches reported good performance. also avoid measures many parameters since parameters diﬃcult learn small training data sets furthermore could lead over-ﬁtting. alternative measures found consistently less accurate considered apart aforementioned measures also include random measure consisting uniformly distributed random number random baseline. positive integer length time series i-th element time series respectively. measures based norms correspond group so-called lock-step measures compare samples exactly temporal location notice case time series length always re-sample length other approach works well number data sources using obtain euclidean distance used time series dissimilarity measures favored computational simplicity indexing capabilities. applications range early classiﬁcation time series rule discovery economic communications ecological time series authors state accuracy euclidean distance diﬃcult beat specially large data sets containing many time series best knowledge claims quantitatively supported one-nearest neighbor classiﬁcation experiments using artiﬁcially-generated/synthetic data sets believe claims need carefully assessed extensive experiments broader conditions considering multiple measures diﬀerent distance-exploiting algorithms real-world data sets. complex value pairs denoting i-th fourier coeﬃcient discrete fourier transforms time series notice introduce parameter actual number considered coeﬃcients. symmetry needs performed most half coeﬃcients notice that parseval theorem euclidean distance equivalent standard euclidean distance time series however parameter usually takes opportunity ﬁlter high-frequency coeﬃcients i.e. coeﬃcients whose close eﬀect removing rapidly-ﬂuctuating components signal. hence high frequencies relevant intended analysis high-frequency noise operation usually carry increase accuracy. furthermore relatively small similarity computations substantially accelerated. computing euclidean distance reduced features extremely common approach literature. standard choice eﬃcient time series retrieval exploiting aforementioned acceleration capabilities. pioneering work includes agrawal faloutsos dealing synthetic ﬁnancial data. recent works data domains. instance case-based reasoning system montani uses compare medical time series. apart wavelet coeﬃcients extensively used instance olsson wavelet analysis remove noise extract features system fault diagnosis industrial equipment. research suggests that although provide advantages wavelet coeﬃcients generally outperform considered task comparatively less used time series features based singular value decomposition piece-wise aggregate approximations coeﬃcients ﬁtted polynomials among others. option computing similarities time series using features extracted employ time series models main idea behind model-based measures learn model time series parameters computing similarity value. literature several approaches follow idea. instance maharaj uses p-value chi-square statistic cluster auto-regressive coeﬃcients representing stationary time series. ramoni present bayesian algorithm clustering time series. transform series markov chain cluster similar chains discover probable generating processes. povinelli gaussian mixture models reconstructed phase spaces classify time series diﬀerent sources. serr`a study error several learned models identify similar time series corresponding musical information. denotes j-th regression coeﬃcient order model estimate coeﬃcients e.g. yule-walker function then dissimilarity time series calculated instance using euclidean distance estimated coeﬃcients analogously number coefﬁcients controlled parameter which similarly directly aﬀects ﬁnal speed similarity calculations dynamic time warping classic approach computing dissimilarity time series. exploited countless works construct decision trees retrieve similar shapes large image databases match incomplete time series medical applications align signatures identity authentication task etc. addition several extensions speeding calculations exist works optimally aligning time series temporal domain accumulated cost alignment minimal canonical form accumulated cost obtained dynamic programming recursively applying time series domain-speciﬁc knowledge local cost function must chosen appropriately although euclidean distance often used. ﬁnal dissimilarity measure typically corresponds total accumulated cost i.e. ddtw normalization ddtw performed basis alignment time series found backtracking however preliminary analysis found normalized variant equivalent sensibly less accurate unnormalized one. progressively adjusted dealing diﬀerent time series lengths i.e. in/m using round-to-the-nearest-integer operator. notice ddtw correspond squared euclidean distance notice furthermore that using unconstrained version thus include variants single formulation. general introduction constraints specially window parameter carries advantages instance constraints prevent ‘pathological alignments’ therefore usually provide better similarity estimates moreover constraints allow reduced computational costs since percentage cells needs examined currently stands main benchmark similarity measures need compared measures proposed systematically outperform number diﬀerent data sources. measures usually complex sometimes requiring extensive tuning parameters. additionally often case careful rigorous extensive evaluation accuracy measures done studies fail assess statistical signiﬁcance improvement. thus could superiority measures best unclear. paper special attention aspects order formally assess considered measures common framework. shown exists similarity measure outperforming statistically signiﬁcant margin turning previous evidence observe perhaps measure able seriously challenge edit distance real sequences corresponds extension original edit levensthein distance real-valued time series. extensions commonplace recent research starting focus noted chen outperformed previous edit distance variants time series similarity. controls degree resemblance time series samples considered match. ﬁrst initialized ﬁrst column following chen initially reported accuracy improvements local cost function absolute time-warped edit distance perhaps interesting extension dynamic programming algorithms like edr. sense combination two. like edit distances twed comprises mismatch penalty like dynamic time warping introduces so-called stiﬀness parameter controlling ‘elasticity’ uniformly-sampled time series formulation twed corresponds takes time stamp diﬀerences account. therefore able cope time series diﬀerent sampling rates including down-sampled time series. interesting aspect contrasting measures twed metric thus exploit triangular inequality speed search metric space. finally worth mentioning combination previous characteristics results lower bound twed dissimilarity used speed nearest neighbor retrieval. done notice that akin general formulation twed term introduces nonlinear penalty depends temporal gap. here value proportional standard deviation expected time series time proportional eﬃcacy time series similarity measure commonly evaluated classiﬁcation accuracy achieves that error ratio distance-based classiﬁer calculated given labeled data understanding error ratio number wrongly classiﬁed items divided total number tested items. standard choice classiﬁer one-nearest neighbor classiﬁer. following wang enumerate several advantages using approach. first error classiﬁer critically depends similarity measure used. second classiﬁer parameter-free easy implement. third theoretical results relating error classiﬁer errors obtained classiﬁcation schemes. fourth works suggest best results time series classiﬁcation come simple nearest neighbor methods. details aspects refer mitchell hastie references provided wang perform experiments publicly-available time series data sets time series repository world’s biggest time series repository authors estimate makes publicly-available labeled data sets repository comprises synthetic well real-world data sets also includes one-dimensional time series extracted two-dimensional shapes data sets considered correspond totality repository march within data sets number classes ranges number time series data ranges time series lengths samples. details data sets refer properly assess classiﬁer’s error out-of-sample validation needs done experiments follow standard fold cross-validation scheme using balanced data sets i.e. using number items class. repeat validation times report average error ratios. balancing data sets allows balanced error estimations regarding class distribution repeating cross-fold validation several times allows precise estimations cross-fold validation scheme essential avoiding bias particular split data could introduce also computed error ratios original splits provided time series repository allowed conﬁrm error ratios implementations euclidean distance agree values reported there. addition observed error ratios obtained splits substantially diﬀerent ones obtained cross-validation point even modifying ranking algorithms respect error ratios data sets. indicates potential bias individual splits aspect well-known machine learning community refer interested reader machine learning textbook in-depth discussion cross-fold validation schemes appropriateness individual splits. besides using single split diﬃcults statistical signiﬁcance assessment full account error ratios measures data sets available online including error ratios aforementioned original splits. assess statistical signiﬁcance diﬀerence error ratios employ well-known wilcoxon signed-rank test wilcoxon signed-rank test non-parametric statistical hypothesis test used comparing repeated measurements order assess whether population mean ranks diﬀer. natural alternative student’s t-test dependent samples population distribution cannot assumed normal given data fold accuracies). besides comparing similarity measures global basis using data sets employ input average accuracy values obtained data set. following common practice threshold signiﬁcance level additionally compensate multiple pairwise comparisons apply holm-bonferroni method post-hoc statistical analysis method controlling so-called family-wise error rate powerful usual bonferroni correction performing experiments time series data sets z-normalized individual time series zero mean unit variance. furthermore optimized measures’ parameters training phase cross-validation. optimization step consisted grid search within suitable range parameter values forcing number parameter combinations algorithm values grid chosen according common practice speciﬁcations given papers introducing measure speciﬁcally table parameter grid considered similarity measures consider extra value corresponding unconstrained euclidean conﬁguration respectively. parameter values linearly spaced except logarithmically spaced. corresponding squared euclidean distance variant grid search parameter value yielding lowest leave-one-out error ratio training kept out-of-sample testing. look overall results considered measures clearly outperform random baseline practically data sets furthermore achieve near-perfect accuracies number data sets however single measure achieves best performance data sets. euclidean distance found best-performing measure data sets best-performing data sets twed count data sets measure statistically signiﬁcantly outperforms rest numbers reduce euclidean data words adiac beef chlorineconcentration cinc torso coﬀee cricket cricket cricket diatomsizereduction ecgfivedays point haptics mallat medicalimages motestrain noninvasivefetalecg noninvasivefetalecg oliveoil osuleaf trace patterns twoleadecg uwavegesturelibrary uwavegesturelibrary uwavegesturelibrary wafer wordssynonyms yoga table error ratios considered measures data sets. symbol denotes statistically signiﬁcant diﬀerence respect measures given data last contains average rank measure across data sets beyond accuracies latter aspect potentially highlight inherent data qualities. instance fact feature/model-based measure clearly outperforms others particular data indicates time series well characterized extracted features/ﬁtted model addition good performance euclidean elastic measures gives intuition importance alignments warping sample correspondences general twed outperforms measures several data sets average rank fact compare considered measures global scale taking matched error ratios across data sets obtain twed statistically signiﬁcantly superior rest next form group equivalent measures statistically signiﬁcant diﬀerence them. performed statistical analysis also separates remaining measures also themselves. apart global analysis pairwise comparisons made conﬁrming aforementioned global tendencies choosing optimal parameters given measure data solely dispose training data. hence important know whether error ratios training testing sets similar otherwise could incurring so-called texas sharpshooter fallacy i.e. could predict measure’s utility ahead time looking training data. comparing train test error ratios compute error gain value couple measures data check whether values train test agree. kind real-valued contingency table plotted called texas sharpshooter plot batista space reasons show contingency tables twed euclidean distance results show error gains twed dtw/euclidean mostly agree training testing. mentioned sec. full account train test errors available online. close look full results that general figure plot distribution performance ranks measure across data sets. dashed lines denote statistically signiﬁcantly equivalent groups measures best-performing measure training stage also best-performing measure testing stage. exceptions easily listed relative rankings measures perform best also mostly agree train test. ﬁnally report parameters chosen measure training balanced data firstly observe that vast majority cases speciﬁc value given parameter consistently tributions fig. among consistent choices perhaps twed present spread distributions. aspect together fairly good accuracies obtained speciﬁc measures indicates certain degree robustness speciﬁc parameter choices. desirable quality time series similarity measure even train classiﬁer potentially incomplete training instances. figure error ratios comparison twed lower-right triangular part corresponds twed outperforming whereas upper-left part corresponds opposite case. green squares indicate statistically signiﬁcant performance diﬀerences speciﬁed ranges thus indicating reasonable choice made particularly true edr. measure could potentially beneﬁt reconsidering parameters’ range twed. seen seem consistently chosen lower upper parts speciﬁed ranges respectively. suggests best combination data sets could outside parameter case twed could potentially achieve even much higher accuracies. interestingly twed best-performing measure data sets ‘border’ parameter values chosen finally comment particularities data sets relation classiﬁcation. instance relatively large window parameter chosen data sets denotes tracking alignments warping paths beyond main diagonal might advantageous classiﬁcation data sets. interestingly stiﬀness figure texas sharpshooter plots twed euclidean distance here error gain measured subtracting twed error ratio dtw/euclidean. dots around diagonal indicate agreement error gain train test. false positives i.e. dots lower-right quadrant indicate twed best measure training reach lowest error testing. instance case twed euclidean oliveoil data false positive stands parameter accounts similar opposite concept takes relatively small values. agreement across diﬀerent measures reinforces hypothesis tracking intricate alignments strongly warped paths advantageous data sets. analogous complementary conclusions derived data sets. instance data sets small number coeﬃcients chosen. achieve competitive accuracies speciﬁc data sets could suspect low-frequency components important correctly classifying instances data sets general perspective obtained results show group equivalent similarity measures statistically signiﬁcant differences among existing literature suggests longest common sub-sequence approaches together alternative variants actually outperformed others training column gain corresponds absolute value train error gain i.e. absolute diﬀerence error ratios training stage morse patel could potentially join group however according results reported here twed measure originally proposed marteau seems consistently outperform considered distances including mjc. thus believe often unconsidered measure take baseline role future evaluations time series similarity measures euclidean distance although somehow competitive generally performs statistically signiﬁcantly worse twed edr. accuracy large data sets also impressive. euclidean distance statistically signiﬁcantly random baseline measures. course general statements exclude possibility particular measure variant could well-suited speciﬁc data statistically signiﬁcantly outperform rest sec. enumerated several examples that. comparing train test errors seen mostly agree train errors generally providing good guess test errors unseen data. listed notable exceptions rule used texas sharpshooter plots assess aspect twed euclidean. assessing best parameter choices measure similarity measure crucial step computational approaches dealing time series. however additional issues worth mentioning particular regard post-processing steps focused improving similarity assessments interesting post-processing step complexity-invariant correction factor introduced batista correction factor prevents assigning dissimilarity values time series diﬀerent complexity thus preventing inclusion time series diﬀerent nature cluster. assess complexity depends situation batista introduce quite straightforward norm sample-based derivative time series. overall considering diﬀerent types ‘invariance’ sensible approach here already implicitly considered number them although pre-processing method-speciﬁc strategy global amplitude scale invariance warping invariance phase invariance occlusion invariance another interesting post-processing step hubness correction time series classiﬁcation introduced radovanoviˇc based ﬁnding instances high-dimensional spaces tend become hubs unexpectedly considered nearest neighbors several instances correction factor introduced. usually harm classiﬁcation accuracy deﬁnitely improve performance data sets strategy enhancing time series similarity potentially reducing hubness unsupervised clustering algorithms prune nearest neighbor candidates future work focus real quantitative impact strategies enhancing time series similarity like ones above special emphasis impact diﬀerent measures classiﬁcation schemes. besides getting global picture highlighting relevant approaches pushes towards uniﬁed validation procedures analysis tools. hoped article serve steppingstone interested advancing time series similarity clustering classiﬁcation. thank people made available contributed time series repository. research funded -sgr- generalitat catalunya jaedoc/ consejo superior investigaciones cient´ıﬁcas tin--c- tin-c- spanish government.", "year": 2014}