{"title": "Failures of Gradient-Based Deep Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four types of simple problems, for which the gradient-based algorithms commonly used in deep learning either fail or suffer from significant difficulties. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.", "text": "recent years deep learning become go-to solution broad range applications often outperforming state-of-the-art. however important theoreticians practitioners gain deeper understanding difﬁculties limitations associated common approaches algorithms. describe four types simple problems gradient-based algorithms commonly used deep learning either fail suffer signiﬁcant difﬁculties. illustrate failures practical experiments provide theoretical insights explaining source might remedied. success stories deep learning form ever lengthening list practical breakthroughs state-ofthe-art performances ranging ﬁelds computer vision audio natural language processing generation well robotics name few. list success stories matched surpassed list practical tips tricks different optimization algorithms parameter tuning methods initialization schemes architecture designs loss functions data augmentation current theoretical understanding deep learning sufﬁcient rigorous analysis difﬁculties faced practitioners. progress must made parties practitioner’s perspective emphasizing difﬁculties provides practical insights theoretician turn supplies theoretical insights guarantees strengthening sharpening practical intuitions wisdom. particular understanding failures existing algorithms important understanding succeed. goal paper present discuss families simple problems commonly used methods show exceptional performance might expect. empirical results insights ground build theoretical analysis characterising sources failure. understandings aligned sometimes lead different approaches either architecture loss function optimization scheme explain superiority applied members families. interestingly sources failure experiment seem relate stationary point issues spurious local minima plethora saddle points topic much recent interest start section discussing class simple learning problems gradient information central deep learning algorithms provably carries negligible information target function attempt learn. result property learning problems themselves holds speciﬁc network architecture choose tackling learning problem implying gradientbased method likely succeed. analysis relies tools insights statistical queries literature underscores main deﬁciencies deep learning reliance local properties loss function objective global nature. next section tackle ongoing dispute common approaches learning. most learning optimization problems viewed structured sub-problems. ﬁrst approach refer end-to-end approach tend solve sub-problems together shot optimizing single primary objective. second approach refer decomposition tend handle sub-problems separately solving deﬁning optimizing additional objectives rely solely primary objective. beneﬁts end-to-end approach terms requiring smaller amount labeling prior knowledge perhaps enabling expressive architectures cannot ignored. hand intuitively empirically extra supervision injected decomposition helpful optimization process. experiment simple problem application approaches possible distinction clear intuitive. observe end-to-end approach much slower decomposition method extent that scale problem grows progress observed. analyze showing theoretically empirically gradients much noisy less informative end-to-end approach opposed decomposition approach explaining disparity practical performance. section demonstrate importance network’s architecture optimization algorithm training time. choice architecture usually studied context expressive power show even architectures expressive power given task tremendous difference ability optimize them. analyze required runtime gradient descent architectures lens condition number problem. show conditioning techniques yield additional orders magnitude speedups. experimental setup section around seemingly simple problem encoding piece-wise linear one-dimensional curve. despite simplicity problem show following common rule perhaps deeper/wider network signiﬁcantly help here. finally section consider deep learning’s reliance vanilla gradient information optimization process. previously discussed deﬁciency using local property objective directing global optimization. here focus simple case possible solve optimization problem based local information form gradient. experiment architectures contain activation functions regions leads well known vanishing gradient problem. practitioners take great care working activation functions many heuristic tricks applied order initialize network’s weights non-ﬂat areas activations. here show using different update rule manage solve learning problem efﬁciently. moreover show convergence guarantees family functions. provides clean example non-gradient-based optimization schemes overcome limitations gradient-based https//github.com/shakedshammah/failures_of_dl. command lines appendix http//joelgrus.com////fizz-buzz-in-tensorflow/ inspiration behind quote. existing deep learning algorithms gradient-based methods; namely algorithms optimize objective access gradient w.r.t. weight vector estimates gradient. consider setting goal optimization process learn underlying hypothesis class member responsible labelling data. yields optimization problem form underlying assumption gradient objective w.r.t. contains useful information regarding target function help make progress. below discuss family problems high probability ﬁxed point gradient essentially regardless underlying target function furthermore prove holds independently choice architecture parametrization using deeper/wider network help. family study compositions linear periodic functions experiment classical problem learning parities. empirical theoretical study shows indeed there’s little information gradient using learning cannot succeed. experiment begin simple problem learning random parities choosing uniformly random goal train predictor mapping uniformly distributed. words indicates whether number certain subset coordinates even. experiments hinge loss simple network architecture fully connected layer width relu activations fully connected output layer linear activation single unit. note class realizes parity function corresponding empirically dimension increases difﬁculty learning measured accuracy arrive ﬁxed number training iterations point around advance beyond random performance observed reasonable time. figure illustrates results. intuitively measures expected amount signal underlying target function contained gradient. later variance correlates difﬁculty solving using gradientbased methods. proof given appendix theorem implies learn unknown target function possibly coming large collection uncorrelated functions sensitivity gradient target function point decreases linearly |h|. make general statement return case parities study lens framework. suppose target function parity function chosen uniformly random i.e. random element functions }d}. non-zero therefore theorem exponentially small dimension chebyshev’s inequality implies gradient point extremely concentrated around ﬁxed point independent phenomenon exponentially-small variance also observed distributions learning problems parities. indeed shown also holds general setup output corresponds linear function composed periodic input sampled smooth distribution theorem ﬁxed periodic function suppose sampled arbitrary mixture distributions following ˆϕdx property square root density function fourier transform satisfying exp). denotes objective function respect squared loss condition fourier transform density generally satisﬁed smooth distributions thus bound extremely small long norm dimension moderately large indicates gradients contains little signal underlying target function. based bounds also formally prove gradient-based method reasonable model fail returning reasonable predictor unless number iterations exponentially large provides strong evidence gradient-based methods indeed cannot learn random parities linear-periodic functions. emphasize results hold regardless class predictors problem lies using gradient-based method train them. also note difﬁculty lies random choice problem difﬁcult known ﬁxed advance problem known solvable appropriate lstm network finally remark connection parities difﬁculty learning orthogonal functions already made context statistical query learning refers algorithms constrained interact data receiving estimates expected value query underlying distribution well-known parities cannot learned algorithms. recently formally shown gradient-based methods approximate gradient oracle implemented statistical query algorithm implies gradient-based methods indeed unlikely solve learning problems known hard statistical queries framework particular parities. discussion random parities above simply made connection gradient-based methods parities explicit direct examination gradients’ variance w.r.t. target function. many practical learning problems generally algorithmic problems viewed structured composition sub-problems. applicable approaches solution either tackling problem end-to-end manner decomposition. whereas traditional algorithmic solution divideand-conquer strategy obvious choice ability deep learning utilize data expressive architectures made end-to-end training attractive alternative. prior results end-to-end decomposition added feedback approaches show success directions. here address following questions price rather appealing end-to-end approach? letting network learn itself idea? necessary worth effort help various aspects considered context. example analyzed difference approaches sample complexity point view. here focus optimization aspect showing end-to-end approach might suffer non-informative noisy gradients signiﬁcantly affect training time. helping process decomposing problem leads much faster training. present simple experiment motivated questions every practitioner must answer facing trivial problem exactly required training data network architecture used right distribution development efforts. correlated questions clear answer. experiments analysis show making wrong choice expensive. experiment compares approaches computer vision setting convolutional neural networks become widely used successful algorithmic architectures. deﬁne family problems parameterized show performances end-to-end decomposition approaches. sample image associated sample everywhere except straight line length centered rotated angle note images space discrete round values corresponding points lines closest integer coordinate. concatenate scores tuple’s entries transform range using sigmoid function feed resulting vector another network similar architecture deﬁned section outputting single tuple-score thresholded obtaining binary prediction. whole architecture denoted assuming expressive enough provide least weak learner express relevant parity function obtain architecture potential good performance. figure performance comparison section experiment. blue curves correspond end-to-end decomposition approaches respectively. plots show zero-one accuracy respect primary objective held test function training iterations. trained end-to-end network iterations decomposition networks iterations. speciﬁc value hence priori irrelevant. decomposition approach would minimize losses assumption direct towards area know resulting outputs separated note using possible values known empirically comparing performances based primary objective end-to-end approach signiﬁcantly inferior decomposition approach using decomposition quickly arrive good solution regardless tuple’s length solvable described section however using end-to-end approach works completely fails already somewhat surprising end-to-end approach optimizes exactly primary objective composed sub-problems easily solved additional irrelevant objectives. study experiment directions theoretically analyzing gradient variance somewhat idealized version experiment empirically estimating signalto-noise ratio measure stochastic gradients used algorithm. approaches point similar issue end-to-end approach gradients seem sufﬁciently informative optimization process succeed. continuing note conceptually similar experiment reported however experiment came without formal analysis failure attributed local minima. contrast analysis indicates problem local-minima gradients non-informative noisy. begin theoretical result considers experimental setup simplifying assumptions first input assumed standard gaussian second assume labels generl= signxl). ﬁrst assumption merely simplify ated target function form second assumption equivalent assuming labels individual images realized linear predictor roughly case simple image labelling task ours. proof given section theorem shows signal regarding drawn uniformly random functions decreases exponentially similar parity result section important difference whereas base exponent much smaller quantity indicates already small log/ values information contained gradients become extremely small prevent gradient-based methods succeeding fully according experiment. complement analysis consider related signal-to-noise quantity empirically estimated actual experiment. motivate note quantity used proof theorem estimating amount signal carried gradient squared norm correlation gradient predictor estimate ratio experiment. well-known amount noise stochastic gradient estimates used stochastic gradient descent crucially affects convergence rate. hence smaller correlated worse performance. empirically estimated measure sigy/noiy gradients w.r.t. weights last layer initialization point parameter space. estimate various values plotted figure indeed appears approach extremely small values estimator’s noise additional section appendix also present second synthetic experiment demonstrates case decomposition approach directly decreases stochastic noise optimization process hence beneﬁting convergence rate. network architecture choice crucial element success deep learning. variants development novel architectures main tools achieving practical breakthroughs choosing architecture consideration inject prior knowledge problem hand improving network’s expressiveness problem dramatically increasing sample complexity. another aspect involves improving computational complexity training. section formally show choice architecture affects training time lens condition number problem. study practice conditioning techniques convex non-convex problems gained much attention recently show architectural choice dramatic effect applicability better conditioning techniques. learning problem consider section encoding one-dimensional piecewise linear curves. show different architectures sufﬁcient expressive power solving problem orders-of-magnitude difference condition numbers. particular becomes apparent considering convolutional fully connected layers. sheds light success convolutional neural networks generally attributed sample complexity beneﬁts. moreover show conditioning applied conjunction better architecture choice decrease condition number orders magnitude. direct effect convergence rate analyzed aligned signiﬁcant performance gaps observed empirically. also demonstrate performance signiﬁcantly improve employing deeper powerful architectures well price comes choosing sub-optimal architecture. experiment various deep learning solutions encoding structure one-dimensional continuous piecewise linear curves. curve pieces written difference slope i’th segment segf outputting values b{ai θi}k encoding problem since would like able rebuild values b{ai θi}k assume without loss generality ment sample i.i.d. uniformly hence start attempting learn linear transformation directly using connected architecture layer output channels. weights layer denoted therefore minimize objective sampled according distribution. convex realizable problem convergence guaranteed explicitly analyze rate. however perhaps unexpectedly observe slow rate convergence satisfactory solution signiﬁcant inaccuracies present non-smoothness points. figure illustrates results. proof given appendix next analyze iteration complexity learning matrix give explicit expression expected value learned weight matrix iteration denoted proof given appendix note assumption holds distributional assumption curves changes direction curve independent sampled time distribution. following theorem establishes lower bound ˆut+ jensen’s inequality implies lower bound ˆut+ expected distance ˆut+ note lower bound holds even data updating theorem singular value decomposition diverges. otherwise proof given appendix theorem implies condition number hence number iterations required convergence scales quite poorly next subsection decrease condition number problem. empirically convergence accurate solution faster using architecture. figure illustrates examples. theoretically understand beneﬁt using convolution perspective required number iterations training consider problem’s condition number providing understanding training time. previous section requires iterations learn full matrix appendix show mild assumptions condition number requires order iterations learn optimal ﬁlter section despite observing improvement fully connected architecture still requires iterations even simple problem learning ﬁlter motivates application additional conditioning techniques hope extra performance gains. first explicitly represent convolutional architecture linear regression problem. perform vecrow operation follows given sample construct matrix size t’th then obtain vanilla linear regression problem ﬁlter solution. given sample approximate correlation matrix denoted setting calculate matrix replace every instance instance c−/. construction correlation matrix resulting instances approximately identity matrix hence condition number approximately follows converges using order iterations independently empirically quickly converge extremely accurate results illustrated figure note convolution architecture crucial efﬁciency conditioning; dimension problem reduced dramatically difﬁculty estimating large correlation matrix scales strongly furthermore inversion becomes costly operation. combined better architecture conditioning allows gain dramatic improvement. solution arrived section indicates suitable architecture choice conditioning scheme provide training time speedups multiple orders magnitude. moreover beneﬁt reducing number parameters transition fully connected architecture convolutional shown helpful terms convergence time. however rule possibility deeper wider network suffer deﬁciencies analyzed convex case. motivated success deep auto-encoders experiment deeper architecture enef deep networks coding namely minimize minvv parametrized weight vectors output dimension enough realization encoding problem. networks three layers relu activations except output layer linear activation. dimensions layers aligned intuition gained previous experiments observe additional expressive power unnecessary solve inherent optimization problems stronger auto-encoder fails capture details non-smooth points. figure examples. examine different aspect gradient-based learning poses difﬁculties optimization namely ﬂatness loss surface saturation activation functions leading vanishing gradients slow-down training process. problem ampliﬁed deeper architectures since likely backpropagated message lower layers architecture would vanish saturated activation somewhere along way. major problem using sigmoids gating mechanisms recurrent neural networks lstms grus non-local search-based optimization large scale problems seems beyond reach variants gradient update whether adding momentum higher order methods normalized gradients quite successful leading consideration update schemes deviating vanilla gradient updates. section consider family activation functions amplify vanishing gradient saturated activation problem; piecewise ﬂat. using activations neural network architecture result gradient equal completely useless. consider different ways implement approximate learn activations error effectively propagate them. using different variant local search-based update based arrive figure examples decoded outputs section experiments learning encode curves. blue original curves. decoded curves. plot shows outputs curves iterations left right. experimental setup consider following optimization setup. sample space symmetrically distributed. target function form monotonically non-decreasing function. objective optimization problem given known. words given function rounds nearest also experiment normally distributed theoretical analysis restricted speciﬁc form normal ﬁgures found figure constant sigmoid function intuitively approximate steps using sigmoids amplitude corresponding step’s height centered step’s position. similar motivation using sigmoids activation functions gates lstm cells non-ﬂat approximation step function. example approximation although objective completely continuous suffers ﬂatness continuity deﬁciencies original training using objective much slower sometimes completely failing. particular sensitivity initialization bias term observed wrong initialization cause starting point wide region hence region network parametrized weight vector simple architecture four fully connected layers ﬁrst three relu activations output channels last output channel activation function. covered section difﬁculty arises regressing smooth functions. case even continuous inaccuracies capturing continuity points brought forefront. moreover solution extra price terms sample complexity training time test time much larger necessary network. advantage course minimal prior knowledge required. approach manages reasonable solution perfect. experiment approach problem general multi-class classiﬁcation problem value image treated separate class. similar architecture end-to-end experiment less hidden layer ﬁnal layer outputting outputs corresponding steps deﬁned zis. problem inaccuracies boundaries classes lack structure imposed predictor. fact linear connection input imposed architecture results blurry boundaries. addition fact rely improper approach sense ignore ordering imposed results higher sample complexity. terms backpropagation algorithm kind update interpreted replacing backpropagation message activation function identity message. notation simplicity omitted bias terms forward-only concept applied too. method empirically achieves best results terms ﬁnal accuracy training time test time cost. mentioned before method proven converge \u0001-optimal solution additional assumptions function l-lipschitz constrained bounded norm. completeness provide short proof appendix figure section experiment backpropagating steps function. horizontal axis value dashed green curves show label curves show outputs learnt hypothesis. plots zoomed around mean non-ﬂat approximation plot dashed magenta curve shows non-ﬂat approximation namely note inaccuracies around boundaries classes multi class experiment. plots show results training iterations except forward plot showing results iterations. paper considered different families problems standard gradient-based deep learning approaches appear suffer signiﬁcant difﬁculties. analysis indicates difﬁculties necessarily related stationary point issues spurious local minima plethora saddle points rather subtle issues insufﬁcient information gradients underlying target function; snr; conditioning; ﬂatness activations consider ﬁrst step towards better understanding standard deep learning methods might fail well approaches might overcome failures. acknowledgements research supported part intel collaborative research institute computational intelligence european research council also supported part marie curie grant israel science foundation grant", "year": 2017}