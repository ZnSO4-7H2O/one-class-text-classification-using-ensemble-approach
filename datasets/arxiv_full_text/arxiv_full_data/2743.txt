{"title": "Bayesian Hypernetworks", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We propose Bayesian hypernetworks: a framework for approximate Bayesian inference in neural networks. A Bayesian hypernetwork, $h$, is a neural network which learns to transform a simple noise distribution, $p(\\epsilon) = \\mathcal{N}(0,I)$, to a distribution $q(\\theta) \\doteq q(h(\\epsilon))$ over the parameters $\\theta$ of another neural network (the \"primary network\"). We train $q$ with variational inference, using an invertible $h$ to enable efficient estimation of the variational lower bound on the posterior $p(\\theta | \\mathcal{D})$ via sampling. In contrast to most methods for Bayesian deep learning, Bayesian hypernets can represent a complex multimodal approximate posterior with correlations between parameters, while enabling cheap i.i.d. sampling of $q(\\theta)$. We demonstrate these qualitative advantages of Bayesian hypernets, which also achieve competitive performance on a suite of tasks that demonstrate the advantage of estimating model uncertainty, including active learning and anomaly detection.", "text": "propose bayesian hypernetworks framework approximate bayesian inference neural networks. bayesian hypernetwork neural network learns transform simple noise distribution distribution parameters another neural network network\"). train variational inference using invertible enable efﬁcient estimation variational lower bound posterior sampling. contrast methods bayesian deep learning bayesian hypernets represent complex multimodal approximate posterior correlations parameters enabling cheap i.i.d. sampling demonstrate qualitative advantages bayesian hypernets also achieve competitive performance suite tasks demonstrate advantage estimating model uncertainty including active learning anomaly detection. simple powerful techniques bayesian inference deep neural networks’ parameters potential dramatically increase scope applications deep learning techniques. real-world applications unanticipated mistakes costly dangerous whereas anticipating mistakes allows agent seek human guidance engage safe default behavior dnns typically trained single likely value parameters approach neglects uncertainty parameters fact best translate higher predictive uncertainty likely parameter values yield highly conﬁdent contradictory predictions. conversely bayesian dnns model full posterior distribution models parameters given data thus provide better calibrated conﬁdence estimates corresponding safety beneﬁts techniques bayesian dnns active research topic. recent work focuses variational inference restricts variational posterior simple family distributions instance factorial gaussian unfortunately safety perspective variational approximations tend underestimate uncertainty direction kl-divergence strongly penalizes putting mass mass bayesian deep learning capture parameter uncertainty approaches including ours emphatically capture uncertainty model correct parameter uncertainty often called \"model uncertainty\" literature prefer terminology emphasizes existence uncertainty model speciﬁcation. putting mass high mass. problem exacerbated using restricted family posterior distribution; instance unimodal approximate posterior generally capture single mode true posterior. mind propose learning extremely ﬂexible powerful posterior parametrized refer bayesian hypernetwork reference bayesian hypernetwork takes random noise input outputs sample approximate posterior another interest insight building model invertible hypernet enables monte carlo estimation entropy term variational inference training objective. begin paper reviewing previous work bayesian dnns explaining necessary components approach explain compose techniques yield bayesian hypernets design choices make work techniques stabilizing training finally present experiments validate expressivity bhns demonstrate competitive performance across several tasks bayesian dnns studied since thorough review broadly speaking existing methods either markov chain monte carlo directly learn approximate posterior distribution using variational inference expectation propagation α-divergences focus popular approach variational inference. notable recent work area includes ghahramani kingma interpret popular dropout algorithm variational inference method advantages simple implement allowing cheap samples yields unimodal approximate posterior allow arbitrary dependencies parameters. important points reference work bayes backprop multiplicative normalizing ﬂows bayes backprop viewed trivial instance bayesian hypernet hypernetwork performs element-wise scale shift noise similar work louizos welling propose dismiss bhns issues scaling bhns large primary networks address. instead work hypernet generate scaling factors means factorial gaussian distribution. follow complicated distribution forms highly ﬂexible approximate posterior network approximate order estimate entropy term variational lower bound resulting lower bound variational lower bound. finally variational autoencoder family generative models likely best known application variational inference dnns note bayesian sense. vaes approximate posterior latent variables given datapoint; bayesian dnns approximate posterior model parameters given dataset. hypernetwork neural outputs parameters another neural hypernet primary together form single model trained backpropagation. ﬁxing parameters hypernet varying inputs generate different primary parameters. previous works inputs hypernetwork either functions input primary activations learned parameters work louizos welling ﬁrst know random noise input hypernet. number parameters scales approximately quadratically number units layer naively parametrizing large primary would require impractically large hypernet efﬁcient parametrization hypernets however actually compress total size network simple illustration factoring weight matrix e×h× viewed using simple hypernet compress rows -dimensional encodings conditional batch norm closely related conditional instance normalization feature-wise linear modulation methods viewed speciﬁc forms hypernet. works weights primary parametrized directly hypernet outputs scale shift parameters every neurons; viewed selecting features signiﬁcant present works condition primary net’s behaviour information task form task natural language task-exemplars work employ related technique weight normalization normalizes input weights every neuron introduces separate parameter scale. proposed bayesian hypernetworks employ differentiable directed generator network generative model primary parameters. ddgns neural transform simple noise samples complex distribution common component modern deep generative models variational autoencoders generative adversarial networks take advantage techniques invertible ddgns developed several recent works generative modeling variational inference latent variables training models uses change variables formula involves computing log-determinant inverse jacobian generator network. computation involves potentially costly matrix inversion works propose innovative architectures reduce cost still express complicated deformations. particular realnvp method employs series \"coupling layers\" introduce dependencies dimensions input. name \"hypernetwork\" comes describe general hypernet framework applications idea convolutional networks previously explored brabandere bertinetto variational inference goal maximize lower bound marginal likelihood data involve estimating parameters statistical model approximating posterior distribution unobserved random variables parameters given bayesian treatment random variables training observed data learned approximation true posterior since divergence always non-negative have right hand side equation evidence lower bound \"elbo\". derivation applies statistical model dataset. experiments focus modeling conditional likelihoods using conditional independence assumption apply stochastic decompose gradient methods optimization. computing expectation generally intractable deep nets estimated monte carlo sampling. given value computed differentiated exactly non-bayesian allowing training backpropagation. entropy term also straightforward evaluate simple families approximate posteriors gaussians. similarly monte carlo estimate likelihood test data-point predictive posterior bayesian hypernets express ﬂexible using ddgn transform random noise independent samples makes cheap compute monte carlo estimations expectations respect include elbo derivatives backpropagated train means bhns trained evaluated samples makes expressing generative model natural strategy. however ddgns convenient sample from computing entropy term elbo additionally requires evaluating likelihood generated samples popular ddgns provide convenient general models many-to-one mappings computing likelihood given parameter value requires integrating latent noise variables discussed section number techniques developed efﬁciently training invertible ddgns. realnvp inverse autoregressive flows could also efﬁciently applied; require ability evaluate likelihood generated samples also means lower-dimensional generate samples along submanifold entire parameter space detailed below. output scaling factors hypernet learn maximum likelihood estimate allows overcome computational limitations naively-parametrized bhns noted louizos welling since computation scales linearly instead quadratically number primary units. using parametrization restricts family approximate posteriors still allows high degree multimodality dependence parameters. since degenerate distribution cannot compute kl||p). instead treat random variable whose distribution induces distribution simply compute kl||p). since scale ﬁxed scale intuitively meaningful easily \"translate\" commonly-used spherical prior distributions priors also employ weight normalization within hypernet found stabilizes training dramatically. initialization plays important role well; recommend initializing hypernet weights small values limit impact noise beginning training. also improve numerical stability clipping outputs softmax within perform experiments mnist cifar regression task. ﬁrst present qualitative proof concepts visualizations demonstrating bayesian hypernets fact learn multimodal dependent distributions. next measure performance bhns quantitatively. single metric well model captures uncertainty; evaluate well bhns capture parameter uncertainty perform experiments regularization active learning anomaly detection active learning anomaly detection problems make natural uncertainty estimates anomaly detection higher uncertainty indicates likely anomaly. active learning higher uncertainty indicates greater opportunity learning. parameter uncertainty also regularization beneﬁts integrating posterior creates implicit ensemble. intuitively likely hypothesis predicts posterior places total mass hypotheses predicting prefer predicting \"b\". improving estimate posterior accurately weigh evidence different hypotheses. hypernet architecture realnvp -layer relu-mlp coupling functions hidden units isotropic standard normal prior weights network. baselines comparison bayes backprop dropout non-bayesian baselines adam default hyper-parameter settings gradient clipping experiments. mini-batch size reduce computation noise-sample examples mini-batch. experimented independent noise computation slower notice beneﬁt. figure illustration non-bayesian problem blundell solid black line shows mean predictions dashed lines show error bars. crosses examples training dataset. colored lines predictions primary network sampled approximate posterior. figure histogram pearson correlation coefﬁcient p-values random scalar parameters line best samples hypernet approximate posterior. hypernet posterior includes correlations different parameters. particular many p-values pearson correlation test demonstrate behavior network d-regression problem blundell figure expected uncertainty network increases away observed data. next demonstrate distinctive ability bayesian hypernets learn multi-modal dependent distributions. figure shows bhns fact learn approximate posteriors dependence different parameters measured pearson correlation coefﬁcient. meanwhile figure shows bhns learn multimodal posteriors. experiment trained over-parametrized linear network dataset generated curves represent optimal solution regression problem posterior learns capture modes show bhns regularizer outperforming dropout traditional mean ﬁeld integrating uncertainty parameters form predictive distribution expected yield superior performance also viewed form ensembling. results presented table mnist train hidden layers hidden units each. cifar train convolutional neural hidden layers channels figure learning identity function overparametrized network parametrization results symmetries shown lines. blue dots samples drawn learned posterior network assigns signiﬁcant mass table generalization results mnist cifar bhns different numbers coupling layers comparison methods bayes-bybackprop models parameter independent gaussian equivalent using hypernet coupling layers. achieved better result outputting distribution scaling factors pooling applied second fourth layers ﬁlter size also includes fully connected layer -class softmax layer. experiments bhns perform dropout full datasets mnist cifar; furthermore increasing complexity posterior adding coupling layers improves performance especially compared models coupling layers cannot model dependencies parameters. also evaluate subset mnist examples; results presented table parameterize hidden nodes method outperforms dropout cases. turn active learning compare mnist experiments replicating architecture training procedure. brieﬂy initial dataset examples acquire examples time training epochs acquisition. re-initialize network every acquisition found \"warm-starting\" current learned parameters essential good performance bhns although it’s likely longer training better initialization schemes could perform role. overall warm-started bhns suffered beginning training outperformed methods moderate large numbers acquisitions. figure active learning bayesian hypernets outperform approaches sufﬁcient acquisitions warm-starting random acquisition function bald acquisition function warm-starting improves stability methods hurt performance approaches compared randomly re-initializing parameters also note baseline model competitive mcdropout outperforms dropout baseline used anomaly detection take hendrycks gimpel starting point perform suite mnist experiments evaluating ability networks determine whether input came training distribution hendrycks gimpel found conﬁdence expressed softmax probabilities trained single dataset actually provide good signal detection problems. demonstrate bayesian dnns outperform non-bayesian counterparts. deterministic baseline value bald acquisition function always zero acquisitions random numerical instability case implementation; surprisingly found bald values implementation computes provide better-than-random acquisition function active learning anomaly detection estimate predictive posterior score datapoints. active learning would generally like acquire points higher uncertainty. well-calibrated model points also likely challenging anomalous examples thus acquisition functions active learning literature good candidates scoring anomalies. consider acquisition functions listed possible scores aopr aoroc metrics found maximum conﬁdence softmax probabilities acquisition function used hendrycks gimpel gave best performance. mcdropout achieve signiﬁcant performance gains non-bayesian baseline mcdropout performs signiﬁcantly better task. results presented table second follow experimental setup using acquisition functions exclude class training mnist time. take excluded class training data out-of-distribution samples. result presented table experiment shows beneﬁt using scores reﬂect dispersion posterior samples bayesian dnns. introduce bayesian hypernets method variational bayesian deep learning uses invertible hypernetwork generative model parameters. bhns feature efﬁcient training sampling express complicated multimodal distributions thereby addressing issues overconﬁdence present simpler variational approximations. validate properties bhns experiments present method parametrizing bhns allows scale successfully real tasks demonstrate beneﬁts effectively modeling uncertainty achieve competitive performance. going forward explore methods parametrizing bhns approaches generating lower-dimensional posteriors using another hypernet reparametrize compress primary would allow bhns provide full posterior parameters larger primary nets. another idea hypernet output different subsets primary parameters. instance following could generate different ﬁlters convolutional architecture could provide stronger training signal might increase speed learning since parameters would error gradients multiple sources single forward pass although would come expense decreasing expressivity approximate posterior. using different sampled different ﬁlters would yield full posterior weights cost losing dependencies ﬁlters’ parameter distributions. table anomaly detection mnist unseen classes. ﬁrst column indicates missing class label training set. top-most block score; middle positive precision-recall; bottom negative precision-recall. goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages graves practical variational inference neural networks. shawe-taylor zemel bartlett pereira weinberger editors advances neural information processing systems pages curran associates inc. hernandez-lobato adams probabilistic backpropagation scalable learning bayesian neural networks. proceedings international conference machine learning pages kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska hassabis clopath kumaran hadsell overcoming catastrophic forgetting neural networks. corr abs/.. salimans kingma welling markov chain monte carlo variational inference bridging gap. proceedings international conference machine learning pages soudry hubara meir expectation backpropagation parameter-free training multilayer neural networks continuous discrete weights. advances neural information processing systems pages", "year": 2017}