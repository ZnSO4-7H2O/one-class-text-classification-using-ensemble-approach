{"title": "Adaptive Neural Networks for Efficient Inference", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (<1%) loss of top5 accuracy.", "text": "speech recognition machine translation however power dnns comes considerable cost namely computational cost applying examples. cost often called test-time cost increased rapidly many tasks ever-growing demands improved performance state-of-the-art systems. point fact resnet architecture layers realizes substantial accuracy gain top- performance googlenet large-scale imagenet dataset slower test-time. high test-time cost state-of-the-art dnns means deployed powerful computers equipped massive accelerators. result technology companies spend billions dollars year expensive power-hungry computer hardware. moreover high testtime cost prevents dnns deployed resource constrained platforms found internet things devices smart phones wearables. problem given rise concentrated research effort reduce test-time cost dnns. work topic focuses designing efﬁcient network topologies compressing pre-trained models using various techniques propose different approach leaves original intact instead changes apply figure performance versus evaluation complexity architectures imagenet challenge past several years. model evaluation times increase exponentially respect increase accuracy. present approach adaptively utilize deep neural networks order reduce evaluation time examples without loss accuracy. rather attempting redesign approximate existing networks propose schemes adaptively utilize networks. ﬁrst pose adaptive network evaluation scheme learn system adaptively choose components deep network evaluated example. allowing examples correctly classiﬁed using early layers system exit avoid computational time associated full evaluation network. extend learn network selection system adaptively selects network evaluated example. show computational time dramatically reduced exploiting fact many examples correctly classiﬁed using relatively efﬁcient networks complex computationally costly networks necessary small fraction examples. pose global objective learning adaptive early exit network selection policy solve reducing policy learning problem layerby-layer weighted binary classiﬁcation problem. empirically approaches yield dramatic reductions computational cost speedup state-of-the-art networks imagenet image recognition challenge minimal loss accuracy. deep neural networks among powerful versatile machine learning techniques achieving state-of-the-art accuracy variety important applications visual object recognition pursue concrete variants idea. first propose adaptive early-exit strategy allows easy examples bypass network’s layers. expensive neural network layer train policy determines whether current example proceed next layer diverted simple classiﬁer immediate classiﬁcation. second approach adaptive network selection method takes pre-trained dnns different cost/accuracy trade-off arranges directed acyclic graph cheapest model ﬁrst expensive last. train exit policy node graph determines whether rely current model’s predictions predict beneﬁcial next branch forward example context pose global objective learning adaptive early exit network selection policy solve reducing policy learning problem layer-by-layer weighted binary classiﬁcation problem. demonstrate merits techniques imagenet object recognition task using number popular pretrained dnns. early exit technique speeds average test-time evaluation googlenet resnet within reasonable accuracy margins. network cascade achieves speed-up compared pure resnet model top- accuracy loss speed-up change model accuracy. also show method approximate oracle policy true errors suffered instance. addition reducing average test-time cost dnns worth noting techniques compatible common design large systems mobile devices smart phone networks smart surveillance-camera networks. systems typically include large number resource-constrained edge devices connected central resource-rich cloud. main challenges involved designing systems determining whether machine-learned models devices cloud. ofﬂoading work cloud problematic network latency limited cloud ingress bandwidth cloud availability reliability issues privacy concerns. approach used design system deploying small inaccurate model exit policy device large accurate model cloud. easy examples would handled devices difﬁcult ones would forwarded cloud. approach naturally generalizes computing topology designs allow method used memory constrained settings well ofﬂoading complex models device. past work reducing evaluation time deep neural networks centered reductions precision arithmetic computational cost design efﬁcient network structure compression sparsiﬁcation networks reduce number convolutions neurons edges. approach proposed paper complimentary. approach modify network structure training applied tandem approaches reduce computational cost. early efforts compress large dnns used large teacher model generate endless stream labeled examples smaller student model wealth labeled training data generated teacher model allowed small student model mimic accuracy. reduced precision networks extensively studied reduce memory footprint networks test-time cost. similarly computationally efﬁcient network structures also proposed reduce computational cost deep networks exploiting efﬁcient operations approximate complex functions inception layers introduced googlenet network sparsiﬁcation techniques attempt identify prune away redundant parts large neural networks. common approach remove unnecessary nodes/edges network. convolutional neural networks expensive convolution layers approximated redundant computation avoided recently researchers designed spatially adaptive networks nodes layer selectively activated. others developed cascade approaches allow early exits based conﬁdence feedback. approach seen instance conditional computation seek computational gains layer-by-layer network-level early exits. however propose general framework optifigure example network selection system topology networks alexnet googlenet resnet. green blocks denote selection policy. policy evaluates alexnet receives conﬁdence feedback decides jump directly resnet send sample googlenet->resnet cascade. example early exit system topology policy chooses multiple exits available stage feedback. sample easy enough system sends exit otherwise sends sample next layer. mizes novel system risk includes computational costs well accuracy. method require within layer modiﬁcations works directed acyclic graphs allow multiple model evaluation paths. techniques adaptive dnns borrow ideas related sensor selection problem goal sensor selection adaptively choose sensor measurements features example. ﬁrst approach reducing test-time cost deep neural networks early exit strategy. ﬁrst frame global objective function reduce policy training optimizing system-wide risk layer-by-layer weighted binary classiﬁcation denote labeled example dimension data classes represented data. deﬁne distribution generating examples predicted label denote loss paper focus task classiﬁcation exposition focus indicator loss ˆy=y section. practice upper bound indicator functions logistic loss computational efﬁciency. running example consider alexnet architecture composed convolutional layers followed fully connected layers. evaluation network computing convolutional layer takes times longer computing fully connected layer consider system allows example exit network ﬁrst convolutional layers. denote label predicted network example assume computing prediction takes constant time moreover denote output convolutional layer computing convolutional layer introduce decision function determines whether example exit network label proceed next layer evaluation. input decision function output corresponding convolutional layer value either architecture depicted right-hand side fig. here tγ...γ prediction time example adaptive system label predicted adaptive system example practice time required predict label excess loss introduced adaptive system recursively deﬁned. reduce early exit policy training minimizing global risk problem. idea that input policy must identify whether future reward outweighs current-stage accuracy. ﬁrst focus problem learning decision function determines example exit fourth convolutional layer whether classiﬁed using entire network. time takes predict label example depends denote regularization term important choose optimal functional form function well natural mechanism deﬁne structure early exit system. rather limiting family function single functional form linear function speciﬁc network architecture assume family functions union multiple functional families notably including constant decision functions |x|. although constant function allow adaptive network evaluation speciﬁc location additionally introduce computational overhead including constant function guarantee technique decrease test-time cost. empirically effective policies operate classiﬁer conﬁdences classiﬁcation entropy. speciﬁcally consider family functions union three functional families aforementioned constant functions linear classiﬁer conﬁdence features generated linear classiﬁers applied linear classiﬁer conﬁdence features generated deep classiﬁers applied rather optimizing jointly three networks leverage fact optimal solution eqn. found optimizing three families functions independently. family functions policy evaluation time constant therefore solving single family functions equivalent solving unregularized learning problem. exploit solving three unregularized learning problems taking minimum three solutions. order learn sequence decision functions consider bottom-up training scheme previously proposed sensor selection scheme learn deepest early exit block ﬁrst outputs. fixing outputs trained function train early exit function immediately preceding deepest early exit function eqn. allows efﬁcient training early exit system sequentially training early exit decision functions bottom network upwards. furthermore including constant functions family functions training early exit functions potential stages system early exit architecture also naturally discovered. finally case single option exit layer-wise learning scheme equivalent jointly optimizing exits respect full system risk. shown fig. computational time grown dramatically respect classiﬁcation performance. rather attempting reduce complexity state-of-the-art networks instead leverage nonlinear growth extending early exiting strategy regime network selection. conceptually seek exploit fact many examples correctly classiﬁed relatively efﬁcient networks alexnet googlenet whereas small fraction examples correctly classiﬁed computationally expensive networks resnet incorrectly classiﬁed googlenet alexnet. fig. adaptive system composed decision functions determine network evaluated example. first applied evaluation determine classiﬁcation decision returned network network evaluated example. examples evaluated determines classiﬁcation decision returned network evaluated. goal learn functions minimize average evaluation time subject constraint average loss induced adaptive network selection. adaptive early exit case ﬁrst learn trade-off average evaluation time induced error trained according eqn. training times examples pass routed deﬁned κ=n. adaptive early exit case train last decision function train earlier function before seek trade-off between evaluation time error evaluate method imagenet classiﬁcation dataset object classes. train using million training images evaluate system using validation images. pre-trained models caffe model alexnet googlenet resnet preprocessing follow routines proposed networks verify ﬁnal network performances within small margin note common ensembles networks multiple crops achieve maximum performance. methods minimal gain accuracy increasing system cost dramatically. speedup margin increases becomes trivial policy show signiﬁcant speedups within accuracy tolerance. believe speedups useful practice focus single crop single model case. temporal measurements measure network times using built-in tool caffe library server utilizes nvidia titan pascal cudnn since focus computational cost networks ignore data loading preprocessing times. reported times actual measurements including policy overhead. policy form meta-features addition outputs convolutional layers earlier networks augment feature space entropy prediction probabilities. relax indicators equations learn linear logistic regression model features policy. experimented pooled internal representations practice inclusion entropy feature baselines full system depicted figure starts alexnet. following evaluation alexnet system determines example either return prediction route example googlenet route example resnet. examples routed googlenet system either returns prediction output googlenet routes example resnet. baselines compare uniform policy myopic policy learns single threshold based model conﬁdence. also report performance different system topologies. provide bound achievable performance show performance soft oracle. soft oracle access classiﬁcation labels sends example fastest model correctly classiﬁes example. since access labels strong made oracle softer adding constraints. first follows network topology also make decisions without observing model feedback ﬁrst getting overhead. second exit cheaper model latter models agree true label. second constraint added fact goal improve prediction performance system reduce computational time therefore prevent oracle correcting mistakes made complex networks. sweep cost trade-off parameter range achieve different budget points. note weights cost formulation even pseudo labels identical policy behavior differ. conceptually weights balance importance samples gain classiﬁcation loss future stages versus samples gain computational savings exiting early stages. results demonstrated figure full tree a->g->r cascade achieve signiﬁcant speedup using resnet maintaining accuracy within classiﬁer feedback policy dramatic impact performance. although alexnet introduces much less overhead compared googlenet a->r policy performs signiﬁcantly worse lower budget regions. full tree policy learns choose best order budget regions. furthermore policy matches soft oracle performance high budget regions. note googlenet well positioned image budget probably efﬁciency oriented architectural design inception blocks budget regions overhead policy detriment even learn send alfigure performance network selection policy imagenet full adaptive system signiﬁcantly outperforms individual network almost budget regions close performance oracle. performances reported validation imagenet dataset. figure different network selection topologies considered. arrows denote possible jumps allowed policy. denote alexnet googlenet resnet respectively. statistics proportion total time spent different networks proportion samples exit network. sampled bottom sampled system evaluation half samples alexnet instead googlenet marginal loss accuracy extra alexnet overhead brings balance point close using googlenet .ms. ratio network evaluation times signiﬁcant factor system. fortunately mentioned before many applications ratio different models high analyzed network usage runtime proportion statistics samples different budget regions. fig. demonstrates results three different budget levels. full tree policy avoids using googlenet altogether high budget regions. expected behavior since a->r policy performs well regions using googlenet decision adds much overhead. level budgets policy distributes samples evenly. note overheads close useful runtime cheaper networks region. possible output prediction following convolutional layer train single layer linear classiﬁer global average pooling layer. added global pooling minimize policy overhead earlier exits. resnet added exit output layers dimensionality exit features global average pooling order layer names. googlenet added exits concatenated outputs every inception layer. table shows early exit performance different networks. gains marginal compared network selection. shows accuracy gains evaluation time different layers. interestingly accuracy gain time linear within architecture comtable early exit performances different accuracy/budget trade-offs different networks. denotes loss full model accuracy reported numbers percentage speed-ups. pared different network architectures. explains adaptive policy works better network selection compared early exits. fig. shows distributions examples networks correctly label example. notably examples correctly classiﬁed networks error respectively. similarly examples incorrectly classiﬁed networks respect error respectively. results verify hypothesis large fraction data need costly networks. particular data change figure analysis top- top- errors different networks. majority samples easily classiﬁed alexnet minority require deeper networks. error respectively network apart alexnet unnecessary adds unnecessary computational time. additionally worth noting balance examples incorrectly classiﬁed networks respectively error fraction examples correctly classiﬁed either googlenet resnet alexnet error respectively. behavior supports observation entropy classiﬁcation decisions important feature making policy decisions examples likely incorrectly classiﬁed alexnet likely classiﬁed correctly later network. note system trained using data used train networks. generally resulting evaluation error network training data signiﬁcantly lower error arises test data therefore system biased towards sending examples complex networks generally show negligible training error. practically problem alleviated validation data train adaptive systems. order maintain reported performance network without expansion training instead utilize data training networks adaptive systems however note performance adaptive systems generally better trained data excluded network training. proposed different schemes adaptively trade model accuracy model evaluation time deep neural networks. demonstrated signiﬁcant gains computational time possible novel policy negligible loss accuracy imagenet image recognition dataset. posed global objective learning adaptive early exit network selection policy solved reducing policy learning problem layer-bylayer weighted binary classiﬁcation problem. believe adaptivity important growing data models high variance computational time quality. also showed method approximates oracle based policy beneﬁt access true error instance networks. material based upon work supported part grants grant u.s. department homeland security science technology directorate ofﬁce university programs grant award -st--ed contract n--c-. views conclusions contained document authors interpreted necessarily representing social policies either expressed implied u.s. bucila cristian caruana rich niculescu-mizil alexandru. model compression. proceedings sigkdd international conference knowledge discovery data mining courbariaux matthieu bengio yoshua david jeanpierre. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems figurnov michael collins maxwell yukun zhang huang jonathan vetrov dmitry spatially adaptive compusalakhutdinov ruslan. arxiv preprint tation time residual networks. arxiv. figurnov mikhail ibraimova aizhan vetrov dmitry kohli pushmeet. perforatedcnns acceleration elimination redundant convolutions. advances neural information processing systems kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition hinton geoffrey deng dong dahl george mohamed abdel-rahman jaitly navdeep senior andrew vanhoucke vincent nguyen patrick sainath tara deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine hubara itay courbariaux matthieu soudry daniel elyaniv bengio yoshua. binarized neural netadvances neural information processing works. systems hubara itay courbariaux matthieu soudry daniel elyaniv bengio yoshua. quantized neural networks training neural networks arxiv preprint precision weights activations. arxiv. iandola forrest song moskewicz matthew ashraf khalid dally william keutzer kurt. squeezenet alexnet-level accuracy fewer paarxiv preprint rameters and< model size. arxiv. krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems kusner chen zhou weinberger chen feature-cost sensitive learning submodular trees classiﬁers. twenty-eighth aaai conference artiﬁcial intelligence leroux bohez steven coninck elias verbelen vankeirsbilck bert simoens pieter dhoedt bart. cascading neural network building internet smart things. knowledge information systems baoyuan wang foroosh hassan tappen marshall pensky marianna. sparse convolutional neural networks. proceedings ieee conference computer vision pattern recognition jiaxiang leng cong wang yuhang qinghao cheng jian. quantized convolutional neural networks mobile devices. proceedings ieee conference computer vision pattern recognition feng wang joseph saligrama venkatesh. prunading random forests prediction budget. vances neural information processing systems annual conference neural information processing systems december barcelona spain rastegari mohammad ordonez vicente redmon joseph farhadi ali. xnor-net imagenet classiﬁcation using binary convolutional neural networks. european conference computer vision springer russakovsky olga deng krause jonathan satheesh sanjeev sean huang zhiheng karpathy andrej khosla aditya bernstein michael imagenet large scale visual recognition challenge. international journal computer vision szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. proceedings going deeper convolutions. ieee conference computer vision pattern recognition saligrama venkatesh. efﬁcient learning directed acyclic graph advances resource constrained prediction. neural information processing systems", "year": 2017}