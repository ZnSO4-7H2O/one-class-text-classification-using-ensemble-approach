{"title": "Look-ahead before you leap: end-to-end active recognition by forecasting  the effect of motion", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "abstract": "Visual recognition systems mounted on autonomous moving agents face the challenge of unconstrained data, but simultaneously have the opportunity to improve their performance by moving to acquire new views of test data. In this work, we first show how a recurrent neural network-based system may be trained to perform end-to-end learning of motion policies suited for this \"active recognition\" setting. Further, we hypothesize that active vision requires an agent to have the capacity to reason about the effects of its motions on its view of the world. To verify this hypothesis, we attempt to induce this capacity in our active recognition pipeline, by simultaneously learning to forecast the effects of the agent's motions on its internal representation of the environment conditional on all past views. Results across two challenging datasets confirm both that our end-to-end system successfully learns meaningful policies for active category recognition, and that \"learning to look ahead\" further boosts recognition performance.", "text": "abstract. visual recognition systems mounted autonomous moving agents face challenge unconstrained data simultaneously opportunity improve performance moving acquire views test data. work ﬁrst show recurrent neural network-based system trained perform end-to-end learning motion policies suited active recognition setting. further hypothesize active vision requires agent capacity reason eﬀects motions view world. verify hypothesis attempt induce capacity active recognition pipeline simultaneously learning forecast eﬀects agent’s motions internal representation environment conditional past views. results across challenging datasets conﬁrm end-to-end system successfully learns meaningful policies active category recognition learning look ahead boosts recognition performance. people consistently direct senses order better understand surroundings. example might swivel around armchair observe person behind rotate coﬀee desk read inscription walk window observe rain outside. sharp contrast scenarios recent recognition research focused almost exclusively static image recognition system takes single snapshot input produces category label estimate output. ease collecting large labeled datasets images enabled major advances task recent years evident example striking gains made imagenet challenge despite recent progress recognition performance remains complex unconstrained images recognition systems mounted autonomous moving agents acquire unconstrained visual input diﬃcult recognize eﬀectively frame time. however similar human actor opening examples above systems opportunity improve performance moving camera apparatus manipulating objects acquire information shown control system sensory input tremendous potential improve recognition performance. mobile agent settings fig. schematic illustrating active categorization objects. moving vision system recognize objects view intelligently choose acquire views disambiguate amongst competing hypotheses. closer reality today ever before problem learning actively move direct acquisition data remains underexplored modern visual recognition research. problem describing realm active vision rich history literature active vision oﬀers several technical challenges unaddressed today’s standard passive scenario. order perform active vision system must learn intelligently direct acquisition input processed recognition pipeline. addition recognition active setting places diﬀerent demands system standard passive scenario. take example nuisance factors still image recognition—such pose lighting viewpoint changes—become avoidable factors active vision setting since principle often overcome merely moving agent right location. calls major change approach. rather strive invariance nuisance factors standard static image recognition intriguing strategy learn identify conditions non-ideal recognition actively select correct agent motion lead better conditions. addition recognition decisions must made based intelligently fusing evidence multiple observations. contend three functions active vision system—control per-view recognition evidence fusion—are closely intertwined must tailored work together. particular ﬁrst contribution paper propose learn three modules active vision system simultaneously end-to-end. employ stochastic neural network learn intelligent motion policies standard neural network process inputs timestep modern recurrent neural network integrate evidence time given initial view possible agent motions approach learns move environment produce accurate categorization results. additionally hypothesize motion planning active vision requires agent internally look leaps. ought simultaneously reason eﬀect motions future inputs. demonstrate this second contribution jointly train active vision system ability predict internal representation environment evolve conditioned choice motion. explain below seen preferring equivariance i.e. predictable feature responses pose changes rather invariance standard passive recognition pipelines. experiments datasets validate ideas rnn-based end-to-end active categorization learning forecast eﬀects self-motion time learns move solve recognition task. study scene categorization scenario system chooses move around previously unseen scene object categorization scenario system chooses manipulate previously unseen object holds. results establish advantage end-to-end approach passive traditional active methods. active vision idea subject’s actions play important role perception traced back almost years cognitive psychology literature active perception idea exploiting intelligent control strategies goaldirected data acquisition improve machine vision pioneered research area targeted low-level vision problems segmentation structure motion depth estimation optical estimation semantic search task object localization approaches targeting active recognition directly related work. prior active recognition approaches attempt identify training canonical/special views minimize ambiguity among candidate labels test time systems iteratively estimate current pose select moves take pre-identiﬁed informative viewpoints. approaches typically applicable instance recognition problems since broader categories diverse appearance shape special viewpoints. contrast approach handles real world categories. best knowledge little prior work attempts challenging task active category recognition increased diﬃculty fact complex real world categories much harder anticipate views conditioned actions. since instances seen test time suﬃcient simply memorize geometry individual instances many active instance recognition methods eﬀectively recently learn predict next views unseen test objects explicitly greedily reason informative next-best move. instead approach uses reinforcement learning stochastic recurrent neural network learn optimal sequential movement policies multiple timesteps. closest methods respect employ q-learning feedforward neural networks perform view selection target relatively simpler visual tasks compared work. addition above important novelty approach learning entire system end-to-end. active recognition approaches must broadly perform three separate functions action selection per-instant view processing belief updates based history observed views. previous approaches explored several choices action selection typically train passive per-instant view recognition module oﬄine fuse predictions across time using manually deﬁned heuristic example recently deep neural network trained learn action policies pretraining per-view classiﬁer using simple naive bayes update heuristic label belief fusion. contrast train three modules jointly within single active recognition objective. saliency attention visual saliency attention related active vision active vision systems form policies acquire data saliency attention systems block distractors existing data identifying portions input images/video focus often faster alternative sliding window-based methods. attention systems thus sometimes take foveated approach contrast setting system never holds snapshot entire environment once. rather input timestep portion complete physical environment must choose motions leading informative—possibly non-overlapping— viewpoints. another diﬀerence settings focus attention move arbitrary jumps without continuity whereas active vision agents move continuously. sequential attention systems using recurrent neural networks particular seen signiﬁcant interest late variants proving successful across several attention-based tasks adopt basic attention architecture starting point model develop accommodate active vision setting instill look-ahead capabilities select camera motions surrounding object facilitate categorization. predicting related features recent interest visual prediction problems various contexts often using convolutional neural networks example train cnns recurrent neural networks predict future frames based previously observed frames entirely passive setting. methods attempt reason causes view transformations e.g. camera motions. closer work methods view synthesis allow synthesis simple synthetic images speciﬁed factors variation given surrounding views high quality unseen views predicted eﬀectively learning geometry end-to-end. methods model feature responses discrete observer motions. diﬀerent above learn predict evolution temporally aggregated features— computed complete history seen views—as function observer motion choices. furthermore integrate idea closely tied active recognition problem. integrating sensors actions work also related research sensorimotor feature embeddings idea combine sensor streams together proprioception knowledge actions agent sensors mounted. various methods learn features transform simple ways response agent’s actions reﬂect geometry agent’s environment neural nets trained perform simple robotic tasks perhaps conceptually relevant work among method learns image feature space determine control actions easily visual inputs applications simulated control tasks. contrast learn embeddings encoding information complete histories observations agent actions exposing information active visual recognition controller. ﬁrst describe active vision setting test time using object category recognition scenario running example. results consider object scene category recognition tasks. active recognition system issue motor commands move camera within viewing sphere around object interest. point viewing sphere indexed corresponding camera pose vector indexing elevation azimuth. system issues motor command e.g. increase camera elevation azimuth available camera motions. experiments discrete consisting small camera motions points elevation-azimuth grid centered previous camera pose pt−. time previous camera pose random unknown vector corresponding agent initializing recognition episode arbitrary position respect object. ﬁnal timestep system must additionally predict category label e.g. object category believes probable. implementation number timesteps ﬁxed valid motor commands uniform cost. system evaluated accuracy prediction however framework generalizes case variable-length episodes. fig. schematic system architecture depicting interaction actor sensor aggregator classifier modules unrolled timesteps. information ﬂows left right. training time additional lookahead acts across timesteps learning predict evolution aggregate feature conditional selected motion details. basic active recognition system modeled recurrent architecture ﬁrst proposed visual attention. system composed four basic modules actor sensor aggregator classifier weights respectively. step actor issues motor command updates camera pose vector next image captured pose sensor together motor command sensor produces view-speciﬁc feature vector sensor aggregator produce aggregate feature vector aggregator. cycle completed when next step actor processes aggregate feature previous timestep issue actor. finally steps category label beliefs predicted classifier vector learnable weights network c-class classiﬁcation problem c-dimensional multinomial probability density function representing likelihoods object belonging classes. schematic showing modules connected. setup aggregator recurrent neural network classifier simple fully-connected hidden layer followed log-softmax sensor separately processes view motor signal disjoint neural network pipelines merging layers processing produce per-instance view feature sensor. actor nonstandard neural architecture involving stochastic units timestep internally produces |m|-dimensional multinomial density function candidate camera motions samples motion. details internal architectures modules supp. training training time network weights trained jointly maximize classiﬁer accuracy time following training follows hybrid procedure involving standard backpropagation connectionist reinforcement learning modules standard deterministic neural network connections trained directly backpropagating gradients softmax classiﬁcation loss actor module contains stochastic units trained using reinforce procedure roughly reinforce treats actor module partially observable markov decision process representing policy learned. reinforcement learning -style approach reinforce iteratively increases weights candidate motions produced higher rewards deﬁned reward function. simple reinforce reward function promote classiﬁcation accuracy could likely label correct not. speed training variance-reduced version loss commonly occuring class. beyond stochastic units reinforce algorithm produces gradients propagated non-stochastic units standard backpropagation. hybrid training approach reinforce gradients actor therefore added softmax loss gradients classifier backpropagation aggregator sensor. formally given training dataset instance-label pairs gradient updates follows. denote i.e. weights except classifier weights similarly denote then indices superscripts denote correspondence training sample show gradients computed reinforce rewards softmax loss respectively diﬀerent subsets weights. reinforce gradients computed using approximation proposed final gradients respect weights module used weight updates given training standard stochastic gradient descent early stopping based validation set. active recognition systems select next motion based expectation next view. though non-trivial even traditional instance recognition setting instances exploit fact pose estimation canonical pose space suﬃcient estimate properties future contrast discussed sec. problem much harder active categorization realistic categories—the domain target. predicting subsequent views setting severely under-constrained requires reasoning semantics geometry together. words next view planning requires element learning objects general change appearance function observer motion. hypothesize ability predict next view conditional next camera motion closely tied ability select optimal motions. thus rather learn separately model view transitions model motion policies propose uniﬁed approach learn jointly. idea knowledge transfer view prediction task beneﬁt active categorization. formulation retain system sec. simultaneously learn predict every timestep impact aggregate features next timestep given choice motion words simultaneously learn accumulated history learned features—not current view—will evolve function candidate motions. auxiliary task introduce additional module lookahead weights setup sec. training time. timestep lookahead takes input predicts lookahead. module thought predictive auto-encoder space aggregate features output aggregator. look-ahead error loss computed every timestep predicted actual aggregate features cosine distance compute error. pertimestep look-ahead loss provides third source training gradients network weights backpropagated aggregator sensor includes denotes lookahead. lookahead module trained solely error ﬁnal gradients used train sensor aggregator change include loss hyperparameter controls much weights core network inﬂuenced look-ahead error loss. look-ahead error loss also interpreted unsupervised regularizer classiﬁcation objective regularizer encodes hypothesis good features active recognition task must respond learnable systematic ways camera motions. related role equivariant image features showed regularizing image features respond predictably observer egomotions improves performance standard static image categorization tasks. work diﬀers several important ways. first explore utility look-ahead active categorization problem recognition individual static images. second proposed look-ahead module conceptually distinct. particular propose regularize aggregate features sequence activity simply per-view features. whereas eﬀect discrete egomotion image estimated linear transformations embedding space proposed look-ahead module takes input history views selected motion estimating eﬀects hypothetical motions. proprioceptive knowledge another useful feature approach allows easy modeling proprioceptive knowledge current position robotic arm. since actor module trained purely reinforce rewards modules access output without backpropagate extra gradients softmax loss. instance sensor module input directly backpropagate gradients train actor. since function solely knowledge readily available components system without changes training procedure described above. append appropriate proprioceptive information inputs actor lookahead detailed experiments. greedy softmax classiﬁcation loss found beneﬁcial training time inject softmax classiﬁcation gradients every timestep rather timesteps. achieve this classifier module modiﬁed contain bank classiﬁcation networks identical architectures note reinforce loss still computed thus given softmax gradients pass actor module remains free learn non-greedy motion policies. evaluate approach object scene categorization. cases system must choose move environment full sequence actions lead accurate categorization results. active vision systems traditionally tested custom robotic setups test system realistic oﬀ-the-shelf datasets interest benchmarking reproducibility. work publicly available datasets germs experiments test scenario agent exploring scene must intelligently turn parts scene enable accurate scene categorization consists spherical panoramas various indoor outdoor scenes together scene category labels. -category subset used panorama represents scene instance around fig. airplane interior class example showing spherical panoramas converted ◦fov view grid. illustration view grid coordinates outlined green view grid right corresponds approximately overlap region left shaded region view grid shows motions available actor starting highlighted view. agent moves rotating head shown experiments agent limited ﬁeld view timestep. sample discrete views elevations azimuths grid. pitch steps spaced ◦apart entire viewing sphere uniformly sampled axis. starting full panorama size view represented ﬁrst image -dim. googlenet features extracted penultimate layer. timestep agent choose move viewpoints grid centered current position. timesteps. proprioceptive knowledge form current camera elevation angle actor lookahead. random train-test split. simulate active agent scene oﬀers realistic scenario benchmark rigorously; note previous work dataset diﬀerent task i.e. recognition full panorama hand results therefore comparable setting. germs experiments consider scenario robot holding object must decide next best motion relative object e.g. gain access unseen facet object recognize instance label. germs videos objects rotated around diﬀerent ﬁxed axes television screen displaying moving indoor scenes video frame annotated angle robotic holding object. video provides collection views active vision system traverse will total train/test instances germs small targets instance rather category recognition aside suitable prior dataset facilitating active recognition. frame represented -dim. vgg-net feature vector provided authors episode lengths steps. proprioceptive knowledge feed current position robotic hand actor lookahead. train-test subsets speciﬁed dataset authors. baselines extensively evaluate look-ahead active simpler active baselines including passive singleview methods random view sampling traditional prior active vision approaches upgraded competitive setting. single view access view like starting view provided active systems. feed-forward neural network used baseline composed appropriate components sensor classifier modules system. baseline entirely poseagnostic i.e. classiﬁer applied views object poses. random views uses architecture single view access views successive views related randomly selected motions motion available active systems. output class likelihood average independent estimates class likelihood view. random views uses core architecture active method except actor module. place random motions selected. note strong baseline nearly aspects proposed approach except active view selection module. particular access selected motions sensor module also learn intelligently aggregate evidence views aggregator module. transinformation closely based views selected greedily reduce information-theoretic uncertainty category hypothesis. make modiﬁcations setting using features place original receptive ﬁeld histogram features using monte carlo sampling approximate information gain. view classiﬁed pose-speciﬁc classiﬁers. class hypothesis identical consecutive views emitted output view selection terminates. like prior approaches method relies canonical world coordinate space object instances registered. since infeasible active categorization setting treat instance’s coordinates world coordinates. transinformation seqdp combines strengths uses bayesian information aggregation across views terminates early predicted class remains unchanged consecutive timesteps. table shows recognition accuracy results scene categorization object instance recognition figure plots results function timesteps. variants method outperform baselines datasets conﬁrming active approach successfully learns intelligent view selection strategies. passive baselines representative current standard approaches visual categorization perform uniformly poorly highlighting advantages active setting. addition look-ahead active outperforms active variant datasets showing value simultaneously learning predict action-conditional next views time learn active vision policy. looking leaping look-ahead module facilitates beneﬁcial knowledge transfer active vision task. even though represents much harder active category recognition problem margins method random view baselines pronounced. furthermore traditional active baselines show signiﬁcant improvements observing multiple views fall short performance method despite upgrading order competitive using features described above. fig. evolution accuracy time various active recognition methods germs methods show steady improvement additional views easily outperform best baselines. also relatively small dataset. number active recognition instances small compared diﬀerent views object instance naturally closer diﬀerent views panorama view-grid even single view diversity single degree motion compared sun. result number possible reinforcement learning episodes also much smaller. upon inspection found factors lead end-to-end network overﬁt training data particular problematic method achieves zero training error single views network incentive learn aggregate information across views well. active results line presented benchmark paper introducing dataset expect training data necessary move end-to-end learning challenge. lack data aﬀects prior active method baselines even since rely pose-speciﬁc instance classiﬁers classiﬁer’s training small. explains poor performance. interesting upshot improvements germs averaging classifier modules’ outputs i.e. class likelihoods estimated aggregated features timestep since factors make diﬃcult learn optimal aggregator end-to-end system like ours second tier aggregation form averaging outputs system yield improvements. contrast since oﬀers much training data averaging per-timestep classifier outputs signiﬁcantly reduces performance system compared directly using last timestep output. exactly would hope successful end-to-end training. reasoning supported fact random views shows slightly poorer performance random views germs much better sun. indeed signiﬁcant gains random views random views points important advantage treating object/scene categorization grounded sequence-based decision process. ability intelligently fuse observations timesteps based views camera motions relating oﬀers substantial rewards. contrast current computer vision literature visual categorization fig. views selected using approach sun. corresponding scene contains panels corresponding selected views panel shows current view position view grid given ﬁrst view method makes reasonable wrong guesses corrects within moves observing crowd following gaze. largely focused categorization strategies process individual images outside context agent motion sequential data much like single view random views baselines. empirical results exciting prompt future work space. also suggest need increased eﬀorts creating large video benchmark datasets support vision research allowing systematically study scenarios outside robot platforms. result particular signiﬁcant since prior active recognition approach shown successfully handle comparably complex dataset. active categorization technically challenging compared instance recognition discussed datasets like containing complex visual data ambiguous views actually suited showing advantages active recognition paradigm. presented end-to-end approach active visual categorization. framework simultaneously learns system move improve sequence observations sequence future observations likely change conditioned possible actions. show impact object scene recognition active approach makes sizeable strides single view passively moving systems. furthermore establish positive impact treating components active recognition system simultaneously. together results encouraging evidence modern visual recognition algorithms venture unconstrained sequential data moving beyond static image snapshot labeling paradigm. acknowledgments research supported part pecase n--. also thank texas advanced computing center generous support mohsen malmir jianxiong xiao assistance sharing germs data respectively.", "year": 2016}