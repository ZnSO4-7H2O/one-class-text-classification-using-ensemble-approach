{"title": "On the Importance of Consistency in Training Deep Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "We explain that the difficulties of training deep neural networks come from a syndrome of three consistency issues. This paper describes our efforts in their analysis and treatment. The first issue is the training speed inconsistency in different layers. We propose to address it with an intuitive, simple-to-implement, low footprint second-order method. The second issue is the scale inconsistency between the layer inputs and the layer residuals. We explain how second-order information provides favorable convenience in removing this roadblock. The third and most challenging issue is the inconsistency in residual propagation. Based on the fundamental theorem of linear algebra, we provide a mathematical characterization of the famous vanishing gradient problem. Thus, an important design principle for future optimization and neural network design is derived. We conclude this paper with the construction of a novel contractive neural network.", "text": "explain diﬃculties training deep neural networks come syndrome three consistency issues. paper describes eﬀorts analysis treatment. ﬁrst issue training speed inconsistency diﬀerent layers. propose address intuitive simple-to-implement footprint second-order method. second issue scale inconsistency layer inputs layer residuals. explain second-order information provides favorable convenience removing roadblock. third challenging issue inconsistency residual propagation. based fundamental theorem linear algebra provide mathematical characterization famous vanishing gradient problem. thus important design principle future optimization neural network design derived. conclude paper construction novel contractive neural network. last decade rapid developments ﬁeld deep neural networks deep learning laid foundation large amount major advancements artiﬁcial intelligence ranging speech recognition image recognition natural language processing motor skill learning various theoretical perspectives proposed explain deep learning successful general consensus community attribute success joint forces straightforward neural modeling simple learning techniques availability data hardware revolution high performance computing. date training large scale deep neural networks still largely unintuitive computationally intensive time-consuming process. generations modern hardware well software architectural innovations addressed fundamental challenges. community strives come simple intuitive explanation mysterious networks facilitate design intelligent networks training schema. among various operations training deep neural networks performing gradient descent iterations costly time-consuming. current training algorithms adopt ﬁrst-order methods i.e. modiﬁcations steepest descent method conduct gradient decent process. though mathematically practically straightforward convergence rate ﬁrst-order methods inherently sub-optimal large scale non-linear regression problems training deep neural networks. natural extension ﬁrst-order methods second-order methods commonly known newton’s method intuitively speaking unlike near-sighted ﬁrst-order methods second-order optimization algorithms take account local geometry underlying problem. provide global point view exact solutions quadratic approximation energy surface. however implementations second-order methods large scale neural networks signiﬁcantly complex require lots memory usually lead better performances terms training speed ﬁnal model quality also iteration second-order methods computationally expensive ﬁrst-order ones. context large scale regression millions billions parameters utilizing second-order information hard. direct computation required second-order derivatives hessian matrix inverse computationally intractable using approximations hessian still costly compared simple steepest descent method. order keep discussion simple refer curious readers classic textbooks detailed discussion implementation methods. diﬃculties works deep neural networks trained ﬁrst-order methods. nevertheless second-order methods several favorable properties superlinear convergence rate ﬁrst-order methods linear convergence rate. importantly better invariance properties optimal step sizes determined curvature information result usually close work analyze diﬃculties training deep neural networks. opinion training networks found technically diﬃcult decades combination three challenges. three explained inconsistency diﬀerent layers networks. paper discuss challenges propose second-order training techniques tackle problems. ﬁrst part paper forward novel method showing second-order information introduced intuitive used eﬀectively train neural networks. demonstrate above-mentioned properties second-order methods utilized shorten neural networks’ training time lead better solutions. helps training speed inconsistency issue. proposed method implemented straightforward adding line computer code well known stochastic gradient descent method. second part pick second-order point view initiate mathematical analysis critical challenges training deep neural networks. challenges remained vague current literature. explain propose ways alleviate scale inconsistency problem help second-order methods. third part present mathematical characterization vanishing gradient problem language linear algebra speciﬁcally using fundamental theorem linear algebra. three analyses provide novel insights long lasting mysteries neural networks. solving challenges likely lead advancements neural network theory applications. conclude paper demonstration. sake clarity focus intuition motivating methods. basic mathematical analysis presented concise self-contained fashion; thorough analysis found classic textbooks training deep layered network structure non-linear regression process minimizes ﬁnal regression loss. usually amounts minimizing gradients network output also known regression residuals following chain rule gradients intermediate layers calculated order deep-to-shallow altogether represent steepest ascent direction. minimization achieved moving parameter along descent direction. layered structure neural network certain layers diﬃcult train others layers impede training whole network. unfortunately usually case ﬁrst-order gradient descent methods guarantee universal learning rate layers. therefore speed training slowest layer becomes bottleneck whole training process. phenomenon reﬂected well known minimum motivates design approach trains individual layer consistent full speed. ﬁnal output network denote parameters i-th layer input output wixi. derivations switch capital letters emphasize computation takes batched data therefore forms matrix. neural networks usually cost loss value computed neural network output natural extension model residual residual layer deﬁned gradient work adopts local linear model given displacement residual approximated thus optimization problem layer formulated intuitively speaking measures change output layer change weights direction optimization problem seeks compensation displacement minimizes layer residual solution satisﬁes so-called normal equations well-conditioned positive deﬁnite descent direction minimizes regression loss. dealing ill-conditioned regularizations used stabilize problem. trust region method simple modiﬁcation seeks solution following equation instead point closely related ﬁrst-order method second-order method. note right hand side negative gradient parameters layer view multiplication λi)− inverse correction parameter gradients adopts matrix using inverse matrix putting things together achieve solution form second-order method sec. paper forward method simple approximation demonstrate eﬃcacy. also view process decorrelation add-on standard stochastic gradient descent algorithm by-itself decorrelate data. attribute optimization solution layer-wise application wellestablished levenberg-marquardt algorithm noteworthy previous attempts designing second-order algorithms make explicit layer residuals. result formulations hessian matrix middle layer depends hessian matrices deeper layers. corresponding algorithms complicated mathematically computationally. layer-wise exposition layer optimization problem simple intuitive given input residual layer adjust parameters layer minimize done gradient descent using iterations. loss assumed done explicitly proposed. simple optimization easily tractable distinguishes method previous second-order methods. procedures optimization optimized numerical linear algebra routines eﬃcient practical problems dimension scale hundreds. therefore techniques used directly medium-sized multilayer perceptron networks. convolutional neural networks applying aforementioned derivation leads non-trivial inverse ﬁltering problem. discuss sec. instead propose simpliﬁed version ignoring pixel-wise correlation focus correlation among feature maps. partial decorrelation seen application preconditioning technique treat feature hidden unit calculate correlation across diﬀerent feature maps. inverse correction applied convolution kernel gradients shown major computational bottlenecks method matrix multiplication convolutional networks usually highly redundant wide matrix. experiments signiﬁcantly accelerate matrix multiplications noticeable loss accuracy performing uniform subsampling. computational cost negligible compared whole training process. matrix inversions performed reasonably fast small scale hundreds entries. larger layers contain thousands hidden units matrix multiplications inversions extremely slow. thus propose speed decorrelation process using simple stochastic preconditioning strategy. training iteration hidden units stochastically divided chunks matrix multiplications inversions calculated quickly. empirically observe stochastic division preconditioning technique eﬀective conducting acceleration. scale consistency training deep neural networks notice even removal computational bottleneck direct application algorithm -layer neural networks leads training failures. analysis individual layer inputs layer residuals leads following observation ﬁrst layer network inputs range residuals range well-posed. deeper layer inputs range residuals range latter optimization becomes much harder aﬀecting method including gradient descent optimization. observation reveals second roadblock training neural networks scale inconsistency. note neural network ordered transforms diﬀerent spaces. transform constraint guaranteeing input signal layer meaningful scale residual scale inconsistency makes simple optimization attempts futile. approach rectify issue normalization techniques inputs layer approximately scale. widely-adopted batch normalization algorithm designed speciﬁcally ﬁrst-order methods. note globally-optimal learning rate almost impossible ﬁrst-order methods deep layered structure. batch normalization algorithm introduces extra pair parameters scales shifts normalized results alleviate problem. hand second-order methods pair parameters redundant scaling shifting ﬁgured automatically normal equations speed consistently optimal layer long inputs approximately scale. also observe rescaling batch normalization potentially leads model instability scaling factor larger reasons adopt minimal normalization strategy divide inputs smoothed root mean square estimated minibatches smoothed using moving average. hand normalization also necessary second-order methods regularization takes consistent eﬀects layer. appropriate inputs range likely ineﬀective inputs latter case displacement eqs. likely explode. according extensive tests notice speciﬁc designs previous diﬃculties applying second-order methods deep learning largely reduced. demonstrated later sections simple design based normalization allows using second-order algorithms train deep neural networks leads construction important class neural networks. clariﬁcation vanishing gradient operator analysis normalizing inputs well-deﬁned ranges observe usually becomes possible second-order information training networks layers. however encounter failures noticeabe worse performance network goes beyond depth. observation motivates conduct mathematical analysis operator theory formally speaking operator ‘the best linear approximate’ several layers forward transform. terms diﬀerential geometry diﬀerential forward transform ‘well-known’ natural state usually expansive non-contractive meaning singular value larger property intuitively understood certain parts network example normalization step relu. using notation operator theory operator maps hilbert space show shortly hilbert adjoint used back propagation process maps back according operator theory characteristics therefore also expansive however investigation gradient propagation contrary observation signiﬁcant energy decay often observed move shallower layers. empirically contractive eﬀect known vanishing gradient problem training deep neural networks. problem haunted neural network community decades even though operator therefore non-contractive. mystery vanishing gradient problem unveiled investigating adjoint operator forward transform real space last equality simply follows describes critical constraint enforced training deep neural networks. unsurprisingly current techniques enabling training deep networks exploit constraint. example relu instead activation functions; introduction identity maps procedure adding random noise gradients help lessen vanishing gradient problem. experiments tested procedure injecting noise showed promising results alleviating vanishing gradient problem. though always successful potentially expansive nature transform. intuitive explanation noise injection works follows optimization process stochastically perturbs gradient order avoid gradients fall kernel space becomes here small random noise valuable ingredient adds factor random exploration training. perturbed gradient direction close better solution approach makes second-order information follow better solution. furthermore analysis suggests relu operator best option gradient propagation. here propose investigate modulus operator solid mathematical foundation ﬁeld wavelets analysis speciﬁcally order maintain large image space equivalently shrink kernel space adopt modulus unit activation function instead relu. real number space modulus unit computes absolute value input well known current neural networks susceptible ‘adversarial attacks’ open problem existing powerful networks modiﬁed become stable. stability important applications neural networks object classiﬁcation biometric veriﬁcation exposed clever hacks. experiments conducted sec. validate choice activation function promising upon diﬃcult problem. focus experiments section demonstrations applications favorable properties second-order methods. second-order methods relatively uncommon deep learning community. currently competitive implementations existing second-order methods available test platform compare proposed second-order modiﬁcation state-of-the-art ﬁrst order stochastic gradient descent algorithms method utilize second-order information name sgd. algorithms paper implemented using matlab source code available lightnet lightweight deep learning package. small network mnist dataset construct small multilayer perceptron network hidden layers. hidden layers hidden nodes network fully-connected. relu functions used activation functions. network weights initialized using gaussian random noise standard deviation batch size optimal learning rate selected using grid search iterations experiment. settings table lowest test errors using initializations diﬀerent scales. initialization distributions normal distributions standard deviations experiment conducted mnist dataset using network section hold ﬁxed following experiments otherwise mentioned. comparison apply decorrelation gradients hidden layers regularization weight ﬁxed value comparison state-of-the-art training algorithm demonstrates signiﬁcant acceleration achieved using second-order information upon ﬁrst-order counterpart convolutional network cifar- dataset convolutional network convolution layers fully-connected layer constructed cifar- dataset network structure borrowed convolution kernels size ﬁrst three layers last layer kernel size test training epochs automatically select best learning rate training process. save computational cost matrix multiplications selectively apply uniform subsamplings keep data samples feature map. modiﬁcation noticeable overhead calculating lowest test error rates best initial learning rates listed table notice complex dataset shows distinct speed ﬁrst epoch error rate reduced closest matching ﬁrst-order method adam error rate. scrutinizing training error curves also tells accelerated ﬁrst-order experience certain levels overﬁtting near training epoch results increased error rates.for performers provide better accuracy tried shorten training epochs. notice fewer epochs required ﬁnish training sgd. ﬁnal error rate also better second-order information training -layer deep network straightforwardly extend multilayer perceptron network layers. unfortunately training algorithm able make progress training. normalize inputs hidden layer dividing smoothed root mean square using minibatch data momentum using simple normalization scheme inputs hidden layer scale. back propagation step gradients scaled normalization factor. networks trained epochs figure training curves various datasets networks. training loss hidden-layer network ﬁrst epoch mnist dataset. test errors convolutional-layer network cifar- dataset. test errors hidden-layer network mnist dataset. increases error rates caused overﬁtting. test errors hidden-layer network mnist dataset. network weights initialized trained using second-order information. results shown mnist data fig. right columns table. deep networks signiﬁcantly slower harder train compared shallow networks. non-trivial ﬁrst-order methods select optimal learning rate layer even though input data properly scaled layer size same. however second-order modiﬁcation training signiﬁcantly faster also ﬁnal error rate signiﬁcantly lower. observation unexpected parameter solving process fundamentally inverse problem. optimal learning rate related condition number diﬀerent data covariance matrices layer used second-order method. using suboptimal learning rate done ﬁrst-order methods slows training result poor local solutions. optimal learning rate approximately uniform thanks invariance properties second-order methods. reach experiment major conclusion second-order methods adopted elegantly train deep neural networks. tolerance initializations next experiment initialize network weights using gaussian distribution standard deviations even though initializations spans wide scale range direct -epoch trainings yield comparable ﬁnal accuracies settings table large initializations slightly increase weight decay regularizations reduce overﬁtting accelerate convergence. initializations using standard deviations lead error rates worst test error rate occurs network badly initialized experiment also tells better convergence properties second-order methods inherited improve training deep neural networks. widely adopted strategy neural networks need initialized small random noise likely originates restriction ﬁrst-order training algorithms. deeper networks since second-order methods allow make consistent progress diﬀerent layers network move focus second-order method analyze remaining challenges training deeper networks. explore highly challenging task training deep narrow networks. following analysis hidden layer narrow neural network limited hidden nodes. order create larger image space equivalently smaller kernel space change activation function popular rectiﬁed linear units relu modulus units odu. settings able train -layer deep narrow neural network odu. relu counterparts lead exploding losses better gradient propagation property also observed fig. however notice shallower networks function leads slightly worse accuracy believe main reason popularity. contractive deep networks nevertheless disappointed somewhat mediocre results using odu. point another valuable property non-expansive zero inputs. experiments normalization factors mostly around weak regularization applied. contrast relu used activation function frequently observe small normalization factors deeper layers leading expansive eﬀects neural network. small fabricated noise potentially leads catastrophic eﬀects figure applications relu activation functions training deep neural networks. test losses using relu activation functions various depth narrow networks. layer hidden nodes. test error rates -layer trained diﬀerent levels weight decay regularization. layer hidden nodes. norm upper bounds linear layers interlaced normalizations. networks activation normalization proposed here upper bound operator norm easy compute. therefore calculate upper bound product largest singular values linear transforms divided smallest normalization factors normalization transforms. layer norm experiment -layer multilayer perceptron network direct training using mnist dataset produces norm upper bound regularization weak regularization applied linear transforms likely expansive maps shown bottom zigzag curves fig. linear layers largest singular values introduce moderate weight decay regularization using factor linear transform regularized contractive normalizations move signal back right range next linear layer. however full regularized network norm upper bound therefore globally contractive experiment showed neural networks simultaneously deep stable. unlike previous attempts penalty jacobian matrix usually huge therefore scale well constructions eﬃcient architecture changes. network trained using oﬀ-the-shelf training procedures. deeper networks better? using second-order training normalization analyzed gradient distribution consistent scale training degree. gradient energy distribution plot clear stable distribution pattern across diﬀerent layers neural networks. gradient energy measured average norm training neural network process improving ability error reduction. interpret mixture factor explains curves fig. decaying trend left right represents error reduction capability neural networks. longer training deeper networks stronger error reduction eﬀect. deeper networks accumulate eﬀect getting better. however decaying trend right left represents information loss gradient decay. gradient decay less information passed ﬁrst layer network. information loss seems limits lower bound error reduction speed improvement. design deep neural network therefore trade-oﬀ error reduction information loss. diﬀerent activation functions diﬀerent trade-oﬀs seen fig. hand diﬀerent network architectures also diﬀerent trade-oﬀs. currently major transform architectures community. classic type goes highly non-linear transforms likely figure gradient energy distribution -layer narrow trained relu. average energy calculated layer. relu functions faster information loss error contraction rates opposite behavior. network trained suﬃciently long error reduction better. notice curves ﬂatter middle. faster rate error reduction information loss. type isometric therefore rates lower. observations suggest classic type activation functions network width provide constraints network depth. steep patterns ends pattern middle curve also suggest error reduction getting slower depth increase. gives hints eﬃcient neural network design. conjecture observations deep connections biological neural networks. expect future work investigate important balances various network settings. pixel-wise decorrelation? referring feature right multiplication represents calculating convolution followed correlation. assume circular boundary conditions operations accelerated fast fourier transform pixel-wise decorrelation f∗◦f future work investigate whether calculated approach lead eﬃcient implementations. made observation long standing challenge training deep artiﬁcial neural network caused syndrome three inconsistency problems. mathematical analysis combined tools linear algebra practical algorithms intuitive explanations mysterious properties deep neural networks.", "year": 2017}