{"title": "Deep Rewiring: Training very sparse deep networks", "tag": ["cs.NE", "cs.AI", "cs.DC", "cs.LG", "stat.ML"], "abstract": "Neuromorphic hardware tends to pose limits on the connectivity of deep networks that one can run on them. But also generic hardware and software implementations of deep learning run more efficiently for sparse networks. Several methods exist for pruning connections of a neural network after it was trained without connectivity constraints. We present an algorithm, DEEP R, that enables us to train directly a sparsely connected neural network. DEEP R automatically rewires the network during supervised training so that connections are there where they are most needed for the task, while its total number is all the time strictly bounded. We demonstrate that DEEP R can be used to train very sparse feedforward and recurrent neural networks on standard benchmark tasks with just a minor loss in performance. DEEP R is based on a rigorous theoretical foundation that views rewiring as stochastic sampling of network configurations from a posterior.", "text": "neuromorphic hardware tends pose limits connectivity deep networks them. also generic hardware software implementations deep learning efﬁciently sparse networks. several methods exist pruning connections neural network trained without connectivity constraints. present algorithm deep enables train directly sparsely connected neural network. deep automatically rewires network supervised training connections needed task total number time strictly bounded. demonstrate deep used train sparse feedforward recurrent neural networks standard benchmark tasks minor loss performance. deep based rigorous theoretical foundation views rewiring stochastic sampling network conﬁgurations posterior. network connectivity main determinants whether neural network efﬁciently implemented hardware simulated software. example mentioned jouppi google’s tensor processing units weights normally on-chip memory neural network applications despite small weight precision tpus. memory also bottleneck terms energy consumption tpus fpgas example implementation long short term memory network memory reference consumes orders magnitude energy operations situation even critical neuromorphic hardware either hard upper bounds network connectivity unavoidable fast on-chip memory local processing cores severely limited example mbyte local memory cores spinnaker system implementation bottleneck become even severe future applications deep learning number neurons layers increase causing quadratic growth number connections them. evolution apparently faced similar problem evolving large neuronal systems human brain given brain volume dominated white matter i.e. connections neurons. solution found evolution convincing. synaptic connectivity brain highly dynamic sense synapses constantly rewired especially learning words rewiring integral part learning algorithms brain rather separate process. aware previous methods simultaneous training rewiring artiﬁcial neural networks able stay within strict bound total number connections throughout learning process. however several heuristic methods pruning larger network network ﬁrst trained convergence network connections neurons pruned subsequently. methods useful downloading trained network neuromorphic hardware on-chip training. number methods proposed capable reducing connectivity training however algorithms usually start full connectivity. hence besides reducing computational demands partially cannot applied computational resources bounded throughout training. inspired experimental ﬁndings rewiring brain propose article deep rewiring algorithm makes possible train deep neural networks strict connectivity constraints. contrast many previous pruning approaches based heuristic arguments deep embedded thorough theoretical framework. deep conceptually different standard gradient descent algorithms respects. first connection predeﬁned sign. speciﬁcally assign connection connection parameter constant sign non-negative corresponding network weight given skθk. standard backprop absolute value weight moved becomes weight opposite sign. contrast deep connection vanishes case randomly drawn connection tried algorithm. second deep gradient descent combined random walk parameter space modiﬁcation leads important functional differences. fact theoretical analysis shows deep jointly samples network weights network architecture posterior distribution distribution combines data likelihood speciﬁc connectivity prior bayes optimal manner. result algorithm continues rewire connections even performance converged. show feature enables deep adapt network connectivity structure online task demands drifting. show several benchmark tasks deep connectivity several deep architectures fully connected deep networks convolutional nets recurrent networks constrained extremely sparse throughout training marginal drop performance. example standard feed forward network trained mnist dataset achieved good performance connectivity fully connected counterpart. show deep reaches similar performance level state-of-the-art pruning algorithms training starts full connectivity matrix. target connectivity sparse deep outperformed pruning algorithms. stochastic tieleman hinton implemented error backpropagation algorithm dominant learning paradigm contemporary deep learning applications. given list network inputs target network outputs gradient descent iteratively moves parameter vector direction negative gradient error function exy∗ local minimum exy∗ eventually reached. reasons become clear below) parameter values training goal formulated posterior distribution parameters training goal consider article produce sample parameter vectors high probability posterior distribution tempered version posterior temperature parameter. recover posterior distribution peaks posterior ﬂattened distribution sharpened leading higher probabilities parameter settings better performance. training goal explored welling chen kappel shown gradient descent combination stochastic weight updates performs markov chain monte carlo sampling posterior distribution. paper extend results allowing algorithm also sample network structure including hard posterior constraint total number connections sampling process. deﬁne training goal follows deep algorithm many situations network connectivity strictly limited training instance hardware memory limitations. limiting factor training algorithm maximal connectivity ever needed training. deep guarantees hard limit. deep achieves learning goal network conﬁgurations samples network weights biases also connectivity given constraints. achieved introducing following mapping network parameters network weights connection parameter constant sign assigned connection negative connection dormant corresponding weight otherwise connection considered active corresponding weight skθk. hence encodes whether connection active network weight connection active. note single index connection weight instead usual double index deﬁnes sending receiving neuron. connectioncentric indexing natural rewiring algorithms connections focus rather neurons. using mapping sampling posterior equivalent sampling posterior network conﬁgurations network connectivity structure network weights. algorithm pseudo code deep algorithm. sampled zero-mean gaussian unit variance independently active update step. note gradient error exy∗ computed backpropagation mini-batch practice. deep deﬁned algorithm gradient updates performed parameters active connections derivatives error function exy∗ computed usual commonly backpropagation algorithm. since consider classiﬁcation problems article used cross-entropy error experiments article. sampled zero-mean gaussian unit variance independently parameter update step. last term alone would implement random walk parameter space. hence whole line algorithm implements combination gradient descent regularized error function random walk. theoretical analysis shows random walk behavior figure visual pattern recognition sparse networks training. sample training images test classiﬁcation accuracy training various connectivity levels example test accuracy evolution training standard feed forward network trained mnist trained cifar- accuracies shown various algorithms. green deep soft-deep blue initially ﬁxed sparse connectivity; dashed gray fully connected. since soft-deep guarantee strict upper bound connectivity accuracies plotted highest connectivity ever training iteration number refers number parameter updates training. rewiring aspect algorithm captured lines algorithm whenever parameter becomes smaller connection dormant i.e. deleted network longer considered updates connection dormant state connection chosen randomly uniform distribution dormant connections activated parameter initialized rewiring strategy ensures exactly connections active time training dormant connections need computational demands except drawing connections activated. note sparse networks efﬁcient keep list active connections none dormant connections. then efﬁciently draw connections whole possible connections reject already active. rewiring fully connected convolutional networks ﬁrst tested performance deep mnist cifar-. mnist considered fully connected feed-forward network used benchmark pruning algorithms. hidden layers neurons respectively -fold softmax output layer. cifar- dataset used convolutional neural network convolutional followed fully connected layers. reproducibility purposes network architecture parameters taken ofﬁcial tutorial tensorﬂow. cifar- used decreasing learning rate figure rewiring recurrent neural networks. network performance example various connectivity levels fig. lstm network trained timit dataset deep soft-deep network ﬁxed random connectivity dotted line fully connected lstm trained without regularization reported greff thick dotted line fully connected lstm regularization. task performed four training sessions. first trained network deep ﬁrst convolutional layer kept fully connected allowed rewiring second convolutional layer. second tested another algorithm soft-deep simpliﬁed version deep however guarantee strict connectivity constraint third trained network standard manner without rewiring pruning obtain baseline performance. finally trained network connectivity randomly chosen training kept ﬁxed optimization. connectivity however completely random. rather layer received number connections number found soft-deep performance network expected much better network layers treated equally. fig. shows performance algorithms mnist cifar- deep reaches classiﬁcation accuracy constrained connectivity. evaluate precisely accuracy reachable connectivity additional experiment doubled number training epochs. deep reached classiﬁcation accuracy training ﬁxed random connectivity performed surprisingly well connectivities around possibly large redundancy mnist images. soft-deep guarantee strict upper bound network connectivity. considering maximum connectivity ever seen training soft-deep performed consistently worse deep networks maximum connectivity low. cifar- classiﬁcation accuracy deep connectivity level performance deep connectivity close performance fully connected network. study rewiring properties deep monitored number newly activated connections iteration found initial transient number newly activated connections converged stable value remained stable even network performance converged appendix rewiring recurrent neural networks order test generality rewiring approach also considered training recurrent neural networks backpropagation time recurrent networks quite different feed forward counterparts terms dynamics. particular potentially unstable recurrent loops inference training signals. test considered lstm network trained timit data set. rewiring algorithms connections potentially available rewiring including connections gating units. timit audio data mfcc coefﬁcients temporal derivatives computed bi-directional lstms single recurrent layer cells followed softmax generate phoneme likelihood appendix considered ﬁrst baseline fully connected lstm standard bptt without regularization training algorithm. algorithm performed similarly described greff turned however performance could signiﬁcantly improved including regularizer training objective. therefore considered setup regularization setup achieved phoneme error rate note better results reported literature using cost function deeper networks sake easy comparison however sticked much simpler setup medium-sized network standard cross-entropy error function. found connectivity reduced signiﬁcantly setup algorithms fig. algorithms deep soft-deep performed even slightly better fully connected baseline connectivities around probably generalization issues. deep outperformed soft-deep connectivities outperformed bptt ﬁxed random connectivity consistently connectivity level considered. comparison algorithms cannot sparse networks wondered much performance lost strict connectivity constraint taken account training compared pruning algorithms achieve sparse networks training. compared performance deep soft-deep recently proposed pruning algorithms ℓ-shrinkage pruning algorithm proposed ℓ-shrinkage uses simple ℓ-norm regularization ﬁnds network solutions connectivity comparable state chose since relatively close deep difference implement rewiring. pruning algorithm complex uses projection network weights constraint. algorithms prune connections starting fully connected network. hyper-parameters learning rate layer size weight decay coefﬁcients kept experiments. validated extensive parameter search settings good settings comparison algorithms appendix results setups considered shown fig. despite strict connectivity constraints deep soft-deep performed slightly better unconstrained pruning algorithms cifar- timit connectivity levels considered. mnist pruning slightly better larger connectivities. mnist timit pruning ℓ-shrinkage failed completely connectivities rewiring deep softdeep still produced reasonable networks case. interesting observation made error rate evolution lstm timit here ℓ-shrinkage pruning induced large sudden increases error rate possibly instabilities induced parameter changes recurrent network. contrast observed small glitches type deep indicates sparsiﬁcation network connectivity harder recurrent networks potential instabilities deep better suited avoid instabilities. reason advantage deep however clear. temperature parameter kept constant transfer learning supported deep training proposed rewiring algorithms converge static solution explore continuously posterior distribution network conﬁgurations. consequence rewiring expected adapt changes task line manner. task demands change online learning setup hope transfer invariant aspects tasks occurs aspects utilized faster convergence later tasks verify hypothesis performed experiment mnist dataset class output neuron respond changed training epoch fig. shows performance network trained deep class-shufﬂed mnist task. observe performance recovered shufﬂing target classes. importantly found clear trend increasing classiﬁcation accuracy even across shufﬂes. indicates form transfer learning network information previous tasks figure efﬁcient network solutions strict sparsity constraints. accuracy connectivity obtained deep soft-deep comparison achieved pruning ℓ-shrinkage accuracy connectivity mnist cifar- algorithm network decent compromise accuracy sparsity chosen connectivity across training iterations shown below. performance timit dataset. phoneme error rates connectivities across iteration number representative training sessions. preserved network utilized following instances. hypothesized reason transfer early layers developed features invariant target shufﬂing need re-learned later task instances. verify hypothesis computed following quantities. first order quantify speed parameter dynamics different layers computed correlation layer weight matrices subsequent training epoch second order quantify speed change network dynamics different layers computed correlation neuron outputs layer subsequent epochs found correlation weights layer outputs increased across training epochs signiﬁcantly larger early layers. supports hypothesis early network layers learned features invariant shufﬂed coding convention output layer. figure transfer learning deep target labels mnist data shufﬂed every epoch. network accuracy training epoch. increase network performance across tasks indicates transfer knowledge tasks. correlation weight matrices subsequent epochs network layer. correlation neural activity vectors subsequent epochs network layer. transfer visible ﬁrst hidden layer since weights outputs layer correlated across tasks. shaded areas represent standard deviation across random seeds inﬂuencing network initialization noisy parameter updates shufﬂing outputs. theoretical analysis deep somewhat involved implemented hard constraints. therefore ﬁrst introduce discuss another algorithm soft-deep theoretical treatment convergence straight forward. contrast standard gradient-based algorithms convergence convergence particular parameter vector convergence target distribution network conﬁgurations. convergence properties soft-deep soft-deep algorithm given algorithm note updates active connections deep also mapping parameters weights deep main conceptual difference deep connection parameters continue random walk dormant random walk connections re-activated random times cross zero. therefore soft-deep impose hard constraint network connectivity rather uses norm regularization impose soft-constraint. since dormant connections simulated algorithm computationally inefﬁcient sparse networks. approximation could used silent connections re-activated constant rate leading algorithm similar deep deep adds additional feature strict connectivity constraint. central result soft-deep proven context spiking neural networks order understand rewiring brain functional perspective. theory however also applies standard deep neural networks. able apply standard inﬁnitesimal updates standard wiener process. describes gradient ascent posterior combined random walk parameter space. show appendix unique stationary distribution parameter dynamics given since considered classiﬁcation tasks article interpret network output multinomial distribution class labels. then derivative likelihood equivalent derivative negative cross-entropy error. together regularization term prior discretization time obtain update line algorithm non-negative parameters. negative parameters ﬁrst term vanishes since network weight constant zero there. leads update line note introduced reﬂecting boundary θmin practical algorithm avoid divergence parameters convergence properties deep detailed analysis stochastic process underlies algorithm provided appendix summarize main ﬁndings. iteration deep algorithm consists parts ﬁrst part connections currently active advanced keeping parameters second part connections became dormant ﬁrst step randomly replenished. describe connectivity constraint connections introduce binary constraint vector represents active connections i.e. element connection allowed active zero else. theorem appendix link deep compound markov chain operator simultaneously updates parameters according soft-deep dynamics constraint constraint vector itself. stationary distribution markov chain given joint probability allowed active. marginalizing obtain posterior distribution deep identical soft-deep constraint connectivity fulﬁlled. marginalizing obtain probability sampling network architecture deep soft-deep proportional another. difference deep exclusively visits architectures active connections appendix details). words deep solves constraint optimization problem sampling parameter vectors high performance within space constrained connectivities. algorithm therefore spend time network conﬁgurations connectivity supports desired network function that connections large support objective function maintained active high probability connections randomly tested discarded found useful. related work freitas considered sequential monte carlo sampling train neural networks combining stochastic weight updates gradient updates. stochastic gradient updates mini-batch learning considered welling also link true posterior distribution established. chen proposed momentum scheme temperature annealing stochastic gradient updates leading stochastic optimization method. deep extends approach using stochastic gradient monte carlo sampling parameter updates also sample connectivity network. addition posterior deep subject hard constraint network architecture. sense deep performs constrained sampling constrained stochastic optimization temperature annealed. patterson considered problem stochastic gradient dynamics constrained probability simplex. methods considered however readily applicable problem constraints connection matrix considered here. additionally show correct sampler constructed simulate dormant connections. sampler efﬁcient sparse connection matrices. thus developed novel method random reintroduction connections analyzed convergence properties conclusions presented method modifying backprop backprop-through-time weights connections also connectivity graph simultaneously optimized training. achieved staying always within given bound total number connections. absolute value weight moved backprop becomes weight opposite sign. contrast deep connection vanishes case randomly drawn connection tried algorithm. setup requires that like neurobiology sign weight change learning. another essential ingredient deep superimposes gradient-driven dynamics weight random walk. feature viewed another inspiration neurobiology important property deep spite stochastic ingredient overall learning dynamics remains theoretically tractable gradient descent usual sense convergence stationary distribution network conﬁgurations assigns largest probabilities best-performing network conﬁgurations. automatic beneﬁt ongoing stochastic parameter dynamics training process immediately adjusts changes task simultaneously transferring previously gained competences network acknowledgements written partial support human brain project european union austrian science fund thank franz pernkopf matthias z¨ohrer useful comments regarding timit experiment. alex graves abdel-rahman mohamed geoffrey hinton. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee anthony holtmaat joshua trachtenberg linda wilbrecht gordon shepherd xiaoqun zhang graham knott karel svoboda. transient persistent dendritic spines neocortex vivo. neuron forrest iandola song matthew moskewicz khalid ashraf william dally kurt keutzer. squeezenet alexnet-level accuracy fewer parameters model size. arxiv preprint arxiv. norman jouppi cliff young nishant patil david patterson gaurav agrawal raminder bajwa sarah bates suresh bhatia boden borchers in-datacenter performance analysis tensor processing unit. arxiv preprint arxiv. paul merolla john arthur rodrigo alvarez-icaza andrew cassidy sawada filipp akopyan bryan jackson nabil imam chen yutaka nakamura million spiking-neuron integrated circuit scalable communication network interface. science zichao yang marcin moczulski misha denil nando freitas alex smola song ziyu wang. deep fried convnets. proceedings ieee international conference computer vision seide deng. exploiting sparseness deep neural networks large vocabulary speech recognition. ieee international conference acoustics speech signal processing march choosing hyper-parameters deep learning rate deﬁned task independently considering number active connections given constraint remaining hyper parameters regularization coefﬁcient temperature found performance deep depend strongly temperature choice done carefully. dataset ideal value order magnitude higher lower typically lead substantial loss accuracy. mnist accuracy constraint connectivity achieved chosen timit cifar- different assigned connectivity matrix. reach accuracy connectivity used layer input output temperature initialized choosing hyper-parameters soft-deep main difference soft-deep deep connectivity given global constraint. considerable drawback strict constraint hardware limitation also advantage simply wants generate sparse network solutions without clear idea connectivities reachable task architecture considered. cases performance depends choice hyper-parameters θmin also unlike deep hyper parameters inter-dependent relationships cannot ignore reason softdeep depends temperature rate re-activation connections driven amplitude noise whereas decoupled deep summarize results high θmin leads high performance also deﬁnes approximate lower bound smallest reachable connectivity. lower bound estimated computing analytically stationary distribution rough approximations assumption gradient likelihood zero. pmin targeted lower connectivity bound needs θmin mnist used data points fig. panel range values θmin scope across different ranges connectivity lower bounds. timit cifar- used simpler strategy lead similar outcome ﬁxed relationships gradient descent iteration. performance algorithm evaluated different varying logarithmic scale privilege sparse connectivity high accuracy. instance mnist figure used form going optimal parameter implemented pruning described algorithm uses several phases training pruning training also another pruning iteration training pruning training pruning training. went latter increased performance. training phase complete training neural network ℓ-regularization. pruning phase standard deviation weights within weight matrix wstd computed active weights absolute values smaller qwstd pruned grid search figure hyper-parameter search pruning algorithm according point grid represents weight decay coefﬁcient quality factor pair. number color indicate performance terms accuracy connectivity rectangle indicates data points used fig. mnist used standard feed forward network architecture hidden layers neurons rectiﬁed linear activation functions followed -fold softmax output. algorithms used learning rate batch size standard stochastic gradient descent. learning stopped epochs. reported performances article based classiﬁcation error mnist test set. cifar- ofﬁcial tutorial convolutional networks tensorﬂow used reference implementation. performance out-of-the-box provides fully connected baseline. used values given tutorial hyper-parameters algorithms. particular layer-speciﬁc weight decay coefﬁcients interact algorithms chosen tutorial deep soft-deep pruning ℓ-shrinkage. fully connected baseline implementation standard stochastic gradient descent used decreasing learning rate initialized decayed factor every epochs. training performed million iterations algorithms. soft-deep includes temperature parameter keeping high temperature weight decays increasing rate re-activation connections. even intermediate solutions rather sparse efﬁcient solutions convergence always dense. therefore weight decay accompanied annealing temperature done setting temperature proportional decaying annealing used deep soft-deep timit timit dataset preprocessed lstm architecture chosen reproduce results greff input time series formed mfcc coefﬁcients energy computed time frame. inputs expanded ﬁrst second temporal derivatives. different phonemes annotated timit dataset report error rate comparable literature performed standard grouping phonemes generate output classes usual dialect speciﬁc sentences excluded phoneme error rate computed proportion misclassiﬁed frames. validation early stopping necessary train network dense connectivity matrix timit performance sometimes unstable suddenly dropped training seen fig. ℓ-shrinkage. therefore validation deﬁned randomly selecting training utterances. algorithms trained epochs reported test error rate minimal validation error. accelerate training comparison reference greff used mini-batches size adam optimizer also opportunity test performance deep soft-deep variant gradient descent. learning rate kept default momentum parameters adam found changing parameter improved stability fully connected networks training recurrent setup. could reference implemented ℓ-shrinkage combination adam simply applied shrinkage operator iteration adam might ideal choice theory. worked well practice minimal error rate reached setup. type regularization combination adam used deep soft-deep lead sparse efﬁcient network solutions. initialization connectivity matrices found performance networks depended strongly initial connectivity. therefore followed following heuristics generate initial connectivity deep soft-deep control setup ﬁxed connectivity. first connectivity matrix individual layer zero entries chosen uniform probability. second given connectivity constraint found learning time increased performance dropped initial connectivity matrices chosen carefully. typically performance dropped drastically output layer initialized sparse. networks number parameters dominated large connectivity matrices hidden layers. basic rule thumb worked cases give equal number active connections large intermediate weight matrices whereas smaller ones typically output layers densely connected. suggest approaches reﬁne guess either look statistics connectivity matrices convergence deep soft-deep possible second alternative initialize soft-deep dense matrix observe connectivity matrix convergence. experiments connectivities convergence coherent rule thumb described need pursue intensive search ideal initial connectivity matrices. mnist number parameters layer input output. using rule thumb given global connectivity layers respectively initialized connectivity cifar- baseline network convolutional layers ﬁlters shapes respectively followed fully connected layer weight matrices shape last layer projected softmax output classes. numbers parameters connectivity matrices therefore input output. connectivity matrices initialized connectivity timit connection matrix input hidden layer size recurrent matrix size size output matrix three connectivity matrices initialized connectivity respectively. would good initialize parameters dormant connections zero soft-deep single noisy iteration half would become active would fail initialize network sparse connectivity matrix. balance problem initialized parameters dormant connections uniformly clipping value θmin zero soft-deep parameters figure experiment provided figure variant mnist experiment target labels shufﬂed every training epoch. make generalization capability deep small number epochs visible enhanced noise exploration setting batch connectivity matrices updated every time step. also used figure rewiring behavior deep network performance versus training iteration absolute number newly activated connections layer output layer iteration. note layers quite different numbers potential connections panel number newly activated connections shown relative number potential connections layer larger network neurons hidden layer. remaining parameters similar used previously connectivity constrained connectivity matrices initialized respective connectivities parameters deep fig. shows rewiring behavior deep network layer feed-forward neural network trained mnist training indicated small gray around green fig. since takes iterations weights connections contribute reduction error driven number newly established connections layer small layers initially. initial transient number newly activated connections stabilized value proportional total number potential connections layer deep continued rewire connections even late training process. prove stochastic parameter dynamics converges target distribution given proof analogous derivation given kappel reiterate proof special case supervised learning. fundamental property synaptic sampling dynamics formalized theorem proven below. state theorem brieﬂy discuss statement simple terms. consider initial parameter setting time parameters change according dynamics since dynamics include noise term exact value parameters time cannot determined. however possible describe exact distribution parameters time denote distribution subscript stands fokker-planck since evolution distribution described fokker-planck equation given below. note make dependence distribution time explicit notation. shown dynamics converges well-deﬁned unique stationary distribution limit large prove convergence stationary distribution show kept invariant sdes reached initial condition. theorem strictly positive continuous probability distribution parameters twice continuously differentiable respect stochastic differential equations leaves distribution invariant. furthermore unique stationary distribution sampling dynamics. denotes distribution network parameters time show leaves distribution invariant show change) plugging presumed stationary distribution right hand side obtains proves stationary distribution parameter sampling dynamics since positive construction markov process sdes ergodic stationary distribution unique include automatic network rewiring deep learning model adopt approach described kappel instead using network parameters directly determine synaptic theorem requires twice differentiable true ﬁnite value simulations used limiting case large dormant connections actually mapped zero weight. limit approaches simple expression results diffusion term integrated gaussian random variable zero mean unit variance. term results exponential prior distribution note prior differentiable approximate gradient assuming zero below. thus parameters negative axis driven random walk parameter values might therefore diverge value). another potential solution would different prior distribution also effects negative axis however found produces good results practice. provide additional details convergence properties deep algorithm. formulate algorithm terms markov chain evolves parameters connectivity constraints application markov transition operators corresponds iteration deep algorithm. show distribution parameters network connectivities iterations deep converges stationary distribution jointly realizes parameter vectors admissible connectivity constraints. transition operators applied parameter updates iteration. transition operatortθ updates corresponds line updates connectivity constraint vector corresponds lines algorithm denote parameter vector connectivity constraint previous time step respectively. active connection constraint dynamics i.e. connections dormant evolution parameters. transition operators conditional probability distributions iteration samples drawn given previous values uniform probability among constraint vectors compatible current parameters write number possible vectors compatible given binomial coefﬁcient denotes vectorized kronecker delta function else. note assigns non-zero probability vectors zero elements true addition vectors fulﬁll therefore proof divided proofs. first show distribution deﬁned normalization constant left invariant second show normalization constant equal coherence notation verbally element proof. show distribution deﬁned factorizes terms depend integral product simpler integrals. ﬁrst study distribution factorizing notice strong property distribution. let’s partition tempered posterior distribution cases constraint satisﬁed study operator factorizes similarly built independent dormant ones. moreover terms evolve active parameters independent dormant ones long know dormant. thus operator ﬁrst integral active connections θ∈c. operator integrates active parameters difference network reduced sparse architecture parameters active. want relationship stationary distribution operator written integral deﬁned equation tempered posterior dense network. fact tempered posterior dense network marginalized conditioned dormant connections prove this detail following paragraph drift evolving sparse network given log-posterior dense network conlihood dense network. likelihood sparse network deﬁned previously exception dormant connections given zero-weight equal difference prior deﬁnes soft-deep dormant connections prior sparse network deﬁned before. prior connection-speciﬁc independent implies independent dormant connection prior equal thus looking back deﬁnition posterior tioned negativity dormant connections posterior βdwk remains unchanged drift term given gradient log-posterior applying theorem conﬁrm tempered conditioned posterior dense network left invariant evolving sparse network. integration given also leaves invariant. yields non-zero probability stationary distribution also unique. seen noting assumption connection become dormant sooner later thus state reached state markov chain therefore irreducible stationary distribution unique. lemma provides case algorithm existence invariant distribution needed apply theorem conclude distribution deﬁned plugging result lemma result theorem left invariant algorithm written setting connections become negative. found process used deep works well practice. reason implement algorithm practice want consume memory storing parameter dormant connections. difference obsolete view point network function given pair negative neither strictly zero inﬂuence network function. difference might seem problematic consider properties convergence speciﬁc stationary distribution proven algorithm extends deep however theorem implementation rather unspeciﬁc regarding choice prior negative sides believe that good choices priors negative side conceptual quantitative difference distribution explored deep minor general algorithm decent mathematical formalization deep purpose paper.", "year": 2017}