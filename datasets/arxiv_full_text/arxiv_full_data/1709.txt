{"title": "A Bayesian Network View on Acoustic Model-Based Techniques for Robust  Speech Recognition", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "This article provides a unifying Bayesian network view on various approaches for acoustic model adaptation, missing feature, and uncertainty decoding that are well-known in the literature of robust automatic speech recognition. The representatives of these classes can often be deduced from a Bayesian network that extends the conventional hidden Markov models used in speech recognition. These extensions, in turn, can in many cases be motivated from an underlying observation model that relates clean and distorted feature vectors. By converting the observation models into a Bayesian network representation, we formulate the corresponding compensation rules leading to a unified view on known derivations as well as to new formulations for certain approaches. The generic Bayesian perspective provided in this contribution thus highlights structural differences and similarities between the analyzed approaches.", "text": "abstract—this article provides unifying bayesian network view various approaches acoustic model adaptation missing feature uncertainty decoding well-known literature robust automatic speech recognition. representatives classes often deduced bayesian network extends conventional hidden markov models used speech recognition. extensions turn many cases motivated underlying observation model relates clean distorted feature vectors. converting observation models bayesian network representation formulate corresponding compensation rules leading uniﬁed view known derivations well formulations certain approaches. generic bayesian perspective provided contribution thus highlights structural differences similarities analyzed approaches. robust automatic speech recognition still represents challenging research topic. main obstacle namely mismatch test training data tackled enhancing observed speech signals features order meet training conditions compensating distorted test conditions acoustic model system. methods modify acoustic model general termed model-based model compensation approaches comprise inter alia following sub-categories so-called model adaptation techniques mostly update parameters acoustic model i.e. hidden markov models prior decoding observed feature vectors. contrast decoder-based approaches re-adapt parameters observed feature vector. common decoder-based approaches missing feature uncertainty decoding incorporate additional timevarying uncertainty information evaluation hmms’ probability density functions various model compensation techniques exhibit distinct steps taken interchangeably first compensation parameters need estimated second actual compensation rule applied acoustic model. compensation rules often motivated based observation model relates clean distorted feature vectors e.g. logarithmic melspectral frequency cepstral coefﬁcient domain. article show compensation rules deduced bayesian network representations observation models several uncertainty decoding missing feature model adaptation techniques addition give bayesian network description generic uncertainty decoding approach maximum posteriori adaptation technique alternative topologies bayesian networks sporadically employed context article give formulations certain considered algorithms order gaps uniﬁed description. throughout following feature vectors denoted bold-face letters time index feature vector sequences written without distinguishing random variable realization random variable denoted normally distributed real-valued random vector mean vector covariance matrix write summation goes possible state sequences superseding explicit dependency right-hand side note scaled without inﬂuencing discrimination capability acoustic score w.r.t. changing word sequences thus deﬁne later use. compensation rules wide range model adaptation missing feature uncertainty decoding approaches expressed modifying bayesian network structure conventional applying inference rules bayesian networks potentially followed suitable approximations ensure mathematical tractability. approaches postulate certain bayesian network structure others indirectly deﬁne modiﬁed bayesian network assuming observed feature vector distorted version underlying clean feature vector introduced latent variable e.g. figure latter case relation expressed analytical observation model incorporates certain compensation parameters note distinguished whether output front-end enhancement process noisy reverberant observation directly recognizer. converting observation model bayesian network representation derived exploiting inference rules bayesian networks case abstract perspective taken paper reveals fundamental difference model adaptation approaches hand missing feature uncertainty decoding approaches hand model adaptation techniques usually assume constant statistics time i.e. exempliﬁed section bayesian network view conveniently illustrates underlying statistical dependencies model-based approaches. approaches share bayesian network underlying joint pdfs involved random variables share decomposition properties. however crucial aspects reﬂected bayesian network particular functional form joint potential approximations arrive tractable algorithm well estimation procedure compensation parameters. approaches estimate parameters acoustic front-end others derive clean distorted data. clarity entirely focus article compensation rules ignoring parameter estimation step. also neglect approaches apply modiﬁed training method conventional hmms without exhibiting distinct compensation step characteristic e.g. case discriminative multi-condition reverberant training past decades range survey papers books published summarizing state-of-the-art noise reverberation-robust recently comprehensive review noise-robust techniques published providing taxonomy-oriented framework distinguishing whether e.g. prior knowledge uncertainty processing explicit distortion model used not. contrast previous survey articles pursue threefold goal article first classifying considered techniques along dimension motivating describing bayesian network formalism. consequently conceptually distinguish whether given method employs time-varying often play role enhanced feature vector e.g. wiener ﬁltering front-end measure uncertainty enhancement process respectively. thus point estimate seen enriched additional reliability information cbn. observation model representable bayesian network figure exploiting conditional independence properties bayesian networks compensation observation likelihood leads uncertainty decoding whether distorted vector preprocessed genuinely noisy reverberant observation. also distinction implicit explicit observation models dissolves formalism. second goal closing gaps presenting derivations formulations considered techniques. instance bayesian network representations concepts subsections section presented far. moreover links bayesian network framework formulations explicitly stated ﬁrst time paper. third goal bayesian network description provide graphical illustration allows easily overview broad class algorithms immediately identify similarities differences terms underlying statistical assumptions. establishing links existing concepts abstract overview therefore also serve basis revealing exploring directions. note however review presented paper claim cover relevant acoustic model-based techniques rather meant inspiration researchers. following consider compensation rules several acoustic model-based bayesian network view. concepts subsections iv-a iv-f belong category uncertainty decoding subsections iv-g iv-j class missing feature approaches. furthermore methods summarized subsections iv-k iv-s commonly referred acoustic model adaptation techniques. concepts subsection iv-t examples model-based techniques using modiﬁed topologies. without loss generality single gaussian assumed since case gaussian mixture model linear mismatch function applied gaussian component separately. residual error term. since analytical derivation intractable approximate evaluated based assumption gaussian compensation applied gaussian component separately observation likelihood becomes according figure stereo piecewise linear compensation environment approach ﬁrst introduced developed popular method cepstral feature enhancement based mapping learned stereo data splice used derive minimum mean square error analytically derived analogously practice compensation parameters µb|kn cb|kn estimated gaussian component regression class comprising gaussian components many techniques reverberation modeling speech recognition concept assumes environmental distortion additive melspectral domain. however remos also considers inﬂuence previous clean speech feature vectors xn−ln− order model dispersive effect reverberation relax conditional independence assumption conventional hmms. observation model reads logmelspec domain normally distributed random variables model additive noise components early part room impulse response weighting late part respectively parameters represent deterministic description late part rir. bayesian network depicted figure contrast compensation rules discussed article remos concept necessitates modiﬁcation viterbi decoder introduced cross-connections figure order arrive computationally feasible decoder marginalization previous clean speech components xn−ln− circumvented estimate recognizer also applicable context uncertainty decoding focus following. order derive bayesian network representation uncertainty decoding version splice ﬁrst note fundamental assumption similar remos generic uncertainty decoding approach proposed considers cross-connections bayesian network order relax conditional independence assumption hmms. concept example uncertainty decoding compensation rule deﬁned modiﬁed bayesian network structure given figure without ﬁxing particular functional form involved pdfs analytical observation model. order derive compensation rule start introducing sequence latent clean speech vectors summand exploited conditional independence properties deﬁned figure dropped last line represents constant factor respect varying state sequence numerator next turned given approximations figure compensation rule deﬁned exhibits decoupling thus carried withmodifying underlying decoder. practice e.g. modeled separate gaussian density separate markov process next turn missing feature techniques used model feature distortion front-end enhancement process noise reverberation major subcategory missing feature approaches called feature vector imputation feature vector component either classiﬁed reliable unreliable denote reliable unreliable components n-th feature vector respectively unreliable components withdrawn original observation reliable components directly plugged pdf. score calculation therefore becomes second major subcategory missing feature techniques called marginalization unreliable components replaced marginalizing clean-speech distribution usually derived separately modeled. posterior likelihood thus becomes approach presented implicitly assumes basic observation model denotes uncertainty enhancement algorithm. corresponding bayesian network depicted figure implying observation likelihood becomes underlying bayesian network corresponds figure explains name combines independent parallel models clean-speech distortion model since resulting adapted cannot derived analytical closed form variety approximations true investigated nonstationary distortions proposes employ separate distortion leading bayesian network representation figure marginalizing concept vector taylor series frequently employed practice yielding promising results fundamental idea linearize nonlinear distortion model taylor series standard approach based log-sum observation model next investigate several acoustic model adaptation techniques starting fundamental framework parallel model combination observation model concept based log-sum distortion model reads static logmelspec domain captures short convolutive distortion models additive noise components. bayesian network represented figure note contrast uncertainty decoding constant time. adapted form thus analytically intractable assumed ﬁrstly applied gaussian component individually secondly approximated taylor series around µx|kn denotes mean component various extensions concept neglected here. comprehensive review refer bayesian network corresponds figure regression classes reﬂected dependency observation model parameters gaussian component afﬁne observation model equivalent transforming mean vector µx|kn covariance matrix cx|kn gaussian component cmllr represents popular adaptation technique practice promising results versatile ﬁelds application speaker adaptation adaptive training well noise reverberation-robust asr. next describe adaptation applied parameters pdfs hmm. parameters considered bayesian parameters i.e. random variables drawn times depicted figure direct consequence observation vectors conditionally dependent given state sequence. predictive therefore explicitly depends adaptation data denote becomes applied mean vectors only mllr turn considered simpliﬁed version cmllr observation model bayesian network figure assumed compensation variances omitted. bayesian network representation figure also underlies general mllr adaptation rule case however seems impossible identify corresponding analytic observation model representation. iterative solution obtained expectation maximization algorithm. note approximation posterior conditional independence assumption fulﬁlled conventional decoder employed. mentioned before uncertainty decoding techniques allow time-varying model adaptation approaches subsections iv-k iv-l iv-q mostly constant time. cases however randomized model parameter assumed redrawn time step figure contrast bayesian estimation mentioned usually refers inference problems random model parameters drawn times figure usually drawn gaussian distribution here consider different regression classes assume single gaussian since case linear mismatch function applied gaussian component separately. likelihood score thus becomes score e.g. approximated frame-synchronous viterbi search another approach apply bayesian integral frame-wise manner standard decoder case integral becomes original assumption identical time steps relaxed case identically distributed times steps approximation representable conversion bayesian network figure constant probability density function additive noise component modeled normally distributed random variable deterministic description reverberant distortion. sake tractability observation model approximated similar manner approach. concept seen alternative remos remos tailors viterbi decoder modiﬁed bayesian network reverberant avoids computationally expensive marginalization previous clean-speech vectors averaging thus smoothing clean-speech statistics possible previous states gaussian components. thus assumed depend extended clean-speech vector figure besides previously mentioned remos reverberant reverberant cmllr concepts three related approaches employing convolutive observation model order describe dispersive effect reverberation three approaches assume following model logmelspec domain denotes deterministic description reverberant distortion differently determined three approaches. observation model represented bayesian network figure without random component log-add approximation derive i.e. µy|kn µx|kn denote mean kn-th gaussian component respectively. previous means µx|qn−l averaged means corresponding hand employs log-normal approximation adapt according perform adaptation prior recognition standard decoder concept proposed performs online adaptation based best partial path pointed variety approximations statistics log-sum gaussian random variables ranging different methods maximum piecewise linear analytical approximations close section turning model-based approaches cannot classiﬁed model adaptation postulate different topologies rather adapting conventional hmm. approaches relaxing conditional independence assumption conventional hmms order improve modeling inter-frame correlation. concept conditional hmms models observation depending previous observations time shifts summand therefore becomes article described compensation rules several acoustic model-based techniques employing bayesian network representations. presented bayesian network descriptions already given original papers others easily derived based original papers contrast link concepts modiﬁed imputation signiﬁcance decoding cmllr/mllr bayesian mllr takiguchi bayesian network framework formulations respectively explicitly stated ﬁrst time paper. clearly graphical model description neglects various crucial aspects related considered concepts importantly neither particular functional form joint potential approximations arrive tractable algorithm provenance compensation parameters reﬂected. cross-connections depicted figures show underlying concept aims improving modeling inter-frame correlation e.g. increase robustness acoustic model reverberation. applied straightforward cross-connections would entail costly modiﬁcation viterbi decoder. paper summarized important approximations allow efﬁcient decoding extended bayesian network subsections iv-e iv-f iv-q iv-r. typically empirically motivated intuitive approximations especially neglected statistical dependencies become obvious bayesian network shown figures approaches introducing instantaneous extensions bayesian network figures usually compensating nondispersive distortions additive short convolutive noise. existence additional latent variable presented bayesian network representations expresses explicit observation model implicit statistical model clean corrupted features employed. contrast graphical representations figures show instead distinct compensation step modiﬁed topology used. summary condensed description various concepts bayesian network perspective shall offer researchers possibility easily exploit combine existing techniques link algorithms presented ones. seems important recent acoustic modeling approaches based deep neural networks raise challenges conventional robustness techniques pointed possible solution exploiting traditional gmm-based robustness approaches ones reviewed within deep learning paradigm could dnn-derived bottleneck features gmm-hmm offers various possibilities research. deng droppo acero dynamic compensation variances using feature enhancement uncertainty computed parametric model speech distortion ieee trans. speech audio process. vol. maas kellermann sehr yoshioka delcroix kinoshita nakatani formulation remos concept uncertainty decoding perspective proc. int. conf. digital signal process. stern missing-feature approaches speech recognition ieee signal process. mag. vol. kolossa klimas orglmeister separation robust recognition noisy convolutive speech mixtures using time-frequency masking missing data techniques proc. waspaa haeb-umbach novel uncertainty decoding rule applications transmission error robust speech recognition ieee trans. audio speech lang. process. vol. maas kotha sehr kellermann combined-order hidden markov models reverberation-robust speech recognition proc. int. workshop cognitive inform. process. haeb-umbach uncertainty decoding conditional bayesian estimation robust speech recognition uncertain missing data kolossa haeb-umbach eds. springer heigold schl¨uter wiesler discriminative training automatic speech recognition modeling criteria optimization implementation performance ieee signal process. mag. vol. matassoni omologo giuliani svaizer hidden markov model training contaminated speech material distanttalking speech recognition comput. speech lang. vol. yoshioka sehr delcroix kinoshita maas nakatani kellermann making machines understand reverberant rooms robustness reverberation automatic speech recognition ieee signal process. mag. vol. delcroix nakatani watanabe static dynamic variance compensation recognition reverberant speech dereverberation preprocessing ieee trans. audio speech lang. process. vol. deng droppo acero recursive estimation nonstationary noise using iterative stochastic approximation robust speech recognition ieee trans. speech audio process. vol. sehr maas kellermann reverberation model-based decoding logmelspec domain robust distant-talking speech recognition ieee trans. audio speech lang. process. vol. vincent barker watanabe roux nesta matassoni second’chime’speech separation recognition challenge overview challenge systems outcomes proc. asru kinoshita delcroix yoshioka nakatani sehr kellermann maas reverb challenge common evaluation framework dereverberation recognition reverberant speech proc. waspaa delcroix kinoshita nakatani araki ogawa hori watanabe fujimoto yoshioka speech recognition presence highly non-stationary noise based spatial spectral temporal speech/noise modeling combined dynamic variance adaptation proc. int. workshop mach. listening multisource environments xiong moritz rehr anemuller meyer gerkmann doclo goetze robust reverberant environments using temporal cepstrum smoothing speech enhancement amplitude modulation ﬁlterbank feature extraction proc. reverb workshop maas sehr gugat kellermann highly efﬁcient optimization scheme remos-based distant-talking speech recognition proc. european signal processing conf. hershey rennie roux factorial models noise robust speech recognition techniques noise robustness automatic speech recognition virtanen singh eds. john wiley sons", "year": 2013}