{"title": "Deconvolutional Latent-Variable Model for Text Sequence Matching", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "A latent-variable model is introduced for text matching, inferring sentence representations by jointly optimizing generative and discriminative objectives. To alleviate typical optimization challenges in latent-variable models for text, we employ deconvolutional networks as the sequence decoder (generator), providing learned latent codes with more semantic information and better generalization. Our model, trained in an unsupervised manner, yields stronger empirical predictive performance than a decoder based on Long Short-Term Memory (LSTM), with less parameters and considerably faster training. Further, we apply it to text sequence-matching problems. The proposed model significantly outperforms several strong sentence-encoding baselines, especially in the semi-supervised setting.", "text": "vectors used predictions relationships between corresponding sentences case matching complexity independent sentence length. however found hard encode semantic information entire sequence single vector models important learn informative sentence representation properties preserves fundamental details e.g. n-gram fragments within sequence text; learned representation contain discriminative information regarding relationship target sequence. motivated propose infer embedding sentence deep generative models ability make effective unlabeled data learn abstract features complex data moreover objective generative model addresses generation/reconstruction thus learns latent codes naturally preserve essential information sequence making particularly well suited sentence matching. recent advances neural variational inference manifested deep latent-variable models text general idea sentence continuous latent variable code inference network generative network reconstruct input sentence conditioned samples latent code ﬁrst attempt proposed variational auto-encoder -based generative model text lstm networks sequence decoder. however recurrent nature lstm decoder model tends largely ignore information latent variable; learned sentence embedding contains little information input even several training modiﬁcations mitigate issue proposed dilated rather lstm decoder latent-variable model. since decoder less dependent contextual information previous words latent-variable representation tends encode information input sequence. latent-variable model introduced text matching inferring sentence representations jointly optimizing generative discriminative objectives. alleviate typical optimization challenges latent-variable models text employ deconvolutional networks sequence decoder providing learned latent codes semantic information better generalization. model trained unsupervised manner yields stronger empirical predictive performance decoder based long short-term memory less parameters considerably faster training. further apply text sequence-matching problems. proposed model signiﬁcantly outperforms several strong sentence-encoding baselines especially semisupervised setting. ability infer degree match text sequences determine semantic relationship central importance natural language understanding reasoning recent advances deep neural networks considerable research focused developing end-to-end deep learning models text sequence matching state-of-the-art models typically ﬁrst encode text sequences hidden units long short term memory model convolutional neural network techniques like attention mechanisms memory networks subsequently applied ﬁnal sequence matching usually addressed classiﬁcation problem. however word-by-word matching nature models typically gives rise high computational complexity either sentence length. therefore approaches computationally expensive difﬁcult scale large datasets long text sequences. another class models matching natural language sentences based sentence encoding methods sentence mapped vector copyright association advancement artiﬁcial intelligence rights reserved. distribution {entailment contradiction neutral}. entailment indicates inferred other contradiction suggests opposite semantic meanings neutral means irrelevant other. framework generalized natural language processing applications paraphrase identiﬁcation paraphrase otherwise. regard text sequence matching viewed either binary multiclass classiﬁcation problem although word/phrase-level attention matching strategies often applied text sequence-matching problems consider sentence encoding-based models promising complexity. speciﬁcally model based siamese architecture consists twin network processes natural language sentence pairs independently interaction sentence representations inferred. classiﬁcation layer built latent representations ﬁnal prediction shared encoder network designed form nonlinear transformation including convolutional neural networks recurrent neural networks multi-layer perceptrons however effectively match natural language sentences siamese architecture learn informative sentence representations encoder network. describe cnns context latent-variable model. latent-variable models text processing sequence-to-sequence models common strategy obtaining robust sentence representations capable leveraging information unlabeled data. models ﬁrst encode input sentence ﬁxed-length vector reconstruct/generate output sequence speciﬁcally autoencoder setup output decoder reconstruction input sentence denoted words cnns used generative network ground-truth words need decoder training potential issues given powerful recursive autoregressive nature decoders latent-variable model tends ignore latent vector altogether thus reducing pure language model i.e. latent representations effective training learned latent vector necessarily encode information needed reconstruct entire sequence since additional guidance provided generating every word i.e. exposure bias propose deconvolutional networks sequence decoder latent-variable model matching natural language sentences. without recurrent structure decoder typical optimization issues associated training latent-variable models text mitigated. further global sentence representations effectively learned since ground-truth words made available decoder training. experiments ﬁrst evaluate deconvolutionbased model unsupervised manner examine whether learned embedding automatically distinguish different writing styles. demonstrate latent codes model informative lstmbased models achieving higher classiﬁcation accuracy. apply latent-variable model textsequence matching tasks predictions made based samples latent variables. consequently without prior knowledge language structure used traditional text analysis approaches deconvolutional latent-variable model outperforms several competitive baselines especially semi-supervised setting. main contributions follows propose neural variational inference framework matching natural language sentences effectively leverages unlabeled data achieves promising results little supervision. employ deconvolutional networks sequence decoder alleviating optimization difﬁculties training latent-variable models text resulting informative latent sentence representations. matching natural language sentences assume sentences wish compute degree match. notational simplicity describe model context recognizing textual entailment thus denote sequences premise hypothesis sentence pair represented total number pairs. goal sequence matching predict judgement corresponding sentence pair modeling conditional deterministic generally nonlinear transformation deterministic result poor model generalization especially limited number labeled data available training. consider probabilistic representation i.e. recently introduced neural variational inference framework text modeling infer stochastic latent variable model input text constructing inference network approximate true posterior distribution strategy endows latent variable better ability generalize conditioning latent code decoder network maps back reconstruct original sequence given observed sentences parameters model learned maximizing marginal since intractable cases variational lower bound typically employed objective maximized denote decoder encoder parameters respectively. lower bound lvae maximized intuitively w.r.t. encoder decoder parameters. model aims minimize reconstruction error well regularize posterior distribution diverge much prior neural variational inference framework achieved signiﬁcant success types data images challenges framework text extracting sentence features text framework shown difﬁcult unsupervised latentvariable model often referred variational autoencoder parameters optimized minimizing reconstruction error sentences well regularizing posterior distribution close prior dkl|p). therefore think variational autoencoder regularized version standard autoencoder additional penalty term coming divergence loss. although divergence term plays role training latent-variable models framework reported that applied text data loss tends insigniﬁcantly small during training result encoder matches gaussian prior regardless input decoder doesn’t take advantage information latent variable moreover reported poor results setting attributed autoregressive nature lstm decoder decoding lstm imposes strong conditional dependencies consecutive words thus information becomes less impactful learning. motivated issues employed dilated cnns instead lstm sentence decoder latent-variable model. latent variable able encode semantic information smaller contextual capacity dilated decoder. however optimization challenges remain ground-truth words employed training dilated autoregressive decoder. consequently inferred latent codes cannot figure diagram deconvolutional sequence decoder comparing lstm sequence decoder. notably contrast lstm decoder ground truth words provided deconvolutional networks training. result failure mode optimization described divergence term vanishingly small largely mitigated. deconvolutional sequence decoder deconvolutional networks also known transposed convolutional layers typically used deep learning models up-sample ﬁxed-length latent representations highlevel feature maps although widely adopted image generative models deconvolutional networks rarely applied generative models text. understand form decoder needed text ﬁrst consider associated convolutional encoder text represented matrix width dictated sentence length height dictated dimensionality word embeddings. convolutional ﬁlters layer model one-dimensional convolution ﬁlters sentence embedding matrix signals manifested. using feature maps similar process repeats substantiate subsequent layers deep model. hence layer model signals manifested convolutions ﬁlters feature-map layer encoder discussed starts bottom sentence-embedding matrix works upward latent code decoder works downward starting arriving sentence-embedding matrix. speciﬁcally decoder network takes input sampled inference network llayer decoder model feature maps layer manifested ﬁlter matrices rhl×m .... corref sponds number components temporal dimension. matrix multiplied column vector yielding feature maps. yields feature-map matrix layer yield layer feature matrix process repeats using ﬁlters rhl−×kl .... convolutions performed feature-map matrix layer yields feature-map matrix layer followed relu nonlinearity. process continues sequentially arrive bottom decoder network yielding ﬁnal matrix sentence-embedding matrix approximated. explicit fig. represent featuremap matrices top-two layers three-layer model. represent matrix recovered bottom layer network process height corresponding dimension word-embedding. suppose word-embedding matrix vocabulary word reconstructed sentence. compute probability word cosine similarity vectors vocabulary contains possible words represents column corresponding word i-th column up-sampled representation parameter controls sparsity resulting probabilities denote temperature parameter. experiments. multilayer coarse-to-ﬁne process implied repeatedly applying decoder process illustrated figure advantages reﬂects natural hierarchical tree structure sentences thus better represent syntactic features useful reconstructing sentences; deconvolutional network allows efﬁcient parallelization generating fragment sentence thus considerably faster lstm decoder. shown figure training procedures deconvolutional lstm decoders intrinsically different. latter ground-truth words previous time steps provided training network. contrast deconvolutional network generates entire sentence alone. distinction lstm decoder autoregressive model powerful recurrence tends explain structure data little insight latent variables provide information beginning sentence thus acting merely prior. deconvolutional latent-variable models section incorporate deconvolutional sequence decoder described previous section latentvariable model text. coarse-to-ﬁne generation process described above model partial access observed data generation process lstm thus latent-variable model must learn encode much information possible input alone. moreover learned latent code truly viewed global feature representation sentences since contains essential information generate text sequence. following describe proposed deconvolutional latent-variable models context unsupervised supervised learning. unsupervised sequence learning demonstrate effectiveness proposed model explore training unsupervised manner. speciﬁcally input sentence latent code inferred encoder network implemented denotes transformation function encoder accomplished learning input parameters represents hadamard vector product. posterior mean variance generated non-linear transformations parameterized neural networks; input parameters note sampled reparameterization trick facilitate model training. sampled deconvolutional sequence decoder described above reconstruct corresponding input sentences. model trained optimizing variational lower bound without discriminative information. supervised sequence matching apply latentvariable model text sequence-matching problems employing discriminative information encoded latent code sentence pair latent code sequence inferred parameters encoder network premise hypothesis respectively shared. decoded shared-weight deconvolution networks recover corresponding input sentence. infer label latent features sampled inference network processed matching layer combine information sentences. matching layer deﬁned heuristic matching layer speciﬁed experimental setup deconvolutional latent-variable model trained unsupervised supervised semi-supervised manner. section ﬁrst train model unsupervised mixed corpus scientiﬁc informal writing styles evaluate sentence embeddings checking whether automatically distinguish different sentence characteristics i.e. writing styles. further apply models standard text sequence matching tasks recognizing textual entailment paraphrase identiﬁcation semi-supervised setting. summary statistics datasets presented table simplicity denote deconvolutional latentvariable model deconv-lvm experiments. facilitate comparison prior work several baseline models implemented basic siamese model cnns encoder sentences sharing conﬁgurations weights; auto-encoder sequence encoder deconv decoder; latent-variable model using inference network generative network implemented lstm -layer convolutional neural networks inference/encoder network order extract hierarchical representation sentences speciﬁcally layers ﬁlter window size stride feature maps layers respectively. latentvariable models -dimension feature vector mlps infer mean variance latent variable generative/decoder network implemented -layer deconvolutional networks decode samples latent variable size model trained using adam learning rate parameters. dropout employed word embedding latent variable layers rates selected validation set. mini-batch size semi-supervised sequence matching experiments norm weight vectors employed regularization term loss function coefﬁcient loss treated hyperparameter tuned validation set. experiments implemented tensorﬂow using nvidia geforce titan memory. unsupervised sentence embedding investigate effectiveness latent-variable model ﬁrst train unsupervised manner using dataset sentences corpora figure deconvolutional latent-variable model text sequence matching. reconstruction/generation discriminative objectives jointly optimized learn robust latent codes sentences. refers parameters classiﬁer controls relative weight generative loss lvae sequence matching loss lmatch deﬁned cross-entropy loss. implementing model anneal value training latent variable learned gradually focus less reconstruction objective retaining features useful sequence matching i.e. minimizing second term. extension semi-supervised learning latentvariable model readily extended semi-supervised scenario subset sequence pairs corresponding class labels. suppose empirical distributions labeled unlabeled data referred respectively. loss function unlabeled data expressed minimize ljoint w.r.t. employ monte carlo integration approximate expectations case unlabeled data leveraged objective standard lower bound. training parameters jointly updated stochastic gradient descent bookcorpus dataset arxiv dataset merged together equal proportion. motivation check whether latent codes learned model automatically distinguish different writing styles i.e. sentences scientiﬁc informal styles represented bookcorpus arxiv dataset respectively. experiment model trained optimizing variational lower bound without label/discriminative information provided. compare model another latent-variable model using lstm decoder especially highlight contribution deconvolutional network overall setup. ensure fair comparison employ model architecture lstm-based latent-variable model except decoder utilized. lstm hidden-state dimension latent variable decoder input every time step. models converge randomly sample sentences test -dimensional latent embeddings vector using t-sne embedding plots deconv-lvm lstm-lvm shown figure cases plot shape sampled latent embeddings close circle means posterior distribution matches gaussian prior well. importantly deconvolutional networks decoder disentangled latent codes writing styles clearly observed majority prior space. indicates semantic meanings sentence encoded latent variable even train model unsupervised manner. contrary latent codes lstm-lvm inferred different writing styles tend other cannot separated easily case deconv-lvm suggesting less information encoded embeddings. better understand advantages deconvolutional networks decoder latent-variable models perform quantitative comparison latent codes deconv-lvm lstm-lvm. table show number parameters training time iterations percentage loss total loss models. moreover extract sentence features model train linear classiﬁer distinguish between scientiﬁc informal writing styles. sentence embeddings ﬁxed training order elucidate quality latent codes learned unsupervised manner. sentences sampled training learn classiﬁer classiﬁcation accuracy calculated whole test set. deconv-lvm performs better lstm-lvm indicating latent codes deconv-lvm informative. observation corresponds well fact percentage loss deconv-lvm much larger lstm-lvm larger divergence loss considered sign useful information encoded latent variable further observe deconv-lvm relatively parameters compared lstm-lvm making promising latent-variable model text. recognizing textual entailment motivated superior performance deconvolutional latent-variable model unsupervised learning apply semisupervised scenario. consider task recognizing text entailment stanford natural language inference dataset check generalization ability latent variable learned experimented different amounts labeled training data results shown figure compared lstm baseline models basic implementation autoencoder latent-variable models make unlabeled data achieve better results simply train encoder network i.e. lstm labeled data. importantly deconv-lvm propose outperforms lstm-lvm cases consistent previous observations latent variable deconv-lvm tends informative. note using labeled data training deconv-ae performs better deconv-lvm surprising since deconv-lvm introduces constraint latent features learned optimal labeled data available training. directly compare semisupervised learning experiments follow experiment setup labeled examples used training. according table turns deconvae model competitive baseline outperform lstm-ae results. moreover deconv-lvm achieves even better results deconv-ae lstm-lvm suggesting deconvolution-based latent-variable model propose makes effective unsupervised information. further tends larger number labeled data smaller demonstrating deconv-lvm promising strategy extract useful information unlabeled data. paraphrase identiﬁcation investigate deconvolutional latent-variable model paraphrase identiﬁcation task quora question pairs dataset following dataset split consider cases labeled examples used training. illustrated table encoder glove pre-trained word embeddings consistently outperforms randomly initialized word embeddings autoencoder model achieves better results training encoder corresponding ﬁndings importantly latent-variable models show even higher accuracy autoencoder models demonstrating effectively utilize information unlabeled data represent effective strategy paraphrase identiﬁcation task. deconv-lvm performs better lstm-lvm cases indicating deconvolutional decoder leverage beneﬁts latentvariable model. however also trend larger number labeled data gaps models smaller. attributed fact lots labeled data available discriminative information tends dominant factor better performance information unlabeled data becomes less important. proposed framework closely related recent research incorporating text modeling presented ﬁrst attempt utilize language modeling results using lstm decoder largely negative. applied framework unsupervised bags-of-words model. however perspective text representation learning model ignores word-order information suboptimal downstream supervised tasks. employed variational autoencoder lstm-lstm architecture semi-supervised sentence classiﬁcation. however illustrated experiments well lstm decoder effective choice learning informative discriminative sentence embeddings. framework also employed textgeneration problems machine translation dialogue generation motivation improve diversity controllability generated sentences. work distinguished prior research principal respects leveraged framework latent variable models text sequence matching tasks ability take advantage unlabeled data learn robust sentence embeddings; employed deconvolutional networks instead lstm decoder network. demonstrated effectiveness framework unsupervised supervised learning cases. presented latent variable model matching natural language sentences deconvolutional networks sequence encoder. show jointly optimizing variational lower bound matching loss model effective inferring robust sentence representations determining semantic relationship even limited amount labeled data. state-of-the-art experimental results semi-supervised sequence matching tasks achieved demonstrating advantages approach. work provides promising strategy towards training effective fast latent-variable models text data. acknowledgements research supported part darpa onr.", "year": 2017}