{"title": "Latent Predictor Networks for Code Generation", "tag": ["cs.CL", "cs.NE"], "abstract": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.", "text": "dictor copy foundation input another answer issac asimov searching database. however training multiple predictors challenging task annotation exists regarding predictor used generate output token. furthermore predictors generate segments different granularity database queries generate multiple tokens word softmax generates single token. work introduce latent predictor networks novel neural architecture fulﬁlls desiderata core architecture exact computation marginal likelihood latent predictors generated segments allowing scalable training. introduce corpus automatic generation code cards trading card games validate model tcgs magic gathering hearthstone games played players build decks ever expanding pool cards. examples cards shown figure card identiﬁed attributes many language generation tasks require production text conditioned structured unstructured inputs. present novel neural network architecture generates output sequence conditioned arbitrary number input functions. crucially approach allows choice conditioning context granularity generation example characters tokens marginalised thus permitting scalable effective training. using framework address problem generating programming code mixed natural language structured speciﬁcation. create data sets paradigm derived collectible trading card games magic gathering hearthstone. these third preexisting corpus demonstrate marginalising multiple predictors allows model outperform strong benchmarks. generation natural formal languages often requires models conditioned diverse predictors models take restrictive approach employing single predictor word softmax predict tokens output sequence. illustrate limitation suppose wish generate answer question wrote foundation? foundation written isaac asimov. generation words issac asimov foundation word softmax trained annotated data unlikely succeed words sparse. robust model might example employ effect described text box. digital implementations games implement game logic includes card effects. attractive data extraction perspective data annotations naturally generated also view card speciﬁcation communicated designer software engineer. dataset presents additional challenges prior work code generation including handling structured input—i.e. cards composed multiple sequences —and attributes length generated sequences. thus propose extension attention-based neural models attend structured inputs. finally propose code compression method reduce size code without impacting quality predictions. paper structured follows ﬁrst describe data collection process formally deﬁne problem baseline method then propose extensions namely structured attention mechanism architecture follow description code compression algorithm model validated comparing multiple benchmarks finally contextualize ﬁndings related work present conclusions work obtain data open source implementations different tcgs java python. statistics corpora illustrated table corpora card implemented separate class strip imports comments. categorize content card different groups singular ﬁelds contain value; text ﬁelds contain multiple words representing different units meaning. singular ﬁelds four text ﬁelds whereas cards eight singular ﬁelds text ﬁelds text ﬁelds tokenized splitting whitespace punctuation exceptions accounting domain speciﬁc artifacts empty ﬁelds replaced token. code card figure shown figure effect drawing cards player many cards opponent implemented computing difference players’ hands invoking draw method number times. illustrates mapping description code nonlinear information given text regarding speciﬁcs implementation. structured attention background attention model bahdanau applies. following chain rule t=..|y| token predicted conditioned previously generated sequence y..yt− input sequence x..x|x|. probability estimated softmax vocabulary recurrent neural network state time stamp modeled recurrent update function generating state based previous token previous state input text representation implement using long short-term memory rnns attention mechanism generates representation input sequence x..x|x| computed weighted i=..|x| attention coefﬁcient obtained token function maps continuous vector. general function projects learning lookup table embedding contextual words deﬁning rnn. coefﬁcients computed softmax input tokens x..x|x| character level context-aware representation built words text ﬁelds using bidirectional lstm computing attention multiple input ﬁelds problematic input ﬁeld’s vectors different sizes value ranges. thus learn linear projection mapping input token vector common dimensionality value range denoting process extend equation function computes afﬁnity token current output context ht−. common implementation apply linear projection ﬁxed size vector followed tanh another linear projection. approach extend computation cases corresponds multiple ﬁelds. figure illustrates card serra angel encoded assuming singular ﬁelds text ﬁeld. ﬁrst encode token using model described ling replacement lookup tables word representations learned background order decode many words must copied code name card attack cost values. observe card figure respective code figure observe name divine favor must copied class name constructor along cost card explained earlier problem speciﬁc task instance dataset model must learn timeout convert timeout integer. name variable timeout must copied output sequence. issue exists proper nouns machine translation typically copied language other. pointer networks address deﬁning probability distribution units copied c..c|c|. probability copying unit modeled row). applies generation attack health cost values predictors element thus deﬁne objective function marginal likelihood function latent variable approach combining pointer networks character-based softmax difﬁcult generate segments different granularity ground truth predictor time stamp. describe latent predictor networks model conditional probability latent sequence predictors used generate assume model uses multiple predictors generate multiple segments yt..yt+|st|− arbitrary length |st| time stamp example illustrated figure observe generate code init pointer network used generate sequences =tirion =fordring sequences also generated using character softmax generprobability probability predictor computed using softmax predictors conditioned previous state input representation then probability generating segment depends predictor type. deﬁne three types predictors copy singular field singular ﬁelds ﬁeld copied instance value attack cost attributes type card. size generated segment number characters copied ﬁeld segment generated probability copy text field text ﬁelds allow words within ﬁeld copied. probability copying word learned pointer network representation word concatenation state input vectors. predictor generates segment size copied word. important note state vector generated building sequence characters time stamp i.e. previous context encoded character level. allows number possible states remain tractable training time. training time back-propagation maximize probability observed code according equation gradient computation must performed respect computed probability derivative denotes cumulative probability values time stamp α|y|+ yields marginal probability βtrt βt+|st|− denotes cumulative probability starting predictor time stamp exclusive. includes probability generated segment probability values starting timestamp |st|− possible sequences generate segment segment produced. completeness denotes cumulative probabilities include illustrate this refer figure consider timestamp segment =fordring generated. case cumulative probability path generates sequence init. finally refers path generates fordring character character. number possible paths grows exponentially computed efﬁciently using forward-backward algorithm semimarkov models associate edges nodes markov chain. intuitive interpretation derivatives gradient updates stronger probability chains likely generate output sequence. instance model learns good predictor copy names fordring predictors also generate sequences character softmax allocate less capacity generation names focus elements excel decoding decoding performed using stack-based decoder beam search. state corresponds choice predictor segment given time stamp state scored prev denotes predecessor state time stamp states highest scores expanded size beam. predictor output generates state. finally timestamp states attention-based model traverses input units generation step generation becomes quite expensive datasets average card code contains characters. essential contribution paper propose simple method compress code maintaining structure code allowing train datasets longer code idea behind method many keywords programming language well frequently used functions classes learned without character level information. exploit mapping strings onto additional symbols formally seek string among strings length maximally reduces size corpus number occurrences training corpus length. seen number characters reduced replacing nonterminal symbol. efﬁciently leverage fact contains follows means maximum compression obtainable size always lower thus current size achieves better compression rate maximum length follows sequences contain discarded candidates. based idea iterative search starts obtaining counts segments size computing best scoring segment then build list segments achieve better compression rate maximum size. size segments contain element need considered making number substrings tested tractable increases. algorithm stops reaches newly generated list contains elements. obtained replace occurrences non-terminal symbol. process repeated desired average size code reached. training performed compressed code decoding undergo additional step compressed code restored expanding table shows ﬁrst replacements dataset reducing average size datasets tests performed datasets provided paper described table additionally test model’s ability generalize domains report results django dataset comprising training development test annotations. data point consists line python code together manually created natural language description. neural benchmarks implement standard neural networks namely sequence-tosequence model attention-based model former adapted work multiple input ﬁelds concatenating them latter uses proposed attention model. models denoted sequence attention. machine translation baselines problem also viewed framework semantic parsing unfortunately approaches deﬁne strong assumptions regarding grammar structure output makes difﬁcult generalize domains however work andreas provides evidence using machine translation systems without committing assumptions lead results competitive systems described above. follow approach create phrase-based model hierarchical model benchmarks work presented here. models optimized generate words characters implement tokenizer splits punctuation characters except character. also facilitate task splitting camelcase words otherwise class names would generated correctly methods. used models implemented moses generate baselines using standard parameters using alignment model word alignments mert tuning -gram kneser-ney smoothed language model models denoted phrase hierarchical respectively. retrieval baseline reported simple retrieval method outputs similar input sample measured using levenshtein distance leads good results. implement baseline computing average levenshtein distance input ﬁeld. baseline denoted retrieval. evaluation typical metric compute accuracy whether generated code exactly matches reference code. informative gives intuition many samples used without human post-editing. however provide illustration degree closeness achieving correct code. thus also test using bleu- token level. clearly problems metrics. instance source code correct without matching reference. code figure could also implemented calling draw function cycle exists players number cards hands. tasks generation queries overcome problem executing query checking result annotation. however shall leave study methologies future work adapting methods tasks trivsetup multiple input types hyper-parametrized follows model used obtain continuous vectors word types uses character embeddings size lstm states size generates vectors size also report results using word lookup tables size replace singletons special unknown token probability training used out-of-vocabulary words. text ﬁelds context encoded bi-lstm size forward backward states. finally linear layer maps different input tokens common space size attention model used hidden layer size applying non-linearity decoder encode output characters size row) lstm state size input representation size row). pointer network intersection input units state units performed vector size training performed using mini-batches samples using adadelta report results using iteration highest bleu score validation decoding performed beam compression performed grid search compressing code original average length intervals django datasets. dataset forced compress code performance issues training extremely long sequences. results baseline comparison results reported table regarding retrieval results observe best bleu scores among baselines card datasets advantage method retrieving existing entities guarantees output well formed syntactic errors producing non-existent function call generating incomplete code. bleu penalizes length mismatches generating code matches length reference provides large boost. phrase-based translation model performs well django mapping input output mostly monotonic hierarchical model yields better performance card datasets concatenation input ﬁelds needs reordered extensively output sequence. finally sequence-to-sequence model yields extremely results mainly lack capacity needed memorize whole input output sequences attention based model produces results phrase-based systems. finally observe including proposed components obtain signiﬁcant improvements baselines three datasets obtains non-zero accuracies card datasets. component comparison present ablation results order analyze contribution modiﬁcations. removing model yields small deterioration word lookup tables susceptible sparsity. exception dataset lookup tables perform better. believe small size training provide enough evidence character model scale unknown words. surprisingly running model compression code actually yields better results. table provides illustration results different compression rates. obtain best results compression rate maximising time card processed reason uncertain similar ﬁnding language models output characters tend under-perform output words applies using regular optimization process character softmax also using also note training speed lpns signiﬁcantly lower marginalization performed dynamic program. finally signiﬁcant decrease observed remove pointer networks improvements also generalize sequence-to-sequence models scores superior sequence-tosequence benchmark result analysis examples code generated cards illustrated figure obtain segments copied pointer networks computing likely predictor segments. observe marked segments model effectively copies attributes match output including name card must collapsed. expected majority errors originate inaccuracies generation effect card. encouraging observe small percentage cards generated correctly worth mentioning result many cards possessing similar effects. madder bomber card generated correctly similar card bomber training implements effect except deals damage instead promising result model able capture difference. however many cases effects radically differ seen ones tend generated incorrectly. card preparation observe properties card generated correctly effect implements unrelated exception value correctly copied. interestingly still generates valid effect sets minion’s attack investigating better methods accurately generate effects object studies. target widely used programming languages namely java python work related studies generation executable code. include generating regular expressions code parsing input documents much research also invested generating formal languages database queries agent speciﬁc language smart phone instructions finally mapping natural language sequence actions generation executable code finally considerable effort task focused semantic parsing recently proposed models focus combinatory categorical grammars bayesian tree transducers probabilistic context free grammars work natural language programming users write lines code natural language also related work. finally reverse mapping code natural language explored character-based sequence-to-sequence models previously used generate code natural language inspired works lpns provide richer framework employing attention models pointer networks character-based embeddings formulation also seen generalization allamanis implement special case predictors granularity finally hmms employed neural models marginalize label sequences modeling transitions labels. introduced neural network architecture named latent prediction network allows efﬁcient marginalization multiple predictors. architecture propose generative model code generation combines character level softmax generate language-speciﬁc tokens multiple pointer networks copy keywords input. along extensions namely structured attention code compression model applied existing datasets also newly created implementations game cards. experiments show model out-performs multiple benchmarks demonstrate importance combining different types predictors.", "year": 2016}