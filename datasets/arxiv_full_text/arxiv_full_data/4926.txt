{"title": "A Laplacian Framework for Option Discovery in Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Representation learning and option discovery are two of the biggest challenges in reinforcement learning (RL). Proto-value functions (PVFs) are a well-known approach for representation learning in MDPs. In this paper we address the option discovery problem by showing how PVFs implicitly define options. We do it by introducing eigenpurposes, intrinsic reward functions derived from the learned representations. The options discovered from eigenpurposes traverse the principal directions of the state space. They are useful for multiple tasks because they are discovered without taking the environment's rewards into consideration. Moreover, different options act at different time scales, making them helpful for exploration. We demonstrate features of eigenpurposes in traditional tabular domains as well as in Atari 2600 games.", "text": "representation learning option discovery biggest challenges reinforcement learning proto-value functions well-known approach representation learning mdps. paper address option discovery problem showing pvfs implicitly deﬁne options. introducing eigenpurposes intrinsic reward functions derived learned representations. options discovered eigenpurposes traverse principal directions state space. useful multiple tasks discovered without taking environment’s rewards consideration. moreover different options different time scales making helpful exploration. demonstrate features eigenpurposes traditional tabular domains well atari games. important challenges reinforcement learning problems representation learning automatic discovery skills. proto-value functions well-known solution problem representation learning problem skill discovery generally posed options framework models skills options. paper together representation learning option discovery showing pvfs implicitly deﬁne options. main contributions introduce concepts eigenpurpose eigenbehavior. eigenpurposes intrinsic reward functions incentivize agent traverse state space following principal directions learned representation. intrinsic reward function leads different eigenbehavior optimal policy reward function. paper introduce algorithm option discovery leverages ideas. options discover task-independent because pvfs eigenpurposes obtained without information environment’s reward structure. ﬁrst present ideas tabular case show generalized function approximation case. exploration traditionally separate problem option discovery also addressed careful construction options paper provide evidence options capable accelerating planning useful exploration. show options traditionally used literature speed planning hinder agents’ performance used random exploration learning. options important properties allow improve exploration operate different time scales easily sequenced. options operate different time scales allows agents make ﬁnely timed actions also decreasing likelihood agent explore small portion state space. moreover because options deﬁned across whole state space multiple options available every state allows easily sequenced. framework agent aims maximize cumulative reward taking actions environment. actions affect agent’s next state rewards experiences. formalism throughout paper. -tuple time agent state takes action leads next state according transition probability kernel encodes pr|st agent also observes reward agent’s goal learn policy maximizes expected commonly deﬁned combinatorial graph laplacian matrix graph’s adjacency matrix diagonal matrix whose entries sums notice adjacency matrix easily generalizes weight matrix pvfs deﬁned eigenvectors obtained eigendecomposition different diffusion models used generate pvfs normalized graph laplacian pvfs capture large-scale geometry environment symmetries bottlenecks. task independent sense information related reward functions. moreover deﬁned whole state space since eigenvector induces realvalued mapping state. imagine options properties also useful. section show pvfs discover options. start example. consider traditional room domain depicted figure gray squares represent walls white squares represent accessible states. four actions available down right left. transitions deterministic agent allowed move wall. ideally would like discover options move agent room room. thus able automatically distinguish different rooms environment. exactly pvfs depicted figure instead interpreting basis function interpret example desire reach highest point plot corresponding centre room. sign eigenvector arbitrary also interpreted desire reach lowest point plot corresponding opposite room. paper eigenvectors directions eigenpurpose formalizes interpretation deﬁning intrinsic reward function. deﬁning purpose agent maximize discounted rewards. deﬁnition eigenpurpose proto-value function trinsic reward function r|s| discounted return discount factor. common policy improvement theorem learning maximize technique alternate solving bellman equations action-value function sometimes feasible learn value stateaction pair size state space. generally addressed parameterizing weights common approximate linear function i.e. denotes linear feature representation state taking action options framework extends introducing temporally extended actions called skills options. option -tuple denotes option’s initiation denotes option’s policy denotes option’s termination set. after agent decides follow option state actions selected according agent reaches state intuitively options higher-level actions extend several time steps generalizing mdps semimarkov decision processes traditionally options capable moving agents bottleneck states sought after. bottleneck states states connect different densely connected regions state space shown efﬁcient planning states states frequently visited considering shortest distance between states proto-value functions learned representations capture large-scale temporal properties environment obtained diagonalizing diffusion model constructed mdp’s transition matrix. diffusion model captures information graph dition deﬁning terminate action selected control returned higher level policy option following polχ terminates deﬁne initiation states exists action thus option’s refer policy maxa∈a∪{⊥} options discovered approach eigenoptions. eigenoption corresponding example beginning section depicted figure reward function deﬁned action augmented action terminate allows agent leave without cost. state space transition probability kernel remain unchanged original problem. discount rate chosen arbitrarily although impacts timescale option encodes. policy expected value cumulative discounted intrinsic reward agent starts state follows policy termination. similarly deﬁne action-value function expected value cumulative discounted intrinsic reward agent starts state takes action follows policy termination. also describe optimal value function eigenpurpose obtained finding optimal policy becomes traditional problem different reward function. importantly reward function tends dense avoiding challenging situations exploration issues. paper policy iteration solve optimal policy. eigenpurpose deﬁnes option corresponding eigenbehavior option’s policy. thus need deﬁne option’s initiation termination set. option available every state possible achieve purpose terminate achieved. deﬁning learn option augmented agent’s action terminate action allowing agent interrupt option anytime. want options terminate agent achieves purpose i.e. unable accumulate positive intrinsic rewards. deﬁned reward function happens agent reaches state largest value eigenpurpose subsequent reward negative. able formalize conused three mdps empirical study open room i-maze -room domain. transitions deterministic gray squares denote walls. agents access four actions down right left. action would taken agent wall chosen agent’s state change. demonstrate three aspects framework theory smoothest eigenvectors corresponding smallest eigenvalues preferred intuition applies eigenoptions eigenpurposes corresponding smallest eigenvalues preferred. figures depict ﬁrst eigenoptions discovered three domains used evaluation. allowing apply algorithm many environments obvious meaningful bottlenecks. discover meaningful options environments walking corridor going corners open room. interestingly doorways ﬁrst options discover -room domain next sections provide empirical evidence eigenoptions useful often bottleneck options. major challenge agents explore environment decisive avoiding dithering commonly observed random walks options provide decisiveness operating higher level abstraction. agents performing random walk equipped options expected cover larger distances state space navigating back forth subgoals instead dithering around starting state. however options need satisfy conditions improve exploration available several parts state space ensuring agent always access many different options; operate different time scales. instance -room domain unlikely agent randomly selects enough primitive actions leading corner options move agent doorways. important result section show unlikely agent explore whole environment keeps going back forth similar high-level goals. eigenoptions satisfy conditions. demonstrated section eigenoptions often deﬁned whole state space allowing sequencing. moreover pvfs seen frequency basis different pvfs associated different frequencies corresponding eigenoptions also operate different frequencies length trajectory termination varying. behavior seen comparing second fourth eigenoptions grid fourth eigenoption terminates expectation twice often second eigenoption. agents spend time. options much longer primitive actions reduce likelihood agent deviate much options’ trajectories since sampling option undo dozens primitive actions. biasing often observed fewer options available. section show eigenoptions improve exploration. introducing metric call diffusion time. diffusion time encodes expected number steps required navigate states randomly chosen following random walk. small expected number steps implies likely agent reach states random walk. discuss metric computed appendix. figure depicts three environments diffusion time options diffusion time using primitive actions. options incrementally order increasing eigenvalue computing diffusion time different sets options. ﬁrst options added hurt exploration enough options added exploration greatly improved compared random walk using primitive actions. fact options hurt exploration surprising ﬁrst based fact useful options generally sought literature. however major difference using options planning learning. planning options shortcut agents’ trajectories pruning search space. actions still taken consideration. exploring uniformly random policy options primitive actions skews discussion made clearer example. -room domain options available leading agent doorways less likely agent reach outer corners. agent would select enough consecutive primitive actions without sampling option. also likely agents always moving rooms never really exploring inside room. issues mitigated eigenoptions. ﬁrst eigenoptions lead agents individual rooms eigenoptions operate different time scales allowing agents explore different parts rooms. figure supports intuition options leading bottleneck states sufﬁcient themselves exploration. shows diffusion time -room domain increased bottleneck options used. literature ideal number options used agent seen model selection problem. illustrate usefulness options agent’s goal accumulate reward. also study impact increasing number options task. experiments agent starts bottom left corfigure agents’ performance accumulating reward options added action behavior policy. results eigenpurposes directly obtained eigendecomposition well negation. goal reach right corner. agent observes reward goal reached observes reward used q-learning learn policy primitive actions. behavior policy chooses uniformly primitive actions options following termination. figure depicts learning given number episodes average trials agents’ ﬁnal performance. episodes time steps long learned episodes grid i-maze episodes -room domain. scenarios eigenoptions improve performance. previous section exceptions occur options added agent’s action set. best results obtained using options. despite additional parameter results show agent’s performance fairly robust across different numbers options. eigenoptions task-independent construction. additional results appendix show eigenoptions able speed-up learning different tasks. appendix also compare eigenoptions random options options random state subgoal. assumed agents access adjacency matrix representing underlying mdp. however practical settings generally true. fact number states settings often large agents rarely visit state twice. problems generally tackled sample-based methods sort function approximation. section propose sample-based approach option discovery asymptotically discovers eigenoptions. extend algorithm linear function approximation. provide anecdotal evidence atari games relatively na¨ıve sample-based approach function approximation discovers purposeful options. online setting agents must sample trajectories. naturally sample trajectories able perfectly construct mdp’s adjacency matrix suggested mahadevan maggioni however approach easily extend linear function approximation. section provide approach build adjacency matrix allowing extend concept eigenpurposes linear function approximation. algorithm sample transition added matrix previously encountered. transition added difference current previous observations i.e. tabular case deﬁne one-hot encoding state enough transitions sampled perform singular value decomposition matrix columns correspond right-eigenvectors generate eigenpurposes. intrinsic reward termination criterion eigenbehavior before. matrix known incidence matrix. transitions graph sampled once tabular representations algorithm discovers options obtain combinatorial laplacian. theorem states equivalence obtained eigenpurposes. theorem consider consisting difference observations i.e. tabular case transitions sampled once orthonormal eigenvectors columns trade-off reconstructing adjacency matrix constructing incidence matrix. mdps states sparsely connected i-maze latter preferred since fewer transitions states. however makes result interesting fact algorithm easily generalized linear function approximation. adjacency matrix useful agent access features state. however intuition incidence matrix propose algorithm compatible linear function approximation. fact apply algorithm proposed previous section need deﬁne constitutes transition. deﬁne vectors identical data structure avoid duplicates storing na¨ıve approach provides encouraging evidence eigenoptions generalize linear function approximation. expect involved methods perform even better. tested method agent’s representation consists emulator’s state ﬁnal incidence matrix rows sampled uniformly observed transitions. provide details experimental setup appendix. tabular case start selecting eigenpurposes generated eigenvectors smallest eigenvalue smoothest ones. however clear intuition holds function approximation setting matrix transitions contain possible transitions. therefore analyzed game discovered options. able obtain options clearly demonstrate intent. freeway game chicken expected cross road avoiding cars observe options agent clearly wants reach speciﬁc lane street. figure depicts chicken tends option executed. right histogram representing chicken’s height episode. clearly chicken’s height varies different options random walk primitive actions explore environment properly. remarkably option scores points episode without ever explicitly taking reward signal consideration. performance close obtained state-of-the-art algorithms. montezuma’s revenge game agent needs navigate room pickup open door also observe agent clear intent reaching particular positions screen staircases ropes doors interestingly options discover similar handcrafted kulkarni evaluating usefulness options tackle game. video highlighted options found online. algorithms option discovery seen topapproaches. agents trajectories leading informative rewards starting point decomposing reﬁning options. many approaches based principle methods observed rewards generate intrinsic rewards leading value functions methods observed rewards climb gradient https//youtu.be/bvicxcdwa deﬁne informative reward signal informs agent reached goal. example trying escape maze consider informative reward agent observes rewards value every time step inside maze. different example positive reward observed agent typically observes rewards value probabilistic inference however approaches applicable large state spaces sparse rewards. informative rewards unlikely found agent using primitive actions requiring long speciﬁc sequences actions options equally unlikely discovered. algorithm seen bottom-up approach options constructed agent observes informative reward. options composed generate desired policy. options discovered tend independent agent’s intention potentially useful many different tasks options also seen useful exploration allowing agents commit behavior extended period time among approaches discover options without using extrinsic rewards global local graph centrality measures clustering states interestingly s¸ims¸ek lakshminarayanan also graph laplacian algorithm identify bottleneck states. baranes oudeyer moulin-frier oudeyer show build policies explicitly assist agents explore environment. proposed algorithms self-generate subgoals order maximize learning progress. policies built seen options. recently solway proved optimal hierarchy minimizes geometric mean number trial-and-error attempts necessary agent discover optimal policy selected task experiments conﬁrm result although propose diffusion time different metric evaluate options improve exploration. idea discovering options learning control parts environment also related work. eigenpurposes encode different rates change agents representation world corresponding options maximizing change. others also proposed ways discover options based idea learning control environment. hengst instance proposes algorithm explicitly models changes variables form agent’s representation. recently gregor proposed algorithm agents discover options maximizing notion empowerment agent aims getting states maximal available intrinsic options. continual curiosity driven skill acquisition closest approach ours. ccsa also discovers skills maximize intrinsic reward obtained extracted representation. pvfs ccsa uses incremental slow feature analysis deﬁne intrinsic reward function. sprekeler shown that given speciﬁc choice adjacency function pvfs equivalent becomes approximation pvfs function space used allow arbitrary mappings observed data embedding. method differs deﬁne initiation termination sets well objective maximized. ccsa acquires skills produce large variation slow-feature outputs leading options seek bottlenecks. approach seek bottlenecks focusing traversing different directions learned representation. able properly abstract mdps smdps reduce overall expense learning mainly learned options reused multiple tasks. hand wrong hierarchy hinder agents’ learning process moving agent away desired goal states. current algorithms option discovery often depend initial informative reward signal readily available large mdps. paper introduced approach effective different environments multitude tasks. algorithm uses graph laplacian directly related concept proto-value functions. learned representation informs agent meaningful options sought after. discovered options seen traversing dimensions learned representation. believe successful algorithms future able simultaneously discover representations options. agents learned representation discover options used explore environment improving agent’s representation. interestingly options ﬁrst discovered approach necessarily bottlenecks commonly sought after. paper showed bottleneck options hinder exploration strategies naively added agent’s action options discover help agent explore. also shown discovered options used accumulate reward multitude tasks leveraging exploratory properties. several exciting avenues future work. noted seen approximation pvfs. would interesting compare approach eigenoptions. would also interesting options discover generated incrementally incomplete graphs. finally also imagine extensions proposed algorithm hierarchy options built. authors would like thank dabney r´emi munos csaba szepesv´ari useful discussions. work supported grants alberta innovates technology futures alberta machine intelligence institute computing resources provided compute canada calculqu´ebec. references bacon pierre-luc. bottleneck concept options discovery theoretical underpinnings extension continuous state spaces. master’s thesis mcgill university baranes adrien oudeyer pierre-yves. active learning inverse models intrinsically motivated goal exploration robots. robotics autonomous systems bellemare marc naddaf yavar veness joel bowling michael. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research ¸ims¸ek ¨ozg¨ur barto andrew using relative novelty identify useful temporal abstractions reinforcement learning. proceedings international conference machine learning ¸ims¸ek ¨ozg¨ur barto andrew skill characterization based betweenness. proceedings advances neural information processing systems ¸ims¸ek ¨ozg¨ur wolfe alicia barto andrew identifying useful subgoals reinforcement learning local graph partitioning. proceedings international conference machine learning kompella varun luciw matthew schmidhuber j¨urgen. incremental slow feature analysis. proceedings international joint conference artiﬁcial intelligence kompella varun stollenga marijn luciw matthew schmidhuber juergen. continual curiosity-driven skill acquisition high-dimensional video inputs humanoid robots. artiﬁcial intelligence press. issn available online february konidaris george barto andrew. skill discovery continuous reinforcement learning domains using proceedings advances neural skill chaining. information processing systems kulkarni tejas narasimhan karthik saeedi ardavan tenenbaum joshua hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. arxiv e-prints lakshminarayanan aravind krishnamurthy ramnandan kumar peeyush ravindran balaraman. option discovery hierarchical reinforcement learning using spatio-temporal clustering. corr abs/. presented icml- workshop abstraction reinforcement learning. machado marlos bowling michael. learning purposeful behaviour absence rewards. corr abs/. presented icml- workshop abstraction reinforcement learning. mahadevan sridhar maggioni mauro. proto-value functions laplacian framework learning representation control markov decision processes. journal machine learning research mankowitz daniel mann timothy arthur mannor shie. adaptive skills adaptive partitions proceedings advances neural information processing systems mannor shie menache ishai hoze amit klein uri. dynamic abstraction reinforcement learning clustering. proceedings international conference machine learning mcgovern barto andrew automatic discovery subgoals reinforcement learning using diverse density. proceedings international conference machine learning menache ishai mannor shie shimkin nahum. qcut dynamic discovery sub-goals reinforcement learning. proceedings european conference machine learning moulin-frier cl´ement oudeyer pierre-yves. exploration strategies developmental robotics uniproceedings probabilistic framework. joint ieee international conference development learning epigenetic robotics junhyuk chockalingam valliappa singh satinder honglak. control memory active percepproceedings tion action minecraft. international conference machine learning sutton richard precup doina singh satinder. between mdps semi-mdps framework temporal abstraction reinforcement learning. artiﬁcial intelligence vezhnevets alexander mnih volodymyr osindero simon graves alex vinyals oriol agapiou john strategic attentive writer kavukcuoglu koray. proceedings advances learning macro-actions. neural information processing systems weber marcus rungsarityotin wasinee schliep alexander. perron cluster analysis connection graph partitioning noisy data. technical report takustr. berlin supporting lemmas respective proofs well detailed proof theorem description easily compute diffusion time tabular mdps; options leading bottleneck states used experiments; performance comparisons eigenoptions options generated reach randomly selected states; demonstration applicability eigenoptions multiple tasks experiments; details empirical setting used arcade learning environment. proof. proof detailed presented main paper. write bellman equation matrix form ﬁnite column vector entry state encoding value function. equation main paper denotes eigenpurpose interest. therefore shift ﬁnite constant without changing reward i.e. therefore assume maxs ||w||∞. clearly otherwise w||∞ |vs∗ ws∗| ||w||∞ arriving contradiction. computing elements main diagonal element ttij transitions leave state arriving state leave state arriving state assume transition sampled once thus main paper introduced diffusion time metric evaluate exploration discuss computed. diffusion time encodes expected number time steps required navigate states following random walk. tabular domains easily compute diffusion time dynamic programming. deﬁne value function state uniform random policy encodes expected number steps required navigate state chosen goal state. compute expected number steps states averaging possible goal value states. value function state encodes expected number time steps goal state reward function agent observes every time step goal state. policy evaluation case encodes expected number time steps agent take arriving goal state. compute diffusion time iterate possible states deﬁning terminal states averaging value function states mdp. figure depicts four options refer section options leading bootleneck states i.e. doorways. option deﬁned room moves agent toward closest doorway. options inspired solway discussion optimal options discovered algorithm. section show importance using information diffusion environment deﬁne option’s purposes. information impacts sequence subgoal locations options’ seek after well time scales operate ordering eigenoptions discovered different time scales operate major impact agents’ performance. demonstrate importance using environment’s diffusion information comparing approach random options simple baseline information. baseline deﬁnes option policy deﬁned whole state space terminates randomly selected state environment. performed experiments tabular case clear extend baseline settings states cannot enumerated. figure depicts diffusion time random options eigenoptions -room domain. used method described section obtain eigenoptions’ performance. random options results added incrementally agent’s action added possible options. repeated process times verify impact adding random options different order. blue line represents performance evaluated sequences. results clearly show eigenoptions going randomly selected state. obtained sequences random options fail reduce agent’s diffusion time. increase several orders magnitude enough options available point graph almost fully connected agent basically option leading possible state mdp. figure generated following protocol described section depicts learning curve agents equipped eigenoptions agents equipped random options. before blue lines indicate agent’s performance individual runs. individual competitive eigenoptions. fewer options used variance across individual runs even larger depending whether random options terminates near goal state. runs agent never even learns reach goal. therefore diffusion time average random options competitive eigenoptions demonstrating importance diffusion model use. section argued eigenoptions useful multiple tasks based results showing eigenoptions allow accumulated rewards faster. explicit demonstrate uselfuness eigenoptions multiple tasks. evaluate agents’ performance different starting goal states -room domain. section q-learning learn policy primitive actions. behavior policy chooses uniformly primitive actions options following termination. episodes time steps long learned episodes. clarity zoom plots interval agents still learning. figure depicts learning pre-determined number episodes average trials agents’ ﬁnal performance well starting goal states. based previous results ﬁxed number used eigenoptions experiments also compare approach traditional bottleneck options obtained results show switching positions starting goal states effect performance algorithm. also almost settings agents augmented eigenoptions outperfom equipped primitive actions. comparison eigenoptions options look bottleneck states subtle. expected agents equipped eigenoptions outperform agents equipped options leading bottleneck states settings goal state doorways discussed main paper. scenarios goal state closer bottleneck states options leading doorways competitive. importantly analysis based results using eigenoptions encode options required speciﬁc region state space. deﬁned different starting states atari game letting agent take random actions point termination. agent follows pre-determined sequence actions leading starting state. store observed transitions leading agent start states well obtained random actions. main paper provided results freeway montezuma’s revenge. section also provide results pac-man. starting states three games depicted figure agent plays rounds episodes episode starting different start state observes least transitions. ﬁnal incidence matrix rows sampled uniformly observed transitions. agent used deterministic version arcade learning environment games’ minimal action frame skip used three games evaluate options discover sample-based setting linear function approximation. discussed results freeway montezuma’s revenge main paper. results obtained pac-man similar already discussed. pac-man game agent needs navigate maze eating pellets avoiding ghosts. games agent clear intent reaching particular positions screen corners intersections. figure depicts positions agents tend spend time video highlighted options found online.", "year": 2017}