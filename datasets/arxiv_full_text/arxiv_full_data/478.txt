{"title": "A Deep Network with Visual Text Composition Behavior", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.", "text": "crementally retrieve words phrases original text. knowledge combined previous layer’s features create current layer’s representation thus resulting composing longer phrases clauses creating higher layers’ representations text. experiments stanford sentiment treebank dataset show method achieves competitive accuracy also exhibits compositional behavior layer-speciﬁc attention. empirically show that given piece text e.g. sentence lower layers networks select individual words negative conjunction words though higher layers composing meaningful phrases clauses negation phrase much phrase length increases networks deeper fully composing whole sentence. interestingly composing sentence compositions different sentence phrases compete become dominating features task. natural languages compositional state-of-the-art neural models achieve compositionality still unclear. propose deep network achieves competitive accuracy text classiﬁcation also exhibits compositional behavior. creating hierarchical representations piece text sentence lower layers network distribute layer-speciﬁc attention weights individual words. contrast higher layers compose meaningful phrases clauses whose lengths increase networks deeper fully composing sentence. deep neural networks leverage task-speciﬁc architectures develop hierarchical representations input higher level representations derived lower level features hierarchical representations visually demonstrated compositionality image processing i.e. pixels combine form shapes contours natural languages also compositional i.e. words combine form phrases sentences. unlike vision deep neural models mainly operate distributed word embeddings achieve compositionality still unclear propose attention gated transformation network layer’s feature generation gated layer-speciﬁc attention mechanism speciﬁcally distributing attention original given text layer networks tends inagt activation transform gate layer depends layer-speciﬁc attention mechanism. formally given piece text sentence words represented matrix matrix corresponds word represented ddimensional vector provided learned word embedding table. consequently selection vector layer softmax weighted word vectors here weight vector weight matrix respectively. varying attention weight focus different rows matrix namely different words given text illustrated different color curves connecting figure intuitively consider learned word selection component choosing different sets words given text distributing distinct attention. consider feedforward neural network multiple layers. layer typically applies nonlinear transformation input output recent previous layer produce output here indicates ﬁrst layer equal given input text namely transform gate networks learn decide feature transformation needed layer. suppose represents sigmoid function. case output lies zero one. consequently transform gate networks pass transformation block pass input yl−; gate zero networks pass unmodiﬁed transformation suppressed. left column figure reﬂects highway networks proposed method adds right columns figure transform gate function function selection vector determined attention distributed given input layer function takes input concatenation finally depicted figure feature representation last layer fully connected layers followed softmax function produce distribution possible target classes. training multi-class cross entropy loss. note that equation indicates representation depends yl−. words although equation states gate activation layer computed gate activation also affected embeds information layers intuitively networks encouraged consider words/phrases input text higher layers. consider fact bottom layer deploys linear transformation bag-of-words features. words used higher layers networks challenge sufﬁciently explore different combinations word sets given text important building accurate classiﬁer. contrast tailoring attention words different layers enables words selected layer effectively combined words/phrases selected previous layers beneﬁt accuracy classiﬁcation task stanford sentiment treebank data contains movie reviews splits training test data ﬁnegrained -class sentiment categories sentences. comparison purposes following trained models using phrases sentences evaluate sentences test time. also initialized word embeddings using dimensional pre-trained vectors glove learned layers dimensions each requires project dimensional word vectors; implemented using linear transformation whose weight matrix bias term shared across words followed tanh activation. optimization used adadelta learning rate mini-batch transform gate bias dropout rate hyperparameters determined experiments validation-set. table presents test-set accuracies obtained different strategies. results table indicate method achieved competitive accuracy compared state-of-the-art results obtained tree-lstm high-order approaches tions attention weights created different layers test data attention weights words sentence i.e. equation normalized range within sentence. ﬁgure indicates generated spiky attention distribution. attention weights either based narrow peaked bell curves formed normal distributions attention weights consider word selected networks attention weight larger i.e. receiving full attention phrase composed selected consecutive words selected. bottom subﬁgure figure present distribution phrase lengths test set. ﬁgure indicates middle layers networks e.g. longer phrases others layers ends contain shorter phrases figure also presented transform gate activities test sentences ﬁrst example sentence figure curves suggest transform gates middle layers tended close zero indicating pass-through lower layers’ representations. contrary gates ends tended away zero large tails implying retrieval knowledge input text. consistent results below. figure presents three sentences various lengths test attention weights numbered highlighted heat map. figure suggests lower layers networks selected individual words higher layers aimed phrases. example ﬁrst second layers seem select individual words carrying strong sentiment conjunction negation words also meaningful phrases composed selected later layers much only... also taste luck emotional development screen. addition middle layer i.e. layer whole sentences composed ﬁltering uninformative words resulting concise versions follows though plot predictable movie never feels formulaic attention nuances emotional development delicate characters company leaves taste luck also staleness script much movie picture screen interestingly relaxing word selection criteria e.g. including words receiving median rather full attention sentences recruited conjunction modiﬁcation words e.g. because thus becoming readable ﬂuent though plot predictable movie never feels formulaic attention nuances emotional development delicate characters company leaves taste luck timing also staleness script much movie picture book screen consider agt’s compositional behavior speciﬁc sentence e.g. last sentence figure ﬁrst layer solely selected word layers gradually pulled words book screen movie given text. incrementally layers selected words form phrases much picture book screen. finally layers added interestingly figures also imply that composing sentences middle layer networks shifted re-focus shorter phrases informative words. analysis transform gate activities suggests that during re-focusing stage compositions sentence phrases competed others well whole sentence composition dominating task-speciﬁc features represent text. discussed section intuitively including words different layers allows networks effectively explore different combinations word sets given text using words bottom layer networks. empirically observed that network namely removing test-set accuracy dropped words transforming linear combination bag-of-words features insufﬁcient obtaining sufﬁcient accuracy classiﬁcation task. instance augmented selection vectors namely removing networks tended select informative words lower layers. caused recursive form equation suggests words retrieved chance combine inﬂuence selection feature words. study found that example frequent words selected ﬁrst layer networks negation words never not. important words sentiment classiﬁcation addition like transform gate highway networks forget gate lstm attention-based transform gate networks sensitive bias initialization. found initializing bias encouraged compositional behavior networks. presented novel deep network. achieves competitive accuracy text classiﬁcation also exhibits interesting text compositional behavior shed light understanding neural models work tasks. future apply networks incrementally generating natural text nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. mach. learn. res. rupesh kumar srivastava klaus greff j¨urgen schmidhuber. training deep networks. proceedings international conference neural information processing systems. nips’ pages matthew zeiler fergus. visualizing understanding convolutional networks. computer vision eccv european conference zurich switzerland september proceedings part pages", "year": 2017}