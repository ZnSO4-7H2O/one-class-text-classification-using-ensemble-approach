{"title": "Deep supervised learning using local errors", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Error backpropagation is a highly effective mechanism for learning high-quality hierarchical features in deep networks. Updating the features or weights in one layer, however, requires waiting for the propagation of error signals from higher layers. Learning using delayed and non-local errors makes it hard to reconcile backpropagation with the learning mechanisms observed in biological neural networks as it requires the neurons to maintain a memory of the input long enough until the higher-layer errors arrive. In this paper, we propose an alternative learning mechanism where errors are generated locally in each layer using fixed, random auxiliary classifiers. Lower layers could thus be trained independently of higher layers and training could either proceed layer by layer, or simultaneously in all layers using local error information. We address biological plausibility concerns such as weight symmetry requirements and show that the proposed learning mechanism based on fixed, broad, and random tuning of each neuron to the classification categories outperforms the biologically-motivated feedback alignment learning technique on the MNIST, CIFAR10, and SVHN datasets, approaching the performance of standard backpropagation. Our approach highlights a potential biological mechanism for the supervised, or task-dependent, learning of feature hierarchies. In addition, we show that it is well suited for learning deep networks in custom hardware where it can drastically reduce memory traffic and data communication overheads.", "text": "deep supervised learning using local errors hesham mostafa vishwajith ramesh gert cauwenberghs institute neural computationdepartment bioengineering diego california e-mail hmmostafaucsd.edu error backpropagation highly eﬀective mechanism learning high-quality hierarchical features deep networks. updating features weights layer however requires waiting propagation error signals higher layers. learning using delayed non-local errors makes hard reconcile backpropagation learning mechanisms observed biological neural networks requires neurons maintain memory input long enough higher-layer errors arrive. paper propose alternative learning mechanism errors generated locally layer using ﬁxed random auxiliary classiﬁers. lower layers could thus trained independently higher layers training could either proceed layer layer simultaneously layers using local error information. address biological plausibility concerns weight symmetry requirements show proposed learning mechanism based ﬁxed broad random tuning neuron classiﬁcation categories outperforms biologically-motivated feedback alignment learning technique mnist cifar svhn datasets approaching performance standard backpropagation. approach highlights potential biological mechanism supervised task-dependent learning feature hierarchies. addition show well suited learning deep networks custom hardware drastically reduce memory traﬃc data communication overheads. gradient descent training techniques remarkably successful training broad range network architectures. success often attributed deep architectures many nonlinearity stages backpropagation used calculate direction weight updates deep layers. convolutional networks particular multiple cascaded convolutional layers allow simple lower-level features successively composed complex features allowing networks obtain highly complex relevant features convolutional layers deep convolutional neural networks trained using backpropagation thus achieving record performance variety large-scale machine vision tasks deep convolutional networks trained supervised setting training objective typically minimization classiﬁcation error network layer. objective sometimes augmented auxiliary objectives deﬁned using outputs intermediate classiﬁers network auxiliary objectives provide additional sources error deeper layers. training however involves error signals must propagate backwards layer. standard backpropagation biologically unrealistic several reasons need buﬀer network states errors arrive layer; weight symmetry forward backward passes; need precisely interleave forward backward passes. several biologically-motivated learning mechanisms proposed explain circuits brain able learn complex hierarchical representations. broad class proposals based contrastive learning energybased models models network trained minimize discrepancy equilibrium points running freely observables clamp values units network. weight symmetry required though synaptic connection neuron another assumes matching synaptic connection identical strength reverse direction. weight symmetry avoided using independent ﬁxed random weights backpropagate errors network layers. however like standard backpropagation error signals non-local. instead backpropagating errors layer layer random feedback connections networks directly ﬁxed random projection layer error error signal deep layers. although permits single global error signal communicated common layers still incurs substantial wait times memory requirements weight updates forward pass entire network completed error signal available requires deep layers hold states duration full forward pass. propose learning approach weights given layer trained based local errors generated solely based neural state variables layer. errors generated directly training labels using classiﬁer ﬁxed random weights hidden layers whose input neural activations layer trained. instead minimizing global objective function training thus minimizes many local objective functions. approach compromises core tenets standard backpropagation adjustment parameters concert minimize uniﬁed objective. nevertheless training local errors still allows deep network compose features learned lower layers complex features higher layers. evidenced improvement accuracy random local classiﬁers deeper layers. training local errors thus retains hierarchical composition features strengths deep networks. implement weight updates based backpropagation biologically inspired network prepost-synaptic neurons need buﬀer past activity pre-synaptic neurons reproduce past activity sync corresponding errors arriving layers order update weights. incompatible biologically motivated synaptic weight update rules typically triggered pre-synaptic events depend relative timing prepost-synaptic spikes and/or state variables post-synaptic neuron. learning mechanism bypasses biological implausibility arguments standard backpropagation generating errors locally layer using ﬁxed random projections. weight updates could thus carried synaptic currents post-synaptic neurons still retain memory recent presynaptic activity. weight symmetry forward backward passes standard backpropagation learning another biologically unrealistic aspect. case weight symmetry requirement arises one-step error backpropagation output local random classiﬁer neurons layer trained. similar ref. experimented relaxing symmetry requirement using diﬀerent random ﬁxed weights classiﬁer error error layer trained. analyze implications proposed learning approach design custom hardware devices learning parameters deep networks. proposed learning approach explicit backward pass errors locally generated used directly update weights. show approach drastically reduces memory traﬃc compared standard backpropagation typical situation network weights activations compute device memory. achieve reduction even despite increased number parameters network addition random local classiﬁer weights layer. weights however ﬁxed allowing generated using pseudo-random number generators negligibly small random seeds prngs layer need stored. discuss related work section describe proposed learning mechanism section quantitatively assess hardware-related computational memory access beneﬁts compared standard learning global objective functions section present results applying proposed learning method standard supervised learning benchmarks section compare learning method’s performance feedback alignment technique present conclusions discussion biological plausibility proposed learning mechanism section training deep convolutional networks currently dominated approaches weights simultaneously trained minimize global objective. typically done purely supervised setting training objective classiﬁcation loss layer. ameliorate problem exploding/vanishing errors deep layers auxiliary classiﬁers sometimes added provide additional error information deep layers unlike training approach however training still involves backpropagating errors across entire network simultaneous adjustments weights. several learning mechanisms traditionally used pre-train deep network layer-by-layer using local error signals order learn probability distribution input layer activations order minimize local reconstruction errors mechanisms however unsupervised networks need augmented classiﬁer layer typically added deepest layer. network weights ﬁne-tuned using standard backpropagation minimize error classiﬁer layer. supervised layer-wise training pursued auxiliary classiﬁers co-trained unlike random ﬁxed auxiliary classiﬁers proposed here. supervised layer-wise training used pre-training step results reported full network ﬁne-tuning using backpropagation classiﬁer layer. approaches forego ﬁne-tuning step keep network ﬁxed unsupervised layer-wise training phase train classiﬁer layer features learned local learning involves iterative procedure learning sparse codes computationally demanding. network architectures fail yield intermediate classiﬁcation results intermediate layers. moreover applicability datasets complex mnist unclear since labels used guide learning feature. complex learning scenarios abundance possible features networks could well learn label-relevant features thereby compromising performance classiﬁer. instead layer-wise pre-training several recent approaches train whole network using hybrid objective contains supervised unsupervised error terms network conﬁgurations unsupervised error terms local layer supervised error term however requires backpropagating errors whole network. requirement avoided training approach used learn extract compact feature vectors documents training proceeds layer layer error layer combination reconstruction error supervised error coming local classiﬁer. local auxiliary decoder classiﬁer pathways still require training however. approaches also make combination supervised unsupervised error signals train boltzmann machines discriminative models learning however computationally demanding approach involves several iterations approach mean-ﬁeld equilibrium point network errors still backpropagated multiple layers. multi-layer networks considered single layer used. refs backpropagation scheme modiﬁed random ﬁxed weights backward path. relaxes biologically unrealistic requirements backpropagation weight symmetry forward backward pathways. errors still non-local however generated layer. learning mechanism able generate error signals locally synthetic gradients mechanism errors generated dedicated error modules layer based layer’s activity label. parameters dedicated error modules updated based errors arriving higher layers order make error modules better predictors true globally-derived error signal. approach generates errors diﬀerent manner local classiﬁer layer receives error information layer above. train multi-layer networks either convolutional fully connected layers based local errors generated random classiﬁers. consider fully connected hidden layer network whose activation vector denoted receiving input weight matrix layer bias vector neuron’s corresponding derivatives heaviside step function. pre-deﬁne hidden layer ﬁxed random classiﬁer matrix matrix number classiﬁcation categories. random matrix used convert layer activation vector category score vector miyi. since supervised learning setting correct input category known training allows layer generate scalar loss error signal could example cross-entropy loss square hinge loss. figure supervised learning multi-layer network using local errors. biases omitted clarity. arrows indicate error pathways. hidden layer trained using local errors generated classiﬁer random ﬁxed weights errors randomly projected back using matrix multiplied element-wise layer’s activation derivative yield error signal used update weights. element-wise multiplication operator outer product operator learning rate. matrix used backpropagate classiﬁer error layer trained. weight bias updates executing exact gradient descent minimize random classiﬁer error case training layer equivalent training network hidden layer hidden layer’s input weights biases trainable output weights ﬁxed. learning scheme illustrated fig. convolutional layers learning scheme remains unchanged. post-activation feature maps tensor simply ﬂattened yield vector multiplying random classiﬁer matrix also dropout training setting minimize overﬁtting. incoming/outgoing weights to/from dropped neurons updated iteration neuron dropped. networks batch normalization layer’s non-linearity. layer’s learnable parameters include scaling factor also trained using local errors. fully connected layer input local classiﬁer taken dropout mask applied convolutional layer input layer’s local classiﬁer taken pooling applying dropout mask. experiments initialize ﬁxed random classiﬁer weights well trainable weights main network uniform zero-mean distribution whose max/min values depend number neurons source target layers according scheme compare approach feedback alignment training method random ﬁxed weights used backpropagate error layer-by-layer layer. layer’s activation derivative still used backpropagating errors. presence max-pooling layers errors backpropagate winner neuron pooling window. using feedback alignment training presence dropout neuron dropped forward pass also dropped backward pass. using convolutional layers ﬁxed random ﬁlters convolve errors convolutional layer yield errors outputs previous/lower figure memory traﬃc number operations diﬀerent training methods. arrows compute device external memory indicate memory traﬃc green arrows indicate data buﬀered reused compute device. computation stage executed number times given enclosing repeat block. standard backpropagation learning. training layers simultaneously using local errors. note backward pass weights updated forward pass. convolutional layer. also batch normalization training using feedback alignment. extra scaling parameters introduced batch normalization trained using randomly backpropagated errors arriving batch-normalized layer’s output. standard learning techniques based backpropagating errors whole network require hardware executing learning algorithm store activation values activation derivatives network layers order calculate weight updates backpropagate errors errors available layer. imposes several communication memory access overheads learning executed hardware whose memory accommodate network weights activations. large scale convolutional networks practically includes devices on-chip memory limited tens mbytes state deep convolutional networks typically require several hundred mbytes several gbytes order store network weights mini-batch activations data thus continuously shuttled compute device external memory. case even custom accelerators developed accelerate inference phase complete forward pass large-scale convolutional network executed completely accelerator without access external memory store intermediate activations load weights. improvements memory bandwidth signiﬁcantly improvements computing elements speed reducing memory traﬃc compute intensive task learning deep networks thus improves performance relaxes requirements memory bandwidth latency needed keep compute elements occupied allowing either compute elements higher frequencies external memory lower frequencies. moreover energy needed drive oﬀ-chip traﬃc from/to external memory well memory read/write energy often contribute signiﬁcantly overall energy consumption reducing memory traﬃc thus signiﬁcant impact overall energy consumption learning hardware. section analyze savings memory traﬃc volume obtained using learning approach based local errors propose paper. consider neural network layers. parameters mini-batch activations layer respectively. |ai| number elements neuron layer fanout neuron layer projects neurons layer convolutional layers ignore border eﬀects might cause neurons borders feature maps project fewer neurons neurons away borders. divide training data mini-batches train network epochs. weight neuron activation takes memory word figure illustrates data traﬃc number operations needed standard backpropagation training. data traﬃc fig. assumes compute device enough on-board memory buﬀer output activations layer order activations calculate next layer’s activation. also assume compute device need parameters layer streamed forward pass backward pass. layer ri−|ai−|. backward pass compute device buﬀers back-propagated errors layer uses calculate errors preceding layer. ri|ai| operations needed calculate weight updates additional ri|ai| operations needed backpropagate errors layer layer ignore special case input layer errors need backpropagated. also ignore operations needed calculate error layer. stored. backpropagate local classiﬁer error obtain error layer additional c|ai| operations needed ri−|ai−| operations needed update parameters layer based layer’s error. table summarizes number operations memory read/write volume required training methods. learning using local errors decisive advantage comes memory traﬃc requires drastically less read write operations compared standard backpropagation. reduction number operations less unequivocal depends number classiﬁcation classes fanout neurons network learning using local error condition easily satisﬁed ﬁrst validate performance training approach mnist hand-written digit recognition task. used standard split examples training/validation/testing respectively. validation added training choosing hyper-parameters. network fully connected hidden layers neurons layer train weights entire network using local errors. baseline also train -hidden layers network -hidden layers network using standard backpropagation hidden layer also neurons. dropout used networks reduce overﬁtting. ﬁrst used ﬁxed symmetric random weights forward backward pathways local error loops layers. results shown fig. local classiﬁer errors improve second third hidden layers compared ﬁrst hidden layer implying network able make depth obtain better accuracy. local classiﬁer errors second third layers similar implying network unable make increased depth beyond hidden layers simple dataset. observation also valid standard backpropagation accuracy improve going hidden layers three hidden layers. training using local errors also experiments local classiﬁer weights trainable parameters. eﬀect accuracy shown table figure mnist test errors obtained three networks -layer network trained using local errors symmetric local feedback weights errors three random local classiﬁers shown network hidden layers trained using standard backpropagation network three hidden layers trained using standard backpropagation. networks trained epochs. line average training trials. except network trained using local errors sign-concordant local feedback weights independent random magnitudes used. next lessen concern biological implausibility exact symmetry feedforward feedback weights relaxed weight symmetry requirement local error loops initialized error feedback weights randomly independently except modiﬁed sign weights sign sign. signs feedback weights local error loops thus match signs feedforward weights ’sign-concordant feedback weights’ case shown fig. performance deteriorates slightly case compared symmetric feedforward feedback local classiﬁer weights. relax symmetry requirement choose random completely independent network failed learn error rates stayed near-chance level. also experimented training based feedback alignment errors layer backpropagated using random ﬁxed weights. network’s performance using feedback alignment worse learning using local errors shown table table mnist ﬁnal test error training epochs. learning using local errors local classiﬁer errors layers reported. mean standard deviation runs. important note feedback alignment feedforward weights eventually ’align’ random weights used backpropagate errors enabling network learn. learning using random ﬁxed local classiﬁers choose random error feedback weights classiﬁer weights ﬁxed thus align random weights used one-step backpropagation. reliable error information however still reach layer trained signs random backpropagation weights match signs ﬁxed local classiﬁer weights in-line previous investigations importance weight symmetry backpropagation argue importance sign-concordance forward backward weights whitening. training images used training/validation report errors images test set. unlike mnist dataset standard backpropagation signiﬁcantly outperforms training using local errors shown fig. table performance local error learning deteriorates slightly using sign-concordant local feedback weights instead symmetric local feedback weights. performance improve local classiﬁer second fully connected layer compared classiﬁer ﬁrst fully connected layer. trained variant network using fully connected layer using standard backpropagation. shown table improvement performance network trained using standard backpropagation minimal going fully connected layers. implies second fully connected layer largely superﬂuous local error learning thus unable capitalize unlike mnist dataset allowing local classiﬁer parameters trainable improves performance signiﬁcantly. case mnist dataset training using feedback alignment leads signiﬁcantly worse performance learning using local errors. feedback alignment results better previously reported refs. bigger network well-regularized using dropout. using well-regularized network particularly crucial investigating alternatives standard backpropagation poorly-regularized learning make worse learning algorithm seem better simply better regularizes learning problem compared superior algorithm overﬁts training data. strong regularization also potential reason exact gradient descent clearly superior unlike previous investigations report better performance using various approximations standard backpropagation better performance better regularization introduced approximate learning algorithms. figure cifar test errors obtained convolutional networks architecture described main text network trained using local errors symmetric local feedback weights errors random local classiﬁers layers shown; network identical architecture trained using standard backpropagation. except network trained using local errors sign-concordant local feedback weights independent random magnitudes used. images respectively previously used refs. validation added training choosing hyper-parameters images preprocessed using local contrast normalization technique ref. figure shows test error curves case symmetric local feedback weights case sign-concordant local feedback weights. test error trends fig. table similar observed cifar. performance standard backpropagation clearly superior followed learning using local errors generated trainable local classiﬁers. learning using local errors generated ﬁxed random classiﬁers lags behind still outperforms learning using feedback alignment. weight symmetry forward backward passes delayed error generation biologically unrealistic aspects backpropagation. recent investigations shown weight symmetry requirement relaxed allowing learning proceed random feedback weights investigations however address problem local learning require network maintain state errors arrive higher layers. local errors often used augment layer errors however relatively little work done supervised learning using exclusively local errors none know investigated local error generation using ﬁxed random classiﬁers. results show learning using local errors generated using random classiﬁers falling short performance standard backpropagation signiﬁcantly outperforms learning using feedback alignment techniques holds true even relaxing weight symmetry requirement local feedback loop using random ﬁxed feedback weights sign-aligned random ﬁxed classiﬁer weights local learning loop. maintaining sign-alignment problematic feedback alignment technique sign feedback weights dynamically track sign feedforward weights training introduces dynamic dependency sets weights. case since sets weights ﬁxed dependency need enforced initially. table cifar ﬁnal test error training epochs. learning using local errors local classiﬁer errors layers reported. mean standard deviation runs. table svhn ﬁnal test error training epochs. learning using local errors local classiﬁer errors layers reported. mean standard deviation runs. cifar svhn results indicate locally generated errors allow convolutional layer learn good features used subsequent layer learn even informative features evidenced increased accuracy local classiﬁers higher layers. however approach solves many small optimization problems problem involves weights layer. therefore lose core advantages standard backpropagation learning using global objective function high probability ﬁnding good minimum parameter space dimensionality parameter space large includes network parameters thus expected classiﬁcation performance suﬀer compared learning using standard backpropagation global objective function. single cell measurements monkey area indicate broad tuning range categories broad category tuning realized proposed training scheme random local classiﬁer weights deﬁne neuron contributes score classiﬁcation category. training actual tuning properties neuron change in-line pre-deﬁned ﬁxed tuning deﬁned random classiﬁer weights minimize local classiﬁer error. error generation mechanism several biologically attractive aspects involves synaptic projections allowing errors generated quickly weight updates carried input-induced changes states neurons decayed. avoids common unrealistic input buﬀering requirement encountered standard backpropagation feedback alignment techniques. figure svhn test errors obtained convolutional networks network trained using local errors symmetric local feedback weights errors random local classiﬁers layers shown. network identical architecture trained using standard backpropagation except network trained using local errors sign-concordant local feedback weights independent random magnitudes used. generation loop particularly simple removes potential problematic interactions learning auxiliary classiﬁer weights learning main network weights. ﬁxed random local classiﬁer weights allows sidestep main hardware-related issues using auxiliary local classiﬁers need store local classiﬁer weights. especially large convolutional layers storing local classiﬁer weights could prohibitively expensive terms memory resources. since local classiﬁer weights need accessed ﬁxed order training iteration order calculate classiﬁer outputs cheaply quickly reproducibly generated using prng small seed. shown approach allows obtain learning mechanism drastically reduces memory traﬃc compared standard backpropagation. inference random classiﬁer weights layer used generate classiﬁcation decision evaluation layer. thus needed series classiﬁcation decisions obtained layer small computational cost virtually memory cost. decisions bottom layers even though less accurate decisions higher layers used situations response time critical. allows network dynamically truncated higher layers evaluated ﬁnal decision taken intermediate layers. feature proposed networks enables dynamical trade-oﬀ accuracy energy consumption/computational load many layers allowed energy budget response time constraint evaluated.", "year": 2017}