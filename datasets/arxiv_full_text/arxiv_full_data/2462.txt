{"title": "Asynchronous Stochastic Gradient MCMC with Elastic Coupling", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider parallel asynchronous Markov Chain Monte Carlo (MCMC) sampling for problems where we can leverage (stochastic) gradients to define continuous dynamics which explore the target distribution. We outline a solution strategy for this setting based on stochastic gradient Hamiltonian Monte Carlo sampling (SGHMC) which we alter to include an elastic coupling term that ties together multiple MCMC instances. The proposed strategy turns inherently sequential HMC algorithms into asynchronous parallel versions. First experiments empirically show that the resulting parallel sampler significantly speeds up exploration of the target distribution, when compared to standard SGHMC, and is less prone to the harmful effects of stale gradients than a naive parallelization approach.", "text": "consider parallel asynchronous markov chain monte carlo sampling problems leverage gradients deﬁne continuous dynamics explore target distribution. outline solution strategy setting based stochastic gradient hamiltonian monte carlo sampling alter include elastic coupling term ties together multiple mcmc instances. proposed strategy turns inherently sequential algorithms asynchronous parallel versions. first experiments empirically show resulting parallel sampler signiﬁcantly speeds exploration target distribution compared standard sghmc less prone harmful effects stale gradients naive parallelization approach. last years ever increasing complexity machine learning models together increasing amount data available train resulted great demand parallel training inference algorithms deployed many machines. meet demand practitioners increasingly relied parallel stochastic gradient descent methods optimizing model parameters contrast this literature efﬁcient parallel methods sampling posterior model parameters given data much scarce. examples algorithms setting typically constrained speciﬁc model classes only consider data parallelism summary exception recent work chen general sampling pendant asynchronous methods missing. paper consider general problem sampling arbitrary posterior distribution parameters given observed datapoints machines available speed sampling process. without loss generality write mentioned posterior exp) deﬁne potential energy many algorithms solving problem hamiltonian monte carlo augment potential energy terms depending auxiliary variables speed sampling. general case write posterior exp) denotes collection variables denotes hamiltonian samples obtained sampling discarding additionally assume marginalizing results constant offset require y∈ra exp)dy stochastic gradient mcmc algorithms solve described sampling problem assuming access possibly noisy gradient potential respect parameters case assuming properly chosen auxiliary variables sampling performed analogy physics simulating system based hamiltonian precisely following general formulation simulate stochastic differential equation form denotes deterministic drift incurred diffusion matrix square root applied element-wise denotes brownian motion. fairly general assumptions functions unique stationary distribution system equivalent posterior distribution speciﬁcally showed following specialized form stationary distribution equivalent posterior positive semi-deﬁnite skew-symmetric. importantly holds also noisy estimates gradient computed randomly sampled subset data available practice choice simulating differential equation digital computer involves approximations. first simulated discretized steps resulting update rule m-dimensional multivariate gaussian distribution. second dealing large datasets exact computation gradient becomes computationally prohibitive thus relies stochastic approximation computed randomly sampled subset data stochastic gradient n|b| gaussian distributed variance leading noise term described sde. using approximations derive following discretized system equations stochastic gradient variant hamiltonian monte carlo show utilize computational power machines speed given sampling procedure relying dynamics described equations mentioned before update equations derived section involve alternating updates parameters uauxiliary variables leading inherently sequential algorithm. want utilize machines speed sampling thus face non-trivial challenge parallelizing updates. general identify solutions naive parallelization strategy send variables different machines every steps parameter server. machine computes gradient estimate current step communication period i.e. server waits gradient estimates sent back simulates system using set-up mcmc chains independently update parameter vector denotes machine) following dynamics second approach clearly results markov chains asymptotically sample correct distribution might result diverse samples simulation using single chain also clear cannot speed convergence individual chains interaction them. ﬁrst approach hand harder analyze. observe wait gradient estimates step obtain sg-mcmc algorithm parallel gradient estimates synchronous updates dynamic equations. consequently setup preserves guarantees standard sg-mcmc requires synchronization machines step. real-world experiment result large communication overhead. choices regime interested cannot rely standard convergence analysis sg-mcmc anymore. nonetheless concentrate analysis interpret stale parameters simply result noisy estimates used within dynamic equations efﬁcacy parallelization scheme intuitively depends amount additional noise introduced stale parameters requires convergence analysis. preparation manuscript concurrent work sgmcmc stale gradients derived theoretical foundation intuition interestingly following empirically show additional noise unproblematic small range becomes problematic growing believe results accordance mentioned recent work chen uniﬁcation theory proposed algorithm remains important future work. given negative analysis section might wonder whether approach altered chains loosely coupled; allowing faster convergence avoiding excessive communication. propose consider following alternative parallelization scheme iia) speed sgmcmc chains couple parameter vectors additional center variable elastically attached. collect updates center variable central server broadcast updated version every steps across machines. note approach based idea recently utilized derive asynchronous optimizer zhang serving main inspiration. discussion connection deterministic stochastic dynamics presented section derive asynchronous sgmcmc variant samplers elastic coupling described consider augmented hamiltonian interpret centering mass asynchronous samplers elastically coupled speciﬁes coupling strength. easy data-set machines allowing parallelize computation without need asynchronous updating. interesting problem right already exists considerable amount literature running parallel mcmc sub-sets data scott rabinovich neiswanger focus parallelization schemes make assumption broad applicability. decompose independent terms constitute hamiltonian corresponding standard sghmc case further obtain joint altered hamiltonian desired parameter vectors elastically coupled. simulating system according hamiltonian exactly would result algorithm requiring synchronization simulation step however assume noisy measurement center variable momentum available step derive mostly asynchronous algorithm updating achieve assume store current estimate center variable momentum central server. server receives updates samplers every steps replies current values. assuming gaussian normal distribution error current value sampler keeps track noisy estimate center variable used simulating updates derived hamiltonian equation assumptions derive following discretized dynamical equations speciﬁes noise stochastic gradient estimates variance aforementioned noisy center variable. before notation refer sample gaussian distribution mean covariance note that presented dynamical equations derived sghmc elastic coupling idea depend basic hamiltonian equation thus derive similar asynchronous samplers sgmcmc variant including ﬁrst order stochastic langevin dynamics advanced techniques reviewed inspecting equations observe that similar approach also contain additional noise source noise injected system potential staleness center variable. however since noise indirectly affects parameters might hope center variable acts buffer damping injected noise. case would expect resulting algorithm robust communication delays naive parallelization approach. addition consideration proposed dynamical equations convenient form makes easy verify fulﬁll conditions valid sgmcmc procedure. proposition dynamics system posterior distribution stationary distribution samplers. section furthermore marginalization auxiliary variables results constent offset ﬁrst identify α/kθi m−pi y∈ra exp)dy amounts evaluating gaussian integrals therefore easily checked constant required. thus simulating dynamical equations results samples discarding auxilliary variables gives desired result. designed initial three experiments validate sampler. first intuition behavior elastic coupling term tested sampler dimensional example. figure show ﬁrst steps taken standard sghmc elastically coupled figure comparison ﬁrst sampling steps performed sghmc elastically coupled sghmc variant sampling simple dimensional gaussian distribution. animated video samplers found https//goo.gl/zzvfg. sampler four parallel threads sampling two-dimensional gaussian distribution hyperparameters observe independent runs sghmc take fairly different initial paths depending noise happen sghmc explores low-density regions distribution ﬁrst steps contrast four samplers elastic coupling quickly sample high density regions show coherent behaviour. second experiment compare ec-sghmc standard sghmc naive parallelization described section method sampling weights layer fully connected neural network applied classifying mnist dataset. purposes interpret neural network function parameterizes probability distribution classes given dataset results experiment depicted figure plot negative likelihood time observe parallel samplers perform signiﬁcantly better standard sghmc. however increase communication delay synchronize threads every steps additional noise injected sampler procedure becomes problematic async sghmc whereas ec-sghmc copes much gracefully. figure comparison different sgmcmc samplers sampling posterior neural network weights fully connected network mnist residual network cifar- best viewed color. finally test scalability approach sampled weights -layer residual applied cifar- dataset. sample posterior given equation neural network -layer residual network described results experiment depicted figure showing that again ec-sghmc leads signiﬁcant speed-up standard sghmc sampling. described section elastic coupling technique similar used manuscript recently used accelerate asynchronous stochastic gradient descent although purpose paper derive method instead arrive scalable mcmc sampler instructive take closer look connection methods. establish connection re-derive elastic averaging method zhang deterministic limit dynamical equations removing added noise equations setting identity matrix performing variable substitutions \u0001mpi \u0001mri yields dynamical equations where additionally zhang propose update every steps drop terms including update equations intermittent steps. expected ﬁrst glance sets update equations look similar. interestingly however differ respect integration elastic coupling term updates center variable eamsgd equations center variables augmented momentum term elastic coupling force inﬂuences parameter values directly rather than indirectly momentum physics perspective adopt paper updates thus wrong sense break interpretation variables generalized coordinates generalized momenta. noted also straight-forward recover valid sgmcmc sampler corresponding stochastic variant equations hamiltonian given equation derivation thus suggests alternative update equations eamsgd. interesting avenue future experiments thus thoroughly compare deterministic updates equations eamsgd updates terms empirical performance respect convergence properties. initial test performed suggests former perform least good eamsgd. also note easgd without momentum exactly recovered deterministic limit approach randomly re-sample auxilliary momentum variables step would thus simulate stochastic gradient langevin dynamics welling elastically coupling multiple sgmcmc chains. first experiments suggest proposed method compares favorably naive parallelization strategy additional experiments required paint conclusive picture. discussed connection method recently proposed stochastic averaging optimizer zhang revealing alternative variant easgd momentum. jeffrey dean greg corrado rajat monga chen matthieu devin mark marc’aurelio ranzato andrew senior paul tucker yang quoc andrew large scale distributed deep networks. proc. nips’ umut simsekli hazal koptagel hakan güldas taylan cemgil figen öztoprak ilker birbil. parallel stochastic gradient markov chain monte carlo matrix factorisation models. arxiv.", "year": 2016}