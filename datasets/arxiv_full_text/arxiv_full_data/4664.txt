{"title": "Bounded Optimal Exploration in MDP", "tag": ["cs.AI", "cs.LG"], "abstract": "Within the framework of probably approximately correct Markov decision processes (PAC-MDP), much theoretical work has focused on methods to attain near optimality after a relatively long period of learning and exploration. However, practical concerns require the attainment of satisfactory behavior within a short period of time. In this paper, we relax the PAC-MDP conditions to reconcile theoretically driven exploration methods and practical needs. We propose simple algorithms for discrete and continuous state spaces, and illustrate the benefits of our proposed relaxation via theoretical analyses and numerical examples. Our algorithms also maintain anytime error bounds and average loss bounds. Our approach accommodates both Bayesian and non-Bayesian methods.", "text": "within framework probably approximately correct markov decision processes much theoretical work focused methods attain near optimality relatively long period learning exploration. however practical concerns require attainment satisfactory behavior within short period time. paper relax pac-mdp conditions reconcile theoretically driven exploration methods practical needs. propose simple algorithms discrete continuous state spaces illustrate beneﬁts proposed relaxation theoretical analyses numerical examples. algorithms also maintain anytime error bounds average loss bounds. approach accommodates bayesian nonbayesian methods. formulation sequential decision making markov decision process successfully applied number real-world problems. mdps provide ability design adaptable agents operate effectively uncertain environments. many situations environment wish model unknown aspects thus agent needs learn interacting environment. words agent explore unknown aspects environment learn mdp. considerable amount theoretical work mdps focused efﬁcient exploration number principled methods derived learning obtain nearoptimal policy. example kearns singh strehl littman considered discrete state spaces whereas bernstein shimkin pazis parr examined continuous state spaces. practice however heuristics still commonly used focus theoretical work apparently diverged practical needs paper modify prevalent theoretical approach develop theoretically driven methods come close practical needs. copyright association advancement artiﬁcial intelligence rights reserved. represented tuple states actions transition probability function reward function discount factor. value policy state cumulative expected reward given many situations transition function and/or reward function initially unknown. conditions often want policy algorithm time yield value close optimal value exploration. here denotes current state time precisely want following probability least exploration time. algorithm policyat said probably approximately correct mdps condition holds polynomial relevant quantities mdps. notion pac-mdp strong theoretical basis widely applicable avoiding need additional assumptions reachability state space access reset action access parallel sampling oracle however pac-mdp approach often results algorithm over-exploring state space causing reward unit time long period time. accordingly past studies proposed pac-mdp algorithms rarely presented corresponding experimental result done tuning free parameters renders relevant algorithm longer pac-mdp problem noted furthermore many problems even possible guarantee close within agent’s lifetime. noted that despite strong theoretical basis pac-mdp approach probability least ﬁrst condition ensures agent efﬁciently learns h-reachable models. second condition guarantees learning method distance function arbitrarily poor. following relate pac-rmdp pac-mdp near-bayes optimality. proofs given appendix paper. proposition algorithm rmdp) pac-mdp given deﬁnition proposition consider modelbased bayesian reinforcement learning planning horizon belief space assume bayesian optimal value function converges h-reachable optimal function that polynomial time steps. then pac-rmdp algorithm policy obtains expected cumulative reward polynomial time steps probv ability least note actual expected cumulative reward expectation true dynamics whereas believed expected cumulative reward expectation current belief belief evolution. addition whereas pac-rmdp condition guarantees convergence h-reachable optimal value function bayesian optimality not. sense proposition suggests theoretical guarantee pacrmdp would stronger near-bayes optimality step lookahead. summarizing above pac-rmdp) implies pac-mdp pac-rmdp related near-bayes optimality. moreover decreases range theoretical guarantee pac-rmdp becomes bayesian estimation random samples converges true value certain assumptions. however exploration selection actions cause bayesian optimal agent ignore state-action pairs removing guarantee convergence. effect well illustrated heuristic-based methods remain popular practice. would appear result issues. summary seems dissonance strong theoretical approach practical needs. practical limitations pac-mdp approach focus correctness without accommodating time constraints occur naturally practice. overcome limitation ﬁrst deﬁne notion reachability model learning relax pac-mdp objective based brevity focus transition model. possible future samples {s|p represent model update rule; maps model sample corresponding model write represent learning method algorithm {f}∈. h-reachable models mlth deﬁned intuitively h-reachable models mlth contains transition models obtained agent updates current model time using combination additional samples note h-reachable models deﬁned separately state-action pair. example mlth contains models reachable using additional samples drawn remark planning horizon belief space assume algorithm used independent dirichlet model determines then algorithm behaves identically near-bayes optimal algorithm bayesian optimistic local transitions sample complexity smaller pac-mdp larger near-bayes optimality note bolt necessarily pac-rmdp misleading priors violate conditions deﬁnition discussion important observation that sample complexity algorithm dominated number samples required reﬁne model rather explicit exploration unknown aspects world. recall internal value function designed force agent explore whereas currently estimated value function results exploitation. difference decreases rate whereas error decreases rate weaker previous theoretical objectives. accommodates practical need improve trade-off theoretical guarantee practical performance concept reachability. discuss relationship bounded rationality bounded optimality well corresponding notions regret average loss appendix. illustrate proposed concept ﬁrst consider simple case involving ﬁnite state action spaces unknown transition function without loss generality assume reward function known. algorithm internal value function used algorithm choose action. actual value function according true dynamics derive algorithm principle optimism face uncerlth tainty achieved using following internal value function pseudocode shown algorithm following consider special case sample mean estimator pt|s nt)/nt number samples state-action pair number samples transition given action case maximum model equation achieved future observations transitions state best value. thus computed maxa nt+h maxs analysis ﬁrst show algorithm pac-rmdp maintains anytime error bound average loss bound related previous algorithms analyze explicit exploration runtime assume algorithm used sample mean estimator determines distance assume stay larger ﬁxed constant small appear corollary shown corresponding case analysis theorem explicit exploration runtime reduced intuitively happens given not-too probability high-consequence transition initially unknown. naturally difﬁcult learn reﬂected corollary experimental example compare proposed algorithm mbie variance-based exploration bayesian exploration bonus bolt algorithms designed pac-mdp near-bayes optimal used parameter settings render neither pac-mdp near-bayes optimal. contrast experiments previous research present results several theoretically meaningful values well theoretically nonmeaningful value illustrate property. algorithm deterministic sampling assumptions input distribution compare algorithms sampling rely heavily knowledge input distribution. mbie pac-mdp parameters pacmdp assumed input distribution parameter bolt near-bayes optimal algorithms whose parameters fully speciﬁed analyses namely following araya-l´opez thomas buffet using \u0001-approximated horizon logγ) sample mean estimator pac-mdp pac-rmdp algorithms independent dirichlet model near-bayes optimal algorithms. interpolate qualitative behaviors values presented here. principle behind results small values causes over-exploration focus near-optimality. consider ﬁve-state chain problem standard problem literature. problem optimal policy move toward state farthest initial state reward structure explicitly encourages exploitation agent even \u0001-greedy agent remain initial state. discount factor convergence criterion value iteration figure shows numerical results terms average reward time step seen ﬁgure proposed algorithm worked better. mbie work reasonably discard theoretical guarantee. maximum reward rmax γirmax rmax thus \u0001-closeness yield useful information similar problem noted kolter araya-l´opez thomas buffet section consider problem continuous state space discrete action space. transition function possibly nonlinear linearly parameterized state represented state parameters ns}) action time assume basis functions known weights unknown. words noise term given brevity unknown transition dynamics method directly applicable unknown reward functions reward represented form. problem slightly generalized version considered abbeel strehl littman algorithm ﬁrst deﬁne variables used algorithm explain algorithm works. vector model parameters state component. rt×ni consist input vectors r×ni time denote eigenvalue decomposition input matrix rni×ni represents diagonal matrix. simplicity notation arrange eigenvectors eigenvalues diagonal elements deﬁne main variables used algorithm assume access exact planning algorithm. assumption would relaxed using planning method provides error bound. assume algorithm used least-squares estimation determines distance function experimental examples consider examples mountain problem standard problem literature problem originates real-world problem. examples compare proposed algorithm directly related pac-mdp algorithm pac-mdp algorithm present results several theoretically meaningful values theoretically non-meaningful value illustrate property. used pac-mdp pac-rmdp algorithms. \u0001-greedy algorithm executed planning phase estimated maxss∈ω| a)|/s states visited planning phase algorithm worked well. best performance terms total reward achieved pac-rmdp. since problem required number consecutive explorations random exploration employed \u0001-greedy algorithm allow agent reach goal. result exploration randomness environment pac-mdp algorithm reached goal several times kept exploring environment ensure near-optimality. figure pac-mdp algorithm quickly converges good behavior discard theoretical guarantee simulated treatment problem described ordinary differential equations action corresponds whether agent administers treatments patients types exploration required learn effect using treatments viruses another learn effect using treatments immune systems. learning former necessary reduce population viruses latter required prevent overuse treatments weakens immune system. episode consists steps conduct simulations episodes. shown figure pac-mdp algorithm worked reasonably well however best total reward exceed pac-mdp guarantee seem useful. \u0001-greedy algorithm work well example required sequential exploration certain periods learn effects treatments. paper proposed pac-rmdp framework bridge theoretical objectives practical needs. although pac-rmdp algorithms worked well experimental examples small possible devise problem pac-rmdp algorithm used large extreme cases algorithm would reduce pac-mdp. thus adjustable theoretical guarantee pac-rmdp concept reachability seems reasonable objective. h-reachable models. ﬂexible model derivation conﬁdence interval would difﬁcult task h-reachable models simply computed lookahead using model update rule. thus future work includes derivation pac-rmdp algorithm ﬂexible and/or structured model. author would like thank prof. michael littman prof. leslie kaelbling prof. tom´as lozano-p´erez thoughtful comments suggestions. gratefully acknowledge support grant grant n--- grant wnf. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views sponsors. section present proofs propositions proposition pac-rmdp) implies pac-mdp given deﬁnition proof. pac-rmdp) algorithm deﬁnition implies polynomial time steps. satisﬁes condition pac-mdp. proposition consider model-based bayesian reinforcement learning planning horizon belief space assume bayesian optimal value function converges h-reachable optimal function that polynomial time steps. then pac-rmdp algorithm policy obtains expected cumulative reward proof. directly follows deﬁnition assumption. polynomial time steps probability least bounded rationality bounded optimality focus limitations planning phase contrast pac-rmdp considers limitations learning phase case bounded rationality performance guarantee pac-rmdp algorithm arbitrary depending choice contrary bounded optimality solves problem arbitrariness seemingly cost applicability. requires strong notion optimality similar instance optimality; roughly must optimal algorithm given available computational resources. automated optimization algorithms difﬁcult task. zilberstein claims bounded optimality difﬁcult achieve resulting successful examples practice promising bounded rational methods. however future research would interesting compare pac-rmdp possible relaxation pac-mdp based concept similar bounded optimality. deﬁnition pac-rmdp focus learning useful models enabling obtain high rewards short period time. instead wish guarantee worst total reward given time horizon several ways achieve goal. solution minimize expected -step regret bound given generated agent follows optimal policy sequence states generated agent follows policy since mistake early stages make impossible return optimal state sequence regret approaches literature rely reachability assumptions state space; example jaksch ortner auer assumed every state reachable every state within certain number steps. state visited algorithm time value functions inside deﬁned averaging -step regrets initial states visited average loss mitigates effects irreversible mistakes early stages dominate regret. lth−v deﬁnition pac-rmdp focuses exploration proposed pac-rmdp algorithms maintain anytime expected h-reachable average loss bounds anytime error bounds thus performances algorithms expected improve time rather number exploration steps. ﬁrst verify main properties algorithm analyze practically relevant property algorithm subsection discussion. assume algorithm used sample mean estimator determines main properties compare results past studies assume rmax ﬁxed constant effect assumption seen proof theorem algorithm requires input parameter related required degree optimism determined independently unknown aspect world. means theorem holds time execution pair corresponding lemma internal value used algorithm least h-reachable optimal value proof. claim follows directly construction algorithm veriﬁed induction step value iteration roll-out planning algorithm. theorem policy algorithm exists o|mdp|)) different event occurs difference furthermore ﬁnite horizon approximation adds error bounded rmax detailed argument involves algebraic manipulations mirror proofs given strehl littman kearns singh different state-action pairs last term right-hand side. probability least case according chernoff bound probability least event occur least times time steps. thus applying union bound probability least event occurring least times state-action pairs carefully consider means. whenever occurs sample used minimize error deﬁnition since holds time whenever occurs sample used reduce error holds randomly event must occur concurrently reduce error right-hand side). thus number time steps goes zero probability least hence union bound inequality becomes probability least t=\u0001thδ. since errors considered theorem corollary inﬁnite horizon factor translates inﬁnite horizon error -step ﬁnite horizon error weak. things noted. first reachable value function value function obtained additional sample requires additional sample |s||a| state-action pairs. second contrast bayesian optimality -reachable value function value function believed obtained |s||a| additional samples possibly reachable terms unknown true world dynamics samples. however certainly possible devise problem pac-rmdp guaranteed conduct sufﬁcient exploration. example consider modiﬁed version ﬁve-state chain problem probability successfully moving away initial state small thus requiring extensive exploration. modiﬁed transition model follows optimal action moves agent away initial state. action numerical results example shown figure expected pac-rmdp algorithm often became stuck initial state. weight vector therefore discuss h-reachability terms instead component best h-reachable model parameter corresponding best h-reachable models assume algorithm used least square estimation determines true world dynamics assumed parametric form known unknown aspect attributed using lth. note current model parameter. following make relatively strict assumption simplify analysis provided inputs estimated values correct satisfy assumption supsa| ˆθ)t assumption relaxed allowing correctness violated constant probability. case must force random event occur concurrently escape event discussed proof theorem furthermore specify inputs need assumption. lemma entire execution algorithm state components following inequality holds probability least lemma theorem following previous work assume exact planning algorithm accessible. assumption relaxed using planning method provides error bound. also assume rmax ﬁxed constants removing assumption results quantities appearing sample complexity produces exponential dependence assume meaning planing algorithm calls every iteration constant number times. following represent average value analyzing proposed algorithm re-derive sample complexity existing pac-mdp algorithm problem setting. lemma appropriate parameter setting pac-mdp algorithm proposed strehl littman following sample complexity proof. proof follows directly theorems previous work difference need take union bound different components varying domains codomains dimensions theorem policy algorithm proof. internal value function used algorithm prove statement following structure proof theorem deﬁne manner proof theorem vector consisting estimation error intervals ˆθ)t ˆθ)t following proof theorem probability least second line used assumption rmax ﬁrst line maxsa difference second line follows fact correctness shown lemma assignment optimistic value within interval impose upper bound following proof given assumption probability least latter case. then rest ﬁrst part statement follows proof theorem show applying chernoff bound escape event happens sample complexity statement probability unless term negligible. taking union bound failure probability obtain sample complexity statement probability finally consider second part statement following proof theorem future model parameter obtained updating current model random future samples based ﬁrst part proof ˆθh)t probability least since distance function implies everywhere else goal. start episode agent always bottom valley zero velocity. moreover small amount gaussian noise standard deviation added velocity. model uses grids residual basis functions control signal velocity features. planning phase ﬁtted value iteration grid residual basis functions. corresponding parameter pac-mdp algorithm velocity bounded episode consists steps conduct simulations episodes. simulated treatment problem described ordinary differential equations action corresponds whether agent administers treatments patients types exploration required learn effect using treatments viruses another learn effect using treatments immune systems. learning former necessary reduce population viruses latter required prevent overuse treatments weakens immune system. select initial state unhealthy following ernst pazis parr previous work assume noise-free data obtained every days. unlike past studies assume noisy data obtained instance noise-free data collected noise term another noise term represent model error dynamic state. model states multiple states features (i.e.", "year": 2016}