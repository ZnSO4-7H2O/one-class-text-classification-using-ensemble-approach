{"title": "Variational Reasoning for Question Answering with Knowledge Graph", "tag": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "Knowledge graph (KG) is known to be helpful for the task of question answering (QA), since it provides well-structured relational information between entities, and allows one to further infer indirect facts. However, it is challenging to build QA systems which can learn to reason over knowledge graphs based on question-answer pairs alone. First, when people ask questions, their expressions are noisy (for example, typos in texts, or variations in pronunciations), which is non-trivial for the QA system to match those mentioned entities to the knowledge graph. Second, many questions require multi-hop logic reasoning over the knowledge graph to retrieve the answers. To address these challenges, we propose a novel and unified deep learning architecture, and an end-to-end variational learning algorithm which can handle noise in questions, and learn multi-hop reasoning simultaneously. Our method achieves state-of-the-art performance on a recent benchmark dataset in the literature. We also derive a series of new benchmark datasets, including questions for multi-hop reasoning, questions paraphrased by neural translation model, and questions in human voice. Our method yields very promising results on all these challenging datasets.", "text": "knowledge graph known helpful task question answering since provides well-structured relational information entities allows infer indirect facts. however challenging build systems learn reason knowledge graphs based question-answer pairs alone. first people questions expressions noisy nontrivial system match mentioned entities knowledge graph. second many questions require multi-hop logic reasoning knowledge graph retrieve answers. address challenges propose novel uniﬁed deep learning architecture end-to-end variational learning algorithm handle noise questions learn multi-hop reasoning simultaneously. method achieves state-of-the-art performance recent benchmark dataset literature. also derive series benchmark datasets including questions multi-hop reasoning questions paraphrased neural translation model questions human voice. method yields promising results challenging datasets. question answering long-standing research problem machine learning artiﬁcial intelligence. thanks creation large-scale knowledge graphs dbpedia freebase systems armed well-structured knowledge speciﬁc open domains. many traditional approaches kg-powered based semantic parsers ﬁrst question formal meaning representation translate query. answer question retrieved executing query. disadvantages approaches model trained end-to-end errors cascaded. networks key-value memory networks gated graph sequence neural networks however neural approaches treat ﬂattened table itemized knowledge records making hard exploit structure information graph thus weak logic reasoning. answer direct neighbor topic entity question requires logic reasoning neural approaches usually perform poorly. instance easy handle single-hop questions like wrote paper titled ...? querying itemized knowledge records triples however logic reasoning required multihop questions co-authored papers ...? start mentioned author follow author authored remedy so-called knowledge graph completion create relations non-neighbor entity pairs however multi-hop reasoning combinatorial nature i.e. number multi-hop relations grow explosively increase hops. example create relation types like friend-of-friend friend-of-friend-of-friend number edges explode intractable storage computation. another challenge locate topic entities existing works assume topic entity question located simple string matching often true. people questions either text speech various noise introduced expressions. example people likely make typos name ambiguity question. even harder case audio questions people pronounce entity diﬀerently diﬀerent questions even person. noises hard exact matching locate topic entities. text questions broad matching techniques widely used entity recognition however require domain experts lots human eﬀort. speech questions even harder match topic entities directly. existing systems ﬁrst speech recognition converting audio text match entities text. unfortunately error rate typically high speech recognition system recognize entities voice human names street addresses. since end-to-end error speech recognition system cascade aﬀect downstream system. typically training data system provided question-answer pairs ﬁnegrained annotation pairs available available few. speciﬁcally explicit annotations exact entity present question type questions exact logic reasoning steps along knowledge graph leading answer. thus challenging simultaneously learn locate topic entity question ﬁgure unknown reasoning steps pointing answer based training question-answer pairs alone. address challenges mentioned above propose end-to-end learning framework question answering knowledge graph named variational reasoning network following features intended research question-answering systems. datasets contain questions singlemulti-hop reasoning. test systems realistic scenarios metaqa also provides neural-translation-model-paraphrased datasets text-to-speech-based audio datasets. extensive experiments show method achieves state-of-the-art performance singlemulti-hop datasets demonstrating capability multi-hop reasoning. moreover obtain promising results challenging audio datasets showing eﬀectiveness endto-end learning framework. rise virtual assistant tools systems even closer daily life. paper step towards realistic systems handle noisy question input text speech learn examples reason knowledge graph. semantic parser traditional approaches kg-powered based semantic parsers question certain meaning representation logical form directly question executable program approaches require domain-speciﬁc grammars rules ﬁne-grained annotations. also designed handle noisy questions support end-to-end training since separate stages question parsing logic reasoning. neural approaches family memory networks achieves state-of-the-art performance various kinds tasks. able reasoning within local context using attention mechanism miller achieves state-ofthe-art performance outperforming previous works benchmark datasets. recent work uses neural programmer model single knowledge table. however multi-hop reasoning capability approaches depends recurrent attentions explicit traversal graph embedding recently researchers built deep architectures embed structured data trees graphs also works extend sequential case like multi-step reasoning. however approaches work small instances like sentences molecules. instead work embeds reasoning-graph source entity every target entity large-scale knowledge graph. multi-hop reasoning works knowledge graph completion traversal requires path sampling dynamic programming work handle natural language human speech reasoning-graph embeddings represent complicated reasoning rules. summary existing approaches separate stages entity locating keyword matching frequency-based method domain-speciﬁc methods since jointly trained reasoning part errors entity locating cascaded downstream system. figure end-to-end architecture variational reasoning network question-answering knowledge graph. model consists probabilistic modules topic entity recognition logic reasoning knowledge graph respectively. inside knowledge base plate scope entity lost christmas illustrated colored ellipsoid plate corresponds reasoning graph leading potential answer colored yellow. reasoning graphs eﬃciently embedded scored question embeddings retrieve best answer. training handle relations represented nodes edges respectively i.e. furthermore edge triplet representing directed relation subject node entity knowledge graph entity also contain additional information type text description. instance entity described actor jennifer lawrence entity movie passengers. relation knowledge graph corresponding acted work assume knowledge graph given. question answering given question algorithm asked output entity knowledge graph properly answers question. example question like acted movie passengers? possible answer jennifer lawrence entity challenging setting even audio segment reading contains pairs question answers. note ﬁne-grained annotation present exact entity present question question type exact logic reasoning steps along knowledge graph leading answer. thus system able handle noisy entity questions learn multi-hop reasoning directly question-answer pairs. modules described below. module topic entity recognition recognizing topic entity ﬁrst step performing logic reasoning knowledge graph. example topic entity mentioned movie passenger. denote topic entity found module logic reasoning knowledge graph given topic entity question need reason knowledge graph answer described algorithm learn reasoning rule question. since annotations reasoning step system learn question-answer pairs. thus model likelihood answer correct given entity question sums possibilities latent variable. given training dtrain questionanswer pairs parameters estimated maximizing log-likelihood latent variable model existing approaches assume topic entities annotated simply found string matching. however realistic questions even audio questions general approach build recognizer trained jointly logic reasoning engine. handle unlabeled topic entities notice full context question helpful. example michael could either name movie actor. hard tell relates question merely looking entity name. however able resolve unique entity checking surrounding words question. similarly knowledge graph could multiple entities name connected edges entity nodes diﬀerent helps resolve unique entity. example movie name michael connected directed edge pointing entity director; actor name michael connected birthday height edges. retrieving answer requires multi-step traversal gigantic graph. thus paper propose reasoning-graph embedding architecture inference rules complex combinations represented nonlinear embeddings vector space learned. scope speciﬁcally assume maximum number steps logic reasoning known algorithm. starting topic entity perform topological sort entities within hops according knowledge graph. that ordered list entities relations reasoning graph given potential answer scope denote gy→a minimum subgraph contains paths actual logic reasoning learn vector representation gy→a denoted scoring speciﬁcally suppose question embedded using neural network depending number hops. computing embeddings separately computationally expensive. instead develop neural architecture compute embeddings jointly share intermediate computations. joint embedding reasoning graphs. speciﬁcally propose forward graph embedding architecture analogous forward ﬁltering hidden markov model bayesian network. embedding reasoning graph computed recursively using parents’ embeddings one-hot encoding relation type model parameters nonlinear function relu parent counts number parents boundary case overall computing embedding takes |e|) time proportional number nodes edges scope formulation able capture various reasoning rules. take example embedding entity killing softly sums embeddings propagated parents. thus tends match reasoning paths parent entities. note formulation signiﬁcantly diﬀerent work embedding computed small molecular graph separately. furthermore graph embedding methods often contain iterative processes visit nodes multiple times. algorithm often used learn latent variable models. however performing exact updates objective intractable since posterior cannot computed closed form. instead variational inference optimize negative helmholtz variational free energy essentially optimizing lower bound thus reduce approximation error powerful posterior distributions necessary. variational posterior. computes likelihood topic entity question additional information answer thus besides direct text acoustic compatibility also introduce logic match help similar forward propagation architecture used deﬁne scope answer inverse reasoning graph ga→y inverse embedding architecture eﬃciently compute embedding finally variational posterior consists parts since latent variable variational objective takes discrete values diﬀerentiable respect reinforce algorithm variance reduction tackle problem. second reduce variance gradient center normalize signal also subtract baseline function finally gradient approximated monte carlo method using samples latent variable estimate mean standard deviation moving average. another neural network expected normalized learning signal. experiments simply build two-layer perceptron concatenated one-hot answer question features. minimizing square loss. parameters several limitations questions single-hop thus able evaluate ability reasoning; noise topic entity question easily located knowledge graph; generated limited number text templates easy exploited models limited practical value. small datasets like webquestions mostly single-hop questions; wikitablequestions involves tiny knowledge table question instead large-scale knowledge graph shared among questions. thus paper introduce challenging question-answer benchmark metaqa contains questions single multi-hop reasoning provides realistic text audio versions. metaqa serves comprehensive extension wikimovies. page limit brieﬂy list datasets included metaqa below details appendix introduce variations vanilla datasets. trained dual learning techniques paraphrase question ﬁrst translating english french sample translations back english beam search. questions dataset diﬀerent wordings keep meaning. dataset also contains -hop -hop -hop categories. help text-to-speech system. google service read questions vanilla. also provide extracted mfcc features question. audio dataset also contains -hop -hop -hop categories. note although audio machinegenerated still much less regulated compared text-template-generated data variations waveforms. example even word system diﬀerent intonations depending word position question context words. visualization audio data found appendix three competitor methods discussed miller proposed key-value memory networks reported state-of-the-art results time wikimovies; bordes system also tries embed inference subgraph reasoning representation simply unordered bag-of-relationships neighbor entities; supervised embedding considered another baseline method simple approach often works surprisingly well reported dodge implement baseline methods tensorﬂow results vanilla -hop consistent reported performance take whichever higher report table example kv-memnn obtains test accuracy original paper reports dataset report table. training kv-memnn number internal hops number dataset. also internal hops dataset number helpful. also insert knowledge items within hops located topic entity memory slots ensures topic entity correctly matched answer existing somewhere memory array. datasets metaqa experiments. follow split train/validation/test datasets. number questions part listed appendix tune hyperparameters validation methods. vanilla bag-of-words representation entity name parameterize vanilla diﬀerent settings provide entity labels questions compare kv-memnn setting miller vanilla -hop dataset; provide entity labels among questions named vanilla-eu make methods bag-of-words representation question avoid hard entity matching. setting sanity check much method make task realistic challenging experiment setting audio datasets. ntm-eu topic entity labels among questions provided. audio-eu higher labeled ratio since much diﬃcult text data. handle variant length audio questions simple convolutional neural network three convolutional layers three max-pooling layers embed audio questions ﬁxed-dimension vectors. details embedding appendix setting above small entity labeled questions used initialize topic entity recognizer. that methods train entire dataset without entity labels. show pretrained recognizer also improved variational joint training; baselines entity recognizer ﬁxed. experimental results listed table table vanilla since topic entities labeled vanilla mainly evaluates ability logic reasoning. note vanilla -hop wikimovies included sanity check. baseline methods achieve similar performance reported original papers method performs best. clear -hop questions harder leading signiﬁcant accuracy drop methods. nevertheless method still achieves promising results lead competitors large margin. notice kv-memnn performing well multi-hop reasoning perhaps explosion relevant knowledge items. vanilla-eu without topic entity labels reasoning-based methods getting worse multihop questions. however supervised embedding gets better case since learns remember pair question answer entities. according statistics appendix portion questions answered memorizing pairs training data. explains supervised embedding behaves diﬀerently dataset. ntm-eu questions dataset paraphrased neural translation model increases variety wordings makes task harder. reasonable methods getting slightly worse results compared vanilla-eu. explanation applies supervised embedding reasoning memorizing pairs. indeed weak generalization takes advantage nature dataset likely perform well entity pairs. audio-eu audio dataset challenging one. mentioned even word pronounced variety intonations. hard recognize entity audio data also hard tell question type. surprising methods perform worse compared text data. method achieves -hop audio questions promising. -hop -hop questions method still outperforms methods. clearly large room improvement audio leave future work hopefully metaqa benchmark facilitate researchers working systems. reasoning graph embedding approach necessary inference? variational method helpful joint training? importance reasoning graph embedding results shown table proposed outperforms baselines especially -hop setting. since experiment compares reasoning ability clearly shows simply representing inference rule linear combination reasoning graph entities enough. improvement entity recognition joint training show using joint training framework variance reduction reinforce improve entity recognition performance without corresponding topic entity label supervision. -hop -hop questions model improve greatly. -hop since inference task much harder marginally improve performance. audio data we’ve improved -hop case hard improve multi hops. table baselines perform signiﬁcantly worse setting absence joint training. study convergence learning algorithm appendix shows variance reduction technique helps convergence signiﬁcantly simpler tasks converge better. also present example inference path highest score reasoning graph appendix answer what main languages david mandel ﬁlms? model learns movie eurotrip ﬁrst directed wrote relationships follow language correct answer german. visualizing general multi-hop reasoning attention mechanism aggregation operator node would helpful.", "year": 2017}