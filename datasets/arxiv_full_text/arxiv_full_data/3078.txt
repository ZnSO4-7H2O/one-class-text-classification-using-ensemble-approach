{"title": "Stacked Cross Attention for Image-Text Matching", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "In this paper, we study the problem of image-text matching. Inferring the latent semantic alignment between objects or other salient stuffs (e.g. snow, sky, lawn) and the corresponding words in sentences allows to capture fine-grained interplay between vision and language, and makes image-text matching more interpretable. Prior works either simply aggregate the similarity of all possible pairs of regions and words without attending differentially to more and less important words or regions, or use a multi-step attentional process to capture limited number of semantic alignments which is less interpretable. In this paper, we present Stacked Cross Attention to discover the full latent alignments using both image regions and words in sentence as context and infer the image-text similarity. Our approach achieves the state-of-the-art results on the MS-COCO and Flickr30K datasets. On Flickr30K, our approach outperforms the current best methods by 22.1% in text retrieval from image query, and 18.2% in image retrieval with text query (based on Recall@1). On MS-COCO, our approach improves sentence retrieval by 17.8% and image retrieval by 16.6% (based on Recall@1 using the 5K test set).", "text": "abstract. paper study problem image-text matching. inferring latent semantic alignment objects salient stuﬀs corresponding words sentences allows capture ﬁne-grained interplay vision language makes image-text matching interpretable. prior works either simply aggregate similarity possible pairs regions words without attending diﬀerentially less important words regions multi-step attentional process capture limited number semantic alignments less interpretable. paper present stacked cross attention discover full latent alignments using image regions words sentence context infer image-text similarity. approach achieves state-of-the-art results ms-coco flickrk datasets. flickrk approach outperforms current best methods text retrieval image query image retrieval text query ms-coco approach improves sentence retrieval image retrieval people describe observed descriptions make frequent reference objects salient stuﬀ images well attributes actions sense sentence descriptions weak annotations words sentence correspond particular unknown regions image. inferring latent correspondence image regions words interpretable image-text matching capturing ﬁne-grained interplay vision language. similar observations motivated prior work image-text matching models often detect image regions object/stuﬀ level simply aggregate similarity possible pairs image regions words sentence fig. sentence descriptions make frequent reference particular unknown salient regions images well attributes actions. reasoning underlying correspondence interpretable image-text matching. infer global image-text similarity; e.g. karpathy fei-fei proposed taking maximum region-word similarity respect word averaging results corresponding words. shows eﬀectiveness inferring latent region-word correspondences aggregation consider fact importance words depend visual context. strive take step towards attending diﬀerentially important image regions words context inferring image-text similarity. introduce novel stacked cross attention enables attention context image sentence stages. image text formulation ﬁrst attends words sentence respect image region compares image region attended information sentence decide importance image regions likewise text image formulation ﬁrst attend image regions respect word decide less attention word. compared models perform ﬁxed-step attentional reasoning focus limited semantic alignments stacked cross attention discovers possible alignments simultaneously. since number semantic alignments varies diﬀerent images sentences correspondence inferred method comprehensive thus making image-text matching interpretable. identify salient regions image follow anderson analogize detection salient regions object/stuﬀ level spontaneous bottom-up attention human vision system practically implement bottom-up attention using faster r-cnn represents natural expression bottom-up attention mechanism. uate performance approach comparison architectures perform comprehensive ablation studies look ms-coco flickrk datasets. model stacked cross attention network uses proposed attention mechanism achieves state-of-the-art results. flickrk approach outperforms current best methods text retrivel image query image retrieval text query ms-coco improves sentence retrieval image retrieval rich line studies explored mapping whole images full sentences common semantic vector space image-text matching kiros made ﬁrst attempt learn cross-view representations hingebased triplet ranking loss using deep convolutional neural networks encode images recurrent neural networks encode sentences. faghri leveraged hard negatives triplet loss function yielded signiﬁcant improvement. peng suggested incorporating generative objectives cross-view feature embedding learning. opposed proposed method works consider latent vision-language correspondence level image regions words. specifically discuss lines research addressing problem using attention mechanism follows. image-text matching bottom-up attention. bottom-up attention terminology anderson proposed work image captioning visual question-answering referring purely visual feedforward attention mechanisms analogy spontaneous bottom-up attention human vision system similar observation motivated study several works karpathy fei-fei proposed detecting encoding image regions object level r-cnn inferring image-text similarity aggregating similarities between possible region-word pairs. presented model maps noun phrases within sentences objects images shared embedding space full sentences whole images embeddings. huang combined image-text matching sentence generation model learning improved image representation including objects properties actions etc. contrast model studies conventional attention mechanism learn focus image regions given semantic context. conventional attention-based methods. attention mechanism focuses certain aspects data respect task-speciﬁc context computer vision visual attention aims focus speciﬁc images subregions similarly attention methods natural language processing adaptively select aggregate informative snippets infer results recently attention-based models proposed image-text matching problem. huang developed context-modulated attention scheme selectively attend pair instances appearing image sentence. similarly proposed dual attentional network capture ﬁne-grained interplay vision language multiple steps. however models adopt multi-step reasoning predeﬁned number steps look semantic matching time despite number semantic matchings change diﬀerent images sentence descriptions. contrast proposed model discovers latent alignments thus interpretable. section describe stacked cross attention network objective words sentence image regions common embedding space infer similarity whole image full sentence. begin bottom-up attention detect encode image regions features. also words sentence along sentence context features. apply stacked cross attention infer image-sentence similarity aligning image region word features. ﬁrst introduce stacked cross attention section objective learning alignments section detail image sentence representations section section respectively. word sentence. output similarity score measures similarity image-sentence pair. nutshell stacked cross attention attends diﬀerentially image regions words using context inferring similarity. deﬁne complimentary formulations stacked cross attention below image-text text-image. image-text stacked cross attention. formulation illustrated figure entailing stages attention. first attends words sentence respect image region. second stage compares image region corresponding attended sentence vector order determine importance image regions respect sentence. speciﬁcally given image detected regions sentence words ﬁrst compute cosine similarity matrix possible pairs i.e. fig. image-text stacked cross attention stage ﬁrst attend words sentence respect image region feature generate attended sentence vector determine importance image regions compute similarity score. determine importance image regions given sentence context deﬁne relevance i-th region sentence using cosine similarity attended sentence vector image region feature i.e. fig. text-image stacked cross attention stage ﬁrst attend image regions respect word feature generate attended image vector j-th word sentence stage compare determine importance image regions compute similarity score. essentially region mentioned sentence feature would similar corresponding attended sentence vector since would able collect good information computing thus comparing determine important region respect sentence. text-image stacked cross attention. likewise ﬁrst attend image regions respect word compare word corresponding attended image vector determine importance words. call formulation text-image depicted figure speciﬁcally normalize cosine similarity i-th region j-th word similarity score function ﬁrst taken negative sentences given image second considers negative images given sentence closer another joint embedding space negatives pairs margin hinge loss zero. practice computational eﬃciency rather summing negative samples usually considers hard negatives mini-batch stochastic gradient descent. study focuses hardest negatives mini-batch following fagphri positive pair hardest negatives given argmaxm=i argmaxd=t therefore deﬁne triplet loss deﬁnition image region generic. however study focus regions level object entities. following anderson refer detection salient regions bottom-up attention practically implement faster r-cnn faster r-cnn two-stage object detection framework. ﬁrst stage region proposal network grid anchors tiled space scale aspect ratio used generate bounding boxes region interests high objectness scores second stage representations rois pooled intermediate convolution feature regionwise classiﬁcation bounding regression. multi-task loss considering classiﬁcation localization minimized ﬁnal stages. adopt faster r-cnn model conjunction resnet- pretrained anderson visual genomes order learn feature representations rich semantic meaning instead predicting object classes model predicts attribute classes instance classes instance classes contain objects salient stuﬀs diﬃcult localize selected region deﬁned mean-pooled convolutional feature region dimension image feature vector fully-connect layer transform h-dimensional vector connect domains vision language would like language h-dimensional semantic vector space image regions. given sentence simplest approach mapping every word individually. however approach consider semantic context sentence. therefore employ embed words along context. bi-directional vector ﬁnal word feature along sentence context summarizing information directions sentence. bi-directional contains forward reads sentence carry extensive experiments evaluate stacked cross attention network compare various formulations scan stateof-the-art approaches. also conduct ablation studies incrementally verify approach thoroughly investigate behavior scan. common information retreival measure performance sentence retrieval image retrieval recall deﬁned fraction queries correct item retrieved closest points query. hyperparameters scan selected validation set. evaluate approach ms-coco flickrk datasets. flickrk contains images collected flickr website captions each. following split images validation images testing rest training. ms-coco contains images image annotated text descriptions. dataset split training images validation images test images. follow images originally validation ms-coco left split training set. image comes captions. results reported either averaging folds test images testing full test images. note early works training containing images. visual bottom-up attention faster r-cnn model conjunction resnet- pre-trained extract rois image. faster r-cnn implementation uses intersection union threshold region proposal suppression object class suppression. select salient image regions class detection conﬁdence threshold used. rois highest conﬁdence scores selected following extracted features average pooling resulting ﬁnal representation dimensions. table comparison cross-modal retrieval restuls terms recallk flickrk. denotes text-image. denotes image-text. denotes average logsumexp pooling respectively. table presents quantitative results flickrk formulations proposed method outperform recent approaches measures. denote text-image formulation image-text formulation logsumexp pooling average pooling avg. best sentence retrieval given image query achieved scan improvement comparing furthermore combine models averaging predicted similarity scores. possible combinations single models. best result model ensembles achieved combining selected validation set. combined model gives image retrieval improvement current state-of-the-art assumption diﬀerent formulations stacked cross attention approach diﬀerent aspects data model ensemble improves results. table lists experimental results ms-coco comparison prior work. test single scan achieves comparable results current state-of-the-art sco. best result test achieved combining improves image query comparing sco. test choose list best single model ensemble selected validation space limitation. table comparison cross-modal retrieval restuls terms recallk ms-coco. denotes text-image. denotes image-text. denotes average logsumexp pooling respectively. begin with would like incrementally validate approach revisiting basic formulation inferring latent alignments image regions words without attention; i.e. sum-max text-image proposed compliment sum-max image-text summax models adopt learning objectives hard negatives sampling bottom-up attention-based image representation sentence representation scan. diﬀerence simply aggregates similarity scores possible pairs image regions words. results comparison presented table vse++ matches whole images full sentences table eﬀect inferring latent vision-language alignment level regions words. results reported terms recallk. refer eqs. deﬁnition sum-max. denotes text-image. denotes image-text. table eﬀect diﬀerent scan conﬁgurations flickrk. results reported terms recallk. denotes image-text. denote summation pooling instead avg/lse pooling step respectively. single embedding vector. used pre-deﬁned resnet- trained imagenet extract feature image training also leveraged hard negatives sampling scan. essentially represents case without considering latent correspondence keeping conﬁgurations similar sum-max models. comparing sum-max vse++ eﬀectiveness inferring latent alignments. better bottom-up attention model sum-max even outperforms current state-of-the-art flickrk. comparing scan sum-max models show stacked cross attention improve performance signiﬁcantly. investigate several diﬀerent conﬁgurations scan baseline model present results table experiment performed alternation. observed gain obtain hard negatives triplet loss signiﬁcant model improving model terms sentence retrieval normalizing image embedding changes importance image sample scan signiﬁcantly aﬀected factor. summing taking maximum instead average logsumexp pooling similarity scores attended sentence vector image region features yields weaker results. finally fig. visualization attended image regions respect word sentence description outlining region maximum attention weight red. regional brightness represents attention strength considers importance region word estimated model. model generates interpretable focus shift stresses words like tennis racket well attributes actions visualizing attention component learned model able showcase interpretablity model. figure qualitatively present attention changes predicted text-image model. selected image visualize attention weights respect word sentence description young holding tennis racket. diﬀerent sub-ﬁgures. regional brightness represents attention weights considers importance region word corresponding sub-ﬁgure. observe holding tennis racket receive strong focused attention relatively precise locations attention weights corresponding weaker less focused. shows attention component learns interpretable alignments image regions words able generate reasonable focus shift attention strength weight regions words importance inferring image-text similarity. figure shows qualitative results sentence retrieval given image queries flickrk. image query show top- retrieved sentences ranked similarity scores predicted model. figure illustrates qualitative results image retrieval given sentence queries flickrk. sentence corresponds ground-truth image. sentence query show top- retrieved images ranking left right. outline true matches green false matches red. fig. qualitative results sentence retrieval given image queries flickrk dataset. image query show top- ranked sentences. observe stacked cross attention model retrieves correct results ranked sentences even image queries complex cluttered scenes. model outputs reasonable mismatches e.g. hand incorrect results possibly poor detection action static images. fig. qualitative results image retrieval given sentence queries flickrk. sentence query show top- ranked images ranking left right. outline true matches green boxes false matches boxes. examples show model retrieves ground truth image top- list. note results also reasonable outputs. propose novel stacked cross attention mechanism gives state-ofthe-art performance flickrk ms-coco datasets measures. carry comprehensive ablation studies verify stacked cross attention essential performance image-text matching revisit prior work conﬁrm importance inferring latent correspondence image regions words sentences. furthermore show learned stacked cross attention leveraged give interpretablity vision-language models.", "year": 2018}