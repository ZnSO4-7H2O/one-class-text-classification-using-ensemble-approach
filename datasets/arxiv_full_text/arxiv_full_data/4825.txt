{"title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "tag": ["cs.LG", "cs.AI"], "abstract": "For complex, high-dimensional Markov Decision Processes (MDPs), it may be necessary to represent the policy with function approximation. A problem is misspecified whenever, the representation cannot express any policy with acceptable performance. We introduce IHOMP : an approach for solving misspecified problems. IHOMP iteratively learns a set of context specialized options and combines these options to solve an otherwise misspecified problem. Our main contribution is proving that IHOMP enjoys theoretical convergence guarantees. In addition, we extend IHOMP to exploit Option Interruption (OI) enabling it to decide where the learned options can be reused. Our experiments demonstrate that IHOMP can find near-optimal solutions to otherwise misspecified problems and that OI can further improve the solutions.", "text": "complex high-dimensional markov decision processes necessary represent policy function approximation. problem misspeciﬁed whenever representation cannot express policy acceptable performance. introduce ihomp approach solving misspeciﬁed problems. ihomp iteratively learns context specialized options combines options solve otherwise misspeciﬁed problem. main contribution proving ihomp enjoys theoretical convergence guarantees. addition extend ihomp exploit option interruption enabling decide learned options reused. experiments demonstrate ihomp near-optimal solutions otherwise misspeciﬁed problems improve solutions. reinforcement learning algorithms learn near-optimal solutions well-deﬁned problems. however real-world problems rarely come form concrete problem description. human translate poorly deﬁned target problem concrete problem description. misspeciﬁed problem occurs optimal solution problem description inadequate target problem. unfortunately creating well-deﬁned problem description challenging art. furthermore serious consequences many domains ranging smart-grids robotics inventory management systems paper introduce hierarchical approach mitigates consequences problem misspeciﬁcation. problems often described markov decision processes solution function generates action presented current state called policy. solution policy maximizes long term rewards. problems continuous high dimensional state-spaces explicitly representing policy infeasible thus remainder paper restrict discussion linearly parametrized policy representations problems misspeciﬁed? problem description misspeciﬁed many reasons important case state representation. well established machine learning literature good features dramatic impact performance. finding good features represent state challenging domain speciﬁc problem generally considered outside scope unfortunately domain experts supply useful features either fully understand target problem technicalities reinforcement learning. addition prefer limited state representation several reasons regularization wish limited feature representation improve generalization avoid overﬁtting memory system constraints ﬁnite number features used computational constraints real-time systems querying feature take long. physical systems sensor required measure desired feature prohibitively expensive. learning large data learning large amounts data augmenting feature features improved performance non-trivial often inefﬁcient mitigate misspeciﬁcation? learning hierarchical policy mitigate problems associated contrast policy approach single parameterized policy used solve entire mdp. illustrate learning hierarchical policy repair consider s-shaped domain shown figure solve task agent must move bottom left corner goal region denoted letter right. state representation permits policies move straight line. problem misspeciﬁed solvable policy approach however break state-space shown figure a.ii learn policy cell problem solvable. partial policies shown figure a.ii example abstract actions called options macro-actions skills learning useful options topic intense research however previous approaches proposed algorithms learning options learn plan faster. contrast objective learn options repair proposed algorithm introduce meta-algorithm iterative hierarchical optimization misspeciﬁed problems uses algorithm black iteratively learn options repair mps. force options specialize ihomp uses partition state-space trains option class partition arbitrary partitioning scheme used however partition impacts performance. iteration ihomp algorithm updates option. options initialized arbitrarily ﬁrst iteration options access goal region non-zero rewards learn exploit rewards iterations newly acquired options propagate reward back regions state-space. thus options previously reward signal exploit rewards options received meaningful reward signals although option learned single partition class initialized state. partitions? options trained data options would specialize defeating purpose learning multiple policies. partitions necessary foster specialization. natural partitionings arise many different applications often easy design. consider navigation tasks ever-present robotics partitions naturally lead agent location another state space. addition partitions well suited cyclical tasks; tasks repeatable cycles state space easily partitioned based time. examples include inventory management systems well maintenance scheduling generation units transmission lines smart grids automatically learning partitions availability pre-deﬁned partitioning state space strong assumption domains. developed relaxation assumption enable partitions learned automatically using regularized option interruption figure episodic s-shaped state-space goal region flat approach single policy failing solve entire task resulting misspeciﬁed model hierarchical approach using combining simple policy representations solve task. learning options denoted black arrows. domain partitioned classes resulting option-set iteration options except arbitrary. iteration propagates reward back process repeats useful options learned entire state-space. contributions main contributions introducing iterative hierarchical optimization misspeciﬁed problems learns options repair solve mps. theorem shows ihomp converges near-optimal solution relating quality learned policy quality options learned black algorithm. theorem proves regularized option interruption safely incorporated ihomp. experiments demonstrating that given misspeciﬁed problem ihomp learn options repair solve problem. experiments showing ihomp-roi learning partitions discovering reusable options. divide-and-conquer approach also enable scale solve larger mdps. background states ﬁnite actions mapping state-action pairs probability distributions next states maps state-action pair reward discount factor. policy gives probability executing action state mdp. value function policy respect state learning options option typically deﬁned triple however want learn options specialized speciﬁc regions state-space potentially reusable useful general contexts. focus special case options option deﬁned tuple parametric policy parameter vector indicates whether option ﬁnished given current state given options size inter-option policy deﬁned state-space index options inter-option policy selects options execute current state returning index options. deﬁning inter-option policies select index policy even options adapting. figure shows arbitrary partitioning consisting sub-partitions {pi|i deﬁned original mdp’s state space. initialized arbitrary option corresponding local-mdp episodic terminates agent escapes upon terminating receives reward equal value state agent would transitioned original mdp. therefore construct modiﬁed called local-mdp apply planning algorithm solve resulting solution specialized option. given good options planning signiﬁcantly faster however many domains given good options. therefore necessary learn improve options. next section introduce algorithm dynamically learning improving options using iterative hierarchical optimization. iterative hierarchical optimization misspeciﬁed problems takes original partition state-space number iterations returns pair containing inter-option policy options number options equal number classes partition inter-option policy returned ihomp deﬁned maxi∈ i{·} indicator function returning argument true otherwise denotes class partition thus simply returns index option associated partition class containing current state. line ihomp initializes arbitrary options algorithm iterative hierarchical optimization misspeciﬁed problems require m{mdp} p{partitioning k{iterations} partitions.} maxi∈ initialize options. option partition.} iterations.} return next ihomp performs iterations. iteration ihomp updates options note value option depends combined options. allowed options change simultaneously options could reliably propagate value other. therefore ihomp updates option individually. multiple iterations needed option converge process updating option starts evaluating current option-set number policy evaluation algorithms could used here function approximation lstd modiﬁed used options. experiments used straightforward variant lstd original construct local-mdp next ihomp uses planning algorithm approximately solve local-mdp returning parametrized policy planning algorithm regular mdps could role provided produces parametrized policy. however experiments used simple actor-critic algorithm created policy unless otherwise stated. option otherwise deﬁnition means option derived line terminate leaves partition. finally update option replacing provide ﬁrst convergence guarantee combining hierarchically iteratively learning options continuous state using ihomp guarantee well lemma prove theorem theorem enables analyze quality inter-option policy returned ihomp. turns quality policy depends critically quality option learning algorithm. important parameter determining quality policy returned ihomp misspeciﬁcation error deﬁned below. deﬁnition partition target mdp’s state-space. misspeciﬁcation error policy misspeciﬁcation error quantiﬁes quality local-mdp solutions returned option learning algorithm. used exact solver learn options however approximate solver non-zero quality depend partition generally using ﬁner grain partitions decrease however theorem reveals adding many options also negatively impact returned policy’s quality. theorem ihomp partition logγ iterations algorithm returns stitching policy proof theorem divided three parts main challenge updating option impact value options. analysis starts bounding impact updating option. note represents option represents option updated option set. ﬁrst part show error globally optimal contraction bound value function otherwise second part apply inductive argument show updating options results contraction entire state space third part apply contraction recursively proves theorem provides ﬁrst theoretical guarantees convergence near optimal solution combining hierarchically iteratively learning options continuous state mdp. theorem tells misspeciﬁcation error small ihomp returns near-optimal inter-option policy. ﬁrst term right hand side approximation error. loss parametrized class policies learn options over. since represents number classes deﬁned partition formal analyzing effect partitioning structure. addition complex options need designed domain expert; partitioning needs provided a-priori. second term convergence error. goes number iterations increases. guarantee provided theorem appear similar however hauskrecht derive options beginning learning process update them. hand ihomp updates option-set dynamically propagating value throughout state space iteration. thus ihomp require prior knowledge optimal value function. theorem explicitly present effect policy evaluation error occurs approximate policy evaluation technique. however policy evaluation error bounded simply replace again smaller policy evaluation error leads smaller approximation error. ihomp assumed partition given a-priori. however non-trivial design partition many cases partition sub-optimal. relax assumption incorporate regularized option interruption work enable ihomp automatically learn near-optimal partition initially misspeciﬁed problem. ihomp keeps track action value function represents expected value state executing option given inter-option policy option uses estimate action-value function enable agent choose switch options according following termination rule corresponds termination probability option partition maxi∈ qµσ. rule illustrated figure user designed partition resulting compared optimal partition domain ihomp applies ‘modify’ initial partition optimal one. learning optimal action-value function q∗µσ ihomp builds near-optimal partition implicitly stored within action-value function. agent executing option partition class value continuing option less regularization function switch option partition otherwise continue executing current option leads algorithm ihomp-roi algorithm found supplementary material. difference ihomp ihomp-roi applying policy evaluation step options updated. ihomp-roi automatically learns improved partition iterations. show safely incorporated ihomp theorem theorem shows incorporating improve policy produced ihomp. full proof given supplementary material. figure regularized option interruption initial misspeciﬁed partition pre-deﬁned user. actual optimal partitioning task. partition learned using roi. option executing option continue execute location terminate location point option begin execute. performed experiments three well-known benchmarks mountain puddle world pinball domain also perform experiments sub-domain minecraft minecraft domains similar results therefore moved supplementary material. variations pinball domain namely maze-world created pinball-world standard pinball benchmark domains. finally created domain call rooms domain demonstrate ihomp-roi improve partitions. experiment deﬁned policy adequate tasks cannot solve task all. experiments simulate situations policy representation figure puddle world domain. average reward ihompcompared solution obtained policy initially misspeciﬁed problem approximately optimal policy derived using q-learning. average cost grid partition. constrained avoid overﬁtting manage system constraints coping poorly designed features. case ihomp learns signiﬁcantly better policy compared non-hierarchical approach. rooms domain ihomp-roi improves initial partition. experiments demonstrate potential scale higher dimensional domains hierarchically combining options simple representations ihomp meta-algorithm. provide algorithm policy evaluation policy learning domains used smdp-lstd modiﬁed version regular-gradient actor-critic pinball domains used nearest-neighbor function approximation random policy search rooms domain variation lstdq option interruption rg-ac rooms domains intra-option policy represented probability distribution actions compare performance original misspeciﬁed problem using policy representation. grid-like partitions generated task. binary-grid features used estimate value function. pinball domains option represented polynomial features corresponding state dimension bias term. value function represented kd-tree containing state-value pairs uniformly sampled domain. value particular state obtained assigning value nearest neighbor state contained within kd-tree. example representations. principal value function policy representation representative domain utilized. puddle world puddle world continuous -dimensional world containing puddles shown figure successful agent navigate goal location avoiding puddles. state space location agent. initially agent provided misspeciﬁed problem. policy move single direction figure compares policy ihomp policy achieves average reward. however ihomp turns policy options hierarchically composes options together resulting richer solution space higher average reward seen figure comparable approximately optimal average reward attained executing approximate value iteration huge number iterations. experiment ihomp initiated partition class containing goal state still achieves near-optimal convergence iterations. figure compares performance different partitions grid represents policy initially misspeciﬁed problem. option learning error signiﬁcantly smaller partitions greater resulting lower cost. hand according theorem adding options increases cost. trade therefore exists practice tends dominate addition trade importance partition design evident analyzing cost grids. scenario partition design better suited puddle world partition resulting lower cost. pinball tested ihomp challenging pinball-world task konidaris barto agent initially provided -feature policy results misspeciﬁed problem agent unable solve task using limited representation shown average reward figure using ihomp grid options figure rooms domain rooms domain sub-optimal partition performing ihomp learned partition running ihomp-roi average reward executing ihomp-roi compared ihomp without policy. learned. ihomp clearly outperforms policy shown figure less optimal still manages sufﬁciently perform task drop performance complicated obstacle setup non-linear dynamics partition design. nevertheless shows ihomp produce reasonable solution limited representation. improving partitions providing ‘good’ option partitioning a-priori strong assumption. non-trivial design partitioning especially continuous high-dimensional domains. sub-optimal partitioning still mitigate misspeciﬁcation result near-optimal solution. relax assumption incorporated regularized option interruption ihomp produce ihomp-roi algorithm. algorithm learns options improves partition effectively determining options executed state space. tested ihomp-roi rooms domain shown figure agent needs navigate goal region policy parameterization limited distribution actions limited representation results agent unable traverse rooms. ihomp sub-optimal partitioning containing options shown green cells figure problem still misspeciﬁed. here agent leaves cell immediately gets trapped behind wall whilst green cell. using ihomp-roi shown figure agent learns options partition agent navigate goal. green region bottom left corner comes function approximation error prevent agent reaching goal. reader looks carefully figure notice something unexpected. optimal partitioning learned rooms domain includes executing option region intuitive given parameterizations learned options. option dominant right action whereas green option dominant upward action. agent region makes sense execute option reach goal. thus ihomp-roi provides effective learn optimal partition also discover options reused. discussion introduced ihomp planning algorithm iteratively learning options inter-option policy repair provide theoretical results ihomp directly relate quality ﬁnal inter-option policy misspeciﬁcation error. ihomp ﬁrst algorithm provides theoretical convergence guarantees iteratively learning options continuous state space. addition developed ihomp-roi makes regularized option interruption learn improved partition solve initially misspeciﬁed problem. ihomp-roi also able discover regions state space options reused. high-dimensional domains partitions learned expert demonstrations intra-option policies amir abiri-jahromi masood parvania francois bouffard mahmud fotuhi-firuzabad. two-stage framework power transformer asset maintenance management—part models formulations. power systems ieee transactions milos hauskrecht nicolas meuleau leslie pack kaelbling thomas dean craig boutilier. hierarchical solution markov decision processes using macro-actions. proceedings conference uncertainty pages kobi levi yair weiss. learning object detection small number examples importance good features. computer vision pattern recognition cvpr proceedings ieee computer society conference volume pages ii–. ieee", "year": 2016}