{"title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement  Learning with a Stochastic Actor", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.", "text": "model-free deep reinforcement learning algorithms demonstrated range challenging decision making control tasks. however methods typically suffer major challenges high sample complexity brittle convergence properties necessitate meticulous hyperparameter tuning. challenges severely limit applicability methods complex real-world domains. paper propose soft actor-critic offpolicy actor-critic deep algorithm based maximum entropy reinforcement learning framework. framework actor aims maximize expected reward also maximizing entropy—that succeed task acting randomly possible. prior deep methods based framework formulated q-learning methods. combining off-policy updates stable stochastic actor-critic formulation method achieves state-of-the-art performance range continuous control benchmark tasks outperforming prior on-policy off-policy methods. furthermore demonstrate that contrast off-policy algorithms approach stable achieving similar performance across different random seeds. model-free deep reinforcement learning algorithms applied range challenging domains games robotic control combination high-capacity function approximators neural networks holds promise automating wide range decision making control tasks widespread adoption methods real-world domains hampered major challenges. first model-free deep methods notoriously expensive terms sample complexity. even relatively simple tasks require millions steps data collection complex behaviors high-dimensional observations might need substantially more. second methods often brittle respect hyperparameters learning rates exploration constants settings must carefully different problem settings achieve good results. challenges severely limit applicability model-free deep real-world tasks. cause poor sample efﬁciency deep methods on-policy learning commonly used deep algorithms trpo require samples collected gradient step policy. quickly becomes extravagantly expensive number gradient steps learn effective policy increases task complexity. off-policy algorithms instead reuse past experience. directly feasible conventional policy gradient formulations relatively straightforward q-learning based methods unfortunately combination off-policy learning high-dimensional nonlinear function approximation neural networks presents major challenge stability convergence challenge exacerbated continuous state action spaces separate actor network typically required perform maximization q-learning. commonly used algorithm settings deep deterministic policy gradient provides sample-efﬁcient learning notoriously challenging extreme brittleness hyperparameter sensitivity explore design efﬁcient stable model-free deep algorithm continuous state action spaces. draw maximum entropy framework augments standard maximum reward reinforcement learning objective entropy maximization term maximum entropy reinforcement learning alters objective though original objective recovered using temperature parameter importantly maximum entropy formulation provides substantial improvement exploration robustness discussed ziebart maximum entropy policies robust face modeling estimation errors demonstrated haarnoja improve exploration acquiring diverse behaviors. prior work proposed model-free deep algorithms perform on-policy learning entropy maximization well off-policy methods based soft q-learning variants however on-policy variants suffer poor sample complexity reasons discussed above off-policy variants require complex approximate inference procedures continuous action spaces. paper demonstrate devise off-policy maximum entropy actor-critic algorithm call soft actor-critic provides sample-efﬁcient learning stability. algorithm extends readily complex high-dimensional tasks -dof humanoid benchmark off-policy methods ddpg typically struggle obtain good results avoiding complexity potential instability associated approximate inference prior off-policy maximum entropy algorithms based soft q-learning particular present novel convergence proof policy iteration maximum entropy framework. introduce algorithm based approximation procedure practically implemented deep neural networks call soft actor-critic. present empirical results show soft actor-critic attains substantial improvement performance sample efﬁciency off-policy on-policy prior methods. soft actor-critic algorithm incorporates three ingredients actor-critic architecture separate policy value function networks off-policy formulation enables reuse previously collected data efﬁciency entropy maximization enable stability exploration. review prior works draw ideas section. actor-critic algorithms typically derived starting policy iteration alternates policy evaluation—computing value function policy—and policy improvement—using value function obtain better policy large-scale reinforcement learning problems typically impractical either steps convergence instead value function policy optimized jointly. case policy referred actor value function critic. many actor-critic algorithms build standard on-policy policy gradient formulation update actor many also consider entropy policy instead maximizing entropy regularizer tends improve stability results poor sample complexity. efforts increase sample efﬁciency retaining robustness properties incorporating off-policy samples using higher order variance reduction techniques however fully off-policy algorithms still attain better efﬁciency. particularly popular off-policy actor-critic method ddpg lillicrap deep variant deterministic policy gradient algorithm uses q-function estimator enable off-policy learning deterministic actor maximizes q-function. such method viewed deterministic actor-critic algorithm approximate q-learning algorithm. unfortunately interplay deterministic actor network q-function typically makes ddpg extremely difﬁcult stabilize brittle hyperparameter settings consequence difﬁcult extend ddpg complex high-dimensional tasks on-policy policy gradient methods still tend produce best results settings method instead combines off-policy actor-critic training stochastic actor aims maximize entropy actor entropy maximization objective. actually results maximum entropy reinforcement learning optimizes policies maximize expected return expected entropy policy. framework used many contexts inverse reinforcement learning optimal control guided policy search maximum entropy distribution used guide policy learning towards high-reward regions. recently several papers noted connection q-learning policy gradient methods framework maximum entropy learning prior works assume discrete action space nachum approximate maximum entropy distribution gaussian haarnoja sampling network trained draw samples optimal policy. although soft q-learning algorithm proposed haarnoja value function actor network true actor-critic algorithm q-function estimating optimal q-function actor directly affect q-function except data distribution. hence haarnoja motivates actor network approximate sampler rather actor actor-critic algorithm. crucially convergence method hinges well sampler approximates true posterior. contrast prove method converges optimal policy given policy class regardless policy parameterization. furthermore previously proposed maximum entropy methods generally exceed performance state-of-the-art off-policy algorithms ddpg learning scratch though beneﬁts improved exploration ease ﬁnetuning. experiments demonstrate soft actor-critic algorithm fact exceed performance state-of-theart off-policy deep methods wide margin. address policy learning continuous action spaces. consider inﬁnite-horizon markov decision processes deﬁned tuple state space action space assumed continuous unknown state transition probability represents probability density next state given current state action environment emits bounded reward transition abbreviate simplify notation. also denote state state-action marginals trajectory distribution induced policy standard objective used reinforcement learning maximize expected rewards e∼ρπ consider general maximum entropy objective favors stochastic policies augmenting objective expected entropy policy temperature parameter determines relative importance entropy term reward thus controls stochasticity optimal policy. maximum entropy objective differs standard maximum expected reward objective used conventional reinforcement learning though conventional objective recovered limit rest paper omit writing temperature explicitly always subsumed reward scaling maximum entropy objective number conceptual practical advantages. first policy incentivized explore widely giving clearly unpromising avenues. second policy capture multiple modes near-optimal behavior. particular problem settings multiple actions seem equally attractive policy commit equal probability mass actions. lastly prior work observed substantially improved exploration objective experiments observe considerably improves learning speed state-of-art methods optimize conventional objective function. extend objective inﬁnite horizon problems introducing discount factor ensure expected rewards entropies ﬁnite. writing precise maximum entropy objective inﬁnite horizon discounted case involved deferred appendix prior methods proposed directly solving optimal q-function optimal policy recovered next section discuss devise soft actor-critic algorithm policy iteration formulation instead evaluate q-function current policy update policy off-policy gradient update. though algorithms previously proposed conventional reinforcement learning method knowledge ﬁrst off-policy actor-critic method maximum entropy reinforcement learning framework. soft policy iteration soft actor-critic off-policy soft actor-critic algorithm derived starting novel maximum entropy variant policy iteration method. section ﬁrst present derivation verify corresponding algorithm converges optimal policy density class present practical deep reinforcement learning algorithm based theory. begin deriving soft policy iteration general algorithm learning optimal maximum entropy policies alternates policy evaluation policy improvement maximum entropy framework. derivation based tabular setting enable theoretical analysis convergence guarantees extend method general continuous setting next section. show soft policy iteration converges optimal policy within policies might correspond instance parameterized densities. policy evaluation step soft policy iteration wish compute value policy according maximum entropy objective equation ﬁxed policy soft q-value computed iteratively starting function iteratively applying modiﬁed version bellman backup operator deﬁned soft state value function. soft value arbitrary policy repeatingly applying formalized lemma lemma consider soft bellman backup operator equation mapping deﬁne πqk. sequence converge soft q-value policy improvement step update policy towards exponential q-function. particular choice update guaranteed result improved policy terms soft value show section. since practice prefer policies tractable additionally restrict policy policies correspond example parameterized family distributions gaussians. account constraint project improved policy desired policies. principle could choose projection turn convenient information projection deﬁned terms kullback-leibler divergence. words policy improvement step state update policy according partition function πold normalizes second argument intractable general contribute gradient respect policy thus ignored noted next section. choice projection show projected policy higher value policy respect objective equation formalize result lemma lemma πold πnew optimizer minimization problem deﬁned equation qπnew qπold full soft policy iteration algorithm alternates soft policy evaluation soft policy improvement steps provably converge optimal maximum entropy policy among policies although algorithm provably optimal solution perform exact form tabular case. therefore next approximate algorithm continuous domains need rely function approximator represent q-values running steps convergence would computationally expensive. approximation gives rise practical algorithm called soft actor-critic. theorem repeated application soft policy evaluation soft policy improvement converges policy discussed above large continuous domains require derive practical approximation soft policy iteration. function approximators q-function policy instead running evaluation improvement convergence alternate optimizing networks stochastic gradient descent. remainder paper consider parameterized state value function soft q-function tractable policy parameters networks following derive update rules parameter vectors. state value function approximates soft value. need principle include separate function approximator state value since related q-function policy according equation quantity estimated single action sample current policy without introducing bias practice including separate function approximator soft value stabilize training—and discussed later used state-dependent baseline learning policy—and convenient train simultaneously networks. soft value function trained minimize squared residual error found soft actor-critic able learn robustly also absense target network. finally policy parameters learned directly minimizing kl-divergence equation reproduce parametrized form completeness several options minimizing depending choice policy class. simple distributions gaussians reparametrization trick allows backpropagate gradient critic network leads ddpg-style estimator. however policy depends discrete latent variables case mixture models reparametrization trick cannot used. therefore propose likelihood ratio gradient estimator ∇φjπ =eat∼πφ +−qθ+log zθ+b)] state-dependent baseline approximately center learning signal eliminate intractable log-partition function choosing yields ﬁnal gradient estimator complete algorithm described algorithm method alternates collecting experience environment current policy updating function approximators using stochastic gradients batches sampled replay buffer. practice found combination single environment step multiple gradient steps work best using off-policy data replay buffer feasible value estimators policy trained entirely offpolicy data. algorithm agnostic parameterization policy long evaluated arbitrary state-action tuple. next suggest practical parameterization policy based gaussian mixtures. although could simple policy represented gaussian common prior work maximum entropy framework aims maximize randomness learned policy. therefore expressive still tractable distribution endow method effective exploration robustness typically cited beneﬁts entropy maximization propose practical multimodal representation based mixture gaussians. approximate distribution arbitrary precision even practical numbers mixture elements provide expressive distribution medium-dimensional action spaces. although complexity evaluating sampling resulting distribution scales linearly experiments indicates relatively small number mixture components typically four sufﬁcient achieve high performance thus making algorithm scalable complex domains high-dimensional action spaces. unnormalized mixture weights means covariances respectively depend complex ways expressed neural networks. also apply squashing function limit actions bounded interval explained appendix note that contrast soft q-learning algorithm assume policy accurately approximate optimal exponentiated q-function distribution. convergence result soft policy iteration holds even policies restricted policy class contrast prior methods type. figure training curves continuous control benchmarks. note performs consistently across tasks attaining highest score compared on-policy off-policy methods benchmark tasks. goal experimental evaluation understand sample complexity stability method compares prior off-policy on-policy deep reinforcement learning algorithms. evaluate range challenging continuous control tasks openai benchmark suite swimmer humanoid environments rllab implementations well-behaved observation spaces. although easier tasks benchmark suite solved wide range different algorithms complex benchmarks -dimensional humanoid exceptionally difﬁcult solve off-policy algorithms stability algorithm also plays large role performance easier tasks make practical tune hyperparameters achieve good results already narrow basins effective hyperparameters become prohibitively small sensitive algorithms high-dimensional benchmarks leading poor performance source code implementation available online. comparisons compare trust region policy optimization stable effective on-policy policy gradient algorithm; deep deterministic policy gradient algorithm regarded efﬁcient off-policy deep methods well soft q-learning off-policy algorithm learning maximum entropy policies. although ddpg efﬁcient also known sensitive hyperparameter settings limits effectiveness complex tasks figure shows total average reward evaluation rollouts training various methods. exploration noise turned evaluation ddpg trpo. maximum entropy algorithms explicitly inject exploration noise either evaluated exploration noise approximate maximum posteriori action choosing mean mixture component achieves highest q-value comparative evaluation performance across tasks. results show that overall substantially outperforms ddpg benchmark tasks terms ﬁnal performance learns faster baselines environments. exceptions swimmer ddpg slightly faster ant-v initially faster plateaus lower ﬁnal performance. hardest tasks ant-v humanoid ddpg unable make progress result corroborated prior work still learns substantially faster trpo tasks likely consequence improved stability. poor stability exacerbated highdimensional complex tasks. even though trpo make perceivable progress tasks within range depicted ﬁgures eventually solve substantially iterations. quantitative results attained experiments also compare favorably results reported methods prior work indicating sample efﬁciency ﬁnal performance benchmark tasks exceeds state art. hyperparameters used experiment listed appendix sensitivity random seeds. figure show performance multiple individual seeds halfcheetah-v benchmark ddpg trpo individual seeds attain much consistent performance ddpg exhibits high variability across seeds indicating substantially worse stability. trpo unable make progress within ﬁrst episodes. performed gradient steps environment step becomes prohibitively slow terms wall clock time longer experiments results previous section suggest algorithms based maximum entropy principle outperform conventional methods challenging tasks humanoid. section examine particular components important good performance. consider several ablations pinpoint essential differences method standard methods maximize entropy. ddpg representative non-entropy-maximizing algorithm since structurally similar sac. section examines speciﬁc algorithm components study sensitivity hyperparameter settings—most importantly reward scale number gradient steps environment step—is deferred appendix importance entropy maximization. main difference ddpg inclusion entropy maximization objective. entropy appears policy value function. policy prevents premature convergence policy variance value function encourages exploration increase value regions state space lead highentropy behavior figure compares inclusion term policy value updates affects performance removing entropy turn. evident ﬁgure including entropy policy objective crucial maximizing entropy part value function less important even hinder learning beginning training. exploration noise. another difference exploration noise maximum entropy framework naturally gives rise boltzmann exploration whereas ddpg uses external noise next experiment used policy figure tested importance entropy maximization boltzmann exploration separate value network. label curve denotes entropy terms included policy value objectives. blue curve corresponds unaltered boltzmann exploration green curves obtained using exploration noise given parameters. contribution separate value function target network updates policy gradient baseline. single mixture component compared boltzmann exploration exploration external noise repeated experiment noise parameters used ddpg parameters found worked best without boltzmann exploration results indicate boltzmann exploration outperforms external noise. separate value network. method also differs ddpg uses separate network predict state values addition q-values. value network serves purposes used bootstrap updates learning q-function serves baseline reduce variance policy gradients results figure indicates value network important role policy gradient minor effect updates. nevertheless best performance achieved value network employed purposes. ddpg. soft actor-critic closely resembles ddpg stochastic actor. study components differs algorithms important evaluated range variants swapping components ddpg counterparts. figure evaluate four versions obtained incremental modiﬁcations original replaced likelihood ratio policy gradient estimator estimator obtained reparametrization trick replaced stochastic policy deterministic policy boltzmann exploration noise eliminated separate value network using directly q-function evaluated mean action. ﬁgure suggests interesting points first reparametrization trick yields somewhat faster stable learning compared likelihood policy gradient estimator though precludes multimodal mixtures gaussians. second deterministic policy degrades performance substantially terms average return variance across random seeds observation indicates entropy maximization particularly stochastic policy make offpolicy deep signiﬁcantly robust efﬁcient compared algorithms maximizes expected return especially deterministic policies. presented soft actor-critic off-policy maximum entropy deep reinforcement learning algorithm provides sample-efﬁcient learning retaining beneﬁts entropy maximization stability. theoretical results derive soft policy iteration show converge optimal policy. result formulate soft actor-critic algorithm empirically show outperforms state-of-the-art model-free deep methods including off-policy ddpg algorithm on-policy trpo algorithm. fact sample efﬁciency approach actually exceeds ddpg substantial margin. results suggest stochastic entropy maximizing reinforcement learning algorithms provide promising avenue improved robustness stability exploration maximum entropy methods including methods incorporate second order information expressive policy classes exciting avenue future work. references barto sutton anderson. neuronlike adaptive elements solve difﬁcult learning control problems. ieee transactions systems cybernetics bhatnagar precup silver sutton maei szepesv´ari. convergent temporaldifference learning arbitrary smooth function approximation. advances neural information processing systems duan chen houthooft schulman abbeel. benchmarking deep reinforcement learning continuous control. proceedings international conference machine learning holly lillicrap levine. deep reinforcement learning robotic manipulation asynchronous off-policy updates. robotics automation ieee international conference ieee popov heess lillicrap hafner barth-maron vecerik lampe tassa erez riedmiller. data-efﬁcient deep reinforcement learning dexterous manipulation. arxiv preprint arxiv. silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis. mastering game deep neural networks tree search. nature issn article. exact deﬁnition discounted maximum entropy objective complicated fact that using discount factor policy gradient methods typically discount state distribution rewards. sense discounted policy gradients typically optimize true discounted objective. instead optimize average reward discount serving reduce variance discussed thomas however deﬁne objective optimized discount factor objective corresponds maximizing discounted expected reward entropy future states originating every state-action tuple weighted probability current policy. lemma lemma πold πnew optimizer minimization problem deﬁned equation qπnew qπold proof. πold qπold πold corresponding soft state-action value soft state value πnew deﬁned proof. policy iteration lemma sequence monotonically increasing. since bounded sequence converges still need show indeed optimal. convergence must case using iterative argument proof lemma soft value policy lower converged policy. hence optimal unbounded action distribution. however practice actions needs bounded ﬁnite interval. apply invertible squashing function samples employ change variables formula compute likelihoods bounded actions. words random variable corresponding density inﬁnite support. tanh tanh applied elementwise random variable support density given section discusses sensitivity hyperparameters. following experiments halfcheetah-v benchmark although found results representative cases differ environments. environments swept reward scale only. reward scale. soft actor-critic particularly sensitive reward magnitude serves role temperature energy-based optimal policy thus controls stochasticity. figure shows learning performance changes reward scale varied small reward magnitudes policy becomes nearly uniform consequently fails exploit reward signal resulting substantial degradation performance. large reward magnitudes model learns quickly ﬁrst policy quickly becomes nearly deterministic leading poor exploration subsequent failure lack adequate exploration. right reward magnitude model balances exploration exploitation leading fast stable learning. practice found reward scale hyperparameter requires substantial tuning since parameter natural interpretation maximum entropy framework provides good intuition adjust parameter. mixture components. experimented different numbers mixture components gaussian mixture policy found number components affect learning performance much though larger numbers components consistently attained somebetter results. policy evaluation. since converges stochastic policies generally beneﬁcial make ﬁnal policy deterministic best performance. evaluation approximate maximum posteriori action choosing action maximizes q-function among mixture component means. figure compares training returns test returns obtained strategy. test returns substantially better. noted training curves depict rewards different objective optimized maximize also entropy. target network update. figure varied smoothing constant target value network updates. value corresponds hard update weights copied directly every iteration values close zero correspond exponentially moving average network weights. interestingly relatively insensitive smoothing constant smaller tends yield higher performance cost marginally slower progress beginning difference small. prior work main target network stabilize training interestingly able learn stably also effectively separate target network used instead weights copied directly although cost minor degradation performance. replay buffer size. next experimented size experience replay buffer found important environments optimal policy becomes nearly deterministic convergence halfcheetah-v. environments exploration noise becomes negligible making content replay buffer less diverse resulting overﬁtting instability. higher capacity buffer solves problem allowing method remember failures beginning learning slows learning allocating network capacity modeling suboptimal initial experience. note return considered threshold level solving benchmark task performance cases well beyond threshold. used buffer size million samples halfcheetah-v million samples environments. gradient steps time step. finally experimented number actor critic gradient steps time step algorithm prior work observed increasing number gradient steps ddpg tends improve sample efﬁciency point. found gradient updates time step optimal ddpg degradation performance. able effectively beneﬁt substantially larger numbers gradient updates environments performance increasing steadily until gradient updates step. environments humanoid observe much beneﬁt. ability take multiple gradient steps without negatively affecting algorithm important especially learning real world experience collection bottleneck typical learning script asynchronously separate thread. experiment used replay buffer size million samples therefore performance suddenly drops reaching return approximately thousand reason discussed previous section.", "year": 2018}