{"title": "Generating Factoid Questions With Recurrent Neural Networks: The 30M  Factoid Question-Answer Corpus", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "H.3.4; I.5.1; I.2.6; I.2.7"], "abstract": "Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.", "text": "dumais voorhees tice recently researchers started utilize large-scale knowledge bases freebase wikidata bootstrapping systems structured knowledge clearly beneﬁcial unlikely alone overcome lack labeled data. take account rich complex nature human language paraphrases ambiguity would appear labeled question answer pairs necessary. need labeled pairs even critical training neural network-based systems researchers relied mainly hand-crafted rules heuristics synthesize artiﬁcial corpora motivated recent developments paper focus generating questions based freebase frame question generation transduction problem starting freebase fact represented triple consisting subject relationship object transduced question subject object correct answer propose several models largely inspired recent neural machine translation models approach similar luong dealing problem rare-words. evaluate produced questions human-based experiment well respect automatic evaluation metrics including well-established machine translation metrics bleu meteor sentence similarity metric. question-generation model outperforms competing template-based past decade large-scale supervised learning corpora enabled machine learning researchers make substantial advances. large-scale questiondate answer corpora available. paper present factoid questionanswer corpus enormous question answer pair corpus produced applying novel neural network architecture knowledge base freebase transduce facts natural language questions. produced question answer pairs evaluated human evaluators using automatic evaluation metrics including well-established machine translation sentence similarity metrics. across evaluation criteria questiongeneration model outperforms competing template-based baseline. furthermore presented human evaluators generated questions appear comparable quality real human-generated questions. major obstacle training question-answering systems lack labeled data. question answering ﬁeld focused building systems based traditional information retrieval procedures followed increasing interest natural language generation community. simple rule-based approach proposed different studies wh-fronting wh-inversion comes disadvantage making semantic content words apart syntactic role. problem determining question type requires knowledge category type elements involved sentence addressed different ways using named entity recognizers semantic role labelers curto questions split classes according syntactic structure preﬁx question category answer pattern learned generate questions class questions. identiﬁcation points chen apply handcrafted-templates generate questions framed right target expression following analysis graesser classify questions according taxonomy consisting categories. works discussed propose ways unstructured text questions. implies two-step process ﬁrst transform text symbolic representation second transform symbolic representation text question hand going symbolic representation question describe next section involves second step. closer approach work olney take triples input edge relation deﬁnes question template head triple replaces placeholder token selected question template. spirit duma generate short descriptions triples using templates deﬁned relationship replacing accordingly placeholder tokens subject object. baseline similar olney relationship-speciﬁc templates deﬁned. templates include placeholders replace string subject. main difference respect work baseline explicitly deﬁne templates. instead relationship many templates different ways framing question relationship training set. yields diverse semantically richer questions effectively taking advantage fact-question pairs olney access experiments. unlike work berant liang addresses problem deterministically generating candidate logical forms canonical realization natural language each work addresses inverse problem given logical form outputs associated question. also noted recent work question answering used simpler rule-based template-based approaches generate synthetic questions address lack question-answer pairs train models knowledge bases general viewed multirelational graph consists nodes edges linking nodes together. freebase relationships directed always connect exactly entities. freebase entities ﬁres creek nantahala national forest linked together relationship contained since triple {ﬁres creek contained nantahala national forest} represents complete self-contained piece information also called fact ﬁres creek subject contained relationship propose attack problem models inspired recent success neural machine translation models intuitively think transduction task lossy translation structured knowledge human language certain aspects structured knowledge intentionally left models typically consist components encoder encodes source phrase several ﬁxed-size vectors decoder decodes target phrase based results encoder. encoder contrast neural machine translation framework source language proper language instead sequence three variables making fact. propose encoder sub-model encodes atom fact embedding. atom stand subject relationship object respectively fact represented -of-k vector xatom whose embedding obtained eatom einxatom rdenc×k embedding matrix input vocabulary size vocabulary. encoder transforms embedding encatom rhdec encatom wenceatom wenc rhdec×denc. embedding matrix could another parameter model learned however discussed later learned separately beforehand transe model aimed modeling kind multi-relational data. allow encoder tune training. dataset simplequestions dataset order train models. largest dataset question-answer pairs created humans based contains question-answer pairs created users amazon mechanical turk english based freebase order create questions human participants shown whole freebase fact time asked phrase question object presented fact becomes answer question. consequently subject relationship explicitly given question. indirectly characteristics object also given since humans access well. often phrasing question annotators tend informative target object giving speciﬁc information question produced. example question city american actress from? city name given object informs human participant america information provided either subject relationship decoder decoder recurrent neural network attention-mechanism encoder representation generate associated question fact recently shown performs equally well across range tasks compared architectures lstm hidden state decoder computed time step sigmoid function s.t. circle represents element-wise multiplication. initial state given output feedforward neural network fact embedding. product eoutwn rddec decoder embedding vector corresponding word variables rhdec×hdec rhdec×ddec parameters context vector vector called reset gate update gate candidate activation. adjusting appropriately model able create linear skip-connections distant hidden states turn makes credit assignment problem easier gradient signal stronger earlier hidden states. then time step probabilities word tokens given applying softmax layer votanh) ×hdec rhdec×hdec rhdec×ddec. lastly function computed using attention-mechanism figure computational graph question-generation model fact embedding produced encoder model fact representation weighed according attentionmechanism depends fact previous hidden state decoder sake simplicity attentionmechanism shown explicitly. αsn− αrn− αrn− real-valued scalars weigh contribution subject relationship object representations. correspond attention model computed applying one-layer neural network tanh-activation function encoder representations fact previous hidden state followed sigmoid function restrict attention values zero one. need attention-mechanism motivated intuition model needs attend subject generation process attending relationship times generation process. model illustrated figure modeling source language particular problem model presented related embeddings entities relationships tokens learned another. learn naively simplequestions training model perform poorly encounters previously unseen entities relationships tokens. furthermore multi-relational graph deﬁned facts simplequestions extremely sparse i.e. node edges nodes expected high ratio unique entities number examples. therefore even many entities simplequestions model perform poorly embedding learned solely based simplequestions dataset alone. source side resolve issue initializing subject relationship object embeddings learned applying multi-relational embedding-based models knowledge base. multi-relational embeddingbased models recently become popular learn distributed vector embeddings knowledge bases shown scale well yield good performance. simplicity good performance choose transe learn embeddings. transe translation-based model whose energy function trained output values fact expresses true information i.e. fact exists knowledge base otherwise high values. formally energy function deﬁned ||es eo|| real-valued embedding vectors subject relationship object fact. details given bordes embeddings entities connections easy learn quality embeddings depends inter-connected are. extreme case subject object triple appears dataset learned embeddings subject object semantically meaningless. happens often simplequestions since around entities connections graph. thus applying transe directly triples would eventually layentities contain clusters semantically close concepts. order guarantee effective semantic representation embeddings learn together additional triples extracted whole freebase graph complement simplequestions graph relevant information task. information like profession nationality annotators tend phrasing questions accordingly ensured embeddings contain information taking triples coming freebase graph regarding type/instance ensures entities semantic category close other. although might think expected category subject/object could inferred directly relationship ﬁne-grained differences expected types extracted directly observing category information. sometimes annotators included information french nationality germany. subject and/or information given object. location/contained including facts learning ensure existence ﬁne-grained layout embeddings regarding information within category. included facts freebase addition facts simplequestions training. table shows differences embeddings adding additional facts training transe representations. generating questions resolve problem data sparsity previously unseen words target side draw inspiration placeholders proposed handling rare words neural machine translation luong every question answer pair search words question overlap words subject string fact. heuristically estimate sequence likely words question correspond subject string. words replaced placeholder token <placeholder>. example given fact {ﬁres creek contained nantahala national forest} original question forest fires creek transformed question forest <placeholder>in?. model trained modiﬁed questions means model learn decoder embeddings tokens subject string. test time outputting question placeholder tokens replaced subject string outputs evaluated. call single-placeholder model. main difference respect luong placeholder tokens input language because entities relationships input would able transmit semantic information decoder. included placeholder tokens input language model would able generate informative words regarding subject question single placeholder token question types could unnecessarily limit model. therefore also experiment another model called multi-placeholder model uses different placeholder tokens placeholder given question chosen based subject category extracted relationship could make easier model learn phrase questions diverse entities also introduces additional parameters since placeholder embeddings learned therefore model suffer overﬁtting. addressing sparsity output reduces vocabulary size less words. compare neural network models propose template-based baseline model makes entire training generating question. baseline operates questions modiﬁed placeholder preceding section. given fact input baseline picks candidate fact training uniformly random relationship baseline considers questions corresponding model ﬁnal step placeholder token question replaced subject string fact models implemented using theano train neural network models optimized log-likelihood using ﬁrst-order gradient-based optimization algorithm adam decide stop training used early stopping patience meteor score obtained validation set. experiments default split simplequestions dataset training validation test sets. object. based preliminary experiments neural network models ﬁxed learning rate clipped parameter gradients norms larger ﬁxed embedding dimensionality words hidden state decoder dimensionality automatic evaluation metrics bleu widely used evaluation metrics statistical machine translation automatic image-caption generation similar statistical machine translation phrase source language mapped phrase target language task fact mapped natural language question. tasks highly constrained e.g. valid outputs limited. true particular short phrases sentence questions. furthermore tasks majority valid outputs paraphrases other bleu meteor designed capture. therefore believe bleu meteor constitute reasonable performance metrics evaluating generated questions. although believe meteor bleu reasonable evaluation metrics recognize certain paraphrases particular paraphrases entities. therefore also make sentence similarity metric proposed lintean denote embedding greedy metric makes word similarity score experiments cosine similarity wordvec word embeddings metric ﬁnds alignment words questions maximizes similarity aligned words computes sentence similarity mean word similarities aligned words. example questions produced model multiple placeholders shown table neural network models outperform templatebased baseline clear margin across metrics. template-based baseline already relatively strong model makes separate template relationship. given enough training data suggests neural networks generally better question generation task compared hand-crafted template-based procedures therefore useful generating question answering corpora. furthermore appears best performing models models transe trained largest triples contains apart supporting triples described section triples involving entities highly connected entities found simplequestions facts. total around millions facts used generate factoid question-answer corpus. lastly clear whether model single placeholder model multiple placeholders performs best. motivates following human evaluation study. initially considered carrying separate experiments measuring relevancy ﬂuency respectively since common practice machine translation. however relevancy question determined solely single factor i.e. relationship since construction subject always question. measuring relevancy therefore useful task. verify carried internal pairwise preference experiment human subjects repeatedly shown fact questions asked select relevant question. found questions generated triples transe++ model either judged better least good human generated questions w.r.t. relevancy. remaining questions triples transe++ model questions also judged relevant questions although less compared human generated questions. next experiment therefore measure holistic quality questions. table test performance models w.r.t. bleu meteor emb. greedy performance metrics indicates models single placeholder models multiple placeholders. transe++ indicates models transe embeddings pretrained larger triples. best performance metric marked bold font. table test examples corresponding questions using template-based baseline triples transe++ model. examples please refer supplementary material. tions) baseline-mp show human evaluators fact along questions question model corresponding fact choose question relevant fact natural. human evaluator also option choosing either question. important questions equally good neither questions make sense. beginning experiment show human evaluators examples statements corresponding pair questions brieﬂy explain form statements questions relate statements. following introductory examples present facts corresponding pair questions one. avoid presentation bias randomly shufﬂe order examples order questions shown model. experiment also show four check facts corresponding check questions random attentive human annotator able answer easily. discard responses human evaluators fail four checks. total preferences recorded independent human evaluators. questions produced model model pair evaluated batches human evaluated examples batch example evaluated evaluators. agreement automatic evaluation metrics human evaluators strongly prefer either human neural network model template-based baseline. furthermore appears humans cannot distinguish human-generated questions neural network questions average showing preference towards later former ones. hypothesize model penalizes uncommon unnatural ways frame questions sometimes includes speciﬁc information table pairwise human evaluation preferences computed across evaluators conﬁdence intervals. preferred model experiment marked bold font. asterisk next preferred model indicates statistically signiﬁcance likelihood-ratio test shows model preferred least half presented examples conﬁdence. name triples transe++ indicates model multiple placeholders transe embeddings pretrained larger triples. last column shows fleiss’ kappa averaged across batches different evaluators questions. propose neural network models mapping knowledge base facts corresponding natural language questions. neural networks combine ideas recent neural network architectures statistical machine translation well multi-relational knowledge base embeddings overcoming sparsity issues placeholder techniques handling rare words. produced question answer pairs evaluated using automatic evaluation metrics including bleu meteor sentence similarity found outperform template-based baseline model. evaluated untrained human subjects question answer pairs produced best performing neural network appear comparable quality real human-generated questions. finally best performing neural network model generate corpus question answer pairs hope enable future researchers improve question answering systems. authors acknowledge research nserc canada research chairs cifar funding. authors thank yang bing xiang bowen zhou gerald tesauro constructive feedback antoine bordes nicolas usunier sumit chopra jason weston providing simplequestions dataset. research enabled part support provided calcul qubec compute canada dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. international conference learning representations. satanjeev banerjee alon lavie. meteor automatic metric evaluation improved correlation human judgments. proceedings workshop intrinsic extrinsic evaluation measures machine translation and/or summarization volume pages kurt bollacker colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database structuring human knowledge. proceedings sigmod international conference management data pages antoine bordes nicolas usunier alberto garcia-duran jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems pages antoine bordes jason weston open question answernicolas usunier. weakly supervised embedding models. machine learning knowledge discovery databases european conference pages xinlei chen fang tsung-yi ramakrishna vedantam saurabh gupta piotr dollar lawrence zitnick. microsoft coco captions data collection evaluation server. arxiv preprint arxiv.. kyunghyun bart merrienboer caglar gulcehre fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder proceedings statistical machine translation. conference empirical methods natural language processing pages susan dumais michele banko eric brill jimmy andrew question answering always better? proceedings annual international sigir conference research development information retrieval pages saidalavi kalady ajeesh elikkottil rajarshi das. natural language question generation using syntax keywords. proceedings third workshop question generation pages questiongeneration. org. douglas lenat rabuilding large manathan guha. knowledge-based systems; representation inference project. addison-wesley longman publishing inc. minh-thang luong ilya sutskever quoc oriol vinyals wojciech zaremba. addressing rare word problem neural machine translation. proceedings pages rashmi prasad aravind joshi. question generation paragraphs upenn qgstec system description. proceedings third workshop question generation pages tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases advances neural compositionality. information processing systems pages vasile mihai lintean. comparison greedy optimal assessment natural language student input using wordproceedings to-word similarity metrics. seventh workshop building educational applications using naacl pages vasile brendan wyse paul piwek mihai lintean svetlana stoyanchev cristian moldovan. ﬁrst question generation shared task evaluation challenge. proceedings international natural language generation conference pages", "year": 2016}