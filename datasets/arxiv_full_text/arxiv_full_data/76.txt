{"title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior  Knowledge", "tag": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "abstract": "We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural Networks that replaces the softmax output layer by a log-linear output layer, of which the softmax is a special case. This conceptually simple move has two main advantages. First, it allows the learner to combat training data sparsity by allowing it to model words (or more generally, output symbols) as complex combinations of attributes without requiring that each combination is directly observed in the training data (as the softmax does). Second, it permits the inclusion of flexible prior knowledge in the form of a priori specified modular features, where the neural network component learns to dynamically control the weights of a log-linear distribution exploiting these features.  We conduct experiments in the domain of language modelling of French, that exploit morphological prior knowledge and show an important decrease in perplexity relative to a baseline RNN.  We provide other motivating iillustrations, and finally argue that the log-linear and the neural-network components contribute complementary strengths to the LL-RNN: the LL aspect allows the model to incorporate rich prior knowledge, while the NN aspect, according to the \"representation learning\" paradigm, allows the model to discover novel combination of characteristics.", "text": "introduce ll-rnns extension recurrent neural networks replaces softmax output layer log-linear output layer softmax special case. conceptually simple move main advantages. first allows learner combat training data sparsity allowing model words complex combinations attributes without requiring combination directly observed training data second permits inclusion ﬂexible prior knowledge form priori speciﬁed modular features neural network component learns dynamically control weights log-linear distribution exploiting features. conduct experiments domain language modelling french exploit morphological prior knowledge show important decrease perplexity relative baseline rnn. provide motivating iillustrations ﬁnally argue log-linear neural-network components contribute complementary strengths ll-rnn aspect allows model incorporate rich prior knowledge aspect according representation learning paradigm allows model discover novel combination characteristics. recurrent neural networks recently shown remarkable success sequential data prediction applied tasks language modelling machine translation parsing natural language generation dialogue name few. specially popular architectures applications models able exploit long-distance correlations lstms grus groundbreaking performances. working symbolic data conversion real vectors discrete values instance words certain vocabulary becomes necessary. however rnns taken oversimpliﬁed view mapping. particular converting output vectors distributions symbolic values mapping mostly done softmax operation assumes able compute real value individual member vocabulary converts value probability direct exponentiation followed normalization. focus symptomatic defect approach consider following. using words symbols even large vocabularies cannot account actual words found either training test models need resort catch-all unknown symbol provides poor support prediction requires supplemented diverse prepost-processing steps even words inside vocabulary unless witnessed many times training data prediction tends poor word island completely distinct without relation words needs predicted individually. smaller characters observed many times training data. character-based rnns thus advantages word-based ones also tend produce non-words necessitate longer prediction chains words jury still emerging hybrid architectures attempt capitalize levels here propose diﬀerent approach removes constraint dimensionality output vector equal size vocabulary allows generalization across related words. however crucial beneﬁt introduces principled powerful incorporating prior knowledge inside models. approach involves direct natural extension softmax considering special case conditional exponential family class models better known log-linear models widely used prenn nlp. argue simple extension softmax allows resulting log-linear compound aptitude log-linear models exploiting prior knowledge predeﬁned features aptitude rnns discovering complex combinations predictive traits. generative process predicting sequence symbols symbols taken vocabulary prediction conditioned certain observed context generative process written hidden state previous step output symbol produced step neural-network based function computes next hidden state based ht−. function typically computed returns real-valued vector dimension vector log-linear models play considerable role statistics machine learning; special classes often known diﬀerent names depending application domains various details exponential families maximum entropy models conditional random ﬁelds binomial multinomial logistic regression models especially popular example language modelling sequence labelling machine translation name few. initial belief represented taking words always assume initial belief represented background probability along null parameter vector deviations initial belief ﬁrst diﬀerence allow general form input network time step; namely instead allowing latest symbol used input along condition allow arbitrary feature vector used input; feature vector ﬁxed dimensionality allow computed contrarily generality presentation jebara many presentations log-linear models context make explicit reference implicitely taken uniform. however statistically oriented presentations strongly related exponential family models makes mathematics neater necessary presence non-ﬁnite continuous spaces. advantage explicit introduction even ﬁnite spaces makes easier speak prior knowledge overall process. arbitrary combination currently known preﬁx context relatively minor change usefully expands expressive power network. sometimes call features input features. second major diﬀerence following. compute previously however point rather applying softmax obtain distribution apply log-linear model. standard words assume priori ﬁxed certain background function condition given also deﬁned features deﬁning feature vector ﬁxed dimensionality sometimes call features output figure indicated operation combines feature vector background produce probability distribution note that here vector size equal size vocabulary contrast case equation provides particularly intuitive formula gradient namely diﬀerence expectation according log-linear model parameters observed value however expectation diﬃcult compute. ﬁnite vocabulary simplest approach simply evaluate right-hand side equation normalize obtain weight accordingly. standard rnns actually simpler approaches computing softmax gradient sophisticated approaches proposed employing hierarchical softmax general case expectation term needs approximated diﬀerent techniques employed speciﬁc log-linear models generic contrastive divergence importance sampling; recent introduction generic methods provided despite practical importance pursue topic here. consider moderately-sized corpus english sentences tokenized word level consider vocabulary size consisting frequent words occur corpus plus special symbol used tokens among words replacing unknown words corpus train language model corpus training standard lstm type. note translated ll-rnn according section model features along uniform background let’s improve situation moving ll-rnn. start extending much larger ﬁnite words particular includes words union training test corpora keep uniform concerning features keep standard values concerning features keep word-identity features unk-identity one; however note allow features overlap freely nothing preventing word location adjective example also appear frequent words. exposition reasons suppose number always feature φnumber feature apart case also belongs case also word-identity consider behavior ll-rnn training certain point let’s observed preﬁx cost coming prediction next item assume actually number training sample. case ﬁrst term vector null everywhere coordinate φnumber equal second term seen model average feature vector sampled according vector coordinates interval fact strictly consequence gradient strictly positive coordinate φnumber strictly negative coordinates. words backpropagation signal sent neural network point modify parameters increase anumber weight decrease weights slightly diﬀerent situation occurs assume belongs case null everywhere coordinates φnumber equal reasoning gradient strictly positive corresponding coordinates strictly negative everywhere else. thus signal sent network modify parameter towards increasing anumber weights decrease everywhere else. overall occurrence number training network learning increase weights corresponding features never also making mild assumption feature exist strict inequalities follow immediately. training predicting word follows preﬁx cost ll-rnn network tendency produce weight vector especially high weight anumber positive weights appeared similar contexts negative weights features ﬁring similar contexts. note that model able capitalize generic notion number feature φnumber also able learn privilege certain speciﬁc numbers belonging tend appear frequently certain contexts. log-linear model important advantage able handle redundant features φnumber depending prior expectations typical texts domain handled useful introduce features distinguishing diﬀerent classes numbers instance small numbers year-like numbers allowing ll-rnn make useful generalizations based features. features need binary example small-number feature could take values decreasing higher values reserved smaller numbers. numbers appeared context cost would mean nonnumeric features words high expensive etc. course also appear associated features would also receive positive increments. this property log-linear models permitted fundamental advance statistical machine translation beyond initial limited noisy-channel models allowing freer combination diﬀerent assesments translation quality without bother overlapping assesments extension softmax log-linear outputs formally simple opens signiﬁcant range potential applications handling rare words. brieﬂy sketch directions. priori constrained sequences applications sequences generated respect certain priori constraints. case approach semantic parsing starting natural language question decoder produces sequential encoding logical form conform certain grammar. model used implicitely simple case ll-rnn output feature vector remains usual onehot background uniform anymore constrains generated sequence conform grammar. language model adaptation earlier taking uniform onehot ll-rnn standard rnn. opposite extreme case obtained supposing already know exact generative process producing context deﬁne identical true underlying process order best performance test suﬃcient adaptor vector equal null vector then according equal underlying process. task learn null close null easy case adaptor actually nothing adapt interesting intermediary case true process. example could word-based language model trained large monolingual corpus current focus modeling speciﬁc domain much less data available. training rnn-based adaptor speciﬁc domain data would still able rely test words seen input features standard word vector-encoded one-hot representation produced current output network also used next input network. section interest deﬁning output features beyond word-identity features i.e. beyond identiﬁcation onehot kept input features standard rnns namely kept onehot however note issue there. usual encoding input means rarely seen training data network clues distinguish word another rarely observed word computing equation network context preﬁx cost able give reasonable probability thanks however assessing probability euros context preﬁx cost distinguished network preﬁx cost preposterous would allow euros next word. promising solve problem take namely encode input using features output allows network number preposterous adjective compute hidden state based information. note however requirement equal general; point include features help network predict next word. inﬁnite domains example section vocabulary large ﬁnite. quite artiﬁcial especially want account words representing numbers words taken open-ended entity names. back equation deﬁning log-linear models particular well-deﬁned uniformly. however inﬁnite unfortunately true anymore. instance uniformly inﬁnite probability undeﬁned. contrast let’s assume background thus standard rnns uniform background handle inﬁnite vocabularies ll-rnns using ﬁnite-mass can. simple ensure property tokens representing numbers example associate geometric background distribution decaying fast length similar treatment done named entities. condition-based priming many applications rnns machine translation natural language generation etc. depend condition translated ll-rnns condition taken however opportunity exploiting condition inside sketch simple example able predeﬁne weak unigram language model realization depends semantic input example constraining named entities appear realization evidence input. language model usefully represented background process providing form priming combined ll-rnn helping avoid irrelevant tokens. similar approach recently exploited goyal context character-based seqseq lstm generating utterances input dialog acts. approach background formulated weighted ﬁnite-state automaton characters used encouraging system generate character strings correspond possible dictionary words well allow generate strings corresponding non-dictionary tokens named-entities numbers addresses like strings evidence input dialog act. datasets based annotated french corpora provided universal dependencies initiative. corpora tagged level well dependency level. experiments exploit annotations lowercased versions corpora. corpora provide morphological tags word token context sentence appears. table shows tags treat binary features. addition select frequent word types appearing entire corpus additional binary features identify whether given word identical frequent words whether outside set. total binary features. prontypedem prontypein prontypeint prontypeneg prontypeprs prontyperel reﬂexyes tensefut tenseimp tensepast tensepres verbformfin verbforminf verbformpart caseabl caseacc casenom casevoc deﬁnitedef deﬁniteind degreecmp genderfem gendermasc genderneut moodcnd moodimp moodindn moodsubt numberplurj numbersing person person person case ambiguous word binary vector ones several simultaneously. thus here basically corpus proxy morphological analyser french contextual information provided token-level tags. experiments ﬁnite word vocabulary consisting types found entire corpus compare ll-rnn vanilla vocabulary thus none models unknown words. models implemented keras theano backend. ll-rnn architecture parameters following diﬀerences. first direct embedding input words replaced embedding dimension representation words space features followed lstms before dimension followed dense layer output dimension layer transformed deterministic probability distribution incorporation ﬁxed background probability distribution background precomputed unigram probability distribution word types entire corpus. table shows perplexity results obtain diﬀerent cases llrnns compared baseline rnn. notation ll-rnn indicate value number frequent word types considered features. model stopped training validation loss improve three epochs. thus also test corpora estimate unigram probabilities. background requires estimate probability words encounter training also test. however method proxy proper estimate background possible words leave future development. note that similarly baseline need know hand words encounter otherwise resort category want order able direct comparison perplexities. initial informal qualitative look sentences generated model hand best ll-rnn model hand seems indicate much better ability ll-rnn account agreement gender number moderate distances proper evaluation performed. topformelle pospron pospropn genderfem numbersing person prontypeprs topformest posnoun pospropn possconj posverb posx genderfem gendermasc moodind numbersing person tensepres verbformfin topformtr`es posadv topformsouvent posadv topformnottop posverb genderfem numbersing tensepast verbformpart topformen posadp posadv pospron person topform r´eaction posnoun genderfem numbersing topform`a posadp posaux posnoun posverb moodind numbersing person tensepres verbformfin topforml’ pospron pospropn deﬁnitedef genderfem gendermasc numbersing person prontypeprs topformimage posnoun genderfem numbersing topformde posadp posdet pospropn posx deﬁniteind genderfem gendermasc numberplur numbersing prontypedem topformla posadv posdet posnoun pospron pospropn posx deﬁnitedef genderfem gendermasc numbersing person prontypeprs topformr´epublique posnoun pospropn genderfem numbersing topformen posadp posadv pospron person topform posnum topform. pospunct table example sentence generated ll-rnn right column shows non-null features word. note repr´esent´ee among frequent words proper agreement distant pronoun elle. side remark observe ﬂawed features small number goldannotation errors fact appears correct posadp also impossible pos’s attempted ﬁlter gold-annotation mistakes could improve results. individuals without connections share attributes. fundamental property linguistics classical approaches represent words combination linguistic features lemma number gender case tense aspect register etc. standard softmax approach words diﬀerent even single dimension predicted independently done eﬀectively presence large training sets. ll-rnn approach associating diﬀerent features diﬀerent linguistic features model learn predict plural number based observation plural numbers accusative based observation accusatives predict word forms combinations never observed training data. example phenomenon experiments section linguistic features encompass semantic classes generalizations become possible semantic classes also. contrast softmax case models deﬁcient presence sparsity training data word forms also require waste capacity parameters make able large vectors required discriminate many elements ll-based rnns parametrization principle smaller fewer features need speciﬁed obtain word level predictions. second consequence exploit rich prior knowledge input features background output features already gave illustrations incorporating prior knowledge many possibilities. example dialogue application requires answer utterances contain numerical data obtained access knowledge base certain binary expert feature could take value either non-number word speciﬁc number obtained process exploiting context conjunction knowledge base. combination background features would responsible linguistic quality answer utterance feature activated would ensure number produced point equal would decide exactly point number produced whether feature activated would decided large value coordinate would activate feature small value deactivate conclude remark concerning complementarity loglinear component neural network component ll-rnn approach. amply demonstrated recent years standard softmax-based already quite powerful. stand-alone log-linear model also quite powerful older research also demonstrated. roughly diﬀerence log-linear model ll-rnn model ﬁrst log-linear weights ﬁxed training ll-rnn dynamically vary under control neural network component. however strengths classes models diﬀerent areas. log-linear model good exploiting prior knowledge form complex features ability discover combinations features. hand good discovering combinations characteristics input predictive output ill-equipped exploiting prior knowledge. argue ll-rnn approach capitalize complementary qualities. idea reminiscent approach lstm-based mixtures experts similar purpose; diﬀerence here instead using linear mixture log-linear mixture i.e. features combined multiplicatively rather additively exponents given collaborating approach experts competing expert corresponding needs decide exact point produce number rather relying linguistic specialist multiplicative aspect ll-rnns related product experts introduced hinton however case focus learning individual experts combined direct product involving exponentiations therefore log-linear class. case focus exploiting predeﬁned experts letting controlling decide exponents. note standard log-linear model onehot features would make sense ﬁxed would always predict distribution next word. contrast ll-rnn features make sense standard rnn. standard log-linear models employ interesting features.", "year": 2016}