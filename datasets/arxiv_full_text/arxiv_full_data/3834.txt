{"title": "Trainable Frontend For Robust and Far-Field Keyword Spotting", "tag": ["cs.CL", "cs.NE"], "abstract": "Robust and far-field speech recognition is critical to enable true hands-free communication. In far-field conditions, signals are attenuated due to distance. To improve robustness to loudness variation, we introduce a novel frontend called per-channel energy normalization (PCEN). The key ingredient of PCEN is the use of an automatic gain control based dynamic compression to replace the widely used static (such as log or root) compression. We evaluate PCEN on the keyword spotting task. On our large rerecorded noisy and far-field eval sets, we show that PCEN significantly improves recognition performance. Furthermore, we model PCEN as neural network layers and optimize high-dimensional PCEN parameters jointly with the keyword spotting acoustic model. The trained PCEN frontend demonstrates significant further improvements without increasing model complexity or inference-time cost.", "text": "used frontend so-called log-mel frontend consisting mel-ﬁlterbank energy extraction followed compression compression used reduce dynamic range ﬁlterbank energy. however several issues function. first singularity common methods deal singularity either clipped stabilized however choice offset methods different performance impacts different signals. second function uses dynamic range level silence likely least informative part signal. third function loudness dependent. different loudness function produce different feature values even underlying signal content same introduces anfactor variation training inference. although techniques mean–variance normalization cepstral mean normalization used alleviate issue extent nontrivial deal time-varying loudness online fashion. remedy issues compression introduce frontend called per-channel energy normalization essentially pcen implements simple feed-forward automatic gain control dynamically stabilizes signal levels. since pcen operations differentiable propose implement pcen neural network operations/layers jointly optimize various pcen parameters acoustic model. equipped trainable agc-based frontend resulting system found robust distant speech. rest paper organized follows. section introduce describe pcen frontend. section formulate pcen neural network layers discuss advantages formulation. section present experimental results. last section discusses concludes paper. denote time frequency index denotes ﬁlterbank energy time-frequency bin. although restriction ﬁlterbank paper fft-based ﬁlterbank fair comparison log-mel frontend. smoothed version robust far-ﬁeld speech recognition critical enable far-ﬁeld conditions sigtrue hands-free communication. nals attenuated distance. improve robustness loudness variation introduce novel frontend called perchannel energy normalization ingredient pcen automatic gain control based dynamic compression replace widely used static compression. evaluate pcen keyword spotting task. large rerecorded noisy far-ﬁeld eval sets show pcen signiﬁcantly improves recognition performance. furthermore model pcen neural network layers optimize high-dimensional pcen parameters jointly keyword spotting acoustic model. trained pcen frontend demonstrates signiﬁcant improvements without increasing model complexity inference-time cost. index terms keyword spotting robust far-ﬁeld speech recognition automatic gain control deep neural networks speech become prevailing interface enable humancomputer interaction especially mobile devices. important component interface keyword spotting system example often used wake mobile devices initiate conversational assistants. therefore reliably recognizing keywords regardless acoustic environment often prerequisite effective interaction speech-enabled products. thanks development deep neural networks automatic speech recognition dramatically improved past years however current systems perform well relatively clean conditions robustness remains major challenge. also applies system system needs robust various kinds noise interference also varying loudness capability handle loudness variation important allows users talk devices wide range distances enabling true hands-free interaction. similar modern systems system also neural network based however resource-limited embedded system keyword recognizer constraints. importantly expected devices always listening demands small memory footprints power consumption. therefore size neural network needs much smaller used modern systems implying network limited representation power. addition on-device keyword spotting typically sophisticated decoding schemes language models constraints motivate rethink design feature-extraction frontend. pcen relatively ﬂat. addition pcen tends enhance speech onsets important noise reverberation robustness. understand onset enhancement effect rewrite e/)α interpreted performing partial log-domain high-pass ﬁltering followed exponential expansion helps enhance transitions. similar rasta ﬁltering used channel normalization seen equation pcen introduces several parameters. parameters need manually tuned labor intensive inherently suboptimal process. cases manual tuning become impossible. example wants jointly tune parameters moderate resolution number parameter combinations easily explode. original motivations automatically optimize various pcen parameters together human efforts reduced. fortunately pcen operations differentiable permitting gradient-based optimization w.r.t. hyperparameters. ﬁrst describe learn gain normalization related parameters precomputed ﬁlterbank energy corresponding smoother pcen easily expressed matrix operations hence representable standard neural network layers. taking advantage modularity neural network feed pcen layer outputs features original acoustic model jointly optimize them. given acoustic model loss function backpropagation compute gradient w.r.t. parameters update iteratively using stochastic gradient descent ensure parameter positivity gradient updates values take exponentials. neural-network formulation offers automatic parameter ﬁnding. standard pcen formulation restrict parameters scalars shared across bins manually tune them difﬁcult design high-dimensional parameters. neural-networkbased formulation generalize pcen parameters frequencyeven time-frequency dependent optimize values automatically. extend idea learning smoother well. speciﬁcally would like learn smoothing coefﬁcient controls time constant smoother. achieve would model smoother recurrent neural network trainable another predetermine several smoothing coefﬁcients learn weighted comsmoothing coefﬁcient. small constant prevent division zero. arbitrarily chose found signiﬁcant performance impact. essentially e/)α part implements form feed-forward strength controlled parameter larger indicates stronger gain normalization. note smoothing mainly carries loudness proﬁle subsequently normalized out. also note operation causal done channel independently making suitable real-time implementation. emphasizes changes relative recent spectral history adapts channel effects including loudness following perform stabilized root compression reduce dynamic range using offset exponent note offset introduces start stabilized root compression curve resembles optimized spectral subtraction curve worth noting main parameters pcen strength smoothing coefﬁcient whose choices depend loudness distribution data. figure compares log-mel feature pcen feature speech utterance log-mel uses stabilized offset pcen uses note translates -frame time constant smoother. similar log-mel pcen reduces dynamic range ﬁlterbank energy still preserving prominent speech patterns. however note several intervals low-level signals carry useful information ampliﬁed operation whereas figure illustrates joint pcen frontend model. here would like point important design choice. design trainable parameters dataindependent meaning conditioned input features training learned parameters frozen change different signals. design choice avoids introducing additional network weights computational cost inference simply hardcode trained parameters pcen frontend. although entirely possible data-dependent parameter learning improve performance important strike balance accuracy resource usage always-listening kws. system uses convolutional neural network input consists left context frames right context frames log-mel pcen frontend frame computed based -channel ﬁlterbank windowing frame shift. consists convolutional layer feature maps non-overlapping kernels followed linear projection layer size fully connected rectiﬁed linear unit layer size trained large internal training sets detect keyword google. interested reader referred details decoding. improve noise robustness perform multi-condition training artiﬁcially corrupting utterance various interfering background noises reverberations noise sources contain sounds sampled daily-life environments youtube videos. improve loudness robustness also perform multi-loudness training scaling loudness training utterance randomly selected level ranging dbfs dbfs. far-ﬁeld eval sets used paper rerecorded real environments negative data sets mostly contain speech signals mined real voice search logs. size eval sets ranges hours. overlapping condition training eval sets. data used anonymized. evaluation metric uses receiver operating characteristic curve plotting false rejection rates false alarm rates. goal achieve rates maintaining extremely rates ﬁrst present results comparing log-mel frontend proposed pcen frontend. note compare log-mel frontend containing optimized spectral subtraction process already outperforms standard log-mel frontend. experiment pcen uses ﬁxed scalar parameters better demonstrate gain normalization effect pcen compare frontends without multiloudness training. fig. shows comparison rerecorded far-ﬁeld eval set. log-mel without multi-loudness training performs worst adding loudness variations training clearly helps log-mel. contrast proposed pcen frontend signiﬁcantly outperforms log-mel even multi-loudness trained. example hour pcen reduces rate absolute log-mel multi-loudness training. interestingly pcen without multi-loudness training perform similarly eval likely high strength nevertheless multiloudness training helps general needed trainable version pcen hence rest experiments multi-loudness training. emphasize multi-condition training large amounts data strong baseline hence obtained improvements quite signiﬁcant. addition also compared rasta ﬁltering widely used channel normalization technique found performance inferior subsection compare ﬁxed trained pcen. mentioned above trainable pcen generalize parameters frequency-dependent jointly optimize them including here randomly initialized normal distribution mean standard deviation randomly initialized normal distribution mean standard deviation number predetermined smoothers. preliminary experiments computed smoothers using smoothing coefﬁcients observed learned weights mostly assigned slowest fastest smoothers show performance comparisons rerecorded far-ﬁeld eval sets talking distances respectively. trained pcen consistently outperforms ﬁxed pcen eval sets. compared multi-loudness trained log-mel improvements trained pcen quite signiﬁcant especially regions. interesting question whether proposed frontend would actually hurt performance clean near-ﬁeld conditions. seen fig. ﬁxed pcen trained pcen outperform log-mel large clean nearfigure shows example smoother combination weights learned joint model. interestingly learned weights show alternating pattern frequency channels even-numbered channels prefer slower smoother odd-numbered channels faster smoother suspect pattern likely caused redundancy ﬁlterbank energy neighboring channels. network basically tries alternate different features obtain discriminative information. pattern could also tied speciﬁc architecture used. future would like design experiments better understand phenomenon. architecture shown fig. increases computational cost needs compute multiple smoothers ﬁrst. fact alternating pattern utilized reduce cost. inspired spiky alternating weights trained model single smoother computed using alternating smoothing coefﬁcients shown fig. performance resulting model actually similar full trained one. result important implication shows achieve better performance without increasing inference-time complexity. introduced robust frontend called pcen idea replace static compression agc-based dynamic compression. pcen conceptually simple computationally cheap easy implement. signiﬁcantly outperforms widely used log-mel frontend noisy far-ﬁeld conditions. also formulated pcen neural network layers. formulation allows perform end-to-end training also generalize pcen frequencytimefrequency dependent parameters. resulting model provides signiﬁcant improvements without increasing inferencetime complexity. conclude work represents effort embed signal processing components general-purpose neural network model signal processing components viewed structural regularizations. think promising future research direction. chen parada heigold small-footprint keyword spotting using deep neural networks acoustics speech signal processing ieee international conference hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups signal processing magazine ieee vol. prabhavalkar alvarez parada nakkiran sainath automatic gain control multi-style training robust small-footprint keyword spotting deep neural networks acoustics speech signal processing ieee international conference y.-h. chen lopez-moreno sainath visontai alvarez parada locally-connected convolutional neural networks small footprint speaker recognition proc. interspeech atal effectiveness linear prediction characteristics speech wave automatic speaker identiﬁcation veriﬁcation journal acoustical society america vol.", "year": 2016}