{"title": "Meta-learning within Projective Simulation", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Learning models of artificial intelligence can nowadays perform very well on a large variety of tasks. However, in practice different task environments are best handled by different learning models, rather than a single, universal, approach. Most non-trivial models thus require the adjustment of several to many learning parameters, which is often done on a case-by-case basis by an external party. Meta-learning refers to the ability of an agent to autonomously and dynamically adjust its own learning parameters, or meta-parameters. In this work we show how projective simulation, a recently developed model of artificial intelligence, can naturally be extended to account for meta-learning in reinforcement learning settings. The projective simulation approach is based on a random walk process over a network of clips. The suggested meta-learning scheme builds upon the same design and employs clip networks to monitor the agent's performance and to adjust its meta-parameters \"on the fly\". We distinguish between \"reflexive adaptation\" and \"adaptation through learning\", and show the utility of both approaches. In addition, a trade-off between flexibility and learning-time is addressed. The extended model is examined on three different kinds of reinforcement learning tasks, in which the agent has different optimal values of the meta-parameters, and is shown to perform well, reaching near-optimal to optimal success rates in all of them, without ever needing to manually adjust any meta-parameter.", "text": "learning models artiﬁcial intelligence nowadays perform well large variety tasks. however practice diﬀerent task environments best handled diﬀerent learning models rather single universal approach. non-trivial models thus require adjustment several many learning parameters often done case-by-case basis external party. meta-learning refers ability agent autonomously dynamically adjust learning parameters meta-parameters. work show projective simulation recently developed model artiﬁcial intelligence naturally extended account meta-learning reinforcement learning settings. projective simulation approach based random walk process network clips. suggested meta-learning scheme builds upon design employs clip networks monitor agent’s performance adjust meta-parameters distinguish reﬂexive adaptation adaptation learning show utility approaches. addition trade-oﬀ ﬂexibility learning-time addressed. extended model examined three diﬀerent kinds reinforcement learning tasks agent diﬀerent optimal values meta-parameters shown perform well reaching near-optimal optimal success rates them without ever needing manually adjust meta-parameter. many diﬀerent kinds artiﬁcial intelligent schemes. schemes diﬀer design purpose underlying principles feature common non-trivial proposals existence learning parameters reﬂect certain assumptions bias task environment agent cope moreover consequence so-called no-free lunch theorems known impossible ﬁxed parameters optimal task environments. practice parameters typically ﬁne-tuned manually external party case-by-case basis. autonomous agent however expected adjust learning parameters automatically itself. self monitoring adaptation agent’s internal settings often termed meta-learning literature term meta-learning deﬁned learning learn process acquiring metaknowledge used broad sense accounts various concepts. concepts tightly linked practical problems mostly considered context meta-learning. ﬁrst problem meta-learning accounts selection suitable learning model given task combination models including automatic adjustments task changed. second problem meta-learning accounts automatic tuning model learning parameters also referred meta-parameters reinforcement learning hyperparameters supervised learning. concepts meta-learning widely addressed framework supervised learning. problem supervised learning model selection algorithm recommendation solved e.g. knearest neighbor algorithm similarity-based methods meta decision trees empirical error criterion second problem meta-learning tuning hyperparameters usually solved gradient-based optimization grid search random search genetic algorithms bayesian optimization approaches combined algorithm selection hyperparameter optimization recently presented refs. framework agent learns interacting rewarding environment notion meta-learning usually addresses second practical problem need automatically adjust metaparameters discount factor learning rate exploitation-exploration parameter context also worthwhile mention g¨odel machine complexity interest predominantly theoretical construction possible meta-levels learning contained fully self-referential learning system. paper develop simple form meta-learning recently introduced model projective simulation model artiﬁcial intelligence particularly suited problems shown perform well comparison standard machinery toyreal-world tasks grid-world mountaincar cart-pole balancing problem inﬁnite mario game handles inﬁnitely large environments particular generalization mechanism). model physics-oriented aiming embodied realization random-walk memory structure primary process. based special type memory called episodic compositional memory represented directed weighted graph basic building blocks called clips clip represents memorized percept action combinations thereof. percept perceived agent corresponding percept clip activated initiating random-walk clip-network action clip corresponding action performed agent. realizes stochastic processing agent’s experience. elementary process namely randomwalk established theoretical concept known applications randomized algorithms thus providing large theoretical tool designing analyzing model. moreover random walk extended quantum regime leading quantum walks case polynomial even exponential improvements reported e.g. hitting mixing times results theory quantum walks suggest improvements performance achievable employing quantum analogues. recently quantum variant indeed formalized shown exhibit quadratic speed-up deliberation time classical counterpart perspective meta-learning comparatively simple model number learning parameters suggests providing agent meta-learning mechanism done maintaining overall simplicity. addition simplicity also structural homogeneity metalearning component combined basic model natural minimal external machinery. accordance requirements metalearning capability develop based supplementing basic network call base-level network additional meta-level networks dynamically monitor control meta-parameters. extends structure model single network several networks inﬂuence other. general facing challenge meta-learning immediately encounters trade-oﬀ eﬃciency success rates side ﬂexibility side humans example extremely ﬂexible robust changes environment eﬃcient reach sub-optimal success rates. machines hand learn fast perform optimally given task fail completely another. clearly achieve level robustness machines would repeatedly revise update internal design i.e. meta-learn process necessarily takes time. moreover reaching optimal success rates certain tasks require over-ﬁtting scheme’s meta-parameters might harm success tasks. therefore expected form meta-learning expense model’s efﬁciency success rates observe inclination also work. another aspect meta-learning highlight throughout paper underlying principles govern agent’s internal adjustment. here distinguish diﬀerent alternatives call reﬂexive adaptation adaptation learning. informally reﬂexive adaptation mean agent’s meta-parameters adjusted ﬁxed recipe takes account recent performance agent ignoring rest agent’s history. essentially amounts adaptation withneed additional memory. example reﬂexive adaptation approach meta-learning found fundamental parameters namely learning rate exploitation-exploration parameter discount factor tuned according predeﬁned equations; contrast agent adapts parameter learning exploits entire individual experience. accordingly adaptation learning require additional memory. work consider kinds approaches. follows section shortly describes model including metaparameters. section demonstrates advantages meta-learning considering explicit task scenarios model diﬀerent optimal values meta-parameters. section present proposed meta-learning design explain combines basic model. model examined analyzed simulations section performance meta-learning agent evaluated three diﬀerent types changing environments. throughsection proposed meta-learning scheme compared other naive alternatives meta-learning schemes. finally section concludes paper discusses open questions. terminology employ based basic classiﬁcation intelligent agents; perceive meta-learning machinery agent reﬂexive adaptation mechanism corresponds simple reﬂexive agents whereas learning adaptation mechanism corresponds learning agent. model episodic compositional memory formally network clips. possible clips include percept clips action clips also include representations various combinations percept action sequences within clip connected clip weighted directed edge corresponding time-dependent real positive weight larger equal initial value upon encountering percept clip corresponding percept activated random walk initiated. transition probability clip time step corresponds re-normalized h-values negative reward given environment. h-values edges traversed preceding random walk rewarded nonetheless damped away toward initial value update rule place probability take rewarded actions increased time agent learns. damping parameter meta-parameter model. higher faster agent forgets knowledge. certain settings introducing additional parameters network lead better learning performance. particularly useful generalization edge glow mechanism introduced model here additional time-dependent varidecay g-values ensures reward effects edges traversed diﬀerent points time diﬀerent extent. particular recently traversed edges enhanced relative edges traversed remote past. parameter controls strength temporal dependence. instance value implies edges traversed back past nonetheless enhanced. contrast setting last traversed path enhanced case update rule reverts back glow mechanism thus establishes temporal correlations percept-action pairs enables agent perform well also settings reward delayed and/or contingent immediate history agent-environment interaction basic variant model formally contrasted standard schemes closely resembles sarsa algorithm initial analysis relationship models given readers familiar sarsa model beneﬁt observation functional roles parameters sarsa closely matched parameters respectively. however explicitly state-action value function-based model analogy exact. details refer interested reader following section describe behaviour model functional role meta-parameters greater detail. basic memory update mechanism captured meta-parameters namely damping parameter glow parameter follows examine role parameters learning process agent. demonstrate examples none parameters unique value universally optimal i.e. diﬀerent environments induce diﬀerent optimal values. examples provide direct motivation introducing meta-learning mechanism model. getfulness agent continuously damping hvalues clip network. direct consequence non-zero value bounds h-values clip network ﬁnite value turn limits maximum achievable success probability agent. result many typical tasks considered literature environments consistent i.e. changing optimal performance achieved without damping setting however environment change agent need modify action pattern implies varying relative weights h-values. presetting ﬁnite parameter would quicken agent’s learning time environment expense reaching lower success probabilities demonstrated discussed ref. gives rise clear trend higher value faster relearning changing environment lower agent’s asymptotic success probability. trade-oﬀ learning time success probability changing environments demonstrated invasion game example. invasion game special case contextual multi-armed bandit problem temporal dependence. game agent defender block attacker moving direction attacker. making move attacker shows movement. essentially agent learn every given direction symbol. fig. illustrates agent learns receiving rewards successfully blocking attacker. here phase steps attacker goes right whenever shows right symbol then second phase game attacker inverts rules goes right whenever shows left seen higher values yield lower success probabilities allow faster learning second phase game. illustrate slow-down learning time changing environment setting fig. shows average success probability basic agent invasion game function number trials scale. attacker inverts rules whenever agent reaches certain success probability time agent needs learn phase grows exponentially number changes phases requiring time agent learn eventually ﬁnite learning time phase agent fails learn. setting zero damping parameter changing environments even harmful agent merely increasing learning time. give example consider invasion game attacker inverts figure invasion game attacker inverts strategy steps. agent’s average success probability plotted function number trials trade-oﬀ success probability relearning time depicted diﬀerent values. optimal value used. simulation done averaging agents. adapted figure invasion game attacker inverts strategy whenever agent’s success probability reaches agent’s performance plotted function number trials scale demonstrating learning times increase exponentially number inversions. simulation done single agent success probabilities extracted directly agent’s base-level network. meta-parameters performance agent considered examples shown figs. suggests important raise parameter whenever environment changes zero whenever performance steady. show section adjustment implemented means reﬂexive adaptation. however reﬂexive adaptation parameter makes meta-learning less general ﬁxes rule parameter adjustment. make agent general imoptimal strategy described game learned using glow mechanism carefully choosing parameter. optimal value depends number ships shown fig. dependence average reward received best average reward obtained using smaller value i.e. optimal value decreases. makes sense smaller value leads larger sequences rewarded actions. figure n-ship game dependence performance parameter shown diﬀerent performance evaluated average reward gained game. simulation done averaging adapted agents. parameter simulations n-ship game shown fig. emphasize importance setting suitable value. other involved scenarios dependency elusive making task setting proper value even harder. internal mechanism dynamically adapts glow parameter according external environment would therefore further enhance autonomy agent. section show implement internal mechanism means adaptation learning. invasion game attacker changes stratfigure every steps. agent’s average success probability plotted function number trials demonstrating attacker’s strategy learned. moreover performance agent averaged phases converges performance random agent. simulation done averaging agents agent success probabilities extracted directly base-level network. metaparameters plement adaptation also learning gives agent possibility learn opposite rule i.e. decrease whenever agent’s performance goes down. assumed glow mechanism turned setting optimal invasion game. holds environments rewards depend current percept-action pair temporal correlations previous percepts actions. next section however look scenarios temporal correlations exist study inﬂuence optimal value. task environments reward environment consequence series decisions made agent vital ensure last action rewarded entire sequence actions. otherwise previous actions eventually rewarded decision learned. described sec. rewarding sequence actions done model attributing time dependent g-value edge clip network rewarding edge reward proportional g-value. edge excited g-value whereas gvalues decay rate essentially determines extent past actions rewarded. show next actual value parameter plays crucial role obtaining high average reward optimal value depends task ﬁnding trivial. meta-level ecmξ network rewarded positively whenever agent performs better latter time window normalization plays role point however numerical value matter later change performance network rewarded. presented design requires speciﬁcation several quantities meta-level ecmξ network including time window number actions meaning action. follows specify choices meta-level networks. tivated every times steps. time window large enough allow agent gather reliable statistics performance. therefore sensible order learning time agent time takes agent reach certain fraction asymptotic success probability learning time shown linear number percepts actions base-level network. thus time window nηsasηaη also linear number percepts actions meta-level network. free parameter; higher value better statistics agent gathers. work throughout examples study. additional networks meta-parameter meta-level network denote ecmξ obeys principle structure dynamic base-level network described section composed clips activation initiates random-walk clips action-clip update rule given update rule albeit diﬀerent internal reward meta-level networks consider work two-layered single percept-clip several action-clips. action clips meta-level network determine next value corresponding metaparameter. illustrated schematically fig. base-level network activated every interaction environment meta-level ecmξ network activated every interactions environment. following activation action-clip encountered meta-parameter updated accordingly. time window meta-level network receives internal reward reﬂects well agent performed past interactions time steps compared performance previous time window. allows statistical evaluation agent’s performance last time window. hη-values η-network updated internal rewarding described agent learns time preferable values given scenario are. preferable values adjusted account changes environment. continuous adjustments η-network allow agent adapt environments learning. second meta-learning network damping metalevel network presented fig. composed actions correspond updating parameter using functions according following rules deﬁned rule invokes reﬂexive increase parameter agent’s performance deteriorates reﬂexive decrease performance improves. rule natural typical scenarios drop performance assumed signify change environment point agent well forget learned thus focus exploring options achieved increase contrast environment stable phase agent learns performance improves causing decrease lead optimal performance. rule chosen exactly opposite namely performance increase causes agent forget. main purpose introduction rule demonstrate ﬂexibility meta-learning agent learn even correct strategy updating although environments typically considered literature work natural rule better choice thus could principle hard-wired agent challenged learn even this. role parameter throughout work avoid unwanted increase statistical ﬂuctuations. note functions eqs. described γ-network activated every nγτη steps time window glow network deﬁned above free parameter γ-network throughout paper. agent ﬁrst learns estimate range optimal changes afterwards. assured choosing time window γ-network larger η-network. relationship time windows required order agent gain meaningful statistics steps. otherwise learned ﬁrst agent’s performance signiﬁcantly ﬂuctuate leading erratic changes reﬂexive adaptation rule. note large ﬂuctuations yield poor learning results even moderate values lead rapid forgetting agent. meta-learning ecmγ network realized follows. starting initially random value parameter adapted direct learning γ-network reﬂexive adaptation rule rule given overall structure environment learned henceforth adapted reﬂexively. reﬂexive rules reﬂect a-priory knowledge strategy preferable given environments. note parameters could learned without reﬂexive rules using networks directly select values however approaches shown much less eﬃcient. general reﬂexive adaptation meta-parameters preferable adaptation learning simpler. need learning adaptation arises landscape optimal values meta-parameters straightforward case parameter illustrated fig. possible concoct settings opposite rule beneﬁcial using minor major rewards. environment minor rewards train agent deterministic behavior certain time periods major reward issued agent nonetheless produced random outcomes along. periods appropriately tailored train meta-learning network prefer opposite rule. study pathological settings principal interest work. examine proposed meta-learning mechanism next evaluate performance meta-learning agent several environments namely invasion game n-ship game variants grid-world setting. environments chosen diﬀerent structures exhibit diﬀerent optimal damping glow parameters. goal agent adjust meta-parameters properly cope well diﬀerent tasks. challenge agent even further three environments suddenly change thereby forcing agent readjust parameters accordingly. critically tasks meta-level networks used along choice free parameters described sections a-iv demonstrate role meta-learning mechanism compare performance meta-learning agent performance agent without mechanism. without meta-learning agent starts task random parameters change afterwards. show importance learning optimal parameter construct second reference agent comparison uses γ-network adjust parameter takes random choice possible η-actions η-network. start simplest task invasion game before agent rewarded whenever manages block attacker learn whether attacker left right presenting symbols. consider again scenario attacker switches strategies every ﬁxed number trials. phase game goes left showing left symbol whereas phase opposite. repeated several times. task agent block attacker regardless strategy. recall scenario basic agent ﬁxed meta-parameters cope ﬁrst phase fails completely second. times. seen time average success probability agents increases towards optimal values agents manage block attacker equally well strategies. performance achieved meta-learning parameters dynamics shown solid blue fig. respectively. seen fig. time several phase changes value parameter raises sharply whenever attacker changes strategy decays toward zero following phase. allows agent rapidly forget knowledge previous strategy learn one. fig. shows parameter dynamics. explained section optimal glow value invasion-game environment induces temporal correlations previous actions rewards. meta-learning agent begins depicted full meta-learning capability adjusted value value chosen randomly ecmη network agents whose values ﬁxed random values agent values performances diﬀerent agents shown function trials; middle average value shown function trials; bottom average value shown function trials. simulations done averaging agents agent success probabilities extracted directly base-level network. show advantage meta-learning networks next consider performance agents without mechanism. first look performance agent ﬁxed random parameters shown fig. dotted gray. seen average agent performs rather poor average success rate expected values fact harmful agent’s success. average value parameter goes depicted fig. parameter fig. parameter dotted gray challenging comparison shown fig. dashed agent adjusts parameter exactly like meta-learning uses η-network learn update. seen intermediate agent already learn phases extent reach optimal values. small values corresponding sustained glow several learning cycles harmful case. dynamics parameters agent shown fig. dashed behavior similar meta-learning agent average ﬂuctuates around average value possible actions η-network. example encounter ﬁrst time trade-oﬀ ﬂexibility learning time. meta-learning agent exhibits high ﬂexibility robustness manages repeatedly adapt changes environment. however comes price learning time agent slows agent requires millions trials master task. however expected. agent learn changing environment must also learn properly adapt meta-parameters latter occurs time-scales elementary cycles. agent begins bias whatsoever regarding action pattern parameters. furthermore agent begins a-priory knowledge regarding inherent nature rewarding process environment typical environment untypical environment ultimately rewards random behavior needs learned. fig. shows average probability n-ship game environment rewards agent depending previous actions. follows consider dynamic n-ship game allow change time. particular environment starts increases number ships fig. shows solid blue performance meta-learning agent changing n-ship game. best possible reward indicated dashed blue horizontal line seen agents learn perform optimally number ships success made possible meta-learning mechanism. first parameter adjusted agent forgets whenever performance decrease vice versa assume γ-network already learned previous stages environment follows natural rule γ-network leads dynamics average parameter shown fig. solid blue. seen whenever environment changes parameter increases thereby allowing agent forget previous knowledge. slow decrease damping parameter makes possible agent learn setup. second glow parameter adjusted dynamically. fig. shows probability distribution choosing action η-network phase. seen grows η-network learns choose smaller smaller glow parameter value allows back propagation reward ﬁnal ship optimal values. shown fig. metalearning agent essentially captures knowledge η-network. time knowledge obtained agent’s experience rather external party. agent without meta-learning able achieve similar performance. performance agent ﬁxed random poor shown because behavior close random strategy performance expected values harmful agent’s success. show challenging comparison agent adjusts parameter exactly like meta-learning uses η-network learn update. seen intermediate agent cope environment higher values fails reach optimal performance random parameter achieves mixture greedy strategy optimal strategy last example consider benchmark problem form grid-world setup presented ref. delayed-reward scenario agent walks maze gets rewarded reaches goal. position agent move four directions left right down. move counts single step. reaching goal marks current trial agent reset initial place start another round. basic agent shown perform well benchmark task here challenge meta-learning scheme situate agent three diﬀerent kinds grid-worlds basic grid-world ref. illustrated left part fig. sized grid-world walls positioned diﬀerently shown middle part fig. original grid-world additional small distracting reward λmin placed steps agent shown right part fig. game ends either reward λmax small reward λmin reached. cases shortest path reward composed steps. since agent goes phases forget previous knowledge whenever phase encountered adjust metaparameters scenario. third phase poses additional challenge agent optimal performance must avoid taking small reward larger one. trials. types agents depicted full metalearning capability adjusted value value chosen randomly ecmη network performance diﬀerent agents shown function trials terms average reward. phase average reward optimal strategy greedy strategy fully random plotted dashed light-blue dotted-dashed purple dotted orange respectively; middle average values diﬀerent kinds agents shown function trials; bottom meta-learning agent probability choose η-actions plotted phase diﬀerent plot. connecting lines shown guide eyes. simulations done averaging agents agent average reward extracted directly base-level network. figure three setups grid-world task. left basic grid-world presented ref. middle walls positioned diﬀerently; right basic grid-world distracting small reward λmin placed steps agent. three setups large reward λmax awaits agent steps. terms steps reward function trials. cases optimal performance steps reward. last phase greedy agent would reach small reward λmin steps thus resulting steps unit reward. seen meta-learning agent performs optimally phases except last phase performance suboptimal average steps unit reward ﬂexibility phases achieved adjustments parameter whose progress time shown fig. solid blue. similar n-ship game assume γ-network already learned environment uses straightforward logic setting h-value ratio choosing rule agent γ-network withupdated η-network performs similarly ﬁrst phases dashed red) terms ﬁnding eventually optimal path. expected ﬁnding optimal path necessary seen however agent learns much slower full meta-learning agent hundreds thousands steps required average optimal path. also reﬂected behavior parameter random value parameter goes zero much slower shown fig. importance parameter however better demonstrated third phase diﬀerence achieved performance agent without η-learning signiﬁcant. particular agent random converges greedy strategy gets unit reward every steps dashed red). reason optimal performance achieved setting parameter value certain limited range analyze next. range optimal values obtained focusing location grid-world location agent possible actions lead faster large small rewards namely down respectively. actions result faster reward edges corresponding actions enhanced stronger actions. enhancement gained adding increments h-values game update rule agent follows greedy strategy increment equal λmin added edge corresponding action down. optimal strategy prevail increment game larger greedy strategy. case actions η-network agent values larger therefore agent random actions converges greedy strategy best possible reward. agent meta-learning able learn parameter optimal range shown fig. agent indeed mostly uses values developed meta-learning machinery allows agent dynamically adjust metaparameters. shown desirable demonstrating that like schemes unique choice model’s learning parameters account possible tasks optimal values meta-parameters vary task environment another. emphasize presented meta-learning component based design basic using random walk clip-networks central information processing step. therefore naturally integrated learning framework preserving model’s stochastic nature along simplicity. basic principal meta-parameters damping parameter glow parameter meta-parameter assigned meta-learning clipnetwork whose actions control parameter’s value. meta-level network activated every ﬁxed number interactions environment. time window allows agent gather statistics performance monitor recent success rates thereby evaluate setting corresponding parameters. agent’s success increases previous action meta-level network rewarded positively otherwise performance deteriorates negative reward assigned result probability random walk meta-level network hits favorable action clips increases time meta-level network essentially learns properly adjust corresponding parameter current environment. meta-learning process occurs much larger time-scale compared base-level network learning time scale. necessary meta-level learning requires statistical knowledge agent’s performance directly controlled base-level network whose learning time linear state space task represented number percepts actions base-level network. meta-level learning distinguished adaptation learning exploits entire individual history agent update value meta-parameter reﬂexive adaptation updates meta-parameter using recent localized information agent’s performance. glow parameter well adjusted full learning network adaptation learning whereas damping parameter sensible combine kinds adaptations. presented meta-learning scheme examined three diﬀerent environmental scenarios requires diﬀerent meta-parameters optimal performance. speciﬁcally considered invasion game temporal correlations actions rewards n-ship game temporal correlations exist ηopt depends ﬁnally grid-world real-world scenario descenarios environment furthermore suddenly changes thus requiring agent also adjust forgetting parameter overall situating agent changing environments enforces repeatedly dynamically revise internal settings. meta-learning agent shown cope well scenarios reaching success probabilities approach near-optimal optimal values. comparison checked agent ﬁxed random meta-parameters would handle scenarios observed agent would perform signiﬁcantly worse. surprising possible meta-parameter values harmful agent. therefore figure grid-world task types agents depicted full meta-learning capability adjusted value value chosen randomly ecmη network performances diﬀerent agents shown function trials terms average number steps unit reward; middle average values diﬀerent kinds agents shown function trials; bottom meta-learning agent probability choose probability choose either η-actions plotted function trials; ﬁrst phases game last trials whereas last phase lasts trials. phases correspond challenging comparison checked performance agent adapts forgetting parameter exactly meta-learning agent chooses glow parameter randomly actions available η-network used. intermediate agent performed better basic agent random choice meta-parameters substantially worse full meta-learning agent. demonstrates importance adjusting proper way. particular shows learning importantly throughout paper used particset choices meta-learning scheme. ular used meta-level networks ecmγ ecmη time windows indicates suggested meta-learning scheme robust requires adjustment additional parameters external party cases considered. duch grudziski meta-learning search combined parameter optimization. kopotek wierzcho michalewicz intelligent information systems vol. advances soft computing brazdil soares costa ranking learning algorithms using meta-learning accuracy time results. machine learning zhao model selection consistency lasso. journal machine learning research abdulrahman brazdil rijn vanschoren algorithm selection meta-learning sample-based active testing. proceedings metalearning algorithm selection workshop ecmlpkdd kobayashi mizoue kuremoto obayashi meta-learning method based temporal diﬀerence error. leung chan neural information processing vol. lecture notes computer science tokic schwenker palm meta-learning exploration exploitation parameters replacing eligibility traces. zhou z.-h. schwenker partially supervised learning vol. lecture notes computer science bardenet k´egl surrogating surrogate accelerating gaussian-process-based global optimization mixture cross-entropy algorithm. proceedings international conference machine learning thornton hutter hoos leyton-brown auto-weka combined selection hyperparameter optimization classiﬁcation algorithms. proceedings sigkdd international conference knowledge discovery data mining smith mitchell giraud-carrier martinez recommending learning algorithms assoproceedings metaciated hyperparameters. learning algorithm selection workshop ecai achbany fouss pirotte saerens tuning continual exploration reinforcement learning optimality property boltzmann strategy. neurocomputing artiﬁcial schmidhuber completely self-referential optimal reinforcement learners. artiﬁcial neural networks formal models applications vol. lecture notes computer science briegel cuevas projective simulation artiﬁcial intelligence. scientiﬁc reports mautner makmal manzano tiersch briegel projective simulation classical learning agents comprehensive investigation. generation computing sutton integrated architectures learning planning reacting based approximating dynamic programming. proceedings international conference machine learning", "year": 2016}