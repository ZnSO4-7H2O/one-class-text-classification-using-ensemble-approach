{"title": "Visualizing and Understanding Recurrent Networks", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.", "text": "recurrent neural networks speciﬁcally variant long shortterm memory enjoying renewed interest result successful applications wide range machine learning problems involve sequential data. however lstms provide exceptional results practice source performance limitations remain rather poorly understood. using character-level language models interpretable testbed bridge providing analysis representations predictions error types. particular experiments reveal existence interpretable cells keep track long-range dependencies line lengths quotes brackets. moreover comparative analysis ﬁnite horizon n-gram models traces source lstm improvements long-range structural dependencies. finally provide analysis remaining errors suggests areas study. recurrent neural networks speciﬁcally variant long short-term memory hochreiter schmidhuber recently emerged effective model wide variety applications involve sequential data. include language modeling mikolov handwriting recognition generation graves machine translation sutskever bahdanau speech recognition graves video analysis donahue image captioning vinyals karpathy fei-fei however source impressive performance shortcomings remain poorly understood. raises concerns lack interpretability limits ability design better architectures. recent ablation studies analyzed effects performance various gates connections removed greff chung however analysis illuminates performance-critical pieces architecture still limited examining effects global level ﬁnal test perplexity alone. similarly often cited advantage lstm architecture store retrieve information long time scales using gating mechanisms ability carefully studied settings hochreiter schmidhuber however immediately clear similar mechanisms effectively discovered utilized networks real-world data common simple stochastic gradient descent truncated backpropagation time. knowledge work provides ﬁrst empirical exploration predictions lstms learned representations real-world data. concretely character-level language models interpretable testbed illuminating long-range dependencies learned lstms. analysis reveals existence cells robustly identify interpretable high-level patterns line lengths brackets quotes. quantify lstm predictions comprehensive comparison n-gram models lstms perform signiﬁcantly better characters require long-range reasoning. finally conduct error analysis peel onion errors sequence oracles. results allow quantify extent remaining errors several categories suggest speciﬁc areas study. related work recurrent networks. recurrent neural networks long history applications various sequence learning tasks werbos schmidhuber rumelhart despite early successes difﬁculty training simple recurrent networks bengio pascanu encouraged various proposals improvements basic architecture. among successful variants long short term memory networks hochreiter schmidhuber principle store retrieve information long time periods explicit gating mechanisms built-in constant error carousel. recent years renewed interest improving basic architecture modifying functional form seen gated recurrent units incorporating content-based soft attention mechanisms bahdanau weston push-pop stacks joulin mikolov generally external memory arrays content-based relative addressing mechanisms graves work focus majority analysis lstm widespread popularity proven track record. understanding recurrent networks. abundance work modiﬁes extends basic lstm architecture relatively little attention paid understanding properties representations predictions. greff recently conducted comprehensive study lstm components. chung evaluated compared lstms chung jozefowicz conduct automated architecture search thousands architectures. pascanu examined effects depth approaches study recurrent network based variations ﬁnal test cross entropy break performance interpretable categories study individual error types. related work hermans schrauwen also study long-term interactions learned recurrent networks context character-level language models speciﬁcally context parenthesis closing time-scales analysis. work complements results provides additional types analysis. lastly heavily inﬂuenced work in-depth analysis errors object detection hoiem ﬁnal mean average precision similarly broken studied detail. experimental setup ﬁrst describe three commonly used recurrent network architectures describe used sequence learning ﬁnally discuss optimization. recurrent neural network models simplest instantiation deep recurrent network arranges hidden state vectors twodimensional grid thought time depth. bottom depth zero holds input vectors vector vectors computed recurrence used predict output vector intermediate vectors hidden vectors output time step formula based becomes function input vectors xt}. precise mathematical form recurrence recurrence form assume parameter matrix layer dimensions tanh applied elementwise. note varies layers shared time. omit bias vectors brevity. interpreting equation above inputs layer depth transformed interact additive interaction squashed tanh. known weak form coupling sutskever lstm include powerful multiplicative interactions. long short-term memory hochreiter schmidhuber designed address difﬁculties training rnns bengio particular observed backpropagation dynamics caused gradients either vanish explode. later found exploding gradient concern alleviated heuristic clipping gradients maximum value pascanu hand lstms designed mitigate vanishing gradient problem. addition hidden state vector lstms also maintain here sigmoid function sigm tanh applied element-wise matrix. three vectors thought binary gates control whether memory cell updated whether reset zero whether local state revealed hidden vector respectively. activations gates based sigmoid function hence allowed range smoothly zero keep model differentiable. vector ranges used additively modify memory contents. additive interaction critical feature lstm’s design backpropagation operation merely distributes gradients. allows gradients memory cells backwards time uninterrupted long time periods least disrupted multiplicative interaction active forget gate. lastly note implementation lstm requires maintain vectors recently proposed simpler alternative lstm takes form character-level language modeling character-level language modeling interpretable testbed sequence learning. setting input network sequence characters network trained predict next character sequence softmax classiﬁer time step. concretely assuming ﬁxed vocabulary characters encode characters k-dimensional -of-k vectors {xt} feed recurrent network obtain sequence d-dimensional obtain predictions next hidden vectors last layer network character sequence project layer activations sequence vectors {yt} parameter matrix. vectors interpreted holding wyhl probability next character sequence objective minimize average cross-entropy loss targets. optimization following previous work sutskever initialize parameters uniformly range mini-batch stochastic gradient descent batch size rmsprop dauphin per-parameter adaptive update base learning rate decay settings work robustly models. network unrolled time steps. train model epochs decay learning rate epochs multiplying factor additional epoch. early stopping based validation performance cross-validate amount dropout model individually. experiments datasets. datasets previously used context character-level language models penn treebank dataset marcus hutter prize wikipedia dataset hutter however datasets contain common language special markup. goal compete previous work rather study recurrent networks controlled setting ends spectrum degree structure. therefore chose tolstoy’s peace novel consists characters almost entirely english text minimal markup spectrum source code linux kernel shufﬂed header source ﬁles randomly concatenated single form character long dataset. split data train/val/test splits figure left test cross-entropy loss models datasets models nearly equal number parameters. test characters. standard deviation estimated bootstrap samples less cases. right t-sne embedding based probabilities assigned test characters model peace. color size marker correspond model type model size number layers. comparing recurrent networks ﬁrst train several recurrent network models support analysis compare performance controlled setting. particular train models cross product type number layers number parameters datasets -layer lstm used hidden size vectors cells character vocabulary sizes translates approximately parameters respectively. sizes hidden layers models carefully chosen total number parameters case close possible settings. test results shown figure consistent ﬁnding depth least beneﬁcial. however three layers results mixed. additionally results mixed lstm signiﬁcantly outperform rnn. also computed fraction times pair models agree likely character render t-sne maaten hinton embedding plot supports claim lstm make similar predictions rnns form cluster. interpretable long-range lstm cells. lstms principle memory cells remember long-range information keep track various attributes text currently processing. instance simple exercise write cell weights would allow cell keep track whether inside quoted string. however knowledge existence cells never experimentally demonstrated real-world data. particular could argued even lstm principle capable using operations practical optimization challenges might prevent discovering solutions. experiment verify multiple interpretable cells fact exist networks instance cell clearly acting line length counter starting high value slowly decaying character next newline. cells turn inside quotes parenthesis statements inside strings comments increasing strength indentation block code increases. particular note truncated backpropagation hyperparameters prevents gradient signal directly noticing dependencies longer characters still observe cells reliably keep track quotes comment blocks much longer characters hypothesize cells ﬁrst develop patterns shorter characters also appropriately generalize longer sequences. gate activation statistics. gain insight internal mechanisms lstm studying gate activations networks process test data. particularly interested looking distributions saturation regimes networks deﬁne figure left three saturation plots lstm. circle gate lstm position determined fraction time left right-saturated. fractions must right saturation plot -layer model. gate left right-saturated activation less respectively unsaturated otherwise. compute fraction times lstm gate spends left right saturated plot results figure instance number often right-saturated forget gates particularly interesting since corresponds cells remember values long time periods. note multiple cells almost always right-saturated hence function nearly perfect integrators. conversely cells function purely feed-forward fashion since forget gates would show consistently left-saturated output gate statistics also reveal cells consistently revealed blocked hidden state. lastly surprising ﬁnding unlike layers contain gates nearly binary regime operation activations ﬁrst layer much diffuse struggle explain ﬁnding note present across models. similar effect present model ﬁrst layer reset gates nearly never right-saturated update gates rarely ever left-saturated. points towards purely feed-forward mode operation layer previous hidden state barely used. understanding long-range interactions good performance lstms frequently attributed ability store long-range information. section test hypothesis comparing lstm baseline models utilize information ﬁxed number previous steps. particular consider baselines n-nn fully-connected neural network hidden layer tanh nonlinearities. input network sparse binary vector dimension concatenates one-of-k figure left overlap test-set errors best -layer lstm n-gram models middle/right mean probabilities assigned correct character broken character sorted difference models. space character. lstm outperforms -gram model special characters require long-range reasoning. middle dataset right dataset. n-gram unpruned -gram language model using modiﬁed kneser-ney smoothing chen goodman standard smoothing method language models huang models trained using popular kenlm software package heaﬁeld performance comparisons. performance n-gram models shown table n-gram n-nn models perform nearly identically small values larger values n-nn models start overﬁt n-gram model performs better. moreover datasets best recurrent network outperforms -gram model difﬁcult make direct model size comparison -gram model largest checkpoints however assumptions encoded kneser-ney smoothing model intended word-level modeling natural language optimal character-level data. despite concern results already provide weak evidence recurrent networks effectively utilizing information beyond characters. error analysis. instructive delve deeper errors made recurrent networks n-gram models. particular deﬁne character error probability assigned model previous time step figure shows overlap test-set errors -layer lstm best n-nn n-gram models. majority errors shared three models model also unique errors. gain deeper insight errors unique lstm -gram model compute mean probability assigned character vocabulary across test set. figure display characters model largest advantage other. linux kernel dataset lstm displays large advantage special characters used structure programs including whitespace brackets. peace dataset features interesting long-term dependency carriage return occurs approximately every characters. figure shows lstm distinct advantage character. accurately predict presence carriage return model likely needs keep track distance since last carriage return. cell example we’ve highlighted figure seems particularly well-tuned speciﬁc task. similarly predict closing bracket quotation mark model must aware corresponding open bracket appeared many time steps ago. fact lstm performs signiﬁcantly better figure left mean probabilities lstm -gram model assign character bucketed distance matching right comparison similarity -layer lstm n-nn baselines ﬁrst epochs training measured symmetric kldivergence test loss indicates similar predictions positive ∆loss indicates lstm outperforms baseline. gram model characters provides strong evidence model capable effectively keeping track long-range interactions. case study closing brace. structural characters requires longest-term reasoning closing brace linux kernel dataset. braces used denote blocks code nested; such distance opening brace corresponding closing brace range tens hundreds characters. feature makes closing brace ideal test case studying ability lstm reason various time scales. group closing brace characters test distance corresponding open brace compute mean probability assigned lstm -gram model closing braces within group. results shown figure first note lstm slightly outperforms -gram model ﬁrst distance braces characters. point performance -gram model stays relatively constant reﬂecting baseline probability predicting closing brace without seeing matching opening brace. compared baseline lstm gains signiﬁcant boosts characters performance delta slowly decays time becomes difﬁcult keep track dependence. training dynamics. also instructive examine training dynamics lstm comparing trained n-nn models training using divergence predictive distributions test set. plot divergence difference mean loss figure notably ﬁrst iterations lstm behaves like model diverges soon after. lstm behaves like models turn. experiment suggests lstm grows competence increasingly longer dependencies training. insight might related sutskever sutskever observe improvements reverse source sentences encoder-decoder architecture machine translation. inversion introduces short-term dependencies lstm model ﬁrst longer dependencies learned time. error analysis breaking failure cases section break lstm’s errors categories study remaining limitations relative severity error type suggest areas study. focus peace dataset easier categorize errors. approach peel onion iteratively removing errors series constructed oracles. last section consider character error probability assigned model previous time step note order oracles applied inﬂuences results. tried apply oracles order increasing difﬁculty removing error category believe ﬁnal results instructive despite downside. oracles order n-gram oracle. first construct optimistic n-gram oracles eliminate errors might ﬁxed better modeling short dependencies. particular evaluate n-gram model remove character error correctly classiﬁed models. gives approximate idea amount signal present last characters many errors could optimistically hope eliminate without needing reason long time horizons. dynamic n-long memory oracle. motivate next oracle consider string yelled mary mary couldn’t hear him. interesting consistent failure mode noticed figure left lstm errors removed oracles starting chart going counter-clockwise. area slice corresponds fraction errors contributed. n-memory refers dynamic memory oracle context previous characters. word t-train refers rare words oracle word count threshold right concrete examples text test error type. blue color highlights relevant characters associated error. memory category also highlight repeated substrings bounding rectangles. predictions lstm fails predict characters ﬁrst occurrence mary almost always also fail predict characters second occurrence nearly identical pattern errors. however principle presence ﬁrst mention make second much likely. lstm could conceivably store summary previously seen characters data fall back memory uncertain. however appear take place practice. limitation related improvements seen dynamic evaluation mikolov jelinek recurrent language models allowed train test characters evaluation long sees once. mode operation trains ﬁrst occurrence mary probabilities second occurrence signiﬁcantly better. hypothesize dynamic aspect common feature sequence data certain subsequences might frequently occur training data still likely present immediate history. however general algorithm seem learned lstm. dynamic memory oracle quantiﬁes severity limitation removing errors words found substring last characters rare words oracle. next construct oracle eliminates errors rare words occur times training data estimates severity errors could optimistically eliminated increasing size training data pretraining. word model oracle. noticed large portion errors occur ﬁrst character word. intuitively task selecting next word sequence harder completing last characters known word. motivated observation constructed oracle eliminated errors space quote newline. interestingly high portion errors found newline since models learn newline semantics similar space. punctuation oracle. remaining errors become difﬁcult blame particular interpretable aspect modeling. point construct oracle removes errors punctuation. boost oracles. remaining errors show salient structures patterns removed oracle boosts probability correct letter ﬁxed amount. oracles allow understand distribution difﬁculty remaining errors. allow understand error break changes scale model. error breakdown applying oracle models found figure error breakdown. total best lstm model made total errors test characters these n-gram oracle eliminates suggesting model taking full advantage last characters. dynamic memory oracle eliminates errors. principle dynamic evaluation scheme could used mitigate error believe principled approach could involve approach similar memory networks weston model allowed attend recent history sequence making next prediction. finally rare words oracle accounts errors. error type might mitigated unsupervised pretraining increasing size training set. majority remaining errors follow space quote newline indicating model’s difﬁculty word-level predictions. suggests longer time horizons backpropagation time possibly hierarchical context models could provide improvements. figure examples error type. believe type error breakdown valuable tool isolating understanding source improvements provided proposed models. errors eliminated scaling contrast smaller lstm model makes total errors approximately large model. surprisingly errors n-gram errors come boost category remaining distributed across categories relatively evenly. scaling model factor number parameters almost entirely provided gains local n-gram error rate left error categories untouched comparison. analysis provides evidence might necessary develop architectural improvements instead simply scaling basic model. conclusion used character-level language models interpretable test analyzing predictions representations training dynamics error types present recurrent neural networks. particular qualitative visualization experiments cell activation statistics comparisons ﬁnite horizon n-gram models demonstrate networks learn powerful often interpretable long-range interactions real-world data. error analysis broke cross entropy loss several interpretable categories allowed illuminate sources remaining limitations suggest areas study. particular found scaling model almost entirely eliminates errors n-gram category provides evidence architectural innovations needed address remaining errors. kyunghyun merri¨enboer bart bahdanau dzmitry bengio yoshua. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. chung junyoung gulcehre caglar kyunghyun bengio yoshua. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. dauphin yann vries harm chung junyoung bengio yoshua. rmsprop equilibrated adaptive learning rates non-convex optimization. arxiv preprint arxiv. donahue jeff hendricks lisa anne guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate darrell trevor. long-term recurrent convolutional networks visual recognition description. cvpr graves alex mohamed hinton geoffrey. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee heaﬁeld kenneth pouzyrevsky ivan clark jonathan koehn philipp. scalable modiﬁed kneser-ney language model estimation. proceedings annual meeting association computational linguistics soﬁa bulgaria august http//kheafield.com/professional/edinburgh/estimate_paper.pdf. jozefowicz rafal zaremba wojciech sutskever ilya. empirical exploration recurrent network architectures. proceedings international conference machine learning mikolov tomas karaﬁ´at martin burget lukas cernock`y khudanpur sanjeev. recurrent neural network based language model. interspeech annual conference international speech communication association makuhari chiba japan september sutskever ilya martens james hinton geoffrey generating text recurrent neural networks. proceedings international conference machine learning", "year": 2015}