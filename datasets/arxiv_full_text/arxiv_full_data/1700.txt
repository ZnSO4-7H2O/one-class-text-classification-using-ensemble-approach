{"title": "Using NLP to measure democracy", "tag": ["cs.CL", "cs.IR", "cs.LG", "stat.ML"], "abstract": "This paper uses natural language processing to create the first machine-coded democracy index, which I call Automated Democracy Scores (ADS). The ADS are based on 42 million news articles from 6,043 different sources and cover all independent countries in the 1993-2012 period. Unlike the democracy indices we have today the ADS are replicable and have standard errors small enough to actually distinguish between cases.  The ADS are produced with supervised learning. Three approaches are tried: a) a combination of Latent Semantic Analysis and tree-based regression methods; b) a combination of Latent Dirichlet Allocation and tree-based regression methods; and c) the Wordscores algorithm. The Wordscores algorithm outperforms the alternatives, so it is the one on which the ADS are based.  There is a web application where anyone can change the training set and see how the results change: democracy-scores.org", "text": "paper uses natural language processing create ﬁrst machine-coded democracy index call automated democracy scores based million news articles diﬀerent sources cover independent countries period. unlike democracy indices today replicable standard errors small enough actually distinguish cases. produced supervised learning. three approaches tried combination latent semantic analysis tree-based regression methods; combination latent dirichlet allocation tree-based regression methods; wordscores algorithm. wordscores algorithm outperforms alternatives based. democracy central variable economics political science. countries democratic others? coups happen? democracy impact economic policy? democratization aﬀect probability country going war? questions economists political scientists concern themselves. answer questions researchers need democracy measured somehow. democracy indices come least twelve democracy indices today popular polity assigns score countries year period. another popular dena¸c˜ao aperfei¸coamento pessoal n´ıvel superior capes minist´erio planejamento or¸camento gest˜ao mpog allocation computing time ohio state supercomputer. thank irfan nooruddin sarah brooks marcus kurtz janet boxsteﬀensmeier philipp rehm paul debell carolyn morgan peter tunkis vittorio merola margaret hanson helpful comments. errors mine. democracy indices draw extent dahl’s conceptualization democracy mixture competition participation. none replicable provide adequate measures uncertainty. democracy indices today rely directly indirectly country experts checking boxes questionnaires. observe boxes checking why; observe ﬁnal scores. process opaque odds increasingly demanding standards openness replicability ﬁeld. importantly opacity makes easy country experts boost scores countries adopt ‘correct’ policies. coding rules help still leave much open interpretation. instance consider excerpt polity handbook regime bans major rival parties allows minor political parties operate coded here. however parties must degree autonomy ruling party/faction must represent moderate ideological/philosophical although political challenge incumbent regime.’ measure autonomy? always observe ‘moderate’ clearly hard smuggle ideological contraband democracy scores. ideological biases turn make empirical tests circular. association democracy policy genuine association artifact human coders’ preferences regarding democracy measures today hard know. every time regress polity scores freedom house scores policy regressing instead. another problem existing indices lack proper standard errors. popular indices polity freedom house give point estimates without measure uncertainty. prevents knowing whether uruguay really democratic argentina whether uncertainty measurement process suﬃcient make statistically indistinguishable. words cannot descriptive inference. moreover without standard errors cannot causal inference democracy regressors. treier jackman warn ‘whenever democracy appears exploratory variable empirical work errors-in-variables problem potentially invalidating substantive conclusions studies’ measure standard errors uniﬁed democracy scores created pemstein produce pemstein treated democracy latent variable used multirater ordinal probit model extract latent variable twelve diﬀerent democracy measures comes point estimates conﬁdence intervals hence need democracy index replicable comes standard errors small enough actually distinguish cases. paper natural language processing create measure. basic idea simple. news articles north korea cuba contain words like ‘censorship’ ‘repression’ often news articles belgium australia. hence news articles contain quantiﬁable regime-related information create democracy index. produce relied supervised learning. tried three diﬀerent approaches compared results picked approach worked best. speciﬁcally tried combination latent semantic analysis tree-based regression methods; combination latent dirichlet allocation tree-based regression methods; wordscores algorithm. wordscores algorithm outperformed alternatives. total news sources. news sources english available lexisnexis academic online repository journalistic content. list includes american newspapers like york times today washington post; foreign newspapers like guardian daily telegraph; news agencies like reuters agence france presse associated press; online sources like blogs stations’ websites. lexisnexis’ internal taxonomy identify select articles contain regime-related news. particular choose articles following tags ‘human rights violations’ ‘elections politics’ ‘human rights’ ‘human rights civil liberties law’ ‘censorship’ lexisnexis’ news database covers period -present principle could cover period well. practice however lexisnexis provide search codes countries ceased exist cannot reliably retrieve news articles soviet union east germany hence limit interval. selection i.e. regime-related news countries exist today results total million articles organize country-year. help reduce spurious associations remove country-year merge corresponding news articles single document transform term-frequency vector. merge vectors together term-frequency matrix. variations collection news articles. ﬁrst variation call corpus described above changes. contains million unique words proper nouns removed probabilistically. second variation call corpus corpus minus frequent words english language; word appear adopt supervised learning approach. supervised learning feed machine number pre-scored cases training data. machine ‘learns’ training data. text analysis means learning frequency word topic varies according document scores. instance algorithm learn word ‘censorship’ frequent lower democracy score document. finally algorithm uses knowledge assign scores cases i.e. test data. period gives total country-years. choose year training data extract corresponding scores uniﬁed democracy scores data countries year hence samples training data samples test data. select year simply ﬁrst year dataset. select amalgamation several democracy scores reduces measurement noise. start term-frequency matrix rows represent terms columns represent documents entry frequency term document case column represents news articles given country published given year instance france- colombia-. appears document less appears whole corpus. hence helps reduce weights inane words like ‘the’ ‘of’ increase weights discriminant words megabytes longer documents contain unique words larger values skew results avoid normalize number unique words documents number documents). ﬁnally ready lsa. broad strokes particular matrix factorization algorithm singular value decomposition orthogonal matrix whose columns leftsingular vectors diagonal matrix whose non-zero entries singular values orthogonal matrix whose columns right-singular vectors transpose conjugate transpose). decomposed truncate keeping ﬁrst columns ﬁrst rows ﬁrst columns ﬁrst rows call truncated matrices truncated matrices give want. maps words onto topics entry ˜uij gives weight word topic instance collection medical articles resulting matrix might look like this importantly topic contains weights words appear entire corpus. instance topic contains cancer-related words also words insulin mellitus heart etc. hence topics exactly length changes weight topic assigns word. e.g. topic cancer-related words largest weights label topic ‘cancer’. real life applications topics usually clear-cut. stopwords often large weights least topics. also common words similar across topics. finally real life applications large corpora usually extract hundred topics three. importantly topics extracted ordered ﬁrst topic i.e. ﬁrst column captures variation second column second column captures variation third column thus ﬁrst topics matrices. words topic independent topics extracted expect generate regime-related topics ‘elections’ ‘repression’ also extraneous topics. tree-based methods create democracy scores using topics. afterwards able inspect topics inﬂuencing scores most drop rows corresponding topics extraneous inﬂuential generate improved democracy scores. discussed previous section ﬂexible technique assume anything words generated. bottom data reduction technique; core math truncated works well mapping documents onto topics compressing image ﬁles. ﬂexibility comes cost interpretability. word weights topic weights natural interpretation. know word weights represent ‘salience’ word topic topic weights represent ‘salience’ topic document beyond cannot much. ‘salience’ natural interpretation lsa. motivation latent dirichlet allocation created blei jordan whereas model-free models every aspect data-generating process texts. lose generality gain interpretability also word weights topic weights clear meaning unclear conditions tends produce superior results topics extracted generally believed clearcut extracted hand also believed broader hence compare results. next create k-dimensional vector contains topic proportions document. instance words assigned ﬁrst topic second topic third topic draw matrix whose entry probability word selected randomly draw word topic instance word ‘insulin’ probability selected topic ‘diabetes’ selected topic ‘heart diseases’. three levels model documents speciﬁc document speciﬁc word document. disregard because blei note ancillary variable independent everything else model ignore randomness. matrix topicsxdocuments weights. unlike though estimates natural interpretation. word weight probability word selected randomly draw word topic topic weight proportion words document drawn topic principle could ols. explained before training samples independent countries year whose scores extract test samples independent countries years thus could principle regress topic scores training samples respective scores; estimated coeﬃcients compute democracy scores test samples. would work here. topics reference cases would quickly degrees freedom. even topics would still violating observations variable’ rule thumb. sorts interactions non-linearities model would need additional terms would require even degrees freedom. thus tree-based regression techniques instead. techniques split observations recursively allocated homogeneous ‘leaves’. split thus path leaf based certain values regressors tree-based algorithms non-parametric even variables observations moreover tree-based algorithms handle non-linearities well inter-relations variables captured hierarchical structure tree. need specify priori variables interact ways. hence choice tree-based regression techniques alternatives like lasso ridge forward stepwise regression handle large number variables cost ignoring nonlinearities. several tree-based methods instead choosing particular four prominent ones compare results. next subsections explain method turn decision trees random forests extreme random forests adaboost. follows draw heavily hastie tibshirani friedman decision trees ﬁrst introduced breiman dependent variable independent variables observations variables continuous. want split observations subsets based independent variable value iteratively. repeat operation resulting subsets partitioning keep recursively subsets fewer observations. lower better model training data worse generalizes test data. rigorous choose follow popular choices compare results. outcome decision tree relates independent variables instance tried predict individual income based socioeconomic variables decision tree might look like this course extremely contrived example gives concrete idea decision tree looks like. node splits observations groups according independent variable splitting point chosen minimize mean squared errors subsets immediately node. stop growing tree subsets become small enough i.e. subsets contain fewer observations. last subsets tree captures non-linearities. parental income matters individual eight years schooling less. height matters individual eight twelve less years schooling. matters groups people eight years schooling less parental income twelve years schooling. conventional regression would need model non-linearities explicitly positing priori depends what. would need several interactive higher-order terms would take degrees freedom. decision tree learning however non-linearities learned data matter many complex random forests ﬁrst introduced breiman name suggests random forests extension decision trees. idea simple. treat reference population draw multiple bootstrap samples sample grow decision tree. predict observations simply average predicted diﬀerent trees. idea random forests reduce noise. conventional decision tree small perturbations data drastically impact choice averaging predictions multiple bootstrapped trees reduce noise. bootstrapped tree yields poor predictions slightly better random guesses average predictions outperform conventional tree. bootstrapped trees diﬀerent other reduce noise end. thus common grow trees slightly diﬀerent instead picking independent variables pick smaller diﬀerent trees reduce noise end. subset small tree intuitively tree increase weights observations largest errors updated weights grow next tree. goal force learning process concentrate hardest cases. random forests multiple trees make predictions aggregating predictions trees. unlike random forests however trees independent aggregate predictions taking weighted median rather simple mean. wordscores algorithm created laver benoit garry henceforth lbg. unlike need combined regression method; standalone algorithm already gives scores test samples. captures dispersion word scores around score document. square root divided square root gives standard error assess whether cases statistically diﬀerent other. fourth ﬁnal step re-scaling test scores. given text frequent words stopwords stopwords similar relative frequencies across reference texts centrist scores. makes scores virgin documents ‘bunch’ together around middle scale; dispersion metric training documents. lows average score virgin documents standard deviation training scores standard deviation virgin scores. transformation expands virgin scores making standard deviation training scores. martin vanberg propose alternative re-scaling formula benoit laver show original formula appropriate many test samples training samples case here. produced total batches democracy scores covering country-years i.e. country-years period. ﬁrst batch uses wordscores corpus remaining batches vary corpus topic-extraction method prior topic-extraction method number topics observe perform poorly correlations usually range correlation obtained wordscores. highest correlation obtained still implies unacceptably high noise-to-signal ratio. inspected every topic every speciﬁcation. either words disparate form coherent topic topic broad regimerelated. initial idea inspect inﬂuential topics could know exactly aspects democracy driving results drop extraneous topics necessary. feasible topics correspond aspects democracy. sense topics extraneous. hence democracy scores expected average increases time reﬂects several democratization processes happened period. observe change democracy indices well increased average freedom house score decreased average score increased last year dataset). also expected standard errors decrease press coverage. larger document country-year’s news articles narrower corresponding conﬁdence interval. figure shows relationship linear though conﬁdence intervals shrink dramatically change much afterwards even document more. correlations vary much time. good sign means overly inﬂuenced idiosyncrasies year extract training samples. otherwise would correlations decline sharply correlations vary much across indices either somewhat weaker polity data. also good sign means overly inﬂuenced idiosyncrasies extract training scores. also algorithm using years training data using well. also algorithm using multiple years training data using uds. finally also algorithm using polity freedom house indices training data. scenarios correlations remained vicinity corroborates klemmensen hobolt hansen’s ﬁnding wordscores’ results robust choice training data. largest positive diﬀerences i.e. cases higher mostly found small countries little press coverage. expected less press attention fewer news articles harder pinpoint country’s ‘true’ democracy level. largest negative diﬀerences however tell diﬀerent story. seems either repeatedly underestimate israel’s democracy score repeatedly overestimate observe country’s ‘true’ level democracy cannot know sure whether biased unbiased extent managed ﬁlter news articles related political regime; whatever biases exist become large random noise ads. instance imagine biased favor countries generous welfare like sweden. countries ‘boosted’ somewhat. extent news articles selected focused political regime welfare policy wordscores associate boosted scores welfarerelated words hence biased. less eﬃcient particular words associated boosted scores hand rely assumption ‘raters perceive democracy levels noisy unbiased fashion’ bollen paxton shown simply true. hence whatever biases exist polity freedom house wind well. data-generating process behind mitigate bias way. words seems likely overestimating israel’s democracy scores underestimating them. pro-israel bias interesting itself also raises general question whether might overall conservative bias. investigate possibility performed diﬀerence-of-means test splitting data groups countryyears left-wing governments country-years right-wing governments dataset political institutions data government ideological orientation.) test rejected null hypothesis mean ads-uds diﬀerence groups mean ads-uds diﬀerence left-wing country-years statistically smaller mean ads-uds diﬀerence right-wing country-years means negative seems tend reward right-wing governments. also checked whether biased toward economic policy speciﬁcally. split country-years index economic freedom dataset groups statist non-statist diﬀerence-of-means test shows mean ads-uds diﬀerence statists statistically lower non-statists means negative well seems somehow reward free market policies. cannot conclusively indict constituent indices though. perhaps democracy right-wing government positively associated somehow less eﬃcient capturing association. consistent hayek-friedman hypothesis left-wing governments detrimental democracy economic activism expands state’s coercive resources observe country’s true level democracy hard know sure going here. least know whether biased ineﬃcient conservative choice. regress economic policy democratic countries tend less regulation. relationship genuine artifact biased favor free market policies? biased measures tests become circular cannot know eﬀect measure partly based ineﬃciency hand merely makes tests conservative. much smaller standard errors average country dataset year overlaps countries; dataset average conﬁdence intervals tend larger less press coverage country gets cases smaller corresponding ones. instance united states statistically indistinguishable countries whereas united states statistically indistinguishable country country overlaps reason standard errors much smaller ones sheer size data. million news articles give billion words total. total number virgin words goes denominator formula standard errors billion words shrink conﬁdence intervals dramatically. standard errors also tell something nature democracy. large standard errors data might lead believe democracy better modeled categorical variable like alvarez gugiu centellas claim indeed case hierarchical cluster analysis extract latent democracy variable behind existing indices latent variable categorical continuous. conclusion unwarranted though. constituent measures coarse capture ﬁne-grained regime diﬀerences surprising latent variable also coarse capture ﬁnegrained regime diﬀerences. given measure fails capture subtle distinctions mean distinctions exist. standard errors suggest subtle distinctions seem exist. address important limitations democracy indices today. replicable standard errors narrow enough distinguish cases. also cost-eﬀective need training documents training scores already exist; need hire dozens country experts spend months collecting reviewing work. would interesting replicate existing work democracy using instead results change. come standard errors could incorporate regressions perhaps using errors-in-variables models could extend method produce daily real-time democracy index. existing indices year-based outdated months know democratic country today democratic automated text analysis help overcome limitations. cannot score news articles days would enough data produce meaningful results pick -month period immediately preceding certain date instance //-// want democracy scores", "year": 2015}