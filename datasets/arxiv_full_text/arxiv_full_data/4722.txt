{"title": "Efficient Machine Learning for Big Data: A Review", "tag": ["cs.LG", "cs.AI"], "abstract": "With the emerging technologies and all associated devices, it is predicted that massive amount of data will be created in the next few years, in fact, as much as 90% of current data were created in the last couple of years,a trend that will continue for the foreseeable future. Sustainable computing studies the process by which computer engineer/scientist designs computers and associated subsystems efficiently and effectively with minimal impact on the environment. However, current intelligent machine-learning systems are performance driven, the focus is on the predictive/classification accuracy, based on known properties learned from the training samples. For instance, most machine-learning-based nonparametric models are known to require high computational cost in order to find the global optima. With the learning task in a large dataset, the number of hidden nodes within the network will therefore increase significantly, which eventually leads to an exponential rise in computational complexity. This paper thus reviews the theoretical and experimental data-modeling literature, in large-scale data-intensive fields, relating to: (1) model efficiency, including computational requirements in learning, and data-intensive areas structure and design, and introduces (2) new algorithmic approaches with the least memory requirements and processing to minimize computational cost, while maintaining/improving its predictive/classification accuracy and stability.", "text": "emerging technologies associated devices predicted massive amount data created next years fact much current data created last couple years trend continue foreseeable future. sustainable computing studies process computer engineer/scientist designs computers associated subsystems efficiently effectively minimal impact environment. however current intelligent machine-learning systems performance driven focus predictive/classification accuracy based known properties learned training samples. instance machine-learning-based nonparametric models known require high computational cost order find global optima. learning task large dataset number hidden nodes within network therefore increase significantly eventually leads exponential rise computational complexity. paper thus reviews theoretical experimental data-modeling literature large-scale data-intensive fields relating model efficiency including computational requirements learning data-intensive areas’ structure design introduces algorithmic approaches least memory requirements processing minimize computational cost maintaining/improving predictive/classification accuracy stability. today it’s surprise reducing energy costs priorities many energy-related businesses. global information communications technology industry pumps around carbon dioxide emission accounts approximately percent global emissions giants constantly installing servers expand capacity. number server computers data centers increased sixfold million last decade server draws electricity earlier models aggregate electricity servers doubled years period came businesses installing large numbers servers increase energy consumption consequently results higher carbon dioxide emissions hence causing impact environment. furthermore businesses especially uncertain economic climate placed pressure reduce energy expenditure order remain competitive market emerging technologies associated devices predicted much data created created entire history planet earth given unprecedented amount data produced collected stored coming years technology industry’s great challenges benefit past decade mathematical intelligent machine-learning systems widely adopted number massive complex data-intensive fields astronomy biology climatology medicine finance economy. however current intelligent machine-learning-based systems inherently efficient scalable enough deal large volume data. example many years known non-parametric model-free approaches require high computational cost find global optima. high-dimensional data good data fitting capacity makes susceptible generalization problem leads exponential rise computational complexity. designing accurate machine-learning systems satisfy market needs hence lead higher likelihood energy waste increased computational cost. nowadays greater need develop efficient intelligent models cope future demands line similar energy-related initiatives. energy-efficientoriented data modeling important number data-intensive areas affect many related industries. designers focus maximum performance minimum energy break away traditional’ performance energy-use’ tradeoff increase number diversity options available energy-efficient modeling. however despite fact demand efficient sustainable data modeling methods large complex data-intensive fields best knowledge literatures proposed field the-art sustainable/energy-efficient machine-learning literatures including theoretical empirical experimental studies pertaining various needs recommendations. objective introduce perspective engineers scientists researchers computer science green domain well provide roadmap future research endeavors. paper organized follows. section introduces different large-scale data-intensive areas discusses structure nature including relation data models characteristics. section discusses issues current intelligent data modeling sustainability gives recommendations. section concludes paper. repository also known largest database world wdcc archives terabytes earth system model data related observations terabytes data readily accessible including information climate research anticipated climatic trends well terabytes worth climate simulation data. wdcc data accessible standard web-interface data increasingly available many different formats incorporated correctly various climate change models. timely accurate interpretation data provide advance warnings times severe weather changes hence enabling corresponding action taken promptly minimize resulting catastrophic damage. e-science areas typically data-intensive quality results improves quantity quality data available. however current intelligent machinelearning systems inherently efficient enough ends many cases growing fraction quantity data unexplored underexploited. small problem existing methods fail capture data immensity. concepts fail keep change traditions past experience become inadequate guide next. effective understanding wealth information pose great challenge today’s green engineers/researchers. noted scope review limited analytical aspects science areas using immense datasets methods reducing computational complexity distributed grid-computing environment excluded. many recent examples illustrate tremendous growth scientific data generation literature. estimated thousands wireless sensors currently place generates gigabyte data sensor sensors measure record sensory information natural environment joint spatial temporal dimensions never previously possible. environmental information gathered sensors sensing devices attached small low-power computer systems digital radio communications. sensor nodes self-organize network deliver perhaps process collected data base station made available users internet. sensors generate several petabytes data year decisions need taken real time much data analyze much transmit analysis. besides environmentalists similar challenge facing climatologists meteorologists geologists today also making sense vast continually increasing amount data generated earth observation satellites radars high-throughput sensor networks. world data centre climate world-largest climate data biological data produced phenomenal rate international research effort called human genome project. estimated human genome contains around billion base pairs distributed among twenty-three chromosomes translated gigabyte information however gene sequence data relevant data volume easily expand order gigabyte including also x-ray/nmr spectroscopy structure determination proteins data volume increase dramatically several petabytes assuming structure protein. december genbank repository nucleic acid sequences contained million entries swiss-prot database protein sequences contained million entries average databases doubling size every months. compounded data generated myriad related projects study gene expression determines protein structures encoded genes details proteins interact another. that begin imagine enormous amount variety information produced every month. past decade health sector also evolved significantly paper-based systems largely paperless electronic systems. many countries’ public health systems providing electronic patient records advanced medical imaging media. fact already implemented american hospitals days squinting decipher doctor’s untidy scrawl handwritten prescription soon thing past canada many countries insiteone leading service providers offering data archiving storage disaster-recovery solutions healthcare industry. u.s. insiteone’s archives include almost billion medical images million clinical studies coverage area clinical sites combined annual total radiological images exceeds million number still increasing approximate rate year. radiologists currently practicing image typically constitute several megabytes digital data required archived minimum five years. forecasts medical image data north america grow percent year reach nearly million terabytes also worthwhile note digital health data integrity security issues critical importance field. instance former data compression techniques used many cases distort data; latter confidentiality patient data clearly cardinal order foster public confidence technologies. digital data volume stars galaxies universe multiplied past decade rapid development technologies satellites telescopes observatory instruments. recently visible infrared survey telescope astronomy dark energy survey largest universe survey projects initiated different consortiums universities u.k. u.s. expected yield databases terabytes size next decade. according observatory field large single image record data area times size moon seen earth survey image degrees u.s. southern take five years complete. vista performance requirements challenging peaks megabytes/second data rate maximum terabytes data night fairly commonplace. many astro-scientific databases sloan digital survey already terabytes size panoramic survey telescope-rapid response system expected produce science database terabytes size next five years likewise large synoptic survey telescope producing terabytes data night yielding total database petabytes data produced telescopes expected come internet picture change radically. many believe massive data volume ever increasing computing power dramatically change conventional science technology conducted. believe surge data open challenge research field hence instigating search approaches. likewise challenge needs addressed area intelligent information science well. computational/analytic data models designed developed. sustainable data modeling defined form data modeling technology aimed make sense large amount data associated field discovering patterns correlations effective efficient way. sustainable data modeling specifically focuses maximum learning accuracy minimum computational cost rapid efficient processing large volumes data. sustainable data modeling seems ideal ease large quantities data handled efficiently well associated cost reduction observed many cases. wider perspective entails data-modeling revolution e-sciences. fact newly designed sustainable data models effectively cope data issues result bring benefits various e-science areas. excellent examples well discussed patnaik sundaravaradan marwah’s article hence section give recommendations green engineers/researchers mechanics sustainable data modeling. success elements sustainable data modeling maintain improve performance significantly reducing computational cost. recent data-modeling research shown ensemble methods gained much popularity often perform better individual models ensemble method uses multiple models obtain better performance could obtained constituent models however result significant increase computational cost. model deals large-scale data model complexity computational requirements grow exponentially. example ensemble model bayes classifier bayes classifier hypothesis given vote proportional likelihood training dataset would sampled system hypothesis true. facilitate training data finite size vote hypothesis also multiplied prior probability hypothesis. bayes classifier expressed follows predicted class possible classes hypothesis space refers probability training data. ensemble bayes classifier represents hypothesis necessarily hypothesis represented bayes classifier however optimal hypothesis ensemble space considering problem numerical weather prediction ensemble predictions commonly made major operational weather prediction facilities worldwide including national centers environmental prediction u.s. european centre medium-range weather forecasts united kingdom office metro france environment canada japanese meteorological agency bureau meteorology australia china meteorological administration korea meteorological administration cptec brazil. bayes estimation techniques well-adopted general intelligent data modeling provide fundamental formalism combining information available regards parameters estimated optimized time complexity serious problems bayes nonparametric learning models high-algorithmic complexity extensive memory requirements especially necessary quadratic programming large-scale tasks. nonparametric bayes classifier extracts worst-case example uses statistical analysis build classifying model learning algorithm examines every attribute values every training example must least worse complexity many applications machine learning deal problems number features well number examples large. linear support vector machines among prominent machine-learning techniques high-dimensional sparse data. article machine-learning models examples semiparameterized. words models modified efficient fast computationally. time complexity bayes svms well discussed elkan’s joachims’ article respectively proposed different support-vectorbased efficient ensemble models shown reduce computational cost maintaining performance novel learning technique proven successful similar studies nonparametric model unique model must constructed test significantly increase computational complexity cost. reduce computational cost thus proposed partition training samples clusters that build separate local model cluster method called local learning. number recent works demonstrated local learning strategy superior global learning strategy especially data sets evenly distributed local-learning method adopted decision function nonparametric classifier allow classifier semiparameterized. semiparametric approximation expressed follows network training number input training vectors associated center nonparametric classification many different types radial basis functions chosen place gaussian function. radial basis function used many cases actually spherical kernel function specifically used nonparametric function estimation. number training samples approaches infinity nonparametric function estimation hence becomes longer dependent parameters radial basis function however finite training samples always observe forms dependency radial basis function parameters. local learning strategy provides dependence radial basis function parameters nonparametric model local learning model semiparametric approximation nonparametric/global learning model. words semiparametric modeling model assumptions gets stronger nonparametric models less restrictive parametric model. particular approximation avoids practical disadvantages nonparametric methods expense increased risk specification errors. semiparametric models based local learning help reducing model complexity also finding optimal tradeoff between parametric nonparametric models achieve model bias variance short therefore take inherent advantage models reducing computational requirements effectively. examples seen spherical function mixture model data-directed center vector allocation. relative widths spherical functions center directly proportional relative number training vectors associated center. many different types computational local models diverse selection method grouping associated input vectors class used global model semiparametric approximation. local learning strategy provides reasonable approximation since sufficiently close input vector space. case adequately represented single center vector local space. case support vector regression vectors derived either k-means codebook theory. classes separable input space high-dimensional feature space using nonlinear kernel function. kernel function calculates scalar product images examples feature space. number training patterns parameters spherical kernel function bias term. case local model constructed k-means clustering. objective function k-means clustering expressed follows similarity matrix vector representing centroid cluster non-negative scaling parameter element cluster membership matrix whose value equal source vector belongs cluster zero otherwise. first term objective function corresponds cluster cohesion measure. minimization equation would ensure training vectors cluster highly correlated similarity vectors. second term measures skewness class distribution cluster. minimization term would ensure cluster contains balanced number positive negative estimation vectors. cluster centroid cluster membership matrix estimated iteratively follows mentioned local model also constructed principle codebook case basic idea replace values original multidimensional vector space values discrete subspace lower dimension. lower-dimension vector requires less storage space data thus compressed. consider training sequence consisting source vectors t={x xm}. assumed sufficiently large statistical properties source captured training sequence. assume source vectors k-dimensional m=…m. vectors compressed choosing nearest matching vectors form codebook comprising entire codevectors. number codevectors c={cc…cn} codevector k-dimensional n=…n. representative codevector determined closest euclidean distance source vector. euclidean distance defined component source vector component codevector nearest-neighboring region associated codevector partitions whole region denoted p={ss…sn}. source vector region approximation denoted q=cn solution parameters minimization problem must satisfy conditions nearest-neighbor centroid. nearest-neighbor condition indicates subregion consist vectors closer codevectors elkan’s discussed local learning techniques vectors building local model prove intelligent learning model examines attribute values every training example must worse complexity. words local learning strategy efficient global learning strategy especially large volume data problems shallow learning models widely used literature solve simple well-constrained problems. however limited modeling representational power support solving complex problem natural language problems. so-called deep learning emerged area research exploits multiple layers information-processing hierarchical architecture pattern classification representation learning main advantage deep learning referred drastically increased chip processing abilities lowered cost computing hardware recent advances deep neural networks multilayer networks many hidden layers whose weights fully connected often initialized pretrained using stacked restricted boltzmann machine deep belief networks pretraining unsupervised step utilizes large amount unlabeled training data extracting structures regularities input features uses huge amount unlabeled training data also provides good initialization weights dnn. moreover overfitting underfitting problems tackled using pretraining step dbn. shown great performance recognition classification tasks including natural language processing image classification traffic flow detection however high computational cost difficult scale addresses scalability problem simple classifiers stacked order construct complex classifier approximation longer extracts worst-case example able reduce complexity effectively. local learning strategy model assumptions gets stronger nonparametric models less restrictive parametric model reducing computational complexity significantly. data computing systems fall major categories based data analyzed regards time constraint first batch processing large volumes on-disk data time constraints second streaming processing in-memory data real-time short period time huang argued next-generation computing systems data analytics need innovative designs hardware software would provide good match between data algorithms underlying computing several computing frameworks e.g. hadoop shadoop commapreduce dryad piccolo parallel machine learning toolbox systems capabilities scale machine learning. combination deep learning parallel training implementation techniques provides potential ways process data quoc consider problem building high-level class-specific feature detectors unlabeled data. experimental results reveal possible train face detector without label images containing face not. zhang chen presented distributed learning paradigm rbms backpropagation algorithm using mapreduce. dbns trained distributed stacking series distributed rbms pretraining distributed backpropagation fine-tuning. experimental results demonstrate distributed rbms dbns amenable large-scale data good performance terms accuracy efficiency. review provided overview current state research sustainable data modeling. particular discussed theoretical experimental aspects large-scale data-intensive fields relating model energy efficiency including computational requirements learning possible approaches data-intensive areas’ structure design including relation data models characteristics surge e-science data sustainable data modeling shown offer forward ease handling large quantities data. also envisaged data-modeling revolution readily extended various areas e-science. newly designed sustainable data models able cope emerging large-scale data paradigm also provide means maximizing return various e-science areas. http//www.nytimes.com////technology/blue.html?pagewanted=print&_r=. koomey estimating total power consumption servers world stanford university technical report strathman model-based energy management reduces energy costs. online http//www.epmag.com/guests////model-basedenergy-management-reduces-energy-costs. managing data industry challenge zomaya energy-efficient kernel framework large-scale data modeling classiffication parallel distributed processing workshops forum ieee international symposium energy insights learned human sequence learned analysis working draft sequence human genome? still unknown?. online. http//www.ornl. gov/hgmis accessed baluja electronic patient records soon doctors scrawl paper globe mail. online. http//www.theglobeandmail.com/news/national/toronto/electronic-patient-records-will-soon-end-doctors-scrawl-on-paper/article accessed april nuclear cardiology markets. trimark publications news dell launches cloud-based services hospitals physician practices. online. http//www.emrandhipaa.com/news//// dell-launches-new-cloud-based-services-for-hospitals-and-physician-practiceaccessed april zhang berg maire malik svm-knn discriminative nearest neighbor classiffication visual category recognition computer vision pattern recognition ieee computer society conference vol. ieee local prediction non-linear time series using support vector regression pattern recognition vol. sikder taheri zhou zomayadomnet protein domain boundary prediction using enhanced general regression network profiles nanobioscience ieee transactions vol. murphy machine learning probabilistic perspective. press baldominos albacete saez isasi scalable machine learning online service data real-time analysis computational intelligence data ieee symposium apache hadoop. http//hadoop.apache.org. yang wang yuan huang shadoop improving mapreduce performance optimizing execution mechanism hadoop clusters journal parallel distributed computing vol.", "year": 2015}