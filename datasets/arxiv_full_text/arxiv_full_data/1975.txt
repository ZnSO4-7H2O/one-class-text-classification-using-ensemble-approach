{"title": "Tree-CNN: A Deep Convolutional Neural Network for Lifelong Learning", "tag": ["cs.CV", "cs.AI", "eess.IV", "stat.ML"], "abstract": "In recent years, Convolutional Neural Networks (CNNs) have shown remarkable performance in many computer vision tasks such as object recognition and detection. However, complex training issues, such as \"catastrophic forgetting\" and hyper-parameter tuning, make incremental learning in CNNs a difficult challenge. In this paper, we propose a hierarchical deep neural network, with CNNs at multiple levels, and a corresponding training method for lifelong learning. The network grows in a tree-like manner to accommodate the new classes of data without losing the ability to identify the previously trained classes. The proposed network was tested on CIFAR-10 and CIFAR-100 datasets, and compared against the method of fine tuning specific layers of a conventional CNN. We obtained comparable accuracies and achieved 40% and 20% reduction in training effort in CIFAR-10 and CIFAR 100 respectively. The network was able to organize the incoming classes of data into feature-driven super-classes. Our model improves upon existing hierarchical CNN models by adding the capability of self-growth and also yields important observations on feature selective classification.", "text": "abstract—in recent years convolutional neural networks shown remarkable performance many computer vision tasks object recognition detection. however complex training issues catastrophic forgetting hyper-parameter tuning make incremental learning cnns difﬁcult challenge. paper propose hierarchical deep neural network cnns multiple levels corresponding training method lifelong learning. network grows tree-like manner accommodate classes data without losing ability identify previously trained classes. proposed network tested cifar- cifar- datasets compared method tuning speciﬁc layers conventional cnn. obtained comparable accuracies achieved reduction training effort cifar- cifar respectively. network able organize incoming classes data feature-driven super-classes. model improves upon existing hierarchical models adding capability self-growth also yields important observations feature selective classiﬁcation. emerged leading architecture large scale image classiﬁcation recent years krizevsky imagenet large scale visual recognition challenge implementing deep-cnn. catapulted dcnns spotlight since then dominated islvrc surpassed human level performance popular image datasets mnist imagenet today increased access large amount labeled data supervised learning become leading paradigm training dcnns image recognition.traditionally dcnn trained dataset containing large number labeled images. network learns extract relevant features classify images. trained model applied unlabeled images classify speciﬁc classes. training training data presented network training process. however real world hardly information once. instead data gathered incrementally time. humans also learn similar incremental manner. accumulate knowledge acquired past learn things lifetime. inspired humans learn lifelong machine learning emerging paradigm machine learning tries address limitation. lifelong learning based principle learning task easier learning task. while intuitively makes sense; actual implementation network faces several challenges discussed further. dcnn embeds feature extraction classiﬁcation coherent architecture within model. modifying part parameter space immediately affects model globally. another problem incrementally training dcnn issue catastrophic forgetting data dcnn results destruction existing features learned earlier data. means training data previous data must also shown network. interestingly also observed initial layers dcnn learn extract generic features deeper layers learn identify higher level features. demonstrated initializing network transferred features trained network potentially boost performance shelf cnns trained large datasets ﬁne-tuned speciﬁc applications however solutions train original network learn classes. rather network created one. work propose network grows hierarchically classes introduced. branching based similarity features classes. initial nodes tree-cnn divide dataset coarse super-classes approach leaves network ﬁner classiﬁcation done. model allows leverage convolution layers learned previously used bigger network. objective present advantages network model terms incremental learning transferring trained convolution layers tuning regular network. rest paper organized follows. related work incremental learning deep neural networks discussed section next section explain detail proposed network architecture incremental learning method. section describe experiments conducted using cifar- cifar- datasets. followed detailed analysis performance network comparison basic transfer learning tuning section finally section discuss merits limitations network conclude ﬁndings suggest opportunities future work. related work modern world digitized data produces information every second thus fueling need systems learn data arrives. traditional deep neural networks static respect several approaches incremental learning currently explored. oneshot learning bayesian transfer learning technique uses training samples learn classes. fast r-cnn popular framework object detection also suffers catastrophic forgetting. mitigate issue frozen copy original network compute balance loss classes introduced network \"learning without forgetting\" another method uses task data train network preserving original capabilities. however original network trained extensive dataset imagenet task data much smaller dataset. hand propose network knows handful classes grows learns time. work draws inspiration transfer learning. observed initial layers learn generic features work also demonstrated network initialized learned layers trained network performs better. switching initial layers deep neural networks pre-deﬁned gabor ﬁlters shown reduce training time offer energyefﬁcient training process common features shared images different objects exploited build hierarchical classiﬁers. features grouped semantically authors built attention-tree visual semantic hierarchy perform energy-efﬁcient image classiﬁcation. falcon feature driven selective classiﬁcation classiﬁcation methodology inspired biological visual similar progression complexity convolutional layers dcnn upper nodes hierarchical classify images coarse super-classes using basic features like grouping green-colored objects together humans faces together. deeper nodes perform ﬁner discrimination girl apples oranges etc. hierarchical models shown perform even better standard dcnns discriminative transfer learning earliest works classes categorized hierarchically improve network performance. hd-cnn hierarchical model built exploiting common feature sharing aspect images. however works dataset ﬁxed beginning prior knowledge classes properties used build hierarchical model. work tree-cnn starts single root node generates hierarchies accommodate classes. images belonging older dataset required retraining localizing change small section whole network achieve reduction training effort. similar approach applied classes added classes divided super-classes using error-based model. initial network cloned form networks tuned super-classes. motivation \"divide-andconquer\" approach large datasets interested developing model incrementally grow data. sequentially data multiple learning stages. next section detail design principle network topology algorithm used grow network. lifelong learning model network architecture network model inspired hierarchical classiﬁers. built nodes connected directed acyclic graph. node acts classiﬁer predicting label input image. label image passed next node classiﬁes image reach leaf node last step classiﬁcation. begin deﬁning certain terms used frequently model. node represents building block tree. node represents group classes. node following parameters. parent holds node-id node previous node. node parent. parent node inferred coarse-classiﬁer whereas child node ﬁner classiﬁcation. except root node nodes parent node. node parent node. network reach speciﬁc leaf node. node beginning node image dcnn associated node. output node highest classiﬁcation probability next node algorithm moves leaf node class associated node predicted class. else algorithm feeds image dcnn node. pseudo-code class prediction given algorithm next describe process incremental/lifelong learning. tree-cnn already trained classify classes. data belonging classes acquired network needs learn classify trying minimize change network structure training effort. small sample images selected training classes. root node images dcnn class time. obtain dimensional matrix ok×m×i number children root node number classes number sample images class denotes output output neuron image belonging class ok×m average outputs images softmax taken oavg obtain likelihood matrix lk×m indicates strongly image belonging class average associates node. acronym \"labels transform\" lookup table keeps track original label class output neuron number associated with. leaf node record class label node. root node highest node tree. ﬁrst classiﬁcation happens node. output neuron root associated node. next hierarchy branch node. parent children. performs classiﬁcation minimum classes/superclasses. leaf node last level tree. leaf node uniquely associated class. leaf nodes class. fig. shows root node branch nodes stage classiﬁcation network. output second level branch node leaf node. algorithm start with options implementing tree-cnn. handful distinct classes don’t seem like grouped hierarchical fashion network initialized single root node. output root node distinct classes. however beginning data already grouped hierarchical manner coarse classes detailed tree-cnn initialized. root node trained classify images highest level super-classes. branch nodes tree trained classify images ﬁner classes. trained tree-cnn discuss predicts classes treats classes. first describe network predicts class input image. recursive algorithm moves along hierarchies combine child nodes class form child node child nodes class strong likelihood combine form child node. likelihood values .and least leaf node combine class form child node branch node. upper limit number child nodes could combined certain network restrictions applied prevent addition classes child nodes network expands horizontally adding class child node. node leaf node. complete classes root node move next level tree. process applied child nodes classes added them. example classes added child node. child node leaf node changed branch node leaf nodes children. child node branch node repeat process calculating likelihood matrix determining classes added output. decision grow tree semisupervised. algorithm decides grow tree given constraints user. limit parameters maximum children node maximum depth tree etc. system requirements. classes allotted locations tree supervised gradient descent based training nodes modiﬁed/added. saves modifying whole network affected portions network require retraining/ﬁne-tuning. experiments conducted experiments using datasets cifar cifar-. used matconvnet opensource toolbox implementation convolutional neural networks matlab experiments different initial conditions. ﬁrst experiment cifar- show given trained tree-cnn root node branch nodes expands classes arrive. demonstrate single incremental learning stage. second experiment cifar- network starts root node classify classes progressively teach network classes time. training performed simple data augmentation. training images ﬂipped horizontally random probability images whitened contrast normalized activation used networks rectiﬁed linear activation relu max. networks trained using stochastic gradient descent ﬁxed momentum dropout used ﬁnal fully connected layers pooling layers regularize network. also employed batchnormalization output every convolutional layer. additionally weight decay regularize model. ﬁnal layer performs softmax operation output nodes generate class probabilities. adding multiple classes dataset used cifar- dataset experiment. mutually exclusive classes namely airplane automobile bird deer frog horse ship truck. training images test images equally distributed classes. image color image thus input channels green blue pixels. ﬁrst train network classes next stage incremental learning train network remaining classes. network ease reference label network tree-cnn root node dcnn output nodes. classify input image either animals vehicles. child node dcnn ﬁner classiﬁcation. description layers sub-networks given tables fig. fig. depict initial model tree-cnn comparison took another network complexity level similar stage complexity tree-cnn. convolutional blocks block sets convolutional kernels. network inspired architecture vgg-net detailed model given table truck grouped vehicles horse grouped animals. network root node trained classify images animals vehicles. this training images belonging classes re-labeled animals vehicles. root node trained epochs. learning rate kept ﬁrst epochs reduced times every epochs. second level branch nodes separately trained. animals branch node trained training images classes. node classiﬁes image horse. trained epochs learning rate used node. similarly branch node labeled vehicles trained identify distinct vehicles ship truck automobile. root node achieves testing accuracy branch nodes animals vehicles achieve testing accuracy respectively. overall network achieves testing accuracy image classes. network trained epochs. learning rate kept ﬁrst epochs reduced times every epochs. achieve testing accuracy accuracy tree-cnn within range network incremental learning next want train networks identify remaining four classes bird frog deer airplane. training images class given select random images class show images root node. obtain matrix matrix element matrix indicates softmax likelihood classes classiﬁed vehicles second presents information animals. fig. shows likely class classiﬁed animals vehicles. note number times images belonging class classiﬁed super-classes root node. instead perform softmax operation classes training testing images class. image color image input channels green blue pixels. divide classes groups group classes. distribution done random. ﬁrst train networks classes incrementally classes every stage. learning stages remaining classes. network initially tree-cnn root node leaf nodes. label network tree-cnn root node dcnn network output nodes. layers described detail table subsequent learning stages network would extend branches. dcnn model used branch nodes given table comparison network used previous experiment. initial training tree-cnn root node trained classify classes. trained epochs learning rate ﬁrst epochs. reduced every epochs. obtain testing accuracy network trained epochs well learning rate root node. achieve testing accuracy starting accuracy networks almost same. incremental learning divided remaining classes groups containing classes. classes added network incremental learning stages. stage ﬁrst images belonging class shown root node likelihood matrix generated. columns matrix used form ordered described equations experiment applied following constraints system. retrained training images classes classify coarse categories. root node trained epochs learning rate ﬁrst epochs learning rate reduced every epochs. next train branch nodes. nodes train training images classes. nodes trained epochs learning rate variation kept root node. comparison apply tuning already trained network classes output nodes ﬁnal layer. described table made convolutional blocks fully connected block network tuning strategies used. method retrains/ﬁne-tunes certain layers network. listed below different depths back-propagation retraining whole dataset i.e. classes. depth tree constrained ease conducting experiment. algorithm needs implemented root node level. observed developing algorithm classes tend higher softmax likelihood value branch nodes higher number children. prevent network lop-sided large branch limit number children branch node. placement classes determined train root node modiﬁed branch nodes. root node trained epochs learning rate ﬁrst epochs learning rate reduced every epochs. results discuss results obtained sets experiments. reiterate ﬁrst experiment incremental learning stage. while second experiment demonstrated network grew accommodate data time. network grows folds classiﬁcation capacity classes classes. compare proposed method re-training ﬁnal layers standard dcnn. methods compared following metrics every training sample generates corresponding error back-propagated stochastic gradient descent update network weights. training effort number weight updates happen training epoch. batch size number training epochs kept same product number weights number training samples used gives good measure effort used training network. tree-cnn training effort nodes summed together. network case. adding multiple classes fig. compare test accuracy training effort cases ﬁne-tuning network tree-cnn cifar-. retraining layer network requires least training effort. however gives lowest accuracy amongst all. classes introduced method causes much loss accuracy shown cifar- fig. proposed model tree-cnn second lowest normalized training effort less ‘bcase less ‘bcase ii’. time tree-cnn comparable accuracy ‘bcase ‘bcase iii’ less ideal case ‘bcase margin compare tree-cnn different ﬁne-tuning cases network fig. shows training effort test cases cifar- experiment. normalized training effort dividing values highest training effort. i.e. ‘bcase ‘bcase lowest training effort tune ﬁnal fully connected layer. however performs worst accuracy shown fig. tree-cnn requires almost training effort ‘bcase achieves better accuracy ‘bcase ‘bcase iii’. achieves accuracy within range ‘bcase requiring less training effort. ‘bcase ‘bcase require almost similar training effort difference extra training smallest conv layer i.e. ﬁrst layer. ‘bcase gives best accuracy however retraining entire network images. pre-trained kernel sharing good starting anew thereby requiring highest training effort. tree-cnn achieves accuracy full cifar- dataset. less accuracy achieved training full network gives overall accuracy tree-cnn network compared comparable range reported accuracy similar sized networks hd-cnn hertel improvements accuracy done modifying architecture nodes adopting methods convolutional exponential linear units cases training effort required particular learning stage greater effort required previous stage. show images belonging classes avoid catastrophic forgetting. method lower slope training effort learning stage compared network interesting thing note similar looking classes also semantically similar grouped branches. incremental learning stages root node children nodes leaf nodes remaining branch nodes. details classes associated nodes given table branch nodes leaf nodes. ﬁnal groups formed training classes given table certain similar objects grouped together shown fig. groups objects sharing semantic similarity well groups node shown fig. groups formed dependent partly sequence classes arrived. limited number leaf nodes branch even class matched strongly already full branch algorithm added second best match created node. cases branch number street grouped trees images streetcar dataset green color. color shape similarities grouping semantically dissimilar objects. discussion work explore method incremental learning dealing large image databases. motivation work stems idea subsequent addition image classes network easier retraining whole network classes. observed expectedly incremental learning stage required effort previous images belonging classes needed shown cnns. inherent problem catastrophic forgetting deep neural networks. proposed method lower rate increase training effort consecutive learning stages compared effort need ﬁne-tune limited layers large network maintaining comparable accuracy. however tree-cnn continues grow size time implications implementing hardware important future work. tree-cnn grows manner images share common features closer neighbors tree images different. ﬁnal leaf nodes distance also used measure similar different images are. method training classiﬁcation used hierarchically classify large datasets. correlation image similarity location image classes tree-cnn needs analyzed empirically. proposed method tree-cnn thus offers better incremental learning model based hierarchical classiﬁers transfer learning. could also potentially applied large image search operations. inferencing choose parse network certain depth give idea supercategory search image belongs hence could offer quick solution analyze incoming data. work supported part center braininspired computing enabling autonomous intelligence darpa sponsored jump center semiconductor research corporation army research lab. russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge international journal computer vision vol. chen lifelong machine learning synthesis lectures artiﬁcial intelligence machine learning vol. thrun mitchell lifelong robot learning robotics autonomous systems vol. thrun learning n-th thing easier learning ﬁrst? advances neural information processing systems goodfellow mirza xiao courville bengio empirical investigation catastrophic forgetting gradientbased neural networks arxiv preprint arxiv. oquab bottou laptev sivic learning transferring mid-level image representations using convolutional neural networks proceedings ieee conference computer vision pattern recognition shmelkov schmid alahari incremental learning object detectors without catastrophic forgetting ieee international conference computer vision ieee hoiem learning without forgetting ieee transac sarwar panda gabor ﬁlter assisted energy efﬁcient fast learning convolutional neural networks ieee/acm international symposium power electronics design panda ankit wijesinghe falcon feature driven selective classiﬁcation energy-efﬁcient image recognition ieee transactions computer-aided design integrated circuits systems vol. zhang piramuthu jagadeesh decoste hd-cnn hierarchical deep convolutional neural networks large scale visual recognition ieee international conference computer vision xiao zhang yang peng zhang error-driven incremental learning deep convolutional neural network large-scale image classiﬁcation proceedings international conference multime vol. srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting. journal machine learning research vol. ioffe szegedy batch normalization accelerating deep network training reducing internal covariate shift international conference machine learning deboleena received b.tech+m.tech dual degree electronics electrical communications engineering kharagpur kharagpur india currently working towards ph.d. degree electrical computer engineering purdue university west lafayette usa. prior that design engineer qualcomm bangalore design center bengaluru india current research interests include neuro-inspired algorithms cognitive applicapriyadarshini panda received b.e. degree electrical electronics engineering m.sc. degree physics birla institute technology science pilani india currently pursuing ph.d. degree electrical computer engineering purdue university west lafayette usa. component design engineer intel corporation bengaluru india prior that six-month internship nvidia graphics bengaluru. current research interests include low-power neuromorphic computing energy-efﬁcient realization neural networks using novel architectures algorithms. kaushik received b.tech. degree electronics electrical communications engineering kharagpur kharagpur india ph.d. degree department electrical computer engineering university illinois urbana−champaign champaign semiconductor process design center texas instruments incorporated dallas involved fpga architecture development low-power circuit design. joined faculty electrical computer engineering purdue university west lafayette currently edward tiedemann distinguished professor. authored papers refereed journals conferences holds patents graduated ph.d. students co-authored books power cmos vlsi design current research interests include neuromorphic cognitive computing spintronics device-circuit codesign nanoscale silicon non-silicon technologies low-power electronics portable computing wireless communications computing models enabled emerging technologies.", "year": 2018}