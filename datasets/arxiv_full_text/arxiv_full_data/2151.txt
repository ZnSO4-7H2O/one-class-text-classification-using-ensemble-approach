{"title": "Column Networks for Collective Classification", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Relational learning deals with data that are characterized by relational structures. An important task is collective classification, which is to jointly classify networked objects. While it holds a great promise to produce a better accuracy than non-collective classifiers, collective classification is computational challenging and has not leveraged on the recent breakthroughs of deep learning. We present Column Network (CLN), a novel deep learning model for collective classification in multi-relational domains. CLN has many desirable theoretical properties: (i) it encodes multi-relations between any two instances; (ii) it is deep and compact, allowing complex functions to be approximated at the network level with a small set of free parameters; (iii) local and relational features are learned simultaneously; (iv) long-range, higher-order dependencies between instances are supported naturally; and (v) crucially, learning and inference are efficient, linear in the size of the network and the number of relations. We evaluate CLN on multiple real-world applications: (a) delay prediction in software projects, (b) PubMed Diabetes publication classification and (c) film genre classification. In all applications, CLN demonstrates a higher accuracy than state-of-the-art rivals.", "text": "existing models designed collective classiﬁcation mostly shallow emphasize learning local relational features. deep neural networks hand offer automatic feature learning arguably behind recent record-breaking successes vision speech games known challenges relational learning design deep neural network efﬁcient accurate collective classiﬁcation? recent work combines deep learning structured prediction main learning inference problems general multirelational settings remain open paper present column network efﬁcient deep learning model multi-relational data emphasis collective classiﬁcation. design partly inspired columnar organization neocortex cortical neurons organized vertical layered mini-columns responsible small receptive ﬁeld. communications mini-columns enabled short-range horizontal connections. mini-column feedforward takes input vector plays role receptive ﬁeld produces output class. mini-column learns data also exchanges features neighbor mini-columns along pathway input output. despite short-range exchanges interaction range mini-columns increases depth thus enabling long-range dependencies data objects. able learn hundreds layers leverage recently introduced highway nets models mini-columns. design choice becomes network interacting highway nets. unlike original highway nets cln’s hidden layers share parameters allowing depth grow without introducing parameters functionally feedforward nets highway nets functional approximators input vector thought approximator grand function takes complex network vectors input returns multiple outputs. many desirable theoretical relational learning deals data characterized relational structures. important task collective classiﬁcation jointly classify networked objects. holds great promise produce better accuracy non-collective classiﬁers collective classiﬁcation computationally challenging leveraged recent breakthroughs deep learning. present column network novel deep learning model collective classiﬁcation multi-relational domains. many desirable theoretical properties encodes multi-relations instances; deep compact allowing complex functions approximated network level small free parameters; local relational features learned simultaneously; long-range higher-order dependencies instances supported naturally; crucially learning inference efﬁcient linear complexity size network number relations. evaluate multiple real-world applications delay prediction software projects pubmed diabetes publication classiﬁcation genre classiﬁcation. applications demonstrates higher accuracy state-of-the-art rivals. relational data characterized relational structures between objects data instances. example research publications linked citations pages connected hyperlinks movies related directors actors. using relations improve performance classiﬁcation relations entities indicative relations classes. canonical task learning data type collective classiﬁcation networked data instances classiﬁed simultaneously rather independently exploit dependencies data collective classiﬁcation however highly challenging. exact collective inference general dependencies intractable. tractable learning often resort surrogate loss functions pseudo-likelihood approximate gradient iterative schemes stacked properties encodes multi-relations instances; deep compact allowing complex functions approximated network level small free parameters; local relational features learned simultaneously; long-range higher-order dependencies instances supported naturally; crucially learning inference efﬁcient linear size network number relations. evaluate real-world applications delay prediction software projects pubmed diabetes publication classiﬁcation genre classiﬁcation. applications demonstrates higher accuracy state-of-theart rivals. notation convention capital letters matrices bold lowercase letters vectors. sigmoid func− tion scalar deﬁned function vector deﬁned operator used denote elementwise multiplication. superscript denote layers computational steps neural networks subscript element describe collective classiﬁcation setting multiple relations. given graph entities g={e entities connect relations tuple describes relation type entity entity entities connect multiple relations. relation unidirectional bidirectional. example movie movie linked unidirectional relation sequel bidirectional relations same-actor same-director. entities relations represented entity graph node represents entity edge exists nodes least relation. furthermore neighbor link neighbors neighbors related relation immediately implies ∪r∈rnr. local features feature vector entity label either observed latent. given known label entities eobs collective classiﬁcation algorithm simultaneously infers unknown labels entities ehid e\\eobs. probabilistic setting assume classiﬁer produces estimate joint conditional distribution challenging learn infer popular strategy employ approximate efﬁcient iterative methods next subsection describe highly effective strategy known stacked learning partly inspires work. figure collective classiﬁcation stacked learning graph entities connected unidirectional bidirectional links model graph three steps feature vector entity bidirectional link modeled unidirectional links vice versa. stacked learning multi-step learning procedure collective classiﬁcation step classiﬁer used predict class probabilities entity i.e. intermediate outputs used relational features neighbor classiﬁers next step. relation produces contextual features features relation averaged architecture inspired columnar organization neocortex mini-column entity akin sensory receptive ﬁeld. column feedforward passes information lower layer higher layer higher layers neighbors nature inter-column communication dictated relations entities. implements convolution. example standard convolutional kernels images implement relations left right above below above-left above-right below-left below-right. supposed relations shared nodes achieves translation invariance similar cnn. highway network mini-column specify detail mini-column implement extending recently introduced feedforward called highway network recall traditional feedforward nets major difﬁculty learning high number layers. nested non-linear structure prevents ease passing information gradient along computational path. highway nets solve problem partially opening gate lets previous states propagate layers follows parameter sharing compactness feedforward nets number parameters grow number hidden layers. number multiplied number relations highway network implementation mini-columns parameters gates used thus doubling number parameters deep many relations number parameters grow faster size training data leading overﬁtting high demand memory. address challenge borrow idea parameter sharing recurrent neural network layers identical parameters. empirical evidence supporting strategy non-relational data intermediate steps output class probabilities learn higher abstraction instance features relational features. such model end-to-end sense receptive signals passed bottom abstract features inferred along way. likewise training signals passed bottom. input feature vector hidden activation layer entity respectively. connection entity serves input non-linear function tht− rkt×kt− weight rkt×kt− matrices bias vector activation function pre-deﬁned constant used prevent parameterized contexts growing large complex relations. remark several similarities existing neural network operations. implements meanpooling operation often seen cnn. main difference standard mean pooling reduce graph size. suggests forms pooling max-pooling sum-pooling. asymmetric pooling also implemented based concept attention replaced capturing long-range dependencies important property proposed deep ability capture long-range dependencies despite local state exchange shown eqs. consider example fig. modeled modeled therefore although directly connect information still embedded generally hidden layers hidden activation entity contains information expanded neighbors radius number layers large representation entity layer contains local features directed neighbors also information entire graph. highway networks levels representations accumulated layers used predict output labels. training mini-batch described sec. previous layer neighbors. therefore contains information entire graph network deep enough. requires full-batch training expensive scalable. propose simple efﬁcient approximation method allows mini-batch training. mini-batch neighbor activations temporarily frozen scalars i.e. gradients propagated blanket. parameter update activations recomputed usual. experiments showed procedure converge performance comparative full-batch training method. baselines comparison employed comprehensive suit baseline methods include designed collective classiﬁcation deep neural nets non-collective classiﬁcation. former used netkit open source toolkit classiﬁcation networked data netkit offers classiﬁcation framework consisting components local classiﬁer relational classiﬁer collective inference method. experiments local classiﬁer logistic regression settings; relational classiﬁers weighted-vote relational neighbor logistic regression link-based classiﬁer normalized values logistic regression link-based classiﬁer absolute count values collective inference methods include relaxation labeling iterative classiﬁcation total pairs relational classiﬁer collective inference wvrn-rl wvrn-ic nbd-rl nbd-ic nbc-rl nbc-ic. dataset results best settings reported. wang deng stacked learning logistic regression random forests deep neural nets following latest results implemented highway network shared parameters among layers essentially special case without relational connections. experiment settings report three variants basic version uses standard feedforward neural network mini-column versions cln-hwn highway nets shared parameters neural nets relu hidden layers. dropout applied recurrent layers cln-hwns every hidden layers cln-fnn. dataset divided separated sets training validation test sets. hyper-parameter tuning search number hidden layers hidden dimensions optimizers adam rmsprop. cln-fnn hidden layers hidden dimension cln-hwn models equal number parameters. best training setting chosen validation results test reported. result setting reported mean result runs. code model found github software delay prediction task predict potential delay issue unit task iterative software development lifecycle prediction point issue planning completed. dependencies issues prediction delay issue must take account related issues. largest dataset reported jboss contains issues. issue vector features connects issues relations task predict whether software issue risk getting delays table reports f-scores methods. best classiﬁers netkit wvrn-ic wvrn-rl. non-collective hwn-norel works surprisingly well almost reaching performance best collective sl-rf points short. demonstrates deep neural nets highly competitive domain best knowledge fact established. cln-hwn-full beats best collective-method sl-rf points. lost mini-batch training mode gain training speed substantial roughly pubmed publication classiﬁcation used pubmed diabetes dataset consisting scientiﬁc publications citation links among them. publication described tf/idf weighted word vector dictionary consists unique words. conducted experiments classifying publication three classes diabetes melitus experimental diabetes melitus type diabetes mellitus type visualization hidden layers randomly picked samples class visualized relu units activations layers cln-hwn interestingly activation strength seems grow higher layers suggesting learn features discriminative getting closer outcomes. class number hidden units turned every layer. figures samples class similar patterns ﬁgures samples different classes different. classiﬁcation accuracy best setting cln-hwn hidden dimensions recurrent layers. results measured microf-score macrof-score non-relational highway outperforms best baselines netkit. version cln-hwn perform best f-score measures. film genre prediction used movielens latest dataset consists movies. task predict genres movie given plot summary. local features extracted movie plot summary downloaded imdb database. removing movies without plot summary dataset remains movies. movie described bag-of-words vector frequent words. relations movies create rather balanced dataset genres collapsed labels drama comedy horror thriller adventure action mystery crime film-noir romance western documentary musical animation children fantasy sci-fi. frequencies labels reported table dataset cln-hwns work best hidden dimensions recurrent layers. table reports f-scores. best settings netkit nbc-ic nbc-rl. cln-fnn performs well micro-f fails improve macrof-score prediction. cln-hwn-mini outperforms cln-hwn-full points macro-f. fig. shows cln-fnn performs badly macrof cln-fnn works well balanced classes fails handle imbalanced classes example f-score label label contrast cln-hwn performs well classes. paper sits intersection recent independently developed areas statistical relational learning deep learning started late advanced signiﬁcantly noticeable works probabilistic relational models conditional random fields relational markov network markov logic networks collective classiﬁcation canonical task also known various forms structured prediction classiﬁcation networked data components collective classiﬁers relational classiﬁer collective inference relational classiﬁer makes predicted classes entities neighbors features. examples wvrn logistic based stacked graphical learning collective inference task jointly inferring labels entities. subject abundance solutions including message passing algorithms variational meanﬁeld discrete optimization among existing collective classiﬁers closest stacked graphical learning collective inference bypassed stacking idea based learning stack models take intermediate prediction neighborhood account. rent wave offered compact efﬁcient ways build multilayered networks function approximation program construction however much less attention paid general networked data although work pairing structured outputs deep networks parameter sharing feedforward networks recently analyzed sharing eventually transforms networks recurrent neural networks input ﬁrst layer. empirical ﬁndings performance good despite compactness model. among deep neural nets closest work rncc model also aims collective classiﬁcation using rnns. substantial differences however. rncc shufﬂes neighbors entities random sequence uses horizontal integrate sequence neighbors. emphasizes vertical depth parameter sharing gives rise vertical rnns. conceptually simpler nodes trained simultaneously separately rncc. paper proposed column network deep neural network emphasis fast accurate collective classiﬁcation. linear complexity data size number relations training inference. empirically demonstrates competitive performance rival collective classiﬁers three real-world applications delay prediction software projects pubmed diabetes publication classiﬁcation genre classiﬁcation. name suggests network narrow deep networks layer extended incorporate input preceding neighbor layers. somewhat resembles columnar structure neocortex narrow deep network plays role mini-column. wish emphasize although highway networks actual implementation excellent performance feedforward networks potentially used architecture. parameter sharing used feedforward networks become recurrent networks becomes network interacting rnns. indeed entire network collapsed giant feedforward network input/output mappings. relations shared among nodes across network enables translation invariance across network similar cnn. however limited single network shared relations. alternatively networks according distribution allows relations speciﬁc nodes. open rooms future work. extension learn pooling operation using attention mechanisms. considered homogeneous prediction tasks here assuming instances type. however framework easily extended multiple instance types.", "year": 2016}