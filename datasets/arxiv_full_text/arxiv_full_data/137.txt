{"title": "Density estimation using Real NVP", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "text": "unsupervised learning probabilistic models central challenging problem machine learning. speciﬁcally designing models tractable learning sampling inference evaluation crucial solving task. extend space models using real-valued non-volume preserving transformations powerful stably invertible learnable transformations resulting unsupervised learning algorithm exact log-likelihood computation exact efﬁcient sampling exact efﬁcient inference latent variables interpretable latent space. demonstrate ability model natural images four datasets sampling log-likelihood evaluation latent variable manipulations. domain representation learning undergone tremendous advances improved supervised learning techniques. however unsupervised learning potential leverage large pools unlabeled data extend advances modalities otherwise impractical impossible. principled approach unsupervised learning generative probabilistic modeling. generative probabilistic models ability create novel content also wide range reconstruction related applications including inpainting denoising colorization super-resolution data interest generally high-dimensional highly structured challenge domain building models powerful enough capture complexity still trainable. address challenge introducing real-valued non-volume preserving transformations tractable expressive approach modeling high-dimensional data. model perform efﬁcient exact inference sampling log-density estimation data points. moreover architecture presented paper enables exact efﬁcient reconstruction input images hierarchical features extracted model. substantial work probabilistic generative models focused training models using maximum likelihood. class maximum likelihood models described probabilistic undirected graphs restricted boltzmann machines deep boltzmann machines models trained taking advantage conditional independence property bipartite structure allow efﬁcient exact approximate posterior inference latent variables. however intractability associated marginal distribution latent variables training evaluation sampling procedures necessitate approximations like mean field inference markov chain monte carlo whose convergence time complex models figure real learns invertible stable mapping data distribution latent distribution show mapping learned dataset. function maps samples data distribution upper left approximate samples latent distribution upper right. corresponds exact inference latent state given data. inverse function maps samples latent distribution lower right approximate samples data distribution lower left. corresponds exact generation samples model. transformation grid lines space additionally illustrated remains undetermined often resulting generation highly correlated samples. furthermore approximations often hinder performance directed graphical models instead deﬁned terms ancestral sampling procedure appealing conceptual computational simplicity. lack however conditional independence structure undirected models making exact approximate posterior inference latent variables cumbersome recent advances stochastic variational inference amortized inference allowed efﬁcient approximate inference learning deep directed graphical models maximizing variational lower bound log-likelihood particular variational autoencoder algorithm simultaneously learns generative network maps gaussian latent variables samples matched approximate inference network maps samples semantically meaningful latent representation exploiting reparametrization trick success leveraging recent advances backpropagation deep neural networks resulted adoption several applications ranging speech synthesis language modeling still approximation inference process limits ability learn high dimensional deep representations motivating recent work improving approximate inference approximations avoided altogether abstaining using latent variables. autoregressive models implement strategy typically retaining great deal ﬂexibility. class algorithms tractably models joint distribution decomposing product conditionals using probability chain rule according ﬁxed ordering dimensions simplifying log-likelihood evaluation sampling. recent work line research taken advantage recent advances recurrent networks particular long-short term memory residual networks order learn state-of-the-art generative image models language models ordering dimensions although often arbitrary critical training model sequential nature model limits computational efﬁciency. example sampling procedure sequential non-parallelizable become cumbersome applications like speech music synthesis real-time rendering.. additionally natural latent representation associated autoregressive models shown useful semi-supervised learning. generative adversarial networks hand train differentiable generative network avoiding maximum likelihood principle altogether. instead generative network associated discriminator network whose task distinguish samples real data. rather using intractable log-likelihood discriminator network provides training signal adversarial fashion. successfully trained models consistently generate sharp realistically looking samples however metrics measure diversity generated samples currently intractable additionally instability training process requires careful hyperparameter tuning avoid diverging behavior. training generative network maps latent variable sample theory require discriminator network gans approximate inference variational autoencoders. indeed bijective trained maximum likelihood using change variable formula formula discussed several papers including maximum likelihood formulation independent components analysis gaussianization deep density models existence proof nonlinear solutions suggests auto-regressive models seen tractable instance maximum likelihood nonlinear residual corresponds independent components. however naive application change variable formula produces models computationally expensive poorly conditioned large scale models type entered general use. paper tackle problem learning highly nonlinear models high-dimensional continuous spaces maximum likelihood. order optimize log-likelihood introduce ﬂexible class architectures enables computation log-likelihood continuous data using change variable formula. building previous work deﬁne powerful class bijective functions enable exact tractable density evaluation exact tractable inference. moreover resulting cost function rely ﬁxed form reconstruction cost square error generates sharper samples result. also ﬂexibility helps leverage recent advances batch normalization residual networks deﬁne deep multi-scale architecture multiple levels abstraction. change variable formula given observed data variable simple prior probability distribution latent variable bijection change variable formula deﬁnes model distribution exact samples resulting distribution generated using inverse transform sampling rule sample drawn latent space inverse image generates sample original space. computing density point accomplished computing density image multiplying associated jacobian determinant also figure exact efﬁcient inference enables accurate fast evaluation model. figure computational graphs forward inverse propagation. coupling layer applies simple invertible transformation consisting scaling followed addition constant offset part input vector conditioned remaining part input vector simple nature transformation easily invertible possesses tractable determinant. however conditional nature transformation captured functions signiﬁcantly increase ﬂexibility otherwise weak function. forward inverse propagation operations identical computational cost. coupling layers computing jacobian functions high-dimensional domain codomain computing determinants large matrices general computationally expensive. combined restriction bijective functions makes equation appear impractical modeling arbitrary distributions. shown however careful design function bijective model learned tractable extremely ﬂexible. computing jacobian determinant transformation crucial effectively train using principle work exploits simple observation determinant triangular matrix efﬁciently computed product diagonal terms. build ﬂexible tractable bijective function stacking sequence simple bijections. simple bijection part input vector updated using function simple invert depends remainder input vector complex way. refer simple bijections afﬁne coupling layer. given dimensional input output afﬁne coupling layer follows equations given observation jacobian triangular efﬁciently compute determinant since computing jacobian determinant coupling layer operation involve computing jacobian functions arbitrarily complex. make deep convolutional neural networks. note hidden layers features input output layers. another interesting property coupling layers context deﬁning probabilistic models invertibility. indeed computing inverse complex forward propagation figure masking schemes afﬁne coupling layers. left spatial checkerboard pattern mask. right channel-wise masking. squeezing operation reduces tensor tensor squeezing operation checkerboard pattern used coupling layers channel-wise masking pattern used afterward. meaning sampling efﬁcient inference model. note computing inverse coupling layer require computing inverse functions arbitrarily complex difﬁcult invert. partitionings exploit local correlation structure images spatial checkerboard patterns channel-wise masking spatial checkerboard pattern mask value spatial coordinates otherwise. channel-wise mask ﬁrst half channel dimensions second half. models presented here rectiﬁed convolutional networks. although coupling layers powerful forward transformation leaves components unchanged. difﬁculty overcome composing coupling layers alternating pattern components left unchanged coupling layer updated next implement multi-scale architecture using squeezing operation channel divides image subsquares shape reshapes subsquares shape tensor squeezing operation transforms tensor effectively trading spatial size number channels. scale combine several operations sequence ﬁrst apply three coupling layers alternating checkerboard masks perform squeezing operation ﬁnally apply three coupling layers alternating channel-wise masking. channel-wise masking chosen resulting partitioning redundant previous checkerboard masking ﬁnal scale apply four coupling layers alternating checkerboard masks. propagating dimensional vector coupling layers would cumbersome terms computational memory cost terms number parameters would need trained. reason follow design choice factor half dimensions regular intervals deﬁne operation recursively experiments operation sequence coupling-squeezing-coupling operations described performed layer computing layer spatial resolution reduced number hidden layer features doubled. variables factored different scales concatenated obtain ﬁnal transformed output consequence model must gaussianize units factored ﬁner scale factored coarser scale results deﬁnition intermediary levels representation corresponding local ﬁne-grained features shown appendix moreover gaussianizing factoring units earlier layers practical beneﬁt distributing loss function throughout network following philosophy similar guiding intermediate layers using intermediate classiﬁers also reduces signiﬁcantly amount computation memory used model allowing train larger models. improve propagation training signal deep residual networks batch normalization weight normalization described appendix introduce novel variant batch normalization based running average recent minibatches thus robust training small minibatches. also apply batch normalization whole coupling layer output. effects batch normalization easily included jacobian computation since acts linear rescaling dimension. given estimated batch statistics rescaling function found technique allowed training deeper stack coupling layers also alleviated instability problem practitioners often encounter training conditional distributions scale parameter gradient-based approach. algorithm described equation shows learn distributions unbounded space. general data interest bounded magnitude. examples pixel values image typically application recommended jittering procedure order reduce impact boundary effects instead model density logit picked take account transformation computing log-likelihood bits dimension. also augment cifar- celeba lsun datasets training also include horizontal ﬂips training examples. train model four natural image datasets cifar- imagenet large-scale scene understanding celebfaces attributes speciﬁcally train downsampled versions imagenet lsun dataset train bedroom tower church outdoor categories. procedure lsun downsample image smallest side pixels take random crops celeba procedure take approximately central crop resize multi-scale architecture described section deep convolutional residual networks coupling layers rectiﬁer nonlinearity skip-connections suggested compute scaling functions hyperbolic tangent function multiplied learned scale whereas translation function afﬁne output. multi-scale architecture repeated recursively input last recursion tensor. datasets images size residual blocks hidden feature maps ﬁrst coupling layers checkerboard masking. residual blocks used images size batch size cifar- residual blocks feature maps downscale once. optimize adam default hyperparameters regularization weight scale parameters coefﬁcient prior isotropic unit norm gaussian. however distribution could used including distributions also learned training auto-regressive model variational autoencoder. figure left column examples dataset. right column samples model trained dataset. datasets shown ﬁgure order cifar- imagenet imagenet celeba lsun show table number bits dimension improving pixel baseline competitive generative methods. notice performance increases number parameters larger models likely improve performance. celeba lsun bits dimension validation decreasing throughout training little overﬁtting expected. figure manifold generated four examples dataset. clockwise left celeba imagenet lsun lsun sample quality limited capacity setting. result model outputs sometimes highly improbable samples notice especially celeba. opposed variational autoencoders samples generated model look globally coherent also sharp. hypothesis opposed models real rely ﬁxed form reconstruction cost like norm tends reward capturing frequency components heavily high frequency components. unlike autoregressive models sampling model done efﬁciently parallelized input dimensions. imagenet lsun model seems captured well notion background/foreground lighting interactions luminosity consistent light source direction reﬂectance shadows. also illustrate smooth semantically consistent meaning latent variables. latent space deﬁne manifold based four validation examples parametrized parameters project resulting manifold back data space computing results shown figure observe model seems organized latent space notion meaning goes well beyond pixel space interpolation. visualization shown appendix. test whether latent space consistent semantic interpretation trained class-conditional model celeba found learned representation consistent semantic meaning across class labels paper deﬁned class invertible functions tractable jacobian determinant enabling exact tractable log-likelihood evaluation inference sampling. shown class generative model achieves competitive performances terms sample quality log-likelihood. many avenues exist improve functional form transformations instance exploiting latest advances dilated convolutions residual networks architectures paper presented technique bridging auto-regressive models variational autoencoders generative adversarial networks. like auto-regressive models allows tractable exact log-likelihood evaluation training. allows however much ﬂexible functional form similar generative model variational autoencoders. allows fast exact sampling model distribution. like gans unlike variational autoencoders technique require ﬁxed form reconstruction cost instead deﬁnes cost terms higher level features generating sharper images. finally unlike variational autoencoders gans technique able learn semantically meaningful latent space high dimensional input space. make algorithm particularly well suited semi-supervised learning tasks hope explore future work. real generative models additionally conditioned additional variables create structured output algorithm. resulting class invertible transformations treated probability distribution modular also used improve upon probabilistic models like auto-regressive models variational autoencoders. variational autoencoders transformations could used enable ﬂexible reconstruction cost ﬂexible stochastic inference distribution probabilistic models general also beneﬁt batch normalization techniques applied paper. deﬁnition powerful trainable invertible functions also beneﬁt domains generative unsupervised learning. example reinforcement learning invertible functions help extend functions argmax operation tractable continuous qlearning representation local linear gaussian approximations appropriate authors thank developers tensorﬂow thank sherry moore david andersen shlens help implementing model. thank aäron oord yann dauphin kyle kastner chelsea finn maithra raghu david warde-farley daniel jiwoong oriol vinyals fruitful discussions. finally thank poole rafal jozefowicz george dahl input draft paper.", "year": 2016}