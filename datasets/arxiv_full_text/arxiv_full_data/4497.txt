{"title": "Competitive on-line learning with a convex loss function", "tag": ["cs.LG", "cs.AI", "I.2.6; I.5.1"], "abstract": "We consider the problem of sequential decision making under uncertainty in which the loss caused by a decision depends on the following binary observation. In competitive on-line learning, the goal is to design decision algorithms that are almost as good as the best decision rules in a wide benchmark class, without making any assumptions about the way the observations are generated. However, standard algorithms in this area can only deal with finite-dimensional (often countable) benchmark classes. In this paper we give similar results for decision rules ranging over an arbitrary reproducing kernel Hilbert space. For example, it is shown that for a wide class of loss functions (including the standard square, absolute, and log loss functions) the average loss of the master algorithm, over the first $N$ observations, does not exceed the average loss of the best decision rule with a bounded norm plus $O(N^{-1/2})$. Our proof technique is very different from the standard ones and is based on recent results about defensive forecasting. Given the probabilities produced by a defensive forecasting algorithm, which are known to be well calibrated and to have good resolution in the long run, we use the expected loss minimization principle to find a suitable decision.", "text": "consider problem sequential decision making uncertainty loss caused decision depends following binary observation. competitive on-line learning goal design decision algorithms almost good best decision rules wide benchmark class without making assumptions observations generated. however standard algorithms area deal ﬁnite-dimensional benchmark classes. paper give similar results decision rules ranging arbitrary reproducing kernel hilbert space. example shown wide class loss functions average loss master algorithm ﬁrst observations exceed average loss best decision rule bounded norm plus proof technique diﬀerent standard ones based recent results defensive forecasting. given probabilities produced defensive forecasting algorithm known well calibrated good resolution long expected loss minimization principle suitable decision. simple problem sequential decision making consider paper loss caused decision depends following binary observation relevant information available decision maker time makes decision collected call datum example time series applications datum contain recent observations; pattern recognition observations true classes patterns datum vector pattern’s attributes. generated independently probability distribution. recent approach known learning theory prediction expert advice information theory universal prediction avoids making assumptions observations data generated. instead goal decision maker compete less general benchmark class decision rules mapping phrase competitive on-line refer area emphasizing similarities competitive on-line algorithms computation theory). first papers competitive on-line learning general loss functions dealt countable benchmark classes. next step consider ﬁnite-dimensional benchmark classes paper continues inﬁnite-dimensional classes. however assumed benchmark class contains perfect decision rule.) idea central results reader advised start corollaries implicit assumption common work competitive on-line learning decision maker small decisions aﬀect future observations. mathematical assumption already mentioned make assumptions observations generated; however interpretation results becomes problematic decision maker small. decision makers still algorithms prediction main result stated several examples given proved describe main ideas behind proof prove preparatory results decision algorithm explicitly described conclude short list directions research information available decision maker time makes decision collected called datum. assume data elements data space decisions elements decision space remark paper interested ﬁrst prediction future observations. however framework allows fairly wide class loss functions interpreted terms predictions considered borderline; theorem still applicable case ceases applicable tails shrink less fast. symmetric positive deﬁnite αiαj xm). hand every symmetric positive deﬁnite exists unique rkhs kernel th´eor`eme notions kernel rkhs symmetric positive deﬁnite function content sometimes kernel mean symmetric positive deﬁnite function kernels sense main source rkhs learning theory; e.g. remark games section illustrate remark decisions best interpreted predictions loss functions satisfying called proper scoring rules. loss functions encourage honesty optimal predict true probability later another loss function type section describes intuition behind proof. following sections carry proof formally independent section. also describe general research program lead hoped many results. proof technique based game-theoretic alternative standard measure-theoretic axioms probability many standard laws probability including weak strong laws large numbers central limit theorem iterated logarithm restated terms perfect information games involving three players reality forecaster skeptic. typical game-theoretic probability states skeptic strategy which without risking bankruptcy greatly enriches violated. strategies skeptic explicitly constructed continuous functions; game-theoretic laws probability continuous strategy skeptic called continuous laws probability. game-theoretic probability developed large degree parallel measure-theoretic probability. following literature paper spawned paper pointed surprising feature game-theoretic continuous probability forecaster strategy probability prevents skeptic’s capital growing words continuous probability forecasting strategy perfect concerned result obtained binary forecasting extended general protocols. forecasting strategies obtained various laws probability called defensive forecasting strategies. choose goal could achieved knew true probabilities generating observations. important goal practical sense stated terms observable quantities data decisions observations. goal allowed contain theoretical quantities true probabilities themselves achievable matter true probabilities are. construct decision strategy which using true probabilities leads goal. realistically however know true probabilities. them isolate probability proof decision strategy achieves goal depends; typically stated continuous game-theoretic probability. forecasting strategy whose forecasts least good true probabilities isolated concerned. remains feed decision strategy forecasts. allowed range function class excessively wide suppose simplicity singleton element denoted decision strategy make decision round i.e. decision leads smallest expected loss. sometimes choice function. strongest possible result speciﬁc laws probability general large numbers. convenient following informal terminology introduced forecasts output forecasting strategy forecasting strategy good calibration-cum-resolution lefthand side much less relatively wide class functions notice applying large numbers establishing approximate inequalities need general particular need calibration resolution separately calibration-cum-resolution. speciﬁc probability laws concerned with. section core proof theorem first describe forecasting protocol forecaster tries predict observations chosen reality. following introduce another player skeptic allowed odds implied forecaster’s moves. risking bankruptcy forecaster also provides additional number aﬀect skeptic’s capital; intuitively role help forecaster’s customers position buridan’s break tie. claims fullest possible knowledge reality chooses observations skeptic tries prove wrong gambling him. decision protocol decision maker make claims simply tries minimize losses. convenient make forecasts chosen topological space. lexicographic square deﬁned equipped following linear order points means either problem ...) topology lexicographic square usual generated open intervals continuous strategy skeptic exists strategy forecaster allow skeptic’s capital grow regardless reality doing. state observation strongest form make skeptic announce strategy round forecaster’s move round rather announce full strategy beginning game. therefore consider following perfect-information game remark terminology introduced previous section theorem resolution. suﬃcient purpose paper easy similar statements hold calibration-cum-resolution calibration. similar deﬁne right-stripped lexicographic square right-stripped lexicographic always outputs roots consider case corresponds since continuous absence roots conjunction means positive everywhere setting case guarantees †£aln still ensures remains notice still holds. disadvantage typically interested strategy’s actual rather expected loss. derivation shows role randomization choice function randomization required unless typically rarely situation complete uncertainty therefore little randomization needed essentially breaking. actual loss close expected loss. would interesting derive formal statements along lines.", "year": 2005}