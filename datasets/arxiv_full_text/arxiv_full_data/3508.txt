{"title": "The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?", "tag": ["cs.RO", "cs.CV", "cs.LG", "stat.ML"], "abstract": "A successful grasp requires careful balancing of the contact forces. Deducing whether a particular grasp will be successful from indirect measurements, such as vision, is therefore quite challenging, and direct sensing of contacts through touch sensing provides an appealing avenue toward more successful and consistent robotic grasping. However, in order to fully evaluate the value of touch sensing for grasp outcome prediction, we must understand how touch sensing can influence outcome prediction accuracy when combined with other modalities. Doing so using conventional model-based techniques is exceptionally difficult. In this work, we investigate the question of whether touch sensing aids in predicting grasp outcomes within a multimodal sensing framework that combines vision and touch. To that end, we collected more than 9,000 grasping trials using a two-finger gripper equipped with GelSight high-resolution tactile sensors on each finger, and evaluated visuo-tactile deep neural network models to directly predict grasp outcomes from either modality individually, and from both modalities together. Our experimental results indicate that incorporating tactile readings substantially improve grasping performance.", "text": "abstract successful grasp requires careful balancing contact forces. deducing whether particular grasp successful indirect measurements vision therefore quite challenging direct sensing contacts touch sensing provides appealing avenue toward successful consistent robotic grasping. however order fully evaluate value touch sensing grasp outcome prediction must understand touch sensing inﬂuence outcome prediction accuracy combined modalities. using conventional model-based techniques exceptionally difﬁcult. work investigate question whether touch sensing aids predicting grasp outcomes within multimodal sensing framework combines vision touch. collected grasping trials using two-ﬁnger gripper equipped gelsight high-resolution tactile sensors ﬁnger evaluated visuo-tactile deep neural network models directly predict grasp outcomes either modality individually modalities together. experimental results indicate incorporating tactile readings substantially improve grasping performance. humans make extensive multi-modal perception grasping including visual tactile sensing vision allows fast localization objects touch provides accurate perception compliance contact force contact established even grasp hard manipulation tactile sensors demonstrated detecting compensating slip grasping fragile objects nonetheless adoption tactile sensors slow hardware limitations technologies employed importantly challenges associated integrating tactile sensors standard control schemes. tactile readings typically difﬁcult model small errors measurement calibration substantially reduce performance even best analytic models. modeling issues echo challenges that recently also plagued vision-based sensing. recently number works proposed vision-based grasping methods rely end-toend training rather analytic modeling directly predict grasp locations grasp outcomes based camera images however learning process tactile readings grasping particularly end-to-end models deep neural networks studied extensively despite importance modality. paper answer question whether integrating touch sensing aids predicting grasp outcomes. however answer question complicated inherent difﬁculty integrating vision touch multi-modal sensing framework. propose employ end-to-end learning approach predicting grasp outcome allows evaluate relative importance modality well combined multi-modal framework series controlled experiments. touch sensing employ gelsight tactile sensors provide high-resolution images deformation caused contacts graspable objects train deep neural networks vision-based touch-based combined vision touch-based grasp outcome prediction. robot employ models grasping attempting various grasp locations choosing model predicts highest probability success. knowledge work ﬁrst present end-to-end learned system robotic grasping combines rich visual tactile sensing provides controlled evaluation beneﬁts touch sensing grasp performance. method substantially simpler manually designed analytic grasping metrics experimental results demonstrate incorporating tactile sensing improves overall grasping performance substantially. related work learning grasp. signiﬁcant body work robotics studied analytic grasping models models object geometry environments robot grippers typically make manually deﬁned grasping metric methods provide considerable insight physical interactions grasping relationship grasp metrics realworld outcomes vulnerable model misspeciﬁcation unmodeled effects. alternative data-driven approaches sought predict grasp outcomes human supervision simulation autonomous robotic data collection typically using visual depth observations. however methods applied tactile sensing therefore limited ability reason contact forces pressures compliance. substitute capability works used wrist-mounted depth cameras obtain better estimate local geometry gualtieri detected grasping poses depth data using convolutional neural network trained simulated data. work over-the-shoulder cameras show obtain substantial improvement incorporating rich tactile sensing. detailed survey learning grasp refer readers bohg tactile sensors grasping. range touch sensors proposed literature employed variety ways robotic grasping. example bekiroglu schill used tactile sensors estimate grasp stability proposed incorporating tactile readings dynamics models objects dexterous hand. works extract features tactile signals detect slip apply proper grasping force. researchers also proposed robotic systems integrate visual tactile information grasping using model-based methods improved grasping performance single-modality inputs. however best knowledge work ﬁrst propose end-to-end training models process rich visual tactile inputs predict task outcomes case corresponds predicting grasp succeed fail also ﬁrst figure diagram visual-tactile multi-modal model. grasping time images front camera gelsight sensors images deep neural network predict whether grasping successful not. network data sensors ﬁrst passed convolutional neural network resulting features concatenated input fully-connected network. provide controlled evaluation whether incorporating touch actually improves grasp success outcome prediction within learned visuo-tactile system. gelsight sensor. gelsight sensor optical tactile sensor measures highresolution topography contact surface surface sensor soft elastomer painted reﬂective membrane deforms shape object upon contact. underneath elastomer ordinary webcam views deformed gel. illuminated colored leds light different directions. dong showed grasping tasks gelsight signal predict detect slip different kinds information movement object texture loss contact area stretching sensor surface. important advantage gelsight sensor sensory data consists standard image allowing standard convolutional neural network architectures designed visual sensing used process sensor’s readings. prior work material estimation gelsight successfully applied convolutional neural networks pretrained natural image data. examples tactile data gelsight shown figure consider placement robot’s gripper figure robot attempts grasp object position successful? here sensory modalities give different possibly complementary information prospects successful grasp. example tell camera image gripper favorable position near object’s center mass also tell tactile information object rigid many bumps ridges therefore unlikely slip. study much tactile sensing helps predict grasp outcomes train neural network predict whether robot’s grasp successful using combination tactile visual cues. network computes probability successful grasp contains images multiple modalities irgb represents image frontal camera igelsightl igelsightr images recorded ﬁngertip gelsight sensors. experiments compared different combinations inputs vision tactile input using depth place image. since inputs model including tactile input images represent convolutional neural network. following multi-modal learning work fuse different modalities late stage model. illustrated figure images ﬁrst passed standard convolutional network case uses resnet- architecture results independent convolutions concatenated two-layer fully-connected network. visual tactile inputs make temporal information. images simply supply network images image taken grasp moment grasp features networks speciﬁcally spatially-pooled features penultimate layer resnet model concatenated together. gelsight model exploit fact deformations expressed temporal derivatives apply network temporal difference ita. training network initialize weights visual tactile cnns using model pre-trained imagenet share parameters networks modality train network epochs using adam optimizer starting learning rate training apply data augmentation input data. images ﬁrst crop images bounding containing table holds objects. then following standard practice object recognition resize images randomly sample crops them. also randomly images horizontal direction. gelsight images lack stationarity properties natural images ﬁxed locations lights borders gel. however still apply data augmentation techniques prevent algorithm overﬁtting appearance particular sensor’s gel. section detail model learned section used selecting grasping conﬁgurations. since model predicts grasp outcome based visual tactile readings must close ﬁngers around object evaluate model’s prediction. experimental comparison perform randomly chosen gripper closures vicinity object evaluate prediction model accept gripper pose model predicts sufﬁciently high probability successful outcome. speciﬁcally randomly vary grasp parameters end-effector x-y-z coordinates angle gripper force applied gripper. iteration randomly select parameters move gripper desired conﬁguration close gripper evaluate success rate predicted model measured visual tactile readings. success rate pre-deﬁned threshold grasp considered potentially successful object lifted observe outcome grasp. otherwise random parameters selected exploration continues. theory approach might never stop practice experiments trials would last less couple minutes. overall scheme intuitively summarized separate mechanism proposes grasps rejects process proposal accepted. future work process might accelerated incorporating optimization procedure grasp selection instead proposing grasps random. however purposes evaluating practical differences purely visual visuo-tactile grasping found simple approach sufﬁcient. experiments used setup shown figure consisting -dof sawyer weiss wsg- parallel gripper gelsight sensors ﬁnger. design gelsight sensors used project introduced sensors provides raw-pixel measurements resolution area additionally microsoft kinect sensor mounted front robot. noted visuo-tactile model image used depth used hand-engineered data collection procedure comparison section data collection process automated allow large scale continuous data collection. trial depth data kinect used approximately identify position object rough cylindrical proxy. selected grasp positions center cylinder plus small random perturbation. height random height table cylinder random gripper orientation. moreover randomized gripping force collect large variety behaviors stable grasps occasional slips overly gentle grasps fail often. moving chosen position orientation closing gripper desired gripping force gripper would attempt lift object wait seconds. object still gripper seconds robot would object back randomized position trial would start. trial considered three snapshots initial state system measures state gripper completed closure ﬁngers object still ground. finally measured seconds completed lift-off give time object stabilize eventually slip). three snapshots used inputs models used label data. visualization chronology data collection three snapshots shown figure overall trial took seconds robot execution. labels data automatically generated using deep neural network classiﬁer trained detect contacts using gelsight images measured performed additional manual labeling small collected data automatic classiﬁcation borderline ambiguous rare cases visual inspection would indicate wrong label. overall collected grasping trials unique objects shown figure section compare predictive performance multi-modal visuo-tactile model models sensory modalities. evaluate models comparing accuracy predicting grasp outcome unseen test data evaluating performance actually choosing successful grasps real-world robotic experiment. dataset used code videos grasping trials available website https//sites.google.com/view/ the-feeling-of-success/. model tactile sensing compare model trained sensory inputs? combining multiple modalities help predict whether grasp successful? address questions explored several variations model measuring ability predict whether grasp successful. evaluation procedure given dataset collected described section divided dataset training test object appears set. using data measured predictive power model repeated experiments three random splits objects averaged results. multi-modal models first studied tactile model operates gelsight images moment grasp sensors. compared performance visual models operated color images grasp used depth images also explored combinations modalities. particular vision tactile model fuses visual tactile models also tried providing location orientation gripper additional information visual network function additional localization cue. this represent gripper parameters using three-layer fully connected network. also considered hand-crafted tactile features. inspired dong used pixel intensity measure indentation computed average absolute difference pair gelsight images captured grasp. note feature conveys surface area indentation approximately number pixels differ images amount force included feature touch sensors averaging color channels. trained linear features table call indentation model. finally since gel-based sensors contain large amounts variation outputs trained versions model ﬁngertip sensor. variation inter-sensor variation caused differences appearance silicone sensors intra-sensor variation caused wear-and-tear undergoes repeatedly grasps objects. analysis table tactile model signiﬁcantly outperformed visual model. furthermore improvement combining visual tactile information multi-modal visual-tactile model performing best models evaluation. indicates combining multiple complementary modalities improve grasp outcome prediction. strong performance hand-crafted features part explained small size dataset limits capabilities large expressive models. however since principle experiments evaluate relative importance touch sensing combined visuo-tactile model used end-to-end trained models remainder experiments since end-to-end models integrate touch visual sensing type convolutional network provide fair setting evaluating beneﬁts modality. analysis relative performance hand-designed learned features left future work likely feasible larger datasets though current experiments suggest end-to-end training provide improved performance manually designed features. model used tactile sensors outperformed models used also found variations predictive power different gelsight sensors. result fact attached better-performing model replaced fewer times data collection process sensor’s replaced often tearing slippage thus introducing factor another variability data potentially making learning process harder. table classiﬁcation accuracy models trained different modalities input. models trained tactile sensing achieve better accuracy purely visual models. furthermore multi-modal model achieves best overall results. evaluating model prediction accuracy test data give sense predictive power model sensory modality practice interested ability model actually help choose good grasps real world. evaluate this conducted realworld grasping experiment using robot setup compared multiple models. verify generalization capabilities various approaches tested objects never seen either training test shown table object repeated experiment times choose grasps using selection method described section models retrained available data ﬁrst baseline evaluated manually engineered image-based grasping procedure used autonomously collecting data described section baseline made depth-sensors cylinder around object subsequently randomized grasp pose force applied. since used method autonomously collecting data quite heavily ﬁne-tuned perform well. methods evaluation end-to-end trained model used vision multi-modal model combined vision tactile sensing. experimental results presented table indicate multi-modal tactile+vision model outperform approaches improvement grasp selection using vision alone. success rates individual objects shown table indicate objects varied considerably difﬁculty. computer mouse particular extremely difﬁcult grasp baseline visual model unable pick reliably vision tactile model picked time. also observed that several cases purely visual model attempted lift objects ﬁngers making contact visuo-tactile model never exhibited problem since empty grasps easily recognized tactile sensing. overall experimental results suggest learning end-to-end multi-modal model makes vision rich tactile sensors beneﬁcial terms predictive accuracy employed actual grasping real robotic system. grasping scheme data collection vision tactile vision table grasping success rates different grasp outcome prediction models. evaluation performed objects seen training signiﬁcantly differ shape dimension weight color material training objects. data collection shows baseline results obtained detection method used data collection. incorporating tactile sensing yields best results large improvements several objects. paper aimed answer following question touch sensing help predict grasp outcome compared purely visual perception? study question proposed end-to-end approach predicting grasp outcome using visual tactile inputs using tactile sensor provides detailed information contacts forces compliance. end-to-end approach require characterization tactile sensors model robot object. result method requires minimal engineering actual grasping system instead learning suitable visuo-tactile representations data. autonomously collected grasps used train multiple deep neural network model predicting grasp outcomes different input modalities. results indicate visuo-tactile multi-modal model substantially improves ability predict grasp outcomes compared models based single sensorial modality validate result performed real-world evaluation different models active grasp selection. experimental results demonstrate visuo-tactile multi-modal model outperform vision model achieving previously unseen objects grasp success rate compared vision model. although proposed approach allowed study importance tactile sensing robotic grasping method necessarily ideal solution practical robotic grasping. learned model provides rejection mechanism evaluates outcome grasp grasp actually performed such based quality proposing mechanism convergence acceptable grasp might take arbitrarily high number grasp attempts. experiments used basic random search propose grasp location sufﬁcient address main experimental hypothesis work. although experiments good solution would typically accepted within grasps trials took substantially longer trial requiring grasps lifting object successfully. moreover approach explicitly consider post-liftoff event slipping hence making full interactive nature tactile information grasping. future work overcome limitations proposing visuo-tactile solutions practical real-world robot grasping. results obtained demonstrate importance tactile sensing real-world robot grasping effectiveness deep neural network models learn directly visuo-tactile inputs. importance tactile sensing robot grasping demonstrated question arises efﬁciently integrate tactile sensors select successful grasp conﬁgurations? thank chris myers chapman citris invention support printing siyuan dong technical support gelsights. research supported berkeley deepdrive honda ofﬁce naval research young investigator award toyota research institute lincoln laboratory darpa national science foundation. also thank nvidia equipment donations. veiga hoof peters hermans. stabilizing novel objects learning predict tactile slip. ieee/rsj conference intelligent robots systems doi./iros... pinto gupta. supersizing self-supervision learning grasp tries robot hours. ieee international conference robotics automation pages doi./icra... levine pastor krizhevsky ibarz quillen. learning hand-eye coordination robotic grasping deep learning large-scale data collection. international journal robotics research doi./. johns leutenegger davison. deep learning grasp function grasping gripper pose uncertainty. ieee/rsj international conference intelligent robots systems pages doi./iros... mahler pokorny roderick laskey aubry kohlhoff kr¨oger kuffner goldberg. dex-net cloud-based network objects robust grasp planning using multi-armed bandit model correlated rewards. ieee international conference robotics automation pages gualtieri saenko platt. high precision grasp pose detection dense clutter. ieee/rsj international conference intelligent robots systems pages schill laaksonen przybylski kyrki asfour dillmann. learning conieee tinuous grasp stability humanoid robot hand based tactile sensing. embs international conference biomedical robotics biomechatronics pages ieee bekiroglu kragic billard. learning grasp adaptation experience tactile sensing. ieee/rsj international conference intelligent robots systems pages jara pomares candelas torres. control framework dexterous manipulation using dynamic visual servoing tactile sensors’ feedback. sensors doi./s.", "year": 2017}