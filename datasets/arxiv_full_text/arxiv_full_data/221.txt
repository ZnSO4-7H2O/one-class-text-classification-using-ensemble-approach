{"title": "Super-Convergence: Very Fast Training of Residual Networks Using Large  Learning Rates", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "In this paper, we show a phenomenon, which we named \"super-convergence\", where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate. Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence.", "text": "paper show phenomenon named super-convergence residual networks trained using order magnitude fewer iterations used standard training methods. existence super-convergence relevant understanding deep networks generalize well. elements super-convergence training cyclical learning rates large maximum learning rate. furthermore present evidence training large learning rates improves performance regularizing network. addition show super-convergence provides greater boost performance relative standard training amount labeled training data limited. also derive simpliﬁcation hessian free optimization method compute estimate optimal learning rate. architectures code replicate ﬁgures paper available github.com/lnsmith/super-convergence. deep neural networks achieved amazing successes range applications understanding stochastic gradient descent works well remains open active area research. paper provides unique empirical evidence supports theories papers others. speciﬁcally show that certain datasets residual network architectures hyper-parameter values using large learning rates cyclical learning rate method speed training order magnitude. analogous phenomenon super-conductivity happens limited circumstances provides theoretical insights materials named phenomenon super-convergence. super-convergence might practical value primary purpose paper provide empirical support theoretical insights active discussions literature understanding generalization. figure provides comparison test accuracies super-convergence example result typical training regime cifar- using layer residual network architecture. piecewise constant training reaches peak accuracy approximately iterations super-convergence method reaches higher accuracy iterations. figure shows results range stepsize values training lasted cycle. modiﬁed learning rate schedule achieves higher ﬁnal test accuracy typical training iterations. addition total number iterations increases ﬁnal accuracy improves derive simpliﬁcation second order hessian-free optimization method estimate optimal learning rates demonstrates large learning rates wide minima. paper refer typical standard piecewise-constant training regime means practice using global learning rate many epochs test accuracy plateaus continuing train lower learning rate decreased factor process reducing learning rate continuing train often repeated three times. exists extensive literature stochastic gradient descent bottou relevant work. also exists signiﬁcant amount literature loss function topology deep networks review literature). paper contains discussion loss function topology follows work goodfellow characterizing landscape. large learning rate values contrast suggestions literature maximum learning rate value bottou methods adaptive learning rates also active area research. paper uses simpliﬁcation second order hessian-free optimization estimate optimal values learning rate. addition utilize techniques described schaul gulcehre also show adaptive learning rate methods nesterov momentum adadelta adagrad adam sufﬁciently large learning rates effective lead super-convergence without using clr. warmup learning rate strategy could considered discretization also recently suggested note loshchilov hutter subsequently proposed similar method call sgdr. sgdr method uses sawtooth pattern cosine followed jump back original value. experiments show possible observe super-convergence phenomenon using pattern. work intertwined several active lines research deep learning research community including lively discussion stochastic gradient descent understanding solutions generalize well research importance noise generalization generalization small large mini-batches. defer discussion lines research section compare empirical results theoretical insights. work cyclical learning rates learning rate range test ﬁrst suggested smith later updated smith speciﬁes minimum maximum learning rate boundaries stepsize. stepsize number iterations used step cycle consists steps learning rate increases decreases. smith tested numerous ways vary learning rate boundary values found equivalent therefore recommended simplest letting learning rate change linearly suggest discrete jumps). please note cyclical noise obtained combining curriculum learning simulated annealing long history deep learning. range test used determine super-convergence possible architecture. range test training starts zero small learning rate slowly increased linearly throughout pre-training run. provides information well network trained range learning rates. figure shows typical curve range test test accuracy distinct peak. starting small learning rate network begins converge learning rate increases eventually becomes large causes training/test accuracy decrease. learning rate peak largest value maximum learning rate bound using clr. minimum learning rate chosen dividing maximum factor optimal initial learning rate typical training regime usually falls minimum maximum values. runs range test cifar- layer residual networks obtains curves shown figure please note learning rate values tested least order magnitude lager typical values learning rate. test accuracy remains consistently high unusually long range large learning rates. unusual behavior motivated experimentation much higher learning rates believe behavior range test indicative potential super-convergence. three curves ﬁgure runs maximum number iterations test accuracy remains consistently high long range large learning rate values. figure provides example transversing loss function topology. ﬁgure helps give intuitive understanding super-convergence happens. blue line figure represents trajectory training converging indicate location solution iteration indicates progress made training. early iterations learning rate must small order training make progress appropriate direction. figure also shows signiﬁcant progress made early iterations. however slope decreases amount progress iteration little improvement occurs bulk iterations. figure shows close ﬁnal parts training solution maneuvers valley local minimum within trough. cyclical learning rates well suited training loss topology takes form. learning rate initially starts small allow convergence begin. network traverses valley learning rate large allowing faster progress valley. ﬁnal stages experiments observed super-convergence large learning rates. range test reveals evidence regularization results shown figure figure shows increasing training loss decreasing test loss learning rate increases approximately training cifar- dataset resnet- architecture implies regularization occurring training large learning rates. similarly figure presents training test loss curves resnet- resnet- decreasing generalization error. addition range test residual networks layers obtained similar results. additional evidence large learning rates regularizing training. shown figure ﬁnal test accuracy results super-convergence training demonstrably better accuracy results typical training method. literature type improvement ﬁnal training accuracy often taken evidence regularization. furthermore others show large learning rates leads larger gradient noise leads better generalization smith gradient steepest descent optimization method uses slope computed derivative move direction greatest negative gradient iteratively update variable. given initial point gradient descent proposes next point step size learning rate denote parameters neural network loss function apply gradient descent learn weights network; i.e. input solution non-linearity weights layer biases layer hessian-free optimization method suggests second order solution utilizes slope information contained second derivative martens main idea second order newton’s method loss function locally approximated quadratic general feasible compute hessian matrix elements number parameters network unnecessary compute full hessian. hessian expresses curvature directions high dimensional space relevant curvature direction direction steepest descent traverse. concept contained within hessian-free optimization martens suggests ﬁnite difference approach obtaining estimate hessian gradients right hand side learning rate value actually used calculations update weights. equation expression adaptive learning rate weight update. borrow method schaul obtain estimate global learning rate weight speciﬁc rates summing numerator denominator minor difference. schaul expression squared leading positive values therefore absolute values quantity maintain positivity illustrative purposes computed optimal learning rates weights every iteration using equation runs ﬁrst learning rate constant value second range stepsize iterations. since computed learning rate exhibited rapid variations computed moving average estimated learning rate results shown figure ﬁrst iterations. curve qualitatively shows optimal learning rates range architecture. figure used weights computed every iterations learning rate estimation iterations. interesting divergence happens here keeping learning rate constant learning rate estimate initially spikes value drops near hand learning rate estimate using remains high settles value large learning rates indicated figures caused small values hessian approximation small values hessian implies ﬁnding wide local minima. paper perform full evaluation effectiveness technique tangential theme work. method demonstrate training large learning rates indicated approximation. leave full assessment tests method estimate optimal adaptive learning rates future work. section highlights signiﬁcant experiments. additional experiments details architecture illustrated supplemental section. architectures hyper-parameters used experiments available github.com/lnsmith/super-convergence. figure comparisons super-convergence typical training outcome piecewise constant learning rate schedule. figure provides comparison super-convergence reduced number training samples. amount training data limited performance result standard training super-convergence increases. piecewise constant learning rate schedule training encounters difﬁculties diverges along way. hand network trained speciﬁc parameters exhibits super-convergence trains without difﬁculties. highest accuracies attained using standard learning rate schedules listed table super-convergence test accuracy better training cases respectively. hence super-convergence becomes beneﬁcial training data limited. table comparison ﬁnal accuracy results various training regimes resnet- cifar-. value moving_average_f raction parameter batch normalization. pc-lr standard piecewise constant learning rate policy described section initial learning rate also experiments resnets number layers range layers; experiments residual networks layers .... figure illustrates results resnet- resnet- typical training regime standard initial learning rate stepsize iterations. entire range architecture depths super-convergence possible. accuracy increase super-convergence appears greater shallower architectures deeper architectures recently discussions deep learning literature effects larger batch size generalization hence investigated effects total mini-batch size used super-convergence training found small improvement performance larger batch sizes seen figure addition figure shows generalization approximately equivalent small large mini-batch sizes. result differs results reported elsewhere illustrates beneﬁt training large learning rates ability large batch sizes series experiments variety ranges clr. table shows results maximum learning rate bounds experiments show maximum learning rate approximately performed well. although experiments used minimum learning rate training single cycle minimum learning rate performs quite well. tested effect batch normalization super-convergence. initially found use_global_stats true test phase prevents super-convergence. however realized using default value moving_average_f raction appropriate typical long training times. however network trained quickly super-convergence value update accumulated global statistics quickly enough. hence found smaller values listed table appropriate. variety experiments super-convergence continued occur. experiments cifar- demonstrated super-convergence phenomenon. found adding dropout architecture still permitted super-convergence improved results small amount. experiments also investigated whether adaptive learning rate methods piecewise constant training regime would learn adaptively large learning rate improve performance. tested nesterov momentum adadelta adagrad adam cifar- resnet- architecture none methods speed training process similar fashion super-convergence. important indicative failing theory behind approaches. also tried methods appropriate bounds found super-convergence happens adadelta adagrad nesterov momentum unable achieve super-convergence adam. addition investigated effect momentum. experiments momentum range found momentums yield higher ﬁnal test accuracy. illustrate results experiments supplemental materials. substantial discussion literature stochastic gradient descent understanding solutions generalize well chaudhari soatto jastrz˛ebski smith kawaguchi super-convergence provides empirical evidence supports theories contradicts others points need theoretical understanding. hope response super-convergence similar reaction initial report network memorization sparked active discussion within deep learning research community better understanding factors leading solutions generalize well work impacts line research importance noise generalization. paper focused large learning rates adds noise middle part training. recently jastrz˛ebski stated higher levels noise lead solutions better generalization. speciﬁcally showed ratio learning rate batch size along variance gradients controlled width local minima found sgd. independently chaudhari soatto show performs regularization causing equilibrium crucial obtain good generalization performance derive ratio learning rate batch size alone controls entropic regularization term. also state data augmentation increases diversity sgd’s gradients leading better generalization. papers provide theoretical support super-convergence phenomenon. addition several papers literature state wide local minima produce solutions generalize better sharp minima super-convergence results align results. addition several recent papers generalization small large minibatches relationship gradient noise learning rate batch size. results supplements work illustrating possibility time varying high noise levels training. mentioned above jastrz˛ebski showed noise proportional learning rate variance loss gradients divided batch size. similarly smith derived noise scale \u0001n/b gradient noise learning rate number training samples momentum coefﬁcient. furthermore smith showed equivalence increasing batch sizes instead decreasing learning rate schedule. importantly authors demonstrated noise scale relevant training learning rate batch size. keskar study generalization small large mini-batches stating small mini-batch sizes lead wide minima large batch sizes lead sharp minima. also suggest batch size warm start ﬁrst epochs using large batch size amounts training large gradient noise epochs removing results contradicts suggestion found preferable start training little noise increase reach noise peak reduce noise level last part training goyal large mini-batch size adjust learning rate linearly batch size. also suggest gradual warmup learning rate discretized version matches experience increasing learning rate. make point relevant adjusting batch size; network uses batch normalization different mini-batch sizes leads different statistics must handled. hoffer made similar point batch norm suggested using ghost statistics. also hoffer show longer training lengths form regularization improves generalization. hand results show different form regularization comes training large learning rates permits much shorter training lengths. characterizing good noise improves trained network’s ability generalize versus noise interferes ﬁnding good solution lack uniﬁed framework treating noise/diversity architectural noise dropconnect noise hyper-parameter settings adding gradient noise adding noise weights input diversity gradient diversity shown lead ﬂatter local minimum better generalization. uniﬁed framework resolve conﬂicting claims literature value these architectural noise versus hoffer furthermore many papers study factors independently focusing trees might miss forest. time dependent application good noise training described above combining curriculum learning simulated annealing leads cyclical application. best knowledge applied sporadically methods cyclical batch sizes fall single umbrella time dependent gradient diversity also might learn optimal noise level training network. discovering ways stabilize optimization large noise levels evidence indicates normalization catalyst enabling super-convergence face destabilizing noise large learning rates. normalization methods streaming normalization liao techniques stabilize training need investigation discover better ways keep stable presence enormous good noise. classical physics insufﬁcient explaining super-conductivity discovered pointed need theories quantum mechanics. similarly super-convergence indicates need theories generalization. devansh arpit stanisław jastrz˛ebski nicolas ballas david krueger emmanuel bengio maxinder kanwal tegan maharaj asja fischer aaron courville yoshua bengio closer look memorization deep networks. arxiv preprint arxiv. yoshua bengio jérôme louradour ronan collobert jason weston. curriculum learning. proceedings annual international conference machine learning meire fortunato mohammad gheshlaghi azar bilal piot jacob menick osband alex graves vlad mnih remi munos demis hassabis olivier pietquin noisy networks exploration. arxiv preprint arxiv. priya goyal piotr dollár ross girshick pieter noordhuis lukasz wesolowski aapo kyrola andrew tulloch yangqing kaiming accurate large minibatch training imagenet hour. arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition elad hoffer itay hubara daniel soudry. train longer generalize better closing generalization large batch training neural networks. arxiv preprint arxiv. stanisław jastrz˛ebski zachary kenton devansh arpit nicolas ballas asja fischer yoshua bengio amos storkey. three factors inﬂuencing minima sgd. arxiv preprint arxiv. nitish shirish keskar dheevatsa mudigere jorge nocedal mikhail smelyanskiy ping peter tang. large-batch training deep learning generalization sharp minima. arxiv preprint arxiv. qianli liao kenji kawaguchi tomaso poggio. streaming normalization towards simpler biologically-plausible normalizations online recurrent learning. arxiv preprint arxiv. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research matthew zeiler sixin zhang yann fergus. regularization neural networks using dropconnect. proceedings international conference machine learning experiments caffe using cuda nvidia’s cudnn. experiments node cluster nvidia titan black gpus memory dual intel xeon cpus node utilized multi-gpu implementation caffe. code architectures replicate work available github.com/lnsmith/super-convergence. resnet- architecture used consists three stages. within stage residual block structure sequentially repeated. structure given table stages different residual block structure used reduce spatial dimension channels. table shows structure. overall architecture described table following caffe convention batch norm layer followed scaling layer achieve true batch normalization behavior. architectures necessary replicate work made available upon publication. main text showed results super-convergence cifar- dataset. fact super-convergence phenomenon also occurs cifar- implies independence phenomenon number classes. figure shows results range test resnet- cifar- training data. curve smooth accuracy remains high entire range indicating potential super-convergence. example super-convergence cifar- resnet- given figure also comparison results piecewise constant training regime. furthermore ﬁnal accuracy super-convergence discussed main text tested various adaptive learning rate methods resnet- training cifar- determine capable recognizing need using large learning rates. figure shows results training nesterov momentum adadelta adagrad adam found sign methods discovered utility large learning rates indication super-convergence-like behavior. also adaptive learning methods found nesterov adadelta adagrad allowed super-convergence occur unable create phenomenon adam. example figure shows comparison super-convergence piecewise constant training regime nesterov momentum method. super-convergence yields ﬁnal test accuracy iterations piecewise constant training regime iteration accuracy figure shows comparison runs super-convergence phenomenon without dropout. range test without dropout shown figure figure shows results training iterations. cases dropout ratio figure shows small improvement dropout. also values dropout ratio consistently similar improvements. effect mini-batch size discussed main paper present table containing ﬁnal accuracies super-convergence training various mini-batch sizes. table ﬁnal test accuracy results table shows larger mini-batch size better ﬁnal accuracy differs results shown literature. results reported paper total mini-batch size addition experiments resnet- cifar- modiﬁed values momentum weight decay determine might hinder super-convergence phenomenon. figure shows results momentum ﬁnal test accuracies listed table results indicate small change results setting better values. figure results weight decay values case weight decay value prevents super-convergence smaller values not. figure also shows weight decay value performs well. figure shows example range test bottleneck version resnet-. appears peak learning rate approximately even pronounced large swings accuracy around onward. oscillations decay accuracy learning rates greater indicate cannot train network large learning rates experiments conﬁrmed true. also tried cifar- resnext architecture unable produce super-convergence phenomenon. figure shows example range test resnext. result similar figure appears peak large swings accuracy learning rate approximately oscillations experiments densenets experiments imagenet dataset wide variety architectures failed produce super-convergence phenomenon. list experiments tried failed produce super-convergence behavior. super-convergence occur training imagenet dataset; imagenet experiments resnets resnext googlenet/inception alexnet densenet without success. architectures tried cifar- show super-convergence capability included resnext densenet bottleneck version resnet.", "year": 2017}