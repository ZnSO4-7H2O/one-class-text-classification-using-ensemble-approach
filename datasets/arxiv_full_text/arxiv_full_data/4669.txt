{"title": "Sketching and Neural Networks", "tag": ["cs.LG", "cs.AI"], "abstract": "High-dimensional sparse data present computational and statistical challenges for supervised learning. We propose compact linear sketches for reducing the dimensionality of the input, followed by a single layer neural network. We show that any sparse polynomial function can be computed, on nearly all sparse binary vectors, by a single layer neural network that takes a compact sketch of the vector as input. Consequently, when a set of sparse binary vectors is approximately separable using a sparse polynomial, there exists a single-layer neural network that takes a short sketch as input and correctly classifies nearly all the points. Previous work has proposed using sketches to reduce dimensionality while preserving the hypothesis class. However, the sketch size has an exponential dependence on the degree in the case of polynomial classifiers. In stark contrast, our approach of using improper learning, using a larger hypothesis class allows the sketch size to have a logarithmic dependence on the degree. Even in the linear case, our approach allows us to improve on the pesky $O({1}/{{\\gamma}^2})$ dependence of random projections, on the margin $\\gamma$. We empirically show that our approach leads to more compact neural networks than related methods such as feature hashing at equal or better performance.", "text": "high-dimensional sparse data present computational statistical challenges supervised learning. propose compact linear sketches reducing dimensionality input followed single layer neural network. show sparse polynomial function computed nearly sparse binary vectors single layer neural network takes compact sketch vector input. consequently sparse binary vectors approximately separable using sparse polynomial exists single-layer neural network takes short sketch input correctly classiﬁes nearly points. previous work proposed using sketches reduce dimensionality preserving hypothesis class. however sketch size exponential dependence degree case polynomial classiﬁers. stark contrast approach using improper learning using larger hypothesis class allows sketch size logarithmic dependence degree. even linear case approach allows improve pesky dependence random projections. empirically show approach leads compact neural networks related methods feature hashing equal better performance. many supervised learning problems input data high-dimensional sparse. high dimensionality inherent task example large vocabulary language model result creating hybrid conjunction features. applying standard supervised learning techniques datasets poses statistical computational challenges high dimensional inputs lead models large number parameters. example linear classiﬁer d-dimensional inputs weights linear multiclass predictor d-dimensional vectors weights class. case neural network nodes ﬁrst hidden layer parameters layer alone. large models often lead slow training inference require larger datasets ensure generalization error. reduce model size project data lower-dimensional space prior learning. proposed methods reducing dimensionality random projections hashing principal component analysis methods typically attempt project data preserves hypothesis class. eﬀective inherent limitations approaches. example data consists linearly separable unit vectors arriaga vempala show projecting data dimensions suﬃces preserve linear separability original data margin however large small margin work motivated question could fewer dimensions suﬃce? unfortunately shown dimensions needed order preserve linear separability even arbitrary embeddings would appear therefore answer resounding work show using improper learning allowing slightly larger hypothesis class allows positive answer. simplest case show linearly separable k-sparse inputs create o-dimensional sketch input guarantee neural network single hidden layer taking sketches input correctly classify fraction inputs. show simple non-linear function decode every binary feature input sketch. moreover function implemented applying simple non-linearity linear function. case inputs rectiﬁed linear unit commonly used neural networks. results also extend sparse polynomial functions. addition show achieved sketching matrices sparse. thus sketch eﬃcient compute increase number non-zero values input much. result contrast usual dense gaussian projections. fact sketches simple linear projections. using non-linear decoding operation allows improve previous work. present empirical evidence approach leads compact neural networks existing methods feature hashing gaussian random projections. leave open question sketches aﬀect diﬃculty learning networks. models many cases better accuracy. practical message stemming work compact sketch built multiple hash-sketches combination neural network suﬃcient learn small accurate model. random projections. random gaussian projections classical tool dimensionality reduction. general vectors johnson-lindenstrauss lemma implies random gaussian projection o/ε) dimensions preserves inner product pair unit vectors additive factor probability long line work sought sparser sketching. research streaming sketching algorithms addressed related questions. alon showed simple hashing based algorithm unbiased estimators euclidean norm streaming setting. charikar showed algorithm heavy-hitters problem based count sketch. relevant works count-min sketch cormode muthukrishnan projections learning. random projections used machine learning least since work arriaga vempala fast estimation certain class kernel functions sampling proposed dimensionality reduction technique propose using count-min sketch reduce dimensionality approximately preserving inner products sparse vectors. weinberger count-sketch unbiased estimator inner product sparse vectors prove strong concentration bounds. previously ganchev dredze shown empirically hashing eﬀective reducing model size without signiﬁcantly impacting performance. hashing also used vowpal wabbit talukdar cohen also count-min sketch graph-based semi-supervised learning. pham pagh showed count sketch tensor power vector could quickly computed without explicitly computing tensor power applied fast sketching polynomial kernels. sparse vectors results area e.g. imply k-sparse vector reconstructed w.h.p. projection dimension however knowledge known decoding algorithms parameters involve sequential adaptive decisions implementable depth neural network. recent work mousavi empirically explores using deep network decoding compressive sensing also considers learnt non-linear encodings adapt distribution inputs. parameter reduction deep learning. work viewed method reducing number parameters neural networks. neural networks become ubiquitous many machine learning applications including speech recognition computer vision language processing tasks notable examples). successes part enabled recent advances scaling deep networks leading models millions parameters however drawback large models slow train diﬃcult deploy mobile embedded devices memory power constraints. denil demonstrated signiﬁcant redundancies parameterization several deep learning architectures. reduce number parameters training low-rank decompositions weight matrices. cheng impose circulant matrix structure fully connected layers. caruana train shallow networks predict log-outputs large deep network hinton train small network match smoothed predictions complex deep network ensemble models. collines kohli encourage zero-weight connections using sparsity-inducing priors others techniques pruning weights. hashednets enforce parameter sharing random groups network parameters. contrast methods sketching involves applying sparse linear projection inputs require specialized learning procedure network architecture. reader hash functions pairwise independent hash families easily constructed using random bits. moreover hash evaluated using arithmetic operations o-sized words. figure schematic description neural-network sketching. sparse vector sketched sketch using hash functions shaded squares correspond sketching step random learned. sketch used input single-layer sparse linear functions bdk. denote sketch matrix bskh...ht satisfying conditions corollary argue exists network takes input outputs high probability hidden unit network corresponds index non-zero weight ﬁnal property implies using linear classiﬁer small generalization error long number examples least ω).. proved e.g. using standard compression arguments model represented using bits addition representation size similar bounds hold bounds weight coming unit. note even non-trivial input compression. implement decm need slightly non-conventional non-linearity. weight vector input vector gate implements minizi= zixi then using corollary following theorem. sketch ks-sparse vector binary vectors dimension known sketching techniques would construct sketch size practical techniques vowpal wabbit also construct cross features explicitly building exponential dependence. stark contrast neural networks allow away logarithmic dependence natural question arises whether parameters improved. show allow large scalars sketches construct deterministic -dimensional sketch shallow network reconstruct monomial. also show lower bound required dimensionality. words decoding average value indices ﬁrst suppose claim thus average decp olydk case claim implies every non-negative happens indices thus non-negative larger adding gives zero additional reduce sum. last theorem shows sketched arbitrary products variables decoded applying linear function followed relu. natural smallest dimension sketch exists. following theorem shows must least fact true even require decode single variables. theorem mapping every satisfying reluwi least proof. denote function class consisting functions form signwi bdk. hand sub-class class linear separators hence hand claim establishes proof. order prove claim suﬃces show shattered. indicator vector claim would like note endcoding decoding determinitic sketched computed eﬃciently dimension sketch smaller dimension random sketch. following corollaries. show expand hypothesis class even simplest settings linear classiﬁers -sparse vectors required dimensionality projection much larger dimension needed improper learning. result likely folklore present short proof completeness using concrete constants theorem proof below. shatters concretely hypothesis class consisting es}. sgnφ) sgnx points shattered subclass linear separators since linear separators dimension largest shattered larger thus collection vectors positive constant shown exist probabilistic argument standard constructions coding theory. linear separators deﬁned brevity denote sgnx) positions thus assumption expectation taken sκ/. renumber hj’s increasing order |ej| |eκ|. ej’s non-empty aj’s necessarily distinct. call lost deﬁnition sej. distance property implies ejej since ej’s increasing size follows lost |ej| thus expectation lost. follows choice distribution induces distinct subsets since dimension sauer-shelah lemma says short section show boolean inputs polynomial monomials represented neural network hidden layer hidden units. result simple improvement barron’s theorem special case sparse polynomial functions vectors. contrast barron’s theorem works arbitrary section evaluate sketches synthetically generated datasets task polynomial regression. experiments here assume input dimension input sparsity hypothesis support examples. assume subset features relevant regression task generate hypothesis select subsets relevant features cardinality generate gaussian distribution. generate binary feature vectors mixture relevant remaining indices generate target outputs results shown figure expected increasing number hash functions leads better performance. using hash functions size less input sparsity leads poor results increasing hash size beyond reasonable yields modest improvements. results shown figure case linear data neural network yields notably better performance linear model. suggests linear classiﬁers well-preserved projections projection size required linear separability large. applying neural network sketched data allows smaller projections. case polynomial regression neural networks applied sketches succeed learning small model achieve signiﬁcantly lower error network applied original features lead better generalization. linear model showing function well-approximated linear function. previous work hashing projections would imply using signiﬁcantly larger sketches setting. conclude section compared sketches gaussian random projections. generated sparse linear polynomial regression datasets settings before reduce dimensionality inputs using gaussian random projections one-layer neural networks table results demonstrate sketches yield lower error gaussian projections. note also gaussian projections dense hence much slower train. linear sparse polynomial classiﬁcation accuracy examples neural networks constructed compute linear polynomial function attain accuracy least −ε−δ examples. moreover number parameters network relatively small enforcing sparsity bounds weights hidden layers. thus generalization bounds negligible degradation respect nonsketched predictor. section evaluate sketches language processing classiﬁcation tasks described below. entity type tagging. entity type tagging task assigning labels mentions entities text. perform type tagging corpus documents containing mentions annotated labels features mention include surrounding words syntactic lexical patterns leading large dictionary. similarly previous work string feature integer reduce dimensionality using hashing sketches. details features labels task. reuters-news topic classiﬁcation. reuters data consists collection approximately text articles assigned multiple labels. highlevel categories economics commerce medical government multiple speciﬁc categories. focus training binary classiﬁers four major categories. input features binary unigram features. post word-stemming data approximately dimensions. feature vectors sparse however examples fewer non-zero features. news topic classiﬁcation. perform topic classiﬁcation articles news corpus labeled news categories business entertainment health sci/tech sports europe u.s. world. document extract binary word indicator features title description; total unique features average non-zero features document. figure score number non-zero parameters ﬁrst layer entity type tagging. color corresponds diﬀerent sketch size markers indicate number subsketches experiments two-layer feed-forward networks relu experimental setup. activations hidden units layer. softmax output multiclass classiﬁcation multiple binary logistic outputs multilabel tasks. experimented input sizes reduced dimensionality original features using sketches blocks. addition experimented networks trained original features. encouraged parameter sparsity ﬁrst layer using -norm regularization learn parameters using proximal stochastic gradient method. before trained examples evaluated remaining report accuracy values multiclass classiﬁcation score multilabel tasks true positive false positive false negative counts accumulated across labels. results. since motivation work reducing number parameters neural network models plot performance metrics versus number non-zero parameters ﬁrst layer network. results shown figures diﬀerent sketching conﬁgurations settings -norm regularization parameters entity type tagging task compared sketches single hash function size number original features large. case sketching allows improve performance reduce number parameters. reuters task sketches achieve similar performance original features fewer parameters. news sketching results compact models modest drop accuracy. almost cases multiple hash functions yield higher accuracy single hash function similar model size. figure score number non-zero parameters ﬁrst layer reuters topic classiﬁcation. color corresponds diﬀerent sketch size markers indicate number subsketches presented simple sketching algorithm sparse boolean inputs succeeds signiﬁcantly reducing dimensionality inputs. single-layer neural network sketch provably model sparse linear polynomial function original input. k-sparse results viewed showing compressed sensing scheme vectors decoding algorithm depth- neural network. scheme requires measurements leave open question whether improved demonstrated empirically sketches work well linear polynomial regression using neural network improve direct linear regression. show real datasets methods lead smaller models similar better accuracy multiclass multilabel classiﬁcation problems. addition compact sketches lead fewer trainable parameters faster training. figure news topic classiﬁcation accuracy number non-zero parameters ﬁrst layer. color corresponds diﬀerent sketch size markers indicate number subsketches", "year": 2016}