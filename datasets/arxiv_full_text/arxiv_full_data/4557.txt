{"title": "Decision Making Agent Searching for Markov Models in Near-Deterministic  World", "tag": ["cs.AI", "cs.LG"], "abstract": "Reinforcement learning has solid foundations, but becomes inefficient in partially observed (non-Markovian) environments. Thus, a learning agent -born with a representation and a policy- might wish to investigate to what extent the Markov property holds. We propose a learning architecture that utilizes combinatorial policy optimization to overcome non-Markovity and to develop efficient behaviors, which are easy to inherit, tests the Markov property of the behavioral states, and corrects against non-Markovity by running a deterministic factored Finite State Model, which can be learned. We illustrate the properties of architecture in the near deterministic Ms. Pac-Man game. We analyze the architecture from the point of view of evolutionary, individual, and social learning.", "text": "abstract. reinforcement learning solid foundations becomes inefﬁcient partially observed environments. thus learning agent born representation policy might wish investigate extent markov property holds. propose learning architecture utilizes combinatorial policy optimization overcome non-markovity develop efﬁcient behaviors easy inherit tests markov property behavioral states corrects non-markovity running deterministic factored finite state model learned. illustrate properties architecture near deterministic pac-man game. analyze architecture point view evolutionary individual social learning. concerned real world scenarios decision making arbitrarily poor available sensory information insufﬁcient older still ongoing implicating events taken account. past information improve decision making state description markovian problem belongs realm markov decision processes solid theoretical foundations. remaining question mdps whether description memory whether optimization solved reasonable time not. size problem grows exponentially number variables. therefore order reduce problem size state description compressed extracting relevant features decision. result factored reinforcement learning obtained raises question whether compression spared markov property not. interested scenarios world close-to-deterministic. intriguingly unless refer quantum mechanics well-informed laplace’s demon make decent close-to-deterministic approximations surprisingly close-todeterministic nature hold great extent everyday human activities regards human cooperation strong correlation ability reading hidden emotional cognitive ‘parameters’ partners collaborative skills indicating necessity markovian state description ehavior optimization. last least formalism tacitly includes concept determinism states perfectly known agent within ﬁnite time window. turn concept state hides close-to-deterministic ﬁnite time processes mdp. fig. learning continuously changing world. questions changing world methods overcome non-markovian character state description architecture changing world agent born relatively small low-complexity rules selected evolution. temporal difference error uncover non-markovian character behavioral states. decision surfaces based learning diminish non-markovian character. lookahead method valuable factored close-to-deterministic ﬁnite state model environment learned. markovian description environment questionable since inherited rules representations generated observations non-markovian world change markovian property lost agents change changes agents’ behavior turn interested learning agents. restrict considerations environment approximated close-to-deterministic finite-state machines factored fsms practical points so-called factored model tractable point view optimization near-deterministic processes relatively easy learn. address following issues here problem description possess markov-property? feasible solve underlying learning problem? answers negative features learned improve problem description? questions illustrated fig. figure shows certain opportunities checking markov property without increasing temporal depth. occurs considering difference experienced expected approximations longterm cumulated discounted rewards; temporal difference errors. figure depicted optimization non-markovian state description direct global policy search well method overcome traps posed non-markovian states look-ahead based value estimation. address delicate issue pulling close-to-deterministic features/factors high dimensional uncertain observations might main bottleneck artiﬁcial general intelligence instead demonstrate policy optimization lead good performance non-markovian behavioral states look-ahead using close-to-deterministic factors quickly overcome poor value estimations non-markovian states used improve decision making. figure represents main concepts learning agent changing world assume agent born low-complexity rule save her. rule developed e.g. evolution selective global policy optimization. rule however lead non-markovian states rules crude match actual world. agent starts learn estimating values behavioral states collects information distribution errors states. using errors agent develop binary sensors order improve state description decision making. also agent learn close-to-deterministic ffsms overcome problems value estimations non-markovian states utilizing look-ahead methods since lookahead efﬁcient close-to-deterministic ffsms look-ahead easily correct poor value estimations. paper built follows markovity related concepts reviewed section executed experimental demonstrations pac-man game order illustrate concepts fig. pac-man chosen previous experiences paper considerations restricted single pac-man case however proper sensors available enable close-to-deterministic ffsm characterization agent methods value multi-agent case results experiments discussed section connect work theoretically proven favorable features factored leave aside problem learning model environment. elaborate concept state subsection conclude section last decades reinforcement learning reached mature state laid solid foundations. large variety algorithms including valuefunction-based direct policy search hybrid methods. reviews subjects e.g. references therein. deals sequential decision tasks decisions agent based information locally optimal actions strategy rather relying scalar reward associated state transitions. formalism encapsulates task follows markov decision process comprise quadruple ﬁnite states ﬁnite actions state transition probability telling probability arriving state executing action state reward function immediate reward transition behavior agent described policy function assigns probability action state. given policy state associated value expectation value long-term cumulated discounted immediate reward discount factor rt+k+ immediate reward step denotes expectation operator policy goal agent optimal policy chooses actions giving rise largest values state. dynamic programming available iterative method comprised improvement greediﬁcation steps used obtain optimal policy. approach fall short large model available. many real problems resist e.g. state description non-markovian number variables describe state large since size state space grows exponentially number variables. solutions problem considered below. policy search method lack markovian property circumvented extent global policy search methods rely value function rather explicit parametric representation policies maintained updated according measure rewards acquired trial episodes. shown theoretically cross-entropy method efﬁcient policy search furthermore deal non-markovian state-descriptions optimization multiple actions acting simultaneously enabling optimization factored action representations works follows discrete vector so-called goal ﬁtness function deﬁnes ﬁtness particular solution vector goal vector highest corresponding ﬁtness value maintains distribution family parametric distributions possible solutions converges towards distribution family whose density highest arbitrary environment optimal solution algorithm iterative approach marks subsequent iterations high values uniform distribution level concentrated around global optimum optimization adjusted adaptively beginning highly likely sample empty high values. speciﬁes ratio describes number items level therefore determines actual value. best items called elite samples typically comprise samples. samples ordered descending ﬁtness value value calculated function approximation stateaction-space large continuous function approximation methods also involved create value functions approximation sample points using supervised learning methods controlled case fapps goal create explicit representation approximated value functions parameters typically updated consecutive step. convergent general algorithms references therein. special fapps temporal dynamics also used diminish consequences non-markovian observations size |si| case factored markov decision process naive tabular representation transition probabilities would require exponentially large space however next-step value state variable often depends variables full transition probability obtained product several simpler factors. shown uniformly sampling polynomially many samples state space complexity fmdp solver stays polynomial size fmdp description length i.e. |si| convergent fmdp solver makes function approximations modules concept time step ﬂexible mdps. particular solution deﬁnes time intervals actions state described available modules i.e. complex actions used given time. state changes available modules changes deﬁnes state whereas activated module deﬁnes action state formulation advantageous reasons. formulation easily extended robust controllers addition formulation enables activate module macro time giving rise combinatorial ﬂexibility factored descriptions behavioral states upon optimization shall later. model construction planning learning transition probability matrix dynamical model offers several advantages. first model enables planning. useful value estimation imprecise example rewards disappear rewards appear. case learned model like dynamic programming. typical usage model computes expected value state summing discounted future rewards according dynamical model policy. advantages appear real world experimentation limited rewards certain transition probabilities factors change learned easily. upon observation changes dynamical model update value estimation without real world experiments. factored finite state model approximation planning planning pacman game severely restricted size state space. hand state transition easily described factors. example pac-man walks disappears. learning ‘machine’ simple. walls corridors game also simple ‘machines’ since change. other somewhat complex machines game; probabilistic behavior. components simple learn thus approximate sampling based risk avoiding deterministic model pac-man cost memory demand sake look-ahead note look-ahead typical games including pac-man also considered literature references therein). here consider look-ahead point view general architecture inherits knowledge learns faces challenges. many methods solve identiﬁcation task provided close-to-deterministic features available. section illustrate logic algorithmic components shown fig. assume reinforcement learning scenario possesses large actionand/or state-space method known efﬁciently compress information feature extraction preserving markov-property description. pac.man game illustration. player maneuvers pac-man maze pac-man eats dots maze. particular maze dots worth points. level ﬁnished dots eaten. also four ghosts maze catch pac-man succeed pac-man loses life. initially three lives gets extra life reaching points. four power-up items corners maze called power dots pac-man eats power ghosts turn blue short period slow escape pac-man. time pac-man able them worth points consecutively. point values reset time another power eaten player would want four ghosts power dot. ghost eaten remains hurry back center maze ghost reborn. certain intervals fruit appears near center maze remains while. eating fruit worth points. original version pac-man ghosts move complex deterministic route possible learn deterministic action sequence require observations. pac-man randomness added movement ghosts. single optimal action sequence observations necessary optimal decision making. used implementation available internet full description state would include whether dots eaten position direction pac-man position direction four ghosts whether ghosts blue long remain blue whether fruit present time left appears/disappears number lives left. size resulting state space huge kind function approximation feature-extraction necessary action space much smaller four basic actions north/south/east/west. however typical game consists multiple hundreds steps number possible combinations still enormous. indicates need temporally extended actions. provide mid-level domain knowledge algorithm domain knowledge preprocess state information deﬁne action modules. hand role policy search reinforcement learning combine observations modules rule-based policies proper combination. pac-man attractive illustration environment alike many games life-like characteristics agent solve interrelated tasks identify interconnected behaviors changing environment. interrelated tasks separable mutual effects other. purpose demonstration assemble common sense architecture utilizing certain techniques. shall argue later feature extraction main bottleneck architecture shall consider kind features advantageous rule-based policy rules mechanism breaking ties i.e. decide rule executed multiple rules satisﬁed conditions. actions last applied combinations. reproduced actions examples like todot towards nearest topowerdot towards nearest power frompowerdot direction opposite nearest power toedghost towards nearest edible ghost fromghost direction opposite nearest ghost nearestdot distance nearest nearestpowerdot distance nearest power nearestghost distance nearest ghost nearestedghost distance nearest edible ghost. makes potential rule. rule priority assigned. agent make decision checks rule list starting ones highest priority. conditions rule fulﬁlled corresponding action executed decision-making process halts. rule priority switches action module priority action module also taken intuitively makes sense important rule activated effect also important. rule priority switches module executed regardless priority module. procedure depicted fig. rules pre-wired created randomly global search method select good ones assign priorities thus learning concerned combining different conditions actions form rules deciding priorities rules. policy optimization accomplished cross-entropy method. macros simultaneously used rule combinations. optimization procedure gives rise ‘low-complexity policy’ many rules policy rules concurrently active time. effective length policies biased towards short policies. frequent low-complexity rule combinations used building blocks search powerful still low-complexity policies. result i.e. optimized pac-man policy seen action selected certain conditions. details rules score global policy optimization gave rise good performance used ﬁrst level average score average life possible lives. maximum achievable score ·+·+· pac-man able ghosts eating power dot. score follows could ghosts consuming power dot. behavioral states macros correspond group simultaneously active rules interpreted behavioral states behavioral patterns action equivalent application actions macro. little macros emerged fig. decision-making pac-man agent. time step receives actual observations state action modules. checks rules priority list order executes ﬁrst rule satisﬁed conditions. plus sign means action module enabled. table behavioral states macros codes values. column number behavioral states. column individual digits denote action ‘on’ ‘off’. meaning digits left right fromghost+ fromghost- topowdot toedghosts frompowdot constant> tonearestpill+. last digit code applies conditions. column pac-man scores optimization actions time behavioral state effective. averages runs. bold italic optimization actions macro increased score policy greediﬁcation possible. column estimated values inherited behavioral states upon optimization actions. averages runs. column description behavioral states. since table small easy pre-wire rules. since probability based selective global optimization method consider model evolution here. pre-wired rules respective priorities give good chances agent survive collect additional information improve performance individual learning. note thought experiments also apply dangerous experimentation. macros often occur consecutively could joined macro sequences. formation macro sequences means higher level abstraction basically incarnates behavior. example phenomenon observed zig-zagging behavioral pattern pac-man agent performs turns towards power step many potential choices selected rule second priority level ghost density power pill close move away power pill. rule rule considered edible ghost move towards power pill. rules priority level achieve good performance since give rise zig-zagging behavior close power pill ghost density small. looks like pac-man waiting ghosts power approach order better chance catch afterwards. emerging behavior since rule ‘waiting’. unrelated foreseeing ghosts come closer future simply result selection global policy optimization. zig-zagging also gives rise trap shown fig. proper conditions zig-zagging behavior could turn zigzagging zig-zagging could macro. repetitive behavioral pattern easily identiﬁed execution easy compress module undergo optimization following consider individual learning small number episodes i.e. game. computing values states inspecting error histogram building predictors decision surfaces large errors thus increasing ‘state space’. then using states using rule optimization improve performance. fig. lack look-ahead. clyde chooses random move moves right instead chasing pac-man clyde switches behavior approaches pac.man comes form troubled direction crosses power catch pac-man pac-man could escape chases edible ghost falls trap closed pinky. note ﬁrst option develops behavior state without using additional information. method help since extends range priorities introducing priority within ‘state’. options take advantage additional information collected game. consider three options below. subtask optimization optimization individual macro states give rise novel behaviors collect rewards state. executed program could increase collected rewards state pac-man game. markov condition satisﬁed greedy replacement behavior improve performance. however warranty improvement markov condition satisﬁed. third column table shows average performance greedy behavior optimization state. since average performance equals points performance improves changing behaviors state state states. example optimizing behavior state poor performance upon greedy policy optimization. subtask optimization suit individual learning well since applies thus slow. also dangerous since overall performance easily drop even optimization behavior successful. decision surface using errors estimated values behavioral states computed histogram errors large errors occur made visible excluding frequent error determine behavioral success consider state example. average reward collected state cumulated dots ghosts. largest peak around i.e. case pac-man collected single ghost dots. second peak around broader. corresponds eating ghosts dots. third fourth peaks around meaning pac-man could collect three four ghosts respectively. would ideal pac-man avoid errors collect four ghosts. build classiﬁer purpose collecting situations four ghosts captured situations. classiﬁer forecasts reward deﬁnes problem either wins looses. applied optimization actions starting number situations collected game. procedure special form goal-related feature extraction. solved methods execute situation corresponds categorical perception references therein) favored factored description available. come back point later. argue searching close-to-deterministic factors applying factored approximation together look-ahead ﬂexible. world change considerable part knowledge encode decision surface lost world changes. many variables world changes spoil decisions. note human recognition utilizes component based recognition decision category involves presence absence components categorical perception example based learning sensory observation changed around decision surfaces latter version learning appears component based recognition useful. component based recognition seems easier. example consider face recognition. hierarchical structure components i.e. components face change environment changes e.g. light condition changes. furthermore component based recognition works also occlusion. labyrinth actual number position type ghosts make difference. sensitivity small changes individual variables combinations variables compensated enormous number training examples becomes prohibitively expensive. whereas component based recognition suits high dimensional problems typically easy learn categorical perception hard learn. categorical perception appears mostly problems inherent dimension low. examples include decisions female male faces borders colors however easy learn behavior components. example brick wall component pac-man game. bricks coordinates move. dots different also coordinates disappear pac-man crosses them. power dots similar also change behavior ghosts. ghosts either escaping chasing pac-man. state transitions simple chasing behavior randomness. fruits appear randomly close-to-deterministic trajectories upon observation decision could made fruit trajectory intercepted not. thus sophistication pac-man game comes conﬁgurational diversity whereas behavior components simple sometimes deterministic easy learn learning dynamics components enables factored description produces factored world model branching ration produce potential conﬁgurations. changes world like different labyrinth different ghost speed easy observe learn. finally since branching ratio models easy used look-ahead commonly used including games here implemented non-perfect ffsm model model game. lookahead used game model implemented trajectories trajectory contained future state rewards collected future state. trajectories enumerated using depth-ﬁrst search strategy applied state graph original model. look-ahead model approximates world closely improve policy starting state discount factor estimated value make relatively small contribution value estimation estimated value future states closer game exhibit smaller estimation errors. bypassed learning ffsm simple pac-man would misleading. believe pulling close-to-deterministic features relevant piece learning problem. problem treated elsewhere used deterministic model reduced branching factor since stochastic event would require branch look-ahead tree. approximations nonedible ghosts always chasing pac-man edible ghosts ﬂeeing away pac-man pac-man stop positions used perform zig-zagging. illustration used single level game. optimization algorithm make information life lost pac-man could make sudden transitions certain cases. score life improved considerably lookahead increased factor turn lookahead helped pac-man escape traps. overall score improved relatively high lookahead value. score increased considerably ghosts catched eating single power deep look-ahead needed judge this. interesting interplay categorical perception look-ahead since decision surface work well relevant variable ghost density single dimension example based learning could work here. learned decision surface would thus implicitly measure ghost density could used situations too. power high zig-zagging behavior emerged. however lookahead trusts value estimation overrides zig-zagging decreases score. deeper look-ahead overcomes problem. summarize main features architecture relate algorithms consider learning demands. certain cognitive aspects also mentioned. analyze architecture point view evolutionary individual social learning. emphasize ffsm easy learn pac-man game simple run. memory computer time demand look-ahead depends branching ratio size state space. ffsm however works space factored variables considerably reduce exponent size state space given example feature extraction hard machine learning algorithm ﬁgure high level concepts like ghost density make correct decision power instead escape order attract ghosts another power dot. extraction low-dimensional observations features unsolved problem machine learning general case. ghost density illuminating example argued error used learn decision surface decision reﬂect ghost density. parameter ‘ghost density’ could used scenarios too. however general case unclear separate factor power particular structure labyrinth decision made. modules modules spatial-temporal entities. consider playing volleyball scenario multiple modules accompanying activities catching ball running involves body modules. pac-man simple point view consider problems like decoupling catching ball running. problems however pose considerable challenge applications. body modules properly decoupled look-ahead take advantage factored description domains. example above written rule run+ catchtheball+ passtheball+. analog example pac-man doton+ isteponit+ doton-. order illustrate problems missing information showed examples fig. situation pac-man waiting wrong side power dot. case pac-man chasing edible ghost surrounded non-edible ones. human observer simple cases. situation becomes different also human observers odor power odor ghosts available. note poor sensory systems temporal depth could hardly help whereas error would clearly indicate occasions largely differ. additional information needed build ffsm order look-ahead. ffsm based description number advantages ffsm model zeroth approximation enriched e.g. stochastic aspects included information becomes available. also ffsm model offers considerable improvements moderate computer time requirements. example changes labyrinth incorporated fsms like doors open closed included. ffsm becomes costly branching ratio large. case sampling methods used look-ahead becomes time-consuming making look-ahead useless time constraints. furthermore look-ahead work partially observed worlds value estimation poor. cognitive aspects include certain forms autism. autism sensing ‘partner’s mind’ sometimes spoiled; emotion detection emotion understanding weak. then lack information emotional state intentions partner makes computational costs high since branching ratio becomes high. time estimation rewards spoiled circumstances. ffsm description advantageous event-learning formalism described event-learning optimization decides desired next state creating subproblem task. subproblem given backing controller tries solve beyond advantage desires factored event-learning supports ﬂexible deﬁnition time steps utilize backing robust controllers making crudely learned inverse dynamics thus module description used here. state description list available modules choice action choice module ﬁrst described choice single module extended choice low-complexity modules macros means slow evolutionlike globally optimizing policy search cem. low-complexity macros might serve since factored. states deﬁned macros markovian large problems optimized since learning time scale polynomially sampling used advantage appears model i.e. transition probability matrix learned since exploration– exploitation dilemma resolved regards macros found experimentally number optimized macro states small states non-markovian. non-markovity early detected measuring distribution errors state easy. optimize temporally extended macro states applying policy optimization time macro used followed greediﬁcation. extension state description temporal depth feasible general since temporal depth comes exponent size state space. example pac-man huge temporal depths required improve cumulated rewards hand ffsm approximations easy learn look-ahead comes cheap ffsm models even crude approximations improve performance. look-ahead also improve value estimations since works computing immediate rewards adds estimated values inherited macro states. inherited values actual world since estimates rewards discounts effect inherited policy. look-ahead thus changes behavior. however inherited rules related behaviors reappear aging working memory degrades since look-ahead relies working memory. work needed provide solid mathematical framework lookahead extended approximately deterministic ffsm description le-ffsm promising since look-ahead diminish non-markovity factored methods attractive scaling properties apply. however feature extraction problem suits le-ffsm largely unsolved. neurally motivated hierarchical exact matrix completion spatio-temporal representations seem appealing respect. evolutionary learning selective globally optimizing method gives rise low-complexity rules i.e. macros behavioral states. result evolutionary learning simple easy pre-wire. individual learning learns errors optimizes modules e.g. tunes thresholds rules computes values errors macro-states increase state space decision surfaces learns factored applies look-ahead predict behavior learns control individual ffsms learn inverse dynamics apply robust controller improve control solve subtasks. social learning requires state agents including value estimations sensed agents become ffsms model. lack knowledge however means stochastic agents hidden variables enter optimization problem. tasks hard value estimation become highly imprecise prohibit collaboration social learning especially since agents stationary learn close-to-deterministic factored ﬁnite-state machine description seem attractive mdps since laplace’s demon good initial approximation world different reasons ffsms tractable point view problem size reinforcement learning. however extraction close-to-deterministic variables unsolved problem. low-complexity temporally extended close-todeterministic factors satisfy concept states mdps large extent. environment well agent’s action switch factors would correspond state-action-state transitions. controlled optimal switching task mdps concept event-learning considered spatio-temporal modules paper. showed make transition modular description state description states markovian not. treated different options achieving markovian property suggested learning factored since suits look-ahead based improvement value estimation. also argued learning factors pac-man problem happens easy noted general problem unsolved machine learning. introduced look-ahead architecture means deterministic approximate ffsm conjectured factored models ffsm look-ahead many desirable features including close markovian description polynomial scaling diminishing exploration-exploitation dilemma model learning. considered architecture point view evolutionary individual social learning tried separate aspects long-standing problem. shown give rise simple low-complexity policies easy inherit. particular property individual learning construct ffsm look-ahead order diminish errors value estimation non-markovian characteristics inherited macro states. social learning framework depends sensory information; values errors partners sensed ffsm description sufﬁcient collaborative planning eased considerably. thanks ´akos bontovics m´ark farkas help support course work. european union european social fund provided ﬁnancial support project grant agreement ´amop .../b/kmr--.", "year": 2011}