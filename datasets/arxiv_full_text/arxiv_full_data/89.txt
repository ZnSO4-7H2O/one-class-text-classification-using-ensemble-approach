{"title": "Improving speech recognition by revising gated recurrent units", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Speech recognition is largely taking advantage of deep learning, showing that substantial benefits can be obtained by modern Recurrent Neural Networks (RNNs). The most popular RNNs are Long Short-Term Memory (LSTMs), which typically reach state-of-the-art performance in many tasks thanks to their ability to learn long-term dependencies and robustness to vanishing gradients. Nevertheless, LSTMs have a rather complex design with three multiplicative gates, that might impair their efficient implementation. An attempt to simplify LSTMs has recently led to Gated Recurrent Units (GRUs), which are based on just two multiplicative gates.  This paper builds on these efforts by further revising GRUs and proposing a simplified architecture potentially more suitable for speech recognition. The contribution of this work is two-fold. First, we suggest to remove the reset gate in the GRU design, resulting in a more efficient single-gate architecture. Second, we propose to replace tanh with ReLU activations in the state update equations. Results show that, in our implementation, the revised architecture reduces the per-epoch training time with more than 30% and consistently improves recognition performance across different tasks, input features, and noisy conditions when compared to a standard GRU.", "text": "training rnns however complicated vanishing exploding gradients might impair learning longterm dependencies although exploding gradients effectively tackled simple clipping strategies vanishing gradient problem requires special architectures properly addressed. common approach relies socalled gated rnns whose core idea introduce gating mechanism better controlling information various time-steps. within family architectures vanishing gradient issues mitigated creating effective shortcuts gradients bypass multiple temporal steps. popular gated rnns long short-term memory networks often achieve state-of-theart performance several machine learning tasks. lstms rely network design consisting memory cells controlled forget input output gates. despite effectiveness sophisticated gating mechanism might result overly complex model tricky implement efﬁciently. hand computational efﬁciency crucial issue rnns considerable research efforts recently devoted development alternative architectures purpose attempt simplify lstms novel model called gated recurrent unit based multiplicative gates. despite adoption simpliﬁed gating mechanism works literature agree grus lstms provide comparable performance different machine learning tasks work continues efforts revising grus proposing architecture potentially suitable speech recognition. contribution paper twofold first propose remove reset gate network design. similarly found removing reset gate affect system performance since observed certain redundancy role played update reset gates. second propose replace hyperbolic tangent rectiﬁed linear units activations state update equation. past non-linearity avoided rnns numerical instabilities caused unboundedness relu activations. however coupling relu-based architecture batch normalization experience numerical issues. allows take advantage relu neurons proven effective alleviating vanishing gradient problem well speeding network training. results obtained different tasks input features noisy conditions show that implementation revised architecture reduces per-epoch training wall-clock time improving recognition performance experimental conditions considered work. speech recognition largely taking advantage deep learning showing substantial beneﬁts obtained modern recurrent neural networks popular rnns long short-term memory typically reach state-of-the-art performance many tasks thanks ability learn long-term dependencies robustness vanishing gradients. nevertheless lstms rather complex design three multiplicative gates might impair efﬁcient implementation. attempt simplify lstms recently gated recurrent units based multiplicative gates. paper builds efforts revising grus proposing simpliﬁed architecture potentially suitable speech recognition. contribution work two-fold. first suggest remove reset gate design resulting efﬁcient single-gate architecture. second propose replace tanh relu activations state update equations. results show that implementation revised architecture reduces per-epoch training time consistently improves recognition performance across different tasks input features noisy conditions compared standard gru. index terms speech recognition deep learning recurrent neural networks lstm building machines able recognize speech represents fundamental step towards ﬂexible natural human-machine interfaces. primary role improving technology played deep learning recently contributed signiﬁcantly outperform previous gmm/hmm speech recognizers last years deep learning rapidly evolving progressively offering powerful robust techniques including effective regularization methods improved optimization algorithms well better network architectures. early deep learning works speech recognition mainly based standard multilayer perceptrons recent systems beneﬁt advanced architectures convolutional neural networks time delay neural network nevertheless since speech inherently sequential signal would natural address recurrent neural networks potentially able properly capture long-term dependencies. several works already highlighted effectiveness rnns various speech processing tasks speech recognition speech enhancement speech separation well speech activity detection recent results newborn ﬁeld end-to-end speech recognition also shown rnns promising candidates replacing traditional hidden particular vectors corresponding update reset gates respectively represents state vector current time frame activations gates element-wise logistic sigmoid functions constrain take values ranging gate decides much units update activations. note linear interpolation similar adopted lstms component learning long-term dependencies. instance close previous state kept unaltered remain unchanged arbitrary number time steps. hand close zero network tends favor candidate state depends heavily current input reset gate useful signiﬁcant discontinuities occur sequence. language modeling happen moving text another notsemantically related. situations convenient reset stored memory order avoid taking decision biased uncorrelated history. however speciﬁc tasks functionality might useful. instance removing model single-gate architecture called minimal gated recurrent unit achieves performance comparable obtained standard grus handwritten digit recognition well sentiment classiﬁcation task. argue role played reset gate reconsidered also acoustic modeling speech recognition. fact speech signal sequence evolves rather slowly past history virtually always helpful. even presence strong discontinuities instance observable boundary vowel fricative completely resetting past memory harmful. hand helpful memorize phonotactic features since phone transitions likely others. moreover believe certain redundancy activations reset update gates might occur processing speech sequences. instance necessary give importance current information model small values similar effect also achieved update gate only setting small values latter sosired depends heavily current input recent history. similarly high value assigned either order place importance past states. redundancy also highlighted fig. temporal correlation average activations update reset gates readily appreciated trained timit. ﬁrst modiﬁcation standard grus proposed work thus concerns removal reset gate helps limiting redundancy gating mechanism. main beneﬁts intervention related improved computational efﬁciency achieved thanks reduced number parameters necessary reach performance standard gru. second modiﬁcation consists replacing standard hyperbolic tangent relu activations state update equations tanh activations indeed rather critical since saturation slows training process causes vanishing gradient issues. adoption relu-based neurons shown effective improving limitations common past rnns numerical instabilities originated unbounded relu functions applied long time series. nevertheless recent works shown relu rnns effectively trained proper orthogonal initialization batch normalization recently proposed machine learning community addresses so-called internal covariate shift problem normalizing training minibatch mean variance layer pre-activations. technique shown crucial improve system performance speed-up training procedure. batch normalization applied rnns different ways. authors suggest apply feed-forward connections only normalization step extended recurrent connections using separate statistics time-step. work tried approaches observed comparable performance them. also noticed coupling proposed model batch-normalization helps avoiding numerical issues often occur dealing relu rnns applied long time sequences. batch normalization indeed rescales neuron preactivations inherently bounding values relu neurons. allows network take advantage wellknown beneﬁts activations. provide accurate evaluation proposed architecture experimental validation conducted using different training datasets tasks environmental conditions. ﬁrst experiments timit performed test proposed model close-talking scenario. experiments timit based standard phoneme recognition task aligned proposed kaldi recipe experiments also conducted distant-talking scenario wall street journal task validate model realistic situation. reference context domestic environment characterized presence nonstationary noise acoustic reverberation original training dataset contaminated impulse responses measured real apartment. test phase carried dirhaenglish corpus consisting sentences uttered native american speakers apartment. development sentences uttered different speakers also used hyperparameter tuning. details dataset impulse responses found architecture adopted experiments consisted multiple recurrent layers stacked together prior ﬁnal softmax context-dependent classiﬁer. recurrent layers bidirectional rnns obtained concatenating forward hidden states backward hidden states recurrent dropout used regularization technique. since extending standard dropout recurrent connections hinders learning long-term dependencies followed approach introduced tackles issue sharing dropout mask across time steps. moreover batch normalization adopted exploiting method suggested discussed sec. feed-forward connections architecture initialized according glorot initialization recurrent weights initialized orthogonal initialization similarly gain factor batch normalization initialized shift parameter initialized training sentences sorted ascending order according lengths starting shortest utterances minibatches sentences progressively processed training algorithm. sorting approach besides minimizing zero-paddings forming mini-batches exploits curriculum learning strategy shown improve performance ensure numerical stability gradients. optimization done using adam algorithm epochs. performance development monitored epoch learning rate halved performance improvement went certain threshold. gradient truncation applied allowing system learn arbitrarily long time dependences. speech recognition labels derived performing forced alignment procedure original training datasets. standard recipe kaldi details evaluate proposed architecture different input features three sets experiments performed mfccs mel-ﬁlterbank coefﬁcients well fmllr features derived speaker adaptive training procedure feature vectors computed every frame length main hyperparameters model optimized development data. particular guessed initial values according experience starting performed grid search progressively explore better conﬁgurations. total experiments conducted models. result initial learning rate dropout factor chosen experiments. optimal numbers hidden layers hidden neurons instead depend considered dataset/model range hidden layers neurons. following sub-sections comparison proposed architecture popular rnns presented. results reported standard close-talking timit dataset distant-talking task based dirha english dataset. table presents results obtained timit dataset. perform accurate comparison various architectures least experiments varying initialization seeds conducted model. results table reported average phone error rates corresponding standard deviations. ﬁrst table presents results traditional relu activations although architecture recently shown promising results machine learning tasks speech recognition performance results conﬁrm gated recurrent networks still outperform traditional rnns. also observed grus tend slightly outperform lstm tasks addressed experiments. m-gru architecture version without reset gate. table highlights m-gru achieves performance similar obtained standard grus conﬁrming speculation negligible role played reset gate speech recognition application. last table reports performance achieved proposed model which besides removing reset gate relu activations used. results indicate m-relugru consistently achieves best performance across various input features. remarkable achievement average obtained proposed architecture using fmllr features. best knowledge result yields best published performance timit test-set. table reports comparison per-epoch training time considered rnns. results conﬁrm main advantage proposed model ability signiﬁcantly reduce training time. theano implementation running epoch lasts seconds nvidia seconds taken m-relugru training time reduction table also reports best architectures obtained optimizing hyperparameters timit development set. results show models best performance achieved hidden layers neurons. tables summarize results obtained simulated real parts dirha english dataset. sake compactness average performance reported standard deviations omitted. table exhibit trend comparable observed timit dataset conﬁrming effectiveness proposed architecture also realistic challenging scenario. results consistent real simulated data well across different features considered study. reduction training time moreover reset gate removal seems play crucial role addressed distant-talking scenario. closetalking results reported table highlight comparable error rates standard m-gru distant-talking case even observe performance gain removing reset gate. suppose behaviour reverberation implicitly introduces redundancy signal multiple delayed replicas sample. results forward memory effect smooths energy envelopes well sub-band energy contours. effect reset gate mechanism might become ineffective forget past. paper revised standard grus speech recognition purposes. proposed architecture simpliﬁed version reset gate removed relu activations used. experiments conducted different tasks features environmental conditions conﬁrmed effectiveness proposed model reduce computational complexity implementation also slightly improve recognition performance. future efforts focused extending work different tasks larger datasets test revised architecture modern endto-end systems ensure practical value computational gains models experiments replicated using optimized implementations. dahl deng acero context-dependent pretrained deep neural networks large vocabulary speech recognition ieee transactions audio speech language processing vol. abdel-hamid mohamed jiang deng penn convolutional neural networks speech recognition ieee/acm transactions audio speech language processing vol. waibel hanazawa hinton shikano lang phoneme recognition using time-delay neural networks ieee transactions acoustics speech signal processing vol. chen watanabe erdogan hershey speech enhancement recognition using multi-task learning long short-term memory recurrent neural networks proc. interspeech menne heymann alexandridis irie zeyer kitza golik kulikov drude schl¨uter haeb-umbach mouchtaris rwth/upb/forth system combination chime challenge evaluation chime challenge weninger erdogan watanabe vincent roux hershey schuller speech enhancement lstm recurrent neural networks application noiserobust proc. lva/ica eyben weninger squartini schuller real-life voice activity detection lstm recurrent neural networks application hollywood movies proc. icassp ravanelli cristoforetti gretter pellin sosi omologo dirha-english corpus related tasks distant-speech recognition domestic environments proc. asru", "year": 2017}