{"title": "Hard Negative Mining for Metric Learning Based Zero-Shot Classification", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Zero-Shot learning has been shown to be an efficient strategy for domain adaptation. In this context, this paper builds on the recent work of Bucher et al. [1], which proposed an approach to solve Zero-Shot classification problems (ZSC) by introducing a novel metric learning based objective function. This objective function allows to learn an optimal embedding of the attributes jointly with a measure of similarity between images and attributes. This paper extends their approach by proposing several schemes to control the generation of the negative pairs, resulting in a significant improvement of the performance and giving above state-of-the-art results on three challenging ZSC datasets.", "text": "abstract. zero-shot learning shown eﬃcient strategy domain adaptation. context paper builds recent work bucher proposed approach solve zeroshot classiﬁcation problems introducing novel metric learning based objective function. objective function allows learn optimal embedding attributes jointly measure similarity images attributes. paper extends approach proposing several schemes control generation negative pairs resulting signiﬁcant improvement performance giving state-of-the-art results three challenging datasets. among diﬀerent image interpretation methods exploiting kind knowledge transfer design zero shot classiﬁcation considered domain adaptation problem target domain deﬁned using intermediate level representation made human understandable semantic attributes. source domain deﬁned annotated image database expected capture relation data attribute based representation classes. recent approaches addressing rely computation similarity function semantic space. learn semantic embedding either data class description compare embedded data using standard distance. recently proposed metric learning step adapt empirically similarity distance embedding space leading multi-objective criterion optimizing metric embedding. metric learned using empirical optimized criterion random equally sampled pairs similar dissimilar data. paper following observations active learning community show careful choice negative pairs combined multi-objective criterion proposed leads state results. recent work bucher introduced metric learning step zero-shot classiﬁcation pipeline. model trained pairs data positive pairs obtained taking training images associated provided attribute vector assigned class label positive pairs denoted following negatives. paper investigates improved ways select negative pairs. image vector attributes parametric similarity measure. metric matrix denoted transforms attribute embedding space space euclidean distance used. similarity images attributes computed metric learning problem positive pairs ﬁxed given training pair positive image negative pairs chosen freely; indeed many ways diﬀerent equal number negative positive pairs identical. moreover increasing size compared factor leads better overall results. following explore three diﬀerent strategies sample distribution negative pairs using several learning epochs ﬁrst present variant method describe iterative greedy schemes. random negative pairs obtained associating training image attribute vector chosen randomly among seen classes negative pair positive one. variant propose generate randomly negative pairs positive pair chosen i.e. randomly sampling attribute vectors classes. include objective function penalization compensate unbalance positive negative pairs uncertainty strategy inspired hard mining object detection consists selecting informative negative pairs iteratively updating scoring function given denote score time training time step corresponds learning epoch. ﬁrst epoch learned using random negative pairs time pair training image candidate annotation coming diﬀerent classes ranked according uncertainty score similar actual coming diﬀerent classes relevant improving model. deﬁne probability generating pair based similarity score sample distribution. uncertainty/correlation propose improve previous approach taking account intra-class correlation. underlying principle governing selection correlated vectors attribute given class useful ones consider. correlation measured original learning criterion deﬁned assumes negative positive pairs evenly distributed. case proposed approach criterion must adapted compensate imbalance between positive negative pairs weighting positive negative pairs according frequencies datasets section evaluate proposed hard mining strategy diﬀerent challenging zero-shot learning tasks experiments following public datasets apascal&ayahoo animals attributes cub-- attribute datasets. designed evaluate methods contain large number categories described using various semantic attributes make comparisons previous works possible used training/testing splits sults vgg-verydeep- features. unc./cor. uncertainty/correlation method. unc./cor. method can’t apply dataset since images class attributes contrarily ap&y datasets. hyper-parameters estimate three hyper-parameters apply grid search validation procedure randomly keeping training classes. randomly initialized normal distribution optimized stochastic gradient descent. experiments follow standard protocol training images known classes available learning model parameters. test time images unseen classes assigned possible classes. classes described vector attributes. performance measured mean accuracy classes. tables show performance given hard-mining approach outperforms previous methods datasets average smart selection negative pairs plays role decision boundaries especially classes close attribute descriptions. compare results diﬀerent image features. alternatives explored paper give similar performance shown next section uncertainty/correlation faster. table give accuracy performances ap&y dataset three methods function number negative examples positive pair. bucher conﬁguration corresponds random method negative example positive one. negative pair selection method strong impact performance noticeable mean improvement augmenting ratio negative pairs positive ones positive inﬂuence accuracy. also made experiments evaluate impact hard mining selection convergence training. figure shows uncertainty/correlation converges around times faster uncertainty random methods. conﬁrms fact informative pairs selected strategy. negative/positive ratio positive impact convergence. paper extended original work bucher proposing novel hard negative mining approach used training. proposed selection strategy gives close state-of-the-art performance four standard benchmarks positive impact convergence.", "year": 2016}