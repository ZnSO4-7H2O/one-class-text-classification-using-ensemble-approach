{"title": "Unsupervised Basis Function Adaptation for Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate approximation architectures.  We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. This method is \"unsupervised\" in the sense that it makes no direct reference to reward or the VF estimate. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.  A common method of scoring a VF estimate is to weight the squared Bellman error of each state-action by the probability of that state-action occurring. Adopting this scoring method, and assuming $S$ states, we demonstrate theoretically that - provided (1) the number of cells $X$ in the state aggregation architecture is of order $\\sqrt{S}\\log_2{S}\\ln{S}$ or greater, (2) the policy and transition function are close to deterministic, and (3) the prior for the transition function is uniformly distributed - our algorithm, used in conjunction with a suitable RL algorithm, can guarantee a score which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X \\log_2S)$ space complexity and negligible time complexity. The results take advantage of certain properties of the stationary distributions of Markov chains.", "text": "using reinforcement learning algorithms evaluate policy common given large state space introduce form approximation architecture value function exact form architecture signiﬁcant effect accuracy estimate however determining suitable approximation architecture often highly complex task. consequently large amount interest potential allowing algorithms adaptively generate approximation architectures. investigate method adapting approximation architectures uses feedback regarding frequency agent visited certain states guide areas state space approximate greater detail. method unsupervised sense makes direct reference reward estimate. introduce algorithm based upon idea adapts state aggregation approximation architecture on-line. common method scoring estimate weight squared bellman error state-action probability state-action occurring. adopting scoring method assuming states demonstrate theoretically transition function close deterministic prior transition function uniformly distributed algorithm used conjunction suitable algorithm guarantee score arbitrarily close zero becomes large. able despite space complexity negligible time complexity. results take advantage certain properties stationary distributions markov chains. using traditional reinforcement learning algorithms evaluate policies environments large state action spaces common introduce form architecture approximate value function example parametrised functions. approximation architecture allows algorithms deal problems would otherwise computationally intractable. issue introducing approximation however accuracy algorithm’s estimate highly dependent upon exact form architecture chosen. accordingly number authors explored possibility allowing approximation architecture learned agent rather pre-set manually designer overview). common assume approximation architecture adapted linear case methods known basis function adaptation. designing method adapt approximation architectures assume underlying algorithm which given linear architecture generate estimate. assume basis function adaptation occurs on-line algorithm constantly updating estimate basis functions updated simple perhaps under-explored method basis function adaptation involves using estimate frequency agent visited certain states determine states accurately represent. methods unsupervised sense direct reference reward estimate value function made. concept using visit frequencies unsupervised manner completely however remains relatively unexplored compared methods based direct estimation error however possible methods offer unique advantages. particular estimates visit frequencies cheap calculate accurate estimates visit frequencies generated relatively small number samples perhaps importantly many cases visit frequencies contain important information regarding accuracy required estimate. explore quantify possible advantages. next section outline details algorithm performs unsupervised basis function adaptation based state aggregation. algorithm form basis upon develop theoretical results section agent interacts environment sequence iterations particular state take particular action according policy probability agent takes action state si). transition reward functions denoted respectively hence denote probability agent transition state given takes action state assume reward function bounded considering problem policy evaluation always assume agent’s policy ﬁxed. state aggregation approximation architecture deﬁne mapping state cell typically given state aggregation approximation architecture algorithm maintain estimate true value function speciﬁes weight associated cell-action pair. provided particular mapping value converges mapping corresponding estimate policy many different methods used score estimate common score used squared bellman error state-action weighted probability visiting state. called mean squared error true unknown bellman operator obtain approximation mse. approximation denote hence discount factor vector probability state given stationary distribution associated assume task developing adaptive architecture design algorithm adapt minimised. pasa algorithm outline adapts mapping pasa store vector integers dimension suppose start partition state space cells indexed approximately size. using deﬁne partition splitting cell partition. leave half cell index give half index taking partition create partition splitting cell. continuing fashion partition containing cells need additional mechanisms allow update denote states cell partition algorithm store vector real values dimension record approximate frequency certain cells visited agent. deﬁne vector dimension constant step size parameter. update certain intervals pasa algorithm performs sequence operations. temporary copy made call also store dimensional boolean vector entry zero start sequence stage sequence update order follows constant designed ensure threshold must exceeded adjusted. idea behind step sequence non-singleton cell highest value split. details steps well overall pasa process outlined algorithm note algorithm calls procedure split cells. procedure simply updates given latest value also calls convert procedure converts mapping mapping pasa requires modest increase computational resources compared ﬁxed state aggregation. relation time complexity updated parallel algorithm’s update whilst updated large intervals applying mapping state order time complexity algorithm using pasa compared equally sized cells. hence pasa involves material increase time complexity. pasa involve additional space complexity respect storing vector must store real values. also store overall space complexity becomes component space complexity introduction pasa pre-processing algorithm impact overall space complexity regarding sampling efﬁciency since methods based explicitly estimating bellman error require estimate well information reward actions action space expect pasa require comparatively less sampling generate estimates requires update approximation architecture. reassure pasa converge following sense. small enough elements |¯ui| remains within interval size arbitrarily large number iterations arbitrarily high probability possible eventually remain arbitrarily high probability suppose remains ﬁxed then since element calculated step sequence described algorithm remain within interval size arbitrarily high probability value also remain ﬁxed. hence induction exists eventually remain ﬁxed. main result. idea that many important circumstances reﬂective real world problems following ﬁxed policy agent tendency spend nearly time small subset state space. property advantage. means focussing small area eliminate terms signiﬁcantly contribute trick quantify tendency. must make following assumptions close deterministic uniform prior distribution sense that according prior distribution independently distributed si′′ also close deterministic make following observation. deterministic pick starting state agent create path state space eventually revisit previously visited state enter cycle. call states cycle denote number states cycle. place agent state either create cycle terminate path cycle created call states second cycle continue manner sets cs}. call union sets denote number states denote event path created manner terminates itself note that occur this follows solution birthday problem expectation prior distribution description problem formal proof example page flajolet sedgewick variance derived ﬁrst principles using similar techniques used mean birthday problem. bound provided represents signiﬁcant reduction complexity starts take size comparable many real world problems also seems likely bound theorem improved upon provided necessarily tight possible. conditions commonly encountered practice particular taken reﬂect greedy policy. condition interpreted transition function completely unknown note ﬁnally result extended exact redeﬁned also weighted message discussion commonly encountered circumstances unsupervised methods effective creating approximation architecture. however given simplicity time avoid cost associated complex adaptation methods. setting policy improvement advantages potential particularly important especially dealing large state spaces. initial experimentation suggests pasa algorithm signiﬁcant impact algorithm performance policy evaluation policy improvement settings. nature estimate generated pasa associated algorithm well estimated states visited frequently existing policy. come cost however estimates value deviating current policy made less accurate. thus even though immediately follow algorithm optimise policy standard policy iteration ultimately however theoretical implications improved estimate context policy iteration complex would need subject research.", "year": 2017}