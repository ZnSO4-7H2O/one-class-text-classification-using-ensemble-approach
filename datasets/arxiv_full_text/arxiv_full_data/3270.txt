{"title": "Compact Compositional Models", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Learning compact and interpretable representations is a very natural task, which has not been solved satisfactorily even for simple binary datasets. In this paper, we review various ways of composing experts for binary data and argue that competitive forms of interaction are best suited to learn low-dimensional representations. We propose a new composition rule that discourages experts from focusing on similar structures and that penalizes opposing votes strongly so that abstaining from voting becomes more attractive. We also introduce a novel sequential initialization procedure, which is based on a process of oversimplification and correction. Experiments show that with our approach very intuitive models can be learned.", "text": "learning compact interpretable representations natural task solved satisfactorily even simple binary datasets. paper review various ways composing experts binary data argue competitive forms interaction best suited learn low-dimensional representations. propose composition rule discourages experts focusing similar structures penalizes opposing votes strongly abstaining voting becomes attractive. also introduce novel sequential initialization procedure based process oversimpliﬁcation correction. experiments show approach intuitive models learned. recent years multi-layer network architectures drastically improved discriminative performance several classiﬁcation tasks. deep networks constructed based desired invariances stability properties learned usually large datasets cascade nonlinear functions effective classiﬁcation used transformations typically high-dimensional individual features always semantically meaningful. moreover described structures sometimes global desired often many features describe similar structures. pointed bengio learning disentangle factors variation remains challenge deep networks. even simple classes basic task learning compact interpretable representation solved satisfactory manner. example natural representation letter terms vertical horizontal bar. consequently class efﬁciently represented coordinates corresponding row/column location orientation bars. work learn robust representations seeking parsimonious experts corresponding largest stable structures data. apart intuitively appealing low-dimensional explicit representations example useful obtaining coarse scene descriptions computer vision. emphasize focus achieving state-of-the-art performance terms classiﬁcation rates likelihoods rather learning simple models examples. section review various ways composing experts binary data discuss impact resulting representation. composition rule introduced particularly well suited learning compact representations. rule causes extremal competition among experts data dimensions. particular rule ensures identical experts cannot used improve likelihoods opposing expert opinions always lead reduced likelihood. section present appropriate sequential inference procedure type compositional models consider. section describe batch well online version em-style learning procedure composition rule. moreover propose sequential initialization method described process oversimpliﬁcation correction. results synthetic dataset handwritten letters presented section composition rules model binary data product bernoulli distribution i.e. variables assumed conditionally independent given global template composition experts expert bernoulli template experts combined order create composed template speciﬁed composition rule rules generative counterpart activation functions feed-forward neural networks. formally composition rule function varying number arguments. composed template obtained applying composition rule expert opinions dimension following consider classes models. ﬁrst class natural choice binary images referred write-black models models default variable state underlying factors able turn variables multiple causes variable present state still type model appropriate example ﬁgure-ground segmentations experts describe object parts. composition rules write-black models encode vote refer asymmetric rules. note value used regions away expert support. template probability hand used variables close boundary support hence interpreted sure. write-black models samples individual experts look like actual object parts. second class consider write-white-and-black models models experts able cast votes favor well off. composition rules write-whiteand-black models encode abstention refer symmetric rules. note sure also encoded value either meanings. samples single experts image data look like actual object parts half pixels background region turned rule motivated probability observing success drawing independently bernoulli distributions probabilities conditioned observing success. easy composed odds individual odds. global mixture model exactly component responsible generating whole data vector exactly expert responsible dimension turned contrast noisy-or rule responsibility individual variables shared. rule used l¨ucke sahani since strongest template matters experts incentive represent structures already present unless opinion extreme one. consequently experts tend focus different aspects data. contrast noisy-or sum-of-odds rule likelihoods cannot improved using expert multiple times. sum-of-odds rule composition rule variable known expert responsible. fact makes possible analytic formula m-step learning procedures models. composition interpreted equal mixture individual expert templates. rule used amit trouv´e note however composition rule impossible expert abstain voting. consequently support experts restricted manually. logistic function type composition used restricted boltzmann machines sigmoid belief networks followed logistic link function also used generalized linear models binary data. includes logistic latent trait models exponential family sparse coding binary matrix factorization abstention expressed log-odds i.e. probabilities since ﬁrst step individual votes opinions experts voting opposite direction completely canceled out. indeed seen figure even composed probability approaches approaches result little pressure abstain voting. time similar experts complement without reducing likelihood compared single strong expert. linear interpolation used values between. however computational cost linear interpolations dimensions tractable approximation actually used experiments work. propose symmetric composition rule reduces redundancy among experts incentivizes abstention time. analogously composition would like rule possibility increase likelihood using expert multiple times. achieved using extreme opinion. hand similarly normalized opposing opinions result limitation maximum achievable likelihood. considerations naturally lead max-minus-min rule comparison symmetric composition rules shown right panel figure illustrate difference log-odds composition max-minus-min composition compute corresponding log-likelihood functions simple example. consider ground-truth model creates completely white images probability completely black images probability random images probability attempt learn model training experts combined using sum-of-log-odds max-minus-min composition rule respectively. simplicity reduce expert templates single parameter i.e. pixel chance turned show resulting log-likelihood functions figure limit large image resolution. equivalent global maxima corresponds expert black images expert white images. initial parameters sum-of-log-odds model side gradient descent change parameters direction partially explains restricted boltzmann machines randomly initialized weights tendency yield multiple similar experts. situation different max-minus-min composition. parameters side moving along gradient direction would change extreme value expert would remain close vote state note however using gradient descent train max-minus-min models rather employ algorithm example also suggests could beneﬁcial sequential initialization procedure additional experts take care structures cannot explained existing ones. given experts inference task determine posterior distribution expert conﬁgurations given observation least ﬁnding expert conﬁguration likely generated data. purposes important inference procedure yields compact representations. mention common approaches present sequential procedure call likelihood matching pursuit. restricted boltzmann machines experts evaluated independently. computationally efﬁcient activates experts match data sufﬁciently well rather providing sparsest possible activation explain data. works indicator variables expert presence relaxed real values example dayan zemel vincent simple mean-ﬁeld approximations. saund uses gradient descent starting point coordinates equal itfigure panel scene analyzed. panel noisy version. panel resolved scene using robustiﬁed templates panel first detected digit using original templates. erative procedure becomes inefﬁcient however number possible experts much larger typical number active experts. l¨ucke sahani truncated search evaluates expert conﬁgurations small number active components. propose simple sequential inference procedure max-minus-min compositions. data ﬁrst explained expert yields highest likelihood observation additional experts evaluated forming composition current global template candidate experts computing likelihood. best expert added global template updated accordingly. procedure ends likelihood cannot improved anymore. yields sparse activation single sequential pass experts added able explain structures explained before. note procedure quite similar matching pursuit sparse coding. instead minimizing squared error maximize bernoulli likelihood. sequential ﬁtting works well write-white-and-black models however since write-black models encode abstention probability rather value heavily depend structures described present observation consequently easy situations instead. eliminates impact data background region expert. indeed depends variables satisfy means likelihood truncated template depends data support expert similar idea robustify templates used williams titsias original templates uniform distribution. case binary data amounts i.e. convex combination formed. write-black model less effective transformation truncation. illustrate effectiveness proposed modiﬁcation simple scene analysis example. five digits placed random locations image according write-black model task recover identity locations digits noisy version scene. using robustiﬁed templates scene resolved perfectly shown figure however original templates ﬁrst digit placed down corresponds largest structure image explained single template moderately well note suggested possible solution. allow iterative procedure scene also resolved original templates. modiﬁcation simply meant robustiﬁcation greedy inference procedure. describe approximate procedure learning max-minusmin models. working image data explicitly model transformations like shifts rotations. template provides multiple transformed versions denotes transformation. allows share parameters among transformed versions. since templates describe rather large structures assume transformed versions present image. e-step likelihood matching pursuit procedure previous section yields observation representation deﬁne maximizers respectively minimizers transformation corresponding k-th expert representation n-th training example pseudocount added regularization purposes interpreted beta prior expert probabilities. experiments corresponds uniform prior. update formula exact maximizer expert templates good heuristic. reason simple analytic expression extremal composition rules responsibility individual variables split among many experts. composition rules gradient descent methods often required order perform m-step. also note max-minus-min model reduces model. special case update formula involves variable counts many training examples used compute current estimate d-th dimension expert online version attractive allows sequential initialization scheme. since learning problem non-convex crucial good initialization. accordance attempt learn parsimonious representation start single global template derived ﬁrst training example experts later idea oversimpliﬁed models sense explain examples experts learned far. models corrected appending additional templates collection experts. deﬁne ˜µk+ \u0001)/. using max-minus-min composition additional template initialized makes sure background region close successive reﬁnement stark contrast typical bottom-up grouping local structures part-based compositional models. example fidler leonardis start elementary edge features combine simple structures complex ones hierarchical clustering procedure. image data spatial arrangement experts modeled joint gaussian distribution shifts rotations. requires compute mean covariance training conﬁgurations provided inference procedure. emphasized bruna mallat desirable property representation intraclass deformations linearized. experiment section conﬁrms representation transforms complex deformation orbit linear space gaussian distribution satisfactorily describes deformations. compare max-minus-min model denoising autoencoders restricted boltzmann machines synthetic model binary images size pixels. image grid divided regular grid four quadrants. quadrant independently activated probability activated quadrant either entirely black entirely white probability non-activated quadrants pixels drawn independently bernoulli-/ distribution task recover underlying experts corresponding four quadrants polarities. learning algorithm max-minus-min model typically converged iterations. left panel figure visualize obtained templates iterations using training examples. model able almost perfectly recover original experts. denoising autoencoder restricted boltzmann machine epochs learning rates tuned. denoising autoencoder also tuned corruption level. figure shows comparison max-minus-min model terms cross-entropy test samples. reconstruction error max-minus-min model much lower. sum-of-log-odds composition multiple ground-truth experts mixed right panel figure shows templates indeed combinations multiple ground-truth experts i.e. factors variation disentangled successfully. order make fair comparison left sequential initialization procedure initialized models completely random templates. means performance gain fact competitive expert interaction. comparison learned sparse coding dictionary note data treated real-valued task basis vectors whose linear combinations allow good reconstructions squared error sense. basis vectors learned samples visualized right panel figure again ground-truth experts recovered. however basis vectors look structured experts learned denoising autoencoders restricted boltzmann machines. terms reconstruction error dictionary actually even better ground-truth generative model. figure visualizes reconstructions obtained sparse coding learned max-minusmin model. sparse coding reconstruction visually closer data much noisy reconstruction model. indeed max-minus-min reconstruction almost identical ground-truth templates. reason different behavior squared error forgiving small deviations compared cross-entropy penalizes harder cross-entropy deviation large figure left initialization learned experts iterations max-minus-min model trained examples synthetic model. blue gray yellow color corresponds probabilities respectively. right experts obtained denoising autoencoder restricted boltzmann machine dictionary learning sparse coding using samples. figure online learning letter left panel samples used training expert templates step right panel sampled expert conﬁgurations based multivariate gaussian distribution spatial expert arrangement using ﬁnal templates. sample provides label writer corresponding character. since pixels dataset correspond black natural composition rule. purpose experiment illustrate effectiveness sequential initialization procedure section left panel figure visualizes online learning process letter using ﬁrst sample ﬁrst writers. ﬁrst expert initialized ﬁrst training example. second expert initialized characteristics second example cannot explained ﬁrst expert. every additional image updates experts. examples learning process converged vertical horizontal bar. samples learned spatial distribution look realistic cover principal deformations class. also learned four experts other letters using ﬁrst sample ﬁrst writers. results shown figure experts correspond natural elements character class. note contrast lake motion information necessary learn character primitives. considered various composition rules discussed adequacy learning compact representations. inference learning procedures models extremal composition rules proposed performance tested experiments. alternative competitive interaction rules would prior part parameters however would create bias affects experts. approach hand allows maximum-likelihood estimation. focus paper binary data. natural next step study compositions experts real-valued data. includes considering composition rules variances achieving continuity transitioning expert another. figure experts learned examples letter class. expert plotted mean location mean orientation. pixel color indicates maximum expert intensity visualizes template value lake brenden salakhutdinov ruslan tenenbaum josh. one-shot learning inverting compositional causal process. advances neural information processing systems honglak grosse roger ranganath rajesh andrew convolutional deep belief networks scalable unsupervised learning hierarchical representations. international conference machine learning meeds edward ghahramani zoubin neal radford roweis modeling dyadic data binary latent factors. advances neural information processing systems schein andrew saul lawrence ungar lyle generalized linear model principal component analysis binary data. international workshop artiﬁcial intelligence statistics volume vincent pascal larochelle hugo bengio yoshua manzagol pierre-antoine. extracting composing robust features denoising autoencoders. international conference machine learning long chenxi huang haoda chen yuanhao yuille alan. unsupervised structure learning hierarchical recursive composition suspicious coincidence competitive exclusion. computer vision–eccv", "year": 2014}