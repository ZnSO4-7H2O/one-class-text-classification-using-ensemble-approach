{"title": "AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "abstract": "New types of machine learning hardware in development and entering the market hold the promise of revolutionizing deep learning in a manner as profound as GPUs. However, existing software frameworks and training algorithms for deep learning have yet to evolve to fully leverage the capability of the new wave of silicon. We already see the limitations of existing algorithms for models that exploit structured input via complex and instance-dependent control flow, which prohibits minibatching. We present an asynchronous model-parallel (AMP) training algorithm that is specifically motivated by training on networks of interconnected devices. Through an implementation on multi-core CPUs, we show that AMP training converges to the same accuracy as conventional synchronous training algorithms in a similar number of epochs, but utilizes the available hardware more efficiently even for small minibatch sizes, resulting in significantly shorter overall training times. Our framework opens the door for scaling up a new class of deep learning models that cannot be efficiently trained today.", "text": "types machine learning hardware development entering market hold promise revolutionizing deep learning manner profound gpus. however existing software frameworks training algorithms deep learning evolve fully leverage capability wave silicon. already limitations existing algorithms models exploit structured input complex instancedependent control prohibits minibatching. present asynchronous model-parallel training algorithm speciﬁcally motivated training networks interconnected devices. implementation multi-core cpus show training converges accuracy conventional synchronous training algorithms similar number epochs utilizes available hardware efﬁciently even small minibatch sizes resulting signiﬁcantly shorter overall training times. framework opens door scaling class deep learning models cannot efﬁciently trained today. category neural networks emerging whose common trait ability react dynamic unique ways properties input. networks like tree-structured recursive neural networks graph neural networks defy modern gpu-driven paradigm minibatch-based data management. instead networks take tree graph input carry computation depends individual structures. refer class models dynamic control dynamic neural networks. modern neural network frameworks certainly capable expressing dynamic networks. tensorflow introduces cond while_loop higher order functional abstractions chainer dynet pytorch dynamically construct computation graph using control host language. however training networks existing software frameworks hardware painfully slow networks require highly irregular non-uniform computation depends individual instances. makes batching impractical impossible thus causing cost matrix-vector product dominated cost loading weights dram typically orders magnitude slower peak compute cpus gpus moreover frameworks typically optimized single instance batch size mind. dynamically unfolding computation graph example concern enough instances amortize cost limited batching show paper scale dynamic models exploiting extreme form model parallelism amenable distributed execution cluster interconnected compute devices. model parallelism mean computing disjoint parts computational graph parallel also computing sequential operations graph pipeline-parallel fashion figure gantt charts comparing pipelined synchronous model parallelism asynchronous model parallelism. orange blue yellow boxes correspond forward backward parameter update operations respectively. numbers boxes indicate instance ids. conventional model parallelism however maximize device utilization keep pipeline full times. unfortunately show figure keeping conventional pipeline full odds convergence speed decreased parameter update frequency; compare figure analogous trade-off face batching. overcome problem propose asynchronous model-parallel training allow asynchronous gradient updates occur whenever enough gradients accumulated; figure design high device utilization update frequency. setting however model parameters updated forward backward computation instance introducing gradient staleness. despite staleness show training converge fast good hardware utilization. speciﬁcally contributions present intermediate representation explicit constructs branching joining control supports training. unlike previous work considers static computation graphs static control dynamic computation graphs dynamic control encodes static computation graph execute dynamic control ﬂow. consequence training becomes easy distribute parallelize. further nodes process forward backward messages multiple instances time seamlessly support simultaneous training inference. show that thanks explicit control constructs readily encode replicas form data parallelism addition includes operators data aggregation recover forms batching. features improve efﬁciency even cpus. show training converges similar accuracies synchronous algorithms often signiﬁcantly faster. example dataset implementation gated graph sequence neural network core runs faster tensorflow implementation faster tensorflow implementation titanx better exploit sparsity. though compete across-the-board mature frameworks tensorflow evaluation proves ampnet particularly beneﬁcial dynamic networks. summary work demonstrates beneﬁts training gives novel design deploy neural network libraries dynamic control ﬂow. together contributions open ways scale dynamic networks interconnected compute devices. inspired increasing investment innovation custom silicon machine learning asics perform simple calculation dataset shows ampnet network tflops devices faster runtime requiring gb/s network bandwidth variable-length rnns iterate tokens variable-length sequences. pseudo-code simple given figure linear layer substituted sophisticated unit gated recurrent unit though instance different length possible padding enable batching. however lead limited speedup variability sequence lengths. tree-structured neural networks powerful models used parsing natural language images semantic representation sentiment analysis require evaluation trees shared parameters different topology instance. even needs evaluate single computational tree instance tree instance-speciﬁc batching requires nontrivial planning simple form tree neural network performs bottom traversal instance starting embedding leaves. level values child nodes concatenated sent specialized unit result propagated tree. backpropagation tree structure known backpropagation structure graph neural networks combine temporal recurrence variable length recurrence structure tree rnn. gnns seen performing aggregation/distribution operations general graph structure shared parameters. apart models above exist many recently proposed models ﬂexible control neural programmer interpreters adaptive computation networks networks stochastic depth framework applied. basic idea behind training distribute computation graph across compute nodes communicate activations. training nodes computation graph exchange forward backward messages. parameterized computations individually accumulate gradients computed backwards messages. number accumulated gradients since last update exceeds threshold min_update_frequency local update applied accumulator gets cleared. local parameter update occurs without communication synchronization parameterized computations. staleness gradient measured number updates forward backward computation produces gradient. small min_update_frequency increase gradient staleness. hand large min_update_frequency reduce variance gradient result infrequent updates also slow convergence. addition max_active_keys controls maximum number active instances in-ﬂight point time. setting max_active_keys restrict single-instance processing. in-ﬂight messages generally increase hardware utilization also increase gradient staleness. implemented ampnet runtime multi-core cpus details given appendix section demonstrates effects parameters. note usually always equivalent synchronous training. example single instance comprised stream messages depending model updates occur asynchronously even messages in-ﬂight belong single instance. overview motivated need distribute dynamic networks networks interconnected devices apply training designed static graph-like intermediate representation serve target compilation high-level libraries dynamic networks admit multiple backends feature static graph execute dynamic instance-dependent control decisions. node receive process either forward messages backward messages training forward propagation carried passing forward messages graph. message consists payload state. payload typically tensor whereas state typically model-speciﬁc used keep track algorithm control information. example variable-length state contains instance identiﬁer current position sequence total sequence length instance. ﬁnal loss layer initiates backward propagation graph. invariant every forward message generated node speciﬁc state node eventually receive backward message state. depending max_active_keys multiple forward backward messages in-ﬂight instances. payload transformations parameterized payload transform nodes used encode instance fully connected layers. apply transform forward pass also record activation order compute gradients backward pass. activation recorded keying state message hence state must include necessary information allow node process multiple messages potentially different instances without conﬂating activations. require speciﬁcations forward backward transformation operation produce gradient well state keying function used. node decide independently apply accumulated gradients update parameters. transformations involve parameters offers simpler non-parameterized payload transform. loops state control condition node parameterized function queries state incoming message based response routes input successor nodes. join node propagates messages receives ancestor nodes records origin backward pass backpropagate correct origin. like nodes node must parameterized keying function state incoming message. invertible state update node initial hidden state every sequence. message states contain sequence time-step. following embedding messages concatenated hidden state result goes linear node followed relu activation. node increments time-step conditional node tests whether sequence reached. depending answer either propagates hidden state back pushes hidden state ﬁnal linear loss layers. backward mode gradient propagated inside body loop passes reaches node. node either backpropagate cond node controller. hence loop executed forward backward direction. aggregation disaggregation offers several constructs aggregation disagreggation; important ones outlined below behavior summarized figure concat split bcast perform concatenation partition broadcast incoming messages names suggest. group group together several incoming messages based state. output message contains tensor composed input payloads whereas state function incoming states. forward mode group must state cache states original messages restore backward phase. ungroup symmetric version group. flatmap creates sequence outgoing messages incoming message replicated payload states given state generation function parameter node. node keys outgoing states caches incoming state number expected messages gradients restore original state backward mode. figure describes combines aggregation structure graph instance outer iteration. iteration controls effect locality information propagation. controller pumps data before lookup table labels instance loss layer. lookup table emits payloads matrices corresponds embedding instance node states contain current iteration counter instance reference graph structure. messages broadcast ungrouped outgoing message corresponds node graph instance. next message goes flatmap node replicates payload outgoing edge creates states record incoming node outgoing node type edge resulting stream messages edge graph. next edges grouped edge type group sent designated linear layer. group dismantled back edges re-grouped target node. group passed non-parameterized payload transformation sums together payloads. result stream messages message contains aggregated value graph node. finally group back aggregated values send result rnncell another outer iteration. note constitutes form batching information nodes batched together sent rnncell. pipeline-style parallelism often augmented forms data parallelism. consider fig. heavy operation body loop going bottleneck computation. solution split linear layer smaller tiles compute parallel. expressible linear operation needs large enough beneﬁt tiling way. another approach replicate linear layer full. fortunately requires minimal machinery replicate linear layer place replicas inside cond nodes figure different instances messages instance different position sequence processed parallel fashion using replicas case. enable parameters shared among replicas implemented infrequent end-of-epoch replica synchronization keep communication cost negligible well message-passing protocol asynchronous trigger whole-replica group synchronization found infrequent synchronization sufﬁcient fast convergence. evaluate ampnet using dynamic models introduced section completeness additionally consider simple multi-layer perceptron example static network instances easy batch. model select dataset compare throughput convergence proﬁle ampnet traditional training schemes implemented tensorflow. mnist preliminary task train -layer perceptron relus mnist choose dimensional hidden units afﬁnitize linear operations individual workers runtime tensorflow learning rate batch size list reduction dataset starting point experiments networks complex control synthetic dataset solved vanilla rnn. speciﬁcally train perform reduction operations variable length lists digits. training instance sequence tokens ﬁrst token indicates reduction operations performed remaining tokens represent list digits. output result calculation rounded modulo dataset consists training validation instances. present task classiﬁcation problem vanilla relu activation hidden dimension parameterized operations afﬁnitized individual workers. bucket training instances batches sequences tree-lstm stanford sentiment treebank non-synthetic problem consider real-world sentiment classiﬁcation dataset consisting binarized constituency parse trees english sentences sentiment labels node. following trees training trees validation trees testing. tree lstm classiﬁcation task based tensorflow fold benchmark model. fold models trained following additional architectural modiﬁcations proposed furthermore split tree-lstm cell leaf lstm branch lstm cells. affect expressiveness model lstm cell receives either zero input zero hidden states i.e. cells share weights except bias parameters learned independently implementation. compare time reach grained accuracy validation set. table time convergence target validation accuracy. time convergence broken number epochs throughput target accuracy shown inside parentheses next facebook babi datasets verify implementation using logic deduction benchmark study real-world application gnns prediction organic molecule properties structural formulae dataset gnns previously applied tasks respectively. babi dataset inﬂate graphs default nodes nodes increase computational load preserve two-hop complexity deduction task. architecture model follows hidden dimension propagation steps. dataset concentrate prediction norm molecule’s dipole moment using regression layer build propagation model hidden dimension propagation steps initializing graph nodes following molecules contain atoms tensorflow baseline bucket molecules batches atom counts differing within batch. following report regression accuracies multiples target accuracy chemistry community. almost ideal ﬁrst three linear layers heaviest operations. fourth column table mild asynchrony negligible effect convergence greatly improving throughput time convergence. list reduction dataset demonstrates power replicas. heavy operation speedup asynchrony mild however speedup replicas respectively nearly ideal. again epochs convergence affected increasing max_active_keys. slowdown convergence replicas increased effective minibatch size also commonly observed data parallel training. next sentiment tree-rnn dataset shows runtime competitive without batching tensorflow fold using dynamic batching batch size worth mentioning runtime allows specify different min_update_frequency parameter parameterized operation. parameter embedding layer initialized glove vectors layers. greatly reduced gradient staleness embedding layer. finally babi datasets demonstrates importance sparsity. note tensorflow implementation ggsnn implements message propagation aggregation input graph input graph unique connectivity matrix needs constructed instance. contrast handle message passing branching described section result roughly speedup tensorflow implementation cpus number threads. runtime also asynchrony degree asynchrony controlled hyperparameters min_update_frequency max_active_keys. fig. -replica model list reduction dataset investigate parameters affect data time required converge validation accuracy. analogy minibatch size traditional systems min_update_frequency must neither large small. increasing max_active_keys monotonically increases performance number keys similar number individually afﬁnitized heavy operations model case). increasing max_active_keys signiﬁcantly beyond point produces diminishing returns. figure performance -replica model function asynchrony hyperparameters. solid gray lines show constant convergence time trajectories. stands min_update_frequency. chainer dynet pytorch belong class deep learning frameworks deﬁne computation graph dynamically per-instance executing control host language limit cross-instance parallelism cost difﬁcult hide minibatch size small. contrast graph static easier distribute optimize pipeline-parallelize across instances. theano tensorflow provide powerful abstractions conditional execution loops also provides higher-order functions foldl foldr scan. main difference ampnet frameworks ampnet streaming asynchronous whereas theano non-streaming synchronous. although designed streaming support streaming programmatically exposes ﬁrst-class queues well data prefetching called input pipelines. queuing implicit stream-based execution default. additionally support static description dynamic control state update depart classic dataﬂow architecture follows first instead nodes represent mutable reference cells encapsulate state message processed graph message itself. second encapsulate algorithmic state messages introduce notion control dependencies choices complicate algorithmic state management programming point view make task designing high-level compiler non-trivial allow every node asynchronously independently without scheduler without need control messages example nodes dynamically take control path split data simply consult state incoming message instead accept additional control inputs. small states might preferable out-of-band signaling. implement loops simply using state-update conditional nodes state accompanies payload throughout lifetime whereas introduces specialized operators timely dataﬂow achieve effect. tensorflow fold recent extension tensorflow attempts increase batching dynamic networks interesting alternative asynchronous execution. unrolls merges together computation graphs several instances resulting batch-like execution. effectiveness greatly depends model example would batch well random permutations sequence operations whereas would succinctly express achieve pipeline parallelism control-ﬂow nodes. asynchronous data parallel training another popular approach scale optimization removing synchronization orthogonal combinable model-parallel training. example convolutional layers amenable data-parallel training fully connected layers weights smaller activations. moreover control differs data instance data parallelism effective jaderberg like train different parts model decoupled asynchronous manner. precisely goal approximate gradient synthetic gradient computed small neural network locally attached layer. hence local gradient calculation becomes independent layers allows asynchronous parameter updates. would especially useful evaluation local network cheaper computation real gradient; example computation real gradient required communication forward/backward messages devices. presented asynchronous model-parallel algorithm distributed neural network training. described multi-core runtime models irregular and/or instance-dependent control ﬂow. looking forward deploy system specialized hardware. give idea performant fpga implementations ampnet perform simple estimate peak throughput dataset running network tflops fpgas calculation shows achieve graphs/s dataset hidden dimensions nodes graph average. requires reasonable gb/s network bandwidth. equally importantly plan build compiler automatically deduces information placed states generates state keying functions higher-level description models. unlocking scalable distributed training dynamic models hope enable exploration class models currently horizon become mainstream future. would like thank eric chung doug burger catapult team continued discussions feedback early stage work. would also like thank krzysztof jozwik discussions fpgas stavros volos discussions various memory architectures miguel castro discussions data parallelism model parallelism john langford discussion asynchrony reproducibility frank seide discussions dynamic networks.", "year": 2017}