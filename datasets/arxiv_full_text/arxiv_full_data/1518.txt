{"title": "Phrase-based Image Captioning with Hierarchical LSTM Model", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "Automatic generation of caption to describe the content of an image has been gaining a lot of research interests recently, where most of the existing works treat the image caption as pure sequential data. Natural language, however possess a temporal hierarchy structure, with complex dependencies between each subsequence. In this paper, we propose a phrase-based hierarchical Long Short-Term Memory (phi-LSTM) model to generate image description. In contrast to the conventional solutions that generate caption in a pure sequential manner, our proposed model decodes image caption from phrase to sentence. It consists of a phrase decoder at the bottom hierarchy to decode noun phrases of variable length, and an abbreviated sentence decoder at the upper hierarchy to decode an abbreviated form of the image description. A complete image caption is formed by combining the generated phrases with sentence during the inference stage. Empirically, our proposed model shows a better or competitive result on the Flickr8k, Flickr30k and MS-COCO datasets in comparison to the state-of-the art models. We also show that our proposed model is able to generate more novel captions (not seen in the training data) which are richer in word contents in all these three datasets.", "text": "fact natural language sequential data temporal hierarchy information spread multiple time-scales consider english example lowest level shortest time-scale characters followed words phrases clauses sentences documents. therefore undeniable sentence structure prominent characteristics language victor yngve inﬂuential contributor linguistic theory stated language structure involving form other phrase-structure hierarchy immediate constituent organization hence forcing generative model trained sequences generate high-level structure locally step-by-step basis often results limited performance image caption particular least levels structure observed human annotated captions public datasets experimented. within caption several phrases describe objects image. phrases equal time-scale word level conditioned image short-term language structure decoding. thus previous sequence caption excludes phrase itself encoded long term memory redundant generation process. hand structure caption across phrases inter-dependent requires image previous sequences context generate correct description. paper would like investigate capability phrase-based image captioning model incorporated observed structure modeling compared similar model trained sequences. design phrase-based hierarchical lstm model namely phi-lstm consists phrase decoder abbreviated sentence decoder generate image description phrase abstract—automatic generation caption describe content image gaining research interests recently existing works treat image caption pure sequential data. natural language however possess temporal hierarchy structure complex dependencies between subsequence. paper propose phrasebased hierarchical long short-term memory model generate image description. contrast conventional solutions generate caption pure sequential manner proposed model decodes image caption phrase sentence. consists phrase decoder bottom hierarchy decode noun phrases variable length abbreviated sentence decoder upper hierarchy decode abbreviated form image description. complete image caption formed combining generated phrases sentence inference stage. empirically proposed model shows better competitive result flickrk flickrk ms-coco datasets comparison state-of-the models. also show proposed model able generate novel captions richer word contents three datasets. automatic caption description generation images challenging problem requires combination visual information linguistic. words requires complete image understanding also sophisticated natural language generation makes interesting task embraced computer vision natural language processing communities. past years common frameworks applied line research neural network model composed sub-networks convolutional neural network used encode image feature representation; recurrent neural network applied decode natural language description. particular long short-term memory model emerged popular architecture ability capture long-term dependency preserve sequence. recently many variants framework introduced achieved good results attention mechanism attributes however notice works decode image caption fully sequential word-by-word basis. although sequential c.s. chan center image signal processing department artiﬁcial intelligence faculty computer science information technology university malaya kuala lumpur malaysia. e-mail {tanyinghuasiswa.um.edu.my; cs.chanum.edu.my} sentence. illustrated fig. given image encoded phrase decoder employed decode noun phrases describe dominant entities within image using words atomic unit. time phrase decoder also encodes compositional vector representation serves input decoder upper hierarchy. hence equal time step remaining words sentence level then decoder decode abbreviated form caption made last word remaining words connect phrases complete image caption formed combining generated phrases sentence gradually beam search inference stage. empirically proposed model shows better competitive results flickrk flickrk ms-coco datasets comparison state-of-the models. summary contributions two-folds propose novel phrase-based hierarchical lstm model decode image caption phrase sentence. show image caption generated philstm accurate novel richer word content. preliminary version work presented whereas present work adds initial version significant ways. first phrase selection objective replaced prediction last word decoder training simplicity. secondly introduce length normalization inference stage phrase sentence level order generate longer caption. thirdly improve outputs parsing tool phrase reﬁnement strategy. finally considerable analysis intuitive explanations added results. also extend experiment include ms-coco dataset evaluate results four additional evaluation metrics rouge cider spice encode visual information earlier works rely multiple visual detectors classiﬁers capture different aspects image objects attributes relations scene outputs detectors classiﬁers usually form tuples description built upon. approach generally ﬁxes number classes aspect image. since unprecedented success image classiﬁcation object detection tasks growing number works start different variants encode whole image multiple image regions given encoded image description many works train multimodal embedding space using various besides that works rely retrieval approach generate image description. retrieving re-ranking caption similar images training sets query image described human written caption relevant content. however method incapable describing image unseen composition objects correctly. thus works line approach retrieve tuples text snippets form re-rank novel captions. given various contexts described above several approaches developed generate image description template-based composition-based iii) language model-based. template-based approach generates sentence using pre-deﬁned template open-slots ﬁlled image entities mostly used works represent visual content tuples. description generated usually syntactically correct rigid ﬂexible. composition method approach stitches text snippets retrieved entities detected form image description. requires sophisticated predeﬁned rules decide text snippets entities used generating complete caption orders gluing words them. description generated manner broader expressive compared template-based approach also computationally expensive test time nonparametric nature. language model-based recent works jointly embed image language multimodal embedding space neural network based language model generate image caption instance kiros proposed multimodal log-bilinear neural language model biased image feature decode image caption. karpathy used decode caption varying length lstm implemented decode image description respective context. example used encoded image semantic embedding learned normalized canonical correlation analysis inputs lstm decoder. moreover yang incorporated attention mechanism lstm decoder attend various parts image caption generation process. hand implemented attention mechanism semantic space instead multimodal space generating image caption. sequential lstm introduce hierarchical lstm structure decode image description phrase sentence. thus input model sentence level sequence combination words phrases. work different phrase-based approaches retrieval text snippets paired template composition method generate caption rely retrieval. phrase-based approaches place emphasize phrase learning simple language model decode sentence. example lebret ushiku extracted various types phrase image description. former trained phrase relevancy image negative sampling decoded sequence phrases using tri-gram language model conditioned chunking phrase. latter proposed subspaceembedding method phrase learning generated sentence estimated phrases using combinatorial optimization. work differs terms type phrase extracted phrase learning approach iii) sentence decoding method. first extract intuition phrase equivalent entity within image. moreover train phrase decoder using lstm linked hierarchically shown fig. phrase representation learned backpropagation decoder sentence level. lastly generate complete caption decoding progressively replace inferred noun generated phrases. recent work published skeleton-key currently closest work ours. designed course-toﬁne image caption decoder consists submodels skel-lstm learns generate skeleton sentence made original caption replaced last word attr-lstm learns decode nps. although model seems resemble ours still distinct difference links submodels. first wang designed top-down model skeleton sentence ﬁrst generated followed decoding skeletal word form attribute sub-sequences. contrary model bottom-up approach ﬁrst generated complete description. secondly testing stage used length factor control length generated caption manually whereas phrase indication objective normalized probability candidate govern length generated caption automatically. finally implement attention mechanism beyond scope paper. given image-sentence pair equivalent entities within image ﬁrst chunked sentence using phrase chunking algorithm described section then formed replacing caption last word chunked phrase shown example below large tricycle. gray shirt large tricycle shirt sandals pulling tricycle. decompose caption training data as-nps pair processed decoders linked hierarchically. decomposition alters order sequence human annotated caption thus different ground truth sequence training stage compared conventional models. phrase decoder decoder described below. phrase decoder work roles decodes image representation multiple describe entities within image encodes compositional vector representation serves input decoder. given image pre-trained imagenet classiﬁcation task applied encode image ddimensional image feature transformed k-dimensional vector image embedding matrix rk×d bias then lstm model similar used decode nps. train lstm model decode i-th length embedded image feature followed start-word token indicates translation process word input lstm block step-by-step manner shown fig. hence phrase decoder inputs time step phrase rk×v trainable word embedding matrix word vocabulary size represented k-dimensional vector one-hot vector indicating location ground truth word vocabulary time step phrase lstm block time step denote input gate forget gate output gate memory cell hidden state time step. thus lstm transition equations omitting phrase index fig. abbreviated sentence decoder input sequence complete caption occupies time step ground truth sequence caption. also predicts whether next input phrase word. here denotes logistic sigmoid function denotes elementwise multiplication. lstm parameters matrices dimension rk×k. intuitively gating unit controls extent information updated forgotten forwardpropagated memory cell holds unit internal memory regarding information processed current time step. hidden state therefore gated partial view memory cell unit. output lstm time step ptp+ equivalent conditional probability word given previous words image ground truth equivalent input word next time step endword token last time step indicate additionally hidden state last time step used compositional vector representation decoder similar design phrase decoder except inputs outputs shown fig. input decoder complete caption describing image remaining words caption encoded input single time step. denotes time step decoder length caption considering unit input decoder rk×d rk×v another trainable parameters image embedding start-word token word embedding matrix one-hot vector indicator ground truth word time step lstm parameters used decoder. outputs produced lstm model time step decoder binary indicator determines whether next input phrase word softmax prediction next word sequence ground truth second output time step either last word next phrase next word itself preliminary work used phrase token phrase indication resulted limitation unable discern appropriateness different inputs decoding. compensation phrase selection objective introduced solve limitation. however complicated training procedure optimized multiple randomly selected input time step input simplify training process replace phrase token phrase selection objective phrase indication softmax prediction last word respectively. compute perplexity given image description number phrases sentence probability output lstm block time step respectively. perplexity sentence conditioned image candidates inference next word iteratively candidates infer end-word token. score computed candidate summing probability word normalized length including end-word token among candidates generated least candidate kept group last word. remaining candidates discarded score lower threshold value order improve quality image description formed. total complete captions generated list candidates illustrated partially fig. decoder produces outputs time step next word prediction phrase indication next input. thus model infers next input phrase word candidates inferred compared list candidates. last word matches inferred words attached list beam candidates current time step replacing inferred words candidate sentences infer end-word token score caption computed hidden state output lstm block time step trainable parameters classiﬁer next input decoder phrase otherwise. here scales normalizes objective based number phrases words equivalent average loglikelihood word given previous context image described summed regularization term average number training samples. here trainable parameters model. summary proposed phi-lstm optimized predict next word given previous words next word given previous words phrases iii) whether next input phrase. phi-lstm model generates image caption twosteps manner list candidates ﬁrst generated followed complete caption using beam search algorithm. beam size phrase sentence generation respectively. generation work similar given image encoded followed start-word token input model acting initial context phrase decoder generate nps. every time step words highest probability sampled input decoder next time step infer subsequent words. best sequences generated time step kept phrase chunking limitations refinement quick overview structure image descriptions reveals elements compose majority captions usually describe dominant entities image either object group objects scene. entities equivalent abstract level output encoder linked verb prepositional phrase. thus essentially covers half corpus language model trained generate image description. therefore partition learning sentence structure processed evenly compared extract phrases without considering part speech tag. constituents level. section compare as-nps pair formed chunking using parsers. labels examples given denote complete caption as-nps pair formed chunking dependency parser constituency parser dependency parser reﬁnement respectively. underlined text indicates as-np pair wrong. common errors found output parser incorrect recognition verb noun. result missing object formed shown examples below. moreover describe entity image ‘the one’ example blue shirt standing garden. blue shirt standing garden standing garden. group young people preparing skiing. group young people preparing preparing skiing. group young people preparing group preparing skiing. look toward camera front points index ﬁnger. camera front points index ﬁnger look toward camera points ﬁnger. woman chairs outside near water. woman near water woman chairs outside water. snow covered benches snow covered ﬁeld. snow snow covered ﬁeld snow covered benches ﬁeld. snow snow snow covered benches snow covered ﬁeld. truck speeds tree lined street. truck tree lined street truck speeds street. truck tree truck speeds tree lined street. selected dependency relations nominal modiﬁer possessive alteration nmodposs nmodof parse higher constituent level. desired chunked relations correspond entity group entities within image intended shown examples below. much controversy nmodposs relation chunked nmodof relation ambiguity whether whole phrase split remained single example shows case relation necessary example shows another case necessity relation ambiguous. section describes parsing algorithm applied obtain as-nps pair problems arose limitation parsing tool proposed algorithm iii) measure took reduce inﬂuence errors training image captioning model. dependency parse stanford corenlp tool forms structural relation tree sentence providing structural relationships words. though chunk sentence directly constituency parser chunking tools pattern extracted ﬂexible select desirable structural relations. relations selected dependency parser extracts triplet sentence made governor word dependent word relation links them form <relation order form phrase chunks dependency parser simple post-processing step illustrated fig. carried paper. triplets governor dependent word also consecutive complete caption det) grouped single applies consecutive triplet standalone word remains unit substantial ambiguity linguistic structure parsing natural language data still ongoing research perfect solution. result always unavoidable errors parser output regardless chunking tool used. asides dependency parser used also look constituency parser. outputs subject predicate sentence directly chunk bird washes body water. bird body water bird washes water. bird body bird washes body water. lunch full variety foods. lunch full variety foods foods. lunch variety foods full foods. group women walk center tree-lined street. group women center tree-lined street women walk street. group center tree-lined street group women walk center street. limitations parser created unwanted variations across training data turn affect training effectiveness image captioning model. order reduce inﬂuences incorrect parsing model introduce reﬁnement strategy training phrase decoder decoder modify as-nps pair based local statistic training data. phrase decoder ﬁrst trained overall model. yields reasonable result generated training image. then contents as-nps pair modiﬁed based generated gradually restoring non-inferred ﬁrst word followed non-inferred last word. details reﬁnement algorithm shown algorithm chunked start word word ﬁrst words last words generated respectively. reﬁnement step comes play none generated start word ‘full’ start word ‘a’. example ﬁxed reﬁnement step word ‘standing’ inferred last word generated nps. example phrases front points index ﬁnger restored phrase decoder uses image alone context incapable generating word ‘one’ ‘points’ ‘ﬁnger’. three phrases correspond dominant entities within image thus seldom occur among captions similar images. fact ‘the one’ cannot generated image content alone needs subject previous context. hand word ‘camera’ inferred statistic training data captions ’looking camera’ images showing frontal view human. example shows case trained phrase decoder automatically decides entity kept based statistic training data. lunch full variety foods. lunch full variety foods foods. lunch variety foods full foods. blue shirt standing garden. blue shirt standing garden standing garden. blue shirt garden shirt standing garden. look toward camera front points index ﬁnger. camera front points index ﬁnger look toward camera points ﬁnger. camera look toward camera front points index ﬁnger. group women walk center tree-lined street. group women center tree-lined street women walk street. group center tree-lined street women walk street. proposed phi-lstm model tested three benchmark datasets flickrk flickrk ms-coco datasets consist images respectively. image annotated least image descriptions prepared human crowd sourcing. follow publicly available dataset splits used validation testing contains images flickrk flickrk datasets images mscoco dataset. rest images used training. including bilingual evaluation understudy recall-oriented understudy gisting evaluation metric evaluation translation explicit ordering consensus-based image description evaluation semantic propositional image caption evaluation evaluate quality generated image captions. bleu metric measures precision ngrams matching generated caption reference sentences rouge metric measures recall instead precision. here reported rouge-l uses longest common sequence instead n-grams. meteor aligns generated caption reference string mapping unigram using three different modules exact porter stem wordnet synonymy modules. ﬁnal score f-mean computed number unigram mapping. cider metric combines average cosine similarity n-gram generated caption references. gives lower weight n-grams commonly occur across reference captions dataset. lastly spice metric parses image caption references scene graph form tuples semantic proposition. then computes f-score deﬁned conjunction logical tuples. aside proposed phi-lstm model conducted experiment baseline model process image caption sequence words. basically reimplementation work described without ensembling multiple trained models using vggnet instead googlelenet encode image fair comparison model. experimental settings baseline model unless stated otherwise. training stage caption without preprocessing input language parser order appropriate as-nps pair. then words asnps pair converted lower case punctuations removed word occurs less times training data discarded tokenization image captions consistent avoid gradient explosion overlength caption truncate sentence speciﬁed table overlength truncate ﬁrst words instead last words latter part usually hold signiﬁcant semantic content. length as-nps pair considered reﬁnement step described section v-c. truncate length decided number captions affected less whole training data. encoder vgg- pre-trained imagenet classiﬁcation task without ﬁne-tuning parameters. lstm decoder hidden size employed. model optimized rmsprop using minibatch image-sentence pair iteration. learning rate dropout regularization employed avoid overﬁtting. testing stage found proposed model generates better caption large beam size baseline model works better small beam size overﬁtting stated thus compare model using beam size baseline model tested beam size multiple candidates last word generated keep candidates score higher predeﬁned threshold complete caption generation. examples generated shown fig. choose appropriate value examine changes several metrics sentence uniqueness generated captions using varying threshold value dataset example ms-coco dataset shown fig. observed n-grams metrics gradually increase threshold reach optimum t=-. flickrk flickrk datasets t=-. ms-coco dataset. increment yields different effect different n-grams metrics bleu cider decrease meteor rouge-l ﬂuctuate irregularly. besides that sentence uniqueness constantly reduces increment result less choice candidates. also notice much changes spice metric score ﬂuctuate within range across varying value ms-coco dataset. shows threshold value affects words’ order help much predicting correct objects attributes relations. table shows performance proposed model comparison current state-of-the-art models. compare methods encoder model performs better comparable state-ofthe-art models including phrase-based models proposed lebret ushiku note current model lower bleu- bleu- score higher bleu- bleu- score compared preliminary results published added length normalization beam search algorithm order generate longer caption. reported lower order bleu metrics bias towards short sentence especially brevity penalty thus increase most authors sota models table report brevity penalty bleu evaluation. nevertheless default setting publicly available code https//github.com/karpathy/neuraltalk https//github.com/tylin/coco-caption without brevity penalty. thus assume setting others used. average length generated caption better comparison models. that added reﬁnement strategy replace phrase selection objective predicting last word using softmax decoder observe approximately bleu score gained phrase reﬁnement algorithm. compared skel-key model similar architecture ours performance much better mainly three components. first employed better image model resnet- vgg. order cope large dimension fully convolutional resnet model lstm hidden layer dimension large skel-lstm attr-lstm level ms-coco dataset. secondly ﬁne-tuned image model parameters training. thirdly implemented attention mechanism generate image caption not. since objective work investigate capability phrase-based image captioning model compared similar model trained sequences implement attention mechanism provide extra information model beyond scope paper. nevertheless argue model comparable soft-attention model attention mechanism requires extra computation relative importance location feature maps every time step. review network relative importance location feature maps computed total eight review time steps using lstm outputs review lstm attended decoding image caption. finally interesting ﬁnding skel-key model performance gain skel-key model baseline model trained complete captions bleu rouge-l meteor cider metrics. contrary model outperforms baseline three datasets. state-of-the-art methods outperform require extra information input model cnnencoded feature. instance g-lstm provides semantic representation cross-modal retrieval model extra input lstm model acvt model requires training external module convert image attributes input decoder. region attention requires extraction image regions training objectness classiﬁer region computation relative importance image regions every time step. performance boosted using better image model additional scene-speciﬁc context model ensembling. lastly semantic attention model requires training attributes detector computation relative importance attribute every time step. evaluation spice metrics shown table observe improvements terms object attribute size color decoding image caption phrasebased hierarchical manner. improvements gained object level. essentially broken table performance phi-lstm state-of-the-art methods evaluated automatic metrics. stands n-gram bleu meteor rouge-l cider respectively. indicates results obtained ensembling multiple trained models refer without phrase reﬁnement respectively models mrnn deepvs lrcnn pbic cosmos phi-lstm baseline baseline phi-lstmv phi-lstmv state-of-the-art results using attention mechanism soft-atten hard-atten review skel-key state-of-the-art results using extra information extra information+attention mechanism g-lstm acvt reg-atten sem-atten generation process subsequences global sequence proposed model. therefore phrase decoder need shift time-scale generative process repeatedly focuses particular aspect image generating nps. note difference attention mechanism implemented provides guidance transit attention image region sequence spreads multiple time-scales. model attend image regions ﬁxes timescale subsequence decoder object level. nonetheless model global sequence mixed time-scales non-object phrases decoded multiple time steps sentence level. improvement terms object relations encoder used hold information regarding relative position objects. therefore object relations mostly inferred local statistic training data posture subject image. lastly cardinality measurement correctness terms object counting. score obtained baseline model indicate neither able count objects image encoder trained object recognition instead counting. nevertheless still small chances guess object counts correctly interaction multiple subjects class captured image. pointed multimodal rnn-based approach tends reconstruct previously seen captions hence compare model baseline terms uniqueness novelty generated captions. compute tabulate percentage unique captions generated percentage generated captions seen training data iii) average length captions number unique words generated table obtain upper bound performance measures evaluate human annotated captions test images reference. table measure caption uniqueness novelty. higher ‘seen’ percentage indicates generated captions less novel. number unique words captions shown ‘words’ ‘within vocab.’ considers words training corpus. baseline word less compared human annotated captions. experiment vocabulary size flickrk flickrk ms-coco datasets words respectively. therefore total out-of-vocabulary words test three datasets respectively would penalize automatic metrics used. assume within-vocabulary words reference captions upper bound test image relevant words well-trained image captioning model infer observe model baseline generate captions made around possible words. nevertheless number unique words generated using model still higher baseline longer average caption length. insights number occurrence word training corpus affects word prediction generating caption record least seen words inferred models shown table then examine manually caption contains words highlight words used correctly describing respective image either correct object action attribute. image-caption pair correctly inferred least seen words shown fig. examples. table phrase-based model generally able infer correctly words less seen compared baseline flickrk mscoco datasets. flickrk dataset baseline able infer word ‘snowboarding’ appears times training data ﬁrst image fig. nonetheless model inferred ‘snowboarder’ image furthermore record seen words absent generated captions model baseline table better understanding ﬁndings group list table according word usually appears caption. starting words salient image notice models able infer ‘sits’ ’child’ ‘several’ alternatives much probable. goes abstract scene ‘outside’ ‘area’ deﬁnite scene description probable. attribute ‘green’ inference challenging flickrk dataset green objects training data described word ‘green’ grass ﬁeld leaves etc. although model able infer green shirt’ cases image actually people covered leaves rather people wearing green shirt shown image fig. also observe model able generate word ‘an’ three datasets baseline model ms-coco dataset. important reason test ms-coco dataset contains objects starting vowels cases flickr datasets. nonetheless still attributes starting vowels orange shirt’ outdoor market’ flickr datasets shown fig. note generating caption phrase-based manner increases chance inference retain beam search. caption generated pure sequential model would result word inferred ﬁrst local statistic data greatly reduce chance attribute object starts vowel inferred next. applies word ‘there’ ms-coco dataset. since decoder word ﬁrst word better chance generating caption starting ‘there’. reasons model capable generating unique captions compared baseline. possessive pronoun ‘her’ ‘their’ inferred models flickr datasets usually appear human body parts salient human appear pets words gains higher probability inference. nevertheless dataset large ms-coco would solve problem. hand cardinal number ‘three’ much less chances obtain high probability score ‘two’. still model infer word ‘three’ mscoco dataset shown ﬁrst image fig. also found word ‘by’ used mostly preposition ms-coco dataset ‘next front ‘near’ preferable alternatives models. nonvisual words particle ‘up’ conjunction ‘from’ better chance inferred baseline model result longer generated caption. however models still incapable inferring conjunction ‘while’ ‘as’ mostly used describe multiple actions performed different individuals. applies word ‘one’ mostly used describe different individuals within group reason sentence structure might require attention mechanism image generate though researchers ﬁeld reported model capability developed best knowledge. presented phrase-based lstm model generate image caption hierarchical manner describe salient objects image ﬁrst generated complete caption formed nps. generated encoded compositional vector acts input time step sentence level. design allows decoded consistent timescale reducing variation time-scale sentence level. empirical results show image caption generated manner precise terms object attribute compared pure sequential model using words atomic unit. moreover hierarchical decoding process allows novel captions diverse word content generated. future work focus designing phrase-based bi-directional model image captioning. bernardi cakici elliott erdem erdem ikizler-cinbis keller muscat plank automatic description generation images survey models datasets evaluation measures journal artiﬁcial intelligence research vol. yang wang huang yuille deep captioning multimodal recurrent neural networks iclr karpathy fei-fei deep visual-semantic alignments generating image descriptions proceedings ieee conference computer vision pattern recognition donahue anne hendricks guadarrama rohrbach venugopalan saenko darrell long-term recurrent convolutional networks visual recognition description proceedings ieee conference computer vision pattern recognition kiros courville salakhudinov zemel bengio show attend tell neural image caption generation visual attention proceedings international conference machine learning shen wang dick hengel image captioning visual question answering based attributes external knowledge ieee transactions pattern analysis machine intelligence serban sordoni lowe charlin pineau courville bengio hierarchical latent variable encoder-decoder model generating dialogues thirty-first aaai conference artiﬁcial intelligence rashtchian young hodosh hockenmaier collecting image annotations using amazon’s mechanical turk naacl workshop creating speech language data amazon’s mechanical turk young hodosh hockenmaier from image descriptions visual denotations similarity metrics semantic inference event descriptions transactions association computational linguistics vol. vinyals toshev bengio erhan show tell lessons learned mscoco image captioning challenge ieee transactions pattern analysis machine intelligence vol. wang shen cohen cottrell skeleton image captioning skeleton-attribute decomposition proceedings ieee conference computer vision pattern recognition deng dong socher l.-j. fei-fei imagenet large-scale hierarchical image database computer vision pattern recognition cvpr ieee conference ieee manning surdeanu bauer finkel bethard mcclosky stanford corenlp natural language processing toolkit association computational linguistics system demonstrations papineni roukos ward w.-j. bleu method automatic evaluation machine translation proceedings annual meeting association computational linguistics. association computational linguistics szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition hinton srivastava swersky lecture overview mini– batch gradient descent coursera lecture slides https//class. coursera. org/neuralnets--/lecture srivastava hinton krizhevsky sutskever salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. banerjee lavie meteor automatic metric evaluation improved correlation human judgments proceedings workshop intrinsic extrinsic evaluation measures machine translation and/or summarization vol. vedantam lawrence zitnick parikh cider consensusbased image description evaluation proceedings ieee conference computer vision pattern recognition anderson fernando johnson gould spice semantic propositional image caption evaluation european conference computer vision. springer kulkarni premraj ordonez dhar choi berg berg babytalk understanding generating simple image descriptions ieee transactions pattern analysis machine intelligence vol. kulkarni berg berg choi composing simple image descriptions using web-scale n-grams proceedings fifteenth conference computational natural language learning. association computational linguistics yang daumé aloimonos corpus-guided sentence generation natural images proceedings conference empirical methods natural language processing. association computational linguistics mitchell dodge mensch goyal berg yamaguchi berg stratos daumé midge generating image descriptions computer vision detections proceedings conference european chapter association computational linguistics. association computational linguistics kuznetsova ordonez berg berg choi collective generation natural image descriptions proceedings association computational linguistics long papers-volume association computational linguistics kuznetsova ordonez berg choi treetalk composition compression trees image descriptions transactions association computational linguistics vol. socher karpathy manning grounded compositional semantics ﬁnding describing images sentences transactions association computational linguistics vol. gavves fernando tuytelaars guiding longshort term memory model image caption generation proceedings ieee international conference computer vision ushiku yamaguchi mukuta harada common subspace model similarity phrase learning caption generation images proceedings ieee international conference computer vision fang gupta iandola srivastava deng dollár mitchell platt from captions visual concepts back proceedings ieee conference computer vision pattern recognition", "year": 2017}