{"title": "$\\mathbf{D^3}$: Deep Dual-Domain Based Fast Restoration of  JPEG-Compressed Images", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "In this paper, we design a Deep Dual-Domain ($\\mathbf{D^3}$) based fast restoration model to remove artifacts of JPEG compressed images. It leverages the large learning capacity of deep networks, as well as the problem-specific expertise that was hardly incorporated in the past design of deep architectures. For the latter, we take into consideration both the prior knowledge of the JPEG compression scheme, and the successful practice of the sparsity-based dual-domain approach. We further design the One-Step Sparse Inference (1-SI) module, as an efficient and light-weighted feed-forward approximation of sparse coding. Extensive experiments verify the superiority of the proposed $D^3$ model over several state-of-the-art methods. Specifically, our best model is capable of outperforming the latest deep model for around 1 dB in PSNR, and is 30 times faster.", "text": "zhangyang wang† ding liu† shiyu chang† qing ling‡ yingzhen yang† thomas huang†∗ ‡department automation university science technology china hefei china {zwang dingliu chang yyang t-huang}illinois.edu paper design deep dual-domain based fast restoration model remove artifacts jpeg compressed images. leverages large learning capacity deep networks well problem-speciﬁc expertise hardly incorporated past design deep architectures. latter take consideration prior knowledge jpeg compression scheme successful practice sparsity-based dual-domain approach. design one-step sparse inference module efﬁcient lightweighted feed-forward approximation sparse coding. extensive experiments verify superiority proposed model several state-of-the-art methods. speciﬁcally best model capable outperforming latest deep model around psnr times faster. visual communication computing systems common cause image degradation arguably compression. lossy compression jpeg hevc-msp widely adopted image video codecs saving bandwidth in-device storage. exploits inexact approximations representing encoded content compactly. inevitably introduce undesired complex artifacts blockiness ringing effects blurs. usually caused discontinuities arising batch-wise processing loss highfrequency components coarse quantization artifacts degrade perceptual visual quality also adversely affect various low-level image processing routines take compressed images input practical image compression methods information theoretically optimal resulting compression code streams still possess residual redundancies ∗zhangyang wang thomas huang’s research works supported part army research ofﬁce grant number wnf--. qing ling’s research supported grant xda. makes restoration original signals possible. different general image restoration problems compression artifact restoration problem-speciﬁc properties utilized powerful priors. example jpeg compression ﬁrst divides image pixel blocks followed discrete cosine transformation every block. quantization applied coefﬁcients every block pre-known quantization levels moreover compression noises difﬁcult model common noise types. contrast tradition assuming noise white signal independent non-linearity quantization operations makes quantization noises non-stationary signal-dependent. various approaches proposed suppress compression artifacts. early works utilized ﬁlteringbased methods remove simple artifacts. data-driven methods considered avoid inaccurate empirical modeling compression degradations. sparsitybased image restoration approaches discussed produce sharpened images often accompanied artifacts along edges unnatural smooth regions. et.al. proposed sparse coding process carried jointly pixel domains simultaneously exploit residual redundancies jpeg code streams sparsity properties latent images. recently dong ﬁrst introduced deep learning techniques problem speciﬁcally adapting sr-cnn model however incorporate much problem-speciﬁc prior knowledge. time constraint often stringent image video codec post-processing scenarios. low-complexity even real-time attenuation compression artifacts highly desirable inference process traditional approaches example sparse coding usually involves iterative optimization algorithms whose inherently sequential structure well data-dependent complexity latency often constitute major bottleneck computational efﬁciency deep networks beneﬁt feed-forward structure enjoy much faster inference. however maintain competitive performances deep paper focus removing artifacts jpeg compressed images. major innovation explicitly combine prior knowledge jpeg compression scheme successful practice dual-domain sparse coding designing task-speciﬁc deep architecture. furthermore introduce one-step sparse inference module acts highly efﬁcient light-weighted approximation sparse coding inference also reveals important inner connections sparse coding deep learning. proposed model named deep dual-domain based fast restoration proves effective interpretable general deep models. gains remarkable margins several state-of-the-art methods terms restoration performance time efﬁciency. work inspired prior wisdom previous works restored compressed images either pixel domain domain solely. however isolated quantization error single coefﬁcient propagated pixels block. aggressively quantized coefﬁcient produce structured errors pixel-domain correlate latent signal. hand compression process sets high frequency coefﬁcients zero making impossible recover details domain. view complementary characteristics dualdomain model proposed spatial redundancies pixel domain exploited learned dictionary residual redundancies domain also utilized directly restore coefﬁcients. quantization noises suppressed without propagating errors. ﬁnal objective combination dctpixel-domain sparse representations could cross validate other. date deep learning shown impressive results high-level low-level vision problems sr-cnn proposed dong showed great potential end-to-end trained networks image super resolution recent work proposed four-layer convolutional network tuned based sr-cnn named artifacts reduction convolutional neural networks effective dealing various compression artifacts. authors leveraged fast trainable regressors constructed feed-forward network approximations learned sparse models. turning sparse coding deep networks expect faster inference larger learning capacity better scalability. similar views adopted develop ﬁxed-complexity algorithm solving structured sparse robust rank models. paper summarized methodology deep unfolding. proposed deeply improved sparse coding incarnated end-to-end neural network. lately proposed deep encoders model sparse approximation feed-forward neural networks. further extended task-speciﬁc strategy graphregularized approximation. task-speciﬁc architecture shares similar spirits works. ﬁrst review sparsity-based dual-domain restoration model established considering training uncompressed images pixel-domain blocks {ˆxi} patch; jpeg) input dictionaries rm×pφ rm×pψ constructed training data {yi} {ˆxi} pixel domains respectively locally adaptive feature selection projection. following optimization model solved testing stage coefﬁcient block sparse codes pixel domains respectively. denotes inverse discrete cosine transform operator. positive scalars. noteworthy point inequality constraint represents quantization intervals according jpeg quantization table constraint incorporates important side information conﬁnes solution space. finally provides estimate original uncompressed pixel block ˆxt. sparsity-based dual-domain model exploits residual redundancies domain without spreading errors pixel domain time recovers high-frequency information driven large training set. however note inference process relies iterative algorithms computational expensive. also three parameters manually tuned. authors simply equal hamper performance. addition dictionaries individually learned patch allows extra ﬂexibility also brings heavy computation load. figure illustration deep dual-domain based model black solid lines denote network inter-layer connections black dash lines connect loss functions. dash-line boxes depict stages incorporate pixel domain sparsity priors respectively. grey blocks denote constant idct layers respectively. notations within parentheses along pipeline remind corresponding variables note above correspond intermediate outputs variables order help understand close analytical relationship proposed deep architecture sparse coding-based model. necessarily imply exact numerical equivalence since allows end-to-end learning parameters however experiments enforcement speciﬁc problem structure improves network performance efﬁciency remarkably. addition relationships remind deep model could well initialized sparse coding components. implementation analysis synthesis modules appears core synthesis process naturally feed-forward multiplying dictionary less straightforward transform sparse analysis process feed-forward network. learned ista parameterized encoder further proposed natural network implementation ista. authors time-unfolded truncated ﬁxed number stages jointly tuned parameters training data good feed-forward approximation sparse inference. similar unfolding methodology lately exploited training pixel-domain blocks {xi} after jpeg compression well original blocks {ˆxi}. testing input compressed block goal estimate original using redundancies pixel domains well jpeg prior knowledge. illustrated fig. input ﬁrst transformed coefﬁcient block feeding constant matrix layer subsequent layers enforce domain sparsity refer concepts analysis synthesis dictionaries sparse coding sparse coding analysis module implemented solve following type sparse inference problem domain sparse coding synthesis module outputs dct-domain sparsity-based reconstruction i.e. intermediate output constrained auxiliary loss encodes inequality constraint design following signaldependent box-constrained loss ||]+|| φα]+|| note takes also inputs since actual jpeg quantization interval depends operator keeps nonnegative elements unchanged setting others zero. eqn. thus penalize coefﬁcients falling quantization interval. constant idct matrix layer dctdomain reconstruction transformed back pixel domain sparse representation. analysis module solves γ||β|| work launch aggressive approximation keeping iteration leading onestep sparse inference module. major motivation lies observation overly deep networks could adversely affect performance low-level vision tasks. note analysis modules original lista applies synthesis modules even iterations kept six-layer network suffers difﬁculties training fragility generalization task. eqn. indicates original neuron trainable thresholds decomposed linear scaling layers plus unit-threshold neuron. weights scaling layers diagonal matrices deﬁned elementwise reciprocal respectively. unit-threshold neuron could essence viewed double-sided translated variant relu figure illustration analysis synthesis modules. former implemented proposed module fully-connected layers diag diag denotes diagonal scaling layers. related form obtained different case non-negative sparse coding. authors studied connections soft-threshold feature classiﬁcation correlate network architectures. model overview plugging module ready obtain analysis synthesis modules fig. comparing fig. eqn. easy notice analytical relationships well fact network hyperparamters could well initialized sparse coding parameters could obtained easily. entire model consisting four learnable fully-connected weight layers trained fig. intentionally combine layer reason still wish keep layers tied element-wise reciprocal. proves positive implications experiments. absorb diagonal layers fig. reduced fully connected weight matrices concatenated layer hidden neurons however keeping decomposed model architecture facilitates incorporation problem-speciﬁc structures. obvious sparse coding inference dramatically higher time complexity. also interested inference time complexity competitive deep models especially ar-cnn fully convolutional architecture total complexity training deep learning gradient descent scales linearly time space number training samples. primarily concerned time complexity testing relevant practical usages. since learnable layers model fully-connected inference process theoretical time complexities represent actual running time depend different conﬁgurations sensitive implementations hardware. actual running time scales nicely theoretical results. disjoint training test bsds database training set; validation used validation follows training model ﬁrst divide original image overlapped patches subtract pixel values jpeg mean shifting process. perform jpeg encoding matlab jpeg encoder speciﬁc quality factor generate corresponding compressed samples. whereas jpeg works non-overlapping patches emphasize training patches overlapped extracted arbitrary positions. testing image sample blocks stride apply model patch-wise manner. patch misaligns original jpeg block boundaries similar coding block local neighborhood whose quantization intervals applied misaligned patch. practice effective important removing blocking artifacts ensuring neighborhood consistency. ﬁnal result obtained aggregating patches overlapping regions averaged. proposed networks implemented using cudaconvnet package apply constant learning rate batch size momentum. experiments workstation intel xeon .ghz cpus gpu. losses equally weighted. parameters table ﬁxed different values experiments. based solved eqn. could initialize domain block fig. pixel domain block respectively. practice initialization strategy beneﬁts performances usually leads faster convergence. test quality factor train dedicated model. easy-hard transfer suggested useful. images values contain complex artifacts helpful features learned images high values starting point. practice ﬁrst train model jpeg compressed images initialize model model similarly initialize model one. sparsity-based dual-domain method could viewed shallow counterpart outperformed traditional methods dictv thus compare again. algorithm parameters manually tuned. especially dictionary atoms adaptively selected nearest-neighbour type algorithm; number selected atoms varies every testing patch. therefore parameter complexity cannot exactly computed. ar-cnn latest deep model resolving jpeg compression artifact removal problem. authors show advantage sa-dct sr-cnn adopt default network conﬁguration authors adopted easy-hard transfer training. test resulting models denoted respectively. addition verify superiority task-speciﬁc design construct fully-connected deep baseline model complexity named dbase-. consists four weight matrices dimensions d-’s four trainable layers. d-base- utilizes relu neurons dropout technique. images live dataset evaluate quantitative qualitative performances. three quality assessment criteria psnr structural similarity psnr-b evaluated last designed speciﬁcally from common experiences choosing dictionary sizes d-base- four-layer neural network performed pixel domain without dct/idct layers. diagonal layers contain small portion parameters ignored here. compared gain remarkable advantages thanks end-to-end training deep architectures. grow observes clear improvements psnr/ssim/psnr-b. outperformed state-of-the-art arcnn around psnr. moreover also demonstrates notable performance margin d-base- although possess number parameters. thus veriﬁed beneﬁt task-speciﬁc architecture inspired sparse coding process rather large learning capacity generic deep models. parameter numbers different models compared last table impressive also takes less parameters ar-cnn. display three groups visual results bike monarch parrots images figs. respectively. ar-cnn tends generate oversmoothness edge regions butterﬂy wings parrot head. capable restoring sharper edges textures. models reduce unnatural artifacts occurring results. especially results still suffer small amount visible ringing artifacts shows superior preserving details also suppresses artifacts well. attribute impressive recovery clear details combination speciﬁc pipeline initialization box-constrained loss. task-speciﬁc interpretable pipeline beneﬁts speciﬁcally designed architecture demonstrated comparison experiments baseline encoders. further provide intermediate outputs idct layer i.e. recovery dct-domain reconstruction. hope helps understand component i.e. dctdomain reconstruction pixel-domain reconstruction contributes ﬁnal results. shown fig. figure intermediate comparison results bike monarch parrot intermediate recovery results dct-domain reconstruction; results trained random initialization; results trained without box-constrained loss. psnr values reported. intermediate reconstruction results contain sharpened details become recognizable) unexpected noisy patterns blockiness ringing-type noise along edges textures). implies stage dct-domain reconstruction enhanced high-frequency features introducing artifacts simultaneously quantization noises. afterwards stage pixel-domain reconstruction performs extra noise suppression global reconstruction leads artifact-free visually pleasing ﬁnal results. sparse coding-based initialization conjecture reason capable restoring text bike subtle textures hinges sparse coding-based initialization important training detail verify that re-train random initialization testing results fig. turn visually smoother example characters hardly recognizable. notice results original fig. also presented sharper recognizable texts details ar-cnn. observations validate conjecture. next question sparse coding helps signiﬁcantly here? quantization process considered low-pass ﬁlter cuts high-frequency information. dictionary atoms learned ofﬂine highquality training images contain rich high-frequency information. sparse linear combination atoms thus richer high-frequency details might necessarily case generic regression box-constrained loss loss acts another effective regularization. re-train without loss obtain results fig. observed box-constrained loss helps generate details fig. bounding coefﬁcients brings psnr gains. image video codecs desire highly efﬁcient compression artifact removal algorithms post-processing tool. traditional digital cinema business uses frame rate standards emerging standards require much higher rates. example high-end high-deﬁnition systems adopt ultra-hd standard advocates p/.p/p; hevc format could reach maximum frame rate higher time efﬁciency desirable improved performances. compare averaged testing times ar-cnn proposed models table live dataset using machine software environment. running time collected tests. best model takes approximately image; times faster ar-cnn. speed difference mainly caused different implementations. completely feed-forward ar-cnn relies time-consuming convolution operations takes matrix multiplications. accordance theoretical time complexities computed too. result able process image sequences best knowledge fastest among state-of-the-art algorithms proves practical choice hdtv industrial usage. introduce model fast restoration jpeg compressed images. successful combination jpeg prior knowledge sparse coding expertise made highly effective efﬁcient. future extend methodology related applications. srivastava hinton krizhevsky sutskever salakhutdinov. dropout simple prevent neural networks overﬁtting. jmlr wang bovik sheikh simoncelli. image quality assessment error visibility structural similarity.", "year": 2016}