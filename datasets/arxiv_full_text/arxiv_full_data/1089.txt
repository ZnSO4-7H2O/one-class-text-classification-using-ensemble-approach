{"title": "Fatiguing STDP: Learning from Spike-Timing Codes in the Presence of Rate  Codes", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Spiking neural networks (SNNs) could play a key role in unsupervised machine learning applications, by virtue of strengths related to learning from the fine temporal structure of event-based signals. However, some spike-timing-related strengths of SNNs are hindered by the sensitivity of spike-timing-dependent plasticity (STDP) rules to input spike rates, as fine temporal correlations may be obstructed by coarser correlations between firing rates. In this article, we propose a spike-timing-dependent learning rule that allows a neuron to learn from the temporally-coded information despite the presence of rate codes. Our long-term plasticity rule makes use of short-term synaptic fatigue dynamics. We show analytically that, in contrast to conventional STDP rules, our fatiguing STDP (FSTDP) helps learn the temporal code, and we derive the necessary conditions to optimize the learning process. We showcase the effectiveness of FSTDP in learning spike-timing correlations among processes of different rates in synthetic data. Finally, we use FSTDP to detect correlations in real-world weather data from the United States in an experimental realization of the algorithm that uses a neuromorphic hardware platform comprising phase-change memristive devices. Taken together, our analyses and demonstrations suggest that FSTDP paves the way for the exploitation of the spike-based strengths of SNNs in real-world applications.", "text": "represent relative timing events second massively distributed neuronal synaptic computations performed asynchronously third network exploit full information embedded structure spike timing sequences apart mean rate. addition artiﬁcial snns implementable compact low-power neuromorphic hardware. hence snns physical implementations actively researched alternative conventional anns fig. integrate-and-ﬁre neuron equipped array plastic synapses stdp learning rule used detect temporal correlations event-based data streams. correlations individual input streams inferred steady-state distribution synaptic weights ﬁring activity neuron. snns could play central role domain unsupervised learning computational task data analytics either stand-alone technique pre-training phase subsequent supervised learning many today’s big-data sources lack labeled samples reliable training sets needed supervised learning. information carried temporal structure streams events occur naturally generated event-based biological artiﬁcial monitoring systems exploited snns unsupervised spike-timing-dependent plasticity rules based temporal relationship postsynaptic activity i.e. postsynaptic spikes related variables presynaptic input. instance integrate-andﬁre neuron used learn temporal correlations event-based data streams shown fig. abstract—spiking neural networks could play role unsupervised machine learning applications virtue strengths related learning temporal structure event-based signals. however spike-timing-related strengths snns hindered sensitivity spike-timingdependent plasticity rules input spike rates temporal correlations obstructed coarser correlations ﬁring rates. article propose spike-timingdependent learning rule allows neuron learn temporally-coded information despite presence rate codes. long-term plasticity rule makes short-term synaptic fatigue dynamics. show analytically that contrast conventional stdp rules fatiguing stdp helps learn temporal code derive necessary conditions optimize learning process. showcase effectiveness fstdp learning spike-timing correlations among processes different rates synthetic data. finally fstdp detect correlations real-world weather data united states experimental realization algorithm uses neuromorphic hardware platform comprising phase-change memristive devices. taken together analyses demonstrations suggest fstdp paves exploitation spike-based strengths snns real-world applications. spiking neural networks algorithms underlie brain’s function artiﬁcial implementations viewed emerging generation artiﬁcial neural networks could enable efﬁcient solutions tasks machine intelligence. evidence snns’ potential previous generations anns loosely based biological ones recently exploited increases available computational resources large annotated datasets provide record-breaking performance several machinelearning tasks image speech recognition naturallanguage processing playing complex games data analytics scientiﬁc business purposes conventional anns achieve impressive results following supervised learning rules receiving input communicating neurons discrete time-steps real-valued signals. contrast networks snns brain learn largely unsupervised need uncover structure natural data provided without labels. despite lack supervision biological snns perform variety tasks levels unsurpassed unsupervised systems. characteristic underlies difference performance input communication snns handled streams binary events called spikes. allows ﬁrst time event arrives spike corresponding synapse postsynaptic potential generated added membrane potential neuron. temporal correlations presynaptic input spikes neuronal ﬁring events result evolution synaptic weights feedbackdriven competition among synapses. steady state correlations individual input streams inferred distribution synaptic weights resulting ﬁring activity postsynaptic neuron. problem fully exploiting spike-timingrelated strengths snns stdp algorithms highly sensitive rate input spikes addition relative timing individual spikes data streams simultaneously temporal coding input streams carry additional information rate-coded i.e. carried rate occurrence events learning temporally-coded information obstructed. thus detecting temporal correlations presence rate codes challenge addressed. address problem present learning rule combines long-term stdp dynamics mechanism short-term synaptic fatigue dynamics. first motivate present description proposed learning rule call fatiguing stdp fstdp. subsequently present analytical study fstdp enables learning spiketiming codes stdp not. present simulation study using synthetic data shows effectiveness fstdp detecting low-rate correlated poisson processes among processes different rates. finally illustrate concept real-world event-based rainfall data weather stations united states america. show fstdp used learn temporal correlations spite signiﬁcant differences rate rainfall across stations. demonstrate experimental realization algorithm memristive neuromorphic platform synaptic weights stored modiﬁed in-situ phase-change memory devices. fatiguing spike-timing-dependent plasticity section start formalizing task learning spike-timing codes problem coexisting rate codes motivate learning rule propose introduce describe rule. learning task spike-timing code consists learning detect temporal correlations across streams carry code. therefore neurons need learn associate strongly covarying inputs. well illustrated discovery clusters high covariance covariance matrix input synapse pairs stdp neuron approximates potentiating i.e. strengthening depressing i.e. weakening synapses whose input shows high covariance respectively neuron’s output i.e. weighted input streams. good approximation inputs covary strongly neuron’s output drive likely covary strongly another. covariances neuron’s stdp synapses compare inputs directly neuron’s output neuron’s learning sensitive speciﬁcally uncentered covariance inputs given input streams addition covariances introduced correlations timings individual spikes covariances introduced correlations rates inputs rate-induced covariances dominate uncentered covariance spurious correlations individual spike timings added slower covarying rates. detect fast covariances presence slow ones individual spikes high-rate channels must contribute less computed covariance low-rate channels. based this introduce measure covariance normalized means inputs case spike trains mean ﬁring rates. call measure normalized covariance therefore neuron learn spike-timing code presence rate code modiﬁed stdp rule needed. rule approximate learning covariances rate-normalized inputs cnorm learning covariances cnorm rate-normalized inputs neuron’s output. introduce stdp rule achieves including component normalizes postsynaptic contributions presynaptic spike increasing function recent rate. call rule fatiguing stdp ratenormalization component synaptic fatigue mechanism combine stdp. synaptic fatigue form short-term plasticity speciﬁcally short-term depression. changes synaptic efﬁcacy transient manner. observed biological synapses depletion neurotransmitters presynaptic terminal transmitted spike subsequent gradual replenishment efﬁciently implemented cmos-based neuromorphic circuits synapses capable long-term plasticity. denotes stored synaptic weight function depends time arrival presynaptic spikes. absence spikes tends zero thus instance implemented spike-based function increases value ﬁxed amount upon arrival presynaptic spike decays exponentially. postsynaptic effect decreased. contrast stdp longterm plasticity mechanism changes synaptic weight permanently. synaptic weight changes depending prepostsynaptic spikes shown schematically fig. fig. synaptic fatigue strongly motivated ﬁndings depletion synaptic neurotransmitter vesicles induces form short term depression synapses throughout nervous system fig. introduction synaptic fatigue synaptic efﬁcacy given schematic illustration synaptic weight change stdp characteristic evolution function response presynaptic spikes. section present analysis fstdp versus stdp. start general conditions need satisﬁed learning rule neuron learn spiketiming code show fstdp provide stdp cannot. consider spatio-temporally coded spike trains sequences spikes contain temporal correlations sequences. hebbian learning tasks neuron input synapses successfully learns code weights ncor synapses receive temporally correlated input sequences potentiated nuncor synapses receive uncorrelated sequences depressed. requires expected value weight updates temporally correlated synapses positive i.e. probability potentiation exceed probability depression vice versa uncorrelated synapses uniform prepostsynaptic spike trains uncorrelated balance perturbed causal effect prepostsynaptic spikes. speciﬁcally added probability causalp presynaptic spike synapse bring postsynaptic neuron ﬁring threshold causing postsynaptic spike. caused spike occurs delay delay given temporal dynamics excitatory postsynaptic potential caused spike. epsp immediate instantaneous delay. added causalp renormalization factor probability density keeping integral equal addition causal term; dirac delta function. precise value essential rest analysis. then becomes shown inequality ratio causalpuncor causalpcor understanding learning temporal code facilitated stdp. express causalp causalpi probability presynaptic spike belongs synapse fig. learning spike rate correlations fstdp stdp. probabilities stdp fstdp function input rate synapse probability causalpi stdp fstdp. fstdp presynaptic spike timing correlations cause postsynaptic spikes stdp dominated high rates section present simulation study investigate effectiveness fstdp detecting correlated streams binary events real time adverse condition correlated streams characterized signiﬁcantly lower frequencies rest uncorrelated inputs. addition contrast learning fstdp learning stdp. generated poisson binary processes correlated correlation coefﬁcient rest uncorrelated. correlated processes mean rate uncorrelated processes higher mean rate sample data depicted fig. substantially lower rate ﬁrst processes clearly visible. validate results stdp fstdp algorithms quantitatively computed uncentered normalized covariance matrices input streams respectively used deﬁne objective correlation detection task learning rules best suited for. predict outcome learning computed covariances normalized covariances input stream mean input postsynaptic neuron’s ﬁring threshold efﬁcacy synapse factor accounting excitatory postsynaptic potential caused spikes synapses coincide spike question temporal correlations across synapses. increases increasing correlation synapses synapse assumption linear increase membrane potential integrated synaptic inputs standard stdp rules independent equals weight therefore constant value causalpi rescaled version increases hence causalpi higher synapses higher rates explains stdp sensitive input ﬁring rates rates correlated inputs equal higher uncorrelated ones causalpi higher correlated inputs stdp fstdp cases blue lines labeled dp). component increases correlations correlated inputs increase suggesting stdp sufﬁces learn temporal code case. however rates uncorrelated inputs much higher rates correlated inputs bluevs red-shaded region respectively) then stdp causalpi higher uncorrelated inputs. case fstdp even though independent decreases rate threshold fatigue effect also decreases. result decreases high rates reaching minimum value pic. therefore fstdp causalpi curve increases reach maximum decreases reach minimum pic. rate causalpi maximum depends rth. slope decrease correspond parameters synaptic fatigue component fstdp. speciﬁcally spike-based implementation fatigue time constant fatigue transient determines increase fatigue upon presynaptic spike determines slope decreasing part causalpi curve. fstdp reduce causalpi enough high-rate uncorrelated inputs satisfy increases fig. sample synthetic data used. uncentered covariance matrix. correlations low-rate inputs obscured high rates channels. normalized covariance matrix revealing correlations low-rate channels. matrices off-diagonal elements shown. covariance normalized covariance input spike train average inputs. weight distribution learning stdp. high uncorrelated rates cause stdp fail detect correlated inputs. learning fstdp leads clear separation correlated uncorrelated synapses correlated ones potentiated uncorrelated ones depressed. synapses leaky integrate-and-ﬁre neuron synaptic weights changed according pre-deﬁned spiketiming-dependent rule plasticity. synaptic weights bounded zero initialized middle values training. trained network standard stdp also trained fstdp compared results. presence synaptic fatigue difference instantiations simulation i.e. relevant parameters ﬁrstly jump synaptic fatigue presynaptic spike equal time constant exponential decay synaptic fatigue equal time steps. learning stdp fails detect temporally correlated inputs mostly strengthens high-rate synapses instead result shows stdp detects spurious correlations introduced high rates salient uncentered covariance matrix. fstdp coincidences across input channels rare relative corresponding input rates drive postsynaptic ﬁring learning. result predicted high values normalized covariance lowrate inputs average input fstdp leads successful potentiation correlated inputs matching well target normalized covariance matrix coefﬁcients used generating data. section present application fstdp correlation detection weather patterns. also present implementation network neuromorphic platform phase-change memory devices serve synaptic elements. weather data obtained national oceanic atmospheric administration database quality-controlled local climatological data. provides hourly summaries climatological data approximately weather stations united states america. measurements obtained -month period january june generated binary stochastic process weather station. rained given period hour particular geographical location corresponding weather station process takes value else selected stations included locations scarce correlated rainfall frequent uncorrelated rainfall. groups stations found k-means clustering. event-based data streams presented fig. shown fig. compute normalized covariance matrix possible identify highly correlated weather stations spite rate rainfall. performed experiment obtain result using spiking neural network equipped fstdp. experiment performed neuromorphic hardware platform based phase-change memory chip containing million devices. device accessed thin-oxide n-type ﬁeld-effect transistor optical micrograph chip schematic device shown fig. mushroom-type devices based doped gesbte sub-lithographically deﬁned bottom electrode radius addition cell array chip integrates circuitry cell addressing on-chip analog-to-digitalconverter cell readout voltagecurrentfig. depiction event-based data streams based rainfall information weather stations across continental usa. station hour rain marked black. normalized covariance matrix visualizes low-rate highly correlated weather stations. mode cell programming chip connected analog-front-end board contains number digital-to-analog-converters adcs along discrete electronics power supplies voltage current reference sources. ﬁeld-programmable-gate-array board embedded processor ethernet connection used implement higher-level routines associated experiments. synaptic weight stored conductance states devices experimental sequence shown schematically fig. results experiments presented fig. normalized synaptic weights experiment given fig. synaptic weight distribution shows using fstdp able identify weather stations highly correlated rainfall. fig. gives geographical location weather stations; stations correlated rainfall shown red. would expect temporal correlations rainfall also corresponds geographical proximity. moreover weather stations located california known relatively levels precipitation compared regions fig. phase-change memory chip fabricated cmos technology contains million devices based doped gesbte phase-change material. chart experiment. tasks shown blue rectangle done hardware whereas tasks done software. fig. histograms correlated uncorrelated synaptic weights learning show stdp detect correlated weather stations. synaptic weights stored conductance states devices hardware platform. illustration geographical location weather stations found temporally correlated according learned synaptic weights. experiments highlight applicability type learning algorithms realistic data sets well feasibility neuromorphic hardware memristive synapses realize learning in-situ efﬁcient manner. even though particular demonstration used devices mostly synaptic weight storage weight updates could foresee development memristive devices also short-term synaptic dynamics could realized using device physics presented analyzed demonstrated effectiveness fstdp i.e. learning rule spiking neural networks allows learning temporal codes spike-timing codes presence slower codes rate codes. regard fstdp compares favorably unsupervised spike-based learning rules face problems learning spatiotemporal structure spiking inputs show theoretically experimentally. rule local event-based. despite spatially temporally local event-based nature rule gives rise global long-term learning effects network uses lends efﬁciency well-aligned scope snns. dynamics emerge combination eventbased component. despite simplicity show overcomes signiﬁcant limitations unsupervised learning rules snns face learning spike-timing codes presence rate codes. potential approaches would address issue partly. instance setting postsynaptic neuron’s time constant leak match time constant temporal correlations would able address high enough ﬁring rates introduce uncorrelated spikes comparable intervals. alternatively probabilistically down-sampling spikes high-rate input channels would reduce postsynaptic effect high-rate channels would also collaterally omit spikes form spiketiming code hindering task learning them. another alternative mechanism would target learning issue would introduction fatigue synaptic efﬁcacy fstdp directly weight updates weight updates smaller frequent. however would generically prevent learning high-rate channels disallowing potentiation depression regardless spike-timing correlations channels. neural networks time encodes timing events learn operate real time. spike timings embed information neural code additional mean ﬁring rate effectively increasing bandwidth communication neurons network compared single analog value traditional anns. furthermore asynchronous event-based nature combined dual role synapses memory computational elements snns well-suited efﬁcient hardware implementations massively-parallel post-von-neumann computing architectures. however potential advantages stem spiking events hard exploit real-world applications largely lack algorithms effectively learn extract useful information arbitrary complexity eventbased codes. particularly true structure underlying complexity training data unknown network’s user algorithm must learn unsupervised manner. fstdp addresses signiﬁcant source complexity neural code results differences timescales useful distracting information embedded spatiotemporal spike patterns. paves applications unsupervised learning real-world temporally-coded data rarely isolated rate codes. furthermore well variations fstdp employed biological nervous system. firstly spiketiming-dependent plasticity synaptic fatigue experimentally observed synapses connecting biological neurons. moreover brain likely evolved exploit large potential information content fast-varying temporal codes. codes need operate presence slower temporal codes varying ﬁring rates encoding sensory stimuli motor outputs large-scale neural activity rhythms state-dependent responses stimuli. based existence fstdp’s constituent elements brain’s likely need learn temporal codes presence rate codes plausible fstdp used brain. limitation work present concerns implementation fstdp neuromorphic hardware. storage weights weighted transmission presynaptic spikes postsynaptic neuron updates stored weight accommodated neuromorphic hardware. however overall synaptic efﬁcacy hybrid software hardware fatigue component synaptic dynamics simulated software. effectiveness computational power fstdp motivate development future pure hardware fstdp implementations. second limitation experimental demonstrations cover full scope fstdp. instance fstdp could used correlation detection also unsupervised classiﬁcation sequence generation dimensionality reduction tasks. furthermore network used comprised single postsynaptic neuron certain applications require complex network architectures. nevertheless successful detection temporal correlations synaptic level computational primitive tasks relevant network architectures clearly demonstrated examples. rule handle type data include spatiotemporal correlations. conclusion analyses demonstrations fstdp learning rule suggest enable efﬁcient software hardware snns variety applications learning real-world temporal data. would like thank team research zurich particular nikolaos papandreou technical assistance lorenz m¨uller institute neuroninformatics insightful discussion. would also like acknowledge partial ﬁnancial support swiss national science foundation. maass networks spiking neurons third generation neural network models neural networks vol. merolla million spiking-neuron integrated circuit scalable communication network interface science vol. tsodyks markram neural code neocortical pyramidal neurons depends neurotransmitter release probability proceedings national academy sciences vol. rosenbaum rubin doiron short term synaptic depression imposes frequency dependent ﬁlter synaptic information transfer plos computational biology vol. qiao mostafa corradi osswald stefanini sumislawska indiveri reconﬁgurable on-line learning spiking neuromorphic processor comprising neurons synapses frontiers neuroscience vol. breitwisch nirschl chen lamorey burr joseph schrott philipp novel lithographyindependent pore phase change memory ieee symposium vlsi technology. wang joshi savel/’ev jiang midya strachan barnell g.-l. williams yang memristors diffusive dynamics synaptic emulators neuromorphic computing nature materials vol. advance online publication", "year": 2017}