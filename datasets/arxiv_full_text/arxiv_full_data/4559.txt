{"title": "Feature Selection for MAUC-Oriented Classification Systems", "tag": ["cs.LG", "cs.AI"], "abstract": "Feature selection is an important pre-processing step for many pattern classification tasks. Traditionally, feature selection methods are designed to obtain a feature subset that can lead to high classification accuracy. However, classification accuracy has recently been shown to be an inappropriate performance metric of classification systems in many cases. Instead, the Area Under the receiver operating characteristic Curve (AUC) and its multi-class extension, MAUC, have been proved to be better alternatives. Hence, the target of classification system design is gradually shifting from seeking a system with the maximum classification accuracy to obtaining a system with the maximum AUC/MAUC. Previous investigations have shown that traditional feature selection methods need to be modified to cope with this new objective. These methods most often are restricted to binary classification problems only. In this study, a filter feature selection method, namely MAUC Decomposition based Feature Selection (MDFS), is proposed for multi-class classification problems. To the best of our knowledge, MDFS is the first method specifically designed to select features for building classification systems with maximum MAUC. Extensive empirical results demonstrate the advantage of MDFS over several compared feature selection methods.", "text": "feature selection important pre-processing step many pattern classiﬁcation tasks. traditionally feature selection methods designed obtain feature subset lead high classiﬁcation accuracy. however classiﬁcation accuracy recently shown inappropriate performance metric classiﬁcation systems many cases. instead area receiver operating characteristic curve multi-class extension mauc proved better alternatives. hence target classiﬁcation system design gradually shifting seeking system maximum classiﬁcation accuracy obtaining system maximum auc/mauc. previous investigations shown traditional feature selection methods need modiﬁed cope objective. methods often restricted binary classiﬁcation problems only. study ﬁlter feature selection method namely mauc decomposition based feature selection proposed multiclass classiﬁcation problems. best knowledge mdfs ﬁrst method speciﬁcally designed select features building classiﬁcation systems maximum mauc. extensive empirical results demonstrate advantage mdfs several compared feature selection methods. feature selection important data pre-processing technique machine learning data mining community selecting feature subset original feature time storage requirements classiﬁcation tasks reduced. addition reducing number features facilitate data visualization understanding even improve performance classiﬁcation systems generally speaking feature selection methods divided categories i.e. ﬁlter wrapper ﬁlter method whole selection procedure conducted solely based data set. hand wrapper method employs classiﬁer used classiﬁcation task afterward evaluate merit candidate feature subset. well known that general ﬁlter methods computationally eﬃcient wrapper methods lead better classiﬁcation performance speciﬁc classiﬁer recent years emergence many large-scale problems involve thousands features text classiﬁcation eﬃciency feature selection methods become greater concern researchers practitioners. therefore ﬁlter methods although sometimes leading inferior classiﬁcation performance attracting increasing interest. ﬁlter methods since classiﬁer used evaluate candidate feature subsets alternative metrics needed evaluate utility. consideration computational eﬃciency many so-called feature ranking methods evaluate utility individual features pick ones. fisher’s ratio pearson’s correlation coeﬃcient chi-square information gain symmetrical uncertainty distance discriminant utilized metrics purpose. addition measuring utility individual features relief methods take interaction features local characteristics sample space consideration thus likely obtain good feature subset rather good individual features. however methods suﬀer selecting redundant features provide additional information cause computation time classiﬁcation. address disadvantage recent ﬁlter methods minimal redundancy maximal relevance fast correlation based filter methods equipped schemes exclude redundant features. since methods usually involve calculating relevance pairs features major payoﬀ much higher computational cost. good feature selection method eﬃcient also guarantee high classiﬁcation performance. traditionally means feature selection process select feature subset leads high classiﬁcation accuracy. state-of-the-art methods including mentioned above demonstrated eﬀective regards objective. however recent progresses machine learning area application domains revealed accuracy necessarily good performance metric first using accuracy assumes prior probabilities diﬀerent classes data sets approximately equal case many real-world applications second using accuracy assumes diﬀerent types misclassiﬁcations induce cost hold many real-world applications address shortcomings accuracy alternative metrics called area receiver operating characteristic curve multi-class extension named mauc introduced recent years. speciﬁcally used evaluate binary classiﬁers mauc multi-class ones. measure performance classiﬁers without making implicit assumptions prior probability classes misclassiﬁcation costs. therefore classiﬁcation systems maximized auc/mauc useful real-world problems involve unequal unknown even changing class distribution misclassiﬁcation costs moreover theoretically proved powerful accuracy discriminating classiﬁcation systems extensive empirical studies showed similar conclusion also holds mauc words even balanced data sets involve diﬀerent costs mauc still superior accuracy sense facilitate choosing best classiﬁcation system number candidates. therefore designing classiﬁcation system gradually shifting maximizing accuracy system maximizing mauc. hereafter refer classiﬁcation systems designed according objective auc/mauc-oriented classiﬁcation systems. traditional feature selection methods cope challenge raised auc/maucoriented classiﬁcation systems. recently initial studies carried address issue. employed rank features directly. work extended empirical studies shown methods although derived traditional ﬁlter methods minor modiﬁcations signiﬁcantly outperform traditional methods. observation unexpected since stated successful feature selection method consider objective classiﬁcation systems however methods focus binary classiﬁcation problems. best knowledge work published literature address multi-class classiﬁcation problems. multiclass problems common practice need suitable mauc-oriented feature selection methods. therefore propose paper novel feature selection method mauc-oriented classiﬁcation systems. proposed method namely mauc decomposition based feature selection essence ﬁlter method. mdfs multi-class problem ﬁrst divided batch binary class sub-problems one-versus-one manner that used rank features within sub-problem. thus feature ranking list obtained sub-problem. finally sub-problems accessed iteratively. every time sub-problem considered feature picked corresponding feature ranking list. siren pitfall phenomenon usually encountered multi-class feature selection avoided. extensive empirical studies conducted compare mdfs feature selection methods multi-class data sets diﬀerent types classiﬁers. results clearly demonstrated superiority mdfs. rest paper organized follows related feature selection methods brieﬂy reviewed section section mauc introduced. that mdfs described detail section section presents experimental setup results. finally conclusions discussions given section section review ﬁlter methods closely related work including three feature ranking methods spreadfx approach proposed relieff method multi-class extension relief method minimal redundancy maximal relevance method main notations used paper summarized follows. training data instance. original feature feature. addition class variable denoted whose value instance. feature ranking methods score feature individually according pre-deﬁned criterion. features largest scores selected. methods category popular high computational eﬃciency simplicity. following brieﬂy revisit three popular feature ranking methods. feature ranking method based chi-square metric utilizes simple selection strategy. nominal feature chi-square statistics class variable calculated follows instead chi-square statistics method symmetrical uncertainty statistics rank features. variant mutual information symmetrical uncertainty avoids bias mutual information features many distinct values lies range nominal feature class variable calculated select features separate diﬀerent classes keep instances class close another feature selection based distance discriminant proposed fsdd calculates utility feature according overcome siren pitfall phenomenon adheres traditional feature ranking methods multi-class problems spreadfx feature selection approach proposed. generally speaking speadfx methods ﬁrst decompose multi-class problem binary sub-problems one-versus-all manner then feature ranking list obtained sub-problem. finally features selected applying dynamic scheduling policy ranking lists. components need speciﬁed employing spreadfx type method feature ranking method dynamic scheduling policy. practice former feature ranking method latter shown selecting sub-problem turn performed satisfactorily. relief methods ﬁrst calculate weight feature. select features largest weights diﬀerent feature ranking methods weights features calculated iterative way. calculation based assumption instances belonging class close another similar values useful feature instances close another diﬀerent classes quite diﬀerent values. iteration relief methods choose instance nearest neighbors class. then diﬀerence instance neighbors every feature employed update weights features. procedure repeated pre-deﬁned number iterations. extension original relief method relieff proposed tackle multi-class problems robust noisy missing values data sets recent ﬁlter feature selection methods equipped schemes explicitly exclude redundant features. representative method minimal redundancy maximal relevance method speciﬁcally mrmr ﬁrst evaluates relevance feature based mutual information class variable feature largest relevance score selected ﬁrst iteration. that features selected time according following criterion fselected denotes feature selected iteration currently selected feature subset cardinality. algorithm terminates pre-deﬁned number features selected. since mrmr detects redundancy among features calculating mutual information pairs features higher computational complexity comparison ﬁlter methods. assume data consists instances belonging class belonging class used evaluate performance classiﬁcation system binary class data set. generally speaking mainstream classiﬁers allow assigning numerical score instances training. then value classiﬁer calculated based scores corresponding true class labels. speciﬁc value classiﬁer equals probability randomly chosen positive instance assigned larger score randomly chosen negative instance example given classiﬁer whose value data randomly chosen positive instance randomly chosen negative instance expected probability higher score although studied extensively literature applicable binary classiﬁcation problems. simple extension used multi-class problems proposed multi-class classiﬁcation problem containing classes classiﬁer assigns scores every instance. score corresponds classes indicates conﬁdence instance belongs class. hence scores instances represented n-by-c matrix columns rows correspond classes instances respectively. calculated according i-th column matrix respect instances class class mauc deﬁned follows mauc value classiﬁcation system actually average value one-versus-one binary sub-problems. words means maximizing value binary sub-problem equal importance calculating classiﬁcation system’s mauc. given diﬀerence accuracy mauc question addressed study feature selection process facilitates construction mauc-oriented classiﬁcation systems? general three methodologies employed purpose. first traditional feature selection methods applied directly. second analogy mauc used relevance metric rank features. third also develop novel feature selection method. start considering eﬃcacy former methodologies. then mdfs method belongs third methodology described detail. mentioned before previous work showed traditional feature selection methods easily outperformed methods speciﬁcally designed auc-oriented classiﬁcation systems binary problems. natural expect situation also holds multi-class problems although solid evidence absent literature. fact traditional feature selection methods might unsuitable mauc-oriented classiﬁcation systems diﬀerence accuracy mauc also so-called siren pitfall phenomenon occur traditional feature ranking methods since diﬃculties separating diﬀerent classes usually diﬀerent multi-class problems features perform well easy subproblems usually obtain relatively higher utility scores features perform well diﬃcult sub-problems usually obtain lower scores. result features compared other perform well easy sub-problems likely selected consequence easy sub-problems focused diﬃcult sub-problems. however calculating mauc value classiﬁcation system equally important maximize value every sub-problem. hence siren pitfall phenomenon feature selection methods also degenerate performance mauc-oriented classiﬁcation systems. literature siren pitfall phenomenon reported text classiﬁcation systems whose performance measured precision f-measure recall verify whether also exists mauc-oriented classiﬁcation systems carried case study indiana data hyperspectral imagery data section scenes taken northwest indiana’s indian pines airborne visible/infrared imaging spectrometer consists classes features. similar experimental setup naive bayes classiﬁer applied every one-versus-one sub-problem using features. feature selection methods namely feature ranking based chi-square metric feature ranking based symmetrical uncertainty metric employed select features whole data then applied sub-problem separately rank features. higher rank useful feature corresponding sub-problem. global best features work well sub-problem high rank fig. presents obtained naive bayes classiﬁer sub-problems. figs. illustrate ranks global best features sub-problem. observed global best features selected rather ranks diﬃcult sub-problems example sub-problem global best features within features. observation suggests siren pitfall also taken care mauc-oriented systems. spreadfx proposed cope siren pitfall phenomenon. however designed mauc-oriented classiﬁcation systems suit mauc maximization well. hence feature selection methods need developed. since value feature instance interpreted output single feature classiﬁer straightforward feature selection method mauc-oriented classiﬁcation systems using mauc relevance metric rank features directly mauc score feature calculated steps first decompose multi-class problem batch one-versus-one binary class sub-problems calculate score feature every sub-problem. second calculate feature’s mauc score using method utility feature measured averaging utility sub-problems thus quite features favor easy sub-problems large mauc scores. consequently features likely selected features useful diﬃcult sub-problems. words directly using mauc rank select features induce siren pitfall phenomenon well thus might ideal solution mauc-oriented classiﬁcation systems. furthermore likely diﬀerent features useful diﬀerent sub-problems anticipate conducting feature selection every sub-problem separately collecting obtained feature subsets form feature subset yields good performance. therefore instead direct mauc feature ranking metric design mauc decomposition based feature selection method. give data instance belong classes represented features. mdfs method works follows. first decomposed binary class sub-problems one-versus-one manner then features ranked according scores every sub-problem. leads feature ranking lists. that feature selection carried iteratively. iteration sub-problem randomly chosen previously unselected feature highest rank corresponding feature ranking list moved selected feature subset. since score used rank features every sub-problem mdfs deal numerical ordered binary type features. nominal features take possible values need converted appropriate numerical features using mdfs. algorithm presents main steps mdfs. subsection time complexity mdfs analyzed compared existing feature selection methods. number instances i-th class data complexity calculating score feature sub-problem consisting i-th j-th class since mdfs requires calculating class number usually small practice complexity mdfs roughly main computational cost maucd also induced calculating scores features binary sub-problems. hence complexity maucd mdfs. according time complexity fsdd methods designed deal nominal features. numerical features discretization procedure needed convert numerical features nominal ones. typical discretization method requires sorting numerical values ﬁrst scanning sorted values convert interval continuous values discrete value hence complexity feature ranking methods dealing data sets consists numerical type features also following analysis complexity spreadfx feature selection approach using feature ranking methods rank features every sub-problem again omitting constant spreadfx’s complexity well. complexity relieff number iterations update features’ weights conﬁguration involves many factors non-trivial task recommended complexity relieff also addition since mrmr detects redundancy among features calculating mutual information pairs features time complexity mrmr summarize computational complexity mdfs comparable existing ﬁlter feature selection methods. experimental studies carried evaluate performance mdfs. experiments designed based three considerations. first eﬃcacy mdfs needs veriﬁed traditional ﬁlter feature selection methods. second since focus work ﬁlter methods experiments restricted speciﬁc type classiﬁer. finally experiments conducted data sets suﬃcient numbers features. otherwise necessary conduct feature selection all. considerations mind ﬁlter methods diﬀerent types classiﬁers selected comparison. altogether combinations feature selection methods classiﬁers applied multi-class data sets collected various domains features. eight benchmark multi-class data sets various domains collected experiments. isolet data mnist data usps data handwriting recognition problems. phoneme data speech recognition ﬁeld. washington data indiana data hyperspectral imagery data sets. details data sets found synthetic data synthetically generated control chart data thyroid microarray data set. table summarizes information data sets. three feature ranking methods introduced section relieff picked baseline feature selection methods experiments. compare spreadfx separately feature ranking method spreadfx employed round-robin scheduling scheme. resulting algorithms referred spreadfx spreadfx respectively. besides mrmr feature selection method introduced section maucd introduced section also included comparison. since none methods automatically decide many features selected given problem compared diﬀerent feature subset sizes interval since synthetic data consists features feature subset sizes considered data set. order examine whether mdfs biased towards certain type classiﬁer diﬀerent types classiﬁers used experiments -nearest neighbor naive bayes kernel function number training instances. parameters values maximize average mauc -fold cross-validation training data -fold cross-validation sampled sampled parameters implementations followed default conﬁguration weka. classiﬁcation systems diﬀerent conﬁgurations evaluated data sets applying -fold crossvalidation times. average mauc values classiﬁcation systems feature subset size classiﬁcation algorithm used indicator compare diﬀerent feature selection methods. wilcoxon signed-rank test conﬁdence level employed examine whether diﬀerences performance mdfs feature selection methods statistically signiﬁcant. results experiments summarized table table fig. tables present mauc value conﬁguration table data set. results pairwise wilcoxon signed-rank test mdfs feature selection methods also labeled superscript mauc values corresponding classiﬁcation systems. means corresponding result statistically worse result mdfs. otherwise diﬀerence detected wilcoxon test. largest mauc value conﬁguration boldface. isolet data mdfs outperformed compared methods every classiﬁer feature subset size larger mnist data except conﬁgurations mdfs always obtained best results. similar situation observed usps data set. hence superiority mdfs proved three data sets. speech recognition data none feature selection methods dominated others. washington data mdfs outperformed others cooperated naive bayes conﬁgurations schi performed better. indiana data mrmr worked signiﬁcantly well naive bayes classiﬁer schi performed better otherwise. synthetically control chart data classiﬁers working mdfs resulted largest mauc cases. thyroid data mdfs performed well svm. naive bayes mdfs comparable methods. general classiﬁcation systems employing mdfs feature selection method largest mauc value often not. cases mdfs best still outperformed compared methods. speciﬁcally maucd almost always inferior mdfs surprising reasons stated section three feature ranking methods fsdd clearly outperformed mdfs throughout experiments also coincided analysis section despite appealing performance accuracy-oriented classiﬁcation systems mrmr method dominate mdfs mauc-oriented classiﬁcation systems cases throughout experiments. schi supposed biggest challenger mdfs. however overall results data sets clearly demonstrated advantage mdfs. fig. summarizes results wilcoxon signed rank tests conducted mdfs compared methods. sub-graphs corresponding comparisons diﬀerent types classiﬁers. hight number times mdfs counterpart feature selection method corresponding classiﬁer figure results wilcoxon signed rank test conﬁdence level mdfs compared feature selection methods diﬀerent types classiﬁers. hight indicates times mdfs counterpart feature selection method feature subset sizes data sets. finally table shows runtime every feature selection method data sets. analyzed section computational complexity mdfs comparable compared feature selection methods however constant factor complexity analysis implementation details algorithms actual runtime deviate complexity analysis indicative. although numerous successful feature selection methods developed accuracyoriented classiﬁcation systems recent studies revealed accuracy appropriate performance metric many real-world practices lead undesirable classiﬁcation systems. instead mauc adopted commonly literature. shift performance metric raises need feature selection methods. study proposed mdfs feature selection method mauc-oriented classiﬁcation systems. inspired observation mauc value classiﬁcation system actually average table average mauc obtained nine compared methods isolet data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. table average mauc obtained nine compared methods mnist data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. table average mauc obtained nine compared methods usps data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. table average mauc obtained nine compared methods phoneme data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. table average mauc obtained nine compared methods washington data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. table average mauc obtained nine compared methods indiana data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. table average mauc obtained nine compared methods synthetic data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. table average mauc obtained nine compared methods thyroid data set. results obtained repeating -fold cross-validation procedure times. classiﬁer feature subset size wilcoxon signed-rank test conﬁdence level employed compare mdfs methods. methods performed signiﬁcantly worse mdfs highlighted superscript used statistical signiﬁcant diﬀerence detected. largest mauc value conﬁguration boldface. values every binary sub-problem consists pair classes. therefore mdfs ﬁrst decomposes multi-class problem batch binary class sub-problems one-versus-one manner. then features iteratively selected based utility sub-problem. equal focus every sub-problem implemented choosing equal probability iteration. advantage mdfs traditional ﬁlter methods justiﬁed comparative studies benchmark data sets. results obtained types classiﬁers demonstrated mdfs overall superior compared ﬁlter methods terms mauc values classiﬁcation systems. experimental studies also showed direct mauc feature ranking metric inferior performance compared mdfs. finally computational complexity mdfs comparable compared ﬁlter feature selection methods. mdfs might improved aspects first employment random strategy mdfs selecting sub-problems take possible correlation subproblems account. number sub-problems highly correlated another many features would selected sub-problems. lead redundant features make weight problem equal feature selection lead inferior performance mauc-oriented classiﬁcation systems. hence ﬁnding detect correlation among sub-problems relative importance diﬀerent subproblems maximizing mauc great interest. second redundancy among features considered mdfs. great success redundancy-exclusive strategies mind incorporating mdfs promise enhanced performance. investigate issues future.", "year": 2011}