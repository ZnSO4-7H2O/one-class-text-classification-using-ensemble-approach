{"title": "An effective algorithm for hyperparameter optimization of neural  networks", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "A major challenge in designing neural network (NN) systems is to determine the best structure and parameters for the network given the data for the machine learning problem at hand. Examples of parameters are the number of layers and nodes, the learning rates, and the dropout rates. Typically, these parameters are chosen based on heuristic rules and manually fine-tuned, which may be very time-consuming, because evaluating the performance of a single parametrization of the NN may require several hours. This paper addresses the problem of choosing appropriate parameters for the NN by formulating it as a box-constrained mathematical optimization problem, and applying a derivative-free optimization tool that automatically and effectively searches the parameter space. The optimization tool employs a radial basis function model of the objective function (the prediction accuracy of the NN) to accelerate the discovery of configurations yielding high accuracy. Candidate configurations explored by the algorithm are trained to a small number of epochs, and only the most promising candidates receive full training. The performance of the proposed methodology is assessed on benchmark sets and in the context of predicting drug-drug interactions, showing promising results. The optimization tool used in this paper is open-source.", "text": "major challenge designing neural network systems determine best structure parameters network given data machine learning problem hand. examples parameters number layers nodes learning rates dropout rates. typically parameters chosen based heuristic rules manually fine-tuned time-consuming evaluating performance single parametrization require several hours. paper addresses problem choosing appropriate parameters formulating box-constrained mathematical optimization problem applying derivative-free optimization tool automatically effectively searches parameter space. optimization tool employs radial basis function model objective function accelerate discovery configurations yielding high accuracy. candidate configurations explored algorithm trained small number epochs promising candidates receive full training. performance proposed methodology assessed benchmark sets context predicting drug-drug interactions showing promising results. optimization tool used paper open-source. data scientists routinely faced task choosing best parameters machine learning model i.e. parameters yields best performance predictor available dataset. example training testing neural network requires determining structure network learning parameters learning dropout rate. parameters type typically called hyperparameters must determined actual training model takes place. training phase parameters model activation thresholds neurons determined. dataset large training testing single configuration hyperparameters take long time—several hours more. furthermore choosing values hyperparameters yield high accuracy sometimes artful process typically starts conforming heuristic rules followed manually fine-tuning hyperparameters hand. results long often tedious process. paper propose methodology automate task present discuss corresponding software implementation. methodology propose based formulating problem choosing optimal hyperparameters box-constrained mathematical optimization problem goal maximizing prediction accuracy test applying derivativefree optimization algorithm problem. derivative-free optimization algorithm algorithm require information derivatives order. box-constrained problem optimization problem defined hyperrectangle. derivative-free optimization well-established area within field mathematical optimization comprises several techniques—and comprehensive treatment subject paper appear final form journal research development vol. part special issue deep learning. please cite journal official paper version record. information journal http//www.research.ibm.com/journal/. ©ibm. given recent literature contains several works hyperparameter optimization methodologies applied automatic configuration machine learning models including optimization hyperparameters also studied example using random search approach bayesian optimization weighted probabilistic extrapolation approaches however best knowledge present paper first applies rigorous derivative-free optimization machinery task. paper following contributions. first discuss transform hyperparameter optimization problem normally constrained problem box-constrained problem provide empirical evidence support formulation compared natural formulation. second describe extension derivativefree optimization algorithms implemented open-source library rbfopt allow parallel asynchronous evaluations function optimized i.e. performance tested nns. best knowledge approach parallelize derivative-free methodology based radial basis function methods first kind. proposed extension incorporated latest release rbfopt available github. third provide numerical results indicate hyperparameter optimization approach adopted paper yield significant benefits particular performs better popular approach. paper also limitations. notably address problem choosing optimal number training epochs keep fixed hyperparameter optimization process. state-of-the-art bayesian optimization methods drawback. note rbfopt allows exploitation computationally cheaper inaccurate version objective function capability could used accelerate convergence hyperparameter optimization algorithm varying number training epochs direction left future research. furthermore treat objective function deterministic although case. however practice assume never evaluate hyperparameter configuration twice limited time availability. finally numerical evaluation limited scope note even limited scope draw statistically significant conclusions related topics outlined above. rest paper organized follows. second section define hyperparameter optimization problem. third section review fundamental concepts derivative-free optimization. fourth section discuss derivative-free optimization applied hyperparameter optimization specifically context neural networks provide overview approach parallelize hyperparameter optimization algorithms implemented rbfopt. fifth section describe experimental setup present numerical results. sixth section concludes paper. context paper machine learning model data-driven mathematical model prediction property unseen data. examples property predicted function value membership class relative rankings rest paper prediction model refer examples specific case classification problem illustrate approach. structure training phase prediction models governed hyperparameters. depends specific model corresponding software implementation typical parameters found implementations number size hidden layers learning rate dropout rate. describe valid assignments values hyperparameters prediction model. example simplest case hyperparameters taking unrestricted real values interpreted real-valued n-dimensional vector space often though hyperparameters prediction model take values different domains; e.g. learning rate hyperparameter take positive real values hyperparameter defining number hidden layers assigned positive integers hyperparameter defines type kernel function support vector machine would take categorical values. even implicit constraints among hyperparameter values implies usually rich structure increases problem difficulty. given point prediction model trained available dataset accuracy unseen data points estimated. typically done reserving part available dataset validation using k-fold cross-validation scheme integer dataset. estimated performance parametrized dataset according chosen performance metric. paper since mostly working classification problems accuracy performance metric principle reasonable metric used. function also called objective function. problem determining best hyperparameters formulated follows label stands hyperparameter optimization notice mathematical expression function closed form unknown stochastic computable function; words even though unable provide analytical expression function computable given however computing single point require large amount time involves training prediction model possibly large dataset testing performance validation set. many applications engineering fields require maximization performance criterion description analytical form available. rules utilization classical numerical optimization approaches algorithms based gradient descent information first higher-order derivatives cannot computed analytically. cases first second order derivatives numerically estimated finite differences requires large number function evaluations instance estimating gradient function defined n-dimensional space point requires evaluating function points least. evaluation objective function computationally expensive case application discussed paper approach obviously viable practice different methodology becomes necessary. derivative-free optimization area mathematical optimization dedicated task devising optimization approaches rely zero-order information exclusively i.e. function values. derivative-free optimization comprises several different methodologies refer reader overview. because problem evaluation particularly computationally expensive require several hours time interested approaches optimize performing small number function evaluations. contrast meta-heuristic approaches genetic algorithms rely zero-order information typically perform thousands function evaluations resulting prohibitive computing times. briefly describe approaches implemented opensource library rbfopt version provides implementation derivative-free optimization algorithms based surrogate models. approach describe numerical experiments based metric stochastic response surface method several modifications. algorithm applied problem form label stands derivative-free optimization vectors lower upper bounds decision variables indices decision variables constrained take integer variables only. note software implementation problem assumed minimization form discuss maximization problem consistency without loss generality since maximization problem transformed minimization problem negating objective function. notice optimized domain described simple lower upper bounding constraints i.e. hyperrectangle decision variables restricted integer values. thus box-constrained problem integrality constraints. types constraints allowed formulation. vast majority derivative-free optimization algorithms apply box-constrained problems difficulty handling general constraints algorithm relies surrogate model objective function select points evaluate trying balance exploration unknown parts domain exploitation surrogate model identify potential global optimum. several types surrogate models possible default rbfopt uses radial basis function model thin plate splines combined polynomial tail degree algorithm starts evaluating points selected according randomly generated latin hypercube design within domain. points evaluated. then every iteration algorithm fits surrogate model interpolates points chooses next evaluation point according criteria maximized value surrogate model euclidean distance closest point bi-objective optimization problem general possibly infinite pareto-optimal points since interested determining single point objectives normalized reduced single objective considering weighted combination weight determines tradeoff. specifically weight chosen according cyclic strategy oscillates favoring maximum minimum distance criterion emphasizing exploration unknown parts domain favoring surrogate model value criterion emphasizing choice points supposed large value according surrogate model. resulting single-objective optimization problem choice solved several ways authors recommend random sampling rbfopt employs simple genetic algorithm default. note genetic algorithm applied auxiliary optimization problem determines choice original problem —while evaluating expensive evaluating value surrogate model minimum distance points computationally cheap; therefore application genetic algorithm typically requires fractions second. determined evaluated point added iteration complete. algorithm iterates following scheme stopping criterion satisfied typically based maximum allowed number evaluations maximum time. discuss tailored optimization algorithm rbfopt solution specific case main difficulty overcome optimizing hyperparameters including parameters determine architecture contains constraints simple constraints must enforce hyperparameters describe valid architecture. particular cannot empty hidden layers network non-empty hidden layers. issue resolved adjusting mathematical formulation problem discussed next subsection. box-constrained considered several approaches based experience similar contexts decided proceed follows. upper bound number hidden layers willing consider; could large value restrict search space practice machine learning practitioner easily proviode reasonable upper bound problem hand typically small number. similarly maximum size hidden layer willing consider terms number nodes. formulation utilize decision variables determine size hidden layers without loss generality. decision variable constrained integer used determine number active hidden layers. remaining variables constrained integers size hidden layers indicated taking first variables. example would construct hidden layers size respectively. notice mapping values structure introduces symmetry several values decision variables correspond structure values corresponding structure obtained certain permutations decision variables. symmetry general harmful optimization algorithms find global optimum problem however derivative-free approach cannot give global optimality guarantees finite time simply aims find solution large objective function value within objective function evaluations. reasons symmetry necessarily harmful problem studied paper computational experiments show approach propose successful practice despite symmetry. briefly explain rationale approach. variable ensures design space explored uniformly naïve formulation simply bounds variables without imposing explicit bound number nonempty hidden layers would lead dense hyperparameter optimization algorithms tend spread tested configurations entire design space. consequence tested configurations would ..u. unless reason believe dense would effective problem hand formulation proposed likely yield higher accuracy similar shorter computing times naive formulation. empirical results reported computational evaluation section support claim. evaluation objective function time-consuming operation solution training assessing performance single take several hours reason extended optimization algorithm implemented rbfopt perform task parallel. notice training time prediction model deterministic quantity introduces difficulty parallelization—indeed many parallel derivative-free optimization algorithms make simplifying assumption synchronous parallel objective function evaluations approach described next allows asynchronous. idea follows. keep queue tasks performed. tasks types—the first type evaluation objective function point second type computation search point objective function must evaluated. long available processors task removed queue assigned processor. longer computing time tasks type always priority tasks type within tasks type follow first come first served policy. whenever task type completed yields interpolation point added whenever task type completed check newly determined search point discarded several criteria also employed serial version optimization algorithm search point accepted queue task type evaluate undesirable event principle occur evaluated point point generated search point concurrent tasks type therefore evaluation performed multiple times. clearly must avoided. whenever task type submitted processing temporary interpolation node objective function value determined value existing surrogate model since next search point never coincide interpolation node even serial version algorithm guaranteed search points distinct. temporary interpolation node removed soon associated task type completed. important note several decisions taken optimization algorithm depend difference largest smallest function value among interpolation nodes; details. addition temporary interpolation node extends range known function values could alter decisions especially risky result inaccurate surrogate model large oscillations. therefore allow temporary interpolation nodes extend range known function values achieved clipping surrogate model value temporary interpolation node range existing function values. describe detail empirical evaluation carried study. first discuss experiments standard benchmark dataset publicly available code describe specific provide comparison performance rbfopt popular existing hyperparameter optimization methodologies perform case study optimization convolutional well-known publicly available mnist hand-written digit recognition dataset classification problem classes. implemented using caffe framework trained using stochastic gradient descent. search space defined instance corresponds convolutional layers filter size followed pooling layer filter size stride fully connected layers level always consists units class winning class determined applying softmax. number units convolutional layer fully connected layer varies multiples whenever layer present. model described earlier paper whereby additional decision variables determine number hidden layers used type layer hyperparameter determines fully connected layers rectified linear unit linear unit sigmoid activation function. hyperparameters indicated discrete. remaining hyperparameters continuous base learning rate momentum weight decay rate gamma base learning rate weight decay rate scale; words corresponding decision variables represent values used train rather values themselves ensures effective exploration hyperparameter space. scale used sample hyperparameters well following experiments section performed ibm’s cloud using server intel xeon clocked running linux. compare rbfopt sequential model-based algorithm configuration using hyperparameter space methodologies. algorithm executed different random seeds random seeds used initialize stochastic gradient descent algorithm training phase. algorithms given sequence random seeds. mnist dataset consists training test set. split training train data validation data objective function best accuracy recorded validation within first training epochs test interval epochs. number training epochs chosen preliminary testing observed good trade-off speed accuracy validation set. algorithm allowed explore exactly different hyperparameter configurations equivalent budget objective function evaluations time limit given. budget depleted best configuration found declared winner assess performance test set. final test first determine number training epochs configuration using previous split train validation allow epochs determine number epochs yields best accuracy validation set. then number epochs perform final training testing. experiments training stopped early training epoch results summarized table table table reports accuracy test best configuration discovered following procedure described above table reports snapshots accuracy validation iterations. report average standard deviation accuracy pairs algorithms many times algorithm produces accuracy least good other whether statistically significant difference detected friedman test followed post-hoc analysis confidence level tables interpreted follows number indicates many times runs algorithm least accurate algorithm column. furthermore friedman test detected significant difference report direction difference; e.g. indicates algorithm achieves larger accuracy column significance results clearly show difference algorithms emerges early iterations rbfopt smac perform better trend continues rest optimization process. results test consistent validation set. rbfopt achieves slightly higher average accuracy methods overall marginally better smac statistically significant difference detected. notice number different runs differences detected algorithm clearly dominates another. note hyperparameter optimization phase well final training/test phase sometimes encountered numerical troubles occasionally stochastic gradient descent algorithm implemented caffe converge training error increased algorithm aborted. issue could ascribed specific reason occurred similar frequency three tested hyperparameter optimization algorithms. unfortunately encountered issue final testing phase best configurations discovered rbfopt discovered smac. exclude corresponding runs algorithms results reported table verified change results friedman test hence would change ranking algorithms. conclude section brief comment performance specific unconstrained formulation chosen previously discussed empirically tested proposed formulation naïve formulation employ decision variable determine number nonempty hidden layers. used framework experiments above employing rbfopt solve terms accuracy validation test iterations naïve formulation proposed formulations perform equally achieves higher accuracy runs. however time optimization process naïve formulation requires time average times larger proposed formulation—this optimization algorithm explores mostly dense since statistically significant advantage terms validation score time requirement larger results suggest naïve formulation employed. aside results previous section hyperparameter optimization algorithm also applied software system called tiresias built receive various sources drug-related data knowledge inputs provide drug-drug interaction predictions outputs. ddis major cause preventable adverse drug reactions causing significant burden patients’ health healthcare system tiresias constructed semantic integration data originating variety sources followed building large knowledge graph containing relations drugs diseases genes/proteins computing several similarity measures drugs. resulting similarity metrics used features large-scale logistic regression model predict potential ddis. previous work similarity features limited ways. first important chemical structure based similarity measures computing fingerprints drugs similarity measures computed cosine similarity fingerprints. second similarity measure constructed examining local neighborhood drug knowledge graph. describe limitations addressed present work. important light extensions introduced here tiresias framework also applied publicly available benchmarks concerning knowledge graphs numerical results benchmarks discussed. address first limitation exploring multi-layer perceptron directly predict ddis given fingerprints drugs. hyperparameters include number hidden layers number units hidden layer unit activation function whether pre-training enabled learning parameters address second limitation relying graph embedding models provide dense representations entities relations knowledge base. output models used define additional similarity metrics used directly link prediction tasks predicting ddis. graph embedding models trained minimizing global cost function entire knowledge graph considered. dense representation entity encodes global information graph. although highly expressive graph embedding models exist recent translation-based models achieve high quality predictions scalable large datasets. context explored extensions models capture knowledge assertions also ontology information commonly present large knowledge graphs. translational graph models generally depend hyperparameters include type similarity function used representing cost vectors learning parameters rates sampling sizes specific update function used minimize cost function margin parameters. implementation models done python theano framework. experiments optimize hyperparameters i.e. multi-layer perceptron graph embedding models using accuracy predictions validation objective function prediction problem target application tests publicly available dataset compare state-of-the-art. tiresias framework relatively used benchmarks current version know region hyperparameter space likely contain optimal values hyperparameter optimization methodologies perform. however tiresias framework described capable handling problem types different problem; thus perform experiments multi-relational data prediction problem described described problem predicting missing edges knowledge graph compare results reported literature. datasets derived wordnet freebase large knowledge bases containing multi-relational data. wordnet dataset designed produce intuitively usable dictionary thesaurus freebase dataset knowledge base general facts corresponding machine learning problem aims learn relationships words datasets used experiments called fbk. overview relational machine learning knowledge graphs refer recent survey experiments tiresias conducted computer cluster composed nextscale machines; machine cores tesla gpus. compute jobs deployed submission platform load sharing facility. note experiments cluster several concurrent jobs evaluating exact computing time difficult depends load machines. first evaluate performance hyperparameter optimization methodology datasets employing methodology used paper. test performance single validation point consists triplet words element triplet removed replaced possible entities dictionary. newly constructed triplets ranked prediction mechanism based similarity score. following report proportion correct entities ranked labeled hits well mean rank correct entities. higher percentage hits lower mean rank indicate better performance. account fact newly constructed triplets valid although originally part validation authors propose filtering method address issue. thus report results filtered results discussed earlier allow optimizer perform small number training epochs increase speed. benchmark allow epochs subsequently used training. figure shows progress parameter search multi-relational data prediction problem data set. blue curve indicates performance training curve indicates performance validation set. within less iterations clear jump performance additional iterations. oscillating behavior cyclic search strategy discussed earlier description methodology implemented rbfopt whereby oscillate exploration exploitation. resulting best configuration obtained running training phase epochs trained using epochs evaluated test set. compare performance hyperparameter configuration found automatic approach performance configuration obtained manually tuning best configuration reported best methodology reported called transe. note best configuration determined heuristic know exactly required effort however reasonable assume manually tuned version best configuration representative performance obtained manual heuristic hyperparameter optimization procedure. results reported table using performance metrics mentioned above. obtained performance terms hits match results obtained manually tuning mean rank performance vastly superior indicating particular instance approach least competitive better manually tuning hyperparameters. furthermore performance configuration found rbfopt tiresias significantly better best results reported previously considered state terms mean rank terms hits. slightly different scenario occurs second data fbk—in case best hyperparameter configuration reported leads numerical problems training phase tiresias despite attempts manually tuning hand rbfopt discovers configuration performance least comparable transe reported significantly better performance metrics results filtered results reports better mean rank rbfopt tiresias although approach yields much larger percentage hits. figure shows impact parallelism hyperparameter optimization. clearly observe stages search less pronounced asynchronous runs break periodicity cyclic search strategy. however general upward trend performance increasing number iterations apparent best configuration determined parallel search comparable determined sequential accuracy mean rank hand parallel much faster serial run—while serial required approximately hours evaluated configurations parallel required approximately hours evaluated configurations. unfortunately different loads computing cluster cannot report precise timing statistics. however accurately estimate speed improvement achieved parallel report statistics benchmark cluster identical machines cpus user-submitted jobs ours. benchmark consists finding global optimum instances coming various sources details) location value true optimum function known kept hidden optimization algorithm. experiment optimization stopped number function evaluations exceeded dimension problem objective function evaluation took time drawn uniformly random seconds perform optimization runs different random seeds problem instance yielding runs total. table report given convergence tolerance number problem instances algorithm converged specified tolerance average wall-clock time instances solved methods speedup relative serial run. results indicate sublinear still significant speedup factor cpus used parallel although unsurprisingly price number solved instances smaller. finally report results prediction problem. instance construct hidden layers units each. unfortunately lack well-established baseline method extension tiresias framework discussed paper previously tested. figure report accuracy validation set. figure shows progress hyperparameter optimization algorithm training epochs. exist initial improvements obtaining validation scores larger appears difficult achieved frequently later stages search. next best configuration discovered validation train epochs comparing resulting accuracy test accuracy hyperparameter configuration chosen domain experts heuristic way. main difference configurations user-generated parameterization uses hidden layer selected rbfopt. resulting accuracies showing substantial improvement. benchmark marginally significant include comparison rigorous hyperparameter optimization methodologies serves purpose showing practical application approach quickly successfully finds configuration higher accuracy would guessed domain experts fully automated. presented methodology hyperparameter optimization problem corresponding software implementation. hyperparameter optimization problem consists finding values parameters machine learning algorithm achieve largest prediction accuracy dataset. methodology based casting problem derivative-free optimization problem solving such. methodology cannot guaranteed find global optimum finite number iterations show empirically finds values hyperparameters yielding higher prediction accuracy determined domain expert. moreover benchmark instances literature obtain results least comparable sometimes better popular existing algorithms random search sequential model-based algorithm configuration. suggests using derivative-free optimization techniques hyperparameter optimization problem machine learning promising approach potential impact practice. future research address problem choosing number training epochs combining approach discussed paper learning-curve-based optimization approaches", "year": 2017}