{"title": "Influence Functions for Machine Learning: Nonparametric Estimators for  Entropies, Divergences and Mutual Informations", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We propose and analyze estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators.", "text": "kirthevasan kandasamy akshay krishnamurthy barnab´as p´oczos school computer science carnegie mellon university larry wasserman department statistics carnegie mellon university james robins department biostatistics harvard university propose analyze estimators statistical functionals distributions nonparametric assumptions. estimators based theory inﬂuence functions appear semiparametric statistics literature. show estimators based either data-splitting leave-one-out technique enjoy fast rates convergence favorable theoretical properties. apply framework derive estimators several popular information theoretic quantities empirical evaluation show advantage approach existing estimators. entropies divergences mutual informations classical information-theoretic quantities play fundamental roles statistics machine learning across mathematical sciences. addition analytical tools arise variety applications including hypothesis testing parameter estimation feature selection optimal experimental design. many applications important estimate functionals data used downstream algorithmic scientiﬁc tasks. paper develop recipe estimating statistical functionals nonparametric distributions based notion inﬂuence functions. entropy estimators used applications ranging independent components analysis intrinsic dimension estimation several signal processing applications divergence estimators useful statistical tasks two-sample testing. recently also gained popularity used measure -similarity objects modeled distributions known machine learning distributions framework mutual information estimators used learning tree-structured markov random ﬁelds feature selection clustering neuron classiﬁcation parametric setting conditional divergence conditional mutual information estimators used conditional sample testing building blocks structure learning graphical models. nonparametric estimators quantities could potentially allow generalise several algorithms nonparametric domain. approach gives sampleeﬃcient estimators quantities often outperfom existing estimators theoretically empirically. approach estimating functionals based post-hoc correction preliminary estimator using mises expansion vaart fernholz idea used semiparametric statistics literature however hitherto studies restricted functionals distribution focused data-split approach splits samples density estimation functional estimation. data-split estimator known achieve parametric convergence rate suﬃciently smooth densities birg´e massart laurent practical settings splitting data results poor empirical performance. paper introduce calculus inﬂuence functions machine learning community considerably expand existing results proposing leave-one-out estimator makes eﬃcient data better empirical performance technique. also extend framework inﬂuence functions functionals multiple distributions develop estimators. main contributions paper extend framework functionals multiple distributions analyse convergence. suﬃcient smoothness estimators achieve parametric rate estimator limiting normal distribution. approach construct implement estimators various entropy divergence mutual information quantities conditional versions. subset functionals listed table several functionals known estimators. software publicly available github.com/kirthevasank/if-estimators. compare estimators several approaches simulation. despite generality approach estimators competitive many cases superior existing specialized approaches speciﬁc functionals. also demonstrate estimators used machine learning applications image clustering task. focus information theoretic quantities relevance machine learning applications rather limitation approach. indeed techniques apply smooth functional. history provide brief history post-hoc correction technique inﬂuence functions. defer detailed discussion approaches estimating functionals section knowledge ﬁrst paper using post-hoc correction estimator bickel ritov line work following paper analyzed integral functionals single dimensional density form considers polynomial functionals form pαqβ densities moreover works data fundamental quantity design estimators inﬂuence function appears robust semiparametric statistics. indeed work inspired robins emery propose inﬂuence-function based estimator functionals single distribution. analysis nonparametric problems rely ideas semiparametric statistics deﬁne inﬂuence functions parametric models analyze estimators looking parametric submodels true parameter. real valued lipschitz functions twice diﬀerentiable. framework permits general functionals e.g. functionals based conditional densities focus form ease exposition. facilitate presentation main deﬁnitions easiest work functionals central development mises expansion distributional analog taylor expansion. introduce gˆateaux derivative imposes notion diﬀerentiability topological spaces. introduce inﬂuence function. measures continuous densities control using metric densities. essentially means functionals satisfy stronger form diﬀerentiability called fr´echet diﬀerentiability metric. consequently write derivatives terms densities reduces functional taylor expansion densities ideas generalize functionals multiple distributions settings functional involves quantities density. functional distributions gˆateaux derivatives formed perturbing argument ﬁxed. inﬂuence functions satisfy since inﬂuence function depend unknown distribution ﬁrst term right hand side simply expectation second half data estimate expectation sample mean. leads following preliminary estimator rate estimating expectation. lower bounds several literature conﬁrm minimax optimality estimator suﬃciently smooth. data splitting trick commonly used several works birg´e massart laurent krishnamurthy analysis estimators straightforward rate directly follows cauchy-schwarz inequality. theory estimators enjoy good rates convergence practical stand point data splitting unsatisfying since using half data estimation averaging invariably decreases accuracy. ˆf−i kernel density estimate using samples except theoretically prove estimator achieves rate convergence estimator emprically performs much better. average. estimator cycle points pairings xi’s yj’s. however latter approach computationally burdensome. moreover straightforward modiﬁcation analysis appendix shows estimators rate convergence order. examples demonstrate generality framework presenting estimators several entropies divergences mutual informations conditional versions table several functionals table ﬁrst estimators proposed. hope table serve good reference practitioners. several functionals estimators listed expressions long table. software implements total functionals include estimators table. appendix illustrate apply framework derive estimator functional example. discussed section compared alternatives technique several favourable properties. computationally complexity method compared methods several functionals require numeric integration. additionally unlike methods require tuning hyperparameters. table deﬁnitions functionals corresponding estimators. px|z etc. conditional joint distributions. conditional divergences take samples respectively. mutual informations samples conditional versions kernel tsybakov selecting achieves minimax rate mean squared error. further bounded h¨older class truncate achieve convergence rate analysis density estimators ˆf−i ˆg−i formed either truncated make results. directly uses interpretable assumptions. bounding bias variance estimator establish convergence rate follows straightforward conditioning argument cauchy schwarz. however attractive property analysis agnostic density estimator used provided achieves correct rates. technical challenge analysing estimator bounding variance several correlated terms summation. bounded diﬀerence inequality popular trick used settings requires supremum inﬂuence functions leads signiﬁcantly worse rates. instead efron-stein inequality provides integrated version bounded diﬀerences recover correct rate coupled assumption proof asymptotic normality result useful allows construct asymptotic conﬁdence intervals functional. even though asymptotic variance inﬂuence function known slutzky’s theorem consistent estimate variance gives valid asymptotic conﬁdence interval. fact inﬂuence function based estimator asymptotic variance since also diﬀerentiable functional densities. demonstrate example appendix condition somewhat technical. zero ﬁrst order terms vanishes estimator converges fast however asymptotic behavior estimator unclear. degeneracy occurs meagre arise important choices. example null hypothesis two-sample testing problems. many functionals h¨olderian assumption alone suﬃcient guarantee rates theorems however functionals require bounded below. existing results demonstrate estimating quantities diﬃcult without assumption. turn attention question statistical diﬃculty. lower bounds given birg´e massart laurent know estimators minimax optimal functionals distribution. following theorem present lower bound estimating functionals distributions. proof given appendix based lecam’s method tsybakov generalises analysis birg´e massart functionals distribution. establishes minimax optimality technique lower bound natural possible improve rates regime. series work shows that integral functionals distribution achieve rate estimating second order term functional taylor expansion. second order correction also done polynomial functionals distributions similar statistical gains believe possible here estimators conceptually complicated computationally expensive requiring eﬀort compared eﬀort estimator. ﬁrst order estimator favorable balance statistical computational eﬃciency. further much known limiting distribution second order estimators. estimation statistical functionals nonparametric assumptions received considerable attention last decades. large body work focused estimating shannon entropy– beirlant gives nice review results techniques. recent work single-distribution setting includes estimation r´enyi tsallis entropies also several papers extending techniques divergence estimation many existing methods categorized plug-in methods based estimating densities either using k-nearest neighbors evaluating functional estimates. plug-in methods conceptually simple unfortunately suﬀer several drawbacks. first typically worse convergence rate approach achieving parametric rate k-nn obtaining best rates plug-in methods requires undersmoothing density estimate aware principled approaches hyperparameter tuning here. contrast bandwidth used estimators optimal bandwidth density estimation number approaches cross validation available. convenient practitioner method require tuning hyper parameters. secondly plugin methods based always require computationally burdensome numeric integration. approach numeric integration avoided many functionals interest also another line work estimating -divergences. nguyen estimate -divergences solving convex program analyse technique likelihood ratio densities rkhs. comparing theoretical results straightforward since clear port assumptions setting. further size convex program increases sample size problematic large samples. moon hero weighted ensemble estimator -divergences. stronger smoothness assumption required technique. works consider -divergences. method wider applicability includes -divergences special case. dimensions. ds/loo estimators estimate density smoothing kernels constructed using legendre polynomials cases plug estimator choose bandwidth performing -fold cross validation. integration plug estimator approximated numerically. distributions methods compared given below. results shown figures make following observations. cases estimator performs best. estimator approaches estimator many samples generally inferior estimator samples. this explained data splitting make eﬃcient data. k-nn estimator divergences p´oczos requires choosing estimator used default setting given software. performance sensitive choice performs well cases poorly cases. reiterate estimators require setting hyperparameters. next present results asymptotic normality. test estimators dimensional hellinger divergence estimation problem. samples estimation. repeat experiment times compare empiritical asymptotic distribution otherwise sample points pick maximum. third ﬁgure comparing divergence generate data dimensional density beta distribution. second fourth ﬁgures sampled dimensional density ﬁrst dimension second ﬁfth sixth dimensional density ﬁrst dimension second ﬁgures fig. ﬁrst distribution -dimensional density dimensions latter noughabi noughabi based vasicek’s spacing method learned-miller john based voronoi tessellation. divergence compare k-nn method p´erezcruz ramırez based power spectral density representation using szego’s theorem. r´enyi-α tsallis-α hellinger divergences compared k-nn method p´oczos software estimators obtained either directly papers szab´o figure comparison estimator estimating conditional tsallis divergence dimensions. note plug-in estimator intractable numerical integration. known estimators conditional tsallis divergence. figs plots obtained using samples hellinger divergence estimation dimensions using estimators respectively. demonstrate simple image clustering task using nonparametric divergence estimator. images eth- dataset. objective champion approach image clustering methods image clustering there. rather wish demonstrate estimators easily intuitively applied many machine learning problems. three categories apples cows cups randomly select images category. sample images shown convert images grey scale extract sift features image. sift features -dimensional project dimensions pca. necessary nonparametric methods work best dimensions. treat image collection features hence sample dimensional distribution. estimate hellinger divergence distributions. construct aﬃnity matrix similarity figure depicts aﬃnity matrix images ordered according class label. aﬃnity matrix exhibits block-diagonal structure indicates hellinger divergence estimator fact identify patterns images. approach achieved clustering accuracy used k-nn based estimator p´oczos achieved accuracy instead generalise existing results mises estimation proposing empirically superior technique estimating functionals extending framework functionals distributions. also prove lower bound latter setting. demonstrate practical utility technique comparisons alternatives image clustering application. open problem arising work derive limiting distribution estimator.", "year": 2014}