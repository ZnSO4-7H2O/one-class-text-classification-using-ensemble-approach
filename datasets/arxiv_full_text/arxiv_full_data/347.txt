{"title": "Bayesian GAN", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching, or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.", "text": "generative adversarial networks implicitly learn rich distributions images audio data hard model explicit likelihood. present practical bayesian formulation unsupervised semi-supervised learning gans. within framework stochastic gradient hamiltonian monte carlo marginalize weights generator discriminator networks. resulting approach straightforward obtains good performance without standard interventions feature matching mini-batch discrimination. exploring expressive posterior parameters generator bayesian avoids mode-collapse produces interpretable diverse candidate samples provides state-of-the-art quantitative results semi-supervised learning benchmarks including svhn celeba cifar- outperforming dcgan wasserstein gans dcgan ensembles. learning good generative model high-dimensional natural signals images video audio long milestones machine learning. powered learning capabilities deep neural networks generative adversarial networks variational autoencoders brought ﬁeld closer attaining goal. gans transform white noise deep neural network generate candidate samples data distribution. discriminator learns supervised manner tune parameters correctly classify whether given sample come generator true data distribution. meanwhile generator updates parameters fool discriminator. long generator sufﬁcient capacity approximate inverse-cdf composition required sample data distribution interest. since convolutional neural networks design provide reasonable metrics images gans using convolutional neural networks turn provide compelling implicit distribution images. although gans highly impactful learning objective lead mode collapse generator simply memorizes training examples fool discriminator. pathology reminiscent maximum likelihood density estimation gaussian mixtures collapsing variance component achieve inﬁnite likelihood memorize dataset useful generalizable density estimate. moreover large degree intervention required stabilize training including feature matching label smoothing mini-batch discrimination help alleviate practical difﬁculties recent work focused replacing jensen-shannon divergence implicit standard training alternative metrics f-divergences wasserstein divergences much work analogous introducing various regularizers maximum likelihood density estimation. difﬁcult choose right regularizer also difﬁcult decide divergence wish training. contention gans improved fully probabilistic inference. indeed posterior distribution parameters generator could broad highly multimodal. training based mini-max optimization always estimates whole posterior distribution network weights point mass centred single mode. thus even generator memorize training examples would expect samples generator overly compact relative samples data distribution. moreover mode posterior network weights could correspond wildly different generators meaningful interpretations. fully representing posterior distribution parameters generator discriminator accurately model true data distribution. inferred data distribution used accurate highly data-efﬁcient semi-supervised learning. paper propose simple bayesian formulation end-to-end unsupervised semisupervised learning generative adversarial networks. within framework marginalize posteriors weights generator discriminator using stochastic gradient hamiltonian monte carlo. interpret data samples generator showing exploration across several distinct modes generator weights. also show data iteration efﬁcient learning true distribution. also demonstrate state semi-supervised learning performance several benchmarks including svhn mnist cifar- celeba. simplicity proposed approach greatest strengths inference straightforward interpretable stable. indeed experimental results obtained without feature matching ad-hoc techniques. made code tutorials available https//github.com/andrewgordonwilson/bayesgan. bayesian gans given dataset variables pdata) wish estimate pdata. transform white noise generator parametrized produce candidate samples data distribution. discriminator parametrized output probability comes data distribution. considerations hold general practice often neural networks weight vectors placing distributions induce distributions uncountably inﬁnite space generators discriminators corresponding every possible setting weight vectors. generator represents distribution distributions data. sampling induced prior distribution data instances proceeds follows sample sample pgenerator. posterior inference propose unsupervised semi-supervised formulations note exciting recent pre-print tran brieﬂy mention using variational approach marginalize weights generative model part general exposition hierarchical implicit models nice theoretical exploration related topics graphical model message passing). promising approach several differences representation quite different preserving clear competition generator discriminator; representation posteriors straightforward requires interventions provides novel formulations unsupervised semi-supervised learning state results many benchmarks. conversely tran pursued fully supervised learning small datasets; sampling explore full posterior weights whereas tran perform variational approximation centred modes posterior marginalize addition ratio estimation approach limits size neural networks whereas experiments comparably deep networks maximum likelihood approaches. experiments illustrate practical value formulation. although high level concept bayesian informally mentioned various contexts best knowledge present ﬁrst detailed treatment bayesian gans including novel formulations sampling based inference rigorous semi-supervised learning experiments. priors parameters generator discriminator hyperparameters respectively. numbers mini-batch samples discriminator generator respectively. deﬁne {x}nd intuitively understand formulation starting generative process data samples. suppose sample weights prior condition sample weights form particular generative neural network. sample white noise transform noise network generate candidate data samples. discriminator conditioned weights outputs probability candidate samples came data distribution. says discriminator outputs high probabilities posterior increase neighbourhood sampled setting posterior discriminator weights ﬁrst terms form discriminative classiﬁcation likelihood labelling samples actual data versus generator belonging separate classes. last term prior following similar derivation speciﬁc setup several nice features monte carlo integration. first white noise distribution take efﬁcient exact samples. secondly viewed function reasonably broad construction since used produce candidate data samples generative procedure. thus term simple monte carlo typically makes reasonable contribution total marginal posterior estimates. note however approximation typically worse conditioning minibatch data equation classical gans maximum likelihood proposed probabilistic approach natural bayesian generalization classical uses uniform priors performs iterative optimization instead posterior sampling local optima algorithm goodfellow thus sometimes refer classical ml-gan. moreover even prior difference bayesian marginalization whole posterior versus approximating posterior point mass optimization posterior samples iteratively sampling every step epoch limit obtain samples approximate posteriors samples useful practice. indeed different samples alleviate collapse generate data samples appropriate level entropy well forming committee generators strengthen discriminator. samples turn form committee discriminators ampliﬁes overall adversarial signal thereby improving unsupervised learning process. arguably rigorous method assess utility posterior samples examine effect semi-supervised learning focus experiments section extend proposed probabilistic formalism semi-supervised learning. semisupervised setting k-class classiﬁcation access unlabelled observations well observations class goal jointly learn statistical structure unlabelled labels labelled examples order make much better predictions class labels test examples access labelled training inputs. context redeﬁne discriminator gives probability sample belongs class reserve class label indicate data sample output generator. infer posterior weights follows every iteration samples generator unlabeled samples labeled samples typically section approximately marginalize using simple monte carlo sampling. much like unsupervised learning case marginalize posteriors compute predictive distribution class label test input model average collected samples respect posterior bayesian wish marginalize posterior distributions generator discriminator weights unsupervised learning semi-supervised learning purpose stochastic gradient hamiltonian monte carlo posterior sampling. reason choice three-fold sghmc closely related momentum-based know empirically works well training; import parameter settings directly sghmc; importantly many practical beneﬁts bayesian approach inference come exploring rich multimodal distribution weights generator enabled sghmc. alternatives variational approximations typically centre mass around single mode thus provide unimodal overly compact representation distribution asymmetric biases kl-divergence. posteriors equations amenable techniques compute gradients loss respect parameters sampling. sghmc extends case noisy estimates gradients manner guarantees mixing limit large number minibatches. detailed review sghmc please chen using update rules chen propose sample posteriors generator discriminator weights algorithm note algorithm outlines standard momentum-based sghmc practice found help speed burn-in process replacing part algorithm adam ﬁrst thousand iterations revert back momentum-based sghmc. suggested appendix chen employed learning rate schedule decayed according number unique real datapoints seen far. thus learning rate schedule converges limit deﬁned |d|. algorithm iteration sampling bayesian gan. friction term sghmc learning rate. assume stochastic gradient discretization noise term dominated main friction term take simple samples generator discriminator respectively sghmc samples simple sample. rescale accommodate minibatches appendix represent posteriors samples {θjm number iterations evaluate proposed bayesian benchmarks four different numbers labelled examples. consider multiple alternatives including dcgan recent wasserstein ensemble dcgans formed random subsets size training fully supervised convolutional neural network. also compare reported mnist result lfvi-gan brieﬂy mentioned recent pre-print fully supervised modelling whole dataset variational approximation. interpret many results mnist detail section observations carry forward cifar- svhn celeba experiments. real experiments -layer bayesian deconvolutional generative model details structure). corresponding discriminator -layer -class dcgan unsupervised -layer class dcgan semi-supervised performing classiﬁcation classes. connectivity structure unsupervised semi-supervised dcgans bayesgan. note structure networks radford slightly different cannot directly compare results salimans exact architecture speciﬁcation also given codebase. performance methods could improved calibrating architecture design individual benchmark. bayesian place prior generator discriminator weights approximately integrate using simple monte carlo samples. algorithm iterations collect weight samples every iterations record out-of-sample predictive accuracy using bayesian model averaging algorithm experiments performed single titanx consistency bayesgan dcgan- could sped approximately runtime dcgan multi-gpu parallelization. table summarize semi-supervised results consistently improved performance alternatives. runs averaged random subsets labeled examples estimating error bars performance also qualitatively illustrate ability bayesian produce complementary sets data samples corresponding different representations generator produced sampling posterior generator weights supplement also contains additional plots accuracy epoch accuracy runtime semi-supervised experiments. emphasize alternatives required special techniques described salimans mini-batch discrimination whereas proposed bayesian needed none techniques. present experiments multi-modal synthetic dataset test ability infer multi-modal posterior ability helps avoid collapse generator couple training examples instance overﬁtting regular training also allows explore generators different complementary properties harmonizing together encapsulate rich data distribution. generate d-dimensional synthetic data follows regular bayesian dataset generator models two-layer neural network fully connected relu activations. dimensionality order dcgan converge consistently discriminator network following structure fully-connected relu activations. dataset place prior weights bayesian approximately integrate using monte-carlo samples. figure shows bayesian much better qualitatively generating samples quantitatively terms jensen-shannon divergence true distribution fact dcgan begins eventually increase testing certain number training iterations reminiscent over-ﬁtting. still good performance bayesian gan. also multidimensional scaling samples posterior bayesian generator weights clearly form multiple distinct clusters indicating sghmc sampling exploring multiple distinct modes thus capturing multimodality weight space well data space. mnist well-understood benchmark dataset consisting labeled images hand-written digits. salimans showed excellent out-of-sample performance using small number labeled inputs convincingly demonstrating importance good generative modelling semi-supervised learning. here follow experimental setup mnist. evaluate bayesian dcgan semi-supervised learning using labelled training examples. table bayesian improved accuracy dcgan wasserstein even ensemble dcgans moreover remarkable bayesian labelled training examples able achieve testing accuracy comparable state fully supervised method using training examples show fully supervised model using samples generally highlight practical utility semi-supervised learning. moreover tran showed fully supervised lfvi-gan whole mnist training produces classiﬁcation errors almost twice error proposed bayesian approach using labelled examples suspect difference largely comes simple practical formulation bayesian section marginalizing simple monte carlo exploring broad multimodal posterior distribution generator weights sghmc approach versus variational approximation centred single mode. also qualitative differences unsupervised data samples bayesian dcgan standard dcgan figure shows sample images produced generators figure left samples drawn pdata visualized applying pca. right columns samples drawn pmlgan pbgan visualized applying pca. data inherently -dimensional explain variance using principal components. clear bayesian capturing modes data whereas regular unable right jensen-shannon divergence pdata function number iterations training divergence computed using kernel density estimates large sample datasets drawn pdata applying dimensionality reduction cases bayesian effective minimizing jensen-shannon divergence reaching convergence towards true distribution exploring full distribution generator weights possible maximum likelihood sample convergence viewed using multidimensional scaling clearly several clusters meaning sghmc sampling discovered pronounced modes posterior weights. produced samples posterior generator weights bottom shows sample data images dcgan. panels qualitative differences almost different person writing digits panel. panel example quite crisp panel fairly thick panel thin fainter strokes. words bayesian learning different complementary generative hypotheses explain data. contrast data samples bottom dcgan homogenous. effect posterior weight sample bayesian corresponds different style standard dcgan style ﬁxed. difference illustrated datasets figure figure also emphasizes utility bayesian marginalization versus optimization even vague priors. however necessarily expect high ﬁdelity images arbitrary generator sampled posterior generators; fact generator would probably less posterior probability dcgan show section viewed maximum likelihood analogue approach. advantage bayesian approach comes representing whole space generators alongside posterior probabilities. practically speaking also stress convergence maximum-likelihood dcgan resort using tricks including minibatch discrimination feature normalization addition gaussian noise layer discriminator. bayesian dcgan needed none tricks. table detailed supervised semi-supervised learning results datasets. almost experiments bayesgan outperforms dcgan w-dcgan substantially typically even outperforms ensembles dcgans. runtimes epoch minutes provided rows including dataset name. experiments performed single note dcgan- bayesgan methods sped straightforwardly using multiple gpus obtain similar runtime dcgan. note also bayesgan generally much efﬁcient epoch alternatives figure results averaged random supervised subsets stdev. standard train/test splits used mnist cifar- svhn. celeba test size test error rates across entire test set. robustness arises gaussian prior weights provides useful inductive bias mcmc sampling procedure alleviates risk collapse helps explore multiple modes balanced also stress practice risk collapse fully eliminated indeed samples still produce generators create data samples little entropy. practice sampling immune becoming trapped sharply peaked modes. leave analysis future work. figure data samples different generators corresponding samples posterior data samples show explored setting weights produces generators complementary high-ﬁdelity samples corresponding different styles. amount variety samples emerges naturally using bayesian approach. bottom data samples standard dcgan contrast samples homogenous style. cifar- also popular benchmark dataset training test images harder model mnist since data images real objects. figure shows datasets produced four different generators corresponding samples posterior generator weights. mnist meaningful qualitative variation panels. table also using bayesian gans generative model within semi-supervised learning setup signiﬁcantly decreases test error alternatives especially streetview house numbers dataset consists images house numbers taken streetview vehicles. unlike mnist digits signiﬁcantly differ shape appearance. experimental procedure closely followed cifar-. approximately training test images. table particularly pronounced difference performance bayesgan alternatives. data samples shown figure large celeba dataset contains celebrity faces amongst variety backgrounds reduce background variations used standard face detector crop faces standard size. figure shows data samples trained bayesian gan. order assess performance semi-supervised learning created -class classiﬁcation task predicting -bit vector indicating whether face blond glasses male pale young. table shows pattern promising performance celeba. exploring rich multimodal distributions weight parameters generator bayesian capture diverse complementary interpretable representations data. shown representations enable state performance semi-supervised problems using simple inference procedure. effective semi-supervised learning natural high dimensional data crucial reducing dependency deep learning large labelled datasets. often labeling data option comes high cost human labour expensive instrumentation moreover semi-supervised learning provides practical quantiﬁable mechanism benchmark many recent advances unsupervised learning. although mcmc recent years variational approximations favoured inference bayesian neural networks. however likelihood deep neural network broad many shallow local optima. exactly type density amenable sampling based approach explore full posterior. variational methods contrast typically centre approximation along single mode also provide overly compact representation mode. therefore future generally advantages following sampling based approach bayesian deep learning. aside sampling could better accommodate likelihood functions common deep learning using general divergence measures instead divergence variational methods alongside ﬂexible proposal distributions. future could also estimate marginal likelihood probabilistic integrated away distributions parameters. marginal likelihood provides natural utility function automatically learning hyperparameters performing principled quantiﬁable model comparison different architectures. would also interesting consider bayesian conjunction non-parametric bayesian deep learning framework deep kernel learning hope work help inspire continued exploration bayesian deep learning. goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets. advances neural information processing systems pages nowozin cseke tomioka f-gan training generative neural samplers using variational divergence minimization. advances neural information processing systems pages supplementary material provide futher details mcmc updates illustrate tutorial ﬁgure show data samples bayesian svhn cifar- celeba give performance results function iteration runtime. rescaling conditional posteriors accommodate mini-batches updates algorithm involve iteratively computing semi-supervised learning case equations evaluated minibatch data necessary scale likelihood follows example total number training points increases likelihood dominate prior. re-scaling conditional posterior well semi-supervised objectives follow similarly. figure illustrate multimodal posterior parameters generator. setting parameters corresponds different generative hypothesis data. show samples generated different settings weight vector corresponding different writing styles. bayesian retains whole distribution parameters. contrast standard represents whole distribution point estimate missing potentially compelling explanations data. figure test accuracy function iteration number. sg-hmc iterations sampler mixing reasonably well. also iteration bayesian sg-hmc learning data distribution efﬁciently alternatives. figure data samples cifar svhn celeba datasets four different generators created using four different samples posterior panel corresponding different different qualitative properties showing complementary nature different aspects distribution learned using fully probabilistic approach. figure larger data samples cifar four different generators created using four different samples posterior panel corresponding different different qualitative properties showing complementary nature different aspects distribution learned using fully probabilistic approach. figure larger data samples svhn four different generators created using four different samples posterior panel corresponding different different qualitative properties showing complementary nature different aspects distribution learned using fully probabilistic approach. figure larger data samples celeba four different generators created using four different samples posterior panel corresponding different different qualitative properties showing complementary nature different aspects distribution learned using fully probabilistic approach.", "year": 2017}