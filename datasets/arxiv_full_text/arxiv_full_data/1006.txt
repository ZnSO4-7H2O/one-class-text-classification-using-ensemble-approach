{"title": "Practical Riemannian Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We provide the first experimental results on non-synthetic datasets for the quasi-diagonal Riemannian gradient descents for neural networks introduced in [Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as a previously unpublished electroencephalogram dataset. The quasi-diagonal Riemannian algorithms consistently beat simple stochastic gradient gradient descents by a varying margin. The computational overhead with respect to simple backpropagation is around a factor $2$. Perhaps more interestingly, these methods also reach their final performance quickly, thus requiring fewer training epochs and a smaller total computation time.  We also present an implementation guide to these Riemannian gradient descents for neural networks, showing how the quasi-diagonal versions can be implemented with minimal effort on top of existing routines which compute gradients.", "text": "provide ﬁrst experimental results non-synthetic datasets quasidiagonal riemannian gradient descents neural networks introduced include mnist svhn face datasets well previously unpublished electroencephalogram dataset. quasi-diagonal riemannian algorithms consistently beat simple stochastic gradient gradient descents varying margin. computational overhead respect simple backpropagation around factor perhaps interestingly methods also reach ﬁnal performance quickly thus requiring fewer training epochs smaller total computation time. also present implementation guide riemannian gradient descents neural networks showing quasi-diagonal versions implemented minimal eﬀort existing routines compute gradients. present practical eﬃcient implementation invariant stochastic gradient descent algorithms neural networks based quasi-diagonal riemannian metrics introduced implemented data rmspropadagrad-based schemes namely collecting gradients squared gradients data sample. thus present easily incorporated existing software providing gradients neural networks. main goal algorithms obtain invariance properties neural network insensitivity training algorithm whether logistic tanh activation function used insensitivity simple changes variables parameters scaling parameters. neither backpropagation adagrad-like schemes oﬀer properties. invariance properties important reduce number arbitrary design choices guarantee observed good behavior instance transfer cases when e.g. diﬀerent scalings involved. cases alleviate burden associated hyper-parameter tuning turn sensible hyper-parameter values sometimes become invariant. perhaps well-known invariant training procedure statistical learning natural gradient promoted amari however natural gradient rarely used practice training neural networks. main reason fisher information metric relies computationnally hard compute. diﬀerent approximations proposed bring little beneﬁts compared implementation eﬀort; instance moreover clear whether approximations preserve invariance properties. simpler techniques proposed patch ﬂaws plain stochastic gradient descent. using balanced initialization rectiﬁed linear units dropout adagrad enough obtain good results many datasets. nevertheless tricks trade little justiﬁed mathematical point view. riemannian framework neural networks allows deﬁne several quasi-diagonal metrics exactly keep invariance properties natural gradient smaller computational cost. quasi-diagonal structure means gradient obtained step optimization preconditioned inverse matrix almost diagonal well-chosen non-diagonal terms make easy invert special algebraic structure ensure invariance respect simple transformations changing sigmoid tanh activation function. report assess performance quasi-diagonal metrics turn quite competitive even taking account computational overhead methods consistently improve performance various amounts depending dataset also work well dropout. especially convergence fast early training procedure opens door using fewer epochs shorter overall training times. derivative w.r.t. iteration scheme homogeneity problem terms physical units unit derivative inverse unit means learning rate homogeneity gradient descent homogeneous. reason sensible learning rates change problem another. adagrad-like methods solve half problem rescaling recent magnitude becomes order learning rate homogeneity particularly relevant known advance relevant parameter values order themselves; learning rate still needs value compatible order magnitude instance rank-one approximation deﬁned reference euclidean norm diﬀerence exact approximated fisher matrices thus parameterizationindependent. white black inputs encoded black white round equivalent changing inputs sigmoid unit bias weights reacts sigmoid unit reacts gradient descent behave diﬀerently gradient descent correspondence mixes weights bias. leads potentially diﬀerent performances depending whether inputs white-on-black black-onwhite shown figure speciﬁc problem solved using tanh activation instead sigmoid would nice optimization procedures insensitive simple changes. mathematically problem gradient descent diﬀerentiable function deﬁned abstract vector space well-deﬁned choice basis. indeed point diﬀerential linear form taking vector argument returning scalar derivative direction current point diﬀerential converted vector deﬁnition gradient unique vector vectors clearly depends deﬁnition inner product h··i. non-orthonormal basis symmetric positive-deﬁnite matrix deﬁning using vanilla gradient descent amounts deciding whatever basis currently working basis orthonormal. diﬀerent parameterizations intrinsic quantity vanilla gradient descent produces completely diﬀerent trajectories eventually reach diﬀerent local minima. striking example consider points obtained gradient step varying inner product span whole half-space notice inner product deﬁnes norm gives notion length points choice inner product makes easier diﬃcult gradient descent move certain directions. indeed gradient descent equivalent general easy deﬁne relevant choice inner product h··i nothing known problem. however statistical learning possible canonical choice inner product depends current point point inner product vectors v>mv particular positive deﬁnite matrix depending several choices given below property scalar product associated metric kvkθ depend choice basis vector space depend basis since expression vectors does.) resulting riemannian gradient trajectory thus invariant change basis actually learning rate tends resulting continuous-time trajectory invariant smooth homeomorphism space linear ones trajectory deﬁned seen manifold. thus main idea behind invariance coordinate changes deﬁne inner product depend numerical values used represent parameter give several constructions together associated gradient update involving invariant gradient descents neural networks. machine learning training dataset datum associated target label; denote xn’s likewise suppose model neural network input produces output depending parameter trained. suppose outputs interpreted probability distribution possible targets instance classiﬁcation task vector containing probabilities various possible labels. regression task might assume actual target follows normal distribution centered predicted value namely probability distribution targets deﬁned output ˆex∈dx empirical average feature vectors dataset denotes vector derivatives loss respect parameter neural networks latter computed backpropagation. note expression involves losses possible values targets actual target data sample. however natural gradient several features make unsuitable large-scale learning tasks. literature natural gradients neural networks since amari’s work deals issues. first fisher matrix full matrix size dim×dim makes costly invert impossible store. instead low-storage easily inverted quasi-diagonal version matrices keeps many invariance properties full matrix. second fisher matrix involves expectation pseudo-targets drawn distribution deﬁned output network. might problem classiﬁcation tasks number possibilities small requires another approach gaussian model want avoid numerical integration y+σn describe three ways around this outer product approximation monte carlo approximation exact version requires backpropagations sample. third fisher matrix given value parameter whole dataset. thus natural gradient update would need whole sweep dataset compute fisher matrix current parameter. moving average instead. quasi-diagonal riemannian metrics. instead full fisher matrix quasi-diagonal reduction involves computing storing diagonal terms oﬀ-diagonal terms matrix quasi-diagonal reduction introduced exactly keep invariance properties fisher matrix price close diagonal matrices. maintaining ﬁrst block accounts possible correlations biases weights appearing input values transformed bias plays special role ﬁrst parameter block. operations need perform matrix types computing adding rank-one contributions form applying inverse vector parameter update quasi-diagonal matrices operations explicited algorithms cost twice using diagonal matrices. online riemannian gradient descent metric initialization. fisher matrix given value parameter whole dataset natural gradient update would need whole sweep dataset compute fisher matrix current parameter. suitable batch learning online stochastic gradient descent. metric update rate mminibatch metric computed small subset data i.e. replacing empirical average ˆex∈dx empirical minibatch typically subset average ˆex∈d gradient loss computed stochastic gradient scheme. results online riemannian gradient descent. minibatches ensures metric mostly renewed whole sweep dataset. usually initialize metric ﬁrst minibatch empirically better using identity metric ﬁrst update. startup using small minibatches advisable initialize metric larger number samples ﬁrst parameter update. natural gradient outer product monte carlo natural gradient. fisher matrix involves expectation pseudo-targets drawn distribution deﬁned output network backpropagation needed possible value order compute ∂θ‘. acceptable classiﬁcation ˆe∈d thus replacing ˆex∈dx e˜t∼p empirical average ˆe∈d. training sample compute rank-one matrix given outer product gradient sample hence name. method sometimes used directly name natural gradient although diﬀerent properties discussion retrieve data sample corresponding target forward network; compute loss backpropagate compute derivative loss ∂θ‘; update quasi-diagonal metric using retrieve data sample corresponding target forward network; compute loss backpropagate compute derivative loss ∂θ‘; generate pseudo-target according probability distribution deﬁned output layer network backpropagate compute derivative loss ∂θ‘; update quasi-diagonal metric using ˜v˜v> third option compute expectation exactly using algebraic properties fisher matrix. done cost backpropagations sample instead one. details depend type output layer follows. k-th component network output. derivative ∂θyk obtained backpropagation backpropagation initialized setting k-th output unit units done separately output unit thus input metric contribution several rank-one terms ∂θyk> obtained backpropagation weighted another model predicting binary data bernoulli output activities output units interpreted probabilities retrieve data sample corresponding target forward network; compute loss backpropagate compute derivative loss ∂θ‘; depending output layer interpretation categorical output classes notably monte carlo approximation keep invariance properties natural gradient. case natural gradient approximations blockwise rank-one approximation used approaches strengths weaknesses. easy implement relies quantities computed anyway monte carlo exact natural gradient must make backpropagation passes values output network. model data well distribution gives high probability actual targets thus metric good approximation fisher metric. however startup model data might poorly approximate fisher metric. similarly output model misspeciﬁed perform badly even last stages optimization; instance fails miserably optimize quadratic function absence noise example keep mind. exact quasi-diagonal natural gradient aﬀordable dimension output network large backpropagations sample required. thus monte carlo approximations appealing output dimensionality large typical auto-encoders. however case probability distribution deﬁned high-dimensional space approximation using single deterministic point poor. approximation performed poorly auto-encoding task.) hand monte carlo integration often performs relatively well high dimension might sensible choice large-dimensional outputs auto-encoders. diagonal versions adagrad invariance properties. algorithms presented quasi-diagonal matrices also diagonal version obtained simply discarding non-diagonal terms. corresponding log-loss gaussian model variance data points gradient loss matrix square gradient gradient descent ησ/θ. much slow startup much fast ﬁnal convergence. hand natural gradient behaves nicely. catastrophic behavior small reﬂects fact data variance behavior large reﬂects fact data follow model startup. cases approximation unjustiﬁed hence huge diﬀerence natural gradient. monte carlo natural gradient behave better instance. particular divergence gradient close disappears soon noise data. mada follows update thus although adagrad shares similar framework naturally interpreted riemannian metric parameter space taking element-wise square-root breaks potential invariances. summarize diagonal versions dmcnat dnat exactly invariant rescaling parameter component. addition quasi-diagonal versions algorithms exactly invariant aﬃne change activity unit including activities input units; covers instance using tanh instead sigmoid using white-on-black instead black-on-white inputs. contrast knowledge adagrad invariance properties experimental studies. demonstrate invariance eﬃciency riemannian gradient descents experiments series classiﬁcation regression tasks. task choose architecture activation function perform grid search various powers step-size. step-size kept ﬁxed optimization. then algorithm report curve associated best step size. algorithms tested standard stochastic gradient descent adagrad diagonal quasi-diagonal versions monte carlo natural gradient exact natural gradient dimension output layer large compute first study classiﬁcation task mnist street view housing numbers work permutation-invariant setting i.e. network convolutional natural topology image taken account network structure. also converted svhn images grayscale images order reduce dimensionality input. note experiments small-scale geared towards obtaining stateof-the-art performance comparing behavior several algorithms common ground relatively small architectures. experiments classiﬁcation task conﬁrm riemannian algorithms invariant eﬃcient non-invariant algorithms. ﬁrst experiment train network hidden layers hidden units layer without regularization mnist; results given figure quasi-diagonal algorithms converge faster algorithms. especially exhibit steep slope ﬁrst epochs quickly reach satisfying performance. trajectories also similar every activation function. particular used test sets datasets validation sets extracted training sets. want information test sets testing riemannian algorithms architectures done future work. note invariance properties guarantee similar performance sigmoid tanh represent equivalent models using diﬀerent variables necessarily relu diﬀerent model. trajectories adagrad diagonally approximated riemannian algorithms variable instance close enough quasi-diagonal algorithms relu activation sigmoid tanh adagrad performs well tanh sigmoid relu. finally monte carlo natural gradient seems good approximation exact natural gradient even monte carlo sample. related simplicity problem since probabilities output layer converges rapidly optimal values. quasi-diagonal algorithms also eﬃcient practice. computational cost reasonably close pure shown table overhead could expected. often much faster learn terms number training examples processed compared adagrad. quasi-diagonal algorithms also behave well dropout regularization seen figure several runs went classiﬁcation error simple permutation-invariant architecture. three groups trajectories clearly visible figure adagrad fairly close quasi-diagonal algorithms example. clear diﬀerence quasi-diagonal algorithms diagonal approximations keeping diagonal breaks invariance aﬃne transormations activities thus appears factor well almost experiments below. diﬃcult dataset permutation invariant svhn grayscale images observe pattern mnist dataset quasi-diagonal algorithms leading adagrad however relu severely impacting diagonal approximations riemannian algorithms diverge step-sizes larger emphasizes importance quasi-diagonal terms. also test algorithm deeper architecture mnist dataset goal testing whether riemannian algorithms handle vanishing gradients better. sparse network connectivity factor incoming weights unit. connection graph built output input randomly choosing unit units previous layer. last output layer fully connected. choose network hidden layers following architecture ------. number parameters relatively small since architecture deep diﬃcult problem already purely optimization task figure observe three groups slow converge quasi-diagonal algorithms reach quite small loss values training perform reasonably well validation set. adagrad diagonal approximations stand between. more quasi-diagonal algorithms steep descent ﬁrst epochs. note various plateaus several algorithms reach small loss values; related numerical issues small values especially used ﬁxed step size. indeed plateaus minor instabilities cause small rises loss values. disappear step sizes tending schedule. next evaluate riemannian gradient descents regression task namely reconstruction inputs. mnist dataset faces wild dataset permutation-invariant non-convolutional setting. faces dataset crop border pixels around image convert grayscale image. faces still recognizable lower resolution variant dataset. dataset autoencoder three hidden layers gaussian output. also dataset signal recordings. signals captured electrodes measurements. goal compress signals hidden units bottleneck layer still able reconstruct signal well. notice signals noisy. dataset autoencoder seven hidden layers gaussian output. setting outer product monte carlo natural gradient well-suited exact natural gradient scales like dimension figure classiﬁcation task mnist --------- architecture sigmoid activation function. sparse network parameters without regularization. training trajectory left validation trajectory right. experiment adagrad riemanndmcnat stuck epoch around mean square error fact continue minimize loss function slowly cannot observed ﬁgure. behavior consistent every step-size algorithms related ﬁnding local optimum. finally trained autoencoder multivariate gaussian output mnist variances outputs also learned time network parameters. depicted figure performances consistent previous experiments. interestingly qdop eﬃcient algorithm even though real noise model outputs departs diagonal gaussian model output approximation natural gradient necessarily accurate. learning rates regularization. riemannian algorithms still sensitive choice gradient step-size also numerical regularization term experiments. numerical regularization term necessary ensure metric invertible. however term also breaks invariance thus chosen small possible limit numerical stability. practice riemannqdmcnat seems sensitive numerical stability riemannqdop riemannqdnat. mnist data quantized values constrained standard deviations larger otherwise reported negative log-likelihoods reach arbitrarily large negative values error becomes smaller quantization threshold. gradient descents based quasi-diagonal riemannian metrics including quasi-diagonal natural gradient easily implemented existing framework computes gradients using routines qdrankoneupdate qdsolve. overhead respect simple factor typical situations. resulting quasi-diagonal learning algorithms perform quite consistently across board whereas performance simple adagrad sensitive design choices using relu sigmoid activations. quasi-diagonal learning algorithms exhibit fast improvement ﬁrst epochs training thus reaching ﬁnal performance quite fast. eventual gain well-tuned adagrad trained many epochs small large depending task. quasi-diagonal riemannian metrics widely outperform diagonal approximations latter necessarily perform better classical algorithms. supports speciﬁc inﬂuence invariance properties performance interest designing algorithms invariance properties mind.", "year": 2016}