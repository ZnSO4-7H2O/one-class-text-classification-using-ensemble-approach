{"title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art in a variety of machine learning tasks and are deployed in increasing numbers of products and services. However, the computational requirements of training and evaluating large-scale DNNs are growing at a much faster pace than the capabilities of the underlying hardware platforms that they are executed upon. In this work, we propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep) to reduce the computational requirements of DNNs during inference. Previous efforts propose specialized hardware implementations for DNNs, statically prune the network, or compress the weights. Complementary to these approaches, DyVEDeep is a dynamic approach that exploits the heterogeneity in the inputs to DNNs to improve their compute efficiency with comparable classification accuracy. DyVEDeep equips DNNs with dynamic effort mechanisms that, in the course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computa- tions, while skipping or approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks - one for CIFAR-10 and four for ImageNet (AlexNet, OverFeat and VGG-16, weight-compressed AlexNet). Across all benchmarks, DyVEDeep achieves 2.1x-2.6x reduction in the number of scalar operations, which translates to 1.8x-2.3x performance improvement over a Caffe-based implementation, with < 0.5% loss in accuracy.", "text": "deep neural networks advanced state-of-the-art variety machine learning tasks deployed increasing numbers products services. however computational requirements training evaluating large-scale dnns growing much faster pace capabilities underlying hardware platforms executed upon. work propose dynamic variable effort deep neural networks reduce computational requirements dnns inference. previous efforts propose specialized hardware implementations dnns statically prune network compress weights. complementary approaches dyvedeep dynamic approach exploits heterogeneity inputs dnns improve compute efﬁciency comparable classiﬁcation accuracy. dyvedeep equips dnns dynamic effort mechanisms that course processing input identify critical group computations classify input. dyvedeep dynamically focuses compute effort critical computations skipping approximating rest. propose effort knobs operate different levels granularity viz. neuron feature layer levels. build dyvedeep versions popular image recognition benchmarks cifar- four imagenet across benchmarks dyvedeep achieves .×-.× reduction number scalar operations translates .×-.× performance improvement caffe-based implementation loss accuracy. deep neural networks greatly advanced state-of-the-art variety machine learning tasks different modalities including image video text natural language processing. however computational standpoint dnns highly compute data intensive workloads. example topologies imagenet large-scale visual recognition contest past years contain million parameters require giga operations compute classify single image. requirements projected increase future data sets larger sizes topologies larger complexity actively explored. indeed growth computational requirements dnns outpaced improvements capabilities commodity computational platforms recent years. scenarios exemplify computational challenges imposed dnns large-scale training dnns trained massive data-sets using high-performance server clusters cloud low-power inference models evaluated energy-constrained platforms mobile deeply-embedded devices. towards addressing latter challenge propose dynamic variable effort deep neural networks dynamic approach improve computational efﬁciency inference. related research directions. prior research efforts improve computational efﬁciency dnns classiﬁed broad directions. ﬁrst comprises parallel implementations dnns commercial multi-core gpgpu platforms. parallelization strategies model data hybrid parallelism techniques asynchronous -bit alleviate communication overheads representative examples. next efforts design specialized hardware accelerators realize dnns trading programmability cost specialized hardware design effort efﬁciency. spectrum architectures ranging low-power cores large-scale systems proposed chen jouppi). third efforts focus developing device technologies whose characteristics intrinsically match computational primitives neural networks leading improvements energy efﬁciency ramasubramanian ﬁnal efforts exploit fact dnns typically over-parametrized non-convex nature optimization space therefore approximate dnns statically pruning network connections representing weights reduced precision and/or compressed format thereby improving compute efﬁciency negligible loss classiﬁcation accuracy venkataramani anwar dyvedeep motivation concept. contrast efforts proposal dynamic variable effort deep neural networks leverages heterogeneity characteristics inputs improve compute efﬁciency. motivation behind dyvedeep stems following insights. first real-world data inputs created equal i.e. inputs vary considerably difﬁculty. intuitively inputs close decision boundary require full effort classiﬁer rest could classiﬁed much simpler decision boundary. context dnns increasing network size provides valuable nevertheless diminishing increase accuracy. example context imagenet increasing network’s computational requirements yields additional increase classiﬁcation accuracy. raises question whether inputs classiﬁed substantially fewer computations expending increased effort inputs require second given input effort needs expended across different parts network. example image recognition problem computations corresponding neurons operate image region object interest located critical classiﬁcation output others. also features less relevant others context given input. example features detect sharp edges less relevant current input comprised mostly curved surfaces. notwithstanding observations state-of-the-art dnns static i.e. computationally agnostic nature input processed expend computational effort inputs leads signiﬁcant inefﬁciency. dyvedeep addresses limitation dynamically predicting computations critical classify given input focusing compute effort computations skipping approximating rest. effect network expends computational effort different subsets computations input reducing computational requirements case without sacriﬁcing classiﬁcation accuracy. dynamic effort knobs. efﬁciency dyvedeep lies favorably navigating trade-off cost identifying critical computations beneﬁts accrued skipping approximating computations. identify three dynamic effort mechanisms different levels granularity viz. neuron feature layer-levels. mechanisms employ run-time saturation prediction early termination operates neuron-level. monitors intermediate output neuron processing subset inputs predicts likelihood neuron eventually saturating applying activation function. partial deep within saturation regime computations corresponding neuron deemed non-critical skipped. signiﬁcance-driven selective sampling operates within feature exploits spatial locality neuron activations. uniformly spatially sampled version feature ﬁrst computed. activations remaining neuron either approximated accurately computed based magnitude variance neighbors. similarity-based feature approximation operates layer level examines similarity neuron activations feature map. neuron activations similar convolution operation feature approximated single scalar multiplication average neuron activation value precomputed kernel weights. develop systematic methodology identify hyper-parameters mechanisms training phase given dnn. built dyvedeep versions popular benchmarks viz. cifar- alexnet overfeat-accurate vgg- weight-compressed alexnet model. experiments demonstrate dynamically exploiting heterogeneity across inputs dyvedeep achieves .×-.× reduction total number scalar operations loss classiﬁcation accuracy. reduction scalar operations translates .×-.× improvement performance software implementation dyvedeep using caffe deep learning framework intel xeon .ghz server memory. rest paper organized follows. section describes prior research efforts related dyvedeep. section details proposed dynamic effort mechanisms integrated dyvedeep. section outlines methodology used experiments. experimental results presented section section concludes paper. section provide brief summary prior research efforts related dyvedeep highlight distinguishing features work. prior research improving computational efﬁciency dnns follows distinct directions. ﬁrst class efforts focus parallelizing dnns commercial multi-cores gpgpu platforms. different work distribution strategies model data hybrid parallelism hardware transparent on-chip memory allocation/management schemes virtualized dnns representative examples. second class efforts design specialized hardware accelerators realize computation kernels dnns. range architectures targeting low-power mobile devices high-performance server clusters jouppi) explored. third efforts investigate device technologies whose characteristics intrinsically match compute primitives present dnns. memristor-based crossbar array architectures spintronic neuron designs representative examples. ﬁnal efforts improve efﬁciency approximating computations dnn. dyvedeep falls category propose dynamically skip approximate computations based criticality context given input. therefore describe approaches fall category detail. classify approaches static dynamic optimizations. static techniques almost efforts approximate computations dnns static nature i.e. apply approximation uniformly across inputs. static techniques primarily reduce model size dnns using mechanisms pruning connections reducing precision computations anwar storing weights compressed format example context fully connected layers hashnets hash function randomly group weights bins share common parameter value thereby reducing number parameters needed represent network. deep compression attempts prune connections network adding regularization term training removing connections weights certain threshold. context convolution layers denton jaderberg exploit linear structure network suitable rank approximation. hand propose sparse convolutional dnns wherein almost parameters kernels zeroed adding weight sparsity term objective function. contrast mathieu demonstrate performing convolution fourier domain yield substantial improvement efﬁciency. finally /citedblpjournals/corr/figurnovvk propose perforated cnns subset neurons feature evaluated. neurons evaluated feature determined statically training time. dynamic techniques. dynamic optimizations adapt computations approximated based input currently processed. dynamic techniques powerful statically optimised dnns capture additional input-dependent opportunities efﬁciency static methods lack. notwithstanding this little focus devoted developing dynamic approximation techniques. ﬁrst efforts direction utilizes stochastic neurons gate regions within dnn. along similar lines frey propose standout dropout probability neuron estimated using binary belief network. dropout mask computed network shot conditioned input network. bengio extends similar idea wherein dropout distribution layer computed based output preceding layer. dynamic effort mechanisms proposed dyvedeep qualitatively different aforementioned efforts. rather stochastically dropping computations effort knobs dyvedeep exploit properties saturating nature activation directly predict effect approximation neuron output. further prior dynamic approaches applied fully-connected networks trained small datasets. applicability large-scale dnns remains unexplored. hand dyvedeep naturally applicable convolutional fully connected layers demonstrate substantial beneﬁts large-scale networks imagenet. idea behind dyvedeep improve computational efﬁciency dnns modulating effort expend based input processed. shown figure achieve equipping dynamic effort mechanisms dynamically predict criticality groups computations overhead correspondingly skip approximate them thereby improving efﬁciency negligible impact classiﬁcation accuracy. identify three dynamic effort mechanisms dnns operate different levels granularity. also propose methodology tune hyper-parameters associated mechanisms variable effort versions obtained negligible loss classiﬁcation accuracy. saturation prediction early termination works ﬁnest level granularity level neuron dnn. case leverage fact almost convolutional fully connected layers followed activation function saturates least side. example commonly used rectiﬁed linear unit activation function saturates truncating negative inputs zero passing positive inputs idea spet actual value weighted impact neuron’s output provided eventually cause neuron’s activation function saturate. case relu unnecessary compute actual eventually negative value negative value would result neuron output zero. based observation shown figure spet monitors partial weighted neuron predeﬁned fraction inputs multiplied-and-accumulated. spet predicts whether ﬁnal partial would cause neuron’s activation function saturate. introduce following hyper-parameters etlt hresh etut hresh thresholds partial value neuron. time prediction shown equation partial found smaller etlt hresh greater etut hresh partial computation terminated early appropriate saturated activation function value returned neuron’s output. continue completely evaluate partial value neuron. note activation function saturates direction spet thresholds useful predict saturation. example case relu etlt hresh used predict saturation. demonstrate potential beneﬁts spet figure shows fraction neurons convolutional layers cifar- saturate. neuron activations zeros relu activation function. figure also reveals fraction neurons saturating increases proceed deeper network. observed similar trends larger networks alexnet overfeat. since majority neuron activations saturation prediction interval. aspect spet interval predict saturation. hand predicting saturation processing small number inputs neuron would frequently result prediction incorrect leading loss classiﬁcation accuracy. hand larger prediction interval yields progressively smaller computational savings. quantifying trade-off figure illustrates cifar- fraction neuron predicted saturated correctly various prediction intervals. illustration figure assume etlt hresh i.e. neuron predicted saturate partial point prediction negative. fraction neurons predicted correctly increases prediction interval. etlt hresh etut hresh hyper-parameters determined training. note prediction interval could also learnt training process. however found simpler scheme prediction interval worked quite well practice. rearranging neuron inputs. spet effective weights processed decreasing order magnitude larger weights likely impact partial sum. however feasible practise affects regularity memory access pattern directly offsetting savings skipping computations. also case convolutional layers prediction interval inputs half feature maps ignored time prediction. maximize range inputs processed prediction maintaining regularity memory access pattern rearrange neuron inputs indexed inputs processed ﬁrst prediction made. even indexed inputs computed neuron predicted saturate. signiﬁcance-driven selective sampling operates granularity feature convolutional layers dnn. sdss leverages spatial locality neuron activations within feature. example context images adjacent pixels input image frequently take similar values. neuron activations computed sliding kernel image spatial locality naturally permeates feature outputs convolutional layers. behavior also observed deeper layers network. fact saturating nature activation function enhances locality variations weighted neighbors masked fall within saturation regime. sdss adopts -step process exploit spatial locality within features. uniform feature sampling. ﬁrst step compute activation values subset neurons feature uniformly sampling feature. purpose deﬁne parameter denotes periodicity sampling dimension. value chosen based size feature correlation adjacent neuron activations. experiments used sampling period across convolutional layers dnn. signiﬁcance-driven selective evaluation. second step shown figure selectively approximate activation values neurons sampled ﬁrst step. deﬁne following hyper-parameters maximum activation value threshold delta activation value threshold neuron feature computed examine activation values immediate neighbors directions compute maximum range neighbors’ activation values. maximum value axactthresh threshold thus sdss effort knob utilizes magnitude variance neighbors gauge whether neuron lies within region interest accordingly expends computational effort compute activation value. similarity-based feature approximation also exploits correlation activation values feature different way. sdss spatial locality exploited computing neuron activations themselves. contrast case sfma spatial locality used approximate computations feature input. consider convolutional layer input features neuron activations similar other. convolution operation performed input feature sliding kernel matrix entries convolution output likely close other. therefore shown figure approximate entire convolution operation follows. first average value neuron activations feature computed. next weights kernel matrix evaluated. note precomputed stored along kernel matrix. approximate outputs convolution product average input activation kernel weights. equation convoutw convolution output window size kernel size. mean activation values feature. approximation valid negligible. determine convolutions apply aforementioned approximation deﬁne following hyper-parameters feature sizes large check variance across entire feature. instead split feature multiple regions overlap dimension size kernel window. check variance within region variance arthresh kernel windows entirely within region approximated. describe different effort knobs—spet sdss sfma—are combined dyvedeep. since effort knob operates different level granularity easily integrated other. combine spet sdss neuron activation uniformly sampled features sdss computed spet. however apply spet neurons selectively computed sdss located midst neurons large activation values and/or variance hence unlikely saturate. sfma fundamentally amounts grouping inputs neuron single input therefore directly process evaluating neuron spet/sdss. summary spet effort knob applies convolutional fully connected layers dnns effective majority neurons saturate. since convolutional layers towards middle large number inputs neuron contain substantial fraction saturated neurons expect spet beneﬁcial layers. sdss effort knob primarily applies convolutional layers effective features sizes large. therefore initial convolutional layers would beneﬁt sdss. hand sfma works best large number features layer feature sizes small. hence middle later convolutional layers likely beneﬁt sfma. described previous subsections dynamic effort knobs together contain hyper parameters viz. etlt hresh etut hresh axactthresh delactthresh sigthresh arthresh. hyper-parameters control aggressively effort knobs skip approximate computations thereby yielding direct trade-off computational savings classiﬁcation accuracy. using pre-trained network training dataset systematically determine dyvedeep hyper-parameters model deployed. ideally could deﬁne parameters uniquely neuron dnn. example neuron could unique etlt hresh threshold predict saturates arthresh threshold deem input feature approximated partial evaluation clearly results prohibitively large hyper-parameter search space adds substantial overhead overall size model. since neurons given layer computationally similar deﬁne hyper-parameters layer-wise granularity i.e. neurons within layer share hyper-parameters. also since benchmarks utilized relu activation function ignored etut hresh identifying hyper-parameter conﬁguration. algorithm shows pseudocode hyper-parameter tuning process. empirically observed parameters corresponding effort knob independently tuned. therefore adopt strategy wherein ﬁrst identify range possible values hyper-parameter. since computational savings monotonically increase decrease value parameter perform greedy binary search range. range parameter identiﬁed follows. etlt hresh axactthresh parameters vary entire range values partial neurons take layer. however typically observe zero good lower bound parameters relu sets negative values upper bound determined evaluating input training dataset recording maximum partial value layer. parameters delactthresh sigthresh arthresh naturally lowered bounded thresholds absolute magnitudes. similar etlt hresh axactthresh upper limit parameters also estimated evaluating training set. given hyper-parameter range highest possible value parameter yields maximum computation savings adversely affects classiﬁcation accuracy. extreme lowest value parameter impact classiﬁcation accuracy. however yields computation savings fact adds penalty criticality prediction. therefore perform binary search range identify highest value parameter yields negligible loss classiﬁcation accuracy case sfma observed hyper-parameters need searched together. since range arthresh coarser sigthresh loop values arthresh search possible values sigthresh case. summary embedding dynamic effort knobs dnns dyvedeep seamlessly varies computational effort across inputs achieve signiﬁcant computational savings maintaining classiﬁcation accuracy. section describe methodology used experiments evaluate dyvedeep. benchmarks. evaluate dyvedeep utilized pre-trained models available publicly caffe model benchmark repository. reinforces dyvedeep’s ability adapt given trained network. used following benchmarks experiments cifar- caffe network cifar- dataset alexnet overfeat-accurate vgg- compressed alexnet imagenet ilsvrc data inputs imagenet dataset generated using center crop images test set. randomly selected test inputs used validation tune hyper parameters. report speedup classiﬁcation accuracy results remaining test inputs. performance measurement. implemented dyvedeep within caffe deep learning framework however could directly integrate dyvedeep within caffe composes computations within layer given batch size single gemm operation offered blas libraries. blas libraries speciﬁcally optimize matrix operations assembly level. since dyvedeep requires ﬁne-grained computation skipping/approximation unable directly incorporate within routines. therefore prototyped implementation convolutional layers within caffe used experiments. experiments conducted intel xeon server operating .ghz frequency memory. added performance counters dyvedeep baseline implementation measure software execution time. timing results reported single-threaded sequential execution. also experiments introduced dynamic effort knobs convolutional layers dominated overall runtime benchmarks. however note reported execution times performance beneﬁts include time taken layers network. ﬁrst present reduction scalar operations execution time achieved dyvedeep figure please note y-axis figure normalized scale represent beneﬁts scalar operations runtime. that across benchmarks dyvedeep consistently achieves substantial reduction operation count ranging .×-.×. translates .×-.× beneﬁts software execution time. cases difference classiﬁcation accuracy baseline dyvedeep <.%. average runtime overhead dynamic effort knobs dyvedeep baseline dnn. also runtime beneﬁts dyvedeep quite signiﬁcant smaller compared reduction scalar operations. expected applying knobs require alter memory access patterns perform additional book keeping operations. also control operations loop counters etc. inherent software implementation limits fraction runtime dyvedeep beneﬁt. figure shows break time savings across different layers alexnet layers plotted x-axis average time layer normalized total baseline time y-axis. achieve reduction time initial convolutional layers increases deeper convolutional layers layer alexnet kernel size operates stride hence output less likely correlation ssds expects. also since input features sfma also effective. also fraction neurons saturating relatively small ﬁrst layers impacts effectiveness spet. hence achieve better savings deeper convolutional layers compared initial ones. figure compares contribution effort knob overall savings convolutional layer alexnet. layers sdss knob yields highest savings reducing total scalar operations. spet sfma knobs contribute respectively. effectiveness knob pronounced deeper convolutional layers. figures illustrate normalised effort dyvedeep features layer sample images cifar- data set. layer closest layer actual image allows better visualization. normalization done respect number operations would performed compute neuron knobs place. darker regions represent computations. remarkable dyvedeep focuses effort precisely regions image contains object interest. compare activation corresponding features. here darker regions represent activated neurons. done highlight correlation activation values effort dyvedeep expends corresponding neurons. activation demonstrates regions activation value neurons high higher variance values makes harder approximate them. however delactthresh parameter ensures dyvedeep constrains effort spent regions uniform activation values. effort maps corroborate knobs’ effectiveness identifying critical computations current input. deep neural networks signiﬁcantly impacted ﬁeld machine learning enabling stateof-the-art functional accuracies variety machine learning problems involving image video text speech modalities. however large-scale structure renders compute data intensive remains challenge. observe state-of-the-art dnns static i.e. perform computations inputs. however many real-world datasets exists signiﬁcant heterogeneity compute effort required classify input. leveraging opportunity propose dynamic variable effort deep neural networks dnns modulate compute effort dynamically ascertaining computations critical classify given input. build dyvedeep versions popular image recognition benchmarks. experiments demonstrate dyvedeep achieves .×-.× reduction scalar operations .×-.× reduction runtime caffe-based sequential software implementation maintaining level classiﬁcation accuracy. references sajid anwar kyuyeon hwang wonyong sung. fixed point optimization deep convolutional ieee international conference acoustics neural networks object recognition. speech signal processing icassp south brisbane queensland australia april jimmy brendan frey. adaptive dropout training deep neural networks. advances neural information processing systems annual conference neural information processing systems proceedings meeting held december lake tahoe nevada united states. http//papers.nips.cc/ paper/-adaptive-dropout-for-training-deep-neural-networks. emmanuel bengio pierre-luc bacon joelle pineau doina precup. conditional computation neural networks faster models. corr abs/. http//arxiv.org/ abs/.. wenlin chen james wilson stephen tyree kilian weinberger yixin chen. compressing neural networks hashing trick. corr abs/. http//arxiv. org/abs/.. yunji chen shaoli shijin zhang liqiang wang ling tianshi chen zhiwei ninghui olivier temam. dadiannao machine-learning supercomputer. proceedings annual ieee/acm international symposium microarchitecture micro- washington ieee computer society. isbn ---. ./micro... http//dx.doi.org/./ micro... dipankar sasikanth avancha dheevatsa mudigere karthikeyan vaidyanathan srinivas sridharan dhiraj kalamkar bharat kaul pradeep dubey. distributed deep learning using synchronous stochastic gradient descent. corr abs/. http //arxiv.org/abs/.. jeffrey dean greg corrado rajat monga chen matthieu devin quoc mark marcaurelio ranzato andrew senior paul tucker yang andrew large scale distributed deep networks. nips imagenet large ieee computer society conference computer scale hierarchical image database. vision pattern recognition june miami florida ./cvprw... http//dx.doi.org/./ cvprw... misha denil babak shakibi laurent dinh marc’aurelio ranzato nando freadvances neural information proitas. cessing systems information processing systems proceedings meeting held december lake tahoe nevada united states. http//papers.nips.cc/paper/ -predicting-parameters-in-deep-learning. emily denton wojciech zaremba joan bruna yann lecun fergus. exploiting linear structure within convolutional networks efﬁcient evaluation. corr abs/. http//arxiv.org/abs/.. song huizi william dally. deep compression compressing deep neural network pruning trained quantization huffman coding. corr abs/. http//arxiv.org/abs/.. geoffrey hinton nitish srivastava alex krizhevsky ilya sutskever ruslan salakhutimproving neural networks preventing co-adaptation feature detectors. corr jaderberg andrea vedaldi andrew zisserman. speeding convolutional neural networks british machine vision conference bmvc nottingham rank expansions. september http//www.bmva.org/bmvc//papers/ paper/index.html. yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. arxiv preprint arxiv. baoyuan wang hassan foroosh marshall tappen marianna pensky. sparse convolutional neural networks. ieee conference computer vision pattern recognition cvpr boston june ./cvpr.. http//dx.doi.org/./cvpr... chao zhiyong zhang dong wang. pruning deep neural networks optimal brain damage. interspeech annual conference international speech communication association singapore september http//www. isca-speech.org/archive/interspeech_/i_.html. xiaoxiao mengjie beiye yiran chen boxun wang jiang mark barnell qing jianhua yang. reno high-efﬁcient reconﬁgurable neuromorphic computing accelerator design. proceedings annual design automation conference york acm. isbn ----. http//doi.acm.org/./.. shankar ganesh ramasubramanian rangharajan venkatesan mrigank sharad kaushik anand raghunathan. spindle spintronic deep learning engine large-scale neuromorphic computing. proceedings international symposium power electronics design islped york acm. isbn ---. ./.. http//doi.acm.org/./. minsoo natalia gimelshein jason clemons arslan zulﬁqar stephen keckler. virtualizing deep neural networks memory-efﬁcient neural network design. corr abs/. http//arxiv.org/abs/.. frank seide jasha droppo gang dong -bit stochastic gradient descent application data-parallel distributed training speech dnns. interspeech september pierre sermanet david eigen xiang zhang michael mathieu fergus yann lecun. overfeat integrated recognition localization detection using convolutional networks. http//arxiv.org/abs/.. shawn chai sim. towards implicit complexity control using variable-depth deep neural networks automatic speech recognition. ieee international conference acoustics speech signal processing icassp shanghai china march ./icassp... http//dx.doi.org/./ icassp... swagath venkataramani ashish ranjan kaushik anand raghunathan. axnn energyefﬁcient neuromorphic systems using approximate computing. proceedings international symposium power electronics design islped york acm. isbn ----. ./.. http//doi.acm.org/./..", "year": 2017}