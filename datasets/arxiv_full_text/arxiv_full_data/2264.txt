{"title": "Information Directed Sampling for Stochastic Bandits with Graph Feedback", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We consider stochastic multi-armed bandit problems with graph feedback, where the decision maker is allowed to observe the neighboring actions of the chosen action. We allow the graph structure to vary with time and consider both deterministic and Erd\\H{o}s-R\\'enyi random graph models. For such a graph feedback model, we first present a novel analysis of Thompson sampling that leads to tighter performance bound than existing work. Next, we propose new Information Directed Sampling based policies that are graph-aware in their decision making. Under the deterministic graph case, we establish a Bayesian regret bound for the proposed policies that scales with the clique cover number of the graph instead of the number of actions. Under the random graph case, we provide a Bayesian regret bound for the proposed policies that scales with the ratio of the number of actions over the expected number of observations per iteration. To the best of our knowledge, this is the first analytical result for stochastic bandits with random graph feedback. Finally, using numerical evaluations, we demonstrate that our proposed IDS policies outperform existing approaches, including adaptions of upper confidence bound, $\\epsilon$-greedy and Exp3 algorithms.", "text": "choose user time online social network offer promotion time decision maker offers promotion user also opportunity survey user’s neighbors network regarding potential interest similar offer. users found responsive surveys using social network information compared generic surveys effect leveraged construct side observations. consider another example actions advertisements decision maker constructs graph different vacation places links capture similarities between different places. customer shows interest places also asked provide opinion neighboring places graph. side-observation models also applicable sensor networks monitor events agent must choose sensor sample time. reward obtained choosing sensor related accuracy monitoring event. neighboring sensors communicate observations data aggregation desired affect obtaining side-observations neighboring sensors determining sensor select. authors propose policies side observation model stochastic bandit setting exploit graph structure accelerate learning. authors propose extensions upper conﬁdence bound based policies originally proposed classical setting policies proposed namely \u0001t-greedy-lp ucblp shown asymptotically optimal terms network structure time. used quintessential models sequential decision making. classical setting time decision maker must choose action actions unknown probability distributions. choosing action time reveals random reward drawn probability distribution action goal policies minimize expected loss uncertainty actions’ distributions given time horizon. work consider important setting called graph-structured feedback side-observation model choosing action generates reward action also reveals observations subset remaining actions. scenario occurs social networks sensors networks advertising. example decision maker must copyright association advancement artiﬁcial intelligence rights reserved. dubhashi authors analyze bayesian regret performance another well known classical bandit policy called thompson sampling side-observation model. make following important contributions existing literature graphical bandits graph structured feedback model allow side observation graph vary time. focus developing problem-independent bayesian regret bound. provide tighter bound thompson sampling given terms clique cover number side-observation graph bound presented also propose three algorithms based approach information directed sampling developed show algorithms enjoy theoretical bounds thompson sampling terms clique cover number graph. however using numerical evaluations show section based policies outperform existing policies provably asymptotically optimal terms network structure time. hence raises open question determine better bayesian regret bounds based policies terms network structure. contrast existing works also consider novel setting time variant random graph feedback model side observations neighboring actions obtained probability time provide bayesian regret bounds thompson sampling proposed based policies probabilistic model well. believe work provides ﬁrst result stochastic bandits random graph feedback. work related authors propose novel approach called ids. samples action manner minimizes ratio squared expected single-period regret mutual information optimal action next observation. shown using numerical simulations outperforms upper conﬁdence bound classical bandit setting. motivated investigate extensions policy stochastic bandits graph feedback compare adaptions policies studied difﬁcult obtain case also studied however many cases problem promotions online social networks sensor networks graph structure revealed learned priori. knowledge available show using graphical information make choices time helps obtaining optimal regret terms network structure time asymptotically. work motivated investigate extensions policies exploit knowledge network structure work. using numerical evaluations policies outperform asymptotically optimal policies presented non-stochastic bandits graph feedback studied line work related partial feedback models include label efﬁcient bandit prediction limited advice side observations limited budget. summary bandits graphs found stochastic bandit model consider bayesian formulation stochastic karmed bandit problem uncertainties modeled random variables. time decision maker chooses action ﬁnite action receives corresponding random reward ytat. without loss generality assume space possible rewards note results work extended case reward distributions sub-gaussian. random variable associated action assume {yta∀a independent time vector random variables time true reward distribution distribution randomly drawn family distributions unknown decision maker. conditioned independent identically distributed sequence element sampled distribution maxa∈k true optimal action conditioned period regret decision maker expected difference total rewards obtained oracle always chooses optimal action accumulated rewards time horizon formally study expected regret graph feedback model problem assume existence side observations described graph action time graph directed undirected dependent time time decision maker observes reward ytat playing action well outcome action et}. note becomes classical bandit feedback setting graph empty becomes full-information setting graph complete time work study types graph feedback models deterministic graph erd˝os-r´enyi random graph. deterministic graph. deterministic graph feedback model assume graph ﬁxed decision made time rk×k adjacent matrix represents deterministic graph feedback structure element i-th j-th column matrix. exists edge otherwise. note assume deﬁnition clique graph subset sub-graph formed complete graph. clique cover graph partition denoted clique cardinality smallest clique cover called clique cover number denoted erd˝os-r´enyi random graph. erd˝os-r´enyi random graph feedback model assume graph generated erd˝os-r´enyi model time-dependent parameter decision made time words decision maker reveal outcome probability action time feedback model also known probabilistically triggered arms generalize adjacent matrix representation deterministic graph feedback model random graph feedback model element matrix probability observing action playing action time adjacent matrix denoted unify representation algorithms analysis. then note parameter fully characterizes random graph feedback model. randomized policies deﬁne random variables respect probability space consider ﬁltration σ-algebra generated observation history ot−. observation history includes decisions rewards side observations time time time decision maker chooses action based history possibly randomness. policy decision maker viewed randomized policy ft-adapted sequence t∈n. time decision maker chooses action randomly according probability distribution bayesian regret deﬁned decisions chosen according uncertainty induces uncertainty true optimal action described prior distribution posterior distribution given history i.e. then updated bayes rule given decision reward ytat side observations. shannon entropy log). slightly abuse notion represent distributions ﬁnite well vectors instantaneous regret vector coordinate expected regret playing action time information gain vector i-th coordinate h|ft expected information gain playing action time note information gain playing action consists observing reward possibly side observations. deﬁne information gain observing action mutual information posterior distribution random variables yta. kullback-leibler divergence distributions. deﬁnition mutual information intuitively proposition shows information gain observing reward side observations least information gain individual observation. formal proof provided appendix supplemental material. idsn-lp policy greedily minimizes expected instantaneous regret time constraint information gain least obtained ts-n policy. ids-lp policy replaces generating policy step solution following ids-lp policy greedily minimizes expected instantaneous regret time constraint information gain least obtained policy without graph feedback. ids-lp policy reduces extent exploration compared idsn-lp policy. intuitively greedily exploits current knowledge optimal action controlled exploration. though better regret bound ids-lp idsn-lp ids-n ts-n idslp outperforms others numerical results shown section section ﬁrst present known general bound randomized policy provide regret upper bound results proposed policies deterministic random graph feedback. regret analysis relies following bound shown lemma policy time horizon lemma shows need bound expected information ratio obtain upper bound randomized policy. next result follows fact information ratio ids-lp policy bounded theorem graph feedback bayesian regret ids-lp idea proof comparing information ratio ids-lp bandit feedback. detailed proof theorem found appendix next proposition shows general bound information ratios ts-n ids-n idsn-lp policies. proposition graph feedback sampling sample according play action receive reward ytat. observations observe graph generated expected instantaneous regret note sampling distribution expected information gain sampling distribution information ratio measures energy cost information acquired. idea based policy keeping information ratio bounded order balance expected instantaneous regret obtaining knowledge optimal action words information ratio bounded expected regret bounded terms maximum amount information could expect acquire entropy prior distribution i.e. show section upper bounds information ratios policies provide here. practice information gain vector quite complicated compute even assuming bernoulli distribution model action. however computing information gain observing individual action i.e. much easier since mutual information random variables. proposition gtht). design based poli proposed expres policy guarantee holds theorem recovers guarantee without restriction since following result time-invariant. corollary time-invariant random graph feedback bayesian regrets ts-n ids-n idsn-lp upper-bounded corollary shows beneﬁt side observations measured expected number observations time step i.e. words regret upper bound scales ratio number actions expected number observations. ratio equals yields regret result stochastic bandit full information ratio equals yields regret result stochastic bandit bandit feedback analogous result found corollary non-stochastic bandits. algorithm offers abstract design principle availability statistics however additional work required design efﬁcient computational methods update statistics speciﬁc problems. general challenge updating statistics compute represent posterior distribution given observations also faced thompson sampling. posterior distribution complex often generate samples distribution using markov chain monte carlo algorithms enabling efﬁcient implementation ids. detailed discussion applying mcmc methods implementing randomized policy found proof proposition found appendix combining result lemma obtain uniﬁed regret result ts-n ids-n idsn-lp bounding ratio ready present regret results separately deterministic random graph feedback. deterministic graph following result shows uniﬁed regret upper bound ts-n ids-n idsn-lp deterministic graph feedback. detailed proof presented apendix theorem deterministic graph feedback bayesian regrets ts-n ids-n idsn-lp upper-bounded recently similar result ts-n shown apparently theorem provides tighter bound. following result graph also time-invariant. corollary time-invariant deterministic graph feedback bayesian regrets ts-n ids-n idsn-lp upper-bounded corollary shows ts-n ids-n idsn-lp beneﬁt side observations. words problem-independent regret upper bound scales clique cover number graph instead number actions similar result disclosed caron form problemdependent upper bound. show ucb-n scales clique cover number compared without side observations. information theoretic lower bound problemindependent regret shown scale independence number. general independence number less equal clique cover number. however equality holds large class graphs star graphs perfect graphs. words policies order-optimal large class graphs. erd˝os-r´enyi random graph following result shows uniﬁed regret upper bound ts-n ids-n idsn-lp erd˝os-r´enyi random graph feedback. detailed proof presented apendix however posterior distribution closed form conjugate prior well studied posterior distributions efﬁciently computed stored cases beta-bernoulli bandits gaussian bandits numerical experiment implement algorithm represent posterior distribution compute statistics beta-bernoulli bandits. idea beta distribution conjugate prior bernoulli distribution. speciﬁcally given prior expectation drawn beta posterior distribution observing bernoulli yi). posterior distribution beta. stated practical implementation updating statistics involves integrals evaluated discrete grid points within interval computational cost updating statistics number points used discretization proof adaption proof proposition replacing gtht. proposition shows problem convex optimization problem solved standard convex optimization solver. what’s more exists optimal solution support size search pairs actions optimal solution brute force. pair remains solve convex optimization problem parameter closed form. computational complexity problems linear programming problems solved efﬁciently polynomial time standard methods. moreover following result shows solved much faster. proof presented appendix proposition optimization problems solved iterations. computational complexity proposed based policies iteration number points used discretization note complexity based policies based policies improve regret performance reasonable computation cost. section presents numerical results experiments evaluate effectiveness based policies comparison alternative algorithms. consider classical beta-bernoulli bandit problem independent actions. reward action bernoulli random variable independently drawn beta. experiment regret results averaged trials. figure presents cumulative regret results deterministic graph feedback. time-invariant case graph cliques presented appendix time-variant case sequence graphs generated erd˝os-r´enyi model. compare policies three algorithms proposed stochastic bandit deterministic graph feedback. caron proposed ucb-n ucb-maxn closely follow policy side observations better reward estimates choose neighboring nodes better empirical estimate shown regret ucb-n ucb-maxn scale clique cover number time-invariant case. buccapatnam improved results lp-based algorithms guarantees scaling domination number time-invariant case. ts-n policy outperforms three algorithms consistent empirical observation bandit feedback setting addition ids-n idsn-lp ids-lp outperform ts-n policy cases. improvements stem exploitation graph structure based policies raises open question determining better regret bounds based policies terms graph structure. figure presents cumulative regret results erd˝os-r´enyi random graph feedback. time-invariant case parameter time-variant case parameter independently drawn uniform distribution interval compare policies ucb-n algorithms exp-res) designed nonstochastic bandit random graph feedback. average regrets exp-set exp-res dramatically larger based policies. reason parts exp-set exp-res omitted figure although exp-set exp-res similar problemindependent upper bounds regret policies utilize stochastic model outperform counterparts. addition based policies outperform ts-n ucbn well. interesting ids-lp policy performs well experiments though upper bound scales number actions. reason ids-lp greedy minimizing expected instantaneous regret however guaranteed extent exploration. proposed information directed sampling based policies presented thompson sampling stochastic multi-armed bandits deterministic random graph feedback. establish uniﬁed bayesian regret bound scales clique cover number graph ts-n ids-n idsn-lp policies deterministic graph case. also present ﬁrst known theoretical guarantee scales ratio number actions expected number observations iteration ts-n ids-n idsn-lp policies random graph case. results allow uncover gain partial feedback bandit feedback full information work raises following open questions. would interesting problem-independent regret bound scales independence number graph instead clique cover number ids-n idsn-lp policies deterministic graph case. believe improvement ts-n established exploiting graph structure ids-n idsn-lp policies shown figure another interesting problem tighter bound ids-lp policy. intuitively ids-lp policy regret greedy nature. further would interesting extension work consider preferential attachment random graph growing graphs time model growth process social networks time. russo information-theoretic analysis thompson sampling. journal machine learning research scott modern bayesian look multiarmed bandit. applied stochastic models business industry seldin bartlett crammer abbasi-yadkori prediction limited advice multiarmed bandits paid observations. international conference machine learning thompson likelihood unknown probability exceeds another view evidence samples. biometrika tossou dimitrakakis dubhashi thompson sampling stochastic bandits graph feedback. aaai conference artiﬁcial intelligence. ugander karrer backstrom marlow anatomy facebook social graph. arxiv preprint arxiv.. bandits graphs structures. valko ph.d. dissertation ´ecole normale sup´erieure cachanens cachan. gy¨orgy szepesv´ari online learnading gaussian payoffs side observations. vances neural information processing systems", "year": 2017}