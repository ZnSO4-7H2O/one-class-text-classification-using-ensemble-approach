{"title": "Double Q($σ$) and Q($σ, λ$): Unifying Reinforcement  Learning Control Algorithms", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Temporal-difference (TD) learning is an important field in reinforcement learning. Sarsa and Q-Learning are among the most used TD algorithms. The Q($\\sigma$) algorithm (Sutton and Barto (2017)) unifies both. This paper extends the Q($\\sigma$) algorithm to an online multi-step algorithm Q($\\sigma, \\lambda$) using eligibility traces and introduces Double Q($\\sigma$) as the extension of Q($\\sigma$) to double learning. Experiments suggest that the new Q($\\sigma, \\lambda$) algorithm can outperform the classical TD control methods Sarsa($\\lambda$), Q($\\lambda$) and Q($\\sigma$).", "text": "temporal-difference learning important ﬁeld reinforcement learning. sarsa q-learning among used algorithms. algorithm uniﬁes both. paper extends algorithm online multi-step algorithm using eligibility traces introduces double extension double learning. experiments suggest algorithm outperform classical control methods sarsa reinforcement learning ﬁeld machine learning addressing problem sequential decision making. formulated interaction agent environment number discrete time steps time step agent chooses action based environment’s state environment takes input returns next state observation reward scalar numeric feedback signal. rewards short-term signals goodness action values represent long-term value state state-action pair. action value function deﬁned expected return taking action state thereafter following policy value-based reinforcement learning concerned ﬁnding optimal action value function maxπ temporal-difference learning class model-free methods estimates sample transitions iteratively updates estimated values using observed rewards estimated values successor actions. step update following form applied estimate step size error difference current estimate newly computed target value. following control algorithms characterized different errors. action values represented table call tabular reinforcement learning else speak approximate reinforcement learning e.g. using neural network compute action values. sake simplicity following analysis done tabular reinforcement learning easily extended function approximation. term called target consists reward plus discounted value next state next action. sarsa on-policy method i.e. target consists sampled using current policy. general policy used sample state actions called behaviourpolicy different target policy used compute target. behaviour target policy different call off-policy learning. example off-policy control algorithm well known q-learning algorithm proposed watkins sarsa states actions sampled using exploratory behaviour policy e.g. \u0001-greedy policy target computed using greedy policy respect current values. error q-learning current state-action pair updated using expectation subsequent action values respect action value. q-learning special case expected sarsa greedy policy respect sutton barto propose control algorithm called uniﬁes sarsa expected sarsa. target algorithm weighted mean sarsa expected sarsa targets parameter controls weighting. equal sarsa equal expected sarsa. intermediate values algorithms obtained achieve better performance eligibility trace scalar numeric value state-action pair. whenever state-action pair visited eligibility increased eligibility fades away time. state-action pairs visited often higher eligibility visited less frequently state-action pairs visited recently higher eligibility visited long time ago. corresponding algorithm using one-step sarsa error update using eligibility traces called sarsa. though looks like one-step algorithm fact multi-step algorithm current error assigned back previously visited states actions weighted eligibility. off-policy algorithms like q-learning different eligibility updates proposed. watkin’s uses updates long greedy action chosen behaviour policy sets values whenever non-greedy action chosen assigning credit state-action pairs would actually visited following target policy behaviour policy generally eligibility weighted target policy’s probability next action. update rule whenever action occurs unlikely target policy eligibility previous states decreased sharply. target policy greedy policy eligibility complete history. paper introduce kind eligibility trace update extend algorithm on-line multi-step algorithm call recall one-step target weighted average on-policy sarsa off-policy expected sarsa targets weighted factor paper propose weight eligibility accordingly factor eligibility weighted average on-policy eligibility used sarsa off-policy eligibility used eligibility trace updated step one-step target equal sarsa one-step target therefore eligibility update reduces standard accumulate eligibility trace update. one-step target equal expected sarsa target accordingly eligibility weighted target policy’s probability current action. intermediate values eligibility weighted target. asis showed n-step intermediate dynamic value outperform q-learning sarsa. extending algorithm on-line multi-step algorithm make good initial performance sarsa combined good asymptotic performance comparison n-step algorithm algorithm learn on-line therefore likely learn faster. pseudocode tabular episodic given algorithm easily extended continuing tasks function approximation using eligibility weight function approximator. double learning another extension basic algorithms. mostly studied q-learning hasselt prevents overestimation action values using q-learning stochastic environments. idea decouple action selection action evaluation implementation simple instead using value function value functions actions sampled \u0001-greedy policy respect step either updated e.g. selected double learning also used sarsa expected sarsa proposed michael ganger using double learning algorithms robust perform better stochastic environments. decoupling action selection action evaluation weaker double qlearning next action selected according \u0001-greedy behavior policy using evaluated either expected sarsa policy used target equation could \u0001-greedy behavior policy proposed michael ganger probably better policy according also used off-policy double q-learning special case greedy policy respect paper propose extension double learning double obtain algorithm good learning properties double learning generalizes q-learning expected sarsa sarsa. course double also used eligibility traces. double following error selected section performance newly proposed algorithm tested gridworld navigation task compared performance classical control algorithms like sarsa qlearning well column grid agent pushed upward wind. action would take agent outside grid agent placed nearest cell inside grid. stochastic windy gridworld variant state transitions random probability agent transition surrounding eight states independent action. task treated undiscounted episodic task reward transition. figure visualizes gridworld. experiments conducted using \u0001-greedy behaviour policy performance terms average return ﬁrst episodes measured different values function step size expected sarsa part update greedy target policy chosen i.e. exactly q-learning. results averaged independent runs. figure shows intermediate value performed better sarsa q-learning best performance found dynamically varying time i.e. decreasing factor episode. multi-step bootstrapping trace decay parameter performed better one-step algorithms dynamically varying value allows combine good initial performance sarsa good asymptotic performance expected sarsa. conﬁrms results observed asis n-step algorithms. paper presented extensions algorithm unify q-learning expected sarsa sarsa. extends algorithm on-line multi-step algorithm using eligibility traces double extends algorithm double learning. empirical results suggest outperform classic control algorithms like sarsa dynamically varying obtains best results. figure stochastic windy gridworld results averaged episodes independent runs. performance different values function step size trace decay parameter used. best performance found using dynamic value multiplying factor episode.", "year": 2017}