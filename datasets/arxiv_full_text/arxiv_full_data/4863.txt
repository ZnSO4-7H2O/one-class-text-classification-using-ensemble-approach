{"title": "Adaptive Training of Random Mapping for Data Quantization", "tag": ["cs.LG", "cs.AI"], "abstract": "Data quantization learns encoding results of data with certain requirements, and provides a broad perspective of many real-world applications to data handling. Nevertheless, the results of encoder is usually limited to multivariate inputs with the random mapping, and side information of binary codes are hardly to mostly depict the original data patterns as possible. In the literature, cosine based random quantization has attracted much attentions due to its intrinsic bounded results. Nevertheless, it usually suffers from the uncertain outputs, and information of original data fails to be fully preserved in the reduced codes. In this work, a novel binary embedding method, termed adaptive training quantization (ATQ), is proposed to learn the ideal transform of random encoder, where the limitation of cosine random mapping is tackled. As an adaptive learning idea, the reduced mapping is adaptively calculated with idea of data group, while the bias of random transform is to be improved to hold most matching information. Experimental results show that the proposed method is able to obtain outstanding performance compared with other random quantization methods.", "text": "abstract—data quantization learns encoding results data certain requirements provides broad perspective many real-world applications data handling. nevertheless results encoder usually limited multivariate inputs random mapping side information binary codes hardly mostly depict original data patterns possible. literature cosine based random quantization attracted much attentions intrinsic bounded results. nevertheless usually suﬀers uncertain outputs information original data fails fully preserved reduced codes. work novel binary embedding method termed adaptive training quantization proposed learn ideal transform random encoder limitation cosine random mapping tackled. adaptive learning idea reduced mapping adaptively calculated idea data group bias random transform improved hold matching information. experimental results show proposed method able obtain outstanding performance compared random quantization methods. many information processing problems refers eﬃcient data representation basic procedure pattern analysis. advancement information technology emerges much demands useful handling tools data resulting solutions data proceeding. fast retrieve required information data quantization methods widely applied matched data binary codes plays bridge query feedback systems ﬁrstly learns outstanding features data transforms intrinsic patterns binary codes global description. query straightforward best matched binary codes among data simple ’xor’ comparisons usually temps learn data similar category. computational eﬃciency processing performance able calculated logical bits comparison metric measures. work unsupervised learning based quantization considered holds ubiquitously applicable advantages methods fact label information required. till many the-state-of-arts unsupervised quantization solutions widely applied data hashing. among methods learn binary codes beginning reduced generation inputs binarization data matching basic step random approaches. literature many quantization method adopts wellknown principal component analysis appropriate projections binary embedding. popular hashing method locality sensitive hashing learns low-dimensional representation coming data randomly generated projected directions. spectral hashing encodes input data low-dimensional binary ones preserving local structures. similar idea also used best quantization iterative procedures methods fact that resulting codes always falling binary formalism belief results cognitive contents. words obtained codes sequences binary codes whatever range inputs given replacement. furthermore needs necessary threshold constrain ﬁlter inputs original value extents positive negative opinion logically employed natural deterministic decision data distribution. furthermore speciﬁc methods e.g. randomised-based hashing attracted attentions recent years encoding features randomised manners. address binarization arbitrarily given data random function based hashing devised avoid fussy deduction calculation burden deterministic results believed that random mapping holds intrinsic relationship fourier features couple inherence binary codes erasing periodic cycles inputs works treat hashing learning data naturally projected function values range makes bounding values naturally binary encoders. generally popular deﬁnitions referred respectively denotes sign function cosine function denotes random mapping vector denotes linear oﬀset inspired motivation behind items referred shift-invariant transform e.g. encoded data obtained binary quantization. obviously ﬁnal inputs sign function constrained certain cosine function mapping values. certain works also regard speciﬁcal hashing function sift-invariant mapping feature reformed angular transforms. convenience simply referred cosine quantization context associated respective transforms reduced features. also existing works conducting idea randomized selection data samples however beyond scope work. mostly original randomly generated accordance normal gaussian distribution common sense. nevertheless existing discussion alternative choices random mapping optimal solutions learn binary balances unavailable development. work adaptive trained quantization random inputs discussed variant trigonometric function based quantization proposed learn optimal mapping initial inputs light -way clustering oﬀset rotations random mapping improved maximum binary side information. following contents paper organized follows section present proposed improved hashing learning adaptive cycle rotation section evaluate performance proposed method several random hashing methods. section draw conclusion work. supposed given data samples rd×n aims learn linear mapping matrix rd×r oﬀset transform original data r-dimensional reduced features. discussed foregoing straightforward randomly inputs low-dimensional space usually ranged phase results ensured disciplinarian. however randomly selected usually fall random result data distributed range ideal quantization cosine-like mapping solution optimization. here two-stage modiﬁed approach devised improve resulting random mapping acceptable outputs optimized obtain ideal quantization simplify depiction discussions employed denote resulting whole vector matrix cosine sine transforms data feature indicate transforms single input. terms linear transform pre-transform mapping randomly generated ﬁnal binary coding. nevertheless considerable whether two-side groups preserved reduced features possible. class label information ignored original problem come simple problem -way groups. without supervised sense whole data grouped certain distribution self-cognitive nature reduced space. literature widely adopted seek principal components global distribution data able preserve main information original data addition also competent learn ideal data groups feature structures data decomposition method informative contents data models preserved. address adaptive training linear transform groups based objective function devised improve ideal mapping original transform. here oﬀset linear mapping ignored ﬁrstly handling ﬁrst stage could smooth however conducted following step. without loss generality objective deﬁned objective actually kind problems data clusters principal subspaces widely applied many self-taught applications. original problem conducts -way clustering data seeks solutions principal decomposition data nevertheless diﬀerent based clustering methods general objective function traditional eigen-solution unavailable results. also works involving iterative approach solve optimization problem pattern analysis optimize involved objective conjugate gradient adopted seek local optimum general data mapping. advantages method methods e.g. newton method require calculate second-order gradient optimized steps directions adaptively searched iterations speciﬁcally conjugate direction method obtained applying gram-schmidt procedure gradient results iteratively step size decided according gradient values current previous iterations. here diag denotes diagonal matrix elements input vector diagonal. re-deﬁning t-th gradient objective function method selects suitable step size conjugate direction rd×r previously obtained conjugate direction calculated subsection choice oﬀset constant linear transform cosine random mapping discussed appropriate rotation angular possible acceptable results binary embedding. best knowledge ours existing work conducting topic random constant usually referred. indeed oﬀset constant plays weak impact ﬁnal quantization usual concept. nevertheless randomly selected oﬀset actually cannot match optimal projective direction cycle rounding generally illustrated fig. well-known step size decides reduction objective function updated variable iteration make decrease suﬃcient. literature popular condition suﬃcient decrease referred armijo condition denotes obtained t-th optimization hand stopping rule plays important role iterative optimization problems appropriate able reduce overall computational cost. general objective optimization problems though global minimum diﬃcult achieve feasible limit points stationary local minimum found. literature gradient based stopping rule successful tool stopping iterative search mainly applied quadratic optimization diseased learning problem usually unavoidable general objective functions numerical stopping criterion applied work found much eﬀective standard gradient based ones diseased learning problem avoidable values objective function deﬁned denotes constant stopping rule. obviously stopping criterion focuses reduction objective function value gradient needed decide stopping optimization. compared gradient based stopping rule component iterative optimization general objective function. addition improved eﬃciency attainable simple calculation adopted stage speciﬁc demand required. alternative choice could reached using randomized nonlinear component analysis methods. two-way group idea considered work demonstrate possibility adaptive training random mapping. original bias cosine function determined randomly selected oﬀsets ﬁnal hashing outputs unbalanced hyperplane result. then resulting mapping fails maximally depict projective values binary embedding possible. words random biases unable make optimal side decision binary information input data. problem intrinsic limitation random hashing intuitively projective extension required enlarged horizontal direction deterministic cosine hashing maximum probability. motivation inspired work initial attempt ideal choice mapping bias devise adaptive learning oﬀset decision believable binary embedding. generally several projective directions selected linear mapping aﬀord representation data patterns reduced dimension. terms this original bias linear mapping adjusted every projective hyperplane distinctive binary codes obtained ﬁnal thresholding. discuss conveniently simple linear mapping function referred here further combination implementation following similar deduction. optimally chosen oﬀset linear mapping mapping function deﬁned discussed ideal oﬀset holds eﬃciency maximizing extension projective directions. hand adjusts results every horizontal hyperplanes pushes aﬀective impacts linear mapping. associated mapping results correspondence given oﬀset. respect objective function diﬃcult handle square items reach available solution directly amounts data. order optimize objective objective function simpliﬁed unfolding item original adjustment tangent function. learned oﬀset side information hashing codes able maximized matching binary embedding trigonometric function. though idea pushed intuitively performed results improved cases. suspicion comes fact phases adaptive learning search reverse directions along total objective cost. nevertheless quantization chosed oﬀset modiﬁed minor rotation spherical angular actually periodically repeated results hardly make deﬁnite conclusion ideal exploitation quantization uncertain data. section performance proposed method evaluated compared several the-state-of-arts random quantization methods binary pattern matching including data sets cifar image dataset minist digit database used experiments. furthermore mean average precision computed experiments measure performance diﬀerent algorithms. iterative learning initial step alterant step size constants armijo condition stopping rule images cifar- dataset dimensional gist descriptors dimensional sift descriptors learned every tiny image data represented dimensional sample vector. experiments diﬀerent batches cifar- dataset combined ignoring groups several subsets randomly selected form aforehand hashing data hand results minist data quite diﬀerent ones cifar-. presents best performance though large vibration limited data samples used calculation map. reason this considered reconstruction desire minist data respect intrinsic characteristics furthermore also large vibrations occurring results following ones distinctively able hold results stable performance diﬀerent number involved samples map. seems that give better results neighbors considered data matching. data quantization quite popular data matching eﬃcient query related data encoding technologies studied broad recent years. specially randomized quantization widely studied binary query data sequential data. data minist dimensional gray features used describe visual handwritten pattern. similarly whole data combined original order data disordered lots subsets randomly selected involved experiments. experiment search performance encoders evaluated databases. cifar- data data randomly selected encoded binary embedding data randomly selected push forward matching query. minist data training testing data sets combined respective data randomly selected form original query data. noticeable supervised semisupervised corporation involved self-taught hashing considered adaptive learning. ﬁrst experiment evaluates query performance diﬀerent algorithms ﬁxed neighbors measure search results diﬀerent hashing bits shown fig. according experimental results proposed able outperform the-state-of-arts random quantization methods cifar- data set. obtained results quite similar increasing encoding bits tendency kept large ﬂuent data length. hashing methods give results arounding involved data bits used binary embedding. give better results compared enough bits used query matching. present strong intensity vibration short bits ameliorated increased data bits. minist data results algorithms vibrated strongly diﬀerent data bits. similar dilemma also occurs also presents vibration changing bits. nevertheless always present stable results involved bits aﬀection performance. furthermore proposed method gives better results compared original limited binary bits adopted depict data information. another experiment evaluates performance diﬀerent algorithms obtained results diﬀerent number samples used matching results shown fig. experiments proceeded ﬁxing length hashing bits involved neighbors turns. details diﬀerent neighbors used calculate ﬁxed bits used generate results. obviously results diﬀerent algorithms change strong vibrations involved samples increasing stepwise. nevertheless present much ﬂuent results stable performance compared methods. terms results cifar- data work better methods higher rank takes best results cases diﬀerent map. compared gives much better results cases diﬀerent neighbors involved matching query. presents ﬂuctuant results changing bits", "year": 2016}