{"title": "Exploring Neural Transducers for End-to-End Speech Recognition", "tag": ["cs.CL", "cs.NE"], "abstract": "In this work, we perform an empirical comparison among the CTC, RNN-Transducer, and attention-based Seq2Seq models for end-to-end speech recognition. We show that, without any language model, Seq2Seq and RNN-Transducer models both outperform the best reported CTC models with a language model, on the popular Hub5'00 benchmark. On our internal diverse dataset, these trends continue - RNNTransducer models rescored with a language model after beam search outperform our best CTC models. These results simplify the speech recognition pipeline so that decoding can now be expressed purely as neural network operations. We also study how the choice of encoder architecture affects the performance of the three models - when all encoder layers are forward only, and when encoders downsample the input representation aggressively.", "text": "work perform empirical comparison among rnn-transducer attention-based seqseq models end-to-end speech recognition. show that without language model seqseq rnn-transducer models outperform best reported models language model popular hub’ benchmark. internal diverse dataset trends continue rnntransducer models rescored language model beam search outperform best models. results simplify speech recognition pipeline decoding expressed purely neural network operations. also study choice encoder architecture affects performance three models encoder layers forward only encoders downsample input representation aggressively. recent years deep neural networks advanced stateof-the-art large scale automatic speech recognition tasks deep neural networks extract acoustic features used inputs traditional models like hidden markov models also sequence transducers results end-to-end neural systems major challenge sequence transduction input output sequences differ lengths lengths variable. result speech transducer learn alignment mapping acoustic inputs linguistic outputs simultaneously. several neural network-based speech models proposed past years solve challenge. work focus understanding differences transduction mechanisms. speciﬁcally compare three transduction models connectionist temporal classiﬁcation rnn-transducer sequence-to-sequence attention task models differ mainly along assumptions made three axes conditional independence predictions different time steps given audio. reasonable assumption task. makes assumption rnn-transducers attention models not. alignment input output units monotonic. reasonable assumption task enables models streaming transcription. rnn-transducers make assumption attention models not. hard soft alignments. rnn-transducer models explicitly treat alignment input output latent variable marginalize possible hard alignments attention mechanism models soft alignment output step every input step. unclear matters task. conclusive studies comparing architectures scale. work train three models datasets using methodology order perform fair comparison. models assume conditional independence predictions given full input able learn implicit language model training corpus optimize directly models. therefore perform quite competitively even outperforming models without external language model. among them rnn-transducers simplest decoding procedure fewer hyper-parameters tune. following sections ﬁrst revisit three models describe interesting speciﬁc details implementations. then section present results hub’ benchmark hours training data) internal dataset hours). section study well train using forward-only layers excessive pooling encoder layers dataset controlling number parameters model. section presents related work section summarizes takeaways presents scope future work. here focus vanilla seqseq models full attention though exist efforts enforcing local monotonic attention recently typically results loss performance fig. illustration probability transitions three transducers utterance length labelled cat. node represents probability output ﬁrst elements output sequence point transcription sequence. vertical arrow represents predicting multiple characters time step horizontal arrow represents predicting repeating characters predicting nothing solid arrows represent hard alignments soft ones noticed rnn-transducer states move towards right direction step attention input frames could potentially attended decoding step. produces linguistic outputs encoded representations. challenge input output sequences variable lengths usually alignments unavailable. neural transducers learn classiﬁcation acoustic features linguistic predictions well alignment them. transducer models differ formulations classiﬁer aligner. length output sequence length dimensional one-hot vector transducers model conditional distribution encoder maps input high level representation shorter input time-scale downsampling. encoder built feed-forward neural networks recurrent neural networks convolution neural networks decoder deﬁnes alignment mapping computes conditional probability marginalizing possible alignments assumes conditional independence output predictions different time steps given aligned inputs. extra ‘blank’ label interpreted label introduced length alignment obtained inserting blanks mapping deﬁned done removing blanks repeating letters conditional probability efﬁciently calculated using forward-backward dynamicprogramming algorithm detailed note conventional deﬁnition softmax. output could decoded greedily picking likely label time-step. make beam search effective conditional independence assumption artiﬁcially broken inclusion language model decoding task ﬁnding argmax decoding approximate performed using beam search typically large beam lattice equation presents discrepancy models trained tested. address this models could ﬁne-tuned loss function also incorporates language model information like smbr principle issue still absence dependence predictions. attention mechanism allows model attend anyinput sequence time thus alignments non-local non-monotonic. however excessive generality comes complicated decoding task since models terminate prematurely well never terminate repeatedly attending encoding steps. therefore decoding task ﬁnds argmax length normalization hyperparameter coverage term encourages model attend encoder time steps stops rewarding repeated attendance time steps. coverage term addresses short well inﬁnitely long decoding. promise end-to-end models simpliﬁcation training inference pipelines speech systems. end-to-end models simpliﬁed training process inference still involves decoding massive language models often requires teams build maintain complicated decoders. since attention rnn-transducers implicitly learn language model speech training corpus rescoring decoding using language models trained solely text speech corpus contribute improvements external trained data available simply rescoring ﬁnal beam recovers performance difference decoding beam search therefore simpliﬁed expressed neural network operations need support massive language models. trend already seen neural machine translation tasks state-of-art systems typically external language model performance models hub’ benchmark presented table along published results in-domain data. models table standard language model paired dataset except rows marked without using language model attention rnn-transducer models outperform donates output timestep aligned input timestep extra recurrent network used help determine predicting decoder logits conditional distribution time computed normalizing summation could parametric function like marginalized alignments local monotonic likelihood label calculated efﬁciently using dynamic programming. decoding uses beam search length normalization originally suggested since necessary. attention model aligns inputs outputs using attention mechanism. like rnn-transducer attention model removes conditional independence assumption label sequence makes. unlike rnntransducer however assume monotonic alignment explicitly marginalize alignments. computes picking soft alignment output step every input step. table samples decoding utterance across different models deepspeech set. reason relatively worse attention model could attributed utterances like ﬁrst contributes edit distance lot. ﬁrst example shows greedy decoding cases models second shows prediction evolves various stages decoding. table comparison previous published results fisher-switchboard hub’ benchmark using in-domain data. list results using single models here. previous works reported using language models. don’t leverage speaker information models though shown reduce previous works model trained corpus highly competitive best results dataset. since also trained training corpus rescoring little effect attention rnn-transducer models. found beam search attention worked best using length normalization however distribution errors table show rnn-transducer obvious problems pre-mature termination number deletions small even though length normalization. attention rnn-transducer beam width deepspeech corpus contains hours speech diverse scenarios far-ﬁeld background noise accents etc. additionally train targets sets drawn different distribution since don’t access large volumes data target distribution. rely external language models trained signiﬁcantly larger corpus text close train test distributions. setting therefore provides rescoring resulting beam candidates. surprisingly attention models start similar models greedy decoding architectures make different errors. models poorer mainly mis-spellings relatively higher attention models could largely attributed noisy utterances. cases attention models similar language model arbitrarily output characters repeatedly attending encoder time steps. coverage term equation helps address issue beam search greedy decoding cannot improved. example situation shown table monotonic left-to-right decoding rnn-transducers naturally avoid issues. further coverage term helps keep correct answers beam language model rescoring ﬁnal beam still required bring correct answers back top. data speciﬁcation. throughout paper audio data sampled normalized constant power. loglinear log-mel spectrograms extracted size window size globally normalized input spectrogram zero mean unit variance. speaker information models. every epoch utterances randomly selected background noise models table trained standard fisher-swbd dataset comprising corpora portion corpus hyper-parameter tuning. language model used decoding model well rescoring models -gram available benchmark kaldi receipe language model used models table built sample common crawl dataset model speciﬁcation. models tables tuned independent perform random search encoder decoder sizes amount pooling minibatch size choice optimizer learning annealing rates. further constraints placed model terms number parameters wall clock time others. training procedure mainly follows uses sortagrad models bi-directional relu encoders batch-normalization depth convolutional front-end. short hand gru] represents stack layers dconvolution followed stack bidirectional relu gru. represents layer downsamples input also encoder layers could replaced lstm layers tanh activation weight noise batch normalization. cases lstm cells weight noise match performance large un-regularized cells batch-normalization along time dimension. short hand best model gru] best rnntransducer’s encoder gru] decoder best attention model works best without convolutional front-end encoder decoder models therefore parameters. models trained minibatch gpus using synchronous typically converge within iterations ﬁnal solution. section standard dataset understand models perform different encoding choices. since encoder layers away loss functions evaluating expect encoder works well would also perform well attention rnntransducer. however different training targets allow different kinds encoders particularly amount downsampling encoder important factor impacts training wall clock time well accuracy model. encoders forward-only layers also allow streaming decoding also explore aspect. believe results smaller uniform dataset still hold scale therefore focus trends rather optimizing wer. control models section layers bidirectional lstm cells encoder weight noise. perform random search pooling encoder whether convolutional front-end data augmentation weight noise optimization hyper-parameters. report best numbers within ﬁrst iterations training search hyper-parameter space allowed match previously published results. attention model table beam search dev’ matches previously published results similarly model better results reported therefore believe provides good baseline explore trade-offs modeling choices. streaming transcription important requirement models. ﬁrst step towards deploying models setting replace bidirectional layers forward-only recurrent layers. note immediately makes rnn-transducer models deployable attention models still need able process entire utterance outputting ﬁrst character. alternatives proposed circumvent issue build attention models monotonic attention streaming decoders none able completely match performance full attention models. nevertheless believe comparison models full attention important full attention entire audio provides additional performance improves training. experiment replace every layer bidirectional lstm cells encoder layer forward-only lstm cells. table baseline models eval’ set. smaller datasets rnn-transducers attention models enough data learn good implicit language model therefore perform poorer compared even rescoring external table models signiﬁcantly stable easier train perform better forward setting. also since attention models quite better rnn-transducer models full attention encoder time steps seems valuable. best steps second audio given encoder architecture ﬁnal encoder layer attention model layers pyramidal pooling lesser compute compared model. important since attention needs computed small number encoder time steps. since rnn-transducers attention models output multiple characters encoder timestep expect rnn-transducers robust attention models increase amount pooling encoder. figure shows fairly robust compared models attention models signiﬁcantly robust. addition successfully trained attention models layers pooling reduction encoder forces compress second audio encoder steps. three transduction models formulate alignments between input output different ways. rnntransducer models explicitly treat alignment latent variable marginalize possible hard alignments attention models soft alignment output step every input step. addition rnn-transducer attention models allow producing multiple characters reading input locations produce one. herein visualize alignments learned three models understand formulations made model. figure plots alignment utterance devset. since alignment computed based ground-truth text three models produce reasonable alignments especially monotonic attention. several notable observations listed below alignments computed rnn-transducer concentrated compared attention. addition attention model produces diffused distributions beginning audio. effective control memory usage well training time models compress along time dimension encoder recurrent layers unrolled fewer time-steps. previous results shown models work best steps second audio attention models work segmental rnns provide another alternative model task. segmental rnns model using zeroth-order crf. global normalization help address label bias issues believe bigger issue still conditional independence assumptions made segmental rnns. fig. visualization learned alignments utterance using rnn-transducer attention alignments ground-truth text audio features decoder. note attention time-scale downsampling results shorter sequences compared two. papers without control either acoustic models optimization methodology. initial controlled comparison several speech transduction models present results small datset timit. also recent effort introducing local monotonic constraints attention models especially online applications. efforts theory bridge modelling assumptions attention rnn-transducer models. constraints ﬁtting capability attention models would limited might robust noisy test data return. words attention models work without extra tricks beam search decoding coverage penalty. present thorough comparison three popular models end-to-end task scale bidirectional setting three models perform roughly same. however models differ simplicity training decoding pipelines. notably end-to-end models trained loss simplify training process still require decoded large language models. rnntransducers attention also simplify decoding process require language models introduced post processing stage equally effective. rnn-transducers simplest decoding process extra hyper-parameters tuning decoding leads believe rnn-transducers present next generation end-to-end speech models. attempt train rnn-transducer models streaming constraint reducing computation encoder layers attention models still strengths leverage future work rnn-transducers. dario amodei rishita anubhai eric battenberg carl case jared casper bryan catanzaro jingdong chen mike chrzanowski adam coates greg diamos deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. dzmitry bahdanau chorowski dmitriy serdyuk philemon brakel yoshua bengio. end-to-end attention-based large vocabulary speech recognition. abs/. http//arxiv.org/abs/.. eric battenberg rewon child adam coates christopher fougner yashesh gaur jiaji huang heewoo ajay kannan markus kliegl atul kumar reducing bias production speech models. arxiv preprint arxiv. chung-cheng chiu dieterich lawson yuping george tucker kevin swersky ilya sutskever online sequence-to-sequence navdeep jaitly. arxiv preprint model noisy speech recognition. arxiv. alex graves santiago fern´andez faustino gomez j¨urgen schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning pages alex graves navdeep jaitly. towards end-to-end speech recognition recurrent neural networks. proceedings international conference machine learning pages daniel povey vijayaditya peddinti daniel galvez pegah ghahremani vimal manohar xingyu yiming wang sanjeev khudanpur. purely sequence-trained neural networks based lattice-free mmi. interspeech pages george saon gakuto kurata sercu kartik audhkhasi samuel thomas dimitrios dimitriadis xiaodong bhuvana ramabhadran michael picheny lynn-li english conversational telephone speech recognition humans machines. arxiv preprint arxiv. awni hannun andrew maas daniel jurafsky andrew first-pass large vocabulary continuous speech recognition using bi-directional recurrent dnns. abs/. http//arxiv.org/abs/.. jason smith herve saint-amand magdalena plamada philipp koehn chris callison-burch adam lopez. dirt cheap web-scale parallel text common crawl. pages g.e. hinton deng g.e. dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury. deep neural networks acoustic modeling speech recognition. ieee signal processing magazine yajie miao mohammad gowayyed florian metze. eesen end-to-end speech recognition using deep models wfst-based decoding. automatic speech recognition understanding ieee workshop pages ieee yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. wayne xiong jasha droppo xuedong huang frank seide mike seltzer andreas stolcke dong geoffrey zweig. achieving human parity conversational speech recognition. arxiv preprint arxiv.", "year": 2017}