{"title": "Model-Powered Conditional Independence Test", "tag": ["stat.ML", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "abstract": "We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution $f(x,y,z)$ of continuous random vectors $X,Y$ and $Z,$ we determine whether $X \\perp Y | Z$. We approach this by converting the conditional independence test into a classification problem. This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks. These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution $f^{CI}(x,y,z) = f(x|z)f(y|z)f(z)$ -- the joint distribution if and only if $X \\perp Y | Z.$ -- when given access only to i.i.d. samples from the true joint distribution $f(x,y,z)$. To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to $f^{CI}$ in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d near-independent samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.", "text": "consider problem non-parametric conditional independence testing continuous random variables. given i.i.d samples joint distribution continuous random vectors determine whether approach converting conditional independence test classiﬁcation problem. allows harness powerful classiﬁers like gradient-boosted trees deep neural networks. models handle complex probability distributions allow perform signiﬁcantly better compared prior state high-dimensional testing. main technical challenge classiﬁcation problem need samples conditional product distribution joint distribution given access i.i.d. samples true joint distribution tackle problem propose novel nearest neighbor bootstrap procedure theoretically show generated samples indeed close terms total variational distance. develop theoretical results regarding generalization bounds classiﬁcation problem translate error bounds testing. provide novel analysis rademacher type classiﬁcation bounds presence non-i.i.d near-independent samples. empirically validate performance algorithm simulated real datasets show performance gains previous methods. testing datasets conditional independence signiﬁcant applications several statistical/learning problems; among others examples include discovering/testing edges bayesian networks causal inference feature selection markov blankets given triplet random variables/vectors conditionally independent given joint distribution fxyz factorizes fxyz fx|zfy |zfz. problem conditional independence testing deﬁned follows given i.i.d samples fxyz distinguish hypothesis paper propose data-driven model-powered test. central idea model-driven approach convert statistical testing estimation problem pipeline utilizes power supervised learning models like classiﬁers regressors; pipelines leverage recent advances classiﬁcation/regression high-dimensional settings. paper take model-powered approach reduces problem testing binary classiﬁcation. speciﬁcally steps procedure follows suppose provided i.i.d samples fxyz. keep aside original samples remaining original samples processed ﬁrst module nearest-neighbor bootstrap produces simulated samples stored figure illustration methodology. part original samples kept aside rest samples used nearest neighbor boot-strap generate data-set close distribution. samples labeled shown classiﬁer trained training set. test error measured test there-after. test-error close rejected however test error rejected. section show generated samples fact close total variational distance conditionally independent distribution fx|zfy |zfz. fxyz hold; method generates samples close hypotheses). subsequently original samples kept aside labeled samples simulated nearest-neighbor bootstrap labeled labeled samples labeled aggregated data-set broken training test sets containing samples each. given labeled training data-set train powerful classiﬁers gradient boosted trees deep neural networks attempt learn classes samples. trained classiﬁer good accuracy test intuitively means joint distribution fxyz distinguishable therefore reject hand classiﬁer accuracy close random guessing fxyz fact close fail reject independence testing classiﬁers recently used observation given i.i.d samples coordinates randomly permuted resulting samples exactly emulate distribution thus problem converted sample test subset original samples subset permuted binary classiﬁers harnessed two-sample testing; details however case testing need emulate samples harder permutation samples needs dependent technical contributions proving nearest-neighbor bootstrap step achieves task. advantage modular approach harness power classiﬁers above) good accuracies high-dimensions. thus improvements ﬁeld binary classiﬁcation imply advancement test. moreover added ﬂexibility choosing best classiﬁer based domain knowledge data-generation process. finally bootstrap also eﬃcient owing fast algorithms identifying nearest-neighbors reduce problem testing binary classiﬁcation detailed steps fig. simulate samples close novel nearest-neighbor bootstrap given access i.i.d samples joint distribution. mentioned steps samples generated bootstrap close testing problem reduces testing whether data-sets distinguishable other. theoretically justify indeed true. φxyz denote distribution sample produced algorithm supplied i.i.d samples fxyz. theorem prove appropriate smoothness assumptions. dimension denotes total variational distance samples generated nearest-neighbor bootstrap remain i.i.d close i.i.d. quantify property show generalization risk bounds classiﬁer. denote class function encoded classiﬁer denote probability error optimal classiﬁer trained training prove appropriate assumptions high probability upto factors. dimension class thus equivalent error rate classiﬁer close holds loss much lower. provide novel analysis rademacher complexity bounds near-independence independent interest. perform extensive numerical experiments algorithm outperforms state also apply algorithm analyzing relations protein signaling network data cytometry data-set practice observe performance respect dimension scales much better expected worst case theoretical analysis. powerful binary classiﬁers perform well high-dimensions. paper address problem non-parametric testing underlying random variables continuous. literature non-parametric testing vast. review recent work ﬁeld relevant paper. recent work testing kernel based many works build study non-parametric relations characterized using covariance operators reproducing kernel hilbert spaces kcit uses partial association regression functions relating rcit approximate version kcit attempts improve running times number samples large. kcipt perhaps relevant work. speciﬁc permutation samples used simulate data expensive linear program needs solved order calculate permutation. hand simple nearest-neighbor bootstrap provide theoretical guarantees closeness samples terms total variational distance. finally two-sample test based kernel method binary classiﬁers purpose. also recent work entropy estimation using nearest neighbor techniques subsequently used testing estimating conditional mutual information binary classiﬁcation recently used two-sample testing particular independence testing analysis generalization guarantees classiﬁcation aimed recovering guarantees similar non-i.i.d setting. regard recent work proving rademacher complexity bounds β-mixing stationary processes work also falls category machine learning reductions general philosophy reduce various machine learning settings like multi-class regression ranking reinforcement learning structured prediction binary classiﬁcation. section describe algorithmic details testing procedure. ﬁrst formally deﬁne problem. describe bootstrap algorithm generating data-set mimics samples give detailed pseudo-code testing process reduces problem binary classiﬁcation. finally suggest improvements algorithm. problem setting problem setting non-parametric conditional independence testing given i.i.d samples joint distributions random variables/vectors given i.i.d essentially hypothesis testing problem where note notational convenience drop subscripts context evident. instance place fx|z. nearest-neighbor bootstrap algorithm procedure generate data-set consisting samples given data-set i.i.d samples distribution fxyz. data-set broken equally sized partitions sample nearest neighbor terms coordinates. -coordinates sample exchanged -coordinates nearest neighbor modiﬁed sample added algorithm datagen given data-set i.i.d samples returns data-set samples. function datagen main results samples generated algorithm mimic samples coming distribution suppose sample small. case therefore given ﬁxed appropriate smoothness assumptions close independent sample coming hand small rare occurrence contribute adversely. testing algorithm introduce testing algorithm uses algorithm along binary classiﬁers. psuedo-code algorithm partition three disjoint partitions size each randomly. datagen note create labeled data-set )}u∈u )}u∈u divide data-set train test respectively. note |dr| |de| argming∈g training classiﬁer conclude otherwise conclude algorithm original samples nearest-neighbor bootstrapped samples almost indistinguishable holds. however holds classiﬁer trained line able easily distinguish samples corresponding diﬀerent labels. line denotes space functions risk minimization performed classiﬁer. show variational distance distribution samples small large however samples exactly i.i.d close i.i.d. therefore practice ﬁnite small bias i.e. even holds. threshold needs greater order algorithm function. next section present algorithm bias corrected. algorithm bias correction present improved bias-corrected version algorithm algorithm mentioned previous section algorithm optimal classiﬁer able achieve loss slightly less case ﬁnite even true. however classiﬁer expected distinguish data-sets based coordinates joint distribution remains nearest-neighbor bootstrap. idea algorithm train classiﬁer using coordinates denoted also train another classier using coordinates denoted test loss expected roughly bias mentioned previous section. therefore subtract bias. thus true lde) close however holds much lower classiﬁer trained leveraging information encoded coordinates. perform steps algorithm )})∈dr. similarly )})∈de. training test sets without x-coordinates. argming∈g argming∈g ˆlde) conclude otherwise conclude section provide main theoretical results. ﬁrst show distribution samples generated algorithm closely resemble sample coming result holds broad class distributions fxyz satisfy smoothness assumptions. however samples generated algorithm exactly i.i.d close i.i.d. quantify show empirical risk minimization class classiﬁer functions generalizes well using samples. before formally state results provide useful deﬁnitions. deﬁnition total variational distance continuous probability distributions ﬁrst prove distribution samples generated algorithm close terms total variational distance. make following assumptions joint distribution original samples i.e. fxyz smoothness assumption assume smoothness condition generalization boundedness max. eigenvalue fisher information matrix w.r.t smoothness assumptions assume smoothness properties probability density function smoothness assumptions subset assumptions made entropy estimation. deﬁnition deﬁne probability mass distribution areas p.d.f less deﬁnition denote hessian matrix p.d.f respect /∂zi∂zj provided twice continuously diﬀerentiable assumption probability density function satisﬁes following twice continuously diﬀerentiable hessian matrix satisﬁes almost everywhere dependent dimension. theorem denote sample produced algorithm modifying original sample supplied i.i.d samples original joint distribution fxyz. φxyz distribution smoothness assumptions large enough have here volume unit radius ball theorem characterizes variational distance distribution sample generated algorithm conditionally independent distribution defer proof theorem appendix goal characterize misclassiﬁcation error trained classiﬁer algorithm consider distribution samples data-set used classiﬁcation algorithm marginal distribution sample label similarly denote marginal distribution label samples. note construction φxyz deﬁned theorem note even though marginal sample label φxyz exactly i.i.d owing nearest neighbor bootstrap. show actually close i.i.d therefore classiﬁcation risk minimization generalizes similar i.i.d results classiﬁcation first review standard deﬁnitions results classiﬁcation theory ideal classiﬁcation setting consider ideal classiﬁcation scenario testing process deﬁne standard quantities learning theory. recall classiﬁers consideration. ideal distribution given fxyz words ideal classiﬁcation scenario testing loss function classifying function sample true label algorithms loss function loss results hold bounded loss function s.t. |l|. distribution classiﬁer eu∼˜q expected risk ming∈g r˜q. similarly function risk optimal classiﬁer g∗˜q given g∗˜q samples classiﬁer empirical risk samples. deﬁne classiﬁer minimizes empirical loss observed samples ming∈g dimension classiﬁcation model universal constant |s|. guarantees near-independent samples goal prove result like classiﬁcation problem algorithm however case access i.i.d samples samples remain independent. close independent sense. brings main results theorem theorem assume joint distribution satisﬁes conditions theorem assume bounded lipschitz constant. consider classiﬁer algorithm trained according deﬁnition have probability least v.c. dimension classiﬁcation function class deﬁned def. universal constant bound absolute value loss. suppose class classifying functions suppose loss here risk bayes optimal classiﬁer best loss classiﬁer achieve classiﬁcation problem setting least have deﬁned theorem prove theorem theorem theorem appendix. part theorem prove generalization bounds hold even samples exactly i.i.d. intuitively consider sample inputs corresponding coordinates away. expect resulting samples nearly-independent. carefully capturing notion spatial near-independence prove generalization errors theorem part theorem essentially implies error trained classiﬁer close hand error less small. section provide empirical results comparing proposed algorithm state algorithms. algorithms comparison ccit algorithm paper xgboost classiﬁer. experiments data-set boot-strap samples algorithm times. results averaged bootstrap runs. kcit kernel test matlab code available online. rcit randomized test package publicly available. perform synthetic experiments regime post-nonlinear noise similar experiments dimension dimension scales generated according relation noise term non-linear function holds. experiments data generated follows coordinate gaussian unit mean variance cos. here ﬁxed generating single dataset. zero-mean gaussian noise variables independent everything else. everything identical except randomly chosen constant fig. plot performance algorithms dimension scales. generating point plot data-sets generated appropriate dimensions. half according half algorithms data-sets score calculated true labels data-set predicted scores. observe accuracy ccit close dimensions upto algorithms scale well. experiments number bootstraps data-set ccit threshold algorithm upper-bound expected variance test-statistic holds. testing algorithm verify relations protein network data ﬂow-cytometry dataset gives expression levels proteins various experimental conditions. ground truth causal graph known absolute certainty data-set however dataset widely used causal structure learning literature. take three popular learned causal structures recovered causal discovery algorithms verify relations assuming graphs ground truth. three graph consensus graph reconstructed graph sachs reconstructed graph graph generate relations follows node graph identify consisting parents children parents children causal graph. conditioned independent every node graph create conditions types three graphs. process generate relations graphs. order evaluate false positives algorithms also need relations observe edge nodes never given conditioning set. graph generate non-ci relations edge selected random conditioning size randomly selected remaining nodes. construct negative examples graph. fig. display performance three algorithms based considering three graphs ground-truth. algorithms given access observational data verifying non-ci relations. fig. display plot three algorithms data-set generated considering graph table display score algorithms three graphs. seen algorithm outperforms others three cases even dimensionality fairly interesting thing note edges three graphs. however three testers ccit kcit rcit fairly conﬁdent edges absent. edges discrepancies ground-truth graphs therefore algorithms lower expected. figure plot performance ccit kcit rcit post-nonlinear noise synthetic data. generating point plots data-sets generated half according rest according algorithms them score plotted. number samples dimension varies. plot curve three algorithms based data graph ﬂow-cytometry dataset. score algorithms provided considering three graphs ground-truth. paper present model-powered approach tests converting binary classiﬁcation thus empowering testing powerful supervised learning tools like gradient boosted trees. provide eﬃcient nearest-neighbor bootstrap makes reduction classiﬁcation possible. provide theoretical guarantees bootstrapped samples also risk generalization bounds classiﬁcation problem non-i.i.d near independent samples. conclusion believe model-driven data dependent approaches extremely useful general statistical testing estimation problems enable powerful supervised learning tools.", "year": 2017}