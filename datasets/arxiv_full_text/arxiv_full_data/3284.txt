{"title": "Efficient SDP Inference for Fully-connected CRFs Based on Low-rank  Decomposition", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Conditional Random Fields (CRF) have been widely used in a variety of computer vision tasks. Conventional CRFs typically define edges on neighboring image pixels, resulting in a sparse graph such that efficient inference can be performed. However, these CRFs fail to model long-range contextual relationships. Fully-connected CRFs have thus been proposed. While there are efficient approximate inference methods for such CRFs, usually they are sensitive to initialization and make strong assumptions. In this work, we develop an efficient, yet general algorithm for inference on fully-connected CRFs. The algorithm is based on a scalable SDP algorithm and the low- rank approximation of the similarity/kernel matrix. The core of the proposed algorithm is a tailored quasi-Newton method that takes advantage of the low-rank matrix approximation when solving the specialized SDP dual problem. Experiments demonstrate that our method can be applied on fully-connected CRFs that cannot be solved previously, such as pixel-level image co-segmentation.", "text": "conditional random fields widely used variety computer vision tasks. conventional crfs typically deﬁne edges neighboring image pixels resulting sparse graph efﬁcient inference performed. however crfs fail model long-range contextual relationships. fully-connected crfs thus proposed. efﬁcient approximate inference methods crfs usually sensitive initialization make strong assumptions. work develop efﬁcient general algorithm inference fully-connected crfs. algorithm based scalable algorithm lowrank approximation similarity/kernel matrix. core proposed algorithm tailored quasi-newton method takes advantage low-rank matrix approximation solving specialized dual problem. experiments demonstrate method applied fully-connected crfs cannot solved previously pixel-level image co-segmentation. semantic image segmentation pixel labeling problem computer vision. given image task label every pixel multiple pre-deﬁned object categories. clear achieve satisfactory results must exploit contextual information. scalability speed algorithm also concerns design algorithm applicable high-resolution images. conditional random ﬁelds successful approaches semantic pixel labeling solves problem maximum posteriori estimation. standard crfs contain unary potentials typically deﬁned low-level features local texture color locations. edge potentials typically deﬁned -neighboring pixels consist smoothness terms penalize label disagreement similar pixels terms model contextual relationships different classes. although models achieved encouraging results segmentation fail capture long-range contextual information. literature fully-connected crfs proposed purpose. main challenge inference fully-connected crfs stems computational cost. fully-connected image pixels edges. even small images thousand pixels number edges million. although variety methods estimation usually computationally infeasible cases. authors proffered efﬁcient mean ﬁeld approximation method inference multi-label models fully connected pairwise terms. ﬁlterbased method used accelerate computation. assumption pairwise terms form weighted mixture gaussian kernels fast bilateral ﬁltering applied. special type fully-connected edge potentials deﬁned capture spatial relationships among different objects depend relative positions efﬁcient inference algorithm developed method proposed applied generalized kernels instead original gaussian kernels. note still strong assumption limits practical value method. general semideﬁnite programming relaxation provides accurate solutions estimation problems ususally computationally inefﬁcient comparison different relaxation methods). standard interior-point methods require ﬂops solve generic problem worst-case semideﬁnite matrix dimension number constraints respectively. recently several scalable authors school computer science university adelaide australia; centre excellence robotic vision. correspondence addressed shen conference version work appearing proc. ieee conference computer vision pattern recognition matrix column vector space symmetric matrices. cone symmetric positive semideﬁnite matrices. space real-valued vectors. +rn− non-negative non-positive orthants identity matrix. all-zero vector proper dimension. all-one vector proper dimension. inequality scalars element-wise inequality column vectors. vector diagonal elements input matrix diagonal matrix whose main diagonal vector input vector trace matrix. rank matrix. indicator function returns cond ture otherwise. frobenius-norm matrix. inner product matrices. methods proposed estimation. huang proposed alternating direction methods multipliers method solve large-scale estimation problems. wang presented efﬁcient dual approach also applied estimation. however methods still cannot applied directly large-scale fully-connected crfs. efﬁcient low-rank approach estimation proposed estimation large-scale fully-connected crfs. several signiﬁcant improvements sdcut presented makes sdcut much scalable. proposed method also overcomes number limitations mean ﬁeld approximation provide stable accurate solutions. low-rank approximation methods spsd kernels seamlessly integrated proposed method used accelerate computational expensive part proposed method. low-rank approximation relaxes limitation pairwise term gaussian kernels symmetric positive-semideﬁnite kernels. low-rank approximation method also used replace ﬁlter-based method mean ﬁeld approximation. thus method much general scalable much broader range applications. proposed approach handle fully-connected crfs states variables particular show image co-segmentation application fast method applicable method achieves superior segmentation accuracy. knowledge method ﬁrst pixel-level co-segmentation method. previous co-segmentation methods relied super-pixel pre-processing order make computation tractable. wang frostig also proposed efﬁcient approaches nearoptimal solutions relaxation problems. main difference methods solve quadratically constrained quadratic programs projected gradient descent uses quasinewton methods solve convex semideﬁnite least-square problem. notation listed table indicate d-dimentional feature vectors corresponding variables respectively. denotes m-th spsd kernel associated linear combination weight. following term used represent symmetric label compatibility function properties µ)∀ll label compatibility function penalizes similar pixels assigned different/incompatible labels. simple label compatibility function would given potts model form pairwise potential general used represent many potentials practical interest. mean ﬁeld approximation used solving problem considered state-of-the-art. ﬁlter-based method used accelerate computation message passing step following sections brieﬂy revisit mean ﬁeld approximation ﬁlter-based method especially respective limitations. mean ﬁeld approximation variational distribution introduced approximate gibbs distribution marginals variable {qi}i∈n supposed independent completely factorized signiﬁcant limitation mean ﬁeld approximation converge potentially many local optima variational problem optimized non-convex. consequence non-convexity mean ﬁeld often sensitive initialization computational bottleneck updating equation expressed matrix-vector products denotes column vector made naive implementation matrix-vector product needs time. kr¨ahenb¨uhl koltun proposed ﬁlter-based approach compute matrix-vector product time discussed next section. product gaussian kernel matrix arbitrary column vector expressed gaussian convolution w.r.t. feature space details). viewpoint signal processing gaussian convolution seen low-pass ﬁlter feature space. convolution result recovered samples whose spacing proportional standard deviation ﬁlter. number ﬁltering methods used compute convolution efﬁciently computational complexity memory requirement linear general pairwise potentials limited gaussian kernels euclidean feature space. feature dimension cannot high. bilateral ﬁltering method exponential complexity dimension time complexity permutohedral lattice quadratic works well input dimension beacause create lattice points blur step accuracy penalty accumulated growth feature dimension. semideﬁnite programming class convex optimization problems minimize/maximize linear objective function intersection cone positive semideﬁnite matrices afﬁne space. general problem expressed following form relaxation widely incorporated develop approximation algorithms binary quadratic program optimizes quadratic objective function binary variables {}n. sdp-based approximation algorithms typically solve following steps lift binary variable positive semideﬁnite matrix variable solve relaxed problem certain accuracy. round solution obtain approximated solution original bqp. problems solved standard interior-point methods found number optimization toolboxes sedumi sdpt mosek although accurate stable interior-point methods scale poorly matrix dimension number linear constraints computational complexity interior-point methods problems iteration worst-case associated memory requirement penalty parameter. assuming constraints encode trace constant deﬁned problem itself approximation following properties proposition following results holds |p)− optima results show accurate approximation problem solution sufﬁciently close given large enough advantage much simpler lagrangian dual proposition lagrangian dual problem simpliﬁed dual. also proved simpliﬁed dual problem following nice properties proposition yields lower-bound optimal objective function value problem proposition continuously differentiable necessarily twice differentiable gradient given wang adopted quasi-newton methods solve dual problem iteration quasi-newton methods objective function gradient need computed computational bottleneck calculation equivalent obtaining positive eigenvalues corresponding eigenvectors note although problem discussed paper contains linear equality constraints sdcut method proposed method easily extended problems linear inequality constraints. contribution paper low-rank approximation positive semideﬁnite kernel matrix based low-rank quasi-newton methods developed large-scale inference. propose approximate spsd kernel matrix low-rank representation rn×rk computational complexity memory requirement computing aforementioned matrix-vector product linear compared pairwise potential function generalized positive semideﬁnite kernel function restriction input feature dimension. best quality achieved low-rank approximation depends spectral distribution kernel matrix itself related smoothness underlying kernel function details). general eigenvalues smooth kernels decay quickly thus well approximated low-rank matrices. optimal low-rank approximation terms spectral norm frobenius norm obatined eigen-decomposition computationally inefﬁcient whose computational complexity generally cubic number low-rank approximation methods achieving linear complexity including nystr¨om methods incomplete cholesky decomposition random fourier features homogeneous kernel maps detailed discussion please refer review papers adopt nystr¨om methods paper low-rank approximation kernel matrices. sampling proved bounded error optimal rank-r approximation given eigen-decomposition several strategies sample representative landmarks i.e. columns including standard uniform sampling non-uniform sampling k-means clustering paper adopt k-means method select landmarks. round k-means columns rather entire matrix required instantiated. section introduce relaxation problem throughout main body paper label compatibility function assumed given potts model relaxation corresponding arbitrary label compatibility function discussed section viii-a. section follow method solves general bqps. several major improvements proposed make sdcut scalable large-scale energy minimization problem another contribution paper. still several issues addressed problem solved work shown rank)+) drops signiﬁcantly ﬁrst several iterations lanczos methods used efﬁciently compute leading eigenpairs. however necessarily low-rank initial several iterations much time spent ﬁrst several eigen-decompositions. crfs considered paper variables. using original sdcut method time spent ﬁrst several iterations prohibitive. general bfgs-like method superlinear convergence speed condition objective function twice continuously differentiable. however dual objective function necessarily twice differentiable. convergence speed sdcut unknown. practice sdcut usually needs iterations converge. next sections introduce improvements sdcut method address problems increase scalability sdcut signiﬁcantly. improved method refer lr-sdcut procedure summarized algrithm initialization dual variable without affecting optimal solution perturbed reduce rank))+) small integer based {trace matrix problem equivalently replaced suppose eigenpair i.e. eigenpair equivalently replace traditionally feasible solution problem obtained rounding optimal solution corresponding formulation rounding procedure carried quasi-newton algorithm iteration quasi-newton algorithm practice dual objective value i.e. lower-bound optimal value increases dramatically ﬁrst several iterations. simultaneously value also drops signiﬁcantly ﬁrst several observation inspires stop quasi-newton algorithm long convergence without affecting ﬁnal solution quality. work adopt random rounding scheme proposed derive γ))+. note positive semideﬁnite decomposed rn×ry rank). rounding scheme expressed following steps random projection entry independently sampled standard gaussian distribution mean variance i.e. discretization obtain {}n×l discretizing computational bottleneck lr-sdcut eigen-decomposition iteration performed lanczos methods paper. lanczos methods require users implement matrixi= uibi)d denotes so-called lanczos vector produced lanczos algorithms iteratively. section accelerate computation matrix-vector product utilizing speciﬁc structures {bi}i=··· give computational cost memory requirement lr-sdcut. show superiority proposed method evaluate methods applications section image segmentation image co-segmentation. following experiments maximum number iterations kmax lr-sdcut initial rank penalty parameter position color value pixel respectively similarly matrix deﬁned corresponds appearance kernel penalizes case adjacent pixels similar color different labels. label compatibility function given potts model computation requires operations performing nystr¨om separately instead directly brings beniﬁts memory requirement reduced rcrp multiple images resolution need perform nystr¨om experiments proposed algorithm compared mean ﬁeld msrc -class database. test data representative images accurate ground truth provided unary potentials also obtained fig. qualitative results image segmentation. original images corresponding ground truth shown ﬁrst columns. third column demonstrates segmentation results based unary terms. results mean ﬁeld methods different matrixvector product approaches illustrated fourth ﬁfth columns. methods achieves similar visual performance mean ﬁeld methods. table quantitative results image segmentation. method runs slower mean ﬁeld methods gives signiﬁcantly lower energy. unfortunately lower energy lead better segmentation accuracy. parameters respectively. iteration number limit mean ﬁeld inference experiments conducted using single memory. matrix-vector product mean ﬁeld method ﬁlter-based nystr¨om-based approaches evaluated evaluated images around pixels number variables also around image. fig. shows qualitative results image segmentation. method achieves similar results mean ﬁeld approach. table quantitative results demonstrated. althgouh computational complexity mean ﬁeld method linear mean ﬁeld still faster experiment. partially code mean ﬁeld highly optimized using unoptimized. speed expected code optimized parallelized. note ﬁlter-based method also incorporated algorithm compute matrix-vector products likely faster nystr¨om methods limited gaussian kernels general. despite slower speed method achieves signiﬁcantly lower energy mean ﬁeld means method better viewpoint estimation. unfortunately superiority method terms optimization lead better segmentation performance. actually evaluated methods similar segmentation accuracy. image co-segmentation problem requires object segmented multiple images. optimization criteria color spatial consistency within image separability foreground fig. qualitative results image co-segmentation. three classes objects msrc datasets used evaluation. approach mean field performed original pixel-level images. sdlr sdcut cannot scale pixellevel images evaluated superpixels. method performs best visually. randomly repeat mean ﬁeld approximation times dataset select best result. mean ﬁeld stable task sometimes converges undesirable local optimal point sdlr sdcut achieve worse results our’s since image details lost superpixels. table running times image co-segmentation. method slightly faster mean ﬁeld. number variables groups evaluated methods shown third sixth columns. problems solved approach much larger sdlr sdcut. table segmentation accuracy image co-segmentation. method mean ﬁeld work original pixels sdlr sdcut work superpixels. three evaluated datasets method achieves lowest energies highest segmentation scores. pixels locate image; otherwise. regularization parameter. block-diagonal matrix matrix-vector product computed using method described section vi-a. inter-image discriminative clustering cost matrix details). centering projection matrix kernel matrix sift features. rn×rk based matrix approximated low-rank decomposition inversion lemma have equation matrix-vector product computed efﬁciently time pairwise potentials necessarily submodular entries negative. note matrix-vector product cannot performed ﬁlter-based method gaussian kernel. experiments three groups images selected msrc dataset image co-segmentation. besides approach mean ﬁeld sdp-based algorithms also evaluated. method mean ﬁeld evaluated original pixel level sdlr sdcut evaluated superpixels. code sdlr sdcut provided authors original papers default settings used. iteration limit mean ﬁeld prevent mean ﬁeld converging undesirable local optima randomly method times. experiments conducted single memory. intersection-over-union accuracy used measure segmentation performance. table demonstrates number variables computational time method. number variables problem solved method mean ﬁeld around times larger sdlr sdcut. approach slightly faster mean ﬁeld signiﬁcantly scalable sdlr sdcut. quantitative performance shown table approach achieves signiﬁcantly better co-segmentation accuracy methods. energy approach also produces lower energies mean ﬁeld. empirically found mean ﬁeld sensitive initialization. take tree example difference best worst energy repeats mean ﬁeld random initializations. repeat mean ﬁeld times best energy improves still worse fig. shows change rank))+) w.r.t. iteration rank energy drops quickly ﬁrst several iterations. simultaneously lower-bound optimal energy increases paper proposed efﬁcient general method estimation fully-connected crfs. proposed approach stable accurate mean ﬁeld approximation also scalable previous methods. low-rank approximation kernel matrix perform matrix-vector products makes approach even efﬁcient applicable symmetric positive semideﬁnite kernel. contrast previous ﬁlter-based methods assume pairwise potentials based gaussian generalized kernel. computational complexity approach linear number variables. experiments image co-segmentation validate approach applied general problems previous methods. future works proposed method parallelized achieve even faster speed. core method quasi-newton eigen-decomposition parallelized gpus. matrix-vector products main computational cost implemented using cuda function cublassgemm. trace constraints non-convex constraint rank dropped relaxation. constraint constraints problem also expressed form solved sdcut algorithm. case kappes andres hamprecht schnorr nowozin batra kausler lellmann komodakis comparative study modern inference techniques discrete energy minimization problems proc. ieee conf. comp. vis. patt. recogn. szeliski zabih scharstein veksler kolmogorov agarwala tappen rother comparative study energy minimization methods markov random ﬁelds smoothness-based priors ieee trans. pattern anal. mach. intell. vol. parameter learning convergent inference dense random ﬁelds proc. int. conf. mach. learn. zhang chen efﬁcient inference fully-connected crfs stationarity proc. ieee conf. comp. vis. patt. recogn. adams baek davis fast high-dimensional ﬁltering using permutohedral lattice eurographics paris durand fast approximation bilateral ﬁlter using signal processing approach proc. eur. conf. comp. mosek optimization toolbox matlab manual. version mosek denmark. reade eigenvalues positive deﬁnite kernels siam journal mathematical analysis vol. reade eigenvalues smooth positive deﬁnite kernels proceedings edinburgh mathematical society vol. wathen spectral distribution kernel matrices related radial basis functions williams seeger effect input density distribution kernel-based classiﬁers proc. int. conf. mach. learn. using nystr¨om method speed kernel machines proc. adv. neural inf. process. syst. drineas mahoney nystr¨om method approximating gram matrix improved kernel-based learning bach jordan kernel independent component analysis mach. learn. res. vol. rahimi recht random features large-scale kernel machines proc. adv. neural inf. process. syst. weighted sums random kitchen sinks replacing minimization randomization learning proc. adv. neural inf. bach sharp analysis low-rank kernel matrix approximations mach. learn. res. vol. gittens mahoney revisiting nystr¨om method improved large-scale machine learning proc. int. conf. mach. sorensen implicitly restarted arnoldi/lanczos methods large scale eigenvalue calculations. springer bri¨et oliveira filho vallentin positive semideﬁnite grothendieck problem rank constraint automata", "year": 2015}