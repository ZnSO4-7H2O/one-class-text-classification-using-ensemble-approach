{"title": "Faster gaze prediction with dense networks and Fisher pruning", "tag": ["cs.CV", "stat.ML"], "abstract": "Predicting human fixations from images has recently seen large improvements by leveraging deep representations which were pretrained for object recognition. However, as we show in this paper, these networks are highly overparameterized for the task of fixation prediction. We first present a simple yet principled greedy pruning method which we call Fisher pruning. Through a combination of knowledge distillation and Fisher pruning, we obtain much more runtime-efficient architectures for saliency prediction, achieving a 10x speedup for the same AUC performance as a state of the art network on the CAT2000 dataset. Speeding up single-image gaze prediction is important for many real-world applications, but it is also a crucial step in the development of video saliency models, where the amount of data to be processed is substantially larger.", "text": "knowledge distillation pruning show stateof-the-art performance achieved much faster architecture achieving speedup performance. secondly provide principled derivation pruning method molchanov extend show extension works well applied gaze prediction. models build recent state-of-the-art model deepgaze ﬁrst review discussing approach speeding backbone deepgaze formed vgg- deep neural network pre-trained object recognition. feature maps extracted several layers upsampled concatenated. readout network convolutions relu nonlinearities takes feature maps produces single output channel implementing point-wise nonlinearity. output blurred gaussian ﬁlter followed addition center bias take account tendencies observers ﬁxate pixels near image center. center bias computed marginal log-probability ﬁxation landing given pixel dataset dependent. finally softmax operation applied produce normalized probability distribution ﬁxation locations saliency here input image extracts feature maps bilinearly upsamples feature maps readout network. improve efﬁciency made minor modiﬁcations reimplementation deepgaze ﬁrst applied readout network bilinearly upsampled one-dimensional output readout network instead upsampling high-dimensional feature maps. also used separable ﬁlters gaussian blur. make sure size saliency matches size input image upsample crop output applying softmax operation. predicting human ﬁxations images recently seen large improvements leveraging deep representations pretrained object recognition. however show paper networks highly overparameterized task ﬁxation prediction. ﬁrst present simple principled greedy pruning method call fisher pruning. combination knowledge distillation fisher pruning obtain much runtime-efﬁcient architectures saliency prediction achieving speedup performance state network dataset. speeding single-image gaze prediction important many real-world applications also crucial step development video saliency models amount data processed substantially larger. ability predict gaze humans many applications computer vision related ﬁelds. used image cropping improving video compression tool optimize user interfaces instance. psychology gaze prediction models used shed light brain might process sensory information recently advances deep learning human gaze prediction received large performance gains. particular reusing image representations trained task object recognition proven useful however networks relatively slow evaluate many real-world applications require highly efﬁcient predictions. example popular websites often deal large amounts images need processed short amount time using cpus. similarly improving video encoding gaze prediction maps requires processing large volumes data near realtime. assumes close eqn. viewed empirical estimate fisher information expectation model replaced real data samples. fact equal model twice differentiable respect parameters hessian reduces fisher information matrix approximation becomes exact. convolutional architectures makes sense prune entire feature maps rather pruning individual parameters. ankij activation feature spatial location datapoint. also introduce binary mask network modiﬁes activations ankij feature follows since pruning signal therefore pruning. gradient respect tivations available backward pass computing network’s gradient pruning signal therefore computed little extra computational cost. note pruning signal similar used molchanov uses absolute gradients instead squared gradients certain normalization pruning signal derivation provides principled motivation. alternative derivation require close provided supplementary section regularizing computational complexity basic alternative architectures providing different trade-offs computational efﬁciency performance. first instead vgg- faster vgg- architecture performance lost using smaller network part compensated ﬁne-tuning feature representations instead using ﬁxed pre-trained representations. second densenet- feature extractor. densenets shown efﬁcient computationally terms parameter efﬁciency compared state-of-the-art networks object recognition task even starting parameter efﬁcient pre-trained models resulting gaze prediction networks remain highly over-parametrized task hand. decrease number parameters turn pruning greedy removal redundant parameters feature maps. following section derive simple principled method greedy network pruning call fisher pruning. goal remove feature maps parameters contribute little overall performance model. section consider general case network parameters trained minimize cross-entropy loss inputs outputs expectation taken respect data distribution ﬁrst consider pruning single parameters change parameters approximate corresponding change loss order approximation around current parameter value unit vector zero everywhere except entry following related methods also start order approximation assume current parameters local optimum term vanishes average dataset input images. practice found including ﬁrst term actually reduced performance pruning method. diagonal hessian contain weights neural network also contain binary mask describing architecture. measures computational complexity network. optimization quantify computational complexity terms ﬂoating point operations. example number ﬂoating point operations convolution bias term ﬁlters input channels cout output channels producing feature spatial extent given reduction loss estimated previous section. training periodically estimate cost feature maps greedily prune feature maps minimize combined cost. pruning feature expect loss computational cost down. different different architectures become optimal solutions optimization problem. models trained several steps. first trained deepgaze model using adam batch size learning rate slowly decreased course training. model ﬁrst pre-trained using salicon dataset using dataset validation. validation data evaluated every steps training stopped cross-entropy validation data decrease times row. parameters best validation score observed saved. afterwards dataset split training validation sets used train deepgaze models early stopping. ensemble deepgaze models used generate average saliency image salicon dataset. saliency maps used knowledge distillation additional data allows train readout network models also ﬁne-tune underlying feature representation. used weighted combination cross-entropy cross-entropy respect deepgaze saliency maps using weights respectively. training models convergence start pruning network. accumulated pruning signals training steps continuing update parameters pruning single feature map. feature selected maximize reduction combined cost tried using combined cost early stopping automatically determine appropriate number feature maps prune however found early stopping terminated early therefore opted treat number pruned features hyperparameter optimized random search. pruning phase used ﬁxed learning rate momentum found slightly better results using adam. explained regularizing effect many recent papers used pretrained neural networks feature extractors prediction ﬁxations closely related work deepgaze approach contrast deepgaze also optimize feature representations despite limited amount available ﬁxation training data possible combination knowledge distillation pruning regularize networks. kruthiventi also tried ﬁnetune pretrained network using smaller learning rate pretrained parameters parameters. trained smaller network end-to-end start pretrained representation therefore achieve performance current state-of-the-art architectures. similarly trained networks end-to-end initializing layers parameters obtained pretrained networks since outperformed deepgaze recent approaches. many heuristics developed pruning closely related methods directly estimate effect loss. optimal brain damage example starts order approximation squared error loss computes second figure speed performance various models trained evaluated training set. point corresponds different architecture different number pruned features different solid lines mark pareto optimal models. regularized models explicitly optimized reduce amount ﬂoating point operations instead optimizing number feature maps x-axis shows speed measured single image input. except log-likelihood densegaze explicitly regularizing computational performance made difference terms speed. also pruned densegaze models generalize better deepgaze derivatives analytically performing additional backward pass. optimal brain surgeon extends method automatically tries correct parameters pruning computing full hessian. contrast pruning signal requires gradient information already computed backward pass. makes proposed fisher pruning efﬁcient also easier implement. closely related pruning method approach molchanov applying heuristics order approximation arrive similar estimate change loss pruning main contribution section derivation provides principled motivation pruning signal. unlike papers pruning molchanov also explicitly regularized computational complexity network. however approach used ﬁxed weight pruning different number feature maps. contrast treat setting creating separate optimization problem optimal architecture. practice speed architecture network heavily inﬂuenced choice even pruning number feature maps suggesting using different weights important. molchanov estimated computational cost feature starting prune. leads suboptimal pruning computational cost feature changes neighboring layers pruned. explored basic architectures. first tried smaller vgg- variant simonyan feature extraction. contrast readout network k¨ummerer took input feature maps different layers used output last convolutional layer input. extracting information multiple layers less important case since also optimizing parameters feature extraction network. alternative tried densenet- feature extractor using output dense block input readout network. cases readout network consisted convolutions parametric rectiﬁed linear units feature maps hidden layers. following call ﬁrst network fastgaze second network densegaze. optimal pruning parameters multiple experiments randomly sampled number pruned features randomly chosen trade-off parameter total number feature maps fastgaze densegaze. chosen evaluated model terms log-likelihood commonly used metric area curve used publicly available dataset evaluation used training models. many ways measure computational cost mostly interested single-image performance cpus. used single core intel xeon averaged speed images pixels. implementation written pytorch figure graph visualizes feature maps unpruned fastgaze model remaining graphs visualize models pruned different degrees. labels indicate number feature maps. bottom measured runtime models strongly regularized models observed tendency alternate convolutions many feature maps convolutions feature maps. left corner shows example input image ground truth ﬁxations right column shows saliency models generated different models. despite difference speed saliency maps visually similar. terms log-likelihood fastgaze densegaze generalize better reimplementation deepgaze despite fact models regularized imitate deepgaze terms deepgaze performs slightly better fastgaze outperformed densegaze. pruning seems small effect performance even heavily pruned models still perform well. achieve speedup roughly densegaze terms loglikelihood even heavily pruned model yielded better performance comparing densegaze fastgaze densegaze achieves better performance fastgaze able achieve faster runtimes less complex architecture. explicitly regularizing computational complexity important. depending amount pruning observe speedups fastgaze comparing regularized non-regularized models. figure visualize pruned fastgaze models. lower computational complexity optimal architectures tendency alternate convolutions large small numbers feature maps. makes sense considering computational cost convolutional layer interesting architecture still perform well terms ﬁxation prediction requires detection various types objects. qualitative results provided figure even large reductions computational complexity ﬁxation predictions appear similar. speedup compared deepgaze saliency maps start become blurrier generally detect structures. particular model still responds faces people objects signs text. verify models indeed perform close state submitted saliency maps saliency benchmark computed saliency maps test contains images type mit. evaluated fastgaze model took evaluate pytorch densegaze model took models perform slightly state evaluated described principled pruning method requires gradients input efﬁcient easy implement. unlike pruning methods explicitly penalized computational complexity tried architecture optimally optimizes given table comparison deep models evaluated benchmark dataset. reference also include performance gaussian center bias. results competing methods obtained benchmark’s website trade-off performance computational complexity. able show computational complexity state-of-the-art saliency models drastically reduced maintaining similar level performance. together knowledge distillation approach reduced complexity allowed train models endto-end achieve good generalization performance. less resource intensive models particular importance applications data processed well applications running resource constrained devices mobile phones. faster gaze prediction models also potential speed development video models. larger number images processed videos impacts training times making difﬁcult iterate models. another issue amount ﬁxation training data existence fairly limited videos. smaller models allow faster training times efﬁcient available training data.", "year": 2018}