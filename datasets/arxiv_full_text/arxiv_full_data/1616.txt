{"title": "Learning Relational Dependency Networks for Relation Extraction", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "We consider the task of KBP slot filling -- extracting relation information from newswire documents for knowledge base construction. We present our pipeline, which employs Relational Dependency Networks (RDNs) to learn linguistic patterns for relation extraction. Additionally, we demonstrate how several components such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction.", "text": "figure pipeline full relation extraction pipeline. components shaded boxes indicate proposed contributions work adding training labels enhancing feature descriptors initializing human advice rules. sults human advice useful many relations joint learning beneﬁcial cases relations correlated among expected lines. however surprising observations include fact weak supervision useful expected wordvec features predictive domain-speciﬁc features. ﬁrst present proposed pipeline different components learning system. next present relations learn presenting experimental results. ﬁnally discuss results comparisons concluding presenting directions future research. present different aspects pipeline depicted figure ﬁrst describe approach generating features training examples corpus describing core framework boost algorithm. consider task slot ﬁlling extracting relation information newswire documents knowledge base construction. present pipeline employs relational dependency networks learn linguistic patterns relation extraction. additionally demonstrate several components weak supervision wordvec features joint learning human advice incorporated relational framework. evaluate different components benchmark task show rdns effectively model diverse features perform competitively current state-of-the-art relation extraction. problem knowledge base population constructing knowledge base facts gleaned large corpus unstructured data poses several challenges community. commonly relation extraction task decomposed subtasks entity linking entities linked already identiﬁed identities within document entities existing slot ﬁlling identiﬁes certain attributes target entity. present work-in-progress slot ﬁlling based probabilistic logic formalisms present different components system. speciﬁcally employ relational dependency networks formalism successfully used joint learning inference stochastic noisy relational data. consider system current stateof-the-art demonstrate effectiveness probabilistic relational framework. additionally show rdns effectively incorporate many popular approaches relation extraction joint learning weak supervision wordvec features human advice among others. provide comprehensive comparison settings joint learning learning individual relations weak supervision gold standard labels using expert advice learning data etc. questions extremely interesting general machine learning perspective also critical community. show empirically refeature wordstring wordposition caselesswordstring word string lower case wordlemma isneword nextwords prevwords nextpos prevpos nextlemmas prevlemmas nextne prevne lemmabetween canonical form word whether word succeeding words preceding words succeeding words preceding words canonical form successors canonical form predecessors succeeding phrases preceding phrases canonical form word occurring word word text processed using stanford corenlp toolkit extract parts-of-speech word lemmas etc. well generate parse trees dependency graphs named-entity recognition information. full extracted features listed table converted features prolog format given input system. addition structured features output stanford toolkit also deeper features based wordvec input learning system. standard features tend treat words individual objects ignoring links words occur similar meanings importantly similar contexts wordvec provide continuous-space vector embedding words that practice capture many relationships word vectors stanford google along speciﬁc words that experts believe related relations learned. example include words father mother devoutconvert follow generated features word vectors ﬁnding words high similarity embedded space. used word vectors considering relations following difﬁculty task documents come labeled gold standard labels annotation prohibitively expensive beyond hundred documents. problematic discriminative learning algorithms like learning algorithm excel given large supervised training corpus. overcome obstacle employ weak supervision external knowledge heuristically label examples. following work soni employ approaches generating weakly supervised examples distant supervision knowledge-based weak supervision. distant supervision entails external knowledge heuristically label examples. following standard procedure three data sources never ending language learner wikipedia infoboxes freebase. given target relation identify relevant database entries database form entity pairs parent database) serve seed positive training examples. pairs must mapped mentions corpus must sentences corpus contain entities together process done heuristically fraught potential errors noise alternative approach knowledge-based weak supervision based previous work following insight labels typically created domain experts annotate labels carefully typically employ inherent rules mind create examples. example identifying family relationship inductive bias towards believing persons sentence last name related words daughter strong indicators parent relation. call world knowledge describes domain target relation. effect encode domain expert’s knowledge form ﬁrst-order logic rules accompanying weights indicate expert’s conﬁdence. probabilistic logic formalism markov logic networks perform inference unlabeled text potential entity pairs corpus queried yielding positive examples. choose mlns permit domain experts easily write rules providing probabilistic framework handle noise uncertainty preferences simultaneously ranking positive examples. entitytype entitytype nextword word entitytype entitytype prevlemma entitytype entitytype nextlemma parents entitytype entitytype nextlemma parents table rules weak supervision sample knowledge-based rules weak supervision provided labelers. ﬁrst value deﬁnes weight conﬁdence accuracy rule. target relation appears clause. represent entities persons organizations numbers respectively. task rules used shown table example ﬁrst rule identiﬁes number following person’s name separated comma likely person’s third fourth rule provide examples rules utilize textual features; rules state appearance lemma mother father persons indicative parent relationship previous research demonstrated joint inferences relations effective considering relation individually. consequently considered formalism successfully used joint learning inference stochastic noisy relational data called relational dependency networks rdns extend dependency networks relational setting. idea approximate joint distribution random variables product marginal distributions shown employing gibbs sampling presence large amount data allows approximation particularly effective. note that explicitly check acyclicity making particularly easy learned. typically distribution represented relational probability tree however following previous work replace distribution relational regression trees built sequential manner i.e. replace single tree gradient boosted trees. approach shown stateof-the-art results learning rdns adapted boosting learn relation extraction. since method requires negative examples created negative examples considering possible combinations entities present positive example sampled twice many negatives positive examples. advice rules entity preceded number phrase year-old probably refers age. entity present phrase sentence turned probably refers age. entity also known entity probably refers alternate name. entity nicknamed entity probably refers alternate name. entity followed phrase citizen entity probably refers origin. entity followed phrase devout entity probably refers religion. entity followed entity-based company probably refers city/state/country headquarters. entity entity siblings parents other. entity entity spouses parents other. relational learning methods restrict human merely annotating data beyond request human advice. intuition humans read certain patterns deduce nature relation entities present text. goal work capture mental patterns humans advice learning algorithm. modiﬁed work odom learn rdns presence advice. idea explicitly represent advice calculating gradients. allows system trade-off data advice throughout learning phase rather consider advice initial iterations. advice particular become inﬂuential presence noisy less amout data. sample advice rules english presented table note rules soft rules true many situations. odom weigh effect rules data hence allow partially correct rules. table relations relations considered kbp. columns indicate number training examples utilized human annotated weakly supervised available number test examples relations describe person entities last describe organizations present experimental evaluation. considered speciﬁc relations categories person organization competition. relations considered listed left column table utilize documents training utilizing documents corpus testing. results presented obtained different runs train test sets provide robust estimates accuracy. consider three standard metrics area curve score recall certain precision. chose precision since fraction positive examples negatives negative examples re-sampled training run. must mentioned relations number hand-annotated examples documents annotated different number instances relations. train/test gold-standard sizes provided table including weakly supervised examples available. lastly control factors default setting experiments individual learning standard features gold standard examples table weak supervision results comparing models trained gold standard examples models trained gold standard weakly supervised examples combined weak supervision answer generated positive training examples using weak supervision techniques speciﬁed earlier. speciﬁcally evaluated relations show table based experiments utilized knowledge-based weak supervision approach provide positive examples relations. range rules derived relation. examples organization relations countryhq oundedby generated using standard distant supervision techniques freebase databases mapped oundedby wikipedia infoboxes provides entity pairs countryhq. lastly weakly supervised examples utilized experiments performing larger runs part work progress. results presented table compared standard pipeline learned gold standard examples versus system learned weak gold examples combined. surprisingly weak supervision seem help learn better models inferring relations cases. relations origin otherf amily substantial improvements shows improvements otherf amily datef ounded. hypothesize generating examples help nonetheless lack improved models even modest number examples surprising result. alternatively number gold standard examples provided sufﬁcient learn models. thus answered equivocally negative. learning relations jointly within displayed table recall omitted conciseness conclusions across metrics. joint learning appears help half relations particularly person category joint learning gold standard outperforms individual learning counterparts. fact relations parents spouse siblings etc. inter-related learning jointly indeed improves performance. hence answered afﬁrmatively half relations. wordvec table shows results experiments comparing framework without wordvec features. wordvec appears largely impact boosting results relations. hypothesize limitation depth trees learned. learning and/or deeper trees improve wordvec features additional work done generate deep features word vectors. answered cautiously negative although future work could lead improvements. advice table shows results experiments test advice within joint learning setting. advice improves matches performance using joint learning. impact advice mostly seen improvement recall several relations. clearly shows using human advice patterns allows extract relations effectively making noisy less number training examples. in-line previously published machine learning literature humans mere labelers providing useful advice learning algorithms improve performance. thus answered afﬁrmatively. boost relation factory relation factory efﬁcient open source system performing relation extraction based distantly supervised classiﬁers. system competition thus serves suitable baseline method. conservative responses making difﬁcult adjust precision levels. generous present recall returned results recall scores system presented table system performs comparably often better state-of-the-art relation factory system. particular method outperforms relation factory across relations. recall provides mixed picture approaches showing improvements outperforms relations relation factory table relation factory results comparing relation factory algorithm presented paper. values bold indicate superiour performance alternative approach. note instances provides superior recall dramatic improvements also shows rdn’s superior performance outperforming relations. thus conclusion framework performas comparably better across metrics state-of-the-art. presented fully relational system utilizing relational dependency networks knowledge base population task. demonstrated rdn’s ability effectively learn relation extraction task performing comparably state-of-art relation factory system. furthermore demonstrated ability rdns incorporate various concepts relational framework including wordvec human advice joint learning weak supervision. surprising results weak supervision wordvec signiﬁcantly improve performance. however advice extremely useful thus validating long-standing results inside artiﬁcial intelligence community relation extraction task well. possible future directions include considering larger number relations deeper features ﬁnally comparisons systems. believe work developing wordvec features utilizing weak supervision examples reveal insights effectively utilize features rdns.", "year": 2016}