{"title": "Known Unknowns: Uncertainty Quality in Bayesian Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We evaluate the uncertainty quality in neural networks using anomaly detection. We extract uncertainty measures (e.g. entropy) from the predictions of candidate models, use those measures as features for an anomaly detector, and gauge how well the detector differentiates known from unknown classes. We assign higher uncertainty quality to candidate models that lead to better detectors. We also propose a novel method for sampling a variational approximation of a Bayesian neural network, called One-Sample Bayesian Approximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We compare the following candidate neural network models: Maximum Likelihood, Bayesian Dropout, OSBA, and --- for MNIST --- the standard variational approximation. We show that Bayesian Dropout and OSBA provide better uncertainty information than Maximum Likelihood, and are essentially equivalent to the standard variational approximation, but much faster.", "text": "evaluate uncertainty quality neural networks using anomaly detection. extract uncertainty measures predictions candidate models measures features anomaly detector gauge well detector differentiates known unknown classes. assign higher uncertainty quality candidate models lead better detectors. also propose novel method sampling variational approximation bayesian neural network called one-sample bayesian approximation experiment datasets mnist cifar. compare following candidate neural network models maximum likelihood bayesian dropout osba mnist standard variational approximation. show bayesian dropout osba provide better uncertainty information maximum likelihood essentially equivalent standard variational approximation much faster. current deep learning focuses point estimates many real-world applications require full range uncertainty. reliable conﬁdence prediction might useful prediction itself. debate dangers overconﬁdent machine learning reached headlines mass media indeed models drive cars diagnose medical conditions even analyze risk criminal recidivism unreliable conﬁdence appraisal dire consequences. traditional deep learning trains maximum likelihood needing aggressive regularization avoid overﬁtting provides point estimates limited uncertainty information. model outputs vector probabilities quantify uncertainty using entropy prediction. however model predict high conﬁdence samples outside distribution seen training frequentist mitigations like bootstrap scale well deep models. true bayesian models infer posterior distribution unknown factors computational demands often prohibitive. hand proﬁtably reinterpret bayesian perspective regularizations used ordinary deep learning early stopping weight decay ghahramani show multiple dropout forward passes test time equivalent bayesian prediction given particular variational approximation. direct approach variationally approximates posterior weight propose novel bayesian approach neural networks similar variational approximation blundell much cheaper computationally. call approach one-sample bayesian approximation investigate whether achieves better quality uncertainty information traditional maximum likelihood. exactly approach presented blundell section instead sampling weight matrices training example sample matrices mini-batch weights examples mini-batch. approach leads expected gradient trading higher variance computational efﬁciency evaluate quality uncertainty information employ anomaly detection deciding whether test sample belongs classes seen training. concretely pick classiﬁcation problem exclude classes training evaluate much insight candidate model classiﬁcation conﬁdence. expect bayesian neural networks express uncertainty well point decide whether sample belongs known classes. thus employ anomaly detector relative measure quality uncertainty information output candidate models figure uncertainty quality evaluation using anomaly detection task. experimental pipeline follow compare uncertainty quality among candidate models. train candidate probabilistic classiﬁer original task extract uncertainty train linear anomaly detector using information classiﬁer prediction. uncertainty measures features. calculate anomaly detector. higher detector aucs indicate candidate model provides better uncertainty information. contrast experimental protocols. blind protocol separate classes groups train candidate neural network using classes; train classes separate anomaly detector using uncertainty extracted prediction candidate network. calibrated protocol separate classes three groups train candidate network using classes loss function using correct labels unknown classes loss function using equiprobable prediction vector; train classes separate anomaly detector using features before. test used compute anomaly task excludes samples used train anomaly detector samples used train candidate neural network. mnist cifar datasets. mnist candidate networks twolayered fully-connected architecture neurons each dropout applied hidden layer. cifar candidate networks convolutional blocks followed fully-connected layer neurons optimize adam limit training procedure epochs mnist table possible combination unknown classes showing sample class. mnist’s classes crisp semantic separation; cifar’s considerable overlap specialization background overlap might reduce accuracy anomaly detection measure uncertainty quality. methods evaluate usual baseline maximum likelihood bayesian posterior estimated dropout approximation standard variational bayesian neural networks using sample mini-batch mnist also evaluate standard variational approximation features anomaly detection uncertainty measures extracted probabilistic predictions. simplicity detector linear logistic classiﬁer regularization parameter stratiﬁed cross-validation vector predicted probabilities available thus employ feature entropy theoretically sound measure uncertainty vector. bayesian methods provide extra information; feature vector average standard deviation entropy decision vector network prediction samples entropy average decision vector samples average standard deviations predictions class. analyze results using bayesian anova separate mean protocol equivalent two-way anova without interactions global mean experimental protocol factors fused together methods factors variation. constrain effects zero identiﬁability. response variable anomaly detector. weakly informative priors. following model reﬂects choices implement model using stan infer posteriors unknown parameters using nuts algorithm ensure proper convergence chains steps including burn-in thinning factor kruschke’s suggestion present distribution marginal effects distribution differences effects. figure mnist dataset. cell plots distribution inﬂuence factor shown label marginalized factors. highlight means highest posterior density intervals topmost rows consider factors rows differences effects. consider differences signiﬁcant contain domain anomaly detector. show bayesian anova results figures calibration auxiliary unknown classes large effect larger choosing among uncertainty methods. calibration however realistic many applications artiﬁcial constraint picking well-formatted unknown classes. well-controlled scenario provided mnist bayesian methods give signiﬁcantly better uncertainty information mnist bayesian methods outperform effects appear signiﬁcantly different other. cifar however perfect semantic separation classes questionable performance differences disappear slightly outperforms osba slightly outperforms none differences appear signiﬁcant. table shows accuracies candidate models. note competing candidate models similar performance gains anomaly detection rather come enhanced probabilistic information increased accuracy. table test accuracy original classiﬁcation task. show mean accuracy standard deviation parentheses averaged different replications. competing candidate models similar accuracies showing enhanced uncertainty quality comes enhanced probabilistic information extra accuracy. note osba accuracy latter times slower. formalized ascertain uncertainty quality neural networks using anomaly detection. contrasted usual maximum likelihood networks bayesian alternatives. bayesian networks outperformed frequentist network cases. also proposed novel sample variational approximation bayesian neural network osba much faster standard sampling procedure still retains uncertainty quality. osba faster experiments observed relative training computational costs believe thus techniques like osba deserve investigation contexts. finding general measure uncertainty quality however still challenge. experiments suggest anomaly detection gives good uncertainty measures well-separated classes like mnist’s; uncontrolled datasets like cifar need measure tolerates degree semantic intersection classes. future work intend explore forms uncertainty quality evaluation test osba varied settings. figure distributions aucs mnist combinations probabilistic approach experimental protocol. boxplot represents replications obtained picking random unknown classes. thank brazilian agencies capes cnpq fapesp ﬁnancial support. gratefully acknowledge support nvidia corporation donation tesla used research. eduardo valle partially supported google awards latam grant cnpq grant ramon oliveira supported grant motorola mobility brazil.", "year": 2016}