{"title": "SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning  and Control", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.NE", "cs.SY"], "abstract": "In this work, we present an approach to deep visuomotor control using structured deep dynamics models. Our deep dynamics model, a variant of SE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an encoder-decoder structure. Unlike prior work, our dynamics model is structured: given an input scene, our network explicitly learns to segment salient parts and predict their pose-embedding along with their motion modeled as a change in the pose space due to the applied actions. We train our model using a pair of point clouds separated by an action and show that given supervision only in the form of point-wise data associations between the frames our network is able to learn a meaningful segmentation of the scene along with consistent poses. We further show that our model can be used for closed-loop control directly in the learned low-dimensional pose space, where the actions are computed by minimizing error in the pose space using gradient-based methods, similar to traditional model-based control. We present results on controlling a Baxter robot from raw depth data in simulation and in the real world and compare against two baseline deep networks. Our method runs in real-time, achieves good prediction of scene dynamics and outperforms the baseline methods on multiple control runs. Video results can be found at: https://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/", "text": "fig. example scenario showing initial target point cloud se-pose-nets used control robot reach target state based depth data. depth images colorized display purposes only. scenes. observe three points dataassociate across scenes learning predict poses detected objects/parts scene model dynamics object directly predicted low-dimensional pose space predict scene dynamics combining dynamics predictions detected part. combine ideas work propose se-posenets deep network architecture efﬁcient visuomotor control jointly learns data-associate across long term sequences. make following contributions demonstrate deep predictive model used reactive visuo-motor control using simple gradient backpropagation sophisticated gauss-newton optimization reminiscent approaches inverse kinematics modeling scenes dynamics work builds prior work learning structured models scene dynamics unlike se-nets explicitly model data associations low-dimensional pose embedding train consistent across long sequences. similar boots model learns predict point clouds based abstract— work present approach deep visuomotor control using structured deep dynamics models. deep dynamics model variant se-nets learns low-dimensional pose embedding visuomotor control encoder-decoder structure. unlike prior work dynamics model structured given input scene network explicitly learns segment salient parts predict poseembedding along motion modeled change pose space applied actions. train model using pair point clouds separated action show given supervision form point-wise data associations frames network able learn meaningful segmentation scene along consistent poses. show model used closedloop control directly learned low-dimensional pose space actions computed minimizing error pose space using gradient-based methods similar traditional model-based control. present results controlling baxter robot depth data simulation real world compare baseline deep networks. method runs real-time achieves good prediction scene dynamics outperforms baseline methods multiple control runs. video results found https//rse-lab.cs. washington.edu/se-structured-deep-ctrl/ imagine receiving observations scene camera would like control robot reach target scene. traditional approaches visual servoing decompose problem parts data-associating current scene target modeling effect applied actions changes scene combining tight loop servo target. recent work deep learning looked learning similar predictive models directly space observations relating changes pixels points directly applied actions given target scene predictive model generate suitable controls visually servo target using model-predictive control unfortunately pipeline work need external system capable providing long range data associations measure progress. showed prior work instead reasoning pixels predict scene dynamics decomposing scene objects predicting object dynamics instead. signiﬁcantly improves prediction results still provide clear solution data-association problem encounter control still lack capability explicitly associate objects/parts across applied actions structured intermediate representation reasons objects motions. unlike finn operate depth data reason motion using masks transforms training networks supervised fashion given point-wise data associations across pairs frames. visuomotor control recently work visuomotor control primarily deep networks methods either directly regress controls visual data generate controls planning learned forward dynamics models inverse dynamics models reinforcement learning similar methods generates controls planning learned dynamics model albeit learned low-dimensional latent space. speciﬁcally work finn closely related differs main ways unlike approach controls observation space sampled actions controller runs gradient based optimization learned low-dimensional pose embedding real-time also approach requires external tracker measure progress explicitly learn data associate across large motions. work borrows several ideas prior work watter learns latent low-dimensional embedding fast reactive control pairs images related action. unlike work though structured latent representation predict object masks physically grounded loss models change observations opposed restrictive image reconstruction loss. last losses physically motivated similar proposed training position-velocity encoders learned pose embedding signiﬁcantly structured train networks end-to-end directly control. data association related work computer vision literature looked tackling data association problem primarily matching visual descriptors either hand-tuned recently learned using deep networks prior work schmidt learn robust visual descriptors long-range associations using correspondences short training sequences. unlike work correspondences pairs frames learn consistent pose space lets data associate across long sequences. multiple approaches visual servoing years including newer methods deep learned features reinforcement learning methods depend external system data association pre-speciﬁed features system trained end-to-end control directly depth data. deep dynamics model se-pose-nets decomposes problem modeling scene dynamics three subproblems modeling scene structure identifying parts scene move distinctly encoding latent state pose modeling dynamics individual parts effect applied actions change latent pose space transform) ﬁnally combining local pose changes model dynamics entire scene. sub-problem modeled separate component se-pose-net modeling scene structure encoder decomposes input point cloud rigid parts predicting part pose dense segmentation mask highlights points belonging part predicting scene dynamics transform layer predicts next point cloud given current point cloud predicted object masks predicted pose deltas explicitly applying rigid body transforms input point cloud. fig. shows network architecture se-pose-net. next present details three sub-components outline training procedure training se-pose-net end-to-end minimal supervision. encoder three parts ﬁrst convolutional network generates latent representation input point cloud network convolutional layers followed pooling layer. latent representation used input mask pose predictions. object masks de-convolutional network predict dense pixel-wise segmentation scene it’s constituent parts similar prior work fully-convolutional architecture de-convolutional layers skip-add architecture improve sharpness predicted segmentation. masks predicted network full resolution channels pre-speciﬁed hyper-parameter greater equal number moving parts scene predicted segmentation mask learns attend parts scene move together representing areas scene move independently different parts. prior work formalize mask prediction soft-classiﬁcation problem network outputs k-length probability distribution sharpen push towards binary segmentation mask. object poses given encoded latent representation three layer fully-connected network predict pose segmented parts. represent pose numbers position fig. se-pose-net architecture consisting three components encoder predicts dense segmentation masks poses pose transition models change pose space effect applied action transform layer applies pose changes current point cloud generate predicted point cloud bottom left graph showing procedure training se-pose-net along loss functions loss predicted point cloud pose consistency loss relating \"next\" poses predicted transform network encoder bottom right control using se-pose-net. given target point cloud encoded poses learned encoder learned transition model plan sequence actions minimizing error directly pose space initial point cloud orientation represented -parameter axisangle vector. show later pose network learns predict consistent poses used data-associate observations long sequences motions. finally given predicted scene segmentation change poses model dynamics input scene effect applied action transform layer applies predicted rigid rotations translations input point cloud weighted predicted mask probabilities predict transformed point cloud output point corresponding input point effect apply rotation translation points belong corresponding object indicated mask channel predict transformed points belonging object. repeating objects gives transformed output point cloud note part trainable parameters. details please refer prior work high level encoder implicitly learns structure observed scenes persistently identifying parts predicting consistent pose part across multiple scenes. identiﬁed constituent parts scene poses reason effect applied actions parts. model notion \"part dynamics\" fully-connected pose transition network takes predicted poses encoder applied actions input predict change pose segmented parts represented transform part rotation translation vector transition network ﬁrst applies fully connected layers inputs concatenates outputs followed ﬁnal fully-connected layers predict pose-deltas. show refers composition pose space expected pose composing current pose predicted pose change transition model cardinality essence loss constrains encoder predict poses consistent pose-deltas predicted transition model. loss encourages global consistency pose space enforcing local consistency pairs frames crucial learning pose space consistent across long term motions. total loss training losses controls relative strengths losses. experiments. point note provide explicit supervision learn pose space. consistency loss ensures poses less globally consistent anchor speciﬁc position orientation. such poses learned network need correspond directly canonical pose parts predicted part position need correspond center orientation need aligned part’s principal axes. providing constraints regularize physically ground pose space interesting area future work. show se-pose-net used closed-loop visuomotor control reach target speciﬁed target depth image essentially performing visual servoing crucial component every visual servoing system perform data association current image target image used generate controls reduce corresponding offsets. se-pose-nets solve problem making learned low-dimensional latent pose space. enforcing frame-to-frame consistency pose space consistency loss pose space becomes consistent encoder network learns data-associate observations unique poses consistent effect actions. importantly data associations generated mask object level resulting ability akin object detection computer vision. unlike prior work restricted operate observation space points requires data associations current target points provided externally directly minimize error poses automatically extracted initial target depth image recover sequence actions takes robot additionally unlike prior work need external tracking system measure progress toward goal learned encoder implicitly tracks pose space. reactive control algorithm presents simple algorithm reactive control using se-pose-nets efﬁciently computes closedloop sequence controls takes robot initial state speciﬁed target related action i.e. input point know it’s corresponding point next frame visible supervision given learning masks poses change poses. fig. shows schematic procedure. given point clouds encoder predict corresponding masks poses loss penalizes error predicted point cloud data associated target point cloud normalized version mean-squared error measures negative log-likelihood gaussian centered around target standard deviation dependent target magnitude denotes ground truth motion point relative input point cloud number points point cloud number points actually move hyper-parameters loss aimed tackle main issues standard loss normalizing loss separate scalar dimension depends target magnitude make loss scale invariant allowing treat equally parts move less large motion dividing total error number points move scene treat scenes points move equally large parts move. pose consistency loss encourages consistency poses predicted encoder change pose predicted pose transition network receive current observation predict current pose henc initialize control zeros predict change pose htrans predict next pose ˆpt+ compute pose error given target point cloud algorithm uses learned encoder predict poses constituent parts henc. becomes target controller. every time step algorithm computes pose embedding current observation would like controls move poses closer target poses. this algorithm makes prediction learned pose transition model using current poses initial guess controls resulting predicted change poses corresponding predicted next pose move poses towards targets formulate error function based mean-squared error predicted poses target poses. algorithm computes gradient error respect control inputs uses generate next controls. propose ways computing gradient backpropagation simple approach compute gradient update backpropagate gradients pose error pose transition model. unlike backpropagation training compute gradients w.r.t. network weights weights compute gradients input controls. resulting control scheme analogous jacobian transpose method inverse kinematics backprop provides gradient transition model. gauss-newton better approach compute jacobian transition model gradient pose error however instead computing backpropagation condition pose error gradient based jacobian’s pseudoinverse controls strength conditioning practice leads signiﬁcantly faster convergence little additional overhead computation compared backpropagation method jacobian computed efﬁciently ﬁnite differencing. running single forward propagation perturbed control inputs stacked along batch dimension take advantage parallelism. eqn. also analogous damped least squares technique inverse kinematics finally algorithm computes unit-vector direction computed update scales pre-speciﬁed control magnitude umax next control execute control robot repeat closed-loop convergence measured either reaching small error pose space maximum number iterations whichever comes ﬁrst. ﬁrst evaluate se-pose-nets predicting dynamics scene baxter robot moves right front depth camera simulation real world. also present results control performance task control joints baxter’s right reach speciﬁed target observation. ﬁrst provide details task setting simulation. simulator uses opengl render depth images camera pointed towards robot kinematic little dynamics motion depth noise. test parse effectiveness proposed algorithm compare various baselines. collected around hours training data simulator robot moves joints it’s right arm. around half examples whole motions robot plans trajectory reach target end-effector position sampled randomly workspace front robot. rest motions made perturbations individual joints robot various initial conﬁgurations sampled within viewpoint camera. additional motions help de-correlating kinematic chain dependencies training improving performance especially joints lower kinematic chain. overall dataset around training images collected single ﬁxed viewpoint. similar simulated setting collect data real robot baxter moves right front asus xtion camera placed around meters robot. data associations ground truth masks ground truth ﬂows determined dart tracker real data. collected around hours training data real robot whole motions single joint motions. before motions generated planner tries end-effector randomly sampled targets workspace. unlike simulated data depth data real world quite noisy signiﬁcant fig. masks generated different networks simulated real data left right ground truth depth ground truth masks masks predicted se-pose-net se-pose-net joint angles se-net se-net joint angles. table average per-point across tasks networks normalized number points move ground truth data network achieves results slightly worse baseline networks simulated real data. however also solving additional tasks necessary control. se-pose-nets joint angles proposed network joint angles robot given additional input encoder. network strong baseline uses signiﬁcant additional information inform pose prediction. se-nets prior work network directly predicts masks change poses given input point clouds control. explicit pose space network control full point cloud observation space network. implemented networks pytorch using adam optimizer training learning rate networks used batch normalization prelu nonlinearity maximum number moving objects experiments train network iterations simulation iterations real data network achieves least validation loss across training iterations results. across baselines simulated real data. se-nets achieve best results simulated real datasets baseline network performs slightly worse. unsurprisingly networks access joint angles better strictly information highly correlated sensor data. initial surprise se-pose-nets largest prediction errors among baseline models. however makes sense given following considerations sepose-nets trained explicitly embed observations pose space predict scene dynamics rather using input point cloud directly. provides structure within network necessary control task also restricts prediction information bottleneck generally makes training problem harder. se-pose-nets additionally optimize consistency loss enforces constraints different prediction problem evaluated experiment. fig. visualizes masks predicted se-pose-nets baseline se-net example simulated real data along ground truth masks. even without supervision se-pose-nets senets learn detailed segmentation multiple salient parts consistent ground truth segments simulated real data. next test performance different networks controlling baxter’s right reach target conﬁguration speciﬁed point cloud test control algorithms presented sec. using sepose-net model baseline models comparing performance distinct servoing tasks ﬁrst detail speciﬁcs followed analysis results. fig. convergence joint angle error simulated baxter control tasks. without joint angles without joint angles detected failure case removed joint angles. se-pose-nets perform well better baseline methods even though baseline models additional information form ground truth-associations. fig. convergence joint angle error real baxter control tasks without joint angles joint angles space observations thus require external data associations observation space able control all. simulation experiments provide baseline algorithms ground-truth associations procedure outlined alg. using predicted point cloud ˆxt+ target error minimized generating controls. important keep mind baseline models advantage se-pose-nets control task strictly information form ground-truth data associations. metric task speciﬁcation mean absolute error joint angles metric measuring control performance. models convergence maximum iterations. additionally se-pose-nets terminate pose error increases consecutive iterations. integrate joint velocities forward generate position commands robot simulation real world. simulation results fig. plots error joint angles function number control iterations. plots left middle show results networks depth input control ﬁrst joints robot using networks. right ﬁgure shows results networks additionally joint angles input control joints robot networks. general sepose-nets achieve excellent performance compared baseline models converging quickly almost zero error even absence external data associations. model performs comparably se-pose-nets se-nets converge slower. highlight results methods gauss-newton based optimization leads faster convergence backprop. expected gauss-newton conditions gradient based pseudo-second order information. baseline models perform worse given joint angles without. issue credit assignment gradient computation networks learn erroneous causations input joint angles predicted ﬂows diminishes control’s contribution prediction problem subsequently affects gradient. models struggle model motion ﬁnal wrist joint increasing correlations along kinematic chain result small contribution joint’s motion full movement wrist. se-pose-nets overcome problem given input joint angles provides encouraging proof adding joint state supplements information hard parse directly visual state. se-nets converge slowly lack good control initializations needed ensure network starts meaningful segmentation given zero controls se-net choose segment ﬁnally good performance se-posenets indicates learned pose space consistent across large motions used fast reactive control albeit quite robust baseline methods given data associations. se-pose-nets fail minimize pose error tested conﬁgurations leading increasing error fig. left. termination check looks increasing pose errors correctly identify case able succeed examples discuss ways improve robustness approach sec. real robot results test control performance using se-pose-nets real world examples. compare baselines need explicit external data association system feasible. real robot restrict controlling ﬁrst four joints right using se-pose-net control ﬁrst joints using model additionally takes joint angles input. fig. shows errors function iteration count. models converge quickly indicates network able control robustly even presence sensor noise unmodeled dynamics. surprisingly little difference backprop algorithms real data. video showing realtime control results baxter found here. low-dimensional pose space control. leads signiﬁcant speedups compared baselines se-nets operate around se-pose-nets realtime including pose detection part. paper presents se-pose-nets framework learning predictive models enable control objects scene. context robot manipulator showed solve problem learning predictive model individual parts manipulator prior work additionally se-pose-nets learn consistent pose space parts essentially learning detect poses manipulator parts depth images. detection capability enables se-pose-nets solve data association problem crucial relating current observation manipulator desired target observation. difference poses used generate control signals move manipulator target pose similar visual servoing applied image manipulator. also showed learned network used determine gradients needed control signals. experiments show se-pose-nets generate control superior representations learned previous techniques even provided external data associations. furthermore addition providing data associations se-pose-nets allow compute controls directly dimensional pose space enabling efﬁcient control techniques operate perception space. crucially abilities learned single framework based data traces solely annotated frame-to-frame point cloud correspondences. overall control performance shown se-posenets extremely encouraging provides strong proof concept networks learn consistent pose space provides long-range correspondences fast reactive control. provides reason rejoice multiple areas improvement shown real robot results se-pose-nets difﬁculties handling joints kinematic chain whose motions signiﬁcantly correlated motions joints above. additionally end-effector poor visibility depth images. adding state information form encoder data signiﬁcantly alleviates issue fully solve potentially multiple ways improve model tackle problem including curriculum active learning along better regularization physical grounding pose space remove inconsistencies. area future work extending system interact manipulate external objects. here consistent pose space objects scene enable robot plan motion toward objects enabling smooth interactions. finally shown se-pose-nets used single-step reactive control would like long-term planning using model based techniques iterative leverage work funded part national science foundation contract number nsf-nri- sttr number awarded lula robotics. would also like thank nvidia generously providing used research nvidia jonschkowski hafner scholz riedmiller pves position-velocity encoders unsupervised learning structured state representations arxiv preprint arxiv.", "year": 2017}