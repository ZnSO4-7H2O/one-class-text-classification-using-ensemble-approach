{"title": "Clustering is Efficient for Approximate Maximum Inner Product Search", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Efficient Maximum Inner Product Search (MIPS) is an important task that has a wide applicability in recommendation systems and classification with a large number of classes. Solutions based on locality-sensitive hashing (LSH) as well as tree-based solutions have been investigated in the recent literature, to perform approximate MIPS in sublinear time. In this paper, we compare these to another extremely simple approach for solving approximate MIPS, based on variants of the k-means clustering algorithm. Specifically, we propose to train a spherical k-means, after having reduced the MIPS problem to a Maximum Cosine Similarity Search (MCSS). Experiments on two standard recommendation system benchmarks as well as on large vocabulary word embeddings, show that this simple approach yields much higher speedups, for the same retrieval precision, than current state-of-the-art hashing-based and tree-based methods. This simple method also yields more robust retrievals when the query is corrupted by noise.", "text": "efﬁcient maximum inner product search important task wide applicability recommendation systems classiﬁcation large number classes. solutions based locality-sensitive hashing well tree-based solutions investigated recent literature perform approximate mips sublinear time. paper compare another extremely simple approach solving approximate mips based variants k-means clustering algorithm. speciﬁcally propose train spherical kmeans reduced mips problem maximum cosine similarity search experiments standard recommendation system benchmarks well large vocabulary word embeddings show simple approach yields much higher speedups retrieval precision current state-of-the-art hashing-based tree-based methods. simple method also yields robust retrievals query corrupted noise. maximum inner product search problem recently received increased attention arises naturally many large scale tasks. recommendation systems users items recommended represented vectors learnt training time based user-item rating matrix. test time model deployed suggesting recommendations given user vector model perform product user vector item vectors pick items maximum product recommend. millions candidate items recommend usually possible full linear search within available time frame milliseconds. problem amounts solving kmips problem. another common instance k-mips problem arises extreme classiﬁcation tasks huge number classes. inference time predicting top-k likely class labels given data point cast k-mips problem. extreme classiﬁcation problems occur often natural language processing tasks classes words predetermined vocabulary. example neural probabilistic language models probabilities next word given context previous words computed last layer network multiplication last hidden layer representation large matrix many columns words vocabulary. column seen corresponding embedding vocabulary word hidden layer space. thus inner product taken hidden representation yield inner product score vocabulary word. passed softmax nonlinearity yield predicted probabilities possible words. ranking probability values unaffected softmax layer ﬁnding probable words many cases retrieved result need exact sufﬁcient obtain subset vectors whose inner product query high thus highly likely contain exact k-mips vectors. examples motivate research approximate k-mips algorithms. obtain large speedups full linear search without sacriﬁcing much precision direct impact large-scale applications. formally k-mips problem stated follows given points query vector argmax notation corresponds indices providing maximum values. problem solved exactly linear time calculating selecting maximum items method costly used large applications typically hundreds thousands entries set. methods discussed article based notion candidate i.e. subset dataset return exact k-mips making computation much faster. guarantee candidate contains target elements therefore methods solve approximate k-mips. better algorithms provide candidate sets smaller larger intersections actual maximum inner product vectors. mips related nearest neighbor search maximum similarity search. considered harder problem inner product neither satisﬁes triangular inequality distances usually satisfy basic property similarity functions namely similarity entry least large similarity anything else vector guarantee thus cannot directly apply efﬁcient nearest neighbor search maximum similarity search algorithms mips problem. given points query vector k-nns problem euclidean distance deﬁned k-nns k-mcss different problems k-mips easy three become equivalent provided data vectors euclidean norm. several approaches mips make observation ﬁrst transform mips problem mcss problem. paper propose empirically investigate simple approach approximate k-mips problem. consists ﬁrst reducing problem approximate k-mcss problem perform spherical k-means clustering. clusters whose centers best match query yield candidate set. rest paper organized follows section review previously proposed approaches mips. section describes proposed simple solution k-means mips details section discusses ways improve performance using hierarchical k-means version. section empirically compare methods state-of-the-art tree-based hashing-based approaches standard collaborative ﬁltering benchmarks larger word embedding datasets. section concludes paper discussion future work. common types solution mips literature tree-based methods hashingbased methods. tree-based methods data dependent hash-based methods mostly data independent. tree-based approaches maximum inner product search problem ﬁrst formalized gray provided tree-based solution problem. speciﬁcally constructed ball tree vectors database bounded maximum inner product ball. novel analytical upper bound maximum inner product given point points ball made possible design branch bound algorithm solve mips using constructed ball tree. gray also proposes dual-tree based search using cone trees batch queries. issue ball-tree based approach partitions data points based euclidean distance problem hasn’t effectively converted nns. contrast pca-tree current state-of-the-art tree-based approach mips ﬁrst converts mips appending additional component vector ensures vectors constant norm. followed balanced kd-tree style tree construction. hashing based approaches shrivastava ﬁrst work propose explicit asymmetric locality sensitive hashing construction perform mips. converted mips used l-lsh algorithm subsequently shrivastava proposed another construction convert mips mcss used signed random projection hashing method. works based assumption symmetriclsh family exist mips problem. later neyshabur srebro showed explicit construction symmetric-lsh algorithm mips better performance previous alsh algorithms. finally vijayanarasimhan propose winner-takehashing pick top-k classes consider training inference large classiﬁcation problems. hierarchical softmax notable approach address problem scaling classiﬁers huge number classes hierarchical softmax based prior clustering words binary generally n-ary tree serves ﬁxed structure learning process model. complexity training reduced clustering tree structure resembles mips techniques explore paper. however approaches differ fundamental level. hierarchical softmax deﬁnes probability leaf node product probabilities computed intermediate softmaxes leaf node. contrast approximate mips search imposes constraining structure probabilistic model better though efﬁciently searching winners amounts large ordinary softmax. follow previous work shrivastava reducing mips problem mcss problem ingeniously rescaling vectors adding components making norms vectors approximately same. dataset. parameters algorithm. ﬁrst step scale vectors dataset factor maxi ||xi|| apply mappings data points another query vector. mappings simply concatenate components vectors making norms data points roughly same. mappings deﬁned follows mcss using spherical k-means assuming data points transformed scaled norm approximately spherical k-means algorithm efﬁciently used approximate mcss. algorithm formal speciﬁcation spherical k-means algorithm denote centroid cluster index cluster assigned point difference standard k-means clustering spherical k-means spherical variant data points clustered according position euclidian space according direction. vector maximum cosine similarity query point dataset clustered method ﬁrst cluster whose centroid best cosine similarity query vector i.e. maximal consider points belonging cluster candidate set. simply take argmaxj|aj approximation maximum cosine similarity vector. method extended ﬁnding maximum cosine similarity vectors compute cosine similarity query vectors candidate take best matches. issue constructing candidate single cluster quality poor points close boundaries clusters. alleviate problem increase size candidate sets constructing instead top-p best matching clusters construct candidate set. note approximate search methods exploit similar ideas. example bachrach proposes so-called neighborhood boosting method pca-tree considering path leaf binary vector given target leaf consider leaves hamming distance away. using single-level clustering data points might yield sufﬁciently fast search procedure moderately large databases insufﬁcient much larger collections. clusters cluster contains indeed points clustering dataset approximately single closest cluster candidate candidate size order mentioned earlier typically want consider three closest clusters candidate order limit problems arising query points close boundary clusters approximate k-mips fairly consequence increasing candidate sets quickly grow wastefully containing many unwanted items. restrict candidate sets smaller count better targeted items would need smaller clusters search best matching clusters becomes expensive part. address situation propose approach cluster dataset many small clusters cluster small clusters bigger clusters number times. approach thus bottom-up clustering approach. example cluster datasets ﬁrst-level small clusters cluster centroids ﬁrst-level clusters second-level clusters making data structure twolayer hierarchical clustering. approach generalized many levels clustering necessary. figure walk hierarchical clustering tree level candidate next level. ﬁrst level dashed boxed represent best matches gives candidate second level etc. search small clusters best match query point constitute good candidate hierarchy keeping level best matching clusters. process illustrated figure since levels clusters much smaller size take much larger values example formally levels clustering indices clusters level conveniently assignment deﬁned data points themselves clusters layer candidate found using method described centroids algorithm candidate obtained algorithm. approach bottom-up clustering i.e. ﬁrst cluster dataset small clusters cluster small cluster bigger clusters level cluster. approaches suggested method employed top-down clustering strategy level points assigned current cluster divided smaller clusters. approach also addresses problem using single lowest-level cluster candidate inaccurate solution data points multiple clusters. alternative solution consists exploring several branches clustering hierarchy parallel. section evaluate proposed algorithm approximate mips. speciﬁcally analyze following characteristics speedup compared exact full linear search retrieving top-k items largest inner product robustness retrieved results noise query. used collaborative ﬁltering datasets word embedding dataset descibed below movielens-m collaborative ﬁltering dataset movies users. given user-item matrix follow puresvd procedure described generate user movie vectors. speciﬁcally subtracted average rating user individual ratings considered unobserved entries zeros. compute approximation singular components used vector representation user vector representation movie. construct database movies consider randomly selected users queries. netﬂix another standard collaborative ﬁltering dataset movies users. follow procedure described movielens construct dimensional vector representations standard literature consider randomly selected users queries. wordvec embeddings -dimensional wordvec embeddings released mikolov construct database composed ﬁrst word embedding vectors. consider types queries randomly selected word vectors database randomly selected word vectors database corrupted gaussian noise. acts test bench evaluate performance different algorithms based characteristics queries. consider following baselines compare with. pca-tree pca-tree state-of-the-art tree-based method shown superior ip-tree method ﬁrst converts mips appending additional component vectors make constant norm. principal directions learnt data projected using principal directions. finally balanced tree constructed using splitting criteria level median component values along corresponding principal direction. level uses different principal direction decreasing order variance. srp-hash signed random projection hashing method mips proposed shrivastava srp-hash converts mips mcss vector augmentation. consider hash functions hash function considers random projections vector compute hash. wta-hash winner takes hashing another hashing-based baseline also converts mips mcss vector augmentation. consider hash functions hash function different random permutations vector. preﬁx constituted ﬁrst elements permuted vector used construct hash vector. ﬁrst experiments consider collaborative ﬁltering tasks evaluate speedup provided different approximate k-mips algorithms compared exact full search. note section include hierarchical version k-means experiments databases small enough k-means perform well. exact linear search algorithm consists computing inner product training items. want compare preformance algorithms rather specifically optimized implementations approximate time number product operations computed algorithm. words unit time time taken product. algorithms return candidates exact linear seacrh. induces number products least large size identiﬁed candidate set. addition candidate size following operations count towards count products k-means products done cluster centroids involved ﬁnding top-p clusters search. pca-tree product done project query space. note tree depth need products project query. srp-hash total number random projections data hashes random projections each cost wta-hash full random permutation vector involves number query element access operations single product. however consider preﬁxes permutations means need fraction product. product involves accessing components vector permutation wta-hash needs access elements vector. consider cost fraction cost product. speciﬁcally hash functions random permutations consider preﬁxes length total cost would dimension vector. call true top-k actual elements database largest inner products query. call retrieved top-k elements among candidate retrieved speciﬁc approximate mips largest inner products query. deﬁne precision k-mips number elements intersection true top-k retrived top-k vectors divided varied hyper-parameters algorithm computed precision speedup case. resulting precision v.s. speedup curves obtained movielens-m netﬂix datasets reported figure make following observations results pca-tree performs better srp-hash. wta-hash performs better pca-tree lower speedups. however performance degrades faster speedup increases pca-tree outperforms wta-hash higer speedups. k-means clear winner speed increases. also performance k-means degrades slowly increase speedup compared rapid decrease performance algorithms. experiment consider word embedding retrieval task. ﬁrst experiment consider using query embeddings corresponding subset large database pretrained embeddings. note query thus present database guaranteed correspond top- mips result. also we’ll interested top- top- mips performance. algorithms perform better top- top- mips queries already belong database preserve neighborhood data points better. figure shows precision speedup curve top- top- top- mips. results data dependent algorithms better preserve neighborhood compared data independent algorithms surprising. however k-means hierarchical k-means performs signiﬁcantly better pca-tree top- top- mips suggesting better pca-tree capturing neighborhood. reason might k-means global view vector every step pca-tree considers dimension time. figure speedup results collaborative ﬁltering. correspond precision mips movielens-m dataset correspond precision mips netﬂix dataset respectively. k-means means k-means algorithm considers clusters candidate set. figure speedup results word embedding retrieval. correspond precision mips respectively. k-means means k-means algorithm considers clusters candidate set. hier-k-meanss means level hierarchical k-means algorithm considers clusters candidate set. speedup algorithms. take random word embeddings database corrupt random gaussian noise. vary scale noise plot performance. figure shows performance various algorithms top- top- top- mips problems noise increases. k-means always performs better algorithms even increase noise. also performance kmeans remains reasonable compared algorithms. results suggest approach might particularly appropriate scenario word embeddings simultaneously trained thus ﬁxed. scenario robust mips method would allow update mips model less frequently. paper proposed efﬁcient solving approximate k-mips based simple clustering strategy showed good alternative popular tree-based techniques. regard simplicity approach strengths. empirical results three real-world datasets show simple approach clearly outperforms families techniques. achieves larger speedup maintaining precision robust input corruption important property generalization query test points expected exactly equal training data points. clustering mips generalizes better related unseen data hashing approaches evaluated. future work plan research ways adapt on-the-ﬂy clustering approximate kmips input representation evolves learning model leverage efﬁcient k-mips speed extreme classiﬁer training improve precision speedup combining multiple clusterings. finally mention that putting ﬁnal touches paper another recent different mips approach based vector quantization came knowledge highlight ﬁrst arxiv post work predates work. nevertheless time empirically compare approach here hope future work. authors would like thank developers theano developing powerful tool. acknowledge support following organizations research funding computing support samsung nserc calcul quebec compute canada canada research chairs cifar. yoram bachrach yehuda finkelstein gilad-bachrach liran katzir noam koenigstein nice ulrich paquet. speeding xbox recommender system using euclidean transformation inner-product spaces. proceedings conference recommender systems recsys pages james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano proceedings python scientiﬁc computing conference math expression compiler. june paolo cremonesi yehuda koren roberto turrin. performance recommender algorithms top-n recommendation tasks. proceedings fourth conference recommender systems recsys pages mayur datar nicole immorlica piotr indyk vahab mirrokni. locality-sensitive hashing scheme based p-stable distributions. proceedings twentieth annual symposium computational geometry pages noam koenigstein parikshit yuval shavitt. efﬁcient retrieval recommendations proceedings international conference matrix factorization framework. information knowledge management cikm pages york acm. tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. c.j.c. burges bottou welling ghahramani k.q. weinberger editors advances neural information processing systems pages curran associates inc. frederic morin yoshua bengio. hierarchical probabilistic neural network language model. robert cowell zoubin ghahramani editors proceedings tenth international workshop artiﬁcial intelligence statistics pages parikshit alexander gray. maximum inner-product search using cone trees. proceedings sigkdd international conference knowledge discovery data mining pages anshumali shrivastava ping asymmetric sublinear time maximum inner product search advances neural information processing systems annual conference neural information processing systems december montreal quebec canada pages zhong. efﬁcient online spherical k-means clustering. neural networks ijcnn’. proceedings. ieee international joint conference volume pages ieee", "year": 2015}