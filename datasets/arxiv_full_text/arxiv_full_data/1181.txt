{"title": "Scale-Invariant Convolutional Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Even though convolutional neural networks (CNN) has achieved near-human performance in various computer vision tasks, its ability to tolerate scale variations is limited. The popular practise is making the model bigger first, and then train it with data augmentation using extensive scale-jittering. In this paper, we propose a scaleinvariant convolutional neural network (SiCNN), a modeldesigned to incorporate multi-scale feature exaction and classification into the network structure. SiCNN uses a multi-column architecture, with each column focusing on a particular scale. Unlike previous multi-column strategies, these columns share the same set of filter parameters by a scale transformation among them. This design deals with scale variation without blowing up the model size. Experimental results show that SiCNN detects features at various scales, and the classification result exhibits strong robustness against object scale variations.", "text": "even though convolutional neural networks achieved near-human performance various computer vision tasks ability tolerate scale variations limited. popular practise making model bigger ﬁrst train data augmentation using extensive scale-jittering. paper propose scaleinvariant convolutional neural network model designed incorporate multi-scale feature exaction classiﬁcation network structure. sicnn uses multi-column architecture column focusing particular scale. unlike previous multi-column strategies columns share ﬁlter parameters scale transformation among them. design deals scale variation without blowing model size. experimental results show sicnn detects features various scales classiﬁcation result exhibits strong robustness object scale variations. many classical computer vision tasks enjoyed great breakthrough primarily large amount training data application deep convolution neural networks recent ilsvrc competition cnn-based solutions achieved nearhuman accuracies image classiﬁcation localization detection tasks accompanying progress studies trying understand learnt internally contribute success design layers within network progressively larger receptive ﬁeld sizes allowing learn complex features. another point shift-invariance property pattern input recognized regardless position pooling layers contribute resilience slight deformation well small however evident deals shift-variance better scale-invariance dealing scaleinvariance well poses direct conﬂict design philosophy higher layers thus captures features certain plain patterns simply larger input complex. words alignments position ﬁlter complexity captures. more invariance deal internally. examples include rotations ﬂips brutal force solution would make network larger introducing ﬁlters cope scale variations feature accompanied scale-jittering input images often order magnitude. fact popular practice today true even proposals directly deal problem. example drives crops different size positions three differnt scales uses vlad pooling produce feature summary patches. explore radically different approach also simple. observing ﬁlters detect pattern different scales bear strong relationship adopt multi-column design designate column specialize certain scales. call system sicnn unlike conventional multi-column ﬁlters sicnn strongly regulated among columns. goal make network resilient scale variance without blowing number free parameters thus reduce need jittering input. performed detailed analysis veriﬁed sicnn exhibits desired behavior. example column deals larger scale indeed activated input patterns larger scaling factor system whole becomes less sensitive scale variance. unaugmented cifar- dataset method produces best result among previous works using single simple softmax classiﬁer complementary techniques improve performance. model increases training cost linear number columns incremental reﬁnement dramatically reduce cost without compromising performance signiﬁcantly. rest paper organized follows. section presents sicnn covering high-level intuition mathematical foundation architecture. detailed analysis results presented section conclude section consider case classifying objects canonical scale free parameter positions. stack convolution ﬁlters progressively build complex hidden representations. hidden representations invariant shift meaning activations preserve pattern except words conv shifted. shift) arbitrary image ﬁlter relationship upheld layer layer. makes classiﬁer easy. existing architecture dealing multiple scales jointly achieved pooling layers convolution layers. convolution layer needs learn different features also scaled variants multiple feature maps. units pairing pooling layer generate scale-invariance within receptive ﬁelds help save feature maps. multi-scale solution leads bigger model since ﬁlters independently learned need training data. popular practice scale-jittering idea simple inspired invariance-byshift property existing convolution layer. like convolve ﬁlter different positions also convolve ﬁlter different scales. done adding independent columns conventional specialized detecting scale. crucially columns strongly regularized number free parameters convolution layers stay same. thus inject scale-invariance model requiring neither additional data augmentation increasing model size. sicnn uses multiple columns convolutional stack varying ﬁlter size capture objects unknown scales input images. architecture illustrated figure bottom input image columns. column several convolutional layers max-pooling. difference conventional multi-column that although columns different ﬁlter size still share common parameters among ﬁlters. canonical column keeps canonical ﬁlters layer. columns call scale columns transform canonical ﬁlters ﬁlter. collectively canonical ﬁlter transformed ﬁlters detect pattern different scales multiple columns simultaneously. therefore single pattern different scales trigger columns. consider canonical ﬁlter detects pattern image convolution conv image scaled scaling operation expect another column transformed ﬁlter capture pattern instead. thus column generates another convolution result conv invariance-by-shift require convolution equivalent scaling convolution result canonical column. highest responses. otherwise object scale falls scales neighboring columns columns relatively high responses. concatenated feature vector makes possible classiﬁer linear combinations responses multiple columns eliminate variance. filter transformation vector representation image scaling convolution linear transformations. given canonical ﬁlter represented vector solve following equation derived equation transformed ﬁlter equation system linear equations however system doesn’t always valid solution many constraints address problem reduce size makes conv produce single number. then equation turns solve equation obtain however practice can’t always obtain exact unique square invertible matrix. scaling-up matrix equation inﬁnite number solutions; scaling-down matrix equation exact solutions. reason choose minimum-norm solution similar applying weight decay weights i.e. reduce over-ﬁtting. ﬁlter likely generalization various cases. easy solution generalized inverse second case exact solution problem different angle take scaled image input image proximate original scaling ˜s˜i. scaling-up matrix reverted direction turn equation discussion ﬁrst layer. however easy ﬁlter transformation layer satisﬁes equation invariance-by-scaling property preserved layer layer till reaching classiﬁcation layer. figure input images different scale canonical column column separately generate respectively. following relu nonlinear activation function maxpooling keep scale relationship result recursively applying equation know layers columns also keep scale relationship canonical column generates input image column generates image baseline close cifar- dataset alex network layers convolution. layer uses receptive size stride pooling receptive size stride followed local normalization. ﬁrst convolution paired pooling whereas latter followed average pooling. sicnn extends columns. ﬁrst three columns ﬁlter size last three columns ﬂipped versions ﬁrst three. weights regularized tied column non-ﬂipped column. train models standard cifar- training method similar hyper-parameters ﬁrst train whole epochs reduce learning rate factor ten. train epochs tune learning rate again train another epochs ﬁnal result. exploit invariance property model need generate test dataset mixture different scales. crop central cifar- images resize mixed dataset different scales small middle large. refer dataset scaled cifar- later section. experiment results best viewed electronic consider arbitrary image scaled version applying ﬁlter transformation corresponding activations become conv conv respectively. described section achieve scale-invariant pattern matching expect former scaling indistinguishable latter i.e. compare three different kinds ﬁlter transformation methods. transi identity transformation apply original ﬁlter onto scaled image transt ﬁlter transformation described section transs comparison method directly simple image sampling scale ﬁlter. also implementation bicubic interpolation scaling method transform ﬁlters. method produce nice scaling results without losing much information original image. model also consider special scaling operation horizontal ﬂipping. columns ﬂipped ﬁlters capture ﬂipped patterns input. scaling matrix ﬂipping symmetric invertible matrix. easy solve equation training multiple columns integrate columns tied ﬁlters single model train together back-propagation algorithm. observing equation cases transformation canonical ﬁlter scale always linear transformation. means ﬁlters columns tied canonical ﬁlter matrix multiplication training ﬁrst back-propagation column independently. then derivatives ﬁlters distributed columns transformed gathered canonical ﬁlters’ derivatives. canonical ﬁlters updated aggregated derivatives ﬁlters scaled columns recomputed ﬁlter transformation canonical ﬁlters. table take random images test cifar- take averaged result. three canonical ﬁlters size considered random ﬁlter ﬁlters learnt baseline model ﬁlters learnt sicnn comparison non-overlapped pooling usually considered powerful scale-invariance taken non-parametric ﬁlter applied image scaled table scaling-up doubles image size accordingly transt transs scale ﬁlter size table scaling-down. image size halved ﬁlter size scaled table clear convolution ﬁlter without transformation sensitive image scale ﬁlter transformation method sampling-based method much robust image scale. transt almost always better simple-minded transs especially image scaled need precise ﬁlter small size. considering fact transs hard back-propagation normalization method becomes apparent choice transforming ﬁlters. also image scaled ﬁlter transformation even better figure visualization activations layer. examples left column shows original image size activations layer using ﬁlter; middle right columns show enlarged image size activations layer using transformed ﬁlter ﬁlter respectively. ﬁlter chosen randomly sicnn model trained. pooling. considering pooling doesn’t need detect patterns it’s interesting method achieves robust invariance-by-scaling. image scaled down method still comparable pooling. random ﬁlter sicnn trained ﬁlters adapt speciﬁc scale more. consequently invarianceby-scaling ﬁlters also gets worse expected. give concrete feeling approach inspect feature maps generated images different scales. fig. shows particular examples visualizing feature three convolution layers ﬁnal result pooling normalization. example left column activations original image columns scaled image left middle column results using ﬁlter size transformed ﬁlter. feature maps scaled size ease comparison. relatively small difference layer clear applying transformed ﬁlters scaled image preserves essential characteristics original. rightmost column result applying original ﬁlter scaled image. clear ﬁxed ﬁlter generates activations diverge original image signiﬁcantly. table classiﬁcation error rate tested standard cifar- scaled cifar-. last shows classiﬁcation improvement sicnn. last column shows performance drop test dataset scales. table compare results baseline sicnn trained standard cifar- dataset. sicnn achieves statistically signiﬁcant gain standard cifar-. full advantage apparent scaled cifar- performance drop sicnn drops manually examine error cases simple central-crop-resize many signiﬁcant features scaled cifar-. speculate sicnn work better higher quality multi-scale datasets. verify hypothesis pick random images object center scale different sizes; largest central area image resized images sicnn compare probability correct class. results shown fig. seen scale image goes performance drops whereas sicnn stable. exception among samples horse; scaled versions start lose vital features. table compare classiﬁcation error rate sicnn previous approaches cifar- achieve error rate unaugmented data improvement absolute gain baseline sicnn also exceeds improvement dropout spearmint insufﬁcient catch maxout networkrandomly pick ﬁlter last layer visualize images cause largest outputs ﬁlter columns individually method similar used seen column model focuses particular scale orientation images causes largest activations larger left right automobiles rows face opposite directions. addition visual inspection quantify sensitive ﬁlters different columns scales. take images feature given column gets activated most break according scale belong data small middle large. statics reported bottom images fig. clear columns small ﬁlters picks small-scale images more whereas columns larger ﬁlters opposite. object recognition scaled small large columns sicnn also work turn capture object. illustrate figure using method similar fig. ﬁrst select feature detects dogs last layer. pick image cifar- scale object different sizes activation value feature plotted function object size scale column. figure clear object small column ﬁlter ﬁrst captures gives response. object gets larger activations -ﬁlter column drop. columns gradually reach peak responses turn. peaks three columns locate interval along object size equal-interval ﬁlter sizes comparing activation values among different columns meaningless eventually summed different weights classiﬁcation. however study clearly shows tracing column activated more detect object well scale. figure visualization ﬁlter respect columns. column images causes largest activation ﬁlter shown. breakdown analysis images shown column scale in-network current state art. nevertheless method combined techniques sicnn addressing scale-invariance problem different goal others. example using average voting sicnn drop error rate maxout model moreover simply adding extra ﬂipped column maxout model reach error rate single -column maxout-sicnn model. results encouraging expect sicnn work better benchmarks exhibit higher scale variations results section suggest. replicating sicnn larger complex dataset imagenet ongoing work. table classiﬁcation error rate cost incremental training. last rows inc- inc- correspond incremental training methods. training costs normalized cnn’s cost. small value. improving scale invariance come free. current conﬁguration training costs increase linearly number columns. clearly ﬁrst train single column transform ﬁlters columns ﬁnally reﬁne model. ideal setting reasonable expect additional training cost insigniﬁcant. explored incremental training methods. ﬁrst choice ﬁrst train baseline half epochs full training build -column sicnn based current ﬁlters. begin reﬁne entire model left half epochs. second choice continue fully trained baseline ﬁlters build column sicnn. then ﬁlter parameters frozen reﬁne parameters classiﬁer. single softmax layer classiﬁer inc- method small extra cost. results incremental learning summarized table compared sicnn trained scratch inc- training takes nearly half cost model achieving comparable performance. inc- training although extra training cost small still model better performance baseline cnn. also combination maxout units we’re able reach error rate higher previous result. incremental learning help balance performance gain training efﬁciency sicnn. paper propose generalization sicnn incorporate scale invariance model. model improves results traditional complements optimization techniques. results clearly indicate model learns feature different scales different columns. idea generalizable applied aspects employed including supervised unsupervised learning recognition detection localization tasks. preliminary study also implies nice trade-off performance training cost. several open problems remain. example different summarizing columns different connectivity structure among column plan apply sicnn larger complex datasets imagenet.", "year": 2014}