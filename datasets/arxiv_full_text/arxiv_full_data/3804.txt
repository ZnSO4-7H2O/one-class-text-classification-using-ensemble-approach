{"title": "Speech Recognition with Deep Recurrent Neural Networks", "tag": ["cs.NE", "cs.CL"], "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates \\emph{deep recurrent neural networks}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.", "text": "recurrent neural networks powerful model sequential data. end-to-end training methods connectionist temporal classiﬁcation make possible train rnns sequence labelling problems input-output alignment unknown. combination methods long short-term memory architecture proved particularly fruitful delivering state-of-the-art results cursive handwriting recognition. however performance speech recognition disappointing better results returned deep feedforward networks. paper investigates deep recurrent neural networks combine multiple levels representation proved effective deep networks ﬂexible long range context empowers rnns. trained end-to-end suitable regularisation deep long short-term memory rnns achieve test error timit phoneme recognition benchmark knowledge best recorded score. neural networks long history speech recognition usually combination hidden markov models gained attention recent years dramatic improvements acoustic modelling yielded deep feedforward networks given speech inherently dynamic process seems natural consider recurrent neural networks alternative model. hmm-rnn systems also seen recent revival currently perform well deep networks. instead combining rnns hmms possible train rnns ‘end-to-end’ speech recognition approach exploits larger state-space richer dynamics rnns compared hmms avoids problem using potentially incorrect alignments training targets. combination long short-term memory architecture improved memory end-to-end training proved especially effective cursive handwriting recognition however made little impact speech recognition. rnns inherently deep time since hidden state function previous hidden states. question inspired paper whether rnns could also beneﬁt depth space; stacking multiple recurrent hidden layers other feedforward layers stacked conventional deep networks. answer question introduce deep long short-term memory rnns assess potential speech recognition. also present enhancement recently introduced end-to-end learning method jointly trains separate rnns acoustic linguistic models sections describe network architectures training methods section provides experimental results concluding remarks given section terms denote weight matrices terms denote bias vectors hidden layer function. usually elementwise application sigmoid function. however found long short-term memory architecture uses purpose-built memory cells store information better ﬁnding exploiting long range context. fig. illustrates single lstm memory cell. version lstm used paper implemented following composite function crucial element recent success hybrid hmmneural network systems deep architectures able build progressively higher level representations acoustic data. deep rnns created stacking multiple hidden layers other output sequence layer forming input sequence next. assuming hidden layer function used layers stack hidden vector sequences iteratively computed deep bidirectional rnns implemented replacing hidden sequence forward backward sequences ensuring every hidden layer receives input forward backward layers level below. lstm used hidden layers deep bidirectional lstm main architecture used paper. aware ﬁrst time deep lstm applied speech recognition yields dramatic improvement single-layer lstm. focus end-to-end training rnns learn directly acoustic phonetic sequences. advantage approach removes need predeﬁned alignment create training targets. ﬁrst step network outputs parameterise differentiable distribution possible phonetic output sequences given acoustic input sequence log-probability target output sequence differentiated respect network weights using backpropagation time whole system optimised gradient descent. describe ways deﬁne output distribution hence train network. refer throughout length length number possible phonemes ﬁrst method known connectionist temporal classiﬁcation uses softmax layer deﬁne separate output distribution every step along input sequence. distribution covers phonemes plus extra blank symbol represents non-output intuitively network decides whether emit label label every timestep. taken together decisions deﬁne distribution alignments input target sequences. uses forward-backward algorithm cell activation vectors size hidden vector weight matrices cell gate vectors diagonal element gate vector receives input element cell vector. shortcoming conventional rnns able make previous context. speech recognition whole utterances transcribed once reason exploit future context well. bidirectional rnns processing data directions separate hidden layers forwards output layer. illustrated fig. brnn computes forward hidden sequence backward hidden sequence output sequence iterating backward layer forward layer updating output layer uppermost forward backward hidden sequences network hidden sequence prediction network. output network implemented feeding linear layer generate vector feeding tanh hidden layer yield ﬁnally feeding size softmax layer determine transducers trained random initial weights. however appear work better initialised weights pretrained network pretrained next-step prediction network output layers used networks pretraining removed retraining. work pretrain prediction network phonetic transcriptions audio training data; however large-scale applications would make sense pretrain separate text corpus. transducers decoded beam search yield n-best list candidate transcriptions. past networks decoded using either form bestﬁrst decoding known preﬁx search simply taking active output every timestep work however exploit beam search transducer modiﬁcation output label probabilities depend previous outputs pr). beam search faster effective preﬁx search ctc. note n-best list transducer originally sorted length normalised log-probabilty pr/|y|; current work dispense normalisation sort regularisation vital good performance rnns ﬂexibility makes prone overﬁtting. regularisers used paper early stopping weight noise weight noise added training sequence rather every timestep. weight noise possible alignments determine normalised probability target sequence given input sequence similar procedures used elsewhere speech handwriting recognition integrate possible segmentations however differs ignores segmentation altogether sums single-timestep label decisions instead. deﬁnes distribution phoneme sequences depends acoustic input sequence therefore acoustic-only model. recent augmentation known transducer combines ctc-like network separate predicts phoneme given previous ones thereby yielding jointly trained acoustic language model. joint lm-acoustic training proved beneﬁcial past speech recognition whereas determines output distribution every input timestep transducer determines separate distribution every combination input timestep output timestep distribution covers phonemes plus intuitively network ‘decides’ output depending input sequence outputs already emitted. length target sequence complete decisions jointly determines distribution possible alignments integrated forward-backward algorithm determine original formulation deﬁned taking ‘acoustic’ distribution network ‘linguistic’ distribution prediction network multiplying together renormalising. improvement introduced paper instead feed hidden activations networks separate feedforward output network whose outputs normalised softmax function yield allows richer possibilities combining linguistic acoustic information appears lead better generalisation. particular found number deletion errors encountered decoding reduced. phoneme recognition experiments performed timit corpus standard speaker records removed used training separate development speakers used early stopping. results reported -speaker core test set. audio data encoded using fourier-transform-based ﬁlter-bank coefﬁcients distributed mel-scale together ﬁrst second temporal derivatives. input vector therefore size data normalised every element input vectors zero mean unit variance training set. phoneme labels used training decoding mapped classes scoring note experiments once variance random weight initialisation weight noise unknown. shown table nine rnns evaluated varying along three main dimensions training method used number hidden levels number lstm cells hidden layer. bidirectional lstm used networks except ctc-l-h-tanh tanh units instead lstm cells ctc-l-h-uni lstm layers unidirectional. networks trained using stochastic gradient descent learning rate momentum random initial weights drawn uniformly networks except ctc-l-h-tanh pretrans-l-h ﬁrst trained noise then starting point highest log-probability development retrained gaussian weight noise point lowest phoneme error rate development set. pretrans-l-h initialised weights ctcl-h along weights phoneme prediction network trained without noise retrained noise stopped point highest log-probability. pretrans-lh trained point noise added. ctc-lh-tanh entirely trained without weight noise failed learn noise added. beam search decoding used networks beam width advantage deep networks immediately obvious error rate dropping number hidden levels increases ﬁve. four networks ctc-l-h-tanh ctc-l-h ctcl-h-uni ctc-l-h approximately number weights give radically different results. three main conclusions draw lstm works much better tanh task bidirectional fig. input sensitivity deep rnn. heatmap shows derivatives ‘ah’ outputs printed respect ﬁlterbank inputs timit ground truth segmentation shown below. note sensitivity extends surrounding segments; attempts learn linguistic dependencies acoustic data. lstm slight advantage unidirectional lstmand depth important layer size although advantage transducer slight weights randomly initialised becomes substantial pretraining used. shown combination deep bidirectional long short-term memory rnns end-to-end training weight noise gives state-of-the-art results phoneme recognition timit database. obvious next step extend system large vocabulary speech recognition. aninteresting direction would combine frequencydomain convolutional neural networks deep lstm. qifeng barry chen nelson morgan andreas stolcke tandem connectionist feature extraction international conversational speech recognition conference machine learning multimodal interaction berlin heidelberg mlmi’ springer-verlag. hinton deng dong g.e. dahl mohamed jaitly senior vanhoucke nguyen t.n. sainath kingsbury deep neural networks acoustic modeling speech recognition signal processing magazine ieee vol. nov. graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks icml pittsburgh", "year": 2013}