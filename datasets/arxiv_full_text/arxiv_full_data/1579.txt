{"title": "An Empirical Evaluation of Generic Convolutional and Recurrent Networks  for Sequence Modeling", "tag": ["cs.LG", "cs.AI", "cs.CL"], "abstract": "For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks.", "text": "deep learning practitioners sequence modeling synonymous recurrent networks. recent results indicate convolutional architectures outperform recurrent networks tasks audio synthesis machine translation. given sequence modeling task dataset architecture use? conduct systematic evaluation generic convolutional recurrent architectures sequence modeling. models evaluated across broad range standard tasks commonly used benchmark recurrent networks. results indicate simple convolutional architecture outperforms canonical recurrent networks lstms across diverse range tasks datasets demonstrating longer effective memory. conclude common association sequence modeling recurrent networks reconsidered convolutional networks regarded natural starting point sequence modeling tasks. deep learning practitioners commonly regard recurrent architectures default starting point sequence modeling tasks. sequence modeling chapter canonical textbook deep learning titled sequence modeling recurrent recursive nets capturing common association sequence modeling recurrent architectures. well-regarded recent online course sequence models focuses exclusively recurrent architectures hand recent research indicates certain convolutional architectures reach state-of-the-art accuracy audio synthesis word-level language modeling mamachine learning department carnegie mellon university pittsburgh computer science department carnegie mellon university pittsburgh intel labs santa clara usa. chine translation raises question whether successes convolutional sequence modeling conﬁned speciﬁc application domains whether broader reconsideration association sequence processing recurrent networks order. address question conducting systematic empirical evaluation convolutional recurrent architectures broad range sequence modeling tasks. specifically target comprehensive tasks repeatedly used compare effectiveness different recurrent network architectures. tasks include polyphonic music modeling wordcharacter-level language modeling well synthetic stress tests deliberately designed frequently used benchmark rnns. evaluation thus compare convolutional recurrent approaches sequence modeling recurrent networks’ home turf. represent convolutional networks describe generic temporal convolutional network architecture applied across tasks. architecture informed recent research deliberately kept simple combining best practices modern convolutional architectures. compared canonical recurrent architectures lstms grus. results suggest tcns convincingly outperform baseline recurrent architectures across broad range sequence modeling tasks. particularly notable tasks include diverse benchmarks commonly used evaluate recurrent network designs indicates recent successes convolutional architectures applications audio processing conﬁned domains. understand results analyze deeply memory retention characteristics recurrent networks. show despite theoretical ability recurrent architectures capture inﬁnitely long history tcns exhibit substantially longer memory thus suitable domains long history required. knowledge presented study extensive systematic comparison convolutional recurrent architectures sequence modeling tasks. results suggest common association sequence modeling recurrent networks reconsidered. architecture appears accurate canonical recurrent networks lstms grus also simpler clearer. therefore appropriate starting point application deep networks sequences. assist related work made code available http//github.com/locuslab/tcn. convolutional networks applied sequences decades used prominently speech recognition convnets subsequently applied tasks part-of-speech tagging semantic role labelling recently convolutional networks applied sentence classiﬁcation document classiﬁcation particularly inspiring work recent applications convolutional architectures machine translation audio synthesis language modeling recurrent networks dedicated sequence models maintain vector hidden activations propagated time family architectures gained tremendous popularity prominent applications language modeling machine translation intuitive appeal recurrent modeling hidden state representation everything seen sequence. basic architectures notoriously difﬁcult train elaborate architectures commonly used instead lstm many architectural innovations training techniques recurrent networks introduced continue actively explored effectiveness different recurrent architectures. studies motivated part many degrees freedom design architectures. chung compared different types recurrent units task polyphonic music modeling. pascanu explored different ways construct deep rnns evaluated performance different architectures polyphonic music modeling character-level language modeling word-level language modeling. jozefowicz searched thousand different architectures evaluated performance various tasks. concluded architectures much better lstm trivial ﬁnd. greff benchmarked performance eight lstm variants speech recognition handwriting recognition polyphonic music modeling. also found none variants improve upon standard lstm architecture signiﬁcantly. zhang systematically analyzed connecting architectures rnns evaluated different architectures characterlevel language modeling synthetic stress tests. melis benchmarked lstm-based architectures word-level character-level language modeling concluded lstms outperform recent models. recent works aimed combine aspects architectures. includes convolutional lstm replaces fully-connected layers lstm convolutional layers allow additional structure recurrent layers; quasi-rnn model interleaves convolutional layers simple recurrent layers; dilated adds dilations recurrent architectures. combinations show promise combining desirable aspects types architectures study focuses comparison generic convolutional recurrent architectures. multiple thorough evaluations architectures representative sequence modeling tasks aware similarly thorough comparison convolutional recurrent approaches sequence modeling. reported comparison convolutional recurrent networks sentence-level document-level classiﬁcation tasks. contrast sequence modeling calls architectures synthesize whole sequences element element.) comparison particularly intriguing light aforementioned recent success convolutional architectures domain. work aims compare generic convolutional recurrent architectures typical sequence modeling tasks commonly used benchmark variants themselves begin describing generic architecture convolutional sequence prediction. distill best practices convolutional network design simple architecture serve convenient powerful starting point. refer presented architecture temporal convolutional network emphasizing adopt term label truly architecture simple descriptive term family architectures. distinguishing characteristics tcns convolutions architecture causal meaning information leakage future past; architecture take sequence length output sequence length rnn. beyond this emphasize build long effective history sizes using combination deep networks dilated convolutions. architecture informed recent convolutional architectures sequential data distinct designed ﬁrst principles combine simplicity autoregressive prediction long memory. example much simpler wavenet deﬁning network structure highlight nature sequence modeling task. suppose given input sequence wish predict corresponding outputs time. constraint predict output time constrained inputs previously observed formally sequence modeling network function produces mapping satisﬁes causal constraint depends future inputs goal learning sequence modeling setting network minimizes expected loss actual outputs predictions sequences outputs drawn according distribution. regressive prediction setting target output simply input shifted time step. however directly capture domains machine translation sequenceto-sequence prediction general since cases entire input sequence used predict output mentioned above based upon principles fact network produces output length input fact leakage future past. accomplish ﬁrst point uses fully-convolutional network architecture hidden layer length input layer zero padding length added keep subsequent layers length previous ones. achieve second point uses causal convolutions convolutions output time convolved elements time earlier previous layer. major disadvantage basic design order achieve long effective history size need extremely deep network large ﬁlters neither particularly feasible methods ﬁrst introduced. thus following sections describe techniques modern convolutional architectures integrated allow deep networks long effective history. simple causal convolution able look back history size linear depth network. makes challenging apply aforementioned causal convolution sequence tasks especially requiring longer history. solution here following work oord employ dilated convolutions enable exponentially large receptive ﬁeld formally sequence input ﬁlter dilated convolution operation element sequence deﬁned figure architectural elements tcn. dilated causal convolution dilation factors ﬁlter size receptive ﬁeld able cover values input sequence. residual block. convolution added residual input output different dimensions. example residual connection tcn. blue lines ﬁlters residual function green lines identity mappings. alent introducing ﬁxed step every adjacent ﬁlter taps. dilated convolution reduces regular convolution. using larger dilation enables output level represent wider range inputs thus effectively expanding receptive ﬁeld convnet. gives ways increase receptive ﬁeld choosing larger ﬁlter sizes increasing dilation factor effective history layer common using dilated convolutions increase exponentially depth network level network). ensures ﬁlter hits input within effective history also allowing extremely large effective history using deep networks. provide illustration figure within residual block layers dilated causal convolution non-linearity used rectiﬁed linear unit normalization applied weight normalization convolutional ﬁlters. addition spatial dropout added dilated convolution regularization training step whole channel zeroed out. however whereas standard resnet input added directly output residual function input output could different widths. account discrepant input-output widths additional convolution ensure elementwise addition receives tensors shape since tcn’s receptive ﬁeld depends network depth well ﬁlter size dilation factor stabilization deeper larger tcns becomes important. example case prediction could depend history size high-dimensional input sequence network layers could needed. layer speciﬁcally consists multiple ﬁlters feature extraction. design generic model therefore employ generic residual module place convolutional layer. parallelism. unlike rnns predictions later timesteps must wait predecessors complete convolutions done parallel since ﬁlter used layer. therefore training evaluation long input sequence processed whole instead sequentially rnn. flexible receptive ﬁeld size. change receptive ﬁeld size multiple ways. instance stacking dilated convolutional layers using larger dilation factors increasing ﬁlter size viable options tcns thus afford better control model’s memory size easy adapt different domains. stable gradients. unlike recurrent architectures backpropagation path different temporal direction sequence. thus avoids problem exploding/vanishing gradients major issue memory requirement training. especially case long input sequence lstms grus easily memory store partial results multiple cell gates. however ﬁlters shared across layer backpropagation path depending network depth. therefore practice found gated rnns likely multiplicative factor memory tcns. variable length inputs. like rnns model inputs variable lengths recurrent tcns also take inputs arbitrary lengths sliding convolutional kernels. means tcns adopted drop-in replacements rnns sequential data arbitrary length. data storage evaluation. evaluation/testing rnns need maintain hidden state take current input order generate prediction. words summary entire history provided ﬁxed-length vectors actual observed sequence discarded. contrast tcns need take sequence effective history length thus possibly requiring memory evaluation. potential parameter change transfer domain. different domains different requirements amount history model needs order predict. therefore transferring model domain little memory needed domain much longer memory required perform poorly sufﬁciently large receptive ﬁeld. evaluate tcns rnns tasks commonly used benchmark performance different sequence modeling architectures intention conduct evaluation home turf sequence models. comprehensive synthetic stress tests along real-world datasets multiple domains. adding problem. task input consists length-n sequence depth values randomly chosen second dimension zeros except elements marked objective random values whose second dimensions marked simply predicting give first introduced hochreiter schmidhuber adding problem used repeatedly stress test sequence models sequential mnist p-mnist. sequential mnist frequently used test recurrent network’s ability retain information distant past task mnist images presented model sequence digit classiﬁcation. challenging p-mnist setting order sequence permuted random copy memory. task input sequence length ﬁrst values chosen randomly among digits rest zeros except last entries ﬁlled digit goal generate output length zero everywhere except last values delimiter model expected repeat values encountered start input. task used prior works zhang arjovsky wisdom jing chorales nottingham. chorales polyphonic music dataset consisting entire corpus four-part harmonized chorales bach. input sequence elements. element -bit binary code corresponds keys piano indicating pressed given time. nottingham polyphonic music dataset based collection british american folk tunes much larger chorales. chorales nottingham used numerous empirical investigations recurrent sequence modeling performance tasks measured terms negative log-likelihood penntreebank. used penntreebank character-level word-level language modeling. used character-level language corpus contains characters training validation testing alphabet size used word-level language corpus contains words training validation testing vocabulary size highly studied relatively small language modeling dataset wikitext-. wikitext- almost times large featuring vocabulary size table evaluation tcns recurrent architectures synthetic stress tests polyphonic music modeling character-level language modeling word-level language modeling. generic architecture outperforms canonical recurrent networks across comprehensive suite tasks datasets. current state-of-the-art results listed supplement. means higher better. means lower better. seq. mnist permuted mnist adding problem copy memory music chorales music nottingham word-level word-level wiki- word-level lambada char-level char-level text dataset contains wikipedia articles training articles validation articles testing. representative realistic dataset much larger vocabulary includes many rare words used merity grave dauphin lambada. introduced paperno lambada dataset comprising passages extracted novels average sentences context target sentence last word predicted. dataset built person easily guess missing word given context sentences given target sentence without context sentences. existing models fail lambada general better results lambada indicate model better capturing information longer broader context. training data lambada full text novels words. vocabulary size text. also used text dataset character-level language modeling text times larger characters wikipedia corpus contains unique alphabets. compare generic architecture described section canonical recurrent architectures namely lstm vanilla standard regularizations. experiments reported section used exactly architecture varying depth network occasionally kernel size receptive ﬁeld covers enough context predictions. exponential dilation layer network adam optimizer learning rate unless otherwise noted. also empirically gradient clipping helped convergence pick maximum norm clipping training recurrent models grid search good hyperparameters learning rate gradient clipping initial forget-gate bias) keeping network around size tcn. architectural elaborations gating mechanisms skip connections added either tcns rnns. additional details controlled experiments provided supplementary material. synopsis results shown table note several tasks generic canonical recurrent architectures study stateof-the-art. caveat results strongly suggest generic architecture minimal tuning outperforms canonical recurrent architectures across broad variety sequence modeling tasks commonly used benchmark performance recurrent architectures themselves. analyze results detail. figure models chosen roughly parameters. tcns quickly converged virtually perfect solution grus also performed quite well albeit slower converge tcns. lstms vanilla rnns performed signiﬁcantly worse. sequential mnist p-mnist. convergence results sequential permuted mnist epochs shown figure models conﬁgured roughly parameters. problems tcns substantially outperform recurrent architectures terms convergence ﬁnal accuracy task. p-mnist tcns outperform state-of-the-art results based recurrent networks zoneout recurrent batchnorm copy memory. convergence results copy memory task shown figure tcns quickly converge correct answers lstms grus simply converge loss predicting zeros. case also compare recently-proposed eurnn highlighted perform well task. eurnn perform well sequence length clear advantage longer discuss results polyphonic music modeling character-level language modeling word-level language modeling. domains dominated recurrent architectures many specialized designs developed tasks mention specialized architectures useful primary goal compare generic model similarly generic recurrent architectures before domain-speciﬁc tuning. results summarized table polyphonic music. nottingham chorales virtually tuning outperforms recurrent models considerable margin even outperforms enhanced recurrent architectures task hf-rnn diagonal note however models deep belief lstm perform better still believe likely fact datasets relatively small thus right regularization method generative modeling procedure improve performance signiﬁcantly. largely orthogonal rnn/tcn distinction similar variant well possible. word-level language modeling. language modeling remains primary applications recurrent networks many recent works focused optimizing lstms task implementation follows standard practice ties weights encoder decoder layers rnns signiﬁcantly reduces number parameters model. training anneal learning rate factor rnns validation accuracy plateaus. smaller corpus optimized lstm architecture outperforms outperforms vanilla rnn. however much larger wikitext- corpus lambada dataset without hyperparameter search outperforms lstm results grave achieving much lower perplexities. character-level language modeling. character-level language modeling generic outperforms regularone theoretical advantages recurrent architectures unlimited memory theoretical ability retain information sequences unlimited length. examine speciﬁcally long different architectures retain information practice. focus copy memory task stress test designed evaluate longterm distant information propagation recurrent networks lambada task tests local non-local textual understanding. copy memory task perfectly examine model’s ability retain information different lengths time. requisite retention time controlled varying sequence length contrast section focus accuracy last elements output sequence used models size rnns. results focused study shown figure tcns consistently converge accuracy sequence lengths whereas lstms grus size quickly degenerate random guessing sequence length grows. accuracy lstm falls falls results indicate tcns able maintain much longer effective history recurrent counterparts. observation backed real data experiments large-scale lambada dataset speciﬁcally designed test model’s ability utilize broad context shown table outperforms lstms vanilla rnns signiﬁcant margin perplexity lambada substantially smaller network virtually tuning. presented empirical evaluation generic convolutional recurrent architectures across comprehensive suite sequence modeling tasks. described simple temporal convolutional network combines best practices dilations residual connections causal convolutions needed autoregressive prediction. experimental results indicate models substantially outperform generic recurrent architectures lstms grus. studied long-range information propagation convolutional recurrent networks showed inﬁnite memory advantage rnns largely absent practice. tcns exhibit longer memory recurrent architectures capacity. numerous advanced schemes regularizing optimizing lstms proposed schemes signiﬁcantly advanced accuracy achieved lstm-based architectures datasets. beneﬁtted concerted community-wide investment architectural algorithmic elaborations. investment desirable expect yield advances performance commensurate advances seen recent years lstm performance. release code project encourage exploration. preeminence enjoyed recurrent networks sequence modeling largely vestige history. recently introduction architectural elements dilated convolutions residual connections convolutional architectures indeed weaker. results indicate elements simple convolutional architecture effective across diverse sequence modeling tasks recurrent architectures lstms. comparable clarity simplicity tcns conclude convolutional networks regarded natural starting point powerful toolkit sequence modeling. bottou l´eon soulie fogelman blanchet pascal li´enard jean-sylvain. speaker-independent isolated digit recognition multilayer perceptrons dynamic time warping. neural networks boulanger-lewandowski nicolas bengio yoshua vincent pascal. modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription. arxiv. chung junyoung sungjin bengio yoshua. hierarchical multiscale recurrent neural networks. arxiv. collobert ronan weston jason. uniﬁed architecture natural language processing deep neural networks multitask learning. icml conneau alexis schwenk holger lecun yann barrault lo¨ıc. deep convolutional networks text classiﬁcation. european chapter association computational linguistics greff klaus srivastava rupesh kumar koutn´ık steunebrink schmidhuber j¨urgen. lstm search space odyssey. ieee transactions neural networks learning systems jing shen yichen dubcek tena peurifoy john skirlo scott lecun yann tegmark soljaˇci´c marin. tunable efﬁcient unitary neural networks application rnns. icml krueger david maharaj tegan kram´ar j´anos pezeshki mohammad ballas nicolas rosemary goyal anirudh bengio yoshua larochelle hugo courville aaron chris. zoneout regularizing rnns randomly preserving hidden activations. iclr lecun yann boser bernhard denker john henderson donnie howard richard hubbard wayne jackel lawrence backpropagation applied handwritten code recognition. neural computation paperno denis kruszewski germ´an lazaridou angeliki pham quan ngoc bernardi raffaella pezzelle sandro baroni marco boleda gemma fern´andez raquel. lambada dataset word prediction requiring broad discourse context. arxiv. pascanu razvan mikolov tomas bengio yoshua. difﬁculty training recurrent neural networks. icml pascanu razvan g¨ulc¸ehre aglar kyunghyun bengio yoshua. construct deep recurrent neural networks. iclr xingjian chen zhourong wang yeung dit-yan wong wai-kin wang-chun. convolutional lstm network machine learning approach precipitation nowcasting. nips srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. jmlr oord a¨aron dieleman sander heiga simonyan karen vinyals oriol graves alex kalchbrenner senior andrew kavukcuoglu koray. wavenet generative model audio. arxiv. waibel alex hanazawa toshiyuki hinton geoffrey shikano kiyohiro lang kevin phoneme recognition using timedelay neural networks. ieee transactions acoustics speech signal processing zhang saizheng yuhuai tong zhouhan memisevic roland salakhutdinov ruslan bengio yoshua. architectural complexity measures recurrent neural networks. nips table lists hyperparameters used applying generic model various tasks datasets. important factor picking parameters make sure sufﬁciently large receptive ﬁeld choosing cover amount context needed task. discussed section number hidden units chosen model size approximately level recurrent models comparing. table gradient clip means gradient clipping applied. larger tasks empirically found gradient clipping helps regularizing accelerating convergence. weights initialized gaussian disitribution general found relatively insensitive hyperparameter changes long effective history size sufﬁcient. table reports hyperparameter settings used lstm. values picked hyperparameter search lstms layers optimizers chosen {sgd adam rmsprop adagrad}. certain larger datasets adopted settings used prior work wikitext-). hyperparameters chosen similar fashion typically hidden units lstm keep total network size approximately previously noted generic lstm/gru models used outperformed specialized architectures tasks. state-of-the-art results summarized table architecture used across tasks. note size state-of-the-art model different size tcn. section brieﬂy study effects different components layer. overall believe dilation required modeling long-term dependencies mainly focus factors here ﬁlter size used layer effect residual blocks. perform series controlled experiments results ablative analysis shown figure before kept model size depth exactly different models dilation factor strictly controlled. experiments conducted three different tasks copy memory permuted mnist penn treebank word-level language modeling. experiments conﬁrm factors contribute sequence modeling performance. filter size copy memory p-mnist tasks observed faster convergence better accuracy larger ﬁlter sizes. particular looking figure ﬁlter size converges level random guessing. contrast word-level language modeling smaller kernel ﬁlter size works best. believe smaller kernel tends focus local context especially important language modeling residual block. three scenarios compared here observed residual function stabilized training brought faster convergence better ﬁnal results. especially language modeling found residual connections contribute substantially performance component used prior work convolutional architectures language modeling gated activation chosen gating generic model. examine choice closely. dauphin compared effects gated linear units gated tanh units adopted non-dilated gated convnet. following choice compare tcns using relu tcns gating represented elementwise product convolutional layers also passing sigmoid function note gates architecture uses approximately twice many convolutional layers relu-tcn. results shown table kept number model parameters size. improve accuracy certain language modeling datasets like agrees prior work. however observe comparable beneﬁts tasks polyphonic music modeling synthetic stress tests require longer information retention. copy memory task found gating converged worse result relu", "year": 2018}