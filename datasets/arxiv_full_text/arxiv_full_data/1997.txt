{"title": "An Approach to Stable Gradient Descent Adaptation of Higher-Order Neural  Units", "tag": ["cs.NE", "cs.AI", "cs.CE", "cs.LG", "cs.SY"], "abstract": "Stability evaluation of a weight-update system of higher-order neural units (HONUs) with polynomial aggregation of neural inputs (also known as classes of polynomial neural networks) for adaptation of both feedforward and recurrent HONUs by a gradient descent method is introduced. An essential core of the approach is based on spectral radius of a weight-update system, and it allows stability monitoring and its maintenance at every adaptation step individually. Assuring stability of the weight-update system (at every single adaptation step) naturally results in adaptation stability of the whole neural architecture that adapts to target data. As an aside, the used approach highlights the fact that the weight optimization of HONU is a linear problem, so the proposed approach can be generally extended to any neural architecture that is linear in its adaptable parameters.", "text": "early higher-order polynomial publications covering honn refer technique extremely high-order polynomial regression tool work presenting strong approximating capabilities limited number product nodes preserving good generalization extensions principal component analysis higher-order correlations optimal fitting hypersurfaces polynomial basis function discriminant models reduced honn preservation geometric invariants pattern recognition demonstration capabilities honn arbitrary dynamical system approximation dynamic weight pruning multiple learning rates recent works applications honn refer particular focus quadratic neural unit using matrix notation upper triangular weight matrix found significant recent publications devoted concepts works recent works framed within honns found modifications honu introduced order cope curse dimensionality honu higher polynomial orders. interesting earlier-appearing neural network architectures product neural units later logarithmic neural networks another nonconventional neural units continuous time-delay dynamic neural units higher-order time-delay neural units adaptable time delays neural synapses state feedbacks individual neurons introduced similar fuzzy-network oriented concept appeared parallel also another work focusing various types neural transfer functions found review optimization neural weights conventional neural networks nonlinear problem layered networks hidden neurons sigmoid output functions. nonlinear approaches stability evaluation based lyapunov approach energetic approaches commonly adopted. mostly techniques sophisticated require significant time demanding effort users true experts field neural networks. hand newer honu models proposed effective computation learning configuration still suffer take care local minima problem. improve learning nonlinear adaptive models gradient descent based learning propose novel approach abstract— stability evaluation weight-update system higher-order neural units polynomial aggregation neural inputs adaptation feedforward recurrent honus gradient descent method introduced. essential core approach based spectral radius weight-update system allows stability monitoring maintenance every adaptation step individually. assuring stability weight-update system naturally results adaptation stability whole neural architecture adapts target data. aside used approach highlights fact weight optimization honu linear problem proposed approach generally extended neural architecture linear adaptable parameters. igher-order neural units polynomial weighting aggregation neural inputs known fundamental class polynomial neural networks recall polynomial feedforward neural networks attractive reliable theoretical results universal approximation abilities according weierstrass theorem generalization power measured vapnik-chervonenkis dimension re-numbered references). work evaluated computing ability several types honns using pseudo-dimensions dimensions higher-order neural networks used universal approximator. basically pnns honns represent style computation artificial neural networks neurons involve polynomials neurons polynomials themselves involve manuscript received february work supported parts specific research funding czech technical university prague grants sgs//ohk/t/ sgs//ohk/t/ also jsps kakenhi grants noriyasu homma dpt. radiological imaging informatics tohoku university graduate school medicine also intelligent biomedical system engineering lab. graduate school biomedical engineering tohoku university sendai japan decreasing magnitude data decreasing learning rate. basically adopt following standard notation variables small caps scalar bolt vector bolt capital matrix. lower indexes denotes position within vector array upper transposition. discrete time index necessary shown comes round brackets denotes neural output training target. meaning symbols given first appearance. neural output number neural inputs stands neural weights. denote polynomial order honu. adopting matrix formulation vector neural inputs weight array follows weight understood higher-dimensional array section derive formulation array alternative allows gradient descent stability condition honu effectively derived allows connotations adaptive learning rates linearly aggregated filters summarized next useful introduce long-vector operators rowr colr polynomial order case operators rowr colr work follows efficient simple stability evaluation stability maintenance adaptation principally avoids local minima problems given training data in-parameter-linearity honus. proposed approach recalls weight optimization honus nonlinear input-output mapping models linear problem theoretically implies existence unique minimum. minor contribution flattened representation honu using long-vector operations shown need multidimensional arrays weights honu long-vector-operator approach also simplifies direct weight calculation static honu arbitrary polynomial order least square method i.e. variations wiener-hopf equation points connotation levenberg-marquardt algorithm honu. main contribution paper adaptation stability evaluation based fact optimization weights honus linear problem; thus evaluation maximum eigenvalue used assess stability neural weight system. based principle nonlinear extension stability monitoring maintenance static honu well recurrent honu proposed. novel approach evaluation maintenance stability adapted honus. principle derived stability condition enables gradient adaptation honus stabilized time-varying learning rates every sampling moment. also discuss effect data normalization show relationship scaling factor magnitude learning rate moreover achievements might bring novel research directions honu considering adaptive learning rate modifications gradient descent connection that adaptable learning rate modifications honu recalled proposed adaptation stability condition discussed connotation them. paper organized follows. subsection ii.a introduces flattening operator approach honu thus also reveals linear optimization nature honu. then operator approach used derive stability condition weight-update system static honus subsection ii.b recurrent honus ii.c ii.d derives relationship data normalization change learning rate correspondingly section experimentally supports theoretical derivations also discusses possible extensions adaptive-learning-rate principles linear filters honu. derived adaptation stability rule adaptive multiple learning rates static honus demonstrated example fifth polynomial order honu hyperchaotic chua’s time series prediction rule dynamic honu demonstrated chaotic mackey-glass time series prediction. also subsection iii.d computationally demonstrates relationship optimization weights colw clearly represents linear equations solved. further recall weight calculation static honu using least squares introduced operators. assume input vectors matrix defined indexes stand input variables column indexes stand sampled input patterns. yp]t denote vector targets input patterns general polynomial order express square error criteria neural outputs targets using follows formulas direct calculation weights least square method imply existence unique solution i.e. unique global minimum. course acquire select optimum training data enough nonlinearly independent training patterns would result correct calculation weights another issue. practical notice comfortably derive levenberg-marquardt weight-update algorithm static honu e.g. simplest form follows indexes stand input variables column indexes correspond sample index general application colr rowr operators matrix input patterns means application individual input vectors matrix shown regarding neural weights benefit also long-column-vector operator long-row-vector multidimensional weight arrays honn notice weight matrix -dimensional array would become r-dimensional array neural weights r-order neural unit. therefore introduce another compatible functionality operators conversion row; multidimensional arrays neural weights long-column-vector long-row-vector representation. weight matrix e.g. long-vector operators work follows standard vector neural output errors learning rate rowx=colxt already represents jacobian matrix. automated retraining techniques help estimate calculate weights apply algorithm. however focus stability gradient descent learning static recurrent honu paper. subsection defined operators neural architectures higher-order polynomial aggregation neural inputs. functionality operators slightly differs applied vector neural inputs matrix neural weights. used operators derive neural weights using least square method thus existence single minimum weight optimization honu clear relationship operators algorithm shown. next introduced operators honu evaluation maintenance stability weight-update system every gradient-descent adaptation step static well recurrent honus. operator approach introduced used stability evaluation stability maintenance weight updates static honu updated gradient descent recurrent honu updated recurrent version also known rtrl derive approach stability evaluation static honu subsection first. output static honu discrete time samples given weight-update system fundamental gradient descent learning rule update weights honu sampling time given spectral radius identity matrix diagonal length equal number neural weights. improve adaptation stability static honu update learning rate observe impact spectral radius naturally instead single introduce time varying individual learning rates weight diagonal matrix learning rates diagonal ordered accordingly weights colw roww. stability weight-update system static honu every adaptation step classically resulting defined time-indexed learning rate matrix indicates stabilize adaption time varying learning rates starting point developing novel adaptive learning rate algorithms honu e.g. starting inspiration works time honu also condition explains normalization input data affects gradient descent adaptation stability high magnitude input data results large requires small learning rates approach condition recurrent honu feeds step delayed neural output back input. individual weight update recurrent honu fundamental gradient descent given using introduced operators polynomial order follows dimensions nw×nw total number weights also equal number elements rowx colw. denote element jacobian column. case corresponds partial derivative element vector rowx correspond second-order polynomial correlation neural input neural output partial derivative single weight calculated defined time indexing learning rate matrix indicates time variability individual learning rates. condition allows evaluate maintain stability update weight system recurrent honu every sampling time also resetting jacobian zero occasionally considered results eliminating term note resetting jacobian zero matrix condition stability weight-update system recurrent honu yields stability condition static honu thus stability neural weights becomes independent actual weights themselves. proper investigation j-reset effect learning recurrent honu rigorous development analysis sophisticated techniques adaptive learning rates based works exceeds limits paper; however operator approach stability conditions honu allows propose straightforward connotations adaptive learning rates techniques honu introduce subsections iii.b iii.c. bias according correct left-side matrix multiplication arrive rtrl update rule recurrent honu general polynomial order considers matrix dimensions multiplications subsection present results achieved proposed operator approach weight calculation static honu least square method derived subsection ii.a. results support existence unique minimum polynomial nonlinearity honu. benchmark chose variation famous system static honu one-step predictor time series noise suitable task weights found directly least square method even high noise training data honu trained first samples tested next samples mean absolute error neural output true signal neural output noisy training data demonstrating even noise training data high honu learns governing laws approximates original signal tends reject noise seen fig. fig. shows neural weights directly calculated least squares method weights correspond relevant polynomial terms containing terms suppressed neural weights resulted near zero results demonstrated functionality proposed operators weight calculation subsection ii.a static honu polynomial order good quality nonlinear approximation capability honu extract major governing laws noised training data also shown. next experimental part demonstrate main contribution paper i.e. results stability evaluation weight updates gradient descent method recurrent honu. subsection demonstrated good approximation capability honu good extraction governing laws even direct calculation least squares method i.e. variations wiener-hopf equations) implies principal existence single minima honu given training data honu nonlinear models linear parameters. next draw extensions adaptive learning rates static later recurrent honus. subsection introduce connotations static honus adaptive learning rate techniques well known literature adaptive filters linear aggregation neural inputs in-parameter-linearity honus allows draw parallel honus linearly aggregated filters learning rate adaptive techniques; i.e. simple comparison long-vector operator notation honus linearly aggregated filters form rowx plays role colw plays role substitutions adapt learning rate classical normalized least mean square algorithm adaptive learning rate static honus yields fig. testing static honu benchmark snr=. apparently honu extracted governing laws rather noisy training signal training data samples testing mean)=. mean)=.. displayed improved stability faster convergence experiments. straight explanation follows. contrary squared-norm normalization aggressively contributes stability condition suppressing learning rates norm neural inputs exceeds unit i.e. ||rowx||> explains better stability hand squared-norm naturally progressive closest mentioned options practically appeared normalizing approaches static honu superior speed maintaining convergence long adaptation runs gradient based learning rate adaptive techniques adopted honu tend require early stopping well-known issue. fig. training honus square errors training epochs honu real-time estimation spectral radius increase frobenius norm last epoch training order honu. performance comparison algorithms static honu demonstrated fig. furthemore similarly substitution made static honus also implement benveniste’s learning rate updates based algorithm farhang generalized normalized gradient descent algorithm mandic recently showed extensions honu compared performance chaotic time series recalled tab. indicated subsection ii.b also resulted experiments appeared efficient used individual learning rates weight normalize individually. propose following algorithm. redefine a=·s) according multiple learning rates contribute stability normalization individual learning rates euclidean norm corresponding rows individual learning rates multiply corresponding rows thus affect corresponding rows therefore adaptive learning rate normalized position index weight colw also indexes corresponding learning rate diagonal matrix learning rates matrix again squared euclidean norm rows yields again modification normalization algorithm performed faster convergence again squared norm aggressive norm itself individual learning rates normalized equally contribute stability. practically found significant difference performance tuning plays role focus paper. anyhow found adaptive learning rate techniques useful static honu clear connotation weight update stability appeared experiments normalization static honu efficient gradient adaptive learning rate techniques consider stability maintenance straightforwardly. performance learning adaptive learning rates showing stability condition static honus order prediction hyperchaotic chua’s time series shown fig. time series obtained sampling second. configuration honu nonlinear recurrent predictor prediction time steps input honu included bias tapped delayed feedbacks recently measured values. fig. shows later epoch stable adaptation recurrent honu trained according gradient descent learning rule using operator approach derived subsection ii.c. adaptation fig. stable sufficiently small learning rate occasional violations stability condition bottom plot fig. spontaneously diminished resulted instability recurrent honu. example unstable adaptation recurrent honu given fig. weight update becomes unstable appears oscillations neural output thus oscillations error importantly stability condition became significantly violated neural output oscillations appeared well apparent detail fig. bottom plots fig. fig. weight system returns stability reset jacobian zeros recurrently calculated gradients shown mentioned already advanced stability maintenance carried introducing individual learning rates weight optimize magnitudes respect stability condition static honu according respect stability condition recurrent honu according computationally verify derivation subsection ii.d normally distributed zero-mean data unit standard deviation used original input data vector various length static qnu. effect scaling factor versus learning rate adaptation stability thus confirming relationship demonstrated fig. fig. data larger variance magnitude fig. –fig. imply adaptive requires adapted within much wider besides good quality nonlinear approximation first example subsection iii.a demonstrated weights static honu calculated least mean square approach using introduced operators also supports fact existence single minimum weight optimization honu; linear nature weight optimization task apparent already neural output equations honu linear problems single solution. although weight optimization benchmark subsection iii.a suitable task static honu known adaptation becomes nontrivial task benchmark weight-update system becomes unstable requires control magnitude learning rate fig. computationally estimated data scaling factor reach spectral radius ρ=.; figure roughly confirms power relationship variation learning rate shown aside recalled optimization honu features unique minimum weight optimization given training data contains enough linearly independent training patterns demonstrated benchmark data. proposed approach honu helpful many identification control applications. also struggle overfitting relieved local minima issue introduced neural architecture reach good generalization nonlinear model focused primarily proper data processing finding appropriate input configuration. practically observed weight convergence static honu using algorithm rapid required epoch comparison conventional multilayered perceptron networks. moreover honus trained various initial weights almost identical outputs compared various instances trained networks whose outputs different input patterns trained different initial weights configurations reasoned principle recalled linear nature weight optimization static honu implies existence single minimum particular input configuration given training data conventional suffers local minima. introduced operator approach online weight-update stability evaluation gradient descent method applicable neural architecture linear parameters neural output expressed colx rowx also include nonlinear terms multiplicative ones case honu paper. introduced adaptation stability honu also derived experimentally showed subsections ii.d iii.d scaling training data factor up-to adaptation stability variation learning rate regards estimation time complexity sense required computational operations sample time output r-th order honu inputs calculated vector roww colx vectors length multiplication case dynamical honu weight update stability condition requires computation matrix involves matrix multiplication matrices thus time complexity estimation increased observations; however mentioned honus found useful efficient esp. small network problems i.e. inputs many problems sufficiently solved honu order using introduced long-vector notation approach gradient descent adaptation stability static recurrent honus general polynomial order introduced monitoring spectral radius weight-update systems every adaptation step. experiments method verified adaptation instability detected well prediction error divergence became visually clear. in-parameter linearity honu adaptive learning rate techniques honu adopted known linear adaptive filters adaptation stability monitoring applied honus well. also derived experimentally shown scaling-down training data factor takes up-to r-power stronger influence adaptation stability rather decrease learning rate itself. implies importance training data normalization esp. adaptive learning rate techniques. presented approaches honus highlighted neural architectures offer adjustable strong nonlinear input-output mapping models linear optimization nature propose novel comprehensible approach toward stability gradient descent weight-update system useful prediction control system monitoring tasks. special thanks belongs madan gupta intelligent system research laboratory university saskatchewan author’s collaborative research honu since long-term support advices. special thanks belongs matsumae international foundation funded first author’s cooperation colleagues tohoku university japan cooperation still vitally continuing. cotter stone-weierstrass theorem application neural networks ieee trans. neural netw. vol. dec. holden anthony quantifying generalization linearly weighted neural networks complex syst. vol. nikolaev learning polynomial feedforward neural networks genetic programming backpropagation ieee trans. neural netw. vol. mar. y.-j. chen g.-x. tong adaptive neural output feedback tracking control class uncertain discrete-time nonlinear systems ieee trans. neural netw. publ. ieee neural netw. counc. vol. jul. schmitt complexity computing learning multiplicative neural networks neural comput. vol. feb. ivakhnenko polynomial theory complex systems ieee trans. syst. cybern. vol. smc- shin ghosh pi-sigma network efficient higher-order neural network pattern classification function approximation ijcnn--seattle international joint conference neural networks vol. vol.. softky kammen correlations high dimensional asymmetric data sets hebbian neuronal processing neural netw. vol. taylor coombes learning higher order correlations neural netw vol. mar. chen manry conventional modeling multilayer perceptron using polynomial basis functions ieee trans. neural netw. publ. ieee neural netw. counc. vol. schmidt davis pattern recognition properties various feature spaces higher order neural networks ieee trans. pattern anal. mach. intell. vol. aug. kosmatopoulos polycarpou christodoulou ioannou high-order neural network structures identification dynamical systems ieee trans. neural netw. vol. mar. kadirkamanathan billings on-line identification nonlinear systems using volterra polynomial basis function neural networks neural netw. vol. dec. advanced theory. york wiley redlapalli gupta k.-y. song development quadratic neural unit applications pattern classification fourth international symposium uncertainty modeling analysis isuma bukovsky redlapalli gupta quadratic cubic neural units identification fast state feedback control unknown non-linear dynamic systems gupta correlative type higher-order neural units applications ieee international conference automation logistics ical bukovsky jiri bila madan gupta zeng-guang noriyasu homma foundation classification nonconventional neural units paradigm nonsynaptic neural interaction discoveries breakthroughs cognitive informatics natural intelligence global durbin rumelhart product units computationally powerful biologically plausible extension backpropagation networks neural comput. vol. mar. hines logarithmic neural network architecture unbounded non-linear international conference neural networks vol. vol.. bukovsky jiri bila madan gupta zeng-guang noriyasu homma foundation classification nonconventional neural units paradigm nonsynaptic neural interaction discoveries breakthroughs cognitive informatics natural intelligence global oysal time delay dynamic fuzzy networks time series prediction computational science iccs sunderam albada sloot dongarra eds. springer berlin heidelberg zhao zhang pipelined chebyshev functional link artificial recurrent neural network nonlinear adaptive filter ieee trans. syst. cybern. part cybern. vol. feb. stavrakoudis theocharis pipelined recurrent fuzzy neural networks nonlinear adaptive speech prediction ieee trans. syst. cybern. part cybern. vol. oct. zhao zhang adaptively combined functional link artificial neural network equalizer nonlinear communication channel ieee trans. neural netw. vol. apr. mathews stochastic gradient adaptive filter gradient adaptive step size ieee trans. signal process. vol. jun. bukovsky oswald cejnek benes learning entropy novelty detection cognitive approach adaptive filters sensor signal processing defence cannas cincotti hyperchaotic behaviour bi-directionally coupled chua’s circuits int. circuit theory appl. vol. bukovsky bila adaptive evaluation complex dynamical systems using low-dimensional neural architectures advances cognitive informatics cognitive computing vol. wang zhang kinsner eds. berlin heidelberg springer berlin heidelberg bukovsky learning entropy multiscale measure incremental learning entropy vol. sep. mackey glass oscillation chaos physiological control systems science vol. jul. bodyanskiy kolodyazhniy real-time identification forecasting chaotic time series using hybrid systems computational intelligence integration fuzzy logic chaos theory halang chen eds. springer berlin heidelberg bukovsky lepold bila quadratic neural unit network validation process data steam turbine loop energetic boiler bukovsky homma smetana rodriguez mironovova vrana quadratic neural unit good compromise linear models neural networks industrial applications ieee international conference cognitive informatics bukovsky graduated czech technical university prague received ph.d. field control system engineering currently department instrumentation control engineering prague forming adaptive signal processing informatics computational centre within department. research interests include higher-order neural networks adaptive novelty detection dynamical systems real-data based modeling multiscale-analysis approaches adaptive control biomedical applications. long-term visiting researcher intelligent systems research lab. university saskatchewan canada cyberscience center tohoku university japan short-term visiting researcher dpt. electrical computer engineering university manitoba appointed visiting associate professor tohoku university. noriyasu homma received electrical communication engineering tohoku university japan respectively. lecturer tohoku university japan. currently full professor dpt. radiological imaging informatics tohoku university graduate school medicine also intelligent biomedical system engineering lab. graduate school biomedical engineering tohoku university. noriyasu visiting professor intelligent systems research laboratory university saskatchewan canada. current research interests include neural networks complex chaotic systems soft-computing cognitive sciences medical systems brain sciences. published journal conference papers edited book co-authored book chapters research books fields. serving many ieee conferences associated editor several international journals.", "year": 2016}