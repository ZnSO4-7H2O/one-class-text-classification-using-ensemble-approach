{"title": "Parallel training of DNNs with Natural Gradient and Parameter Averaging", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multi-core machines. In order to be as hardware-agnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.", "text": "daniel povey xiaohui zhang sanjeev khudanpur center language speech processing human language technology center excellence johns hopkins university baltimore {dpoveygmail.com} {xiaohuikhudanpurjhu.edu} describe neural-network training framework used kaldi speech recognition toolkit geared towards training dnns large amounts training data using multiple gpu-equipped multi-core machines. order hardware-agnostic possible needed multiple machines without generating excessive network trafﬁc. method average neural network parameters periodically redistribute averaged parameters machines training. machine sees different data. itself method work well. however another method approximate efﬁcient implementation natural gradient stochastic gradient descent seems allow periodicaveraging method work well well substantially improving convergence single machine. parallel training neural networks generally makes combination model parallelism data parallelism normal approach data parallelism involves communication model parameters minibatch. describe neural-net training framework uses different version data parallelism multiple processes separate machines infrequently average model parameters redistribute individual machines. effective large-scale training systems speech recognition– case works well combined efﬁcient implementation natural gradient stochastic gradient descent developed. don’t attempt paper develop framework explains parameter averaging work well despite non-convexity dnns ng-sgd helpful. point paper describe methods establish empirically work well. signiﬁcance work show possible linear speedup increasing number gpus without requiring frequent data transfer section describe problem setting deep neural networks applied speech recognition– although ideas general this. section introduce parallel training method. section describe general ideas behind natural gradient method although technical details relegated appendices. paper don’t give proofs discuss section think can’t proven methods. section experiments convergence without natural gradient parallelism. conclude section derived short-time spectral properties acoustic signal; corresponds clustered states context-dependent phones typical. pair corresponds single frame speech data; frames typically extracted rate second duration milliseconds contains spectral information several adjacent frames spliced together ultimately interested frame log-probabilities since costs viterbi search path corresponding likely word sequence. objective function training frames training data log-probability given misnomer; gradient ascent. supervision labels derived viterbi alignment hidden markov model derived reference word sequence training utterance. parameter-averaging aspect training quite simple. machines separately different randomized subsets training data allow parameters gradually diverge. machine processed ﬁxed number samples average parameters across jobs re-distribute result machines. repeated processed data speciﬁed number epochs e.g. useful deﬁne effective learning rate parallel procedure learning rate used individual jobs divided number jobs increase number jobs order linear speed need increase learning rate proportional effective learning rate stays same. concept parameter averaging parameter update individual gets diluted -fold. reason parameter-averaging instead summing parameter changes respective jobs concern stability. imagine direction parameter space hessian large enough stochastic gradient descent nearly reaches equilibrium processing samples. jobs processing examples summing parameter changes parameters close equilibrium value opposite direction times farther away start. clear would lead divergence. other less directly relevant issues relegated appendix namely versus gpu-based data randomization issues generalized model averaging mixture components a.k.a. sub-classes input data normalization parameter initialization sequence training online decoding i-vectors found training dnns speech recognition exponentially decreasing learning rate works well independently found thing. generally learning rate decreases factor training exponential schedule. unless mentioned otherwise experiments reported learning rate starts ends specify number epochs advance; typically number range note experiments presented separately tune learning rate schedule ng-sgd done extensive experiments past smaller setups learning rate schedule tuned independently; circumstances found ng-sgd helpful. feasible repeat experiments large-scale setup. common pathology deep learning training parameters suddenly start getting large objective function negative inﬁnity. known parameter divergence. normal solution decrease learning rate start training again inconvenient. avoid pathology modiﬁed procedure enforce maximum parameter change minibatch. limit tends active early training particularly layers closer output. provided details appendix technically speaking natural gradient means taking step along gradient riemannian parameter surface follows curving path conventional parameter space extremely hard compute. however previous work used term natural gradient describe methods like approximated inversefisher matrix learning rate matrix follow precedent calling method natural gradient. matrix component learning-rate; convenient proofs keep separate rather absorbing acceptable random bound eigenvalues below positive constants known advance independently sampled given parameter prove convergence kinds conditions using scalar learning rate general learning-rate matrix function data sample currently processing prevent convergence local optimum. example this matrix systematically smaller particular type training data would clearly bias learning downweighting data. reasons statistical learning theory related natural gradient idea want learning-rate matrix inverse fisher information matrix. example fisher matrix directly deﬁned situations learning distribution opposed classiﬁcation problems current one. suppose discrete continuous variable whose distribution modeling probability likelihood given parameters fisher information matrix deﬁned expected outer product derivative called score information theory. part justiﬁcation fisher matrix that certain conditions fisher matrix identical hessian; obvious inverse hessian would good gradient descent direction. conditions quite stringent include model correct value corresponding true data distribution; even conditions apply fisher information matrix sense dimensionally hessian– transforms changes parameterization– inverse still good choice learning-rate matrix. quite easy generalize notion fisher matrix prediction task write data distribution assume independently known hard score equals since depend additional term involving expectation take computing fisher matrix taken joint distribution argument also appears still generally compute quantity analogous fisher matrix objective function even represent log-probability log-likelihood; still matrix transforms hessian changes variables i.e. inverse still reasonable choice learning-rate matrix. large-scale problems dnns speech recognition millions parameters even inversion fisher matrix impractical would take time parameter dimension. however practical deal factored forms previous literature this. fisher matrix divided diagonal blocks block approximated low-rank matrix. idea diagonal blocks also explored block weight matrix; approach uses idea. unpublished manuscript authors attempted show analytically certain quite strong assumptions fisher matrix single-hidden-layer neural network form kronecker product. although interested general networks considered kronecker product also appear factorization fisher matrix. note ways natural gradient without factorizing fisher information matrix willing accept signiﬁcantly increased time iteration. example uses truncated newton method approximate multiplication inverse fisher matrix. factored form fisher matrix follows given neural network weight matrices divide fisher matrix diagonal blocks weight matrix. consider i’th diagonal block fisher matrix corresponding parameters weight matrix assume separate bias term i’th block fisher matrix kronecker product symmetric positive deﬁnite matrices whose dimension output dimension whose dimension input dimension factorize matrices low-rank symmetric matrix plus multiple identity matrix. write approximated fisher matrix form factorized form λi+xxt order appear kronecker product depends vectorize weight matrices– row-wise column-wise. practice don’t ever deal explicitly kronecker products vectorized weight matrices algorithm choice doesn’t matter. hard show fisher matrix factored inverse factored way. simple method estimate fisher matrix samples minibatch currently training holding current sample avoid bias. done surprisingly efﬁciently. details appendix generally online method signiﬁcantly faster gpus usually seems lead faster learning probably less noisy estimate fisher matrix. describe simple method easier understand helps motivate online method. derivative objective function w.r.t. output i’th weight matrix computed current sample input weight matrix acts quantities naturally occur backpropagation. rather processing training examples time process minibatches instead vector-valued derivatives inputs matrices corresponds quantities update follows simple method given minibatch vectors element minibatch estimate fisher-matrix factors holding sample multiplication inverses return modiﬁed vectors ¯xti. natural gradient methods want prevent fisher-matrix multiplication affecting overall magnitude update much compared step-sizes standard sgd. several reasons this conventional convergence-proof techniques require matrix component learning rate matrix eigenvalues bounded constants known advance cannot guarantee unmodiﬁed fisher matrix. scaling introduces slight problem convergence proofs. issue sample affect value learning-rate matrix mentioned before permissible general per-sample learning rate function sample itself. however don’t view practical problem never minibatch size less resulting bias tiny. versions ng-sgd smooth estimates factors fisher matrix adding multiple identity matrix inverting them. simple method necessary general fisher matrix estimated minibatch full rank. online method strictly necessary deal factorization fisher matrix already contains multiple unit matrix found adding additional multiple unit matrix simple method improve convergence training. cases smoothing following form. rd×d fisher matrix factor estimated directly data uncentered covariance quantities instead using fisher-matrix factor instead used stop smoothed ever exactly zero. smooth fisher identity matrix scaled times average diagonal element found tuning experiments relatively large value suitable wide range circumstances simple online methods even settings noise problem– e.g. large minibatch sizes. interpretation fairly large using smaller normal learning rate directions quantities quite high covariance relatively constant learning rate remaining directions. assume distribution quantities gaussian independent hard show fisher matrix form quantities correspond uncentered covariances quantities inverse-fisher form replacing course conditions won’t hold practical deep learning applications believe it’s reasonable factorization. could show experimentally follows given task. could make linear change variables make approximated fisher matrix equal unit matrix measure eigenvalue distribution full fisher matrix co-ordinates. believe eigenvalue distribution transformed fisher matrix would probably much closerly centered around change variables. since motivation work published practical allocated effort towards type experiment. regarding convergence using factored-fisher learning rate matrices think easily provable slightly modiﬁed form method would converge similar conditions unmodiﬁed sgd. smoothing constant give bound ratio largest smallest eigenvalues factors; using together rescaling section bound eigenvalues rescaled factors. multiplying together lower upper bounds eigenvalues overall inverse-fisher matrix learning-rate matrix necessary fisher matrix randomly chosen independent identity current sample. unfortunately quite true rescaling done minibatch level; mentioned section would problem proofs. mentioned would easy rescaling factor previous minibatch; gives back independence cost longer easy bounds upper lower eigenvalues rescaled factors. alternately could keep algorithm prove instead parameter value converge differ much sense optimum true objective function minibatch size gets large. might interesting things online natural gradient method described appendix estimate uncentered covariance matrices factored form online estimation covariance matrices involves multiplying weighted combination observed covariance matrix current minibatch previous value factored approximation like matrix version power method probably analysis would done initially steady state addition assume inﬁnite minibatch size covariance matrix equals expected value conﬁdent could show stable ﬁxed point update equations gives suitable sense closest approximation covariance; little effort updates converge probability best approximation. analysis ﬁnite minibatch size would involve different methods. noise ﬁnite forgetting factor would never converge true value; might possible deﬁne objective function measures kind goodness approximation something convergence distribution objective function. frequently used modiﬁcation momentum helpful preventing parameter divergence momentum allows higher effective learning rate before parameter divergence encountered. original reason none experiments involve momentum found quite hard successfully incorporate momentum multi-threaded parameter updates needed version training method; likely reason downpour momentum. developed methods prevent instability– namely limit parameter change layer minibatch natural gradient method itself. another popular modiﬁcation adagrad method divides learning rate parameter standard deviation gradients parameter averaged time naturally gives learning rate schedule believed theory optimal well giving separate learning rates diagonal element. reasons felt adagrad unlikely helpful large-scale speech recognition. firstly learning rate found empirically inferior exponentially decaying learning rate secondly p-norm nonlinearities non-saturating don’t believe networks susceptible kind pathologies would make neurons layer require higher learning rates others. also true different hidden layers special properties p-norm networks here. essentially reason believe directions requiring higher learning rates others concerned interesting action particular type network diagonal– cannot captured diagonal matrix. investigated adagrad smooth estimates factors fisher matrix identity diagonal matrix. show experiments speech recognition setup called fisher english englishlanguage conversational telephone speech sampled transcribed quick relatively low-quality way. total amount training data hours test held-out subset data hours long deﬁned ourselves. detailed argument involves scale invariance network output w.r.t. parameters layer; invariance learning procedure respect scaling parameters layer scaling learning rate time; notion parameters layer tend grow size parameter noise learning rate high actually another reason this. previously derived efﬁcient online update factored fisher matrix low-rank plus diagonal form diagonal term caused math become signiﬁcantly complicated. system based mfcc features spliced across frames processed lda+mllt -dimensional features adapted feature-space mllr training test time. explanation terms normal system build steps. systems used phonetic context decision tree context-dependent states; system gaussians total. system uses speaker adapted features system requires ﬁrst pass decoding adaptation. -dimensional features spliced across frames context used input dnn. p-norm hidden layers p-norm dimensions respectively i.e. nonlinearity reduces dimension tenfold. sub-classes number parameters million. trained epochs learning rate varying trained parallel jobs online natural gradient system trained samples outer iteration machine. system trained online decoding setup geared towards applications reduced latency important audio data must processed strictly order received. input features equivalent unadapted un-normalized dimensional log-mel ﬁlterbank features spliced frames plus -dimensional i-vector representing speaker characteristics extracted speaker’s audio including current time. results shown here include previous utterances speaker conversation computing i-vector. system intended real-time decoding single limit number parameters using hidden layers p-norm dimensions sub-classes total million parameters. trained using online ng-sgd parallel jobs epochs learning rate decreasing exponentially experiments based setup. server hardware fairly typical majority dell poweredge servers intel xeon cpus cores each running .ghz; single nvidia tesla card providing gpus– corresponds single machine notation becomes incidental co-located. also similar machines cards reporting time taken report slightly optimistic ﬁgures obtained running jobs faster gpus. main result figure plot objective function versus amount training data processed parallel training method without natural gradient jobs. order keep effective learning rate constant make initial/ﬁnal learning rates proportional number jobs default learning rates corresponding -job case. natural gradient method always helps– ng-sgd curves plain-sgd curves. also using online natural-gradient curves shown figure close jobs– i.e. processing amount data different numbers jobs objective function; however -job runs converge little slower. thus small getting linear speed number machines time taken epoch proportional gets larger around need epochs improvement speedup becomes sub-linear. plot also shows simple online natural gradient converge show ﬁnal word error rates table ng-sgd sensitive number jobs. figure shows plots figure time x-axis. simulated clock time obtained multiplying time taken outer iteration training number outer iterations; actual clock time depends queue load. time outer iteration seconds plain seconds online ng-sgd seconds plain ng-sgd measured gpu. circles mark training epochs. makes possible data parallelization method periodically average redistribute parameters across multiple runs. enables train parallel even machines lack fast interconnections. although show results setup conﬁdent based past experience holds true types neural network sigmoid activations) improves ﬁnal results well convergence speed. good explanation parallel training method works using natural gradient except statements prevents large parameter steps robust reorderings training relevant. would like thank karel vesely wrote original nnet neural network training code upon work based; ehsan variani pegah ghahremani work cuda kernels; hagen soltau oriol vinyals steven eliuk fruitful discussions; many others numerous mention contributed aspect neural setup kaldi generally. authors supported darpa bolt contract hr--c- iarpa babel contract wnf--c-. gratefully acknowledge support cisco systems inc. google inc. funds used computer equipment cloud computing time used development methods. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied darpa iarpa dod/arl u.s. government. bahl brown souza mercer maximum mutual information estimation hidden markov model parameters speech recognition. acoustics speech signal processing ieee international conference icassp’. davis steven mermelstein paul. comparison parametric representations monosyllabic word recognition continuously spoken sentences. acoustics speech signal processing ieee transactions dean jeffrey corrado greg monga rajat chen devin matthieu quoc mark ranzato marc’aurelio senior andrew tucker paul yang andrew large scale distributed deep networks. neural information processing systems dehak najim kenny patrick dehak r´eda dumouchel pierre ouellet pierre. front-end factor analysis speaker veriﬁcation. audio speech language processing ieee transactions glorot xavier bengio yoshua. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics maas andrew hannun awni andrew rectiﬁer nonlinearities improve neural network acoustic models. icml workshop deep learning audio speech language processing saon george soltau hagen nahamoo david picheny michael. speaker adaptation neural network acoustic models using i-vectors. automatic speech recognition understanding ieee workshop ieee seide frank gang chen dong. feature engineering context-dependent deep neural networks conversational speech transcription. automatic speech recognition understanding ieee workshop ieee senior andrew heigold georg ranzato marc’aurelio yang empirical study learning rates deep neural networks speech recognition. acoustics speech signal processing mentioned section interface described follows. given matrix represents element minibatch inversefisher multiplication return modiﬁed matrix core inverse-fisher multiplication this fisher matrix estimated rows i.e. minibatch size extend basic idea adding smoothing identity matrix scaling output frobenius norm input. section describe compute simple natural gradient method without considering compute efﬁciently. described section smooth fisher matrix identity. deﬁning note computing scalars without holding current sample violating rule randomly sampled learning rate matrix independent current sample. however since always fairly large minibatch size scalar quantities don’t believe small amount contamination takes place signiﬁcantly bias training. fact might turn difﬁcult modify equations properly hold current sample purposes don’t believe would perceptibly affect results haven’t gone trouble this. derived rather surprising row-space version formulation expanding inverted expression right column-space expression using morrison-woodbury formula simplifying resulting expression. efﬁciency choose method minibatch size greater dimension method otherwise. formula derived expressing rank-one correction computing corresponding correction differs corresponding turns correction direction itself becomes scalar multiple deﬁning row-index working cpus small minibatch sizes large hidden-layer dimensions computation efﬁcient comprise time overall backprop computation. however using gpus larger minibatch sizes take majority time. even though typically takes considerably less half total ﬂoating point operations overall computation contains matrix inversion matrix inversions easy compute gpu. online method describe designed solve efﬁciency problem. interface online natural-gradient method essentially simple method user provides matrix return matrix that’s multiplied inverse-fisher rescaled frobenius norm again corresponds element minibatch column dimension corresponds column dimension weight matrices. difference simple method online method stateful maintain running estimate fisher matrix. time process minibatch fisher matrix estimated previous minibatches; update estimate using current minibatch. single neural number separate copies state need maintain corresponds twice number trainable weight matrices neural quantities equation input rn×d minibatch size column size weight matrix we’re updating introduce user-speciﬁed parameter rank non-identity part fisher matrix. subscript correspond minibatch. deﬁne rr×d rr×r estimated online data; orthonormal rows diagonal nonnegative. we’ll estimate quantities online data good estimate covariance rows quantities. good low-rank approximation obvious would make correspond eigenvalues corresponding eigenvectors would slow. instead method inspired power method ﬁnding eigenvalue matrix. iteration compute rr×d. useful think containing eigenvector scaled corresponding eigenvalue update scaling factors puts diagonal puts corresponding eigenvectors rows rt+. work correct amount unit-matrix factorization covariance matrix subtract amount diagonals dt+. give equations below. observant readers might noted would seem straightforward singular value decomposition instead symmetric eigenvalue decomposition ytyt speed. note point still unknown. desired covariance ensuring dimension corresponding value inner product ft+r equals precisely true convergence. choose order ensure value worked previous section described computing online natural gradient method; describe compute efﬁciently. essential idea reduce multiplication multiplications matrix since typically much smaller quite efﬁcient. also address efﬁciently keep matrices updated level optimizing matrix expressions. section mostly derivation likely interest someone considering implementing method. section below summarize algorithm derive here. recall symmetric matrix rr×r deﬁned involved update factorization. following expressions going useful computing ﬁrst appears sub-expression convenience state dimensions quantities below strategy compute symmetric quantities transfer compute using expression done symmetric eigenvalue decomposition cpu. repeat equation convenience note might seem like factor missing second term fact fact commutes move left we’re using computed time transferred gpu; compute efﬁciently scaling rows adding multiply gpu. noticed invariance rtrt sometimes lost roundoff. proper analysis roundoff algorithm something time describe detect problem practice. speed following operations diagonal matrix condition number greater elements ﬂoored mentioned note computations describe paper done single precision. part parentheses computed transferred cpu. element differs corresponding element unit matrix consider sufﬁciently orthogonal nothing more. otherwise cholesky decomposition compute reorthogonalizing factor copy mwt+ reorthogonalize. re-orthogonalization happens extremely rarely usually something already happened parameter divergence. implementation don’t bother dumping state’ computation disk process reinitializes ﬁrst minibatch processes. initialize closely approximate covariance ﬁrst minibatch features. done taking mentioned fast computing quantities needed compute using compute ρ′t+ using compute side effect fact need ¯xti output squared norm ¯xti. required enforce maximum parameter change minibatch described section suppose we’ve already computed using compute inner products rows expression obtained expanding using moving left recognizing sub-expressions already computed. case updating parameters fisher-matrix factorization need efﬁcient compute directly; course done operations require matrix multiply. scaling factor scale quantities square equal quantities ¯xti we’ll need enforcing maximum parameter change. written geared towards operation using also support operation cpus implementation multithreaded. case consider interaction multithreaded code stateful nature computation. wanted avoid bottleneck different threads wait update parameters sequentially. solution part computation update parameters lock fails simply apply ﬁxed inverse-fisher matrix don’t update fisher-matrix parameters since model parameters don’t move fast don’t expect make noticeable difference convergence seen evidence does. important user-speciﬁed parameters algorithm rank constant controls smoothing unit matrix. value seems work well wide variety conditions normally leave value. rank generally increase dimension vectors multiplying. experiments p-norm networks nonlinearity dimension reducing like maxout typically reducing dimension something like typical parameter matrix increase dimension something like normal rule ranks input side matrix output side. part originally tuned look diagonal matrices matrices diagonal values etii sorted greatest least etii interpreted amount input scaled certain direction space. value etii close means strongly scaling input value close means leaving unchanged. last etii value reducing like taking scaling factor applied gradient setting instead; seems unlikely make difference it’s like changing learning rate direction ﬁnal etii values normally range another conﬁgurable constant forgetting factor closer rapidly track changes fisher matrix changes parameters noise estimates. don’t want tune change minibatch size follows. user speciﬁes parameter order increase speed algorithm normally conﬁgure actually update parameters fisher matrix every minibatches except ﬁrst minibatches process always update them. summarize online natural-gradient method– summarize core part algorithm takes matrix rn×d outputs matrix rn×d. understand bigger picture back-propagation section summary ignore issues multithreading. explanation instance algorithm corresponding column dimension weight matrices; weight matrices separate copies variables describe here. running steps initialize parameters described section b... note section describe variables actually store compute need equations input rn×d despite notation require sometimes last minibatch process smaller normal size. divides exactly updating factored fisher matrix; otherwise apply don’t update. slightly versions algorithm depending whether updating fisher matrix. overwriting input next compute row-products using compute compute using next scale produce ¯xt. also output quantity ¯xti needed enforce maximum parameter change minibatch constraint. point we’re using transfer symmetric matrices cpu. compute small derived quantities using using well diagonal hard. point compute symmetric matrix using expression looks scary computed time. compute row-products using compute using obtain scaling factor using compute main output per-row inner products output equal next compute ρ′t+ using using using computed using matrix multiply working factors point ﬂoored diagonal elements condition number after ﬂooring exceeds orthogonality check possible reorthogonalization described section above. section discuss cpu-based gpu-based versions implementation differ; section discuss randomize training examples store disk. section explain enforce maximum parameter-change minibatch; explain generalized model-averaging procedure; explain mixture components dnns; introduce method input data normalization; give details initialize parameters; give overview implemented sequence training dnns; discuss online decoding using i-vectors speaker adaptation. cpu-based computation uses typically threads order take advantage multi-core processors. threads share parameters without locks; known hogwild referred asynchronous. order prevent divergence thread processes relatively small minibatches typically size mention point formulation gradients elements minibatch rather averaging ensures make amount progress sample regardless minibatch size gives consistent results changing minibatch size. need limit minibatch size multithreaded case understood follows think effective minibatch size minibatch size times number threads. product learning rate effective minibatch size relevant stability update becomes large increased danger divergence. spinning hard disks sequential data access orders magnitude efﬁcient random data access access small ﬁles. kaldi toolkit hard ensure high-volume data access takes form sequential reads writes large ﬁles. neural network training keep data access sequential dumping pre-randomized training examples disk. training example corresponds class label together corresponding input features including left right temporal context needed network. randomization done entire data data accessed order epoch. probably ideal point view convergence expectation large amounts data same-order access affect results noticeably. break training data rougly equal-sized blocks number parallel jobs speciﬁed user number outer iterations epoch chosen ensure number samples processed iteration close user-speciﬁed value process randomly distributing data blocks ensuring order randomized within block done parallel; won’t give details here problem straightforward nothing particularly special method. reduce disk network access compress features disk byte ﬂoat using lossy compression method. mentioned section order prevent instability parameter divergence enforce maximum parameter-change minibatch applied layer network separately. explain done. don’t claim exceptionally good method preventing excessive parameter changes describe anyway sake completeness. change standard would give equal derivative objective function minibatch multiplied learning rate enforce maximum parameter chanbge scale change scalar would like choose ensure ||αt∆t||f exceed speciﬁed limit frobenius norm. however don’t implement scheme exactly described would involve creating temporary matrix store product matrices order compute norm don’t want incur penalty. max-change-per-minibatch user-speciﬁed maximum parameter-change minibatch. empirically found tends necessary increase max-change-per-minibatch using larger minibatch size simplify conﬁguration process deﬁne minibatch size. always max-change-per-sample experiments reported here. clarify method interacts natural gradient methods described section natural gradient implemented modiﬁcation matrices simply apply maximum-change logic modiﬁed quantitities. convergence theory stochastic gradient descent suggests that convex problems take last iteration’s model parameters average iterations improve convergence rate particularly ‘poorly-conditioned’ problems applicable non-convex problems ours suggest related method. mentioned above deﬁne outer iteration length time takes jobs process samples outer iteration dumps ﬁnal model disk average produce single model. store models outer iteration. training instead choosing model ﬁnal outer iteration take models last outer iterations search generalized weighted combination models optimizes objective function subset training data– tried using validation data here task found worked best training data. generalized weighted combination mean parameters weighted combination parameters input models layer different weighting factors. thus models layers number parameters learn data subdset details idea neural networks allowing posterior speech state written posterior sub-classes analogous gaussians gmm. halfway training model increasing dimension softmax layer user-speciﬁed number greater number classes softmax layer introduce sum-group layer sums input ﬁxed groups indexes produce posterior class posteriors hidden sub-classes. also tried sharing sub-classes across classes groups helpful. rather distributing sub-classes evenly allocate sub-classes common classes. allocate proportional power count class training data; based rule allocate gaussians gmms. initializing parameters mixed-up ﬁnal weight matrix make correspond quite closely original weight matrix. weight matrix corresponds weight matrix plus small noise term allow values rows diverge; modify bias term normalize fact classes sub-classes others. generally found slightly improves results again focus current paper won’t showing experimental results this. recent experimental results show mixture components diverged sufﬁciently regard truly distinct; method helping effect decreasing learning rate ﬁnal-layer parameters corresponding higher-frequency classes. describe completeness. note added publication iclr workshop paper. experiments conducted minhua discovered mixture components diverging sufﬁciently regarded distinct mixtures observed small improvement likely stabilizing effect update parameters higher-count classes splitting gradient multiple pieces. able improve results slightly removing mixture-component adding instead ﬁxed scaling separate component/layer ﬁnal weight matrix softmax scales proportional renormalized average mentioned training neural networks helpful normalize input data zero mean important dimensions input data larger variance. wanted generic achieve would invariant arbitrary afﬁne transforms input. technique developed requires statistics within-class covariance between-class covariance accumulated class-labeled data preparation multi-class linear discriminant analysis assume follows already normalized data zero-mean. technique describe make sense number classes much smaller feature dimension; fortunately case much larger– give typical numbers. suppose multi-class actually reduce dimension. would transform space unit diagonalized. suppose space diagonal elements total covariance dimension desirable property data covariance higher more important directions doesn’t drop fast we’d like unimportant directions– never goes method lda-type creating transform matrix described above singular value decomposition ﬂoor singular values reconstruct again. motivation avoid rarely encountered pathology occurs training data covariance close singular leads transform large elements might produce large transformed data values mismatched test data roundoff. step rarely ﬂoors handful singular values little effect transform. decided implement generative pre-training well established improves results small datasets understanding amount training data gets larger eventually gives improvement compared suitable random initialization discriminative layer-wise backpropagation could published reference this; something told verbally. refer speciﬁcally speech recognition tasks; apply tasks like computer vision much larger networks used. fact alternative nnet implmentation dnns kaldi support pre-training small datasets generally gives slightly better results nnet implementation speak here. larger datasets nnet implementation eventually becomes impractical takes long detailed comparison beyond scope paper. instead pre-training described layer-wise backpropagation means initialize network hidden layer train short time remove ﬁnal softmax layer randomly initialized hidden layer existing hidden layer; train short time again; repeat process desired number hidden layers. similar standard deviation weights fan-in weight matrix; initialize parameters softmax layers zero. note found essential discard parameters ﬁnal softmax layer adding hidden layer prescribed smaller datasets improve results versus layer-wise initializing last layer network network trained another large dataset possibly another language. initializing typically best larger network otherwise would used. noticed sometimes outer iteration immediately following random initialization parameters parameter averaging degrade rather improve objective function modiﬁed parallel training method iterations instead averaging parameters choose best objective function computed subset data trained sequence training term meaning dnns discriminative training speech recognition community gmms. collective term various objective functions used training dnns sequence tasks make sense whole-sequence level. contrasts cross-entropy objective function which given ﬁxed viterbi alignment states easily decomposes frames training data. gmm-based speech recognition term discriminative training contrasts maximum likelihood estimation; dnn-based speech recognition contrasts cross-entropy training. popular classes sequence/discriminative objective functions maximum mutual information -like objective functions properly called conditional maximum likelihood form utterances log-posterior correct word sequence utterance given model data. include popular ’boosted’ variant inspired margin-based objective functions. minimum bayes risk -like objective functions popular variants include minimum phone error state-level minimum bayes risk form expectation given data model edit-distance type error. compute derivative w.r.t. model parameters posteriors different sequences vary model parameters. state-level minimum bayes risk although also implemented minimum phone error boosted high-level details lattice-based training procedure similar note paper describe alternative implementation deep neural nets exists within kaldi; paper alternative nnet setup. items common sequence training described paper include following parallel multiple machines periodic model averaging. rather randomizing utterance level split lattice small pieces possible given lattice topology excise parts lattice would contribute nonzero derivatives; randomize order remaining pieces. ensure layers network trained amount modify learning rates order ensure relative change parameters outer iteration layer; geometric average constrained equal user-speciﬁed ﬁxed learning rate mentioned above. something note connection learning rates p-norm networks since network output invariant scaling parameters p-norm layers since generalized weighted combination section output arbitrarily scaled weights hard specify advance suitable learning rate. solve problem ﬁrst scale parameters p-norm layers expected square randomly chosen matrix element one. sequence training frames minibatch drawn independently training data consist sequential frames utterances simple ng-sgd method applicable apply online method. speech recognition applications sometimes necessary process data continuously arrives latency response. makes necessary algorithms used dependencies backwards time. backwards-in-time dependencies conventional neural recipes e.g. reported include cepstral mean normalization subtract mean input features; fmllr adaptation also known constrained mllr adaptation baseline system compute likelihood-maximizing linear transform features. although online versions things online gmm-based decoding makes system complex ideal combination dnns. order system easier turn online algorithm i-vectors additional input neural network addition spliced cepstral features. done before e.g. saon bacchiani i-vector vector normally dimension range several hundred represents speaker characteristics form suitable speaker identiﬁcation extracted maximum likelihood conjunction single mixture-of-gaussians model parameters factor analysis model extracts i-vectors trained without supervision large number audio recordings. case extract i-vectors dimension i-vector extractor trained switch online extraction i-vectors training actual inputs setup normally consist i-vector plus frames frequency cepstral coefﬁcients without cepstral mean normalization. authors ﬁlterbank energies; mfcc features equivalent ﬁlterbank energies mfccs linear transform input data normalization invariant transforms; mfccs easily compressible training example data structure compresses input features. order train models well matched per-speaker decoding statistics previous utterances speaker included i-vector estimation per-utterance decoding make fresh start time generally train splitting speakers fake speakers utterances. experiments number datasets generally found method gives performance previous recipe trained frames standard -dimensional features consisting mean-normalized mfcc features processed mllt speaker adapted fmllr prefer convenience applications convenience cross-system transfer learning.", "year": 2014}