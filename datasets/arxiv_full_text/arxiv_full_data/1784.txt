{"title": "Latent Sequence Decompositions", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes sequences with variable lengthed output units as a function of both the input sequence and the output sequence. We present a training algorithm which samples valid extensions and an approximate decoding algorithm. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9% WER compared to a character baseline of 14.8% WER. When combined with a convolutional network on the encoder, we achieve 9.6% WER.", "text": "sequence-to-sequence models rely ﬁxed decomposition target sequences sequence tokens words word-pieces characters. choice tokens decomposition target sequences sequence tokens often static independent input output data domains. potentially lead sub-optimal choice token dictionaries decomposition informed particular problem solved. paper present latent sequence decompositions framework decomposition sequences constituent tokens learnt training model. decomposition depends input sequence output sequence. training model samples decompositions incrementally left right locally sampling valid extensions. experiment wall street journal speech recognition task. model achieves compared character baseline wer. combined convolutional network encoder achieve sequence-to-sequence models attention successfully applied many applications including machine translation parsing image captioning automatic speech recognition previous work assumed ﬁxed deterministic decomposition output sequence. output representation usually ﬁxed sequence words phonemes characters even mixture characters words however cases models trained towards ﬁxed decomposition output sequence. argue using ﬁxed deterministic decompositions sequence deﬁned priori. word segmented models often deal large softmax sizes rare words out-of-vocabulary words. character models overcome problem modelling smallest output unit however typically results long decoder lengths computationally expensive inference. even mixed character-word models unclear whether predeﬁned segmentation optimal. examples output decomposition function output sequence. acceptable problems translations inappropriate tasks speech recognition segmentation also informed characteristics inputs audio. want model capacity ﬂexibility learn distribution sequence decompositions. additionally decomposition sequence variable length tokens deemed probable. example language naturally represented word pieces rather individual characters. many speech language tasks probably efﬁcient model output unit rather separate output units word piece models also naturally solve rare word problems similar character models. output sequence decomposition function input sequence output sequence example speech choice emitting word piece separate tokens function current output word well audio signal present latent sequence decompositions framework. assume ﬁxed decomposition output sequence rather learns decompose sequences function input output sequence. output sequence decomposed latent sequence decompositions using dictionary variable length output tokens. framework produces distribution latent sequence decompositions marginalizes training. test inference best decomposition output sequence using beam search likely output sequence model. section describe formally. input sequence output sequence latent sequence decomposition latent sequence decomposition consists sequence constructed token space. token need length rather framework expect tokens different lengths. speciﬁcally singleton tokens length largest output token. would typically english characters would word pieces give concrete example consider tokens cat}. tokens word represented sequence sequence alternatively single token cat. since appropriate decomposition word known priori decomposition latent. decomposition collapses target output sequence using trivial collapsing function collapse. clearly decompositions collapse sequence using non-trivial token combinatorially large. known unique correct segmentation given pair could simply train model output ﬁxed deterministic decomposition however problems know best possible decomposition indeed possible output correctly decomposed multiple alternative valid segmentations. example end-to-end typically characters output unit choice word pieces better units closely align acoustic entities syllables. however appropriate decomposition given pair often unknown. given particular best could even change depending input sequence want learn probabilistic segmentation mapping model produces distribution decompositions given input sequence objective maximize log-likelihood ground truth sequence accomplish factorizing captures path decompositions collapses exponential number decompositions exact inference search intractable nontrivial token sequence length |y|. describe beam search algorithm approximate inference decoding section equation uses identity ∇θfθ fθ∇θ assuming equation gives unbiased estimator gradient. tells sample latent sequence decomposition model’s posterior constraint valid sequence collapses i.e. collapse train model sample compute gradient using backpropagation. however sampling difﬁcult. exactly computationally expensive would require sampling correctly posterior would possible using particle ﬁltering like algorithm would require full forward pass output sequence this. instead implementation heuristic sample output time step producing tokens sample left-toright fashion. words sample valid extensions time step start training left-to-right sampling procedure good approximation posterior since next step probabilities time step include probabilities future paths point. example consider case target word vocabulary includes possible characters tokens cat. time step valid next step options relative probabilities reﬂect possible sequences cat* respectively start ﬁrst time step model. sets sequences include sequences target sequence cat. thus sampling distribution step biased procedure. however training proceeds model places mass correct hypotheses relative probabilities model produces valid extensions gets closer posterior. practice model trained method quickly collapses using single character targets never escapes local minima. thus follow ǫ-greedy exploration strategy commonly found reinforcement learning literature sample mixture uniform distribution valid next tokens relative probability using uniform distribution varied training. modiﬁcation model learns longer n-grams characters appropriately shown later sections. work model latent sequence decompositions attention-based seqseq model output token modelled conditional distribution previously emitted tokens input sequence using chain rule input sequence processed encodernn network. encodernn function transforms features higher level representation experimental implementation encodernn stacked bidirectional lstm hierarchical subsampling decodernn produces transducer state function previously emitted token previous attention context previous transducer state si−. implementation decodernn lstm function without peephole connections. attentioncontext function generates content-based attention network energies computed function encoder features current transducer state energies normalized attention distribution attention context created weighted linear however obviously intractable non-trivial token space sequence lengths. simply approximate decoding best word piece sequence collapsing corresponding word sequence table wall street journal test eval word error rate varying sized word piece vocabulary without dictionary language model. compare latent sequence decompositions versus maximum extension decomposition. models learn better decompositions compared baseline character model maxext decomposition appears sub-optimal. experimented wall street journal task. used standard conﬁguration train dataset training validation eval test evaluation. input features dimensional ﬁlterbanks computed every delta delta-delta acceleration normalized speaker mean variance generated kaldi encodernn function layer blstm lstm units per-direction time factor reduction. decodernn layer lstm lstm units. weight matrices initialized uniform distribution bias vectors gradient norm clipping used gaussian weight noise weight decay used adam default hyperparameters described however decayed learning rate used workers asynchronous tensorflow framework monitor word error rate convergence report corresponding eval wer. models took around days converge. created token vocabulary looking n-gram character counts training dataset. explored took tokens based count frequencies found minor differences based vocabulary size word piece experiments used vocabulary size word piece experiments used vocabulary size additionally restrict hspacei unigram token included word pieces forces decompositions break word boundaries. table compares effect varying sized word piece vocabulary. latent sequence decompositions models trained framework described section maxext decomposition ﬁxed decomposition. maxext generated left-to-right fashion step longest word piece extension selected vocabulary. maxext decomposition shortest possible sequence however deterministic decomposition easily generated linear time on-the-ﬂy. decoded models simple n-best list beam search without external dictionary language model baseline model simply unigram character model achieves wer. word piece vocabulary model perform best yielding relative improvement baseline character model. none maxext models beat character model baseline suggesting maximum extension decomposition poor decomposition choice. however models perform better baseline suggesting framework able learn decomposition better baseline character decomposition. also look distribution characters covered based word piece lengths inference across different sized word piece vocabulary used training. deﬁne distribution characters covered percentage characters covered word pieces length across test exclude hspacei statistic. figure plots figure distribution characters covered n-grams word piece models. train latent sequence decompositions maximum extension models sized word piece vocabulary measure distribution characters covered word pieces. bars solid represents models bars star hatch represents maxext models. maxext models prefer sized word pieces cover majority characters. maxext models prefers longer word pieces cover characters compared models. distribution }-ngram word pieces model decides decompose sequences. model trained bigram word piece vocabulary found model prefer bigrams characters decomposition. suggest character vocabulary best vocabulary learn from. best model word piece vocabulary covered word characters time using sized word pieces respectively. word piece vocabulary model model uses sized word pieces cover approximately characters. suspect used larger dataset could extend vocabulary cover even larger maxext model trained greedily emit longest possible word piece consequently prior meant model prefer emit long word pieces characters. decomposition results shorter length slightly worse character baseline. suggest much shorter decompositions generated maxext prior best decomposition. falls onto principle best decomposition function function case segmentation function acoustics well text. table compares results published end-to-end models. best model achieved reinforce optimization previously best reported basic seqseq model achieved task loss estimation baseline also seqseq model achieved wer. main differences models convolutional locational-based table wall street journal test eval word error rate results across connectionist temporal classiﬁcation sequence-to-sequence models. latent sequence decomposition models word piece vocabulary convolutional neural network model deep residual connections batch normalization convolutions. best end-to-end model seqseq wer. model using word piece vocabulary achieves relatively better baseline seqseq model. combine model model achieve combined relatively better baseline seqseq model. numbers reported without language model. singh mcgraw built probabilistic pronunciation models hidden markov model based systems. however models still constraint conditional independence markovian assumptions hmm-based systems. connectionist temporal classiﬁcation based models assume conditional independence rely dynamic programming exact inference. similarly ling latent codes generate text also assume conditional independence leverage dynamic programming exact maximum likelihood gradients. models learn output language language distribution multimodal. seqseq models makes markovian assumptions learn multimodal output distributions. collobert zweig developed extensions used word pieces. however word pieces used repeated characters decompositions ﬁxed. word piece models seqseq also recently used machine translation. sennrich used word pieces rare words used word pieces words however decomposition ﬁxed deﬁned heuristics another model. decompositions models also function output sequence decomposition vinyals used seqseq outputs sets output sequence unordered used ﬁxed length output units; decompositions maintain ordering variable lengthed output units. reinforcement learning learn different output sequences yield different task losses. however methods don’t directly learn different decompositions sequence. future work incorporate task loss optimization methods. presented latent sequence decompositions framework. allows learn decompositions sequences function input output sequence. presented biased training algorithm based sampling valid extensions ǫ-greedy strategy approximate decoding algorithm. wall street journal speech recognition task sequenceto-sequence character model baseline achieves model achieves using deep convolutional neural network encoder achieve wer. thank ashish agarwal philip bachman dzmitry bahdanau eugene brevdo chorowski jeff dean chris dyer gilbert leung mohammad norouzi noam shazeer luke vilnis oriol vinyals google brain team many insightful discussions technical assistance. martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. software available tensorﬂow.org. dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. international conference learning representations dzmitry bahdanau chorowski dmitriy serdyuk philemon brakel yoshua bengio. endto-end attention-based large vocabulary speech recognition. ieee international conference acoustics speech signal processing dzmitry bahdanau dmitriy serdyuk philemon brakel rosemary chorowski aaron courville yoshua bengio. task loss estimation sequence prediction. international conference learning representations workshop william chan navdeep jaitly quoc oriol vinyals. listen attend spell neural network large vocabulary conversational speech recognition. ieee international conference acoustics speech signal processing kyunghyun bart merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwen yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. conference empirical methods natural language processing alex graves santiago fernandez faustino gomez jurgen schmiduber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. international conference machine learning sebastien jean kyunghyun roland memisevic yoshua bengio. using large target vocabulary neural machine translation. association computational linguistics wang ling edward grefenstette karl moritz hermann tomas kocisky andrew senior fumin wang phil blunsom. latent predictor networks code generation. association computational linguistics liang arnab ghoshal steve renals. acoustic data-driven pronunciation lexicon large vocabulary speech recognition. automatic speech recognition understanding workshop minh-thang luong christopher manning. achieving open vocabulary neural machine translation hybrid word-character models. association computational linguistics minh-thang luong ilya sutskever quoc oriol vinyals wojciech zaremba. addressing rare word problem neural machine translation. association computational linguistics mcgraw ibrahim badr james glass. learning lexicons speech using pronunciation mixture model. ieee transactions audio speech language processing daniel povey arnab ghoshal gilles boulianne lukas burget ondrej glembek nagendra goel mirko hannenmann petr motlicek yanmin qian petr schwarz silovsky georg stemmer karel vesely. kaldi speech recognition toolkit. automatic speech recognition understanding workshop marc’aurelio ranzato sumit chopra michael auli wojciech zaremba. sequence level training recurrent neural networks. international conference learning representations yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey jeff klingner apurva shah melvin johnson xiaobing lukasz kaiser stephan gouws yoshikiyo kato taku kudo hideto kazawa keith stevens george kurian nishant patil wang cliff young jason smith jason riesa alex rudnick oriol vinyals greg corrado macduff hughes jeffrey dean. google’s neural machine translation system bridging human machine translation. arxiv. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. international conference machine learning zhang william chan navdeep jaitly. deep convolutional networks end-to-end speech recognition. ieee international conference acoustics speech signal processing give hypothesis generated baseline seqseq character model latent sequence decompositions word piece model maximum extension word piece model. note shamrock’s out-of-vocabulary word shamrock in-vocabulary. ground truth shamrock’s pretax proﬁt sale hundred twenty million dollars spokeswoman said. note model generates multiple decompostions word sequence happen maxext model. c|h|a|m|r|o|c|k|’|s| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d c|h|a|m|r|o|x| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d c|h|a|m|r|o|c|k|s| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d c|h|a|m|r|o|c|k|’|s| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |o|f| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d c|h|a|m|r|o|d|’|s| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d c|h|a|m|r|o|x| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |o|f| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d c|h|a|m|r|o|c|’|s| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d c|h|a|m|r|o|c|k|s| |p|r|e|t|a|x| |p|r|o|f|i|t| |f|r|o|m| |t|h|e| |s|a|l|e| |w|a|s| |o|n|e| |h|u|n|d|r|e|d| |t|w|e|n|t|y| |f|i|v|e| |m|i|l|l|i|o|n| |d|o|l|l|a|r|s| |o|f| |s|p|o|k|e|s|w|o|m|a|n| |s|a|i|d sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|lio|n| |doll|a|r|s| |sp|ok|e|s|wo|ma|n| |said sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|li|o|n| |doll|ar|s| |sp|ok|e|s|wo|ma|n| |said sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|lio|n| |doll|a|r|s| |sp|ok|e|s|w|om|a|n| |said sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|li|o|n| |doll|a|r|s| |sp|ok|e|s|w|om|a|n| |said sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|lio|n| |doll|a|r|s| |sp|ok|e|s|w|om|a|n| |sa|id sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|lio|n| |doll|a|r|s| |sp|o|k|e|s|w|o|ma|n| |said sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|li|o|n| |doll|a|r|s| |sp|ok|e|s|w|om|a|n| |sai|d sh|a|m|ro|c|k|’s| |pre|ta|x| |pro|ﬁ|t| |fro|m| |t|h|e| |sa|l|e| |was| |on|e| |hu|n|dr|e|d| |t|we|nt|y| |ﬁv|e| |mil|li|o|n| |doll|a|r|s| |sp|ok|e|s|w|om|a|n| |sa|id sh|am|ro|ck|’s| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said sh|am|ro|x| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said sh|ar|ro|x| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said sh|e| |ro|x| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said sh|e| |mar|x| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said sh|ar|ro|ck|s| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said sh|e| |ro|ck|ed| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said sh|e| |ro|ck|s| |pre|ta|x| |pro|ﬁ|t| |from| |the| |sa|le| |was| |one| |hu|nd|red| |tw|ent|y| |ﬁve| |mil|lion| |doll|ars| |sp|ok|es|wo|man| |said", "year": 2016}