{"title": "Context Attentive Bandits: Contextual Bandit with Restricted Context", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We consider a novel formulation of the multi-armed bandit model, which we call the contextual bandit with restricted context, where only a limited number of features can be accessed by the learner at every iteration. This novel formulation is motivated by different online problems arising in clinical trials, recommender systems and attention modeling. Herein, we adapt the standard multi-armed bandit algorithm known as Thompson Sampling to take advantage of our restricted context setting, and propose two novel algorithms, called the Thompson Sampling with Restricted Context(TSRC) and the Windows Thompson Sampling with Restricted Context(WTSRC), for handling stationary and nonstationary environments, respectively. Our empirical results demonstrate advantages of the proposed approaches on several real-life datasets", "text": "consider novel formulation multiarmed bandit model call contextual bandit restricted context limited number features accessed learner every iteration. novel formulation motivated different online problems arising clinical trials recommender systems attention modeling. herein adapt standard multi-armed bandit algorithm known thompson sampling take advantage restricted context setting propose novel algorithms called thompson sampling restricted context windows thompson sampling restricted context handling stationary nonstationary environments respectively. empirical results demonstrate advantages proposed approaches several real-life datasets. sequential decision problems encountered various applications clinical trials recommender systems visual attention models decision-making algorithm must choose among several actions time point. actions typically associated side information context reward feedback limited chosen option. example image processing attention models context input image action classifying image given categories reward classiﬁcation correct otherwise. different example involves clinical trials context patient’s medical record actions correspond treatment options compared reward represents outcome proposed treatment setting inherent exploration exploitation trade-off exists many sequential decision problems traditionally formulated multi-armed bandit problem stated follows given arms possible actions associated ﬁxed unknown reward probability distribution agent selects play iteration receives reward drawn according selected arm’s distribution independently previous actions. particularly useful version contextual multi-arm bandit simply contextual bandit problem iteration choosing agent observes n-dimensional context feature vector. agent uses context along rewards arms played past choose play current iteration. time agent’s collect enough information relationship context vectors rewards predict next best play looking corresponding contexts introduce novel practically important special case contextual bandit problem called contextual bandit restricted context observing full feature vector iteration expensive impossible reasons player request observe limited number features; upper bound feature subset ﬁxed iteration within budget player choose feature subset given size. problem select best feature subset overall reward maximized involves exploring feature space well arms space. instance analysis clinical trials shows doctor patient limited number question deciding drug prescription. visual attention models involved visual pattern recognition retinalike representation used moment small region image observed high resolution image classiﬁcation performed based partial information image. furthermore recommender system setting recommending advertisement depends user’s proﬁle usually limited aspects proﬁle available. examples modeled within proposed framework assuming limited number features full context selected observed choosing iteration. overall main contributions paper include formulation bandit problem restricted context motivated practical applications limited budget information access algorithms stationary non-stationary settings restricted-context contextual bandit problem empirical evaluation demonstrating advantages proposed methods range datasets parameter settings. paper organized follows. section reviews related works. section introduces background concepts. section introduces contextual bandit model restricted context proposed algorithms stationary non-stationary environments. experimental evaluation several datasets varying parameter settings presented section finally last section concludes paper points possible directions future works. related work multi-armed bandit problem extensively studied. different solutions proposed using stochastic formulation bayesian formulation however approaches take account context. linucb neural bandit contextual thompson sampling linear dependency assumed expected reward action context; representation space modeled using linear predictors. however context assumed fully observable unlike paper. motivated dimensionality reduction task studied sparse variant stochastic linear bandits relatively small subset features relevant multivariate function optimization. similarly also considered high-dimensional stochastic linear bandits sparsity combining ideas compressed sensing bandit theory. problem formulated high-dimensional covariates efﬁcient bandit classical online learning setting actual label mislabeled sample revealed classiﬁer authors investigate problem online feature selection make accurate predictions using small number active features. finally tackles online feature selection problem addressing combinatorial optimization problem stochastic bandit setting bandit feedback utilizing thompson sampling algorithm. note none previous approaches addresses problem context restriction contextual bandit setting main focus work. background section introduces background concepts approach builds upon contextual bandit combinatorial bandit thompson sampling. contextual bandit problem. following problem deﬁned follows. time point player presented context choosing denote features deﬁning context. denote reward vector reward time associated herein primarily focus bernoulli bandit binary reward i.e. denote policy. also denotes joint distribution assume expected reward linear function context i.e. e|c] unknown weight vector associated thompson sampling also known basyesian posterior sampling classical approach multi-arm bandit problem reward choosing time assumed follow distribution parameter given prior parameters posterior distribution given bayes rule particular case thomson sampling approach assumes bernoulli bandit problem rewards parameters following beta prior. initially assumes prior beta time observed successes failures algorithm updates distribution beta fk). algorithm generates independent samples combinatorial bandit. feature subset selection aproach build upon combinatorial bandit problem speciﬁed follows associated corresponding variable indicates reward obtained choosing k-th time consider constrained subsets power associated variables variable indicates reward associated selecting subset arms time function combinatorial bandit setting viewed game player sequentially selects subsets observes rewards corresponding played subsets. deﬁne reward function used compute outcomes arms although sophisticated nonlinear rewards also possible. objective combinatorial bandit problem maximize reward time. consider stochastic model observed random variables independent distributed according unknown distribution unknown expectation outcomes distribution different arm. global rewards also random variables independent distributed according unknown distribution unknown expectation problem setting section deﬁne type bandit problem contextual bandit restricted context combinatorial task feature subset selection treated combinatorial bandit problem approach based thompson sampling contextual bandit restricted context denote value assignments vector random variables features time indexes. furthermore denote sparse vector assignments features indexes subset zeros assigned features indexes outside ˆπ)} compound-function policies function maps given subset function maps action mentioned before setting rewards binary objective contextual bandit algorithm would learn hypothesis iterations maximizing total reward. algorithm cbrc problem setting repeat drawn according distribution player chooses subset values features revealed player chooses reward revealed player updates policy propose method solving stationary cbrc problem called thompson sampling restricted context summarize algorithm combinatorial task selecting best subset features approached combinatorial bandit problem following approach subsequent decision-making task contextual bandit problem solved thompson sampling respectively. algorithm takes input desired number features well initial values beta distribution parameters iteration update values parameters represent current total number successes failures respectively sample probability success parameter corresponding beta distribution separately feature estimate mean reward conditioned variable select best subset features i∈cd goal maxcd∈c combinatorial bandit tsrc algorithm maxii∈cd note implementing subset features selected using combinatorial bandit approach switch contextual bandit setting steps choose based context consisting subset features. assume expected reward linear function restricted context e|cd] note assumption different linear reward assumption single parameter considered arms restricted context separate context vector assumed. besides difference follow approach contextual thompson sampling. assume reward choosing time follows parametric likelihood function ˜µk) posterior distribution time r|˜µ)p given multivariate gaussian distribution vbkc time point sample ddimensional vbk−) choose maximizing obtain reward update parameters distributions finally reward choosing observed relevant parameters updated. stationary environment context vectors rewards drawn ﬁxed probability distributions; objective identify subset features allowing optimal context-to-arm mapping. however objective changes environment becomes nonstationary. non-stationary cbrc setting assume rewards distribution change certain times remain stationary changes. given non-stationarity reward player continue looking feature subsets allow optimal context-arm mapping rather converge ﬁxed subset. similarly stationary cbrc problem described earlier iteration context describes environment player chooses subset feature observes values features sparse vector features outside zero. given player chooses reward played revealed. reward distribution non-stationary described above reward distributions change points unknown. make speciﬁc simplifying assumptions change points occur every time points i.e. windows stationarity ﬁxed size. windows tsrc algorithm similarly tsrc algorithm proposed earlier stationary cbrc problem algorithm nonstationary cbrc uses thompson sampling best features context. algorithms practically identical except following important detail instead updating beta distribution number successes failures accumulated beginning game successes failures within given stationarity window counted. resulting approach called window thompson sampling restricted context wtsrc. empirical evaluation empirical evaluation proposed methods based four datasets machine learning repository covertype cnae- internet advertisements poker hand order simulate data stream draw samples dataset sequentially starting beginning time draw last sample. round algorithm receives reward instance classiﬁed correctly otherwise. compute total number classiﬁcation errors performance metric. figures presented later section used parameter called sparsity denote percent features selected resulting sparsity levels respectively. following sections present results ﬁrst stationary non-stationary settings. stationary case table summarizes results stationary cbrc setting; represents average classiﬁcation error i.e. misclassiﬁcation error computed total number misclassiﬁed samples number iterations. average errors algorithm computed using cyclical iterations dataset four different sparsity levels mentioned above. expected algorihtm full features achieved best performance compared rest algorithms underscoring importance amount context observed on-line learning setting. however context limited cbrc problem considered work tsrc approach shows superior performance compared rest algorithms except fullfeatures conﬁrming importance efﬁcient feature selection cbrc setting. overall based mean error rate three algorithms tsrc randomﬁx respectively suggesting using ﬁxed randomly selected feature subset still better strategy considering context mab. however turns ignoring context still better approach randomly changing choice feature iteration random-ei; latter resulted worst mean error detailed analysis covertype dataset figure provides detailed evaluation algorithms different levels sparsity speciﬁc ignoring fullfeatures which expected dataset. outperforms methods since considers full context observe that sparsity tsrc lowest error methods followed tightly suggesting that high sparsity level ignoring context similar considering small fraction also mentioned above sticking ﬁxed subset randomly selected features appears better changing random subset iteration. sparsity tsrc lowest error followed closely random-ﬁx implies that levels sparsity random subset selection perform almost good optimization strategy. again observe random-ﬁx performs better randon-ei performance latter close multi-arm bandit without context. sparsity observe tsrc perform practically good fullfeatures implies sparsity level approach able select optimal feature subset. non-stationary case setting dataset experiments iterations change label class iteration simulate non-stationarity. evaluate wtsrc algorithm nonstationary cbrc problem stationarysetting tsrc method baseline methods presented before. similarly stationary case evaluation table reports mean error iterations level sparsity before. overall observed wtsrc performs best conﬁrming effectiveness using time-windows approach nonstationary on-line learning setting. three performing algorithms wtsrc fullfeatures tsrc order underscores importance size observed context even non-stationary on-line learning setting. detailed analysis covertype dataset figure provides detailed evaluation algorithms different levels sparsity speciﬁc dataset. fullfeatures expected performance different level sparsity since access features. observe that sparsity lowest error compared random-ei random-ﬁx implies that high sparsity levels ignoring context better even considering small random subset sparsity nonstationary method wtsrc outperforms algorithms demonstrating advantages dynamic feature-selection strategy non-stationary environment relatively sparsity levels. setting refer contextual bandit restricted context features context used describe current state world; however agent choose limited-size subset features observe thus needs explore feature space simultaneously exploring space order best feature subset. proposed novel algorithms based thompson sampling solving cbrc problem stationary nonstationary environments. empirical evaluation several datasets demonstrates advantages proposed approaches. shipra agrawal navin goyal. analysis thompson sampling multicolt armed bandit problem. annual conference learning theory june edinburgh scotland pages shipra agrawal navin goyal. thompson sampling contextual bandits icml pages linear payoffs. robin allesiardo rapha¨el f´eraud djallel bouneffouf. neural networks committee contextual bandit problem. neural information processing international conference iconip kuching malaysia november proceedings part pages sof´ıa villar jack bowden james wason. multi-armed bandit models optimal design clinical trials beneﬁts challenges. statistical science review journal institute mathematical statistics carpentier r´emi munos. bandit theory meets compressed sensing high dimensional stochastic linear bandit. proceedings fifteenth international conference artiﬁcial intelligence statistics aistats pages j´er´emie mary romaric gaudel philippe preux. bandits recommender systems. machine learning optimization data first international workshop pages", "year": 2017}