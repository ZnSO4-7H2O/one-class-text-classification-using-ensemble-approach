{"title": "Towards a Mathematical Understanding of the Difficulty in Learning with  Feedforward Neural Networks", "tag": ["cs.LG", "cs.AI", "cs.NE", "math.OC"], "abstract": "Training deep neural networks for solving machine learning problems is one great challenge in the field, mainly due to its associated optimisation problem being highly non-convex. Recent developments have suggested that many training algorithms do not suffer from undesired local minima under certain scenario, and consequently led to great efforts in pursuing mathematical explanations for such observations. This work provides an alternative mathematical understanding of the challenge from a smooth optimisation perspective. By assuming exact learning of finite samples, sufficient conditions are identified via a critical point analysis to ensure any local minimum to be globally minimal as well. Furthermore, a state of the art algorithm, known as the Generalised Gauss-Newton (GGN) algorithm, is rigorously revisited as an approximate Newton's algorithm, which shares the property of being locally quadratically convergent to a global minimum under the condition of exact learning.", "text": "training deep neural networks solving machine learning problems great challenge ﬁeld mainly associated optimisation problem being highly non-convex. recent developments suggested many training algorithms suffer undesired local minima certain scenario consequently great efforts pursuing mathematical explanations observations. work provides alternative mathematical understanding challenge smooth optimisation perspective. assuming exact learning ﬁnite samples sufﬁcient conditions identiﬁed critical point analysis ensure local minimum globally minimal well. furthermore state algorithm known generalised gauss-newton algorithm rigorously revisited approximate newton’s algorithm shares property locally quadratically convergent global minimum condition exact learning. deep neural networks successfully applied solve challenging problems pattern recognition computer vision speech recognition despite success training dnns still greatest challenges ﬁeld work focus training classic feedforward multi-layer perceptrons known performance mlps highly dependent various factors complicated way. example studies identify topology mlps determinative factor. works demonstrate impact different activation functions performance mlps. moreover choice error/loss functions also shown inﬂuential even well designed architecture training speciﬁc effectively efﬁciently challenging constructing network. popular method used training mlps well-known backpropagation algorithm although classic algorithm shares great convenience simple suffer major problems namely existence undesired local minima even global optimality assumed; slow convergence speed. early works argue problems algorithms essentially nature gradient descent algorithms associated optimisation problem training often highly non-convex large scale. major approach address problem undesired local minima training error/loss surface analysis even simple tasks classic problem error surface analysis surprisingly complicated results hard conclude early efforts identify general conditions topology mlps eliminate undesired local minima i.e. suboptimal local minima. unfortunately attempts fail provide complete solutions general problems. hand although algorithms often thought sensitive initialisations recent results reported suggest modern training algorithms overcome problem suboptimal local minima conveniently. observations triggered several recent efforts characterise global optimality training work shows deep linear networks deep nonlinear networks rectiﬁed linear unit function hidden layers free suboptimal local minima. attempted technique applicable analysing deep nonlinear networks activation functions e.g. sigmoid softsign. recent work studies special case problem training samples assumed linearly independent i.e. number samples smaller dimension sample space. unfortunately strong assumption even hold classic problem. moreover conditions constructed main result i.e. existence non-degenerate global minima hardly satisﬁed practice recently restricting network output regularisation positively homogeneous respect network parameters work develops sufﬁcient conditions guarantee local minima globally minimal. results apply networks either hidden layer multiple deep subnetworks connected parallel. moreover still fails explain performance mlps non-positively homogeneous activation functions. deal slow convergence speed classic algorithm various modiﬁcations developed momentum based algorithm conjugate gradient algorithm bfgs algorithm adaptive moment estimation algorithm speciﬁcally generalised gauss-newton algorithm proposed demonstrated prominent performance many state training algorithms practice unfortunately justiﬁcation performance still mathematically vague another popular solution deal slow convergence employ newton’s method training. however implementation exact newton’s method often computationally prohibitive. hence approximations hessian matrix needed address issue diagonal approximation structure block diagonal approximation structure however without complete evaluation true hessian performance heuristic approximations hardly convincing. although existing attempts characterised hessian applying partial derivatives results still fail provide structural information hessian limitations partial derivatives. work provide alternative mathematical understanding difﬁculty training mlps smooth optimisation perspective. sufﬁcient conditions identiﬁed critical point analysis ensure local minima globally minimal. convergence properties algorithm rigorously analysed approximate newton’s algorithm. many machine learning tasks formulated problem learning task-speciﬁc ground truth mapping denote input space output space respectively. problem interest approximate given ﬁnite number samples either samples given i=⊂x problem approximating known unsupervised learning. {xi}t input samples desired outputs provided i.e. corresponding problem referred given training samples supervised learning. work focus problem supervised learning. reachable denotes suitable error function evaluates estimate task-dependent prior knowledge supervised learning problems prior knowledge simply corresponding desired output i.e. general given ﬁnite number samples ground less still deﬁne speciﬁc scenario exact learning respect ﬁnite number samples. deﬁnition ground truth mapping. given samples yi}t called ﬁnite exact approximator based samples. describing mlps denote number layers structure number processing units l-th layer specifically refer input layer. unit activation function traditionally chosen non-constant bounded continuous monotonically increasing. recent choices e.g. relu softplus bent identity unbounded functions. work restrict activation functions smooth denote ﬁrst second derivative activation function respectively. rnl− denotes output layer rnl− respectively weight vector bias associated unit. note bias free variable general. however analysis work constant scalar sake convenience presentation. then simply deﬁne l-th layer mapping stacking unit mappings layer rnl−×nl l-th weight matrix. speciﬁcally denote input output l-th layer deﬁned recursively. note last layer commonly employs identify activation function i.e. φl−. finally denoting parameter matrices rn×n rnl−×nl compose layer-wise mappings deﬁne overall network mapping speciﬁcally denote architecture specifying number units layer. utilise approximate ground truth mapping then empirical total loss function based learning formulated important notice that even error function constructed convex respect ﬁrst argument total loss function deﬁned still non-convex since parameters unbounded squashing activation functions employed exploding norm weight matrix drive function value inﬁnity. namely total loss function non-coercive therefore existence attainability global minima guaranteed general. however ﬁnite sample setting appropriate nonlinear activation functions sigmoid tanh employed hidden layer three-layer sufﬁciently large number units hidden layer achieve exact learning ﬁnite samples realising ﬁnite exact approximator. assumption ground truth mapping. given unique sami= exists architecture deﬁned ples {xi}t weight corresponding ﬁnite exact approximator exact learning ﬁnite samples assumed suitable error function critical ensure attainability uniqueness optimisation procedure. optimistically ﬁnite exact approximator corresponds global minimum corresponding total loss function without undesired local minima. propose following assumption practical principle choosing error function. principle error function differentiable respect ﬁrst argument. gradient respect ﬁrst argument vanishes i.e. global minimum remark typical examples error function include classic squared loss smooth approximations norm blake-zisserman loss cauchy loss moreover principle weights assumed assumption global minimiser total loss function critical point analysis training order develop gradient descent algorithm minimise cost function derivative unit mappings building blocks computation. deﬁne derivative activation function unit dfl·hl dfl·hl− refer directional derivative respect ﬁrst second argument respectively. explicitly ﬁrst derivative i.e. rnl−×nl linear acting direction rnl−×nl gradient realised rank-one matrix acts linear rnl−×nl exploring layer-wise structure corresponding vector computed recursively backwards output layer i.e. l∇e. backward mechanism computing gradient recover classic algorithm algorithm follows characterise critical points total loss function deﬁned setting gradient zero i.e. explicitly gradient respect l-th weight computed puts matrix vector form denotes kronecker product matrices. then applying previous calculation samples critical points total loss function characterised solutions following linear system parameterised linear equation system strongly dependent several factors essentially factors designing i.e. structure activation function error function given samples weight matrices. trivial solution reachable weights additionally solution solution linear equation system local minimum loss function global minimum. thus conclude following theorem. theorem architecture satisfy assumption speciﬁc learning task error function satisfy principle following conditions fulﬁlled matrix constructed non-zero obviously condition theorem quite easy ensured condition hardly possible realised since might require enormous efforts design space mlps nevertheless rank matrix equal trivial solution zero solution parameterised linear system. hence following proposition special case theorem principle total number variables architecture nnet needs greater equal i.e. nnet follows investigate possibility difﬁculty fulﬁl condition construct identically partitioned matrices collecting partitions rank property blocks following lemma. proposition given collection matrices rnl×nl collection vectors rnl− rnl× rnl−×t rank khatri-rao product bounded clearly order rank better bounded below important ensure higher rank φl−’s. choosing appropriate activation functions hidden layers φl−’s full rank then follows present three heuristics aiming keep full rank much possible. construction speciﬁed i.e. matrix product remark note that full rank constraint weight matrices introduce local minima constrained total loss function. however whether constraint exclude global minima unconstrained total loss function given architecture still open problem. remark trivial verify popular differentiable activation functions sigmoid tanh softplus softsign satisfy principle note identity activation function employed output layer also satisﬁes principle. however potentially vanishing gradient squashing activation functions still issue practice ﬁnite machine precision. therefore activation functions without vanishing gradients e.g. bent identity leaky relu might preferred. still canguaranteed full rank according sylvester’s rank inequality hence need prevent loss rank matrix product i.e. preserve smaller rank matrices matrix product. remark condition together implies i.e. inequality takes effect hidden layer since principle sufﬁcient ensure l−’s full rank. however inequality together principle ensures loss rank well known gradient descent algorithms suffer slow convergence. information hessian matrix loss function critical developing efﬁcient numerical algorithms. speciﬁcally deﬁniteness hessian matrix indicator isolatedness critical points affect signiﬁcantly convergence speed corresponding algorithm. start hessian sample-wise loss function deﬁned bilinear operator rnnet ×rnnet computed second directional derivative sake readability present component second directional derivative respect speciﬁc layer indices i.e. rni×ni diagonal matrix diagonal entries second derivative activation functions. since exact learning ﬁnite samples assumed global minimum gradients error function samples simply zero i.e. consequently second summand last diagonal matrix hessians samples evaluated trivial conclude following proposition rank proposition rank hessian total loss function global minimum bounded namely designed scratch without insightful knowledge reaches exact learning likely hessian degenerate i.e. classic algorithm suffer signiﬁcantly slow convergence. moreover also obvious conditions constructed main result i.e. existence non-degenerate global minima hardly possible satisﬁed setting considered depend rank ensure full rank need non-degenerate hessian error function global minima i.e. morse function hence addition principle state following principle choice error functions. section evaluate results previous sections case studies namely loss surface analysis training mlps hidden layer development approximate newton’s algorithm. firstly revisit classic results mlps hidden layer exemplify analysis using classic problem learning task unique training samples ﬁnite exact approximator realisable two-layer units hidden layer training process exempts suboptimal local minima. scalar-valued bias unit free variable dummy unit introduced layer except output layer. dummy unit always feeds constant input successor layer. results claim that presence dummy units units hidden layer required achieve exact learning eliminate suboptimal local minima. statement shown false counterexample utilising problem following proposition reinvestigate problem concrete example applying proposition proposition two-layer architecture dummy units satisfy principle learning task unique samples rn×t rank rest section examine statements problem speciﬁc structures extensively studied literature namely network network. squashing activation functions used hidden layer. dummy units introduced input hidden layer. problem unique input samples firstly look network. considering dummy unit hidden layer matrix last then according proposition ﬁrst block pxor smallest rank second block smallest rank one. hence rank pxor lower bounded local minima exist training network similarly network i.e. although ﬁrst block pxor potentially largest rank four still immune collapsing lower rank three. meanwhile second block pxor also smallest rank one. therefore rank pxor lower bounded three. sequel still exist undesired local minima training network approximate netwon’s algorithm important notice hessian neither diagonal block diagonal differs existing approximate strategies hessian classic newton’s method minimising total loss function requires compute exact hessian second directional derivative complexity computing second summand right hand side last equality order number layers namely implementation exact newton’s method becomes much expensive gets deeper. motivated fact computationally expensive term vanishes global minimum shown propose approximate hessian arbitrary weight following expression deﬁned approximation construct approximate newton’s algorithm minimise total loss function speciﬁcally k-th iterate approximate newton’s direction computed solving following linear system remark approximate hessian proposed construction positive semi-deﬁnite arbitrary weights deﬁniteness exact hessian conclusive. trivial approximate hessian coincides ground-truth hessian global minima. hence corresponding approximate newton’s algorithm induced update rule shares local quadratic convergence properties global minimum exact newton’s method general computing approximate newton’s update deﬁned computationally expensive. interestingly approximate newton’s algorithm indeed state algorithm developed efﬁcient implementations algorithm extensively explored next section investigate theoretical convergence properties approximate newton’s algorithm i.e. algorithm. section investigate performance approximate newton’s algorithm i.e. algorithm. test algorithm four regions classiﬁcation benchmark originally proposed around origin square area three concentric circles radiuses four regions/classes interlocked nonconvex details benchmark. samples drawn training corresponding outputs classes ﬁrstly demonstrate theoretical convergence properties an/ggn algorithm deploying architecture dummy units introduced input hidden layer. activation functions hidden layer chosen bend identity error function squared loss. randomly drawn training samples an/ggn algorithm randomly initialised weights. convergence measured distance accumulation point iterates i.e. extension frobenius norm matrices clear figure an/ggn algorithm converges locally quadratically fast global minimiser exact learning. investigate performance an/ggn different activations namely sigmoid tanh softsign softplus bent identity. ﬁrst three activation functions squashing softplus bounded below bent identity totally unbounded. figure gives plot value total loss function independent runs random initialisations convergence. clearly an/ggns softplus bent identity perform well ones sigmoid tanh suffer numerically spurious local minima. however also interesting observe softsign share convergence behaviour squashing counterparts unknown factors. finally investigate an/ggn algorithm comparing classic algorithm terms convergence speed without exact learning assumed. experiment adopt architecture target outputs corresponding standard basis vector step size constant running iterations algorithm took sec. an/ggn algorithm spent sec. average running time iteration an/ggn times required iteration data random initialisation iterations took amount time required iterations an/ggn. shown figure ﬁrst iterations highlighted remaining iterations coloured blue. an/ggn goes beginning smoothly converges global minimal value demonstrates strong oscillation towards end. work provide smooth optimisation perspective challenge training mlps. condition exact learning characterise critical point conditions empirical total loss function investigate sufﬁcient conditions ensure local minimum globally minimal. classic results mlps hidden layer reexamined proposed framework. finally so-called generalised gauss-newton algorithm rigorously revisited approximate newton’s algorithm shares property locally quadratically convergent global minimum. aspects discussed paper require systematic thorough investigation theoretically experimentally expected also applicable training recurrent neural networks. iniqi) selection matrices satisfying iniqi. refer concrete constructions matrices technical details regarding khatrirao product. trivial conclude following corollary. denotes tracy-singh product identity matrix column-wised partitioned matrix operator diag puts sequence matrices block diagonal matrix. rank property tracy-singh product equal rank. further sylvester’s rank rank matrix inequality rank bounded clear bound proposition still problem-dependent hard control. nevertheless special structure actual rank bound given practically largest bound individual block characterised proposition i.e. proof proposition proof. feed samples rn×t gener rt×t inate outputs hidden layer vertible condition achieved employing appropriate activation functions suggested sigmoid tanh. rn×t deoutput layer note rn×t desired outputs. then every pair note proposition consider dummy units introduced scalar-valued bias blk. nevertheless using similar arguments statements proposition also hold true case free variables blk.", "year": 2016}