{"title": "Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep  Reinforcement Learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Reinforcement Learning (RL) can model complex behavior policies for goal-directed sequential decision making tasks. A hallmark of RL algorithms is Temporal Difference (TD) learning: value function for the current state is moved towards a bootstrapped target that is estimated using next state's value function. $\\lambda$-returns generalize beyond 1-step returns and strike a balance between Monte Carlo and TD learning methods. While lambda-returns have been extensively studied in RL, they haven't been explored a lot in Deep RL. This paper's first contribution is an exhaustive benchmarking of lambda-returns. Although mathematically tractable, the use of exponentially decaying weighting of n-step returns based targets in lambda-returns is a rather ad-hoc design choice. Our second major contribution is that we propose a generalization of lambda-returns called Confidence-based Autodidactic Returns (CAR), wherein the RL agent learns the weighting of the n-step returns in an end-to-end manner. This allows the agent to learn to decide how much it wants to weigh the n-step returns based targets. In contrast, lambda-returns restrict RL agents to use an exponentially decaying weighting scheme. Autodidactic returns can be used for improving any RL algorithm which uses TD learning. We empirically demonstrate that using sophisticated weighted mixtures of multi-step returns (like CAR and lambda-returns) considerably outperforms the use of n-step returns. We perform our experiments on the Asynchronous Advantage Actor Critic (A3C) algorithm in the Atari 2600 domain.", "text": "sahil sharma girish raguvir srivatsan ramesh∗ balaraman ravindran indian institute technology madras chennai india sahilcse.iitm.ac.in {girishraguvirsriramesh}gmail.com ravicse.iitm.ac.in reinforcement learning model complex behavior policies goaldirected sequential decision making tasks. hallmark algorithms temporal difference learning value function current state moved towards bootstrapped target estimated using next state’s value function. λ-returns deﬁne target agent weighted combination rewards estimated using multiple many-step look-aheads. although mathematically tractable exponentially decaying weighting n-step returns based targets λ-returns rather ad-hoc design choice. major contribution propose generalization λ-returns called conﬁdence-based autodidactic returns wherein agent learns weighting n-step returns end-to-end manner. contrast λ-returns wherein agent restricted exponentially decaying weighting scheme allows agent learn decide much wants weigh n-step returns based targets. experiments addition showing efﬁcacy also empirically demonstrate using sophisticated weighted mixtures multi-step returns considerably outperforms n-step returns. perform experiments asynchronous advantage actor critic algorithm atari domain. reinforcement learning often used solve goal-directed sequential decision making tasks wherein conventional machine learning methods supervised learning suitable. goal-directed sequential decision making tasks modeled markov decision process traditionally tabular methods extensively used solving mdps wherein value function policy estimates maintained every state. methods become infeasible underlying state space problem exponentially large continuous. traditional methods also used linear function approximators conjunction hand-crafted state spaces learning policies value functions. need hand-crafted task-speciﬁc features limited applicability traditionally. recent advances representation learning form deep neural networks provide effective achieve generalization deep neural networks learn hierarchically compositional representations enable algorithms generalize large state spaces. deep neural networks conjunction objectives shown remarkable results learning solve atari tasks pixels learning solve complex simulated physics tasks showing super-human performance ancient board game building accurate powerful state action value function estimators important successful solutions. many practical solutions sarsa actor-critic methods temporal difference learning learning n-step return used estimate value function means bootstrapping state’s value function estimate. hand monte carlo learning cumulative reward obtained entire trajectory following particular state used estimate value function state. ability build better estimates value functions directly results better policy estimates well faster learning. λ-returns effective regard. effective faster propagation delayed rewards also result reliable learning. provide trade-off using complete trajectories bootstrapping n-step returns model target using mixture n-step returns wherein weights successively longer returns exponentially decayed. advent deep multi-step returns gained popularity however noted exponentially decaying weighting various n-step returns seems ad-hoc design choice made paper start extensively benchmarking λ-returns work propose generalization called conﬁdence-based autodidactic returns agent learns end-to-end manner weights assign various n-step return based targets. also it’s important note weights assigned various n-step returns change based different states bootstrapping done. sense weights dynamic using represents signiﬁcant level sophistication compared usage λ-returns. alleviate need ad-hoc choice weights case λ-returns propose generalization called autodidactic returns present novel derivative called conﬁdence-based autodidactic returns setting. empirically demonstrate using sophisticated mixtures multi-step return methods like λ-returns conﬁdence-based autodidactic returns leads considerable improvement performance agent. preliminaries deﬁned tuple states actions reward function discount factor. consider standard setting wherein sequential decision-making task modeled agent interacts environment number discrete time steps. time step agent receives state selects action available actions given state agent could decide pick action stochastically. policy point goal agent maximize return deﬁned γkrt+k cumulative discounted future reward. state value function policy deﬁned expected return obtained starting state picking actions according policy parameters using policy gradient theorem based objective functions. value function estimates used baseline reduce variance policy gradient estimates. asynchronous advantage actor critic introduced ﬁrst class actor-critic algorithms worked high-dimensional complex visual input space. insight work executing multiple actor learners different threads agent explore different parts state space simultaneously. ensures updates made parameters agent uncorrelated. practice often replaced biased lower variance estimate based multi-step returns. algorithm n-step returns used estimate target hyper-parameter estimates deﬁned form truncated λ-returns td-learning. kind experiment with paper. truncated λ-returns algorithm designed makes suitable extension truncated λ-returns. leave problem generalizing work full λ-returns well eligibility-traces future work. autodidactic returns form weighted returns wherein weight vector also learned alongside value function approximated. generalization makes returns autodidactic. since autodidactic returns propose constructed using weight vectors state dependent denote weight vector autodidactic returns used learning better approximations value functions using learning rule based update equation contrast autodidactic returns λ-returns assign weights various n-steps returns constants given particular reiterate weights assigned λ-returns don’t change learning process. therefore autodidactic returns generalization assign weights returns dynamic construction. autodidactic weights learned agent using reward signal receives interacting environment. n-step returns state estimates bootstrapped using value function corresponding future state value functions estimates themselves. hence natural agent weigh n-step return would compute weight using notion conﬁdence agent value function estimate using n-step return estimated. agent weigh n-returns based conﬁdent bootstrapping order obtain good estimate denote conﬁdence given conﬁdences weight vector computed idea weighing returns based notion conﬁdence explored earlier works learning adapting lambda parameter based notion conﬁdence/certainty bias-variance trade-off attempted reason successful methods emerged body work difﬁcult quantifying measuring optimizing certainty metric. work propose simple robust model also address question means particular state high value conﬁdence leads better estimates value function. critic uses n-step return arriving good estimates value function. however note td-target general based n-step return algorithm speciﬁc well suited using weighted returns λ-returns since algorithm already uses n-step return bootstrapping. using eqs. makes easy incorporate weighted returns framework. respective sample estimates gradients actor critic become propose autodidactic returns place normal n-step returns framework. call combination carac. generic setup forward pass done network obtain value function current state. parameters network progressively updated based gradient loss function value function estimation becomes better. predicting conﬁdence values distinct neural network created shares last layer value function estimation network. every forward pass network state outputs value function conﬁdence network value function prediction figure shows carac network unrolled time visually demonstrates conﬁdence values calculated using network. next using eqs. weighted average n-step returns calculated used target improving algorithm appendix presents detailed pseudo-code training carac agent. policy improvement carried following sample estimates loss function’s gradient given deﬁned terms error term obtained using autodidactic returns td-target. overall sample estimates gradient actor critic loss functions lstm-ac neural networks representing policy value function share last output layer. speciﬁc lstm controller aggregates observations temporally shared policy value networks. stated previous sub-section extend network predict conﬁdence values creating output layer takes input lstm output vector figure contains demonstration computed. since three outputs share last layer depends parameters network used value function prediction. hence autodidactic returns also inﬂuence gradients lstm controller parameters. however observed target allowed move towards value function prediction makes learning unstable. happens loss td-target value function prediction minimized moving td-target towards erroneous value function predictions instead round. avoid instability ensure gradients back conﬁdence values computation’s last layer lstm layer’s outputs. effect gradient critic loss respect parameters utilized computation autodidactic return longer inﬂuence gradients lstm parameters summarize back-propagation gradients network parameters speciﬁc computation autodidactic return contribute gradient ﬂows back lstm layer. ensures parameters conﬁdence network learned treating lstm outputs ﬁxed feature vectors. entire scheme allowing gradients back conﬁdence value computation lstm outputs demonstrated figure forward arrows depict parts network involved forward propagation whereas backward arrows depict path taken back-propagation gradients. performed general game-play experiments carac lrac tasks atari domain. networks trained million time steps. hyper-parameters methods tuned subset four tasks seaquest space invaders gopher breakout. hyper-parameters used rest tasks. baseline scores taken sharma experiments repeated thrice different random seeds ensure results robust random initialization. three random seeds used across experiments results reported average results obtained using three random seeds. since scores taken sharma followed training testing regime well. appendix contains experimental details training testing regimes. appendix documents procedure used picking important hyper-parameters methods. shows mean median normalized scores carac lrac. scores obtained methods task respectively normalized score calculated carac lrac improve carac best average achieves scores obtained scores obtained methods baseline scores found table figure shows percentage improvement achieved sophisticated mixture n-step return methods scores obtained methods task respectively percentage improvement calculated carac achieves staggering performance task kangaroo. carac figure demonstrates difference weights assigned carac lrac lrac) various n-step returns duration episode fully trained agents. four games shown games carac achieves large improvements performance lrac. seen four tasks difference weights evolve dynamic fashion episode goes motivation behind analysis understand different weights used carac compared lrac seen it’s different. agent clearly able weighs it’s returns different lrac this fact seems give edge lrac many games. results figure relation conﬁdence assigned state percent change value estimate. percentage change value estimates obtained calculating value estimate state batch gradient update step. figure denotes average conﬁdence value assigned network states encountered training time million million steps whose value estimates changed value percent graphs initial stages training conﬁdence assigned states approximately equal irrespective change value estimate. training progresses network learns assign relatively higher conﬁdence values states whose value function changes small amount ones whose value function changes more. conﬁdence value interpreted value quantiﬁes certainty network value estimate state. thus weighing n-step returns based conﬁdence values enable network bootstrap target value better. conﬁdence value depends certainty change value estimate round i.e. high conﬁdence value doesn’t make value estimate state change less. true conﬁdence value inﬂuence value estimate state gradients obtained conﬁdence values back propagated beyond dense layer conﬁdence output. figure shows conﬁdence values assigned states duration episode fully trained carac agent games carac achieves large improvements lrac kangaroo beam rider. states encountered. here it’s important note apparent periodicity observed graphs nature conﬁdences learnt instead periodic nature games themselves. game frames shown observe frames high conﬁdence highly recurring states game. case beam rider high conﬁdence states correspond initial frames every wave wherein horde enemies come completion penultimate wave game. case kangaroo apart many aspects game piece fruit keeps falling periodically along left screen jumping appropriate time punching fruit gives points. observing game-play found policy learnt carac agent identiﬁes exactly facet game achieve large improvements score. again series states encompassing transition fruit towards bottom screen form highly recurring states especially states piece fruit \"jumping-distance\" away kangaroo hence closer reward found form peaks. observations reiterate results obtained section highly recurring nature states would enable network estimate value functions better. better estimates value functions suitably used obtain better estimates states bootstrapping greater attention high conﬁdence states. believe ability derive states means attention mechanism provided conﬁdence values enables carac obtain better estimates value function. expanded version graphs tasks found appendix paper propose methods learning value functions sophisticated manner using n-step returns. hence important analyze value functions learned methods understand whether methods indeed able learn better value functions baseline methods not. sub-section trained agents serve baselines. verify claims better learning value functions conducted following experiment. took trained carac lrac agents computed loss value function γkrt+k). averaged quantity episodes plotted function time steps within episode. figure demonstrates novel method carac learns much better estimate value function lrac exception game kangaroo. reason lrac critics manage estimate value function well kangaroo policy better random fact agents often score around propose straightforward incorporate λ-returns algorithm carry large-scale benchmarking resulting algorithm lrac. propose natural generalization λ-returns called conﬁdence-based autodidactic returns agent learns assign weights dynamically various n-step returns bootstrap. experiments demonstrate efﬁcacy sophisticated mixture multi-steps returns least carac lrac out-performing tasks. tasks carac performs best whereas lrac best. gives agent freedom learn decide much wants weigh n-step returns. td-targets turn leading improved performances. proposed modeling autodidactic weights wherein conﬁdence values predicted alongside value function estimates. multiple ways n-step return weights modeled. believe ways modeling weighted returns lead even better generalization terms agent perceives it’s td-target. modeling bootstrapping td-targets fundamental believe proposed idea combined algorithm wherein td-target modeled terms n-step returns. jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. appear international conference learning representations timothy lillicrap jonathan hunt alexander pritzel nicolas heess erez yuval tassa david silver daan wierstra. continuous control deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature february volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. international conference machine learning john schulman philipp moritz sergey levine michael jordan pieter abbeel. high-dimensional continuous control using generalized advantage estimation. arxiv preprint arxiv. sahil sharma aravind lakshminarayanan balaraman ravindran. learning repeat fine grained action repetition deep reinforcement learning. appear international conference learning representations david silver lever nicolas heess thomas degris daan wierstra martin riedmiller. deterministic policy gradient algorithms. proceedings international conference machine learning icml beijing china june http//jmlr.org/proceedings/papers/v/silver.html. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature david silver julian schrittwieser karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton mastering game without human knowledge. nature richard sutton david mcallester satinder singh yishay mansour policy gradient methods reinforcement learning function approximation. nips volume emanuel todorov erez yuval tassa. mujoco physics engine model-based control. ieee/rsj international conference intelligent robots systems ieee martha white adam white. greedy approach adapting trace parameter temporal difference learning. proceedings international conference autonomous agents multiagent systems international foundation autonomous agents multiagent systems used lstm-variant algorithm carac lrac experiments. async-rmsprop algorithm used updating parameters hyper-parameters mnih initial learning rate used linearly annealed million time steps length training period. used n-step returns entropy regularization used encourage exploration similar mnih entropy regularization found hyper-parameter tuning carac lrac separately. tuned optimal initial learning rate found carac lrac separately. learning rate tuned discounting factor rewards retained since seems work well large number methods important hyper-parameter lrac method λ-returns. tuned extensively best four performing models reported figure best performing models models trained million time steps. keeping training regime sharma ensure fair comparisons baseline scores. evaluation done every million steps training followed strategy described sharma ensure fair comparison baseline scores. evaluation done million time steps training episodes episode’s length capped steps arrive average score. evolution average game-play performance training progress demonstrated tasks figure expanded version ﬁgure tasks found appendix table appendix contains scores obtained carac lrac agents atari tasks. evaluation done using latest agent obtained training million steps consistent evaluation regime presented sharma mnih used level architecture similar mnih sharma turn uses level architecture mnih figure contains visual depiction network used carac. common parts carac lrac networks described below. ﬁrst three layers methods convolutional layers ﬁlter sizes strides padding number ﬁlters mnih sharma convolutional layers followed fully connected layers lstm layer. policy value function derived lstm outputs using different output heads. number neurons layers lstm layers design choices taken sharma ensure fair comparisons baseline model apply carac lrac methods. similar mnih actor critic share ﬁnal layer. case carac three functions policy value function conﬁdence value realized different ﬁnal output layer conﬁdence value function outputs non-linearity output-neuron policy softmax-non linearity size equal size action space task. non-linearity used model multinomial distribution. amidar assault asterix bank heist beam rider bowling breakout centipede demon attack freeway frostbite gopher james bond kangaroo koolaid name game phoenix quest space invaders star gunner tutankhamun wizard evaluations done using agent obtained training million steps consistent evaluation paradigm presented sharma mnih carac lrac scores obtained averaging across random seeds. scores column taken table sharma evaluation strategy described appendix executed generate training curves atari tasks. appendix contains training curves. curves demonstrate performance carac lrac agents evolves time. appendix presents expanded version results shown section plots show vast differences weights assigned carac lrac various n-step returns duration single episode. appendix presents expanded version results shown section show conﬁdence values dynamically change duration episode show presence explicit peaks troughs many games. here it’s important note apparent periodicity observed graphs nature conﬁdences learnt instead periodic nature games themselves. algorithm carac assume global shared parameter vectors assume global step counter maximum value n-step returns tmax total number training steps carac policy agent initialize local thread’s step counter local thread’s parameters repeat perhaps important hyper-parameter carac network calculates conﬁdence values. since gradients back conﬁdence computation lstm controller becomes important design choice. experimented extensively different types conﬁdence computation networks including shallow ones deep ones wide ones narrow ones. found \"zero hidden layer\" network lstm controller works best similarly important hyper-parameter λ-returns experimented large number range values best performing ones reported figure", "year": 2017}