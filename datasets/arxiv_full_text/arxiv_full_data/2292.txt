{"title": "Quantifying Uncertainty in Discrete-Continuous and Skewed Data with  Bayesian Deep Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Deep Learning (DL) methods have been transforming computer vision with innovative adaptations to other domains including climate change. For DL to pervade Science and Engineering (S\\&E) applications where risk management is a core component, well-characterized uncertainty estimates must accompany predictions. However, S\\&E observations and model-simulations often follow heavily skewed distributions and are not well modeled with DL approaches, since they usually optimize a Gaussian, or Euclidean, likelihood loss. Recent developments in Bayesian Deep Learning (BDL), which attempts to capture uncertainties from noisy observations, aleatoric, and from unknown model parameters, epistemic, provide us a foundation. Here we present a discrete-continuous BDL model with Gaussian and lognormal likelihoods for uncertainty quantification (UQ). We demonstrate the approach by developing UQ estimates on \"DeepSD\", a super-resolution based DL model for Statistical Downscaling (SD) in climate applied to precipitation, which follows an extremely skewed distribution. We find that the discrete-continuous models outperform a basic Gaussian distribution in terms of predictive accuracy and uncertainty calibration. Furthermore, we find that the lognormal distribution, which can handle skewed distributions, produces quality uncertainty estimates at the extremes. Such results may be important across S\\&E, as well as other domains such as finance and economics, where extremes are often of significant interest. Furthermore, to our knowledge, this is the first UQ model in SD where both aleatoric and epistemic uncertainties are characterized.", "text": "abstract deep learning methods transforming computer vision innovative adaptations domains including climate change. pervade science engineering applications risk management core component wellcharacterized uncertainty estimates must accompany predictions. however observations model-simulations often follow heavily skewed distributions well modeled approaches since usually optimize gaussian euclidean likelihood loss. recent developments bayesian deep learning attempts capture uncertainties noisy observations aleatoric unknown model parameters epistemic provide foundation. present discrete-continuous model gaussian lognormal likelihoods uncertainty quantification demonstrate approach developing estimates deepsd super-resolution based model statistical downscaling climate applied precipitation follows extremely skewed distribution. find discrete-continuous models outperform basic gaussian distribution terms predictive accuracy uncertainty calibration. furthermore find lognormal distribution handle skewed distributions produces quality uncertainty estimates extremes. results important across well domains finance economics extremes often significant interest. furthermore knowledge first model aleatoric epistemic uncertainties characterized. introduction science engineering applications beginning leverage recent advancements artificial intelligence deep learning. climate applications deep learning used make high-resolution climate projections detect tropical cyclones atmospheric rivers remote sensing models deepsat satellite image classification framework also leverage computer vision technologies. physicists using deep learning detecting particles high energy physics transportation deep learning aided traffic flow prediction modeling network congestion scientists even used convolutional neural networks approximate navier-stokes equations unsteady fluid forces however many applications underlying data follow non-normal discretecontinuous distributions. example modeling precipitation days precipitation heavily skewed amounts rainy days shown figure furthermore climate complex nonlinear dynamical system precipitation processes particular exhibit extreme space-time variability well thresholds intermittence thus precipitation data cannot assumed gaussian. hence deep learning harnessed it’s potential applications models must resilient non-normal discrete-continuous distributions. uncertainty quantification another requirement wide adoption deep learning particularly risk management decisions. twenty years jaeger stated uncertainties climate change pervasive reaching tools handling uncertainty provided decision analysis longer sufficient expected uncertainty particular interest climate computer scientists scientists inform social infrastructure adaptation increasing weather extremes natural disasters example studied different sources uncertainty climate change impacts flood frequency model uncertainties included future greenhouse scenarios global climate models structure parameters downscaling gcms hydrological model structure parameters. hence quantifying uncertainty processes critical understanding system’s uncertainty. provides problem quantifying uncertainty discrete-continuous non-normal distributions. figure histogram daily precipitation contiguous united states precipitation data points. precipitation distribution rainy days only. distribution precipitation rainy days. implementing already defined deep neural network makes attractive approach. well-defined likelihood function able capture aleatoric epistemic uncertainty epistemic uncertainty comes noise model’s parameters reduced increasing dataset size. side aleatoric uncertainty accounts noise observed data resulting uncertainty cannot reduced. examples aleatoric uncertainty measurement error sensor malfunctions. aleatoric uncertainty either homoscedastic constant uncertainty different inputs heteroscedastic uncertainty depending input. heteroscedastic especially important skewed distributions tails often contain orders magnitude increased variability. variants methods already successfully applied applications scene understanding medical image segmentation applied domains models generally assume gaussian probability distribution prediction. however discussed applications assumption fail hold. motivates extend aperiodic non-normal distributions defining alternative density functions based domain understanding. particular focus precipitation estimation problem called statistical downscaling discuss section section review deepsd statistical downscaling method bayesian deep learning concepts. section present discretecontinuous likelihood models using gaussian lognormal distributions model categorical continuous data. following section compare predictive accuracy uncertainty calibration statistical downscaling. lastly section summarizes results discusses future research directions. statistical downscaling downscaling either statistical dynamical widely used process producing high-resolution projections coarse gcms. dynamical downscaling often referred regional climate models physics based numerical models encoding localized subgrid processes within boundary conditions generate highresolution projections. similar gcms dynamical downscaling computational expensive simply cannot scale ensemble modeling. statistical downscaling relatively efficient solution aims observed data learn functional mapping lowhigh-resolution gcms illustrated figure uncertainty gcms exacerbated observational data downscale gcms must first learn statistical function apply lowhigh-resolution mapping. fortunately observed datasets widely available directly transfer trained model gcms. observation datasets stem gauges satellite imagery radar systems. downscaling typically either in-situ gauge estimates gridded data product. wish obtain complete high-resolution gridded data product required. gridded-data products generally referred reanalysis datasets combination data sources physical characteristics aggregated well estimated data source. simplicity remainder paper refer reanalysis datasets observations. important dataset high spatial resolution daily time temporal scale spanning many years possible. given constraints choose precipitation prism dataset made available oregon state university spatial resolution daily temporal scale underlying data prism estimated combination gauges measuring many climate variables topographical information. train model data upscaled desired low-resolution. example train neural network downscale upscale prism learn mapping reader useful think dataset image precipitation channel analogous traditional channels. similarly variables added dataset therefore increases number channels. however important aware underlying spatio-temporal dynamics chaotic climate system makes dataset complex images. experiments deepsd included elevation global arc-second elevation data provided usgs. background deepsd statistical downscaling approach taken deepsd differs traditional approaches generally capture spatial dependencies input output. example automated statistical downscaling learns regression models low-resolution high-resolution point independently failing preserve spatial dependencies output requiring substantial computational resources learn thousands regression models. contrast deepsd represents data lowhigh-resolution image pairs adapts super-resolution convolutional neural networks including high-resolution auxiliary variables elevation correct biases. auxiliary variables allows single trained neural network within training domain. super-resolution problem essentially pixel-wise regression high-resolution input convolutional neural network parameterized weights learned optimizing loss function global climate models fifth coupled model intercomparison project provides scientist valuable data study effects climate change varying greenhouse emission scenarios gcms complex non-linear dynamical systems model physical processes governing atmosphere year gcms gridded datasets spatial resolutions around contain range variables including temperature precipitation wind pressure multiple pressure levels earth’s surface. research groups around world contributed cmip developing models encoding understanding climate system. within cmip simulated three four emission scenarios multiple initial conditions. suite climate model simulations used probabilistic forecasts variables interest precipitation temperature extremes suite models gives tools study large scale climate trends localized projections required adaptation. many statistical models explored downscaling bias correction spatial disaggregation automated statistical downscaling neural networks nearest neighbor models multiple studies compared different sets statistical downscaling approaches various climate variables varying temporal spatial scales showing approach consistently outperforms others recently vandal presented improved results alternative approach downscaling representing data \"images\" adapting deep learning based super-resolution model called deepsd deepsd showed superior performance downscaling daily precipitation contiguous united states compared bcsd. even though uncertainty crucial statistical downscaling rarely considered downscaling studies. instance downscaled climate projections used latest national climate assessment report produced nasa earth exchange come uncertainty estimates. though widely used climate impact assessments recurrent complaint users lack uncertainty characterization projections. users often request estimates geographic seasonal uncertainties adaptation decisions made robust knowledge khan presented study assessed monthly uncertainty confidence based intervals daily predictions however approach quantifies epistemic uncertainty therefore cannot estimate full probability distribution. best authors’ knowledge studies modeled aleatoric uncertainty statistical downscaling presenting limitation adaptation. climate data wide variety data sources exists studying earth’s climate satellite observations climate models. discussed complexities uncertainty associated ensembles gcms well corresponding storage computational requirements. goal statistically data points. obtain well calibrated uncertainty estimates crucial select well estimated rather setting constant learn learning using concrete distribution prior gives continuous approximation bernoulli distribution presented divergence term written entropy bernoulli random variable probability note given entropy term learning dropout probability cannot exceed desired effect. brevity encourage reader refer concrete dropout optimization. remainder paper concrete dropout formulation within presented models. section describe three candidate bayesian deep learning models quantify uncertainty super-resolution based downscaling. begin formalizing within srcnn architecture assuming normal predictive distribution similar pixel-wise depth regression approach extended discrete-continuous model conditions amount precipitation given occurrence precipitation. leverages domain knowledge vast majority data samples non-rainy days easy predict contain little information regression. technique used sloughter using discrete-continuous gamma distribution lastly show lognormal distribution applied directly derive corresponding log-likelihood loss unbiased parameter estimates. gaussian likelihood super-resolution ill-posed pixel-wise regression problem directly applied kendall showed predicting depth computer vision discussed previous sections crucial capture aleatoric epistemic uncertainties downscaling. shown section must measure aleatoric uncertainty estimating variance predictive posterior also sampling weights dropout discussed above resolution enhancement needed statistical downscaling much greater enhancements used images. deepsd uses stacked srcnns improving resolution allowing model capture regional local weather patterns depending level. instance downscale deepsd first trains models independently downscale .km. inference models simply stacked output plus next corresponding auxiliary variables inputs next. case downscaling precipitation inputs include precipitation elevation predict precipitation. work focus uncertainty quantification single stacked network translated stacking multiple bayesian neural networks. bayesian deep learning early mackay introduced bayesian neural networks replacing deterministic weights distributions. however common many bayesian modeling problems direct inference bnns intractable networks hidden layers. many studies attempted reduce computational requirements using various approximations recently ghahramani recently presented practical variational approach approximate posterior distribution deep neural networks using dropout monte carlo sampling kendall followed work computer vision applications include aleatoric epistemic uncertainties single model begin define weights neural network number layers network. given random outputs denoted likelihood written then given data defined above infer posterior find distribution parameters best describe data. regression task assuming predictive guassian posterior random outputs applying variational inference weights define containing weight mean shape number hidden units layer dropout probability following minimize kullback-leibler precipitation events especially extremes known follow fattailed distributions lognormal gamma distributions reason above model precipitation using discrete-continuous lognormal distribution. noted lognormal distribution undefined conditional required downscaling precipitation. this slightly modify pixel corresponds input number output pixels. term identical equation given formulation variance pixel implicitly learned data without need uncertainty labels. also note training substiution used stable learning using adam optimization algorithm first-order gradient based optimization stochastic objective functions. monte carlo samples {ˆyt first moments provide necessary information easily obtain prediction intervals aleatoric epistemic uncertainties. details encourage reader refer discrete-continuous gaussian likelihood rather assuming simple gaussian distribution output variables heavily biased many non-rainy days dataset condition model predict whether rain occurred not. formulated mean variance probability precipitation sampled respectively predictive ability begin comparing model’s ability predict ground truth observations. root mean square error bias compared understand average daily effects downscaling. analyze extremes select metrics climdex provides suite extreme precipitation indices often used evaluating downscaling models analysis compute index test well observations.then difference predicted indices observed indices computed results seen table clear trend models performing much better regular gaussian distribution computed metrics. particular dc-gaussian shows lowest rmse error sdii error competitive bias. dclognormal slightly higher errors less bias. furthermore study predictability space figure computing pixel-wise rmses. model performs well mid-west worse southeast region large numbers convective precipitation events. models dc-lognormal particular lower bias regular gaussian distribution. similarly rmse models lead dc-gaussian lowest errors. looking closely improved performance along coasts generally challenging estimate. convolutional operation kernel last layer reconstructs image using linear combination nearby points acting smoothing operation. however applied conditional distributions gradient along edge increased predicting high probabilities precipitation close neighborhood. insight particularly important applied coastal cities. lastly look conditional model’s ability classify precipitous days precision recall curves recall begin decrease precision indicates strong classification performance. assumed classification precipitation would easy dataset. uncertainty quantification remainder analysis focuses model’s performance estimating well calibrated uncertainty quantification. limit analysis uncertainty days precipitation uncertainty non-rainy days interest. calibration metric used computes frequency observations occurring within varying probability range predictive distribution. ideally frequency observations equal probability. seen figure present calibration plot model. right away gaussian distribution over-estimates uncertainty range. dc-gaussian analytically produces least calibration error dc-lognormal slightly overestimates uncertainty much domain still performs well. dc-lognormal gaussian models show better calibrated tails. however could calibrate tails simply forcing variance explode. used compute pixel-wise probabilistic estimates. next section apply three methods downscaling precipitation compare accuracies study uncertainties. precipitation downscaling experimentation define problem downscale precipitation resolution enhancement single srcnn network. begin precipitation prism dataset presented section upscaled dataset labels further upscaled generating training inputs. furthermore elevation global arc-second elevation datset provided usgs auxilary variable also upscaled dataset made precipitation elevation inputs precipitation labels. discrete-continuous models precipitation >.mm considered rainy day. precipitation measured millimeters scaled training optimizing gaussian models. elevation normalized overall mean variance. training data taken years test sub-images selected size stride used generating training examples. super-resolution architecture defined hidden layers kernels using kernel sizes concrete dropout used optimize dropout probability parameters τ=e- prior length scale pixel-wise regression number samples days×height×width. parameters found provide good trade-off likelihood regularization loss terms. model trained iterations using learning rate batch size three models optimized using three log-likelihood loss’s defined above gaussian distribution well discrete-continuous gaussian lognormal distributions conditioned rainy day. monte carlo passes inference used measure first moments estimates given predictive distribution’s parameters. validation important task choosing highly predictive well calibrated downscaling model. experiments study model’s ability predict daily precipitation calibration uncertainty width uncertainty intervals. figure daily root mean square error computed location years conus. left) gaussian middle) conditional-gaussian right) conditional-lognormal. corresponds high rmse blue corresponds rmse. table predictive accuracy statistics computed pixel-wise aggregated. daily intensity index yearly precipitation events greater measure model’s ability capture precipitation extremes. r-err sdii-err measures difference observed indicies predicted indicies figure better understand uncertainties increasingly intense precipitation days. high rainfall days models generally under-predict precipitation gaussian models often fail capture extremes. lognormal wider uncertainty intervals able produce well calibrated distribution extremes. furthermore wide intervals indicate model becomes less confident decreasing domain coverage higher intensities. taking step further present calibration rmses pixel figure visualize spatial patterns gaussian model find weakened variable results highelevations west. models perform well know above. figure calibration computed frequency predictions within given probability range. probability varied x-axis corresponding frequency y-axis. dashed ideal calibration. conclusion paper present bayesian deep learning approaches incorporating discrete-continuous skewed distributions targeted applications. discrete-continuous models contain classifier categorize event conditional regressor given event’s occurrence. derive loss functions moments gaussian lognormal regression models. using precipitation example condition model precipitous days predict daily precipitation high-resolution grid. using lognormal distribution able produce well-calibrated uncertainties skewed fat-tailed distributions. knowledge figure uncertainty widths based quantiles predictive distributions. points observations versus expected value. bands correspond predictive intervals. experiments find approach increases predictive power uncertainty quantification performance reducing errors well calibrated intervals. addition find conditional approach improves performance extremes measured daily intensity index number extreme precipitation days climdex. visually found models perform better regular gaussian coasts challenge statistical downscaling. edge errors appear reconstruction kernel partially overlaps coastal edge acting smoothing operation. however models reduce smoothing increasing expected value’s gradients. overall find distribution approaches provides strong benefits deep super-resolution based statistical downscaling. furthermore lognormal distribution uncertainty slightly less calibrated able produce well understood uncertainties extremes. presents strong point bayesian deep neural networks well non-normal distributions motivated domain knowledge. future extend work stacked superresolution networks used deepsd requires sampling networks. extensions could addition variables extension skewed distributions larger network architectures. finally incorporating theoretical advances uncertainty characterization team plans deepsd produce distribute next generation climate projections upcoming congressionally mandated national climate assessment. acknowledgments work supported nasa earth exchange national science foundation cise expeditions computing grant number national science foundation cybersees grant number national science foundation bigdata grant number gtopo dataset distributed land processes distributed active archive center located usgs/eros sioux falls http //lpdaac.usgs.gov. thank arindam banerjee valuable comments. basu ganguly mukhopadhyay dibiano karki nemani. deepsat learning framework satellite imagery. proceedings sigspatial international conference advances geographic information systems page bürger murdock werner sobie cannon. downscaling extremes-an intercomparison multiple statistical methods present climate. journal climate june h.-k. bowman north. comparison gamma lognormal distributions characterizing satellite rain rates tropical rainfall measuring mission. journal applied meteorology daly halbleib smith gibson doggett taylor curtis pasteris. physiographically sensitive mapping climatological temperature precipitation across conterminous united states. international journal climatology gutmann pruitt clark brekke arnold raff rasmussen. intercomparison statistical downscaling methods used water resource assessments united states. water resources research hinton camp. keeping neural networks simple minimizing description length weights. proceedings sixth annual conference computational learning theory pages kendall badrinarayanan cipolla. bayesian segnet model uncertainty deep convolutional encoder-decoder architectures scene understanding. arxiv preprint arxiv. racah beckham maharaj kahou prabhat pal. extremeweather large-scale climate dataset semi-supervised detection localization understanding extreme weather events. advances neural information processing systems pages sloughter raftery gneiting fraley. probabilistic quantitative precipitation forecasting using bayesian model averaging. monthly weather review vandal kodra ganguly. intercomparison machine learning methods statistical downscaling case daily extreme precipitation. arxiv preprint arxiv. vandal kodra ganguly michaelis nemani ganguly. deepsd generating high resolution climate change projections single image super-resolution. sigkdd conference knowledge discovery data mining wang zuluaga pratt patel aertsen doel david deprest ourselin interactive medical image segmentation using deep learning image-specific fine-tuning. ieee transactions medical imaging", "year": 2018}