{"title": "Teacher Improves Learning by Selecting a Training Subset", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We call a learner super-teachable if a teacher can trim down an iid training set while making the learner learn even better. We provide sharp super-teaching guarantees on two learners: the maximum likelihood estimator for the mean of a Gaussian, and the large margin classifier in 1D. For general learners, we provide a mixed-integer nonlinear programming-based algorithm to find a super teaching set. Empirical experiments show that our algorithm is able to find good super-teaching sets for both regression and classification problems.", "text": "local learner making learner approximate blackbox better. reduced training also serves representative examples local model behavior. another application education. imagine teacher teaching goal reasonable assumption practice e.g. geology teacher knowledge actual decision boundaries rock categories. however teacher constrained teach given textbook extent student quantiﬁed mathematically teacher wants select pages textbook guarantee student learns better pages gulping whole book. question possible? following example says yes. consider learning threshold classiﬁer interval true threshold items drawn uniformly interval labeled according learner hard margin places estimated threshold middle inner-most pair different labels largest negative training item smallest positive training item well known |ˆθs converges rate intuition average space adjacent items teacher knows everything cannot tell directly learner. instead select most-symmetric pair give learner two-item training set. prove later risk symmetric pair learning selected subset surpasses learning thus observe something interesting teacher turn larger training smaller better subset midpoint classiﬁer. call phenomenon super-teaching. call learner super-teachable teacher trim training making learner learn even better. provide sharp superteaching guarantees learners maximum likelihood estimator mean gaussian large margin classiﬁer general learners provide mixed-integer nonlinear programming-based algorithm super teaching set. empirical experiments show algorithm able good super-teaching sets regression classiﬁcation problems. consider following question learner receives training drawn distribution parametrized teacher knows teacher select subset learner estimates better subset question distinct training reduction teacher knowledge carefully design subset. fact coding problem teacher approximately encode using items known decoder learner? such question machine learning task rather machine teaching question relevant several nascent applications. application understanding blackbox models deep nets. often observation blackbox model limited predicted label given input interpret blackbox model locally train interpretable model data points labeled blackbox model around region interest however more reduce size training data space unsupervised learning supervised learning underlying distribution take function view learner learner function learner’s hypothesis space. notation deﬁnes training sets namely multisets size whose elements given training assume returns unique hypothesis learner’s risk deﬁned section present ﬁrst theoretical result super teaching learner maximum likelihood estimator mean gaussian. given sample size drawn learner computes mean deﬁne risk |ˆθs θ∗|. teacher consider optimal k-subset teacher uses best subset size teach former prediction tasks loss function denotes prediction made model latter parameter estimation assume realizable model introduce clairvoyant teacher full knowledge teacher also given training teacher teaches learner incur risk teacher’s goal judiciously select subset super teaching learner course teacher must utilize knowledge learning task thus subset actually function particular teacher knows already sets problem apart machine learning. readability suppress extra parameters rest paper. formally deﬁne super teaching follows. deﬁnition super teacher learner answer ﬁrst question positively exhibiting super teaching learners maximum likelihood estimator mean gaussian section large margin classiﬁer section guarantees super teaching general learners remain future work. nonetheless empirically super teaching many general learners formulate second question mixed-integer nonlinear programming section empirical experiments section demonstrates good effectively. build intuition well-known risk variance items shrinks like consider since teacher knows setting best teaching strategy select item closest forms singleton teaching show large probability closest item away therefore already super teaching ratio generally main result shows achieves super teaching ratio kk−\u0001√ remark introduced auxiliary variable controls implicit tradeoff much super teaching helps soon super teaching takes effect. teaching ratio approaches similarly also affects tradeoff teaching ratio smaller enlarge increases. proof. subsets size overlap indexes. total number distinct indexes appear ways choosing indexes. next determine indexes overlapping ones. ways choosing indexes. finally ways selecting half non-overlapping indexes attribute thus total ordered pairs subsets overlap indexes. assumption therefore total number ordered pairs subsets overlapping identical present second theoretical result time teaching large margin classiﬁer. pzpz maxiyi=− miniyi=+ inner-most pair opposite labels exist. formally deﬁne large margin classiﬁer argmin)∈s| main result shows learning whole achieves well-known risk surprisingly achieves risk therefore approximately super teaching ratio. theorem n-item sample drawn proving theorem ﬁrst show optimal teacher large margin classiﬁer. proposition optimal teacher large margin classiﬁer ˆθs. proof. show |bms| either positive negative. cases deﬁnition. thus otherwise |bms| positive negative thus otherwise inner pair deﬁnition since s−+s+ proof. give proof sketch details appendix. respectively. +|s| deﬁne event {|s| given since +|s| either without loss generality assume divide segments. interval equally figure union bound following event exist point ﬂipped point lies segment point show thus happens note therefore thus s−+s+ terministic given logistic regression estimates θ)). contrast teacher’s risk deﬁned expected loss label predicted since symmetric origin risk rewritten terms angle arccos/π. instantiating experiments study effectiveness scalability neos minlp solver speciﬁcally respect training size dimension ﬁrst experiments vary trials. trial draw n-item sample call solver solver’s solution indicates super teaching compute empirical version super teaching ratio left half table report median following quantities trials fraction training items selected super teaching |b|/n neos server running time. main result means solver indeed selects super-teaching although problem proving super teaching ratios speciﬁc learner interesting focus algorithm super teaching general learners given training subset start formulating super teaching subset selection problem. introduce binary indicator variables means included subset. consider learners deﬁned convex empirical risk minimization simplicity assume unique global minimum returned argmin. note denote convex loss used performing empirical risk minimization. example negative likelihood logistic regression. potentially different used teacher deﬁne teaching risk finally figure visualizes typical trial teaching logistic regression ridge regression. consists dark light points dark ones representing optimized minlp. dashed line shows solid lines shows ˆθb. ground truth essentially overlaps solid lines. speciﬁcally super taught models negligible risks whereas models trained whole sample incur much larger risks respectively. several research threads different communities aimed reducing data maintaining utility. ﬁrst thread training reduction training time prunes items attempt improve learned model. second thread coresets summary models learned summary provably competitive models learned full data know target model methods cannot truly achieve super teaching. third thread curriculum learning showed smart initialization useful nonconvex optimization. contrast teacher directly encode true model therefore obtain faster rates. ﬁnal thread sample compression compression function chooses subset reconstruction function form hypothesis. present work similarity compression allows increased accuracy since compression bounds used regularization theoretical study machine teaching focused teaching dimension i.e. minimum training size needed exactly teach target concept prior work assumed synthetic teaching setting whole item space often unrealistic. considered approximate teaching ﬁnite setting though analysis focused speciﬁc learner. super teaching setting applies arbitrary second note solver tends select large subset since median |b|/n interesting known dense select extremely sparse super teaching sets small items teach effectively understanding different regimes remains future work. second experiments vary left half table shows results. empirical teaching ratio still cases showing super teaching. dimension problem increases deteriorates toward nonetheless even still median super teaching ratio corresponding super teaching training items dimension. interesting minlp algorithm intentionally created high dimensional learning problem achieve better teaching knowing learner regularized. running time change dramatically. experiments. tables show results qualitatively similar teaching logistic regression. again empirical super teaching ratio indicating presence super teaching. machine teaching applications include education computer security interactive machine learning establishing existence super-teaching present paper guide process ﬁnding effective training applications. presented super-teaching teacher already knows target model often choose given training smaller subset trains learner better. proved learners provided empirical algorithm based mixed integer nonlinear programming super teaching set. however much needs done theory super teaching. give counterexamples illustrate learners super-teachable. example given n-item training aint maxi=n risk deﬁned |ˆθs θ∗|. show aint superteachable. maxxi∈b maxxi∈s ˆθs. since |ˆθb |ˆθs generalize classiﬁcation setting show neither least greatest consistent hypothesis super-teachable example interval integer grid. hypothesis space outside}. uniform noiseless labeled according risk size symmetric difference intervals normalized xmax−xmin. given sample least consistent learner learns tightest interval positive items ˆθlc contain positive items. greatest consistent learner extends hypothesis interval directions much possible hitting negative too. points positive deﬁne ˆθgc proposition neither super-teachable. trivially ˆθlc assume thus note ˆθlc least positive point. ˆθlc min{x min{x max{x max{x thus ˆθlc ˆθlc ˆθlc ˆθlc thus ˆθlc proof similar showing ˆθgc leads open question family learners super teachable? offer conjecture here speculate mles satisfy asymptotic normality conditions super teachable. conjecture motivated similarity proof section also note counterexamples classic examples satisfy asymptotic normality conditions. another open question concerns optimal super-teaching subset size given training size example result teaching gaussian mean indicates rate improves grows. however analysis applies ﬁxed research needed identify optimal acknowledgments r.n. acknowledges support iis- ccf-. p.r. supported part grants dms- dms-tripods- darpa wnf--- n--- grant corporation. x.z. supported part ccf- iis- cmmi- dge- ccf-.", "year": 2018}