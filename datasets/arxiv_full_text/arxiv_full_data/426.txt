{"title": "Curiosity-driven Exploration by Self-supervised Prediction", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "abstract": "In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/", "text": "learn explore level- figure discovering play super mario bros without rewards. using curiosity-driven exploration agent makes signiﬁcant progress level-. gained knowledge helps agent explore subsequent levels much faster starting scratch. watch video http//pathak. github.io/noreward-rl/ altogether possible construct shaped reward function. problem agent receives reinforcement updating policy succeeds reaching pre-speciﬁed goal state. hoping stumble goal state chance likely futile simplest environments. human agents accustomed operating rewards sparse experience twice lifetime all. three-year-old enjoying sunny sunday afternoon playground trappings modern life college good house family future provide useful reinforcement signal. three-year-old trouble entertaining playground using psychologists call intrinsic motivation curiosity motivation/curiosity used explain need explore environment discover novel states. french word ﬂˆaneur perfectly captures notion curiosity-driven observer deliberately aimless pedestrian unencumbered obligation sense urgency generally curiosity learning skills might come handy pursuing rewards future. similarly intrinsic motivation/rewards become critical whenever extrinsic rewards sparse. formulations intrinsic reward grouped broad classes encourage agent explore novel states sparse extrinsic reward curiosity allows fewer interactions environment reach goal; exploration extrinsic reward curiosity pushes agent explore efﬁciently; generalization unseen scenarios knowledge gained earlier experience helps agent explore places much faster starting scratch. reinforcement learning algorithms learning policies achieving target tasks maximizing rewards provided environment. scenarios rewards supplied agent continuously e.g. running score atari game distance between robot object reaching task however many real-world scenarios rewards extrinsic agent extremely sparse missmeasuring novelty requires statistical model distribution environmental states whereas measuring prediction error/uncertainty requires building model environmental dynamics predicts next state given current state action executed time models hard build highdimensional continuous state spaces images. additional challenge lies dealing stochasticity agent-environment system noise agent’s actuation causes end-effectors move stochastic manner fundamentally inherent stochasticity environment. give example agent receiving images state inputs observing television screen displaying white noise every state novel would impossible predict value pixel future. examples stochasticity include appearance changes shadows moving entities presence distractor objects agents environment whose motion hard predict also irrelevant agent’s goals. somewhat different related challenge generalization across physically distinct functionally similar parts environment crucial largescale problems. proposed solution problems reward agent encounters states hard predict learnable however estimating learnability non-trivial problem work belongs broad category methods generate intrinsic reward signal based hard agent predict consequences actions i.e. predict next state given current state executed action. however manage escape pitfalls previous prediction approaches following insight predict changes environment could possibly actions agent affect agent ignore rest. instead making predictions sensory space transform sensory input feature space information relevant action performed agent represented. learn feature space using self-supervision training neural network proxy inverse dynamics task predicting agent’s action given current next states. since neural network required predict action incentive represent within feature embedding space factors variation environment affect agent itself. feature space train forward dynamics model predicts feature representation next state given feature representation current state action. provide prediction error forward dynamics model agent intrinsic reward encourage curiosity. role curiosity widely studied context solving tasks sparse rewards. opinion curiosity fundamental uses. curiosity helps agent explore environment quest knowledge further curiosity mechanism agent learn skills might helpful future scenarios. paper evaluate effectiveness curiosity formulation three roles. ﬁrst compare performance agent without curiosity signal navigation tasks sparse extrinsic reward vizdoom environment. show curiosity-driven intrinsic reward crucial accomplishing tasks next show even absence extrinsic rewards curious agent learns good exploration policies. instance agent trained curiosity reward able cross signiﬁcant portion level- super mario bros. similarly vizdoom agent learns walk intelligently along corridors instead bumping walls getting stuck corners question naturally follows whether learned exploratory behavior speciﬁc physical space agent trained enables agent perform better unseen scenarios too? show exploration policy learned ﬁrst level mario helps agent explore subsequent levels faster intelligent walking behavior learned curious vizdoom agent transfers completely textures results suggest proposed method enables agent learn generalizable skills even absence explicit goal. agent composed subsystems reward generator outputs curiosity-driven intrinsic reward signal policy outputs sequence actions maximize reward signal. addition intrinsic rewards agent optionally also receive extrinsic reward environment. intrinsic curiosity reward generated agent time extrinsic policy sub-system trained maximize ward rewards mostly zero. figure agent state interacts environment executing action sampled current policy ends provided environment curiosity state st+. policy trained optimize extrinsic reward generated proposed intrinsic curiosity module encodes states based intrinsic reward signal trained predict forward model takes inputs predicts feature representation st+. prediction error feature space used curiosity based intrinsic reward signal. incentive encode environmental features inﬂuence inﬂuenced agent’s actions learned exploration strategy agent robust uncontrollable aspects environment. unless speciﬁed otherwise notation denote parameterized policy curiosity reward model potentially used range policy learning methods; experiments discussed here asynchronous advantage actor critic policy gradient policy learning. main contribution designing intrinsic reward signal based prediction error agent’s knowledge environment scales high-dimensional continuous state spaces like images bypasses hard problem predicting pixels unaffected unpredictable aspects environment affect agent. making predictions sensory space undesirable hard predict pixels directly also unclear predicting pixels even right objective optimize. consider using prediction error pixel space curiosity reward. imagine scenario agent observing movement tree leaves breeze. since inherently hard model breeze even harder predict pixel location leaf. implies pixel prediction error remain high agent always remain curious leaves. motion leaves inconsequential agent therefore continued curiosity undesirable. underlying problem agent unaware parts state space simply cannot modeled thus agent fall artiﬁcial curiosity trap stall exploration. novelty-seeking exploration schemes record counts visited states tabular form also suffer issue. measuring learning progress instead prediction error proposed past solution unfortunately currently known computationally feasible mechanisms measuring learning progress. observation space right feature space making predictions prediction error provides good measure curiosity? answer question divide sources modify agent’s observations three cases things controlled agent; things agent cannot control affect agent things agent’s control affecting agent good feature space curiosity model unaffected latter because source variation inconsequential agent agent incentive know curiosity formulation intrinsic curiosity module incentive feature space encode environmental features inﬂuenced agent’s actions agent receive rewards reaching environmental states inherently unpredictable exploration strategy robust presence distractor objects changes illumination nuisance sources variation environment. figure illustration formulation. inverse models investigated learn features recognition tasks agrawal constructed joint inverse-forward model learn feature representation task pushing objects. however used forward model regularizer training inverse model features make error forward model predictions curiosity reward training agent’s policy. scalar weighs inverse model loss forward model loss scalar weighs importance policy gradient loss importance learning intrinsic reward signal. evaluate curiosity module ability improve exploration provide generalization novel scenarios simulated environments. section describes details environments experimental setup. environments ﬁrst environment evaluate vizdoom game. consider doom navigation task action space agent consists four discrete actions move forward move left move right no-action. testing setup experiments ‘doommywayhome-v’ environment available part openai episodes terminated either agent ﬁnds vest agent exceeds maximum time steps. consists rooms connected corridors agent tasked reach ﬁxed goal location spawning location. agent provided sparse terminal reward ﬁnds vest zero otherwise. generalization experiments pre-train instead hand-designing feature representation every environment come general mechanism learning feature representations prediction error learned feature space provides good intrinsic reward signal. propose feature space learned training deep neural network sub-modules ﬁrst sub-module encodes state feature vector second submodule takes inputs feature encoding consequent states predicts action taken agent move state st+. training neural network amounts learning function deﬁned where loss function measures discrepancy predicted actual actions. case discrete output soft-max distribution across possible actions minimizing amounts maximum likelihood estimation multinomial distribution. learned function also known inverse dynamics model tuple required learn obtained agent interacts environment using current policy scaling factor. order generate curiosity based intrinsic reward signal jointly optimize forward inverse dynamics loss described equations respectively. inverse model learns feature space encodes information relevant predicting agent’s actions forward model makes predictions feature space. refer proposed different different random textures episode lasts time steps. sample frames vizdoom shown figure maps explained figure takes approximately steps optimal policy reach vest location farthest room second environment classic nintendo game super mario bros consider four levels game pre-training ﬁrst level showing generalization subsequent levels. setup reparametrize action space agent unique actions following game played using joystick allowing multiple simultaneous button presses duration press affects action taken. property makes game particularly hard e.g. make long jump tall pipes wide gaps agent needs predict action times introducing long-range dependencies. experiments mario trained using curiosity signal only without reward game. training details agents work trained using visual inputs pre-processed manner similar input images converted gray-scale re-sized order model temporal dependencies state representation environment constructed concatenating current frame three previous frames. closely following action repeat four training time vizdoom action repeat mario. however sample policy without action repeat inference. following asynchronous training protocol agents trained asynchronously twenty workers using stochastic gradient descent. used adam optimizer parameters shared across workers. figure maps vizdoom environment generalization experiments environment agent pre-trained using curiosity signal without reward environment. denotes starting position. testing vizdoom experiments. green star denotes goal location. blue dots refer agent spawning locations dense case. rooms ﬁxed start locations agent sparse very sparse reward cases respectively. note textures also different train test maps. kernel size stride padding exponential linear unit used convolution layer. output last convolution layer lstm units. seperate fully connected layers used predict value function action lstm feature representation. intrinsic curiosity module architecture intrinsic curiosity module consists forward inverse model. inverse model ﬁrst maps input state feature vector using series four convolution layers ﬁlters kernel size stride padding non-linearity used convolution layer. dimensionality inverse model concatenated single feature vector passed inputs fully connected layer units followed output fully connected layer units predict four possible actions. forward model constructed concatenating passing sequence fully connected layers units respectively. value equation minimized learning rate baseline methods ‘icm denotes full algorithm combines intrinsic curiosity model across different experiments compare approach three baselines. first vanilla ‘ac’ algorithm \u0001-greedy exploration. second ‘icm-pixels variant without inverse model curiosity reward dependent forward model loss predicting next observation pixel space. design this remove inverse model layers append figure comparing performance agent curiosity curiosity pixel space agent proposed curious icm-ac agent hardness exploration task gradually increased left right. exploration becomes harder larger distance initial goal locations dense sparse very sparse. results depict succeeding harder exploration task becomes progressively harder baseline whereas curious able achieve good score scenarios. pixel based curiosity works dense sparse fails sparse reward setting. protocol followed plots involves running three independent runs algorithm. darker line represents mean shaded area represents mean standard error mean. perform tuning random seeds. deconvolution layers forward model. icm-pixels close architecture incapable learning embedding invariant uncontrollable part environment. note icm-pixels representative previous methods compute information gain directly using observation space show directly using observation space computing curiosity signiﬁcantly worse learning embedding icm. finally include comparison state-of-the-art exploration methods based variational information maximization trained trpo. qualitatively quantitatively evaluate performance learned policy without proposed intrinsic curiosity signal environments vizdoom super mario bros. three broad settings evaluated sparse extrinsic reward reaching goal exploration extrinsic reward generalization novel scenarios vizdoom generalization evaluated novel novel textures mario evaluated subsequent game levels. perform extrinsic reward experiments vizdoom using ‘doommywayhome-v’ setup described section extrinsic reward sparse provided agent ﬁnds goal located ﬁxed location map. systematically varied difﬁculty goaldirected exploration task varying distance initial spawning location agent location goal. larger distance means chances reaching goal location random exploration lower consequently reward said sparser. varying degree reward sparsity consider three setups dense sparse very-sparse rewards settings reward always terminal episode terminates upon reaching goal maximum steps. dense reward case agent randomly spawned possible spawning locations uniformly distributed across map. hard exploration task sometimes agent randomly initialized close goal therefore random \u0001-greedy exploration reach goal reasonably high probability. sparse very sparse reward cases agent always spawned room- room- respectively steps away goal optimal policy. long sequence directed actions required reach goals rooms making settings hard goal directed exploration problems. results shown figure indicate performance baseline degrades sparser rewards curious agents superior cases. dense reward case curious agents learn much faster indicating efﬁcient exploration environment compared \u0001-greedy exploration baseline agent. possible explanation inferior performance icm-pixels comparison every episode agent spawned seventeen rooms different textures. hard learn pixel-prediction model number textures increases. vizdoom sparse reward setup described above. trpo general sample efﬁcient takes wall-clock time. show results plots trpo different setups. hyperparameters accuracy trpo vime results follow concurrent work despite sample efﬁciency trpo agents work signiﬁcantly better trpo trpo-vime terms convergence rate accuracy. results shown table below figure evaluating robustness presence uncontrollable distractors environment. created distractor replacing visual observation agent white noise results show succeeds times pixel prediction model struggles. sparse reward case expected baseline agent fails solve task curious agent able learn task quickly. note icm-pixels similar convergence because ﬁxed spawning location agent icm-pixels encounters textures starting episode makes learning pixel-prediction model easier compared dense reward case. finally very sparse reward case agent icm-pixels never succeed agent achieves perfect score random runs. indicates better suited icm-pixels vanilla hard goal directed exploration tasks. robustness uncontrollable dynamics testing robustness proposed formulation changes environment affect agent augmented agent’s observation ﬁxed region white noise made image vizdoom navigation ideally agent unaffected noise noise affect agent anyway merely nuisance. figure compares performance baseline methods sparse reward setup described above. while proposed agent achieves perfect score icm-pixels suffers signiﬁcantly despite succeeded sparse reward task inputs augmented noise indicates contrast icm-pixels insensitive nuisance changes environment. sanity check replaced curiosity network random noise sources used curiosity reward. performed systematic sweep across different distribution parameters sparse reward case uniform gaussian laplacian. found none agents able reach goal showing curiosity module learn degenerate solutions. good exploration policy allows agent visit many states possible even without goals. case navigation expect good exploration policy cover much possible; case playing game expect visit many game states possible. order test agent learn good exploration policy trained vizdoom mario without rewards environment. evaluated portion explore much progress made setting. surprise found cases noreward agent able perform quote well vizdoom coverage exploration. agent trained extrinsic rewards able learn navigate corridors walk rooms explore many rooms doom environment. many occasions agent traversed entire reached rooms farthest away room initialized given episode terminates steps farthest rooms steps away result quite remarkable demonstrating possible learn useful skills without requirement external supervision rewards. example explorations shown figure ﬁrst maps show agent exfigure column ﬁgure shows visitation pattern agent exploring environment. arrow shows initial location orientation agent start episode. room agent visits exploration maximum steps colored. ﬁrst three columns show exploration strategy agent trained curiosity driven internal reward signal only. last columns show rooms visited agent conducting random exploration. results clearly show curious agent trained intrinsic rewards explores signiﬁcantly larger number rooms compared randomly exploring agent. plore much larger state space without extrinsic signal compared random exploration agent often hard time getting around local minima state spaces e.g. getting stuck wall able move mario learning play rewards. train agent super mario world using curiosity based signal. without extrinsic reward environment mario agent learn cross level-. agent received reward killing dodging enemies avoiding fatal events automatically discovered behaviors possible reason getting killed enemy result seeing small part game space making curiosity saturate. order remain curious agent’s interest learn kill dodge enemies reach parts game space. suggests curiosity provides indirect supervision learning interesting behaviors. best knowledge ﬁrst demonstration agent learns navigate environment discovers play game making relatively complex visual imagery directly pixels without extrinsic rewards. several prior works reinforcement learning navigate environments pixel inputs playing atari games rely intermediate external rewards provided environment. previous section showed agent learns explore large parts space curiosity-driven exploration policy trained. however remains unclear whether agent done learning generalized skills efﬁciently exploring environment simply memorized training set. words would like know exploring space much learned behavior speciﬁc particular space much general enough useful novel scenarios? investigate question train reward exploratory behavior scenario evaluate resulting exploration policy three different ways apply learned policy scenario; adapt policy ﬁne-tuning curiosity reward only; adapt policy maximize extrinsic reward. happily three cases observe promising generalization results evaluate evaluate policy trained maximizing curiosity level- mario subsequent levels without adapting learned policy way. measure distance covered agent result executing policy levels shown table note policy performs surprisingly well level suggesting good generalization despite fact level- different structures enemies compared level-. however note running level well. ﬁrst seems contradict generalization results level-. however note level- similar global visual appearance level- whereas level- signiﬁcantly different indeed issue possible quickly adapt exploration policy level- little ﬁne-tuning. explore below. fine-tuning curiosity only table agent pre-trained level- ﬁne-tuned level- quickly overcomes mismatch global visual appearance achieves higher score training scratch number iterations. interestingly training from scratch level- worse ﬁne-tuned policy even training iterations pre-training ﬁne-tuning combined. possible reason level- difﬁcult level learning basic skills moving jumping killing enemies scratch much dangerous relative safety level-. result therefore might suggest ﬁrst pre-training earlier level table quantitative evaluation agent trained play super mario bros. using curiosity signal without rewards game. agent trained rewards level-. evaluate agent’s policy ﬁne-tuned subsequent levels. results compared settings mario agent train scratch level- using curiosity without extrinsic rewards. evaluation metric based distance covered mario agent. fine-tuning extrinsic rewards case agent actually learned useful exploratory behavior able learn quicker starting scratch even external rewards provided environment. perform evaluation vizdoom pre-train agent using curiosity reward showed figure test very sparse reward setting ‘doommywayhome-v’ environment uses different novel textures described earlier section results figure show agent pre-trained curiosity ﬁne-tuned external reward learns faster achieves higher reward agent trained scratch jointly maximize curiosity external rewards. result conﬁrms learned exploratory behavior also useful agent required achieve goals speciﬁed environment. also worth noting icm-pixels generalize test environment. indicates proposed mechanism measuring curiosity signiﬁcantly better learning skills generalize compared measuring curiosity sensory space. curiosity-driven exploration well studied topic reinforcement learning literature good summary found schmidhuber surprise compression progress intrinsic rewards. classic work kearns brafman propose exploration algorithms polynomial number state space parameters. others used empowerment information gain based entropy actions intrinsic rewards stadie prediction error feature space auto-encoder measure interesting states explore. state visitation counts also investigated exploration osband train multiple value functions makes figure performance agents test vizdoom very sparse reward case. fine-tuned models learn exploration policy without external rewards training maps ﬁne-tuned test map. scratch models directly trained test map. ﬁne-tuned signiﬁcantly outperforms indicating curiosity formulation able learn generalizable exploration policies. pixel prediction based agent completely fail. note textures also different train test. ﬁne-tuning later produces form curriculum aids learning generalization. words agent able knowledge acquired playing level- better explore subsequent levels. course game designers purpose allow human players gradually learn play game. however interestingly ﬁne-tuning exploration policy pre-trained level- level- deteriorates performance compared running level hard agent cross beyond certain point agent hits curiosity blockade unable make progress. agent already learned parts environment hard point receives almost curiosity reward result attempts update policy almost zero intrinsic rewards policy slowly degenerates. behavior vaguely analogous boredom agent unable make progress gets bored stops exploring. bootstrapping thompson sampling exploration. many approaches measure information gain exploration houthooft exploration strategy maximizes information gain agent’s belief environment’s dynamics. approach jointly training forward inverse models learning feature space similarities works learned models dynamics planning sequence actions instead exploration. idea using proxy task learn semantic feature embedding used number works self-supervised learning computer vision concurrent work number interesting related papers appeared arxiv present work submission. sukhbaatar generates supervision pre-training asymmetric self-play agents improve data efﬁciency ﬁne-tuning. several methods propose improving data efﬁciency algorithms using self-supervised prediction based auxiliary tasks learn discriminative models gregor empowerment based measure tackle exploration sparse reward setups. work propose mechanism generating curiosity-driven intrinsic reward signal scales high dimensional visual inputs bypasses difﬁcult problem predicting pixels ensures exploration strategy agent unaffected nuisance factors environment. demonstrate agent signiﬁcantly outperforms baseline curiosity recently proposed vime formulation exploration baseline pixel-predicting formulation. vizdoom agent learns exploration behavior moving along corridors across rooms without rewards environment. mario agent crosses level- without rewards game. reason agent unable beyond limit presence game requires speciﬁc sequence presses order jump across agent unable execute sequence falls dies receiving further rewards environment. therefore receives gradient information indicating world beyond could potentially explored. issue somewhat orthogonal developing models curiosity presents challenging problem policy learning. common practice evaluate reinforcement learning approaches environment used training. however feel also important evaluate separate testing well. allows gauge much learned speciﬁc training environment much might constitute generalizable skills could applied settings. paper evaluate generalization ways applying learned policy scenario ﬁnetuning learned policy scenario believe evaluating generalization valuable tool allow community better understand performance various reinforcement learning algorithms. effort make code algorithm well testing environment setups freely available online. interesting direction future research learned exploration behavior/skill motor primitive/lowlevel policy complex hierarchical system. example vizdoom agent learns walk along corridors instead bumping walls. could useful primitive navigation system. rich diverse real world provides ample opportunities interaction reward signals sparse. approach excels setting converts unexpected interactions affect agent intrinsic rewards. however approach directly extend scenarios opportunities interactions also rare. theory could save events replay memory guide exploration. however leave extension future work. acknowledgements would like thank sergey levine evan shelhamer saurabh gupta phillip isola members bair fruitful discussions comments. thank jacob help figure alexey dosovitskiy vizdoom maps. work supported part iis- iis- iis- muri n--- berkeley deepdrive equipment grant nvidia valrhona reinforcement learning fellowship. jaderberg mnih volodymyr czarnecki wojciech marian schaul leibo joel silver david kavukcuoglu koray. reinforcement learning unsupervised auxiliary tasks. iclr lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. iclr lopes manuel lang tobias toussaint marc oudeyer pierre-yves. exploration model-based reinforcement learning empirically estimating learning progress. nips mirowski piotr pascanu razvan viola fabio soyer hubert ballard andy banino andrea denil misha goroshin ross sifre laurent kavukcuoglu koray learning navigate complex environments. iclr mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous icml methods deep reinforcement learning. kempka michał wydmuch marek runc grzegorz toczek jakub ja´skowski wojciech. vizdoom doom-based research platform visual reinforcement learning. arxiv. schmidhuber jurgen. possibility implementing curiosity boredom model-building neural controllers. animals animats proceedings ﬁrst international conference simulation adaptive behavior", "year": 2017}