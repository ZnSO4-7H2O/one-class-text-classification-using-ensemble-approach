{"title": "Learned Optimizers that Scale and Generalize", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Learning to learn has emerged as an important direction for achieving artificial intelligence. Two of the primary barriers to its adoption are an inability to scale to larger problems and a limited ability to generalize to new tasks. We introduce a learned gradient descent optimizer that generalizes well to new tasks, and which has significantly reduced memory and computation overhead. We achieve this by introducing a novel hierarchical RNN architecture, with minimal per-parameter overhead, augmented with additional architectural features that mirror the known structure of optimization tasks. We also develop a meta-training ensemble of small, diverse optimization tasks capturing common properties of loss landscapes. The optimizer learns to outperform RMSProp/ADAM on problems in this corpus. More importantly, it performs comparably or better when applied to small convolutional neural networks, despite seeing no neural networks in its meta-training set. Finally, it generalizes to train Inception V3 and ResNet V2 architectures on the ImageNet dataset for thousands of steps, optimization problems that are of a vastly different scale than those it was trained on. We release an open source implementation of the meta-training algorithm.", "text": "learning learn emerged important direction achieving artiﬁcial intelligence. primary barriers adoption inability scale larger problems limited ability generalize tasks. introduce learned gradient descent optimizer generalizes well tasks significantly reduced memory computation overhead. achieve introducing novel hierarchical architecture minimal perparameter overhead augmented additional architectural known structure optimization tasks. also develop meta-training ensemble small diverse optimization tasks capturing common properties loss landscapes. optimizer learns outperform rmsprop/adam problems corpus. importantly performs comparably better applied small convolutional neural networks despite seeing neural networks meta-training set. finally generalizes train inception resnet architectures imagenet dataset thousands steps optimization problems vastly different scale trained release open source implementation meta-training algorithm. optimization bottleneck almost tasks machine learning well many ﬁelds including engineering design operations research statistics. advances optimization therefore broad impact. historically optimization performed using hand-designed algorithms. recent results machine learning show that given sufﬁcient data well-trained neural networks often outperform hand-tuned approaches supervised tasks. raises tantalizing possibility neural networks able outperform hand-designed optimizers. despite promise approach previous work learned optimizers gradient descent failed produce neural network optimizers generalize problems continue make progress problems meta-trained large numbers steps current neural network optimizers additionally costly memory computation scale larger problems. incorporating features motivated successful handdesigned optimizers build existing techniques. include dynamically adapted input output scaling momentum multiple time scales cross nesterov momentum attention mechanisms. improving meta-optimization pipeline instance introducing meta-objective better encourages exact convergence optimizer drawing number optimization steps training heavy tailed distribution. learning learn long history psychology inspired machine learning researchers proposed meta-learning techniques optimizing process learning itself. schmidhuber example considers networks able modify weights. series papers bengio presents methods learning parameterized local neural network update rules avoid back-propagation. runarsson jonsson extend complex update models. result meta learning cases algorithm i.e. local update rule. andrychowicz learn learn gradient descent gradient descent. rather trying distill global objective local rule work focuses learning integrate gradient observations time order achieve fast learning model. component-wise structure algorithm allows single learned algorithm applied problems different dimensionality. andrychowicz consider issue transfer different datasets model structures focus transferring problems class. fact report negative results transferring optimizers meta-trained optimize neural networks logistic functions networks relu functions. malik proposed approach similar andrychowicz around time rely policy search compute meta-parameters optimizer. learn learn gradient descent reinforcement learning. zoph also meta-train controller time produce string custom domain speciﬁc language describing neural network architectures. architecture matching produced conﬁguration instantiated trained ordinary way. case meta-learning happens network architecture level. ravi larochelle modify optimizer andrychowicz -shot learning tasks. test error optimize meta learner. tasks nice property recurrent neural networks need unrolled small number steps. wang show possible learn solve reinforcement learning tasks reinforcement learning. demonstrate approach several examples bandits cognitive science literature. related approach proposed duan high level hierarchical constructed learned optimizer architecture matched parameters target problem. hierarchical rnn’s parameters shared across target problems despite architecture adapts target problem applied problems. optimization step learned optimizer receives gradients every parameter along additional quantities derived gradients outputs update parameters. figure gives overview. figure hierarchical architecture. lowest level small parameter processes inputs outputs every parameter target problem. intermediate level medium-sized tensor exists every parameter tensor target problem. takes input average latent state across parameter rnns belonging tensor. output enters parameter rnns bias term. level single global receives input average hidden state parameter rnns output enters tensor rnns bias term added parameter bias term. architecture perparameter overhead tensor rnns able capture inter-parameter dependencies global able capture inter-tensor dependencies. order effectively scale large problems optimizer must stay quite small maintaining enough ﬂexibility capture inter-parameter dependencies shape geometry loss surface. optimizers account second order information often particularly effective propose novel hierarchical architecture enable perparameter computational cost aggregation gradient information coordination update steps across parameters lowest level hierarchy small parameter receives direct perparameter gradient inputs. level intermediate tensor incorporates information subset parameter rnns example consider feedforward fully-connected neural network. would tensor layer network layer contains weight matrix therefore parameter rnns. highest level hierarchy global receives output every tensor rnn. allows parameter hidden units larger tensor global rnns keeping track problem-level information. tensor global rnns also serve communication channels parameter tensor rnns respectively. tensor outputs biases parameter parameter state averaged input tensor rnn. similarly global state bias tensor output tensor rnns averaged input global architecture used experimental results parameter hidden state size tensor global state size layer parameter units layer). sizes showed best generalization convnets complex test problems. experimentally found could make parameter small tensor small still good performance problems. also found performance decreased slightly even simple test problems removed global entirely. used architecture three levels. best performing neural networks often knowledge task structure baked design. examples include convolutional models image processing causal models modeling causal time series data merging neural value functions monte carlo tree search alphago similarly incorporate knowledge effective strategies optimization network architecture. emphasize arbitrary design choices. features motivated results optimization recurrent network literature. also individually important ability learned optimizer generalize nesterov momentum powerful optimization approach parameter updates based gradient evaluated current iterate rather location extrapolated ahead current iterate. similarly attention mechanisms proven extremely powerful recurrent translation models decoupling iteration dynamics observed portion input sequence. motivated successes incorporate attention mechanism allows optimizer explore regions loss surface computing gradients away current parameter position. training step attended location +∆φn described equation offset below. note attended location offset previous parameter location rather previous attended location gradient loss respect attended parameter values provide input learned optimizer though transformed passed hierarchical rnn. every parameter tensor momentum exponential moving average typically motivated terms averaging away minibatch noise high frequency oscillations often effective feature provide learned optimizer exponential moving averages ¯gts gradients several timescales indexes timescale average. update equation moving average would like optimizer invariant parameter scale. additionally rnns easily trained inputs well conditioned similar scale latent state. order goals rescale average gradients fashion similar done rmsprop adam smorms running average square average graλn scaled averaged gradient momendient logit shortest timescale output similar timescales momentum computed previous section. useful learned optimizer access gradient magnitudes changing training time. therefore provide input measure relative gradient magnitudes averaging scale speciﬁcally provide relative gradient magnitudes another aspect rmsprop adam learning rate corresponds directly characteristic step length. true gradient scaled running estimate standard deviation scaling characteristic magnitude length update steps therefore scales linearly learning rate invariant scaling gradients. relative learning rate want performance optimizer invariant parameter scale. requires optimizer judge correct step length history gradients rather memorizing range step lengths useful meta-training ensemble. therefore controls step length outputing multiplicative change rather outputing step length directly specstability reasons step length iﬁed relative exponential running average meta-learned momentum attended parameter step length meta-learned constant offset force optimizer dynamically adapt learning rate rather memorizing learning rate trajectory learning rate initialized uniform distribution emphasize direct access learning rate must adjust based purely observations statistics gradients. described preceding sections full parameter inputs tensor rel} corresponding scaled averaged gradi{mn ents relative gradient magnitudes relative learning rate. full parameter outputs tensor corresponding payn rameter attention update directions change step length momentum logits. outputs read learned afﬁne transformation payn rameter hidden state. readout biases clamped computational cost minibatch size total number parameters number parameter tensors latent sizes parameter tensor global rnns respectively. typically regime case computanp tional cost simpliﬁes minibatch size increased computational cost learned optimizer approaches vanilla cost computing gradient dominates cost computing parameter update. optimizer meta-trained standard optimizer ensemble target optimization tasks. call process meta-training parameters optimizer meta-parameters. previous learned optimizers failed generalize beyond problem meta-trained. order address this meta-train optimizer ensemble small problems chosen capture many commonly encountered properties loss landscapes stochastic gradients. meta-training small problems also avoid memory issues would encounter meta-training large real-world problems. bingham examples various loss landscape pathologies. consisted rosenbrock ackley beale booth styblinski-tang matyas branin michalewicz log-sum-exp functions. included number well-behaved convex loss functions consisting quadratic bowls varying dimension randomly generated coupling matrices logistic regression randomly generated generally linearly separable data. logistic regression problem data fully linearly separable global minimum greater problems randomly generated data logistic regression minibatches various sizes also used minibatch quadratic task minibatch loss consisted square inner product parameters random input vectors. included several tasks optimization could proceed slowly despite small problem size. included many-dimensional oscillating valley whose global minimum lies inﬁnity problem loss consisting strong coupling terms parameters sequence. additionally included task loss depends minimum maximum valued parameter gradients extremely sparse loss discontinuous gradients. simulate problems sparse gradients transformation sets large fraction gradient entries training step. simulate problems different scaling across parameters added transformation performs linear change variables change relative scale parameters. simulate problems different steepness-proﬁles course learning added transformation applied monotonic transformations ﬁnal loss. finally simulate complex tasks diverse parts added multi-task transformation summed loss concatenated parameters diverse probsecond term constant full meta-parameters learned optimizer consisting {ψp-rnn ψt-rnn ψg-rnn ψ-rnn indicates weights biases parameter tensor global learning rate momentum attended step offset minimizing average function value rather average function value better encourages exact convergence minima precise dynamic adjustment learning rate based gradient history average logarithm also closely resembles minimizing ﬁnal function value still providing meta-learning signal every training step since small values make outsized contribution average taking logarithm. meta-learning gradients computed backpropagation partial unrolling optimization target problem similarly andrychowicz note andrychowicz dropped second derivative terms backpropagation limitations torch. compute full gradient tensorflow including second derivatives. order encourage learned optimizer generalize long training runs number partial unrollings number optimization steps within partial unroll drawn heavy tailed exponential distribution. resulting distribution shown appendix figure training loss versus number optimization steps mnist learned optimizer paper compared optimizer andrychowicz adam rmsprop optimizer previous work meta-trained -layer fullyconnected network sigmoidal nonlinearities. test problems -layer fully-connected network -layer convolutional network. cases relu activations minibatches size used. figure three sample problems meta-training corpus learned optimizer outperforms rmsprop adam. learning rates rmsprop adam chosen good average performance across problem types training test set. learned optimizer generally beats optimizers problems training set. previous learned optimizer architectures like andrychowicz perform well problems meta-trained. however generalize well architectures scale well longer timescales. figure shows performance optimizer metatrained -layer perceptron sigmoid activations learned optimizer matches performance adam rmsprop momentum four problems never seen meta-training set. non-learned optimizer optimal learning rate problem chosen sweep learning rates actual learning rates used shown inset legend. training loss imagenet data early training function number training examples seen optimizer performance highly dependent hyperparameters learned optimizer performance similar best tuned optimizers cases learned optimizer used distributed synchronized learning effective minibatch size inception plot generated newer version codebase small improvements described appendix inception optimizers used learning rate effective minibatch size resnet optimizers used learning rate effective minibatch size problem type relu activations problem type cases dataset minibatch size used. contrast optimizer meta-trained dataset neural network problems shows performance comparable adam rmsprop even numbers iterations seen meta-training learned optimizer matches outperforms adam rmsprop problem types meta-training exact setup problem type seen python code supplementary materials. meta-training problem include convolutional fully-connected layers. despite this comparable performance adam rmsprop momentum simple convolutional multi-layer networks multi-layer fully connected networks terms ﬁnal loss number iterations convergence also tested learned optimizer inception resnet figure shows learned optimizer able stably train networks ﬁrst steps performance similar traditional optimizers tuned speciﬁc problem. unfortunately later training learned optimizer stops making effective progress loss approaches constant addressing issue would goal future work. figure learned optimizer performance robust learning rate hyperparameter. training curves randomly generated quadratic loss problem different learning rate initializations. time-consuming aspect training neural networks current optimizers choosing right learning rate problem. learned optimizer also sensitive initial learning rate much robust. figure shows learned optimizer’s training loss curve quadratic problem different initial learning rates comfigure wall clock time seconds single gradient update step -layer convnet architecture workstation nvidia titan gpu. batch size increases total computation time learned optimizer approaches adam. large problems like resnet inception imagenet dataset. achieve results introduced novel hierarchical architecture reduces memory overhead allows communication across parameters augmented additional features shown useful previous optimization recurrent neural network literature. also developed ensemble small optimization problems capture common diverse properties loss landscapes. although wall clock time optimizing problems lags behind simpler optimizers difference decrease increasing batch size. shown ability rnn-based optimizers generalize problems look forward future work optimizing optimizers. design choices described section matter performance optimizer. experiments removed different features re-meta-trained optimizer scratch. kept features which average made performance better variety test problems. speciﬁcally kept features described attention momentum multiple timescales dynamic input scaling relative learning rate found important take logarithm meta-objective described addition found helpful learn initial weights accumulation decay multiple gradient timescales though features effect features crucial others terms consistently improved performance. figure shows test problem ﬁnal features learned optimizer matter. experiments small minibatches signiﬁcantly underperform adam rmsprop terms wall clock time. however consistent prediction since overhead constant terms minibatch overhead made small increasing minibatch size. references andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul shillingford brendan freitas nando. learning learn gradient descent gradient descent. advances neural information processing systems bengio yoshua bengio samy cloutier jocelyn. learning synaptic learning rule. universit´e montr´eal d´epartement d’informatique recherche op´erationnelle bengio yoshua bengio samy cloutier jocelyn gecsei jan. optimization synaptic learning rule. conference optimality biological artiﬁcial networks chen yutian hoffman matthew colmenarejo sergio gomez denil misha lillicrap timothy freitas nando. learning learn global optimization black functions. arxiv report kyunghyun merri¨enboer bart bahdanau dzmitry bengio yoshua. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. duan schulman john chen bartlett peter sutskever ilya abbeel pieter. fast reinforcement learning slow reinforcement learning. technical report berkeley openai hochreiter sepp younger steven conwell peter learning learn using gradient descent. international conference artiﬁcial neural networks springer krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems runarsson thomas philip jonsson magnus thor. evolution design distributed learning rules. ieee symposium combinations evolutionary computation neural networks ieee santoro adam bartunov sergey botvinick matthew wierstra daan lillicrap timothy. meta-learning internamemory-augmented neural networks. tional conference machine learning schmidhuber jurgen. evolutionary principles selfreferential learning. learning learn meta-meta-meta...-hook. thesis institut informatik tech. univ. munich silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc mastering game deep neural networks tree search. nature szegedy christian vanhoucke vincent ioffe sergey shlens wojna zbigniew. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition tieleman tijmen hinton geoffrey. lecture .rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning wang jane kurth-nelson tirumala dhruva soyer hubert leibo joel munos r´emi blundell charles kumaran dharshan botvinick matt. learning reinforcement learn. arxiv report since expect primary driver update step direction order reduce information must stored parameter hidden state included meta-trainable linear projection average rescaled gradients update directions figure app.. histogram total number training iterations target problems meta-training. total number unrolls drawn exponential distribution scale plus constant offset number training iterations within unroll drawn exponential distribution scale constant offset small meta-training problems section meta-training learned optimizer often able optimize problem almost exactly early unrolled optimization meta-loss becomes relatively uninformative. order better simulate tasks take many steps optimize small gaussian noise added parameters optimization step. effectively moves loss landscape underneath optimizer providing informative learning signal after many unrolls forcing learned optimizer robust type noise. speciﬁcally parameter update becomes runts root-mean-square magnitude mismatch adam average gradient scaled running estimate root-meansquare magnitude non-averaged gradients. order consistent this order encourage betts modify equation normalize average gradient order simplify interactions parameters longer force normalization parameter attention still decompose update directions update product learning rate step. since attended update direction able take different magnitude separate attention learning rate longer required eliminated. equations thus become distribution meta-loss gradients observed assymmetrical heavy tailed. combination known cause biased parameter updates rmsprop adam since optimizers underweight contribution extremely rare extremely large gradients. order reduce tendency updated mean-quare-gradient momentum term rather metaoptimizer rmsprop", "year": 2017}