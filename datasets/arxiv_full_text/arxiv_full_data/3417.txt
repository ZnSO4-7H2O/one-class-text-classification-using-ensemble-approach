{"title": "Binarized Convolutional Landmark Localizers for Human Pose Estimation  and Face Alignment with Limited Resources", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Our goal is to design architectures that retain the groundbreaking performance of CNNs for landmark localization and at the same time are lightweight, compact and suitable for applications with limited computational resources. To this end, we make the following contributions: (a) we are the first to study the effect of neural network binarization on localization tasks, namely human pose estimation and face alignment. We exhaustively evaluate various design choices, identify performance bottlenecks, and more importantly propose multiple orthogonal ways to boost performance. (b) Based on our analysis, we propose a novel hierarchical, parallel and multi-scale residual architecture that yields large performance improvement over the standard bottleneck block while having the same number of parameters, thus bridging the gap between the original network and its binarized counterpart. (c) We perform a large number of ablation studies that shed light on the properties and the performance of the proposed block. (d) We present results for experiments on the most challenging datasets for human pose estimation and face alignment, reporting in many cases state-of-the-art performance. Code can be downloaded from https://www.adrianbulat.com/binary-cnn-landmarks", "text": "design architectures retain groundbreaking performance cnns landmark localization time lightweight compact suitable applications limited computational resources. make following contributions ﬁrst study effect neural network binarization localization tasks namely human pose estimation face alignment. exhaustively evaluate various design choices identify performance bottlenecks importantly propose multiple orthogonal ways boost performance. based analysis propose novel hierarchical parallel multi-scale residual architecture yields large performance improvement standard bottleneck block number parameters thus bridging original network binarized counterpart. perform large number ablation studies shed light properties performance proposed block. present results experiments challenging datasets human pose estimation face alignment reporting many cases state-of-the-art performance. code downloaded https//www. adrianbulat.com/binary-cnn-landmarks figure original bottleneck layer proposed hierarchical parallel multi-scale structure block increases receptive ﬁeld size improves gradient speciﬁcally designed number parameters original bottleneck contain convolutions general derived perspective improving performance efﬁciency binary networks. note layer depicted rectangular block containing ﬁlter size number input output channels; denotes concatenation element-wise sum. work localizing predeﬁned ﬁducial points objects interest typically undergo non-rigid deformations like human body face. recently work based convolutional neural networks revolutionized landmark localization demonstrating results remarkable accuracy even challenging datasets human pose estimation face alignment however deploying methods computationally expensive requiring high-end gpus learned models typically require hundreds thus rendering completely unsuitable real-time mobile applications. work highly accurate robust efﬁcient lightweight landmark localization using binarized cnns. work inspired recent results binarized architectures image classiﬁcation contrary works ﬁrst study effect neural network binarization ﬁne-grained tasks like landmark localization. similarly binarization results performance drop however address opted investigate propose several architectural innovations introduction completely novel hierar. ﬁrst study effect binarization state-of-the-art architectures problem localization namely human pose estimation face alignment. exhaustively evaluate various design choices identify performance bottlenecks. importantly describe multiple orthogonal ways boost performance; subsections based analysis propose hierarchical parallel multi-scale residual architecture speciﬁcally designed work well binary case. block results large performance improvement baseline binary residual block newly proposed block developed goal improving performance binary networks also show performance boost offered proposed architecture also generalizes extent case real-valued networks section reviews related work network quantization network design gives overview state-ofthe-art human pose estimation face alignment. network quantization. prior work suggests high precision parameters essential obtaining results image classiﬁcation. light this propose -bit quantization showing negligible performance drop small datasets proposes technique allocates different numbers bits network parameters activations gradients. binarization long considered impractical destructive property representation recently showed case quantizing good results actually obtained. introduces technique training cnns uses binary weights forward backward passes however real parameters still required training. work goes step binarizes parameters activations. case multiplications replaced elementary binary operations estimating binary weights help scaling factor ﬁrst work report good results large dataset notably method makes recent ﬁndings using quantizing weights replacing multiplications bit-wise operations. method differs aforementioned works respects instead focusing image classiﬁcation ﬁrst study neural network binarization context ﬁne-grained computer vision task namely landmark localization predicting dense output fully convolutional manner instead enhancing results improving quantization method follow completely different path enhancing performance proposing novel architectural design hierarchical parallel multi-scale residual block. block design. proposed method uses residualbased architecture hence starting point work bottleneck block described recently explores idea increasing cardinality residual block splitting series parallel sub-blocks topology behave ensemble. beyond bottleneck layers szegedy propose inception block introduces parallel paths different receptive ﬁeld sizes various ways lowering number parameters factorizing convolutional layers large ﬁlters smaller ones. follow-up paper authors introduce number inception-residual architectures. latter work related proposed method. method different aforementioned architectures following ways create hierarchical parallel multi-scale structure increases receptive ﬁeld size inside block improves gradient speciﬁcally designed number parameters original bottleneck block contain convolutions block derived perspective improving performance efﬁciency binary networks. network design. target propose network architecture landmark localization; hence used state-of-the-art hour-glass network makes bottleneck block interested efﬁciency experiments conducted using single network i.e. stacking baseline binary obtained directly quantizing using table shows signiﬁcant performance binary real valued hgs. bridge replacing bottleneck block used original proposed block. human pose estimation. recent work using cnns shown remarkable results methods computationally demanding requiring least high-end gpu. contrast network uses binary weights activations intended systems limited resources face alignment. current state-of-the-art large pose face alignment also based cnns however methods computationally demanding. network produces state-of-the-art results task designed devices limited resources. resnet consists type blocks basic bottleneck. interested latter designed reduce number parameters keep network memory footprint control. pre-activation version batch normalization activation function precede convolutional layer. block shown fig. note used version bottleneck deﬁned middle layer channels residual block main building block state-of-the-art architecture landmark localization predicts heatmaps fully convolutional fashion. network extension allowing however symmetric top-down bottom-up processing. also herein describe derive proposed binary hierarchical parallel multi-scale block fig. section reducing number parameters match ones original bottleneck derive block fig. section organized follows finally combining ideas architectures propose binary hierarchical parallel multi-scale block fig. note proposed block trivial combination aforementioned architectures completely structure. start original bottleneck blocks network following binarize keeping ﬁrst last layers network real. crucial especially last layer higher precision required producing dense output note layers account less total number parameters. performance original binarized networks seen table observe binarization results signiﬁcant performance drop. notice almost parts large difference performance clearly indicates binary network signiﬁcant less representational power. failure cases shown fig. illustrating binary network able learn difﬁcult poses. address better architecture detailed next four subsections. figure examples failure cases binarized predictions real-valued counterpart binary misses certain range poses similar accuracy correct parts. width second layer thus greatly reducing number parameters inside module. however unclear whether idea bottleneck structure also successful binary case too. limited representational power binary layers greatly reducing number channels might reduce amount information passed layer another leading lower performance. investigate this modify bottleneck block increasing number channels thin layer match number channels ﬁrst last layer effectively removing bottleneck increasing amount information passed block another. resulting wider block shown fig. here wider refers increased number channels initial thin layer. table illustrates improves performance baseline also raises memory requirements. conclusion widening thin layer offers tangible performance improvement however high computational cost. small ﬁlters shown effective efﬁcient models solely made combination convolutional layers and/or ﬁlters case real-valued networks large number kernels learned. however binary case number possible unique convolutional kernels limited states only size ﬁlter. address limited representation power ﬁlters binary case similarly largely depart block fig. proposing multi-scale structure fig. note implement multi-scale approach using larger ﬁlter sizes max-pooling greatly increase effective receptive ﬁeld within block. also goal analyze impact multi-scale approach alone intentionally keep number parameters similar level original bottleneck block fig. avoid leap number parameters decomposing ﬁlters layers ﬁlters preserving presence thin layer middle block. given above split input branches. ﬁrst branch works scale original bottleneck fig. layer projects channels going one. second branch performs multithe term wider strictly refers moderate increase number channels thin layer effectively removing bottleneck. except naming resemblance performs study wide deep using different building block alongside much higher number channels withform quantization. similar study falls outside scope work. scale analysis ﬁrstly passing input maxpooling layer creating branches using ﬁlter second using decomposed concatenating outputs sub-branches obtain remaining channels finally main branches concatenated adding channels back-projected help convolutional layer ﬁlters. accuracy proposed structure found table observe healthy performance improvement little additional cost similar computational requirements original bottleneck fig. conclusion designing binarized networks multiscale ﬁlters preferred. convolutions previously proposed block fig. opted avoid increase number parameters retaining convolutional layers ﬁlters. subsection relaxing restriction analyze inﬂuence ﬁlters overall network performance. particular remove convolutional layers ﬁlters multi-scale block fig. leading structure fig. motivation remove convolutions binary case following ﬁlters limited states limited learning power. nature behave simple ﬁlters deciding certain value passed not. practice allows input pass layer little modiﬁcations sometimes actually blocking good features hurting overall performance noticeable amount. particularly problematic task landmark localization high level detail required successful localization. examples problem shown fig. results reported table show removing convolutions performance baseline increased even interestingly newly introduced block outperforms subsection less parameters shows presence ﬁlters limits performance binarized cnns. conclusion convolutional ﬁlters binarized cnns detrimental effect performance avoided. binary networks even sensitive problem fading gradients network found gradients times smaller corresponding real-valued counterpart. alleviate this design module form hierarchical parallel multi-scale structure allowing resoluconclude that real case increasing number parameters results performance increase; however case binary networks tailored design proposed needed. table pckh-based performance mpii validation binary blocks parameters original bottleneck increased match parameters proposed block. ﬁrstly gives rise wider block variant without convolutions. binary match number parameters proposed bottleneck block follow paths. firstly increase number parameters bottleneck ﬁrst make block wider described section note order keep number input-output channels equal resulting block fig. higher number parameters proposed block. despite this performance gain moderate found convolutional layers detrimental effect performance multi-scale block fig. opted remove bottleneck block too. modiﬁed wider module removing convolutions halving number parameters order match number parameters proposed block. results table clearly show modiﬁcation helpful close performance achieved proposed block. figure examples features convolutional layer. often features copied little modiﬁcations usually consisting details’ removal. contrast altered better visualization. tion gradients different paths follow shortest always proposed block depicted fig. note that addition better gradient design encompasses ﬁndings previous subsections convolutional layers ﬁlters used block preserve width much possible multi-scale ﬁlters used. contrary blocks described subsections gradients need pass layers reaching output block newly proposed module convolutional layer direct path links output given time layers within module shortest possible path equal presence hierarchical structure inside module efﬁciently accommodates larger ﬁlters decomposed convolutional layers ﬁlters. furthermore design avoids usage element-wise summation layer example improving gradient keeping complexity control. figure different types blocks described evaluated. best performing block shown ﬁgure layer depicted rectangular block containing ﬁlter size number input channels number output channels). denotes concatenation operation element-wise sum. secondly decrease number parameters proposed block match number parameters original bottleneck. block shown fig. reduced number input-output channels proposed block number channels ﬁrst layer modiﬁed second layer third layer proposed block derived binary perspective table shows signiﬁcant performance gain also observed case real-valued networks. order quantify performance improvement allow fair comparison increase number channels inside original bottleneck block networks depth similar number parameters. even case block outperforms original block although gain smaller observed binary case. conclude real-valued networks section present series architectural variations effect performance binary network. reported results obtained using proposed block fig. coined ours final. focus effect augmentation different losses novel experiments reported comment effect pooling relus performance speed-up providing details supplementary material. augmentation required? recent works suggested binarization extreme case regularization light this might wonder whether data augmentation still required. table shows order accommodate presence poses and/or scale variations data augmentation helpful providing large increase performance. effect loss. trained binary network predict heatmaps landmark experimented types losses ﬁrst places gaussian around correct location landmark trains using pixel-wise loss however gradients generated loss usually small even case real-valued network. binarized networks tend amplify problem alternative also experimented sigmoid cross-entropy pixel-wise loss typically used detection tasks found sigmoid cross-entropy pixel-wise loss increased gradients offering improvement trained number epochs. pooling type. line found max-pooling outperforms average pooling resulting performance increase. also supplementary material. relus. line found adding relu activation convolutional layer performance increased observing performance improvement. also supplementary material. performance. line observed speedups compared cublas. conduct experiments cpus. however since used method binarization speed improvements order expected allowing system real-time using single core. terms memory compression achieve compression rate compared single precision counterpart torch. also supplementary material. section compare method current state-of-the-art human pose estimation face alignment. ﬁnal system comprises single network replaces real-valued bottleneck block used proposed binary parallel multi-scale block trained improvements detailed section moreover show proposed block generalizes well producing consistent results across various datasets tasks supplementary material provides results facial part segmentation experiment. human pose estimation. previous experiments used standard training-validation partition mpii table report performance proposed binary block proposed block implemented trained real values real-valued stacked network consisting stacked single realvalued networks trained intermediate supervision ﬁnally real-valued network bottleneck block replaced proposed block. results shown table observe single network proposed block trained real weights performance reaches result clearly illustrates enhanced learning capacity proposed block. moreover still binary real-valued version proposed block indicating margin improvement possible. also observe full-sized model based proposed block performs slightly better original network indicating that realvalued case block effective original smaller computational budget used. face alignment. used three challenging datasets large pose face alignment namely aflw aflw-pifa aflw-d evaluation metric normalized mean error aflw large-scale face alignment dataset consisting faces annotated landmarks. images captured arbitrary conditions exhibiting large variety poses expressions. table shows binarized network outperforms current state-of-the-art methods large real-valued cnns. aflw-pifa grey-scale subset aflw consisting images selected balanced number images angle images annotated points perspective. fig. shows results aflw-pifa. evaluated visible occluded points method improves upon current best result additional numerical results aflw-pifa also supplementary material. aflw-d subset aflw re-annotated perspective points. used dataset evaluation. training done using ﬁrst images w-lp fig. shows aflw-d improvement state-of-thetraining. models trained scratch following algorithm described using rmsprop initialization done human pose estimation randomly augmented data rotation ﬂipping scale jittering trained network epochs dropping learning rate four times similar procedure applied models face alignment difference training done epochs only. input normalized described networks trained using binary cross-entropy loss. models implemented torch proposed novel block architecture particularly tailored binarized cnns tasks human pose estimation face alignment. process exhaustively evaluated various design choices identiﬁed performance bottlenecks proposed solutions. showed hierarchical parallel multi-scale block enhances representational power allowing stronger relations learned without excessively increasing number network parameters. proposed architecture efﬁcient limited resources. adrian bulat funded scholarship university nottingham. georgios tzimiropoulos supported part epsrc project ep/mx/ facial deformable models animals.", "year": 2017}