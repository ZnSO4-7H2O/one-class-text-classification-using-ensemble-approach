{"title": "Learning Probabilistic Programs", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We develop a technique for generalising from data in which models are samplers represented as program text. We establish encouraging empirical results that suggest that Markov chain Monte Carlo probabilistic programming inference techniques coupled with higher-order probabilistic programming languages are now sufficiently powerful to enable successful inference of this kind in nontrivial domains. We also introduce a new notion of probabilistic program compilation and show how the same machinery might be used in the future to compile probabilistic programs for efficient reusable predictive inference.", "text": "develop technique generalising data models samplers represented program text. establish encouraging empirical results suggest markov chain monte carlo probabilistic programming inference techniques coupled higher-order probabilistic programming languages sufﬁciently powerful enable successful inference kind nontrivial domains. also introduce notion probabilistic program compilation show machinery might used future compile probabilistic programs efﬁcient reusable predictive inference. context turing-complete higher-order probabilistic programming languages probabilistic program simultaneously generative model procedure sampling same. probabilistic programming procedures program text describe generate sample value conditioned value arguments. probabilistic programming procedure constructivist description conditional distribution. deterministic procedures merely encode particularly simple degenerate conditional distributions. higher-order probabilistic programming languages open possibility inference generative model program text directly generative prior program text higher-order functionality eval. paper ﬁrst step towards ambitious goal inferring generative model program text directly example data. inference space program text hard start present account effort directly infer sampler program text that evaluated repeatedly produces samples similar summary statistics observational data. reasons make speciﬁc effort itself. potential automation development entries special collection efﬁcient sampling procedures humankind painstakingly developed many decades common distributions example marsaglia box-muller samplers normal distribution others). paper develop preliminary evidence suggests automated discovery might indeed possible. particular perform successful leave-one-out experiments able learn sampling procedure distribution i.e. bernoulli given program text others observed samples. imposing hierarchical generative model sampling procedure text ﬁtting out-of-sample human-written sampler program text inferring program text left-out random variate distribution type given sample values drawn same. second reason making effort compiling probabilistic programs. mean compilation probabilistic programs somewhat broad transformational compilation compiles probabilistic program sampler normal compilation probabilistic program machine code encodes parallel forward inference algorithm mean probabilistic program compilation automatic generation program text generate samples distributed ideally identically posterior distribution quantities interest original program conditioned observed data. concisely; given samples resulting posterior inference probabilistic program learn program text evaluated generate samples directly. reason expressing approaching compilation generality simpler approaches generalizing probabilistic programming posterior samples less-expressive model families suffer precisely compromise expressivity. distributions expressions valid posterior marginals higher-order probabilistic programming languages. compiled probabilistic programs must capable generating same. effort also ﬁrst step towards compiler. approach learning probabilistic programs relates program induction statistical generalization sampled observations. former usually treated search space program text objective deterministic function exactly matches outputs given parameters. latter generalizing data usually referred either density estimation learning. extensive review introduction given references. modern examples functional inductive programming include igor search utilized programs match constraints speciﬁed equations magichaskeller uses traditional search brute force enumeration programs obey constraints speciﬁed terms parameter/output pairs. similar search procedures used constraint satisfying hypotheses inductive logic programming probabilistic variants thereof alternative search techniques genetic programming also used constraint satisfying programs work suggested search easier space functional programming languages imperative insight supports interesting choice made liang searching space combinatory logic rather lambda calculus expressions. work framed similarly impose prior program text bayesian inference machinery infer distribution program text given observations. unlike learn stochastic programs sampled observation data rather deterministic programs input/output pairs. generalizing data main objectives ﬁelds machine learning statistics. important note substantial departure almost prior ﬁelds sense learned representation observed data generative sampling program text rather than parametric nonparametric model samples drawn using extrinsic algorithm. work model sampler represented program code. greedy search generative models structures kernel compositions related work sense search highly expressive generalization class unsupervised manner models explain observational data well. contrast full bayesian inference greedy search model family search ultimately expressive high order language stochastic primitives result capable representing computable probability distributions work relies heavily turing-complete higher-order probabilistic programming language system called anglican borrows modelling language syntax semantics venture generally inherits principles church differentiates anglican substantially others introduced uses particle markov chain monte carlo probabilistic programming inference. anglican means pmcmc metropolis-hastings algorithm inference. high level distance summary statistics computed observed data data generated interpreting latent sampler program text consider ﬁrst single given data generating distribution parameter vector samples consider task learning program text {xi}i repeatedly interpreted returns samples whose distribution close ˆxj}j samples generated repeatedly interpreting times.. summary function samples unnormalized distribution function returns high probability refer probabilistic programming write perform inference model i.e. generate samples marginal generalizations come same. particular system employ uses pmcmc inference. refer probabilistic program code figure ﬁrst line establishes correspondence variable program-text samples productions adaptor-grammar-like prior program text described section particular example implicitly speciﬁed since learning goal sampler standard normal distribution. also corresponds program variable samples here computed last four lines program implicitly deﬁned returning four dimensional vector consisting estimated mean variance skewness kurtosis samples drawn distance function also implicitly deﬁned multivariate normal mean diagonal covariance note means seeking sampler text whose output distributed mean variance skew kurtosis penalize deviations squared exponential loss function bandwidth named noise-level code computed different ways. ﬁrst occurs search efﬁcient code sampling known distributions. many cases standard normal case described summary statistics computed analytically. second happens sample corresponds situations when instance running computationally expensive mcmc sampler asked produce additional samples. frame compilation probabilistic programs. third ﬁxed dataset cardinality setting corresponds setting learning program text generative model arbitrary observed data. figure human-written sampling procedure program text left normal right poisson counts constants procedures expression expansions programs hierarchical generative prior sampler program text. figure illustrates another important generalization formulation learning standard normal sampler take account parameter values. interesting sampler program text endowed arguments allowing generate samples entire family parameterised distributions. consider well known box-muller algorithm shown figure parameterized mean standard deviation parameters. reason refer others like conditional distribution samplers. learning conditional distribution sampler program text requires recasting mcmc-abc target slightly include parameter distribution order proceed must begin make approximating assumptions. case need truly improper learned sampler program text work possible input arguments simply high prior probability subset values. assuming program text works settings input parameters fairly likely generalize well parameter settings approximately marginalize mcmc-abc target choosing small ﬁnite parameters yielding approximate marginalized mcmc-abc target probabilistic program learning conditional sampler program text bernoulli figure shows example kind approximation. samples times accumulating summary statistic penalties invocation. case individual summary distance computation involves computing g-test statistic approximately distributed i.e. construct case computing probability falsely rejecting null hypothesis bernoulli. falsely rejecting null hypothesis equivalent ﬂipping coin probability given p-value test turn heads. summary statistic penalties accumulated observe lines figure expressive power higher-order probabilistic programming language disposal prior conditional distribution sampler program text quite expressive. high level similar adaptor grammar prior used diverges details particularly creation local environments conditioning subexpression choices type signatures. pseudocode prior expressed follows exprtype|env random variable name type type. exprtype|env random constant type type. constants types integer real etc. sampled chinese restaurant process representation marginalized discrete dirichlet process prior pair dptype type base distribution htype mixture distribution. example type real mixture uniform common constants like exprtype|env procedure primitive stochastic procedure global environment output type signature type sampled randomly. exprarg type) compound proceduretype compound procedure sampled representation marginalized discrete dirichlet process prior pair dptype. base distribution gtype generates compound procedures return type type poisson distributed argument count random parameter types. body compound procedure generated using production rules given environment incorporates argument input variable names values. avoid numerical errors interpreting generated programs replace functions like safe-log returns uniform-continuous safe-uc swaps arguments returns types used experiments {real bool} general procedures global environment included safe-div safe-uc. production rule probabilities possible manually specify production rule probabilities grammar section took hierarchical bayesian approach instead learning human-written sampler source code. translated existing implementations common one-dimensional statistical distribution samplers anglican source. examples provided figure conveniently require stochastic procedure uniform-continuous also include single stochastic procedure grammar. figure histograms samples generated repeatedly evaluating probabilistic procedures sampled prior probabilistic sampling procedure text. prior constrained generate samplers univariate output clearly otherwise ﬂexible enough represent nontrivial spectrum distributions. counts sampling code corpus speciﬁcally excluding sampler attempting learn. production rule probability estimates smoothed dirichlet priors. note following experiments production rule priors updated ﬁxed inference. true hierarchical coupling joint inferences approaches straightforward probabilistic programming perspective result inference runs take longer compute. experiments perform illustrate three uses cases outlined automatically learning probabilistic programs. begin illustrating expressiveness prior sampler program text section report results experiments test approach three scenarios compute penalty ﬁrst experiments section tests ability learn probabilistic programs produce samples known one-dimensional probability distributions. experiments either probabilistically conditions p-values one-sample statistical hypothesis tests approximate moment matching. second experiments section addresses cases ﬁnite number samples unknown real-world source provided. ﬁnal experiment section preliminary study probabilistic program compilation possible gather continuing samples. illustrate ﬂexibility prior speciﬁcally production rules employ show samples generated probabilistic programs sampled prior section figure show histograms samples sampled probabilistic programs prior probabilistic programs. randomly generated samplers constructively deﬁne considerably different distributions. note particular variability domain variance even number modes. source code exists efﬁciently sampling many common one-dimensional distributions. conducted experiments test ability automatically discover sampling procedures found encouraging results. particular performed leave-one-out styles experiments infer sampler program text common one-dimensional distributions bernoulli poisson gamma beta normal normal. distribution performed mcmc-abc inference approximately marginalizing parameter space using small random parameters conditioning statistical hypothesis tests moment matching appropriate. figure representative histograms samples drawn repeatedly interpreting inferred sampler program text versus histograms exact samples drawn corresponding true distribution. left right bernoulli normal poisson. bottom same gamma normal beta. parameters used produce plots appear training data. case bernoulli inferred programs sample exactly true distribution ﬁnite-time inference converges good approximate sampler code illustrated beta example. figure inferred univariate sampler program text gamma observational data unknown distribution. program respectively generated green histograms bottom left figure right figure programs manually simpliﬁed display i.e. substitutions like performed. representative histograms samples best posterior program text sample discovered terms summary statistics match shown figure pleasing result discovery exact bernoulli distribution sampler program text shown figure figure shows inferred sampler text gamma. fully characterize divergence between learned sampling algorithms true distribution mechanisms exhaustive computation hypothesis testing remains open question. figure human-written exact bernoulli sampler. inferred sampler program text. ﬁrst also exact sampler bernoulli. last another sampler also assigned non-zero posterior probability exact. figure uncollapsed beta-binomial model probabilistic program. interested posterior distribution latent variable salient line three inferred probabilistic programs produce samples statistically similar distribution posterior distribution induced original probabilistic program. complete program ended line human-written code exactly sampling analytical posterior. inferred compiled posterior samplers indeed close exact sampler. also explored using approach learn generative models form sampler program text real world data unknown distribution. arbitrarily chose three continuous indicator features credit approval dataset inferred sampler program text using two-sample kolmogorov-smirnov distribution equality tests analogously g-test described before. histograms samples best inferred sampler program text versus training empirical distributions shown figure example inferred program shown figure data distribution representation despite expressed form sampler program text matches salient characteristics empirical distribution well. mcmc sampling particularly bayesian context usually quite costly further requires large amounts storage represent learned distribution samples. learning representation posterior terms sampling procedure directly samples approximately posterior distribution interest could potentially improve both particularly purposes repeated posterior predictive inference. probabilistic programming sample-based posterior representations option problem particularly acute. further higher-order probabilistic programming languages require expressivity class approximating distribution least another probabilistic program. ultimate compilation probabilistic program inference learned program sampling directly posterior interest remains quite preliminary experiments encouraging. explore possibility took uncollapsed beta-binomial model prior distribution beta used metropolis hastiings infer sampled-based representation posterior distribution given four successful trials bernoulli. correspondent probabilistic program given figure used approach learn probabilistic program repeatedly invoked produces samples statistically match empirical posterior distribution. examples inferred probabilistic procedures given figure analytical posterior distribution case beta found good approximations. note probabilistic program compilation experiment additional primitives include beta normal higher order stochastic procedures added program text generative model. novel approach program synthesis probabilistic programming raises least many questions answers. high level question kind work sharpens really goal program synthesis? framing program synthesis probabilistic inference problem implicitly naming goal estimating distribution programs obey constraints rather search single best program same. hand notion regularising generative model natural predisposes inference towards discovery programs preferentially possess characteristics interest hand exhaustive computational inversion generative model includes evaluation program text clearly remain intractable foreseeable future. reason greedy stochastic search inference strategies basically options available. employ latter mcmc particular explore posterior distribution programs whose outputs match constraints knowing full-well actual effect problem domain particular ﬁnite time more-or-less stochastic search. could annealing temperature schedule clarify mcmc search however ergodic system sufﬁciently stiff require quenching pleasantly surprising however monte carlo techniques able exemplar programs posterior distribution actually good generalising observed data experiments report. remains open question whether sampling procedures best stochastic search technique problem general however. perhaps directly framing problem search might better particularly goal single best program. techniques ranging genetic algorithms monte carlo tree search show promise bear consideration. interesting take work forward introduce techniques cumulative/incremental learning community perhaps adding time-dependent hierarchical dimensions program text generative model. speciﬁc context learning sampler program text would convenient instance learning program text sampling parameterised normal distribution access already learned subroutine sampling standard normal. related work ﬁeld inductive programming large gains performance observed learning task structured example inference tasks start. inspired continues inspire internal experience ability reason procedure. given examples humans clearly able generate program text procedures compute otherwise match examples. humans physically simulate turing machines would seem clear capable something least powerful deducing action particular piece program text text itself. candidate artiﬁcial intelligence solution complete without inclusion ability. without always deﬁcient sense apparent humans internally represent reason procedure. perhaps generalised representation procedure actual expressivity class human reasoning. certainly can’t less. authors thank many people help wholesome discussions suggestions comments including brooks paige jan-willem meent david tolpin tejas kulkarni. work supported xerox faculty research award somerville college scholarship. opinions ﬁndings conclusions recommendations expressed work authors necessarily reﬂect views sponsors. material based research sponsored darpa u.s. force research laboratory cooperative agreement number fa---. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright notation heron. views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied darpa u.s. force research laboratory u.s. government. brooks paige frank wood. compilation target probabilistic programming languages. sumit gulwani emanuel kitzelmann schmid. approaches applications inductive programming issn http//dx.doi.org/. /dagrep.... http//drops.dagstuhl.de/opus/volltexte//. emanuel kitzelmann. analytical inductive functional programming. pages springer susumu katayama. magichaskeller system demonstration. page stephen muggleton feng. efﬁcient induction logic programs. stephen muggleton raedt. inductive logic programming theory methods. raedt kristian kersting. probabilistic inductive logic programming. springer forrest briggs melissa oneill. functional genetic programming combinators. pages chris maddison daniel tarlow. structured generative models natural source code. ross quinlan. simplifying decision trees. bache lichman. machine learning repository http//archive.ics. cameron browne edward powley daniel whitehouse simon lucas peter cowling philipp rohlfshagen stephen tavener diego perez spyridon samothrakis simon colton. survey monte carlo tree search methods.", "year": 2014}