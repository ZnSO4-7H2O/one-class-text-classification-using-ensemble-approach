{"title": "Statistical Learning for OCR Text Correction", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "The accuracy of Optical Character Recognition (OCR) is crucial to the success of subsequent applications used in text analyzing pipeline. Recent models of OCR post-processing significantly improve the quality of OCR-generated text, but are still prone to suggest correction candidates from limited observations while insufficiently accounting for the characteristics of OCR errors. In this paper, we show how to enlarge candidate suggestion space by using external corpus and integrating OCR-specific features in a regression approach to correct OCR-generated errors. The evaluation results show that our model can correct 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of the OCR-errors (considering the top 3 suggestions), for cases where the theoretical correction upper-bound is 78%.", "text": "accuracy optical character recognition crucial success subsequent applications used text analyzing pipeline. recent models post-processing signiﬁcantly improve quality ocr-generated text still prone suggest correction candidates limited observations insufﬁciently accounting characteristics errors. paper show enlarge candidate suggestion space using external corpus integrating ocr-speciﬁc features regression approach correct ocr-generated errors. evaluation results show model correct ocr-errors ocr-errors cases theoretical correction upper-bound increasing amount data produced transformed digital form days including magazines books scientiﬁc articles. using graphic formats like portable document format joint picture group comprehensive solution efﬁcient digitization well better preserving page laygraphical information since information formats machine-readable analyzing data relies heavily accuracy optical character recognition however systems imperfect prone errors. post-processing important step improving quality output crucial success text analyzing system pipeline. post-processing model attempts detect misspellings noisy output correct errors intended representations. many machine learning approaches correct ocrgenerated errors selecting appropriate correction among candidates. ocr-generated errors diverse handwriting errors many aspects machine learning approaches incorporate different features enabling robust candidate selection instead inferring limited observations example using probabilistic-based model machine learning approach exhibits advantages correcting ocr-generated texts problems emerge existing models first models limit candidate suggestions recognition output engines. errors unrecognized engines thus unable corrected. issue problematic especially original input suffers degradation example historical documents secondly another class models uses frequencies candidate related n-grams corpus features training. although n-gram statistics shown effective correcting real-word spelling errors training n-gram features capture diverse nawork propose postprocessing error correction model leverages different features learning process. model applies different features avoid bias improve correction accuracy. address limitation candidate suggestion enhance scope candidates error considering words available vocabulary within limited damerau-levenshtein distance features narrow candidates number. proposed model ranks candidates regression model shows errors corrected ground truth dataset. uncorrected errors model could provide correction three suggestions. propose post-processing model integrates ocr-speciﬁc features regression approach. evaluation result shows proposed model capable providing high quality candidates suggested list. make available ground truth ocr-error dataset generated book biodiversity heritage library. dataset lists mappings ocr-generated errors intended representations used directly benchmark testing. literature post-processing research exhibits rich family models correcting ocr-generated errors. post-processing model integrated system detects corrects misspellings non-word realword ocr-generated text. studies view post-processing initial step correction pipeline involve continuous human intervention afterwards models designed reduce human effort correcting errors manually. taghva integrate dictionaries heuristics correct many errors possible given human correctors. future work taghva stofsky record previous human corrections update underlying bayesian model automatic correction. extreme case m¨uhlberger build full-text search tool retrieve occurrences original images given text query fully relies user validate correct errors. direction work ensembles outputs multiple engines input selects best word recognition ﬁnal output klein show combining complementary result different models leads better output. lund demonstrate overerror rate decreases addition different models regardless performance added model. lund machine learning techniques select best word recognitions among different outputs. lund apply recognition votes lexical features train conditional random field model evaluate test different domain. models proved useful select words among model recognitions blind candidates. besides require presence original input effort multiple processing. another class post-processing models abstracts engines leverages statistics external resources kissos dershowitz three n-gram statistical features extracted three million documents train linear regressor candidate ranking. bassil alwani make frequencies google n-gram corpus candidate suggestion ranking. candidates suggested models restricted exist recognitions. however existing methods make solely n-gram frequencies withknowing characteristics errors thus bias select common words word error rate engines practice range signiﬁcantly higher .-.% handwriting .-.% edited newswire ocr-generated errors tend distinct characteristics require different techniques spell correction complex non-standard edits humangenerated misspellings character-level edits categorized following four standard types insertion deletion substitution transposition. majority spell correction errors roughly single edit intended word tend within length difference however signiﬁcant fraction ocr-generated errors one-to-one character-level edit multi-factor error generation errors generated different processing steps various factors. taghva stofsky trace errors associated primary steps involved conversion process scanning error caused paper/print quality original document pool condition scanning equipment. zoning error caused incorrect decolumnization complex page layout. segmentation error caused broken characters overlapping characters nonstandard fonts document. classiﬁcation error caused incorrect mapping segmented pixels single character. multi-source dependent characteristics ocr-generated errors vary according human reasons also non-human causes especially sensitive engines. different engines different techniques features recognition leads different confusion probability distribution section describe detail processing steps proposed model. external resources correction process including lexicons word n-gram corpus. lexicon list unique words word n-gram corpus refers list n-grams observed frequency counts. examples word n-gram corpus google book n-gram google -gram corpus annotate denote english strings text font vectors bold lowercase sets cursive uppercase scalar lower-case english greek characters functions followed bracket score). size collection represented error detection step identiﬁes errors tokenized text ﬁrst step correction procedure. since correct word proceed correction steps want weak detection restriction ﬁlter highly conﬁdent words. rely n-gram frequency determine correctness word. word detected error following conditions fulﬁll. consider common word less likely error word -gram frequency word greater frequency threshold. frequency threshold varies different word length. word likely correct word context occurs places. sliding window construct n-gram contexts word. frequency context n-gram corpus greater frequency threshold. select candidate error contains words vocabulary within limited number character modiﬁcations. speciﬁc symbol language lexicon. candidate detected error dist minimum edit distance distance threshold. damerau-levenshtein distance used spell correction models locating candidates considers four character-level editing types since transposition errors common human-generated text rarely occur ocr-generated text apply levenshtein distance uses simpler operation without transposition. levenshtein edit distance minimum edit distance fundamental technique quantifying difference strings spell correction. given string alphabet edit distance dist minimum number edit operations required transform edit operation character-level modiﬁcation levenshtein edit distance reason previously described section score function follows string similarity longest common subsequence alternative approach edit distance matching similar strings. variations normalized longest common subsequence take account length shorter longer string normalization. normalized maximal consecutive longest common subsequence limits common subsequence consecutive. three types modiﬁcations different additional conditions nlcs nlcsn subsequences starting ﬁrst n-th character respectively; nlcsz takes subsequences ending last character. apply normalization nlcs. language popularity using language lexicon common approach detect non-word tokens non-existing tokens detected true errors. candidate string error candidates unigram frequency. candidate conﬁdence unigram popularity given lexicon existance besides english lexicon different lexicons detect existence token different subjects. identiﬁes additional lexical features. example domain speciﬁc lexicon capture terminologies especially useful input text domain. candidate selection english lexicon candidate score boolean value indicates detection result. exact-context popularity appropriate correction candidate coherent context. using word n-gram context analysis broadly researched approach correcting realword errors given error word text n-gram contexts constructed using sliding window score candidate error ﬁrst substitute error word n-gram contexts candidate create contexts candidates suggested reqn n-gram frequency gives non-existing n-gram. score function given relaxed-context popularity context longer n-gram size deﬁnes speciﬁc case given word existence corpus shows higher conﬁdence candidate. general n-gram corpus limited coverage possible n-grams language especially emerging words language. candidates rare word barely suggested contexts limited coverage ngram corpus. deal issue relaxing context matching condition allow mismatching context word. example figure consider ﬁrst -gram context given which candidate. need frequency brightly coloured birds which computing exact context popularity. relaxed context popularity need frequencies four types -grams coloured birds which brightly birds which brightly coloured which brightly coloured birds which matches valid unigram. scoring function exact context matching except candidate context larger relaxed case. formulate conﬁdence prediction task regression problem. given candidate feature scores predict conﬁdence candidate correction error word. conﬁdence used ranking among candidates error. train regressor correction label candidate features candidate intended correction otherwise. training data contains candidates different errors candidates labeled deal unbalanced nature candidates weight samples computing training loss count number samples label respectively. then ratio weight samples labeled samples labeled experimentally apply adaboost.r model decision trees linear loss function. made available dataset ocrgenerated errors along ground truth text benchmark testing. text generated book titled birds great britain ireland made publicly available biodiversity heritage library europe. ground truth text source image data book contains page-separated ﬁles main content included pages. book combines different font types layouts main text leads erroneous results. mismatching words ground truth text digital ocr-text used ground truth errors. ground truth text contains non-punctuation words. thus error rate evaluation dataset errors complex regarding edit distance shown table. challenges include terminologies multilingual meanless words handled using standard techniques. walker amsler claim lexicon published dictionary limited coverage newswire vocabulary vice versa. thus construct language lexicon unigrams google n-gram corpus. corpus contains frequencies unigrams ﬁve-grams generated approximately trillion word tokens extracted publicly accessible pages. unigram corpus ﬁltered frequency less ﬁve-grams google corpus exact relaxed context matching. lexicon existence feature three lexicons build features instances wikipedia entities extracted article names wikipedia. feature gives credit common terminologies. biodiversity terminologies collected biodiversity digital library capture domain speciﬁc terms contained wikipedia. proposed model receives ocr-generated plain text input. apply penn treebank tokenization additional rules google tokenize input text. tokenization method consistent google n-gram corpus. frequency existence rarely hyphenated words poorly estimated using external resources. thus split hyphenated word internal hyphen. experimentally ﬁlter tokens following types tokenization punctuations; numeric tokens contains numeric characters common english words. apply lexicon frequent english words ﬁltering. accuracy system increase relaxed ﬁltering conditions english words example ﬁltering english stop error partially detected overlapped non-identical character sequence treated error. call partially detected case success unbounded detection correct recognition character sequence success bounded detection. unbounded detection potentially corrected inaccurate features scores inﬂuence correction accuracy. error <spend→sp end> unbounded detected exists context candidate edit distance computed instead end. addition exist multiple unbounded errors detected ground truth error splitting mistakes. every ground truth error count successful unbounded detection. model achieves bounded detection recall total detection recall shown table take following steps build training dataset first construct candidate error containing candidates scored feature. then select subset errors whose intended word exists candidate set. finally randomly select errors candidates sets training. train multiple adaboost regressors different settings apply -fold cross-validation select best setting evaluating rest errors. report correction results regarding different error categories table represents precision candidate suggestions words even ﬁltering computation time increases trade-off. similarly reducing candidate detection time maximum levenshtein distance candidate search report confusion matrix error detection table proposed model achieves detection recall. considerable number ture-positive errors correct words detected errors. using type errors training testing word intended word error. correction results regarding types errors reported section tokenizing noisy text tokenization approach inevitably involved common word boundary problem correct boundary errors properly identiﬁed human-generated ocr-generated text problem caused splitting merging mistakes. especially problematic ocr-generated text words containing characters recognized punctuation thus splitted tokenization heuristics. error detection correction techniques deﬁne token character sequence separated white space characters thus intended words cannot found distlev results shown table model could locate correction candidates table percentage errors correction exists among candidates suggested support vector regressor rigid linear regressor multiple layer perceptron rectiﬁed linear unit random forest adaboost.r collaboration applied features. observe performance varies drastically bounded unbounded errors presumably feature score unbounded errors inaccurate better intuition contribution individual feature plot distinctiveness located error corrections feature figure bounded detected errors contextbased features able locate distinctive corrections rarely found features. addition relax context popularity feature shows better coverage exact context popularity. hand four features important false-positive errors context-based features provide little help. given upperbound correction rate within three edit distance regressors achieve good results. seen ensemble methods like random forest adaboost robust others suggesting appropriate candidates. figure distinctiveness features locating error corrections. feature represents number error corrections located feature. color indicates number features locates errors. i.e. white indicates portion error corrections located features black indicates error corrections located feature. able select rank candidates similar shape error suitable domain coherent context. evaluation results show model correct errors could provide correction three suggestions uncorrected errors. suggesting three candidates error model correct error cases theoretical correction upperbound", "year": 2016}