{"title": "Online Learning to Sample", "tag": ["cs.LG", "cs.CV", "cs.NA", "math.OC", "stat.ML"], "abstract": "Stochastic Gradient Descent (SGD) is one of the most widely used techniques for online optimization in machine learning. In this work, we accelerate SGD by adaptively learning how to sample the most useful training examples at each time step. First, we show that SGD can be used to learn the best possible sampling distribution of an importance sampling estimator. Second, we show that the sampling distribution of an SGD algorithm can be estimated online by incrementally minimizing the variance of the gradient. The resulting algorithm - called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to optimize, as well as a set of parameters to sample learning examples. We show that AWSGD yields faster convergence in three different applications: (i) image classification with deep features, where the sampling of images depends on their labels, (ii) matrix factorization, where rows and columns are not sampled uniformly, and (iii) reinforcement learning, where the optimized and exploration policies are estimated at the same time, where our approach corresponds to an off-policy gradient algorithm.", "text": "quence approximate random evaluations. stochastic gradient descent special type stochastic approximation method widely used large scale learning tasks thanks good generalization properties known ﬁxed distribution function maps i.e. family functions metric space parameterized stochastic approximation method consists approximate gradient steps equal average true gradient many applications including supervised learning techniques function loglikelihood empirical distribution density {x··· i.i.d. data convergence properties directly linked variance gradient estimate consequently improvements basic algorithm focus parameter averaging reduce variance ﬁnal estimator sampling mini-batches multiple points sampled time reduce variance gradient adaptive step sizes per-dimension learning rates e.g. adagrad stochastic gradient descent widely used techniques online optimization machine learning. work accelerate adaptively learning sample useful training examples time step. first show used learn best possible sampling distribution importance sampling estimator. second show sampling distribution algorithm estimated online incrementally minimizing variance gradient. resulting algorithm called adaptive weighted maintains parameters optimize well parameters sample learning examples. show awsgd yields faster convergence three different applications image classiﬁcation deep features sampling images depends labels matrix factorization rows columns sampled uniformly reinforcement learning optimized exploration policies estimated time approach corresponds off-policy gradient algorithm. many real-world problems face intractable integrals averaging combinatorial spaces nongaussian integrals. stochastic approximation class methods introduced herbert robbins sutton monro solve intractable equations using setion control variate. full derivation given appendix implemented experimental section. neural community adapting order training samples used called curriculum learning approach seen framework allthough algorithm general speadup learning arbitrary integrals sums losses training data. another obtain good convergence properties properly scale rotate gradient ideally direction inverse hessian type second-order method slow practice. however estimate hessian greedily done quasi-newton methods limited memory bfgs adapt algorithm similarly ﬁrst show section powerful tool optimize sampling distribution monte carlo estimators. motivate adaptive weighted algorithm sampling distribution kept constant learned optimization process. consider family {qτ} sampling distributions absolutely continuous respect parametric denote density importance sampling common method estimate integral corresponds monte carlo estimator form best possible sampling distribution sampling family {qτ} minimize variance respect belongs family {qτ} exists parameter -almost surely. case variance estimator null estimate integral single sample. general however parametric family reduce gradient variance learning sample training points. rather learning ﬁxed optimal sampling distribution optimizing gradient propose dynamically learn optimal sampling distribution time original algorithm. formulation uses stochastic process focuses minimization gradient variance amounts additional step along step constant extra cost iteration iteration simulations expensive data access slow extra computational cost compensated increase convergence speed quantiﬁed experiments. paper organized follows. reviewing related work section show used optimal sampling distribution importance sampling estimator variance reduction technique used iterations algorithm learning reduce variance gradient illustrate algorithm called adaptive weighted three well known machine learning problems image classiﬁcation matrix factorization reinforcement learning finally conclude discussion idea speeding learning modifying importance sampling distribution recently analyzed showed particular choice sampling distribution could lead sub-linear performance guarantees support vector machines. approach generalization idea models including learning sampling distribution part optimization. work shows using simple model choose data resample useful thing learn sampling model optimizing. approaches mentioned viewed extreme case adaptive sampling step learn sampling distribution second step learn model using sampling distribution. training language models shown faster adaptive importance sampling authors directly minimize variance estimator. regarding variance reduction techniques addition aforementioned ones batching adaptive learning rates like adagrad additional technique control variates recently used estimate non-conjugate potentials variational stochastic gradient algorithm. techniques described paper also straightforwardly extended optimizaquantity closed form solution algorithm gradient step equal average quantity. obtain estimator gradient expectation given equation enough sample point according /q∇τ repeated convergence. full iterative procedure summarized algorithm experiments below show learning importance weight importance sampling estimator using lead signiﬁcant speed-up several machine learning applications including estimation empirical loss functions evaluation policy reinforcement learning scenario. following show idea also used sequential setting multivariate outputs control variance gradient standard algorithm ultimately speedup convergence. section ﬁrst analyze weighted version algorithm points sampled non-uniformly similarly importance sampling derive adaptive version algorithm sampling distribution evolves iterations. introduced previously goal minimize expectation parametric function similarly importance sampling need sample according base distribution iteration sgd. instead distribution deﬁned gradient step properly re-weighted density dq/dp iteration algorithm consists steps sample according distribution approximate gradient step depending importance distribution algorithm different convergence properties original algorithm. mentioned previously best sampling distribution would gives small variance weighted gradient main issue depends parameters different iteration. main observation minimize variance gradient using previous iterates assumption variance change quickly updated. argue reasonable practice learning rate policies usually assume small constant learning rate decreasing schedule next section build observation build algorithm learns best sampling distribution online fashion. similarly section consider family {qτ} sampling distributions parameterized parametric using sampling distribution p.d.f. evaluate efﬁciency sampling distributions based variance figure generalization performance training error function training time averaged three independent runs. converged epochs whereas aw-sgd converged performance times less epochs improvement training time. practice noted enough single step inner loop i.e. call simpliﬁed algorithm adapted-weighted algorithm pseudocode given algorithm aw-sgd slight modiﬁcation standard variant adagrad adadelta rmsprop sampling distribution evolves algorithm thanks update algorithm useful gradient variance signiﬁcantly reduced choosing better samples. important design choice algorithm choice decay step sizes sequences {ρt}t> {ηt}t>. using adaptive step sizes appears useful settings appears regime aw-sgd outperforms signiﬁcantly larger meaning algorithm converges quickly smallest variance awsgd tracks course iterations. ideally sequence sampling parameters {τt} remains close optimal trajectory consist best possible sequence sampling parameters given equation large scale image classiﬁcation important machine learning tasks images containing given category much less frequent images containing category. practice learn efﬁcient classiﬁers need optimize class-imbalance hyper-parameter furthermore suggested standard practice hard negative mining positives negatives different importance optimization positives figure displays evolution positive sampling bias parameter along aw-sgd iterations almost classes expose expected behavior sampling negatives optimization progresses negatives correspond anything object interest therefore much varied difﬁcult model. person class exception category largest number positives intra-class variation. note that although dynamics stochastic process similar exact values obtained vary signiﬁcantly depending class shows self-tuning capacity aw-sgd. algorithm. matrix generated rank-k matrix m×k. consider differentiable loss function observed value. squared loss entry real scalar full loss function important ﬁrst negatives gradually gaining importance. however cross-validating best imbalance hyper-parameter iteration prohibitively expensive. instead show aw-sgd used biased sampling depending label bias adapted along learning. measure acceleration convergence experiment widely used pascal image classiﬁcation benchmark following standard practice learn one-versus-rest logistic regression classiﬁer using deep image features last layers pretrained alexnet convolutional network note image classiﬁcation pipeline provides strong baseline comparable state training images labels discrete distribution samples parametrized log-odd probability sampling positive image family sampling distributions {qτ} written initialize positive sampling bias parameter value yields good performance aw-sgd. baseline aw-sgd algorithm adagrad choose learning rates initialized figure shows aw-sgd converges faster training error generalization performance. acceleration time iterations aw-sgd costs iteration respect implementation. experiments noticed positive sampling bias parameter indeed gradually decreases i.e. /algorithm learns focus harder negative class. also show figure results minimal variance importance sampling algorithm curve shows standard deviation estimator loss function number matrix entries observed. matrix factorization experiments used minibatch technique batches size tuned yield minimum convergence separately algorithm. results averaged runs. initialized zeros initial uniform sampling distribution rows columns. model learning rate decrease kept constant. figure simulated rank-k matrix sampling using independent centered gaussian variables unit variance. illustrate beneﬁt adaptive sampling multiply randomly drawn square block size experimentally observe beneﬁt uniform sampling strategy. results minimal variance important sampling scheme shown left. seen number matrix entries standard deviation importance sampling estimator divided meaning would need half samples evaluate full loss compared uniform sampling figure shows loss decrease aw-sgd matrix multiple learning rates. x−axis expressed epochs epoch corresponds sampling values matrix. aw-sgd converges signiﬁcantly faster best uniformly sampled even epoch data. average aw-sgd requires half number iterations converge value. in-painting experiment compared algorithms mnist dataset low-rank decomposition techniques successfully applied factorized training zero digit matrix line image handwritten zero column pixel. figure shows loss decrease algorithms ﬁrst iteration. aw-sgd requires signiﬁcantly less samples reach error. convergence aw-sgd showed average speedup execution time compared showing sampling choices compensate become method choice large number contexts reinforcement learning here optimizing integral related policy gradient algorithms minimizing expected loss maximizing reward episodic setting off-policy estimation. equivalently consider sampling space trajectory markov decision process aw-sgd viewed off-policy gradient algorithm parameterization i.e. objective maximize expected reward target policy minimize variance gradient policy gradient exploration policy considered canonical grid-world problem squared grid size considered. classical reward setting applied reward function discounted instantaneous reward assigned cell grid reward terminal state located right grid. context episode considered successful deﬁned terminal state terreached. finally random distribution ntrap minal states negative reward also positioned. start state located up-left cell grid. space precisely probability action position follows multinomial distribution parameters {pxy indeed context grid type environment section parameters basically correspond log-odds probability moving four directions position grid distribution sampled trajectories different distribution trajectory derived policy optimized using algorithm baseline corresponds policy iteration based trajectories sampled using current policy estimate table table gives average means variances obtained batch learning trials using algorithms properly tuned learning rate tested grid sizes signiﬁcant improvement expected success adaptive weighted used instead on-policy learning algorithm. non-stationary data figure progressively substituted images handwritten zeros images handwritten ones. shows every samples heatmap sampling probability zeros ones made samples distinctly recognize zero digit ﬁrst progressively fades digit. transitions shows aw-sgd learns sample digits likely high impact loss. algorithm adapts online changes underlying distribution combined adaptive step size algorithms adagrad noticed adagrad improved convergence speed aw-sgd matrix factorization experiments. possible explanation adaptive sampling favors rows columns adagrad compensates non-uniform sampling using awsgd adagrad simultaneously converges slightly faster adagrad alone. behave similarly parameterizations indices linked parameters indices. however many experiments adagrad performances matching best crossvalidated learning rates. time speedup achieved time-aware aw-sgd plotted evolution factor figure algorithm summed real execution time simulated access times order take account time-aware aw-sgd sampling overhead. speedup computed epoch dividing total time aw-sgd total time. positive speedups starts slow access time factor roughly corresponds random read ssd. aw-sgd slower since data homogeneous time access difference enough compensate overhead. corresponding read another computer’s memory local network speedup reaches hard drive seek aw-sgd faster. shows time-aware aw-sgd overhead compensated sampling choices. figure shows loss decrease algorithms ﬁrst epochs shows access time uniform aw-sgd would convergence speed standard hence even case theoretical beneﬁt using time-aware awsgd terms epochs fact learn underlying access time bias sampling could potentially lead huge improvements convergence time. figure illustration evolution sampling distribution data i.i.d. heatmap contains sampling probability pixel mnist matrix factorization experiment. many large scale infrastructures computation servers shared many people data access gradient computation unknown advance. example large scale image classiﬁcation images might stored leading access time order times faster images stored hard drive. hardware systems sometimes called nonuniform memory access also case matrix factorization embeddings stored locally others downloaded network. inform algorithm sample often data stored locally? simple modiﬁcation aw-sgd enable algorithm adapt non-uniform computation time. idea learn dynamically minimize expected loss decrease unit time. make aw-sgd take account access time simply weighted update algorithm dividing simulated access time sample summarized algorithm experiment show matrix factorization case time-aware aw-sgd able learn exploit underlying hardware data entirely memory part extra access cost. generate rank- matrix without high variance block variance uniform across rows columns. ﬁrst half rows consider data bematrix i.e. main memory simulate access cost sampling rows inspired jeff dean’s explanations. half rows figure evolution training error function number epochs simulated matrix different access costs uniformly-sampled aw-sgd using best algorithm. figure evolution training error function number epochs simulated matrix different access costs uniformly-sampled aw-sgd using best algorithm.", "year": 2015}