{"title": "Informed Non-convex Robust Principal Component Analysis with Features", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We revisit the problem of robust principal component analysis with features acting as prior side information. To this aim, a novel, elegant, non-convex optimization approach is proposed to decompose a given observation matrix into a low-rank core and the corresponding sparse residual. Rigorous theoretical analysis of the proposed algorithm results in exact recovery guarantees with low computational complexity. Aptly designed synthetic experiments demonstrate that our method is the first to wholly harness the power of non-convexity over convexity in terms of both recoverability and speed. That is, the proposed non-convex approach is more accurate and faster compared to the best available algorithms for the problem under study. Two real-world applications, namely image classification and face denoising further exemplify the practical superiority of the proposed method.", "text": "revisit problem robust principal component analysis features acting prior side information. novel elegant non-convex optimization approach proposed decompose given observation matrix low-rank core corresponding sparse residual. rigorous theoretical analysis proposed algorithm results exact recovery guarantees computational complexity. aptly designed synthetic experiments demonstrate method ﬁrst wholly harness power non-convexity convexity terms recoverability speed. proposed non-convex approach accurate faster compared best available algorithms problem study. real-world applications namely image classiﬁcation face denoising exemplify practical superiority proposed method. many machine learning artiﬁcial intelligence tasks involve separation data matrix low-rank structure sparse part capturing diﬀerent information. robust principal component analysis candes chandrasekaran popular framework logically characterizes matrix separation problem. thus reasonable investigate propitious rpca exploit available features. indeed recent results indicate features redundant all. setting multiple subspaces rpca degrades number subspaces grows increased row-coherence. hand feature dictionaries allows accurate low-rank recovery removing dependency row-coherence. despite theoretical practical merits convexiﬁed rpca features pcpf chiang convex relaxations rank function l-norm necessarily lead algorithm weakening chandrasekarana jordan separate note recent advances non-convex optimization algorithms continue undermine convex counterparts gong kohler lucchi particular non-convex rpca algorithms fast rpca altproj netrapalli exhibit better properties convex formulation. recently niranjan embedded features non-convex rpca framework known irpca-iht faster speed. however remains unclear whether features eﬀectively incorporated non-convex rpca beneﬁts accuracy speed exploited much possible. work give positive answers questions proposing novel non-convex scheme fully leverages features reveal true column subspaces decompose observation matrix core matrix given rank residual part informed sparsity. even though proposed algorithm inspired recently proposed fast rpca contributions means trivial especially theoretical perspective. first fast rpca cannot easily extended consistently take account features. second show paper incoherence assumptions observation matrix features play decisive role determining corruption bound computational complexity non-convex algorithm. third fast rpca limited corruption rate choice hard threshold whereas algorithm rate fourth prove costly projection onto factorized spaces entirely optional features satisfy certain incoherence conditions. although algorithm maintains corruption rate fast rpca show empirically massive gains accuracy speed still obtained. besides transfer coherence dependency observation features means algorithm capable dealing highly incoherent data. unavoidably features adversely aﬀect tolerance corruption irpcaiht always true algorithm relation fast rpca. underlying rank features weakly informative i.e. often higher complexity log) algorithm. although feature-free convex non-convex algorithms higher asymptotic error bounds algorithm show experiments translate accuracy reality. algorithm still best performance recovering accurately low-rank part highly corrupted matrices. attributed fact bounds tight. besides pcpf altproj much higher complexity ours. pcpf exist theoretical analysis deterministic sparsity model. nonetheless show experiments algorithm superior regard recoverability running time. overall contribution paper follows otherwise stated. represent column projection onto support given element-wise absolute value matrix norms matrix frobenius norm; nuclear norm; largest singular value; otherwise lp-norm vectorized maximum matrix l-norms. moreover represents real matrices additionally largest singular value matrix. euclidean metric applicable non-uniqueness bi-factorisation a∗b∗t corresponds manifold rather point. hence deﬁne following distance optimal pair a∗b∗t rpca concerns known observation matrix seeking decompose matrices low-rank sparse arbitrary magnitude. conceptually equivalent solving following optimization problem search consists alternating non-convex projections. cycle hard-thresholding takes place ﬁrst remove large entries projection appropriate residuals onto low-rank matrices increasing ranks carried next. exact recovery also established. fast rpca follows another non-convex approach solve rpca. initialization stage fast rpca updates bilinear factors series projected gradient descent sparse estimations minimize following loss non-convex projection algorithm. similar altproj step sparse estimate calculated hard thresholding monotonically decreasing threshold. that spectral hard thresholding takes place attain lowrank estimate. irpca-iht provably converges solution rpca. problem setup suppose known data matrix rn×n decomposed low-rank component sparse error matrix compatible dimensions. identify underlying matrices hence robustly recover low-rank component help available side information form feature matrices concretely u∗σ∗v∗t singular value decomposition follows random sparsity model. u∗σ∗ support chosen uniformly random collection support sets size. furthermore informed proportion non-zero entries column denoted assume non-convex approach achieve objective. algorithm consists initialization phase followed gradient descent phase. stage keep track factors xpqt theorem follows algorithm converges linear rate assumptions converge initial error iterations needed. iteration costly step matrix multiplication takes time. overall algorithm total running time however guarantee corruption. thus select matrices whose maximum corruption exceed still feed algorithms order demonstrate algorithm need exact value corruption ratio. consider types signs error bernoulli sgn. resulting thus becomes simulated observation. addition uσvt feature formed randomly interweaving column vectors arbitrary orthonormal bases null space permuting expanded columns random orthonormal bases kernel forms feature hence feasibility conditions fulﬁlled ⊇col ⊇col. pair three observations constructed. figures plot results algorithms incorporating features. besides algorithm contrasts fast rpca figure feature-free algorithms investigated figure figures illustrate random sign model figures coherent sign model. previous nonconvex attempts fail outperform convex equivalents. irpca-iht unable deal even moderate levels corruption. frontier recoverability advanced algorithm pcpf phenomenal massively ameliorating fast rpca. anomalous asymmetry sign models longer observed non-convex algorithms. observation matrices generated rank chosen column number random sign error corrupts entries features dimension column number. running times algorithms except irpca-iht plotted irpca-iht able achieve relative error less larger matrices. fair comparison relaxed rank column number error rate compare algorithm irpca-iht matrices dimension column number speed process. result shown figure times averaged three trials. evident that large matrices algorithm overtakes existing algorithms terms speed. note features pcpf even slow recovery process. mnist dataset example contains hand-written digits divided training testing sets. observation matrix composed vectorized random images test stacked column-wise. case left feature obtained training also applicable test eigendigit nature. imparts algorithm supervised learning clean related training samples available. right feature posses property identity matrix. range sparse noise test separately noise sets pixel pcpf take chiang irpca-iht algorithm instead. relative error recovered matrix competing algorithms clean test matrix plotted figure algorithm accurate removing added artiﬁcial noise. evaluate classiﬁers perform recovered matrices train linear kernel using training test corresponding models recovered images. table tabulates linear svm. table tabulates kernel svm. classiﬁers conﬁrm recovery result obtained various models corroborating algorithm’s pre-eminent accuracy. common practice decompose facial images low-rank component faithful face representation sparse component defects. because face convex lambertian surface distant isotropic lighting underlying model spans linear subspace basri jacobs theoretical lighting conditions cannot realised unavoidable occlusion albedo variations real images. demonstrate substantial boost performance facial denoising leveraging dictionaries learnt images themselves. number atoms γi’s count number non-zero elements sparsity code sparsity constraint factor. solved k-svd algorithm. here feature dictionary feature corresponds similar solution using transpose observation matrix input. used iterations. visual illustration recovered images algorithms exhibited figure challenging scenario algorithm totally removed shadows. pcpf smoother still suﬀers shade. altproj fast rpca introduced extra artefacts. although irpca-iht managed remove shadows brought back severely distorted image. quantitatively verify improvement made proposed method examine structural information contained within denoised eigenfaces. singular values recovered low-rank matrices algorithms plotted figure non-convex algorithms competent incorporating rank information keep singular values vastly outperforming convex approaches. among them algorithm rapid decay found naturally wright work proposes non-convex algorithm solve rpca help features error sparsity roughly known. exact recovery guarantee established three diﬀerent assumptions incoherence conditions features data observation matrix. simulation experiments suggest algorithm able recover matrices higher ranks corrupted errors higher sparsity previous state-of-the-art approaches. large synthetic matrices also show algorithm scales best observation matrix dimension. mnist yale datasets justify algorithm leads approaches fair margin. future work involve ﬁnding accurate initialization scheme. corrupted random sign errors. algorithm applied projection several times. uses diﬀerent number iterative steps ranging recoverability plotted number iterative projections figure hardly noticeable improvement convex projection comparison experiments. analysis demanded justify redundency convex projection.", "year": 2017}