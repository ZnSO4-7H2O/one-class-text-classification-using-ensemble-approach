{"title": "An Empirical Comparison of Neural Architectures for Reinforcement  Learning in Partially Observable Environments", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long Short-Term Memory, Gated Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures. A variant of fitted Q iteration, based on Advantage values instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results.", "text": "paper explores performance ﬁtted neural iteration reinforcement learning several partially observable environments using three recurrent neural network architectures long shortterm memory gated recurrent unit recurrent neural architecture evolved pool several thousands candidate architectures variant ﬁtted iteration based advantage values instead values also explored. results show performs signiﬁcantly better lstm problems considered requiring less training episodes less time learning good policy. advantage learning also tends produce better results. reinforcement learning originally developed markov decision processes allows agent learn policy maximize possibly delayed reward signal stochastic environment guarantees convergence optimal policy provided agent sufﬁciently experiment environment operating markovian. many real world problems however agent cannot directly perceive full state environment must make decisions based incomplete observations system state. partial observability introduces uncertainty true environment state renders problem nonmarkovian agent’s point view. deal partially observable environments equip agent memory past observations actions order help discover current state environment memory implemented variety ways including explicit history windows article focuses reinforcement learning using recurrent neural networks function approximation. unlike basic feed-forward networks recurrent neural networks contain cyclic connections neurons. cycles give rise dynamic temporal behavior function internal memory allows networks model values associated sequences observations paper aims comparing different recurrent neural architectures used model value functions reinforcement learning context. next section provides necessary background reinforcement learning recurrent network architectures compared paper. section describes experimental setup environments used comparison. empirical results provided section finally conclude section discrete-time reinforcement learning consists agent repeatedly senses observations environment performs actions. action environment changes state agent receives reward observation agent knowledge interact environment order learn policy r|a| gives probability distribution taking actions given observation. optimal policy that followed agent maximizes reward received agent depends solely current observation action problem reduced markov decision process said completely observable partially observable markov decision problems occur reward depend state whose dynamics still obey underlying agent cannot observe directly. case unknown one-way function part environment. q-learning estimates function maps state-action pair expected optimal cumulative discounted reward reachable taking action given observation time step agent observes takes action observes ot+. equation used update function time step learning factor. advantage learning related q-learning artiﬁcially decreases value non-optimal actions. widens difference value optimal action ones allows learning converge easily even values approximated equation used update advantage values time step smaller widest optimal non-optimal actions becomes. large even continuous environments exact representation q-function longer possible. cases function approximation architecture needed represent target function. shown however on-line q-learning diverge converge slowly used combination function approximation solution problem learn q-values off-line. method used paper neural ﬁtted iteration described adaptation ﬁtted iteration using neural networks. agent interacts environment using ﬁxed policy reaching goal maximum number time steps elapsed collects samples form number episodes model trained batch collected data. model maps sequences observations action values r|a|. lstm cell stores value. output gate allows cell modulate output strength input gate controls intensity input signal continuously added cell’s content. forget gate zero clears content cell. equations show values gates computed. equations show compute value memory cell equation shows output lstm cell. bakker proposes neural network architecture tailored reinforcement learning. network input neuron observation variable output neuron action. softmax layer transforms output neural network probability distribution actions. neural network consists lstm layer simple tanh layer working parallel input network lstm tanh layer layers connected output output tanh layer connected input lstm layer output lstm layer connected input tanh layer article uses simpler version network input connected tanh layer turn connected lstm layer connected output. tanh layer lstm layer contain neurons introduced recently follows design completely different lstm instead storing value memory cell updating using input forget gates unit computes candidate activation based input produces output blend past output candidate activation. equations show gates computed. equations show input mixed last activation order produce candidate activation equation shows last activation candidate activation mixed produce activation. models built keras http//keras.io/ python library providing neural network primitives based theano keras provides lstm dense fully-connected weighted layers layers assembled either stack directed acyclic graph. connection scheme makes network layer graph cyclic hence impossible build using current version keras. j´ozefowicz observed lstm different other wondered whether recurrent neural architectures could used. order discover them developed genetic algorithm evaluated thousands recurrent neural architectures. experiment ﬁnished identiﬁed three architectures performed good better lstm test vectors paper considers produced best results preliminary experiments. equations show compute value gates equations show compute candidate activation equation shows output uses type mixing used gru. order keep training time manageable neural networks trained associate values last observations instead complete history. lstm able associate values arbitrarily long sequences inputs keras requires sequences trained length training done carefully want model forget past experiences batch episodes learned. network conﬁgured perform training epochs data using batch size small number epochs prevents model overﬁtting speciﬁc episodes. three environments used evaluate neural network models. ﬁrst simple fullyobservable grid world initial position goal obstacle agent observe coordinates. receives reward time step hits wall obstacle reaches goal. last environment also based grid world agent observe orientation distance wall front agent-centric environment close actual robots experience. stochastic variant experiments uses random initial position every episode. agent sense initial coordinate ﬁrst time step even otherwise partially observable environments. observations agent consist integer numbers encoded using one-hot encoding easily processed neural networks. instance coordinate grid world take values encoded grid world neural networks therefore input neurons. experiment consists episodes maximum time steps. episodes neural network trained data values computed based stored list. every batch episodes neural networks trained values described shown figure value iteration q-learning advantage learning neural network architecture feed-forward perceptron single hidden layer lstm world gridworld partially observable gridworld agent-centric gridworld fixed initial position random initial position softmax action selection temperature advantage learning leads smaller learning times standard deviations q-learning worlds except partially observable grid world. figure shows behavior advantage learning algorithms partially observable grid world. q-learning allows faster convergence smaller standard deviation. using random initial position model allowing learning environments advantage learning used. lstm give comparable results partially observable worlds statistically signiﬁcant difference them. agents using function approximator nearly always manage learn good enough policy partially observable worlds need large number episodes however plain perceptron-based agents don’t manage learn policy worlds shows allows better learning partially observable worlds simple non-recurrent neural network. feed-forward neural network always achieves best scores grid world followed lstm ﬁnally mut. always outperforms network architectures partially observable worlds. worlds statistically signiﬁcantly better lstm cases except figure average reward runs episode. partially observable grid world qlearning allows higher rewards using lstm using advantage lstm need episodes learning. lstm compared simple reinforcement learning problems. shown agents using lstm approximating advantage values perform signiﬁcantly better ones using obtaining higher rewards learning faster. lstm provide comparable performance often signiﬁcantly better lstm. lstm never signiﬁcantly better gru. considering rewards received agents learned time required learning always achieves better results lstm. shows using instead lstm considered tackling reinforcement problems. moreover machine used experiments simpler cell allowed gru-based agents complete episodes approximately times faster lstm-based agents.", "year": 2015}