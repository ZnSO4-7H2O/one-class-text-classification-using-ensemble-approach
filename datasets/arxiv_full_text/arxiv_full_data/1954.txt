{"title": "Learning image representations tied to ego-motion", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "Understanding how images of objects and scenes behave in response to specific ego-motions is a crucial aspect of proper visual development, yet existing visual learning methods are conspicuously disconnected from the physical source of their images. We propose to exploit proprioceptive motor signals to provide unsupervised regularization in convolutional neural networks to learn visual representations from egocentric video. Specifically, we enforce that our learned features exhibit equivariance i.e. they respond predictably to transformations associated with distinct ego-motions. With three datasets, we show that our unsupervised feature learning approach significantly outperforms previous approaches on visual recognition and next-best-view prediction tasks. In the most challenging test, we show that features learned from video captured on an autonomous driving platform improve large-scale scene recognition in static images from a disjoint domain.", "text": "contend today’s visual recognition algorithms crippled much like passive kitten. culprit learning bags images. ever since statistical learning methods emerged dominant paradigm recognition literature norm treat images i.i.d. draws underlying distribution. whether learning object categories scene classes body poses features themselves idea discover patterns within collection snapshots blind physical source. answer learn video? partially. without leveraging accompanying motor signals initiated videographer learning video data escape passive kitten’s predicament. inspired concept propose treat visual learning embodied process visual experience inextricably linked motor activity behind particular goal learn representations exploit parallel signals ego-motion pixels. hypothesize downstream processing beneﬁt feature space preserves connection move visual surroundings change. cast problem terms unsupervised equivariant feature learning. training input image sequences accompanied synchronized stream ego-motor sensor readings; however need depending context motor activity could correspond either -dof ego-motion observer moving scene second-hand motion object actively manipulated e.g. person robot’s effectors. understanding images objects scenes beresponse speciﬁc ego-motions crucial aspect proper visual development existing visual learning methods conspicuously disconnected physical source images. propose exploit proprioceptive motor signals provide unsupervised regularization convolutional neural networks learn visual representations egocentric video. speciﬁcally enforce learned features exhibit equivariance i.e. respond predictably transformations associated distinct ego-motions. three datasets show unsupervised feature learning approach signiﬁcantly outperforms previous approaches visual recognition next-best-view prediction tasks. challenging test show features learned video captured autonomous driving platform improve large-scale scene recognition static images disjoint domain. visual learning shaped ego-motion? famous kitten carousel experiment psychologists held hein examined question analyze role self-produced movement perceptual development designed carousel-like apparatus kittens could harnessed. eight weeks birth kittens kept dark environment except hour carousel. kitten active kitten could move freely volition attached. kitten passive kitten carried along basket could control movement; rather forced move exactly active kitten. thus kittens received visual experience. however active kitten simultaneously experienced signals motor actions passive kitten not. outcome experiment remarkable. active kitten’s visual perception indistinguishable kittens raised normally passive kitten suffered fundamental problems. implication figure goal learn feature space equivariant ego-motion. train image pairs video accompanied sensed ego-poses produce feature mapping images undergoing ego-pose change move similarly feature space left scatter plot motions among pairs frames apart video kitti car-mounted camera clustered motion patterns pij. center frame pairs right turn left turn zoom motion patterns. right illustration equivariance property seek learned feature space. pairs frames corresponding ego-motion pattern ought predictable relative positions learned feature space. best seen color. possess semantic labels. ego-motor signal could correspond example inertial sensor measurements received alongside video wearable carmounted camera. objective learn feature mapping pixels video frame space equivariant various motion classes. words learned features change predictable systematic ways function transformation applied original input. develop convolutional neural network approach optimizes feature desired egomotion-based equivariance. exploit features recognition augment network classiﬁcation loss class-labeled images available. ego-motion serves side information regularize features learned show facilitates category learning labeled examples scarce. sharp contrast idea previous work visual features—whether hand-designed learned—primarily targets feature invariance. invariance special case equivariance transformations input effect. typically seeks invariance small transformations e.g. orientation binning pooling operations sift/hog modern cnns target invariance local translations rotations. powerful concept invariant representations require delicate balance much invariance leads loss useful information discriminability. contrast general equivariant representations intriguing capacity impose structure output space without forcing loss information. equivariance active exploits observer motor signals like hein held’s active kitten. main contribution novel feature learning approach couples ego-motor signals video. knowledge ﬁrst attempt ground feature learning physical activity. limited prior work unsupervised feature learning video learns passively observed scene dynamics uninformed explicit motor sensory cues. furthermore equivariance explored recent work unlike idea typically focuses image transformations opposed ego-motion considers existing features finally whereas existing methods learn image transformations focus view synthesis applications explore recognition applications learning jointly equivariant discriminative feature maps. apply approach three public datasets. pure equivariance well recognition tasks method consistently outperforms related techniques feature learning. challenging test method show features learned video captured vehicle improve image recognition accuracy disjoint domain. particular unlabeled kitti data regularize feature learning -class scene recognition task dataset results show promise departing images mindset favor embodied approach feature learning. invariant features invariance special case equivariance wherein transformed output remains identical input. invariance known valuable visual representations. descriptors like sift aspects cnns like pooling convolution hand-designed invariance small shifts rotations. feature learning work aims learn invariances data strategies include augmenting training data perturbing image instances label-preserving transformations inserting linear transformation operators feature learning algorithm relevant work feature learning methods based temporal coherence slow feature analysis idea require learned features vary slowly continuous video since visual stimuli gradually change adjacent frames. temporal coherence explored unsupervised feature learning cnns applications dimensionality reduction object recognition metric learning temporal coherence inferred body poses unlabeled video exploited invariant recognition methods exploit video source free supervision achieve invariance analogous image perturbations idea above. contrast method exploits video coupled ego-motor signals achieve general property equivariance. equivariant representations equivariant features also hand-designed learned. example equivariant co-variant operators designed detect repeatable interest points recent work explores ways learn descriptors in-plane translation/rotation equivariance latter perform feature learning equivariance properties crafted speciﬁc image transformations. contrast target complex equivariances arising natural observer motions cannot easily crafted method learns data. methods learn representations disentangled latent factors sort properties like pose illumination etc. distinct portions feature space. example transforming auto-encoder learns explicitly represent instantiation parameters object parts equivariant hidden layer units methods target equivariance limited sense inferring pose parameters appended conventional feature space designed invariant. contrast formulation encourages equivariance complete feature space; show impact unsupervised regularizer training recognition model limited training data. work quantiﬁes invariance/equivariance various standard representations including features terms responses speciﬁed in-plane image transformations adopt deﬁnition equivariance used work goal entirely different. whereas quantiﬁes equivariance existing descriptors approach learns feature space equivariant. learning transformations methods train pairs transformed images infer implicit representation transformation itself. bilinear models multiplicative interactions used learn contentindependent motion features encode transformation image pairs. model gated autoencoder extended perform sequence prediction video recurrent neural networks combined grammar model scene dynamics also predict future frames video whereas methods learn representation image pairs related transformation learn representation individual images behavior transformations predictable. furthermore whereas prior methods abstract away image content method preserves making features relevant recognition. egocentric vision renewed interest egocentric computer vision methods though none perform feature learning using motor signals pixels concert propose. recent methods ego-motion cues separate foreground background infer ﬁrstperson gaze work relies solely apparent image motion method exploits robot’s motor signals detect moving objects uses reinforcement learning form robot movement policies exploiting correlations motor commands observed motion cues. goal learn image representation equivariant respect ego-motion transformations. image original pixel space associated ego-pose representation. egopose captures available motor signals could take variety forms. example encode complete observer camera pose subset parameters reading motor sensor paired camera. input learning algorithm training image pairs associated ego-poses i}nu image pairs originate video sequences though need adjacent frames time. contain pairs multiple videos cameras. note training data semantic labels labeled terms ego-motor sensor readings. following ﬁrst explain translate egopose information pairwise motion pattern annotations then deﬁnes precise nature equivariance seek deﬁnes learning objective. shows equivariant feature learning scheme used enhance recognition limited training data. finally show feedforward neural network architecture trained produce desired equivariant feature space. first want organize training sample pairs discrete ego-motion patterns. instance egomotion pattern might correspond tilt downwards approximately could collect data explicitly controlling patterns prefer data-driven approach leverage video ego-pose data collected wild. discover clusters among pose difference vectors pairs temporally close frames video simplicity apply k-means clusters though methods possible. denote motion pattern i.e. cluster belongs. replace ego-pose vectors motion pattern left panel illustrates motion patterns discovered videos kitti dataset captured moving car. consists position angle camera. clustering space consisting forward distance change yaw. illustrated center panel largest clusters correspond car’s three primary ego-motions turning left turning right going forward. given wish learn feature mapping function parameterized maps single image d-dimensional vector space equivariant ego-motion. equivariant function must respond systematically predictably ego-motion called equivariance feature space represents afﬁne transformation feature space corresponds transformation pixel space. example suppose motion pattern corresponds turn images observed turn respectively. equivariance demands matrix maps pre-turn image post-turn image images expressed feature space hence organizes feature space movement particular direction feature space predictable outcome. linear case also studied ensures structure mapping simple form convenient learning since encoded fully connected layer neural network. prior work focuses equivariance image warp explore case ego-motion pattern reﬂecting observer’s movement world. theory appearance changes image response observer’s ego-motion determined ego-motion alone. also depend depth scene motion dynamic objects scene. could easily augment either frames ego-pose depth maps available. nonobserver motion appears difﬁcult especially face changing occlusions newly appearing objects. however experiments indicate learn effective representations even dynamic objects. implementation train pairs relatively close time avoid pitfalls. training target equivariance discrete ego-motions learned feature space limited preserving equivariance pairs originating ego-motions. linear equivariance maps composable. operating space every ego-motion composed sequence atomic motions equivariance atomic motions sufﬁcient guarantee equivariance motions. this suppose maps turn head right turn head respectively i.e. novel diagonal motion composed atomic motions mrmu equivariance novel ego-motion even though among property lets restrict attention relatively small number discrete ego-motion patterns training still learn features equivariant w.r.t. ego-motions. design loss function encourages learned feature space exhibit equivariance respect ego-motion pattern. speciﬁcally would like learn optimal feature space parameters jointly equivariance maps motion pattern clusters achieve this naive translation deﬁnition equivariance minimization problem feature space parameters equivariance candidate matrices would follows distance measure. problem decomposed independent optimization problems motion corresponding inner summation above dealing disjoint data. g-th problem requires training frame pairs annotated motion pattern approximately satisfy however formulation admits problematic solutions perfectly optimize e.g. trivial allzero feature space all-zeros matrix loss evaluates zero. avoid solutions force learned mg’s different another simultaneously account negatives motion pattern. learning objective video time indices temporal neighborhood size hyperparameter. loss encourages representations nearby frames similar another. however crucially account nature ego-motion frames. accordingly temporal coherence helps learn invariance small image changes target equivariant space. like passive kitten hein held’s experiment temporal coherence constraint watches video passively learn representation; like active kitten method registers observer motion explicitly video learn effectively demonstrate results. thus described formulation generic equivariant image representation learning optionally used visual recognition tasks. suppose addition ego-pose annotated pairs also given small class-labeled static images integrate unsupervised feature learning scheme recognition task optimizing misclassiﬁcation loss together matrix classiﬁer weights. solve jointly maps indicator function. contrastive loss penalizes distance positive mode pushes apart pairs negative mode minimum margin distance speciﬁed constant norm distance objective contrastive loss operates latent feature space. pairs belonging cluster contrastive loss penalizes feature space distance ﬁrst image transformed pair similar above. pairs belonging clusters requires transformation deﬁned must bring image representations close together. objective learns mg’s jointly. ensures distinct ego-motions applied input different locations feature space. denotes softmax loss learned features log) softmax probability correct class. regularizer weight hyperparameter. note neither supervised training data testing data recognition required associated sensor data. thus features applicable standard image recognition tasks. case unsupervised ego-motion equivariance loss encodes prior feature space improve performance supervised recognition task limited training examples. hypothesize feature space embeds knowledge objects change under different viewpoints manipulations allows recognition system sense hallucinate views object improve performance. mapping convolutional neural network architecture parameter vector represents layer weights. loss optimized sharing weight parameters among identical stacks layers siamese network shown rows image pairs stacks. stacks initialized temporal drlim pertinent baselines they like contrastive loss-based formulations represent popular slowness-based family techniques unsupervised feature learning video which unlike approach passive. recall fully unsupervised mode method trains pairs video frames annotated ego-poses supervised mode applied recognition method additionally access class-labeled images similarly baselines receive pool unsupervised data supervised data. detail data composing sets. unsupervised datasets consider unsupervised datasets norb kitti norb dataset ×-pixel images toys captured systematically varying camera pose. generate random train-validation split ego-pose vectors consisting camera elevation azimuth. dataset discrete egopose variations consider ego-motion patterns i.e. step along elevation step along azimuth. equiv available positive pairs motion patterns training images yielding -pair training set. drlim temporal create -pair training pairs within step treated temporal neighbors turntable results kitti dataset contains videos registered gps/imu sensor streams captured driving around types areas campus city residential road. generate random train-validation split ego-pose vectors consisting forward position sensors. discover ego-motion patterns frame pairs second apart. compute clusters automatically retain largest motions upon inspection correspond forward motion/zoom right turn left turn equiv create -pair training positives. drlim temporal create -pair training temporal neighbor positives sampled seconds apart. grayscale camera frames downsampled pixels adopt architecture choices known effective tiny images figure training setup siamese network computing equivariance loss together third tied stack computing supervised recognition softmax loss supp exact network speciﬁcations. optimize array equivarance maps represented fully connected layer connected second stack. equivariance feeds motion-pattern-speciﬁc contrastive loss function whose inputs ﬁrst stack output ego-motion pattern pij. optimize addition siamese minimizes above supervised softmax loss minimized third replica layer stack weights tied siamese networks stacks. labelled images stack output softmax layer whose input class label. complete scheme depicted optimization done mini-batch stochastic gradient descent implemented backpropagation caffe package validate approach public datasets compare existing methods equivariance recognition performance next-best view selection throughout compare following methods temporal coherence approach regularizes classiﬁcation loss setting distance measure distance method aims learn invariant features exploiting fact adjacent video frames change much. table average equivariance error norb ego-motions like training novel ego-motions recognition result datasets accuracy repetitions. next-best view selection accuracy method equiv clearly outperforms baselines. ance. closely follow equivariance evaluation approach solve equivariance maps features produced compared method held-out validation data computing test atomic ego-motions matching provided training pairs composite ego-motions latter lets verify method’s equivariance extends beyond motion patterns used training first sanity check quantify equivariance unsupervised loss isolation i.e. learning equiv method’s average error atomic composite ego-motions norb respectively. comparison drlim—which promotes invariance equivariance— achieves thus without class supervision equiv tends learn nearly completely equivariant features even novel composite transformations. next evaluate equivariance methods using features optimized norb recognition task. table shows results. expected features learned equiv regularization easily equivariant. also methods error lower atomic motions composite motions since equivariant smaller motions next test unsupervised-to-supervised transfer pipeline recognition tasks norb-norb kitti-kitti kitti-sun. ﬁrst dataset pairing unsupervised second supervised. table shows results. datasets method signiﬁcantly improves classiﬁcation accuracy no-prior clsnet baseline also closest previous unsupervised feature learning methods. supervised datasets recognition experiments consider supervised datasets norb select images object training splits random create instance recognition training data. kitti select images location class training splits random create location recognition training data. select images scene categories random create scene recognition training data. preprocess identically kitti images keep supervised datasets small since unsupervised feature learning beneﬁcial labeled data scarce. note video frames unsupervised datasets associated ego-poses static images auxiliary data. network architectures optimization kitti closely follow cuda-convnet recommended cifar- architecture conv-max-relu conv-relu-avg conv-reluavg full feature units. norb fully connected architecture full-relu→ full feature units. parentheses indicate sizes convolution pooling kernels pooling layers stride length nesterov-accelerated stochastic gradient descent. base learning rate regularization selected greedy cross-validation. contrastive loss margin parameter report results methods based repetitions. details architectures optimization supp. figure nearest neighbor image pairs pairwise equivariant feature difference space various query image pairs comparison cols show pixel-wise difference-based neighbor pairs. direction ego-motion query neighbor pairs indicated block. text. posed method. methods based slow feature analysis principle —nearby frames must close another learned feature space. observe practice temporally close frames mapped close training epochs. points possible weakness methods—even parameters cross-validated recognition slowness prior weak regularize feature learning effectively since strengthening causes loss discriminative information. contrast method requires systematic feature space responses ego-motions offers stronger prior. equiv+drlim improves equiv possibly because equiv implementation exploits frame pairs arising speciﬁc motion patterns positives drlim broadly exploits neighbor pairs drlim equiv losses compatible— drlim requires small perturbations affect features small ways equiv requires affect systematically. exciting result kitti-sun. kitti data vastly challenging norb noisy ego-poses inertial sensors dynamic scenes moving trafﬁc depth variations occlusions objects enter exit scene. furthermore fact transfer equiv features learned without class labels kitti useful supervised task different domain indicates generality approach. best recognition accuracy achieved labeled examples class. better nearest competing baseline temporal times better chance. top- accuracy trends similar. thus kept supervised training sets small simulate categorization problems long tail training samples scarce priors useful preliminary tests larger labeled training sets show advantage preserved. samples classes kitti-sun equiv scored .+/-.% accuracy .+/-. clsnet. next show preliminary results direct application equivariant features next-best view selection. given view norb object task tell hypothetical robot move next help recognize object i.e. neighboring view would best reduce object prediction uncertainty. exploit fact equivariant features behave predictably ego-motions identify optimal next view. method task similar spirit described detail supp. table shows results. task equiv features easily outperform baselines. qualitatively evaluate impact equivariant feature learning pose nearest neighbor task feature difference space retrieve image pairs related similar ego-motion query image pair shows examples. variety query pairs show neighbor pairs equiv space well pixeldifference space comparison. overall visually conﬁrm desired equivariance property neighbor-pairs equiv’s difference space exhibit similar transformation whereas original image space often not. consider ﬁrst azimuthal rotation norb query pixel distance perhaps dominated lighting identiﬁes wrong ego-motion match whereas approach ﬁnds correct match despite changed object identity starting azimuth lighting etc. boxes show failure cases. instance kitti failure case shown large foreground motion truck query image causes method wrongly miss rotational motion. last decade visual recognition methods focused almost exclusively learning bags images. argue disembodied image collections though clearly valuable collected scale deprive feature learning methods informative physical context original visual experience. presented ﬁrst embodied approach feature learning generates features equivariant ego-motion. results multiple datasets multiple tasks show approach successfully learns equivariant features beneﬁcial many downstream tasks hold great promise novel future applications. acknowledgements research supported part pecase award n--- gift intel.", "year": 2015}