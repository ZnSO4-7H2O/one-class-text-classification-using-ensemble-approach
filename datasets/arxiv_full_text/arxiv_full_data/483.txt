{"title": "The NarrativeQA Reading Comprehension Challenge", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "Reading comprehension (RC)---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.", "text": "reading comprehension contrast information retrieval—requires integrating information reasoning events entities relations across full document. question answering conventionally used assess ability artiﬁcial agents children learning read. however existing datasets tasks dominated questions solved selecting answers using superﬁcial information thus fail test essential integrative aspect encourage progress deeper comprehension language present dataset tasks reader must answer questions stories reading entire books movie scripts. tasks designed successfully answering questions requires understanding underlying narrative rather relying shallow pattern matching salience. show although humans solve tasks easily standard models struggle tasks presented here. provide analysis dataset challenges presents. figure example question–answer pair. snippets extracted humans summaries full text movie scripts books respectively provided model supervision test time. instead model need read full text locate salient snippets based solely question reading document order generate answer. natural language understanding seeks create models read comprehend text. common strategy assessing language understanding capabilities comprehension models demonstrate answer questions documents read akin reading comprehension tested children learning read. reading document reader usually reproduce entire text memory often answer questions underlying narrative elements document salient entities events places relations them. thus testing understanding requires creation questions examine high-level abstractions instead facts occurring sentence time. strategies guessing based global salience. following section survey existing datasets showing either small answerable shallow heuristics hand questions surface form text rather underlying narrative require formation abstract representations events relations expressed course document. answering questions requires readers integrate information distributed across several statements throughout document generate cogent answer basis integrated information. test reader comprehends language pattern match. present task dataset call narrativeqa test reward artiﬁcial agents approaching level competence dataset consists stories books movie scripts human written questions answers based solely human-generated abstractive summaries. tasks questions answered using summaries full story text. give short example sample movie script dataset figure fictional stories number advantages domain. first largely self-contained beyond basic fundamental vocabulary english information salient entities concepts required understand narrative present document expectation reasonably competent language user would able understand second story summaries abstractive generally written independent authors know work reader. make dataset available online. example names words coined author reader need appeal book understand meaning concepts place narrative. ability form concepts based contexts text crucial aspect reading comprehension part tested part question answering tasks present. comprehension models. summarize features collection popular recent datasets table section brieﬂy discuss nature limitations datasets associated tasks. mctest collection short stories multiple questions. question possible answers labelled correct. could used task mctest corpus fact intended answer selection corpus. data human generated answers phrases sentences. main limitation dataset serves evaluation challenge basis end-to-end training models relatively small size. contrast cnn/daily mail children’s book test booktest provide large amounts question–answer pairs. questions cloze-form produced either short abstractive summaries next sentence document context taken tasks associated datasets selecting answer options explicitly provided booktest implicit cnn/daily mail answers always entities document. signiﬁcantly favors models operate pointing particular token indeed successful models datasets attention reader exploit precisely bias data. however models inappropriate answers requiring synthesis answer. bias towards answers shallowly salient serious limitation cnn/daily mail dataset since context documents news stories usually contain small number salient entities focus single event. squad newsqa offer different challenge. large number questions answers provided documents answers spans context document i.e. contiguous sequences words document. although answers single word/entity answers many plausible questions assessing cannot asked sentence passages children’s books similar paragraphs wikipedia articles news articles dataset passages documents retrieved using queries passages retrieved search engine using queries human generated based document cloze-form based highlights cloze-form sentence cloze-form similar human generated based paragraphs human generated based headline highlights search queries document span would contain answer. provide large number questions relatively small number documents fairly short thereby limiting lexical topical diversity models trained data cope with. answers multiword phrases spans generally short rarely cross sentence boundaries. simple models scoring and/or extracting candidate spans conditioned question superﬁcial signal rest document well models trivially generalize problems answers spans document supervision spans provided several discontinuous spans needed generate correct answer. restricts scalability applicability models well squad newsqa complex problems. marco presents bolder challenge questions paired sets snippets contain information necessary answer question answers free-form human generated text. however restriction placed annotators preventing copying answers source documents many answers fact verbatim copies short spans context passages. models well squad weissenborn extracting spans pointing well concerns general applicability solutions dataset searchqa recent dataset context question documents retrieved search engine using question query. however contrast previous datasets neither questions answers produced annotating context documents rather context documents retrieved collecting pre-existing question–answer pairs. such open annotation bias datasets discussed above. however upon examining answers jeopardy data used construct dataset ﬁnds answers bigrams unigrams tokens fewer. sample answers named entities short noun-phrases. summary limitations. several limitations scope depth problems existing datasets. first several datasets small overly naturalistic second naturalistic documents majority questions require single sentence locate supporting information answering this suspect largely artifact question generation methodology annotators created questions context document context documents explicitly answer question identiﬁed using search engine. although factoidlike jeopardy questions searchqa also appears favor questions answerable local context. finally evidence superﬁciality questions architectures evolved solve them tend exploit span selection based representations derived local context query desiderata discussed features limitations deﬁne desiderata follows. wish construct dataset large number question– answer pairs based either large number supporting documents smaller collection large documents. permits training neural network-based models word embeddings provide decent lexical coverage diversity. questions answers natural unconstrained human generated answering questions frequently require reference several parts larger span context document rather superﬁcial representations local context. furthermore want annotators privilege writing answers expressed words consider higher-level relations entities places events rather copy short spans document. furthermore want evaluate models ﬂuency correctness generated free-form answers answer selection problem requires provision sensible distractors correct answer. finally scope complexity problem current models struggle humans capable solving task correctly motivate research development models seeking human reading comprehension ability. data collection method consider complex self-contained narratives documents/stories. make annotation tractable lead annotators towards asking nonlocalized questions provide human written summaries stories generating question–answer pairs. present books movie scripts stories dataset. books collected project gutenberg movie scripts scraped web. matched stories plot summaries wikipedia using titles veriﬁed matching help human annotators. annotators asked determine story summary refer movie book part series produced year. obtained stories. provides smaller documents compared datasets documents long provides good lexical coverage diversity. bottleneck obtaining larger number publicly available stories ﬁnding corresponding summaries. annotators amazon mechanical turk instructed write question–answer pairs based solely given summary. reading annotating summaries tractable unlike writing questions answers based full stories moreover annotators never full stories much less likely questions answers extracted localized context. annotators instructed imagine writing questions test students read full stories summaries. required questions speciﬁc enough given length complexity narratives provide diverse questions characters events happened annotators encouraged words prevented copying. asked answers grammatical complete sentences explicitly allowed short answers think answering full sentence frequently perceived artiﬁcial asking factual information. annotators asked avoid extra unnecessary information question answer avoid yes/no questions questions author actors. obtained. result collection human written natural questions answers. multiple questions summary/story allows consider answer selection simpler version answer generation scratch. answer selection multiple-choice question answering frequently used. additionally collected second reference answer question asking annotators judge whether question answerable given summary provide answer was. questions judged answerable. core statistics collected stories evenly split books movie scripts. partitioned dataset non-overlapping training validation test portions along stories/summaries. table detailed statistics. dataset contains question–answer pairs. questions grammatical questions written human annotators average tokens length mostly formed ‘wh’-questions categorized sample questions table observe good variety question types. interesting category questions something related occurring together/before/after event answers dataset human written short averaging tokens restricted spans documents. answers appear spans summaries stories respectively; expected lower proportion answers spans stories compared summaries constructed. task answering questions based summaries similar scope previous datasets. however summaries contain complex relationships timelines news articles short paragraphs thus provide task different nature. hope narrativeqa motivate design architectures capable modeling relationships. setting similar previous tasks questions answers constructed based supporting documents. full version narrativeqa requires reading understanding entire stories task present intractable existing neural models box. discuss challenges possible approaches following sections. require metrics generated text. evaluate using bleu- bleu- meteor rouge-l using references question except human baseline evaluate reference other. also evaluate models using ranking metric. allows evaluate good model reading comprehension regardless good generating answers. rank answers questions associated summary/story compute mean reciprocal rank section show narrativeqa presents challenging problem current approaches reading comprehension evaluating several baselines based information retrieval techniques neural models. since neural models quite different processes generating answers present results each. also report human performance scoring second reference answer ﬁrst. consider basic baselines retrieve answer selecting span tokens context document based similarity measure candidate span query. compare queries question gold standard answer. answer oracle provides upper bound documents books movie scripts question–answer pairs avg. tok. summaries tok. summaries avg. tok. stories tok. stories avg. tok. questions avg. tok. answers performance span retrieval models including neural models discussed below. using question query obtain generalization results methods. test results computed extracting either -gram -gram full-sentence spans according best performance validation set. consider three similarity metrics extracting spans bleu- rouge-l cosine similarity bag-of-words embedding query candidate span using pre-trained glove word embeddings neural benchmarks ﬁrst benchmark consider simple bidirectional lstm sequence sequence model predicting answer directly query. importantly provide context information either summary story. model might classify question predict answer similar topic category. previous reading comprehension tasks cnn/daily mail motivated models constrained predicting single token input sequence. reader considers entire context predicts distribution unique word types. adapt model sequence prediction using lstm sequence decoder choosing token input step span-prediction model consider simpliﬁed version bi-directional attention flow network omit character embedding layer learn mapping words vector space rather making pre-trained embeddings; single layer bi-directional lstm model interactions among context words conditioned query proposed adopt output-layer tailored spanprediction leave rest unchanged. state-of-the-art model datasets rather provide strong benchmark. span prediction models trained obtaining supervision training oracle model. start indices span achieving highest rouge-l score respect reference answers labels training set. model trained predict spans maximizing probability indices. design narrativeqa dataset makes straight-forward application existing neural architectures computationally infeasible would require running recurrent neural network sequences hundreds thousands time steps computing distribution entire input attention common. neural models resulting document. question becomes query retrieval. problem much harder traditional document retrieval documents passages here similar question short entities mentioned likely occur many times story. retrieval system considers chunks words story computes representations chunks query. select varying number chunks based similarity query. experiment different representations similarity measures section finally concatenate selected chunks correct temporal order insert delimiters obtain much shorter document. span prediction models select span retrieved chunks described section entity. markers permuted training testing none embeddings learn speciﬁc entity’s representation. allows build representations entities stories never seen training since given speciﬁc identiﬁer generic identiﬁers re-used across documents. entities replaced according simple heuristic based capital ﬁrst character respective word appearing lowercase. reading summaries reading comprehension summaries similar number previous reading comprehension tasks questions constructed based context document. however plot summaries tend contain intricate event time lines larger number characters sense complex follow news articles paragraphs wikipedia. table results. given questions constructed based summaries expected neural models span-selection models would perform well. indeed case neural span prediction model signiﬁcantly outperforming proposed methods. however remains signiﬁcant room improvement compared oracle human scores. attention reader given chunk attention reader given chunks attention reader given chunks attention reader given chunks attention reader given chunks span prediction table experiments full stories. chunk contains tokens. higher better metrics. sections explain neural models respectively. note human scores based answering questions given summaries table plain sequence sequence model reader successfully applied cnn/dailymail reading comprehension task also perform well task. observe reader tends copy subsequent tokens context thus behaving like span prediction model. additional inductive bias results higher performance span prediction model. similar observations reader span models also made wang jiang table summarizes results full narrativeqa task context documents full stories. expected observe decline performance span-selection oracle model compared results summaries. unsurprising questions constructed summaries conﬁrms initial motivation designing task. previously considered spans given length across entire story model. short answers words— typically main characters story—the candidate i.e. closest span reference answer easily found mentioned throughout text. longer answers becomes much less likely compared summaries high-scoring span found story. note distinguishes narrativeqa many reviewed datasets. plus neural two-step approach task ﬁrst retrieve relevant chunks stories apply existing reading comprehension models. questions guide system chunk extraction results standalone baselines giving indication difﬁculty aspect task. retrieval quality direct effect performance neural models; challenge models summaries presented with. considered several approaches chunk selection retrieve chunks based highest rouge-l bleu- scoring span respect question story; comparing topic distributions model questions chunks according symmetric kullback–leibler divergence. finally also consider cosine similarity tf-idf representations. found approach lead best performance subsequently applied model validation irrespective number chunks. note used answer query training question validation test. given retrieved chunks experimented several neural models using context. reader better-performing model summaries task underperforms simple no-context seqseq baseline terms mrr. slightly better metrics clearly fails make retrieved context gain distinctive margin no-context seqseq model. increasing number retrieved chunks thereby recall possibly relevant parts story minor positive effect. span prediction model—which also uses selected chunks context—does especially poorly setup. model provided best neural results summaries task suspect performance particularly badly hurt fact little lexical grammatical overlap source questions context provided reader observed signiﬁcant differences varying number chunks. results leave large human performance highlighting success design objective build task realistic straightforward humans difﬁcult current reading comprehension models. proposed dataset meets desiderata section particular constructed dataset number long documents characterised good lexical coverage diversity. questions answers human generated natural sounding. based small manual examination small number questions answers shallow paraphrases sentences full document. questions require reading segments least several paragraphs long cases even multiple segments spread throughout story. computational challenges identiﬁed section naturally suggest retrieval procedure ﬁrst step. title armageddon a.d. question year rogers awaken deep slumber? answer summary snippet rogers remained sleep years. awakes and. story snippet state therefore anthony rogers know alive whose normal span eighty-one years life spread period years. precise lived ﬁrst twenty-nine years life ﬁfty-two since period nearly hundred years spent state suspended animation free ravages katabolic processes without apparent effect physical mental faculties. began long sleep begun real conquest air. found retrieval challenging even humans familiar presented narrative. particular task often requires referring larger parts story addition knowing least background entities. makes search procedure based short question challenging interesting task itself. show example question–answer pairs figures examples chosen small manually annotated question–answer pairs representative collection. particular examples show larger parts story required answer questions. consider figure relevant paragraph depicting injury appears early next snippet lethal consequences injury revealed. illustrates iterative reasoning process well extremely long temporal dependencies encountered manual annotation. shown figure reading comprehension movie scripts requires understanding written dialogue. challenge dialogue typically non-descriptive whereas questions asked based descriptive summaries requiring models read lines. paper ﬁrst large-scale question answering dataset full-length books movie scripts. however although ﬁrst look task learning understand books modeling objectives become important subproblem nlp. include high level plot understanding clustering novels summarization movie scripts grained processing inducing character types understanding relationships characters understanding plans goals narrative structure terms abstract narratives computer vision movieqa dataset fulﬁlls similar role narrativeqa. seeks test ability models comprehend movies question answering part dataset includes full length scripts. introduced dataset tasks training evaluating reading comprehension systems born analysis limitations existing datasets tasks. task resembles tasks provided existing datasets exposes challenges domain ﬁction. fictional stories—in contrast news stories—are selfcontained describe richer entities events relations them. range tasks simple complex addition issue scaling neural models large documents larger tasks signiﬁcantly difﬁcult questions formulated based sentences summary might require appealing possibly discontiguous sentences paragraphs title jacob’s ladder question fatal injury jacob sustains ultimately leads death answer bayonete stabbing gut. summary snippet terriﬁed jacob ﬂees jungle bayoneted unseen assailant. wartime triage tent military doctors fruitlessly treating jacob reluctantly declare dead story snippet spins around attackers jams eight inches bayonet blade jacob’s stomach. jacob screams. loud piercing wail. source text. requires potential solutions tasks jointly model process searching information serve support generating answer alongside process generating answer entailed said support. end-to-end mechanisms searching information attention scale beyond selecting words n-grams short contexts sentences small documents. likewise neural models mapping documents answers determining entailment supporting evidence hypothesis typically operate scale sentences rather sets paragraphs. provided baseline benchmark results sets tasks demonstrating existing models give sensible results summaries traction book-scale tasks. given quantitative qualitative analysis difﬁculty complex tasks suggest research directions help bridge existing models human performance. hope dataset serve challenge machine reading community driver development class neural models take signiﬁcant step beyond level complexity existing datasets tasks permit.", "year": 2017}