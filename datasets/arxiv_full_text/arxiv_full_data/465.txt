{"title": "Generative Deep Neural Networks for Dialogue: A Short Review", "tag": ["cs.CL", "cs.AI", "cs.NE", "I.5.1; I.2.7"], "abstract": "Researchers have recently started investigating deep neural networks for dialogue applications. In particular, generative sequence-to-sequence (Seq2Seq) models have shown promising results for unstructured tasks, such as word-level dialogue response generation. The hope is that such models will be able to leverage massive amounts of data to learn meaningful natural language representations and response generation strategies, while requiring a minimum amount of domain knowledge and hand-crafting. An important challenge is to develop models that can effectively incorporate dialogue context and generate meaningful and diverse responses. In support of this goal, we review recently proposed models based on generative encoder-decoder neural network architectures, and show that these models have better ability to incorporate long-term dialogue history, to model uncertainty and ambiguity in dialogue, and to generate responses with high-level compositional structure.", "text": "researchers recently started investigating deep neural networks dialogue applications. particular generative sequence-to-sequence models shown promising results unstructured tasks word-level dialogue response generation. hope models able leverage massive amounts data learn meaningful natural language representations response generation strategies requiring minimum amount domain knowledge hand-crafting. important challenge develop models effectively incorporate dialogue context generate meaningful diverse responses. support goal review recently proposed models based generative encoder-decoder neural network architectures show models better ability incorporate long-term dialogue history model uncertainty ambiguity dialogue generate responses high-level compositional structure. researchers recently started investigating sequence-to-sequence models dialogue applications. models typically neural networks represent dialogue histories generate select appropriate responses. models able leverage large amounts data order learn meaningful natural language representations generation strategies requiring minimum amount domain knowledge hand-crafting. although seqseq framework different well-established goal-oriented setting models already applied several real-world applications microsoft’s system xiaoice google’s smart reply system prominent examples. researchers mainly explored types seqseq models. ﬁrst generative models usually trained cross-entropy generate responses word-by-word conditioned dialogue context second discriminative models trained select appropriate response candidate responses related strand work researchers also investigated applying neural networks different components standard dialogue system including natural language understanding natural language generation dialogue state tracking evaluation paper focus generative models trained cross-entropy. weakness current generative models limited ability incorporate rich dialogue context generate meaningful diverse responses overcome challenge propose generative models better able incorporate long-term dialogue history model uncertainty ambiguity dialogue generate responses high-level compositional structure. experiments demonstrate importance model architecture related inductive biases achieving improved performance. figure probabilistic graphical models dialogue response generation. variables represent natural language utterances. variables represent discrete continuous stochastic latent variables. classic lstm model uses shallow generation process. problematic mechanism incorporating uncertainty ambiguity forces model generate compositional long-term structure incrementally word-by-word basis. vhred expands generation process adding latent variable utterance helps incorporate uncertainty ambiguity representations generate meaningful diverse responses. mrrnn expands generation process adding sequence discrete stochastic variables utterance helps generate responses high-level compositional structure. models hred hierarchical recurrent encoder-decoder model type seqseq model decomposes dialogue two-level hierarchy sequence utterances sequence words. hred consists three recurrent neural networks encoder context decoder rnn. utterance encoded real-valued vector representation encoder rnn. utterance representations given input context computes real-valued vector representation summarizing dialogue every turn. summary given input decoder generates response word-by-word. unlike encoders previous seqseq models context updated every dialogue turn uses parameters update. gives hred inductive bias helps incorporate long-term context learn invariant representations. vhred latent variable hierarchical recurrent encoder-decoder model hred model additional component high-dimensional stochastic latent variable every dialogue turn. hred dialogue context encoded vector representation using encoder context rnns. conditioned summary vector dialogue turn vhred samples multivariate gaussian variable given along summary vector input decoder rnn. multivariate gaussian latent variable allows modelling ambiguity uncertainty dialogue latent variable distribution parameters provides useful inductive bias helps vhred encode dialogue context real-valued embedding space even dialogue context ambiguous uncertain helps vhred generate diverse responses. mrrnn multiresolution models dialogue parallel stochastic sequences sequence high-level coarse tokens sequence low-level natural language words coarse sequences follow latent stochastic process—analogous hidden markov models—which conditions utterances hierarchical generation process. hierarchical generation process ﬁrst generates coarse sequence conditioned generates natural language utterance. experiments coarse community.. difference /.bashrc /.bashrc. local they’re different ﬁles default /.bashrc sources /.bashrc. local sorry could undersatnd... write terminal gedit /.bashrc opens open /.bashrc. local gedit /.bashrc. local \"... open blank nothing inside guys general something ubuntu xchat xchat-gnome without -gnome. mean drop xchat-gnome xchat setting rules iptables command writes changes etciptables. rules backup messing anything sudo iptables-save something backup rules restore sudo iptables-restore something model response mrrnn act. -ent. different mirror mrrnn noun something vhred dont know hred mrrnn act.-ent. open gedit /.bashrc called something mrrnn noun empty.. vhred it’s /.bashrc /.bashrc hred trying mrrnn act. -ent. using xchat right mrrnn noun xchat-gnome vhred correct hred mrrnn act. -ent. don’t reason need iptables mrrnn noun using ubuntu vhred hred thanks sequences deﬁned either noun sequences activity-entity pairs extracted natural language utterances. coarse sequences utterances modelled separate hred models. hierarchical generation provides important inductive bias helps mrrnn model high-level compositional structure generate meaningful on-topic responses. experiments apply generative models dialogue response generation ubuntu dialogue corpus example given dialogue context model must generate appropriate response. also present results twitter appendix. task studied extensively recent literature corpus ubuntu dialogue corpus consists half million dialogues extracted ubuntu internet relayed chat channel. users entering chat channel usually speciﬁc technical problem. typically users ﬁrst describe problem users help resolve technical problems range software-related hardware-related issues informational needs evaluation carry in-lab human study evaluate model responses. recruit human evaluators. show evaluator dialogue contexts ground truth response candidate model responses. example evaluators compare candidate responses ground truth response dialogue context rate ﬂuency relevancy scale means incomprehensible relevancy means ﬂawless english relevant. addition human evaluation also evaluate dialogue responses w.r.t. activity-entity metrics proposed serban metrics measure whether model response contains activities entities ground truth responses. models generate responses activities entities ground truth responses—including expert responses often lead solving user’s problem—are given higher scores. sample responses model shown table entities. mrrnn noun representations obtains entity score models obtain less half scores human evaluators consistently rate ﬂuency relevancy signiﬁcantly higher baseline models. mrrnn activity representations obtains activity score models obtain less half activity scores performs substantially better baseline models w.r.t. entity score. indicates mrrnns learned model high-level goal-oriented sequential structure ubuntu domain. followed these vhred performs better hred lstm models w.r.t. activities entities. shows vhred generates appropriate responses suggests latent variables useful modeling uncertainty ambiguity. finally hred performs better lstm baseline w.r.t. activities entities underlines importance representing longer-term context. conclusions conﬁrmed additional experiments response generation twitter domain discussion presented generative models dialogue response generation. proposed architectural modiﬁcations inductive biases towards incorporating longer-term context handling uncertainty ambiguity generating diverse on-topic responses high-level compositional structure. experiments show advantage architectural modiﬁcations quantitatively human experiments qualitatively manual inspections. experiments demonstrate need research generative model architectures. although focused three generative models model architectures memory-based models attention-based models also demonstrated promising results therefore deserve attention future research. another line work researchers started proposing alternative training response selection criteria propose ranking candidate responses according mutual information criterion order incorporate dialogue context efﬁciently retrieve on-topic responses. propose model trained using reinforcement learning optimize hand-crafted reward function. models motivated lack diversity observed generative model responses. similarly propose hybrid model—combining retrieval models neural networks hand-crafted rules—trained using reinforcement learning optimize hand-crafted reward function. contrast approaches without combining several models modify training response selection criterion vhred generates diverse responses previous models. similarly optimizing joint log-likelihood sequences mrrnns generate appropriate on-topic responses compositional structure. thus improving generative model architectures potential compensate even remove need hand-crafted reward functions. time models propose necessarily better language models efﬁcient compressing dialogue data measured word perplexity. although models produce responses preferred humans often result higher test perplexity traditional lstm language models. suggests maximizing log-likelihood sufﬁcient training objective models. important line future work therefore lies improving objective functions training response selection well learning directly interactions real users. references bordes weston. learning end-to-end goal-oriented dialog. arxiv preprint arxiv. gorin riccardi wright. help you? speech communication henderson thomson young. deep neural network approach dialog state tracking challenge. kannan kurach ravi kaufmann tomkins miklos corrado lukács ganea young smart reply automated response suggestion email. proceedings sigkdd conference knowledge discovery data mining volume pages ritter cherry dolan. data-driven response generation social media. emnlp serban klinger tesauro talamadupula zhou bengio courville. multiresolution recurrent neural networks application dialogue response generation. arxiv preprint arxiv. sordoni galley auli brockett mitchell j.-y. dolan. neural network approach context-sensitive generation conversational responses. conference north american chapter association computational linguistics p.-h. vandyke gasic mrksic t.-h. young. learning real users rating dialogue success neural networks reinforcement learning spoken dialogue systems. sigdial vinyals neural conversational model. icml workshop t.-h. gasic mrksic p.-h. vandyke young. semantically conditioned lstm-based natural language generation spoken dialogue systems. proceedings conference empirical methods natural language processing pages lisbon portugal september association computational linguistics. http//aclweb.org/anthology/d-. weston. dialog-based language learning. arxiv preprint arxiv. weston chopra bordes. memory networks. iclr young. probabilistic methods spoken–dialogue systems. philosophical transactions royal society black rudnicky. strategy policy learning non-task-oriented conversational systems. annual meeting special interest group discourse dialogue page corpus experiment twitter dialogue corpus containing million dialogues. task generate utterances append existing twitter conversations. task typically categorized non-goal-driven task ﬂuent on-topic response adequate. evaluation carry human study amazon mechanical turk show human evaluators dialogue context along potential responses response generated model conditioned dialogue context. evaluators choose response appropriate dialogue context. evaluators indifferent choose neither response. pair models conduct experiments example contexts contain least unique tokens contain least tokens experiment lstm hred vhred models well tf-idf retrieval-based baseline model. experiment mrrnn models appropriate coarse representations domain. results results given table show vhred strongly preferred majority experiments. particular vhred strongly preferred hred tf-idf baseline models short long context settings. vhred also strongly preferred lstm baseline model long contexts although lstm model preferred vhred short contexts.for short contexts lstm model often preferred vhred lstm model tends generate generic responses. generic safe responses reasonable wide range contexts useful applied through-out dialogue user would loose interest conversation. conclusion vhred performs substantially better overall competing models suggests high-dimensional latent variables help model uncertainty ambiguity dialogue context help generate meaningful responses.", "year": 2016}