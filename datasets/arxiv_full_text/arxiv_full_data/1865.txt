{"title": "Predicting Movie Genres Based on Plot Summaries", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "This project explores several Machine Learning methods to predict movie genres based on plot summaries. Naive Bayes, Word2Vec+XGBoost and Recurrent Neural Networks are used for text classification, while K-binary transformation, rank method and probabilistic classification with learned probability threshold are employed for the multi-label problem involved in the genre tagging task.Experiments with more than 250,000 movies show that employing the Gated Recurrent Units (GRU) neural networks for the probabilistic classification with learned probability threshold approach achieves the best result on the test set. The model attains a Jaccard Index of 50.0%, a F-score of 0.56, and a hit rate of 80.5%.", "text": "project explores several machine learning methods predict movie genres based plot summaries. naive bayes wordvec+xgboost recurrent neural networks used text classiﬁcation k-binary transformation rank method probabilistic classiﬁcation learned probability threshold employed multi-label problem involved genre tagging task. experiments movies show employing gated recurrent units neural networks probabilistic classiﬁcation learned probability threshold approach achieves best result test set. model attains jaccard index f-score rate supervised text classiﬁcation mature tool achieved great success wide range applications sentiment analysis topic classiﬁcation. applying movies previous work focused predicting movie reviews revenue research done predict movie genres. movie genres still tagged manual process users send suggestions email address internet movie database plot summary conveys much information movie explore project different machine learning methods classify movie genres using synopsis. ﬁrst perform experiment naive bayes using bag-of-word features. next make pretrained wordvec embeddings turn plot summaries vectors used inputs xgboost classiﬁer finally train gated recurrent unit neural network genre tagging task. rest report organized follows. section discusses related work. section describes dataset used project. section outlines models experiments. section presents experiment results. finally section summarizes paper discusses directions future work. proposes naive bayes model predict movie genres based user ratings movie. idea users usually consistent preference prefer genres others. evaluation metric percentage movies model predicts correctly least true labels. focus project natural language processing attempt predict movie genres using movie plot summary. moreover adopt rigorous metrics f-score jaccard index. attempts classify movie scripts building logistic regression model using nlp-related features extracted scripts ratio descriptive words nominals ratio dialogues frames non-dialogue frames. movie scripts model based extracted features estimates probability movie belong genre takes best scores predicted genres hyper-parameter. experiment done small dataset scripts best subset features achieves score investigates different methods classify movies’ genres based synopsis. methods examined include one-vs-all support vector machines multi-label k-nearest neighbor parametric mixture model neural network. methods term frequency inverse document frequency words features. dataset used experiment relatively small movie titles train test sets. addition experiment limited predicting popular genres including action adventure comedy crime documentary drama family romance short ﬁlms thrillers. overall achieves highest score movie belong several genres project related multi-label classiﬁcation problem examined previous work. introduces extensions adaboost algorithm multi-class multi-label text categorization. ﬁrst extension turns multi-label problem multiple independent binary classiﬁcation problems second ranks labels correct labels receives highest ranks. extensions trained using weak hypotheses one-level decisions trees check presence absence term given document. second extension objective minimize average fraction pairs labels misordered quantity called empirical rank loss label respectively feature vector true labels l-th example rank value function label total number training examples. predicted labels highest ranks hyperparameter. trained tested multi-label subset reuter- dataset rank-based extension achieves slightly worse performance. inspired ranking method propose kernel method learn ranking function. instead ﬁxing number label size prediction method also learn threshold speciﬁcally input vector rank values labels threshold value deﬁned minimum unique value segment threshold chosen middle segment. threshold modeled using linear least squares. applies idea neural networks slightly modify rank loss function exponential function severely punish model assigning higher rank value wrong labels. project employs several machine learning methods predict movie genres including naive bayes xgboost recurrent neural network xgboost advanced efﬁcient implementation gradient boosting algorithm ensemble method sequentially predictors trained residual errors made previous predictors. unlike standard gradient boosting algorithm xgboost regularized objective makes second-order approximation quickly greedily predictor iteration. xgboost algorithm choice winning solutions many machine learning data mining challenges. extract features used xgboost make wordvec framework proposed learns high-dimensional word embeddings. wordvec learns embedding training neural network predict neighboring words. idea semantically similar words tend occur near text embeddings good predicting context words also good representing similarity. model architectures wordvec learn word embeddings continuous bag-of-words continuous skip-gram. cbow architecture model predicts current word window surrounding context words. continuous skip-gram architecture model predicts surrounding window context words using current word. vector embeddings learned wordvec shown capture relations words. example result embedding vector vector vector vector close vector. project pretrained -dimensional embeddings trained part google news corpus billion words average embedding vectors words movie plot used vector representation plot. taking average embedding vectors words plot might lossy summary fail capture sequential relationship. therefore consider recurrent neural networks powerful family neural networks processing sequential data. rnns take input input sequence time step. hidden units rrns take inputs data previous layers traditional neural networks also previous time step. allows units take information past sequence inputs. parameters rnns shared across time steps learned gradient descent optimization methods back propagation time algorithm computes gradients loss function respect parameters. gradients rnns however tend vanish explode explored depth making difﬁcult train rnns. lstm networks address problem introducing complex unit called memory cell. central feature lstm’s memory cell constant error carrousel self-connection cell state thus allowing gradient signal stay constant ﬂows backward across time steps. memory cells interract other equipped input gates output gates protect perturbation. input output gates multiplicative operations control network’s sensitivity unit’s inputs cell states original lstm model however could become large saturate output gates thus leading gradient vanishing. problem termed internal state drift tackle issue introduced forget gates reset cell states gates decide cell states’ content out-of-date. reset mean cell states zero immediately gradually reset multiplying number architecture allows gradients long duration lstm found extremely successful many applications speech recognition machine translation. many lstm variants proposed. among them gated recurrent unit become increasingly popular. similar lstm combines forget input gates single update gate. simpler architecture lstm architecture experiments rnns. addition experiment variant term self-normalizing cell short-term memory states squashed range hyperbolic tangent function going update gates. result gradient signals become weaker ﬂowing update gates backward passes. motivation sngru avoid using squashing function. keep cell states bounded sngru applies layer normalization helps cell states converge normal distribution. prepare dataset experiments extract movie plot summaries corresponding genres imdb datasets. data ﬁles including plot list genre list ﬁles downloaded genre list pairs movie name genre. noteworthy pairs movie name movie belongs genres. plot list pairs movie plot summary. movies genres plot summaries available. selecting movies belong least genres listed below results movies. finally ﬁltering movies tokens plot summaries leaves movies. divide data train test sets according proportion. result movies train data movies test data. tokens unique word types train plot summaries. these word types occur times more. genre names percentages movies drama comedy thriller romance action family horror crime adventure animation fantasy sci-ﬁ mystery biography music history western sport musical train dataset unique sets genres. movie belongs average genres. distribution number genres movie belong higher movie plot summaries fewer tokens tokens tokens tokens. average tokens plot summary task experiment three machine learning methods. ﬁrst method naive bayes simple approach shown effective many text classiﬁcation tasks. naive bayes makes assumptions. ﬁrst bag-of-words assumption meaning order words matter. second conditional independence assumption. document ww...wn given class probabilities independent. thus consider approaches apply naive bayes multi-label classiﬁcation task involved project. ﬁrst approach build binary naive bayes classiﬁer genre. second approach build multinomial naive bayes classiﬁer model. plot estimate posterior probability right hand side model predicts movie belong genre prior probability genre intuition model assigns movie genre model stronger prior belief presence genre reading movie’s plot summary. problem naive bayes bag-of-word feature well conditional independence assumption realistic capture meaning sentences text. therefore second method employ rich contextual representations words learned wordvec framework plot summary turned vector representation taking average embedding vectors words text. assumption average vector good semantic summarization plot summary. plot vector representation xgboost apply xgboost multi-label classiﬁcation task similar spirit ranking method discussed section instead learning rank values however train xgboost classiﬁer output probabilities movie represented belongs genre make predictions based probabilities xgboost regressor trained predict probability threshold categorical distribution. train xgboost classiﬁer train data modiﬁed turning pair plot vector corresponding correct genres pairs train xgboost regressor probabilities estimated xgboost classiﬁer movie train data used features probability threshold computed based corresponding correct genres used target. speciﬁcally probability threshold deﬁned genres. test time xgboost classiﬁer regressor estimate probabilities genre threshold output genres higher probabilities threshold. xgboost python package train xgboost classiﬁer regressor. instead reinventing wheel make pretrained -dimensional embeddings trained part google news corpus billion words lastly train task. learns embedding word also nonlinear transformation sequence embedding vectors. thus might learn suitable representation plot summaries genre tagging task. assumption function used transform current state input state. idea parameter sharing across time steps makes possible apply model examples different length. speciﬁcally train self-normalizing network layers memory cells. input network sequence -dimension vectors representing word plot summary. network learns vector representations vocabulary words including words appear least times train dataset represents words stands sequence. given last state linear transformation applied output -dimension output vector matrix -dimension bias vector. three approaches considered make ﬁnal predictions using output ﬁrst approach element logistic sigmoid function estimate probability genre model makes prediction genre independently predict genre larger model called binary gru. second approach interpreted rank values discussed section loss function deﬁned threshold values train dataset estimated following model following approach called rank gru. last approach softmax function estimate probability genre target probability genre number correct genres correct genres otherwise. network trained minimize cross entropy predicted target categorical distribution. threshold values train dataset estimated following model following approach called multinomial gru. second third approaches xgboost regressor trained predict threshold values test dataset described implementation network tensorflow hyperparameter tune naive bayes models. limited time experiment priority default setting xgboost used. network hyperparameters include number layers number units layer recurrent dropout keep rate learning rate adam optimizer different values hyperparameter considered shown table tune hyperparameters divide dataset train validation test sets according proportion. hyperparameters yield highest softmax loss rank loss validation selected. optimal settings number layers number units layers dropout keep rate learning rate. weak metric rate proportion examples model predicts least correct genre. however tagging movies genres yield rate punish incorrect predictions adopt jaccard index deﬁned number correctly predicted labels divided union predicted true labels |t∩p| |t∪p|. compare performance different methods calculate confusion matrix accuracy precision recall f-score genre well test data. metrics deﬁned approaches estimate baseline. ﬁrst approach simple heuristic predict popular genres movies resulting jaccard indices rates respectively. tagging movies drama comedy reasonable baseline. second approach build binary naive bayes model genre using casefolding simple whitespace tokenizer. method achieves jaccard index signiﬁcant improvement heuristic baseline. rate however additionally f-score approach table summarizes experiment results different methods discussed section multinomial achieves highest jaccard index f-score binary-based models binary binary naive bayes attain high precision rank-based models multinomial naive bayes rank multinomial obtain high recall result plausible binary-based models make decision genre independently return positive prediction conﬁdent. meanwhile rank-based models make decisions based relative rank values among different genres thus output positive prediction often. observation conﬁrmed figure shows multinomial naive bayes rank predict average genres movie. result models highest rates binary binary naive bayes lowest. overall multinomial rank achieve better balance among different metrics. figure shows f-score genre model. seen models consistently attain higher f-score naive bayes models xgboost xgboost consistently under-performs across genres. tuning hyperparameters might improve model little result strongly suggests taking average embeddings vector words plot summary might lossy yield good representation. naive bayes models competitive models popular genres drama comedy thriller romance action under-performs less popular genres. models generalize less popular genres better. reason behavior naive bayes takes bag-of-word conditional independence assumption data highly skewed. result naive bayes models biased towards popular genres. figure shows size distribution predicted genre sets different methods. problem binary-based models many movies tagged genre. binary naive bayes binary respectively predict empty test examples. size distributions genre sets predicted rank-based models closer true distribution. following subsections discuss models details. discussed section consider approaches apply naive bayes movie genre tagging task. ﬁrst approach build binary naive bayes models genre. model preprocessed whitespace tokenizer casefolding used baseline. better process data apply casefolding remove stop words nltk word tokenizer. result approach achieves jaccard index signiﬁcant improvement whitespace-tokenizer version’s score. similarly f-score improves using word normalization procedure multinomial naive bayes attains jaccard index score figure compares f-score genre test data. multinomial naive bayes attains much higher f-score binary naive bayes less popular genres. common models tend high recall popular genres drama comedy high precision recall less popular genres sport western illustrated table result plausible given skewed dataset makes naive bayes models predict popular genres often predict less popular genres strong belief. subsection discusses multinomial model. tab. shows multinomial gru’s predictions random test examples. examples illustrate predicting movie genres challenging task even human. plot summaries convey information movie sometimes ambiguous. ﬁrst example misunderstood girlfriend becomes upset runs away. correct genre romance model predicts drama quite plausible. second example model correctly predicts western incorrectly predicts action perhaps chases catches robber. third example predicted genres drama thriller true genres drama comedy. however plot details stalking murder more people kill suggests thriller comedy. noted jaccard index case. fourth example sign musical unless knows frank sinatra singer biography seems reasonable choice. ﬁfth example model correctly predicts comedy family fails include fantasy mystery romance sci-ﬁ. missed genres possible detect mystery even fantasy plot summary clue romance sci-ﬁ. sign comedy plot summary many comical situations changed many unexpected situations model predicts family horror instead. probabilities family comedy horror changed respectively. sixth seventh example shows model’s bias towards popular genres drama comedy. sixth example model assigns probability drama horror thriller mystery selects drama horror although information plot summary suggesting drama. similarly seventh example model assigns probability romance comedy drama predict romance comedy. correct genre romance only. model also sensitive words plot. last example model assigns probability sci-ﬁ animation action fantasy horror predicts sci-ﬁ animation. model obviously biased towards animation train examples belong animation genre. turned culprit miniaturized. removing word plot model assigns sci-ﬁ action horror fantasy animation correctly predicts sci-ﬁ. ﬁrst example also illustrates problem. drives away changed falls love model changes prediction drama drama romance. finally qualitatively evaluate model’s learned word embeddings looking nearest words random words. nearest words certain word vocabulary embedding vectors smallest cosine distance word’s embedding vector. table shows nearest words nouns verbs adjectives. nearest words nouns relevant. results however mixed verbs adjectives. words kill love escape cruel relevant nearest neighbors manipulate preserve arrogant beautiful totally irrelevant neighbors. possible reason behavior verbs adjectives often used wider range contexts nouns. hang’s neighbors smothering re-connect seem related different meanings word. miniaturized interesting example appears times train data making context speciﬁc. result among nearest words karaati gnomes alchemist meadow fairy-tale explain inﬂuence model’s prediction animation previously discussed example. project explores several machine learning methods predict movie genres based plot summaries. task challenging ambiguity involved multi-label classiﬁcation problem. example predicting adventure thriller true labels adventure action yields jaccard index wordvec+xgboost performs poorly average embedding vectors words document proves weak representation. future direction apply docvec learn richer representations documents. experiments naive bayes networks show combining probabilistic classiﬁer probability threshold regressor works better k-binary transformation rank methods multi-label classiﬁcation problem. using network probabilistic classiﬁer approach model achieves impressive performance jaccard index f-score rate several potential directions improve network. words vocabulary occur fewer times train data word embeddings weight updates. using pretrained embeddings might better words. addition tokens turned unk. result network learns embedding words. finding adapt unk’s embedding based context might make network powerful. finally data highly skewed making model biased towards popular genres drama comedy. dealing issue would improve performance. robber robber’s boss denver jack released crooked lawyer larry randall later randall decides wants reform tells denver quitting. denver framed murder charge randall hanged...unless ryder little beaver infamous \"rat pack\". ’sammy davis ’dean martin ’peter lawford’ ’joey bishop worked played together. dramatizes volatile relationships kennedys ’marilyn monroe’ mobster ’sam giancana’ ’judith campbell exner’ fbi. sinatra helps ’john kennedy’ elected little help giancana. lawford married kennedy unhappy go-between. davis ﬁghting racism insecurity. campbell sleeping father olsson family responsible many people’s able receive broadcasts. therefore family escape castle country celebrate christmas there away angry people dangers. however three children soon discover something’s right castle. seems haunted kaden mackenzy teenage girl ability communicate dead stumbles upon secret haunting small town peabody massachusetts ages. kaden’s gift suddenly becomes curse interactions soon unfold mystery historic aromas sensations coffee shop. coffee barista discovers wraps magic sensations atmosphere created combination aromas coffee; people frequent place unexpectedly experience feelings romance odette instant keynes aﬁcionados miserably drowning shine gossip west ﬂirts despise analogy stems sang gauri obsessing sympathizes cabinets language martín abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems software available tensorﬂow.org. tianqi chen carlos guestrin. xgboost scalable tree boosting system. proceedings sigkdd international conference knowledge discovery data mining pages kyunghyun bart merriënboer dzmitry bahdanau yoshua bengio. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. yoav freund robert schapire. desicion-theoretic generalization on-line learning application boosting. european conference computational learning theory pages springer min-ling zhang zhi-hua zhou. multilabel neural networks applications functional genomics text categorization. ieee transactions knowledge data engineering", "year": 2018}