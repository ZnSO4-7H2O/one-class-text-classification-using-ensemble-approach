{"title": "Integrating Document Clustering and Topic Modeling", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "Document clustering and topic modeling are two closely related tasks which can mutually benefit each other. Topic modeling can project documents into a topic space which facilitates effective document clustering. Cluster labels discovered by document clustering can be incorporated into topic models to extract local topics specific to each cluster and global topics shared by all clusters. In this paper, we propose a multi-grain clustering topic model (MGCTM) which integrates document clustering and topic modeling into a unified framework and jointly performs the two tasks to achieve the overall best performance. Our model tightly couples two components: a mixture component used for discovering latent groups in document collection and a topic model component used for mining multi-grain topics including local topics specific to each cluster and global topics shared across clusters.We employ variational inference to approximate the posterior of hidden variables and learn model parameters. Experiments on two datasets demonstrate the effectiveness of our model.", "text": "document clustering topic modeling closely related tasks mutually beneﬁt other. topic modeling project documents topic space facilitates eﬀective document clustering. cluster labels discovered document clustering incorporated topic models extract local topics speciﬁc cluster global topics shared clusters. paper propose multi-grain clustering topic model integrates document clustering topic modeling uniﬁed framework jointly performs tasks achieve overall best performance. model tightly couples components mixture component used discovering latent groups document collection topic model component used mining multi-grain topics including local topics speciﬁc cluster global topics shared across clusters. employ variational inference approximate posterior hidden variables learn model parameters. experiments datasets demonstrate eﬀectiveness model. text domain document clustering topic modeling widely studied problems many applications. document clustering aims organize similar documents groups crucial document organization browsing summarization classiﬁcation retrieval. topic modeling develops probabilistic generative models discover latent semantics embedded document collection demonstrated vast success modeling analyzing texts. document clustering topic modeling highly correlated mutually beneﬁt other. hand topic models discover latent semantics embedded document corpus semantic information much useful identify document groups term features. classic document clustering approaches documents usually represented bag-of-words model purely based terms insuﬃcient capture semantics. topic models able words similar semantics group called topic synonymous words treated same. under topic models document corpus projected topic space reduces noise similarity measure grouping structure corpus identiﬁed eﬀectively. hand document clustering facilitate topic modeling. speciﬁcally document clustering enables extract local topics speciﬁc document cluster global topics shared across clusters. collection documents usually belong several groups. instance scientiﬁc paper archive google scholar papers multiple disciplines math biology computer science economics. group topics. instance computer science papers cover topics like operating system network machine learning economics papers contain topics like entrepreneurial economics ﬁnancial economics mathematical economics. besides group-speciﬁc topics common global topics shared groups. paper archive papers groups share topics like reviewing related work reporting experimental results acknowledging ﬁnancial supports. clustering help identify latent groups document collection subsequently identify local topics speciﬁc group global topics shared groups exploiting grouping structure documents. ﬁne-grained topics facilitate utilities. instance group-speciﬁc local topics summarize browser group documents. global topics used remove background words describe general contents whole collection. standard topic models lack mechanism model grouping behavior among documents thereby extract single topics local topics global topics mixed distinguished. naively perform tasks separately. make topic modeling facilitates clustering ﬁrst topic models project documents topic space perform clustering algorithms k-means topic space obtain clusters. make clustering promotes topic modeling ﬁrst obtain clusters using standard clustering algorithms build topic models extract cluster-speciﬁc local topics cluster-independent global topics incorporating cluster labels model design. however naive strategy ignores fact document clustering topic modeling highly correlated follow chicken-and-egg relationship. better clustering results produce better topic models better topic models turn contribute better clustering results. performing separately fails make mutually promote achieve overall best performance. paper propose generative model integrates document clustering topic modeling together. given corpus assume exist several latent groups document belongs latent group. group possesses local topics capture speciﬁc semantics documents group dirichlet prior expressing preferences local topics. besides assume exist global topics shared groups capture common semantics whole collection common dirichlet prior governing sampling proportion vectors global topics documents. document mixture local topics global topics. words document either generated global topic local topic group document belongs. model latent variables cluster membership document-topic distribution topics jointly inferred. clustering modeling seamlessly coupled mutually promoted. rest paper organized follows. section reviews related work. section propose mgctm model present variational inference method. section gives experimental results. section concludes paper points future research directions. document clustering widely studied problem many applications document organization browsing summarization classiﬁcation. broad overview. popular clustering methods k-means spectral clustering general clustering literature extensively used document grouping. speciﬁc text domain popular paradigm clustering methods based matrix factorization including latent semantic indexing non-negative matrix factorization concept factorization basic idea factorization based methods transform documents original term space latent space. transformation reduce data dimensionality reduce noise similarity measure magnify semantic eﬀects underlying data beneﬁcial clustering. researchers applied topic models cluster documents. investigated clustering performance plsa lda. plsa model corpus topic treated cluster. documents clustered examining topic proportion vector document assigned cluster argmaxjθj. model texts identify latent semantics underlying document collection. topic models posit document collection exhibits multiple latent semantic topics topic represented multinomial distribution given vocabulary document mixture hidden topics. vision domain topic models also widely used image modeling. several models devised jointly model data category labels cluster labels. fei-fei proposed bayesian hierarchical model jointly model images categories. category possesses model categoryspeciﬁc dirichlet prior topics. problem category labels observed. paper interested unsupervised clustering cluster label unknown. wallach proposed cluster based topic model introduces latent variables model groups group owns group-speciﬁc dirichlet prior governing sampling document-topic distribution. document associated group indicator topic proportion vector generated dirichlet prior speciﬁc group. proposed similar model used scene classiﬁcation computer vision. associate group logisticnormal prior rather dirichlet prior. however models groups share single topics. lack mechanism identify local topics speciﬁc cluster global topics shared clusters. another issue topics inherently belonging group used generate documents group problematic. instance modeling scientiﬁc papers unreasonable computer architecture topic computer science group generate economics paper. models proposed prohibit problem since topics shared across groups. eventually inferred topics less coherent discriminative enough diﬀerentiate clusters. idea using ﬁne-grained topics belonging several sets rather topics single model documents exploited represents document combination background distribution common words mixture distribution general topics distribution words treated speciﬁc document. proposed multi-grain topic model online review modeling. local topics capture ratable aspects utilize global topics capture properties reviewed items. proposed multiview topic model ideological perspective analysis. ideology ideology-speciﬁc topics ideology-speciﬁc distribution words. documents share ideology-independent topics. problem ideology label document observed. corpus containing documents group possesses groupspeciﬁc local topics local topics used capture semantics speciﬁc group. besides group group-speciﬁc local dirichlet prior local topic proportion vectors documents group sampled except local topics group also assume exist single shared groups. global topics used model universal semantics whole collection. global dirichlet prior used generate proportion vectors global topics shared documents. global multinomial prior used choose group membership document. denotes prior probability document belongs group document associated group indicator multinomial distribution local topics multinomial distribution global topics. words document either generated local topics global topics. introduce bernoulli variable word indicate whether word sampled global topic local topic. bernoulli distribution document sampled corpus level beta prior generate doci= ﬁrst choose group multinomial distribution parametrized local dirichlet prior corresponding group sample local topic proportion vector global dirichlet prior multinomial distribution global topics sampled. beta distribution parameterized sample bernoulli distribution binary decision made word position make choice local topics global topics. gentopic model component. document clustering accomplished estimating mixture component. topic modeling involves inferring topic model component. described section latent variables inferred maximizing likelihood lower bound. performing clustering modeling separately equivalent inferring latent variables component ﬁxing component. case ﬁrst documents using topic model perform clustering actually clamping latent variables topic model component mgctm predeﬁned values estimating mixture model component maximizing likelihood observations. case topic modeling follows clustering latent variables mixture model component predeﬁned maximize likelihood respect topic model component. contrast performing tasks jointly equivalent maximizing likelihood w.r.t latent variables components simultaneously. suppose maximize function deﬁned partitioned subsets denote optimal value achieved denote optimal value obtained optimizing ﬁxing preset value denote optimal value obtained optimizing ﬁxing preset value clearly following inequalities hold property conclude jointly performing clustering modeling grants better results separately. would interesting make comparison model gaussian mixture model cluster based topic models context document clustering modeling. document converted term vector. associates cluster multivariate gaussian distribution. generate document ﬁrst samples cluster generate document gaussian distribution corresponding cluster. contrast model mixture ldas. cluster characterized model topics speciﬁc cluster unique dirichlet prior document-topic distributions sampled. generate document model ﬁrst samples cluster corresponding generate document. documents represented terms insuﬃcient capture underlying semantics. model documents modeled using erate word ﬁrst pick binary variable bernoulli distribution parameterized assume generated local topic. local topic picked local topic proportion vector generated topic-word distribution corresponding local topic assume generated global topic. case global topic ﬁrst picked global topic proportion vector generated topic-word distribution corresponding global topic well-known capability discover latent semantics. diﬀerent ldas share common topics allocate topics model. speciﬁc design owns advantages. first explicitly infer group-speciﬁc topics cluster. second avoid problem using topics group generate documents another group. maximization achieved iterative ﬁxedpoint method. e-step model parameters ﬁxed update variational parameters maximizing lower bound. m-step variational parameters update model parameters. process continues convergence. experiments conducted reuters- -newsgroups datasets. datasets widely used benchmark document clustering. reuters- retain largest categories discard documents labels left documents. newsgroups dataset contains documents groups. corpus stop words removed document represented tf-idf vector. compare method following baseline methods k-means normalized probably widely used clustering algorithms; non-negative matrix factorization latent semantic indexing locally consistent concept factorization factorization based approaches showing great eﬀectiveness clustering documents. study topic modeling aﬀects document clustering compare three topic model based methods. ﬁrst naive approach ﬁrst uses learn topic proportion vector document performs k-means topic proportion vectors obtain clusters. lda+kmeans denote approach. second proposed treats topic cluster. document-topic distribution deemed mixture proportion vector clusters utilized clustering. document assigned cluster argmaxjθj. note approach naive solution integrating document clustering modeling together. lda+naive denote approach. third cluster based topic model integrates document clustering modeling whole. experiments input cluster number required clustering algorithms ground truth number categories corpus. hyperparameters tuned achieve best clustering performance. gaussian kernel similarity measure between documents. bandwidth parameter retain eigenvectors form subspace. parameters lccf suggested lda+kmeans lda+naive symmetric dirichlet prior draw document-topic distribution topicword distribution. respectively. lda+kmeans number topics number topics reuters- -newsgroups. mgctm local topics cluster global topics reuters- dataset local topics cluster global topics -newsgroups dataset. mgctm initialize clustering results obtained lda+naive. parameters initialized randomly. table table summarize accuracy normalized mutual information diﬀerent clustering methods seen topic modeling based clustering methods including lda+kmeans lda+naive mgctm generally better k-means normalized factorization based methods. corroborates assumption topic modeling promote document clustering. semantics discovered topic models eﬀectively facilitate accurate similarity measure helpful obtain coherent clusters. compared lda+kmeans performing clustering modeling separately three methods including lda+naive mgctm jointly performing tasks achieve much better results. among lda+naive mgctm unify clustering modeling approach generally better comparable two. because mgctm possesses sophistication terms model design turn contributes better clustering results. lda+naive assigns cluster topic suﬃcient capture diverse semantics within cluster. fails diﬀerentiate cluster-speciﬁc topics clusterindependent topics thereby learned topics discriminative distinguishing clusters. since topics shared clusters topic inherently belonging cluster model document cluster unreasonable cause semantic confusion. model assigns cluster topics avoid topics cluster model documents another cluster suitable produce coherent clusters. section study topic modeling capability model. compare methods. ﬁrst naive approach ﬁrst uses k-means obtain document clusters clamps values document membership variables mgctm obtained clusters labels learns latent variables corresponding topic model component. kmeans+mgctm denote approach. again purpose comparing naive approach investigate whether integrating clustering modeling together superior separately. approach three models -newsgroups dataset. reason choose -newsgroups rather reuters topic modeling evaluation categories -newsgroups semantically clear reuters-. topic number mgctm kmeans+mgctm local topics groups global topics. evaluate inferred topics qualitatively quantitatively. speciﬁcally interested things. first coherent topic second local topic related cluster. seen global topics capture common semantics whole corpus speciﬁcally associated certain news group. global topic news archive organization. topic time. topic article writing. topics used generate documents groups. table shows local topics obtained clusters. seen local topics eﬀectively capture speciﬁc semantics cluster. instance cluster four local topics highly related computer including server program windows display. cluster topics middle east politics including race religion diplomacy. cluster topics space technology including space planets spacecraft nasa. cluster topics closely related health including disease patients doctors food. local topics enable understand cluster easily clearly without burden browsing number documents cluster. model documents cluster generated local topics cluster prohibit local topics cluster generate documents cluster thereby local topic highly related cluster almost correlation clusters. words leaned local topics discriminative diﬀerentiate clusters. contrary topics shared groups. consequently semantic meaning topic ambiguous topic related multiple clusters simultaneously. topics suboptimal summarize clusters vagueness. kmeans+mgctm clusters predeﬁned using k-means whose clustering performance much worse mgctm reported section result quality learned topics kmeans+mgctm also worse mgctm. quantitatively evaluate topic models open problem researchers resort perplexity held-out likelihood. measures useful evaluating predictive model however capable evaluate coherent meaningful inferred topics are. large-scale user studies shows topic models perform better held-out likelihood infer less semantically meaningful topics. thereby perplexity held-out likelihood evaluation metric. evaluate coherent topic pick candidate words topic student volunteers label them. first volunteers need judge whether topic interpretable not. candidate words topic automatically labeled irrelevant. otherwise volunteers asked identify words relevant topic. coherence measure deﬁned ratio number relevant words total number candidate words. model background words corpus organized global topics words speciﬁc clusters mapped local topics. kmeans+mgctm learns local topics based cluster labels obtained kmeans. suboptimal clustering performance k-means documents similar semantics diﬀerent clusters dissimilar documents cluster. consequently learned local topics less reasonable since resulted poor cluster labels. lack mechanism diﬀerentiate corpus-level background words cluster-speciﬁc words types words mixed many topics making topics hard interpret less coherent. measure relevance local topics clusters method learned local topics cluster students pick relevant ones. relevance measure deﬁned ratio number relevant topics table presents relevance measure local relevance measure method kmeans+mgctm ctm. suboptimal performance kmeans+mgctm still results poor clustering performance k-means. comparison kmeans+mgctm mgctm table table demonstrates jointly performing clustering modeling produce better local global topics performing separately. topics shared across groups. certain topic used model documents belonging several groups. consequently composition words multiple groups making hard associate certain group clearly. contrary model allocates cluster cluster-speciﬁc topics prohibit local topics cluster model documents another cluster. thereby relevance learned local topics clusters improved greatly. propose multi-grain clustering topic model simultaneously perform document clustering modeling. experiments datasets demonstrate fact tasks closely related mutually promote other. experiments document clustering show topic modeling clustering performance improved. experiments topic modeling demonstrate clustering help infer coherent topics diﬀerentiate topics group-speciﬁc ones group-independent ones.", "year": 2013}