{"title": "Grounded Recurrent Neural Networks", "tag": ["stat.ML", "cs.CL", "cs.LG", "cs.NE"], "abstract": "In this work, we present the Grounded Recurrent Neural Network (GRNN), a recurrent neural network architecture for multi-label prediction which explicitly ties labels to specific dimensions of the recurrent hidden state (we call this process \"grounding\"). The approach is particularly well-suited for extracting large numbers of concepts from text. We apply the new model to address an important problem in healthcare of understanding what medical concepts are discussed in clinical text. Using a publicly available dataset derived from Intensive Care Units, we learn to label a patient's diagnoses and procedures from their discharge summary. Our evaluation shows a clear advantage to using our proposed architecture over a variety of strong baselines.", "text": "work present grounded recurrent neural network recurrent neural network architecture multi-label prediction explicitly ties labels speciﬁc dimensions recurrent hidden state approach particularly well-suited extracting large numbers concepts text. apply model address important problem healthcare understanding medical concepts discussed clinical text. using publicly available dataset derived intensive care units learn label patient’s diagnoses procedures discharge summary. evaluation shows clear advantage using proposed architecture variety strong baselines. ability recurrent neural networks model sequential data capture long-term dependencies makes powerful tools natural language processing. models maintain state time step representing relevant history task-speciﬁc beliefs. based current value recurrent state input state updated time step. recurrent models become popular choice variety natural language processing tasks language modeling text classiﬁcation machine translation success paradigm driven great part number structural innovations since original version elman recurrent cells long short term memory gated recurrent units example alleviate problem vanishing gradients attention mechanisms memory networks also signiﬁcantly increased expressiveness recurrent architectures revealing potential tackle complex tasks question answering notable property models however often require signiﬁcant amounts training data perform best limit application domain. work focus developing recurrent models task extracting medical concepts intensive care unit discharge summaries. multi-class multi-label text classiﬁcation task target vocabulary several thousand concepts. given difﬁculty obtain large medical datasets need come data-efﬁcient architectures. introduce grounded recurrent neural network high level ground model’s hidden state introducing dimensions whose sole purpose track model’s belief presence speciﬁc labels current example. addition aids optimization outperforms standard recurrent models text classiﬁcation. although label adds hidden state size impose semi diagonal constraint recurrent transition matrices size model grows linearly number labels. show lets model scale number labels also helps optimization. furthermore approach leads increased interpretability especially appreciated medical applications. indeed even though provide model location phrases interest label training time track changes grounded dimensions tied speciﬁc concept document read indicating evidence text presence model’s belief changes drastically. evaluate model publicly available mimic datasets predict codes given patient’s discharge summary text codes usually determined humans perusing health records selecting relevant codes long lists. high number codes signiﬁcant human error arising cognitive load task differences human judgment effort needed errors coding process inconsistency labeling mitigated automatically detecting concepts text offering suggestions smarter auto-complete motivates contribution. also show model’s performance prediction dataset built stackoverﬂow data. section presents relevant previous work section describes model present experimental results section section concludes outlines possible future research directions. entity tracking idea keep information relating speciﬁc concepts dedicated cells inspired part work henaff recurrent entity networks. propose deﬁne full entity interest tasked keeping track relevant information story read model able answer questions state world consulting ﬁnal hidden states. approach unfortunately impractical case presents thousands concepts interest. instead dimension recurrent hidden state track speciﬁc piece information likelihood present current example. sparse recurrent units subakan smaragdis diagonal recurrent transition matrices music modeling show model outperforms full transition matrices. upper-left block transition matrix model diagonal. narang explored sparsity recurrent models showed sparse weight matrices result much smaller models without signiﬁcant loss accuracy. weight sparsity introduced model semi diagonal weight matrix additionally limits over-ﬁtting discussed section interpretable rnns propose method encouraging interpretability learning objective selecting predictive phrases ﬁrst step. additionally lample discuss recurrent models followed pairwise conditional random ﬁeld named entity recognition. lstm+crf model used predict word tags determine span word belongs entity span represents. also present transition-based chunking model uses stacked lstm words pushed onto stack popped entity label. medical concept extraction joshi non-negative matrix factorization simultaneous phenotyping co-occurring medical conditions clinical text. obtain identiﬁable sparse latent factors grounding one-to-one mapping ﬁxed chronic conditions. model grounds recurrent hidden state dimensions one-to-one mapping labels task. automatic coding explored clinical machine learning literature perotte work presents tree structured support vector machine takes advantage hierarchical nature codes outperform baseline. section show derive grounded recurrent neural network architecture given label note decide build version grnn gated recurrent unit similar ideas applied grounding types recurrence functions elman lstm unit. sequence labeling grus labels interest. consider text sequence label assignment sequence denoted }|l|. ease exposition identify words embeddings dimension task sequence labeling consists predicting given recurrent neural network obtain vector representation text dimension compute likelihood label present given representation. speciﬁcally recurrent model starts representation updates timestep using recurrence function obtain global sequence representation case gated recurrent unit build upon work recurrence function parameterized matrices computed follows. denote concatenation vectors element-wise product. then given ﬁnal text representation obtain prediction vector applying afﬁne transformation followed sigmoid function parameterized matrix bias vector model parameters learned minimizing expected binary cross-entropy loss pair denoted grounded dimensions high level approach corresponds model summarize relevant information text sequence ﬁnal recurrent state label corresponds different sub-space. presents several challenges. hand dimension recurrent space much smaller label space size model capacity might small store required information labels target set. hand large recurrent space making model expressive make prone over-ﬁtting rendering optimization difﬁcult training data limited. addition even though grus lstms better standard elman units modeling long term dependencies still challenging maintain relevant information beginning longer sequences goal work alleviate aforementioned problems adding grounded dimensions model’s recurrent space. split recurrent state grounded dimensions control dimensions time step value stored grounded dimension corresponds model’s current belief since construction label predictions simply obtained scaling shifting ﬁnal grounded state however also found useful bias term allow grounded dimensions centered regardless corresponding label frequency data. rescaling keeping notations introduced previous paragraph model predictions given figure illustrates process model reads medical note reading phrase patient went model increases belief person diagnosed atrial fibrillation. formulation already presents advantages. example dedicated dimensions make learning long-term dynamics easier. however simply applying update complete recurrent state time step yields model large capacity which stated previously make optimization difﬁcult limited data. address issue next paragraph. semi diagonal updates given liberty grounded dimensions without constraints model choose store information label-speciﬁc beliefs especially case dimensions corresponding rarer concepts. avoid potential issue propose restrict model dynamics making matrices equations semi-diagonal illustrated figure architecture value updated timestep based solely previous state current input control state made responsible modeling correlations. moreover number parameters model computational cost time step semi diagonal transitions grows linearly size label space allows learning remain tractable even large. bidirectional grnn finally many tasks common bidirectional recurrent models outperform unidirectional recurrent models allow context future taken consideration timestep along history. extend grnn bidirectional setting running standard reverse direction document concatenating outputs inputs grnn allowing modiﬁcations grounded dimensions based future context well past. section presented grounded recurrent neural network recurrent architecture designed signiﬁcantly higher capacity comparable models making optimization easier improving interpretability. section analyzes model’s behavior real world data demonstrates properties experimentally. datasets evaluate model task multi-label classiﬁcation three datasets versions mimic medical dataset stack overﬂow questions. mimic-ii dataset intensive care unit medical records. patient admission ends free text discharge summary describing patient’s stay diagnoses procedures performed. here consider problem predicting diagnosis codes text learning tags manually provided humans going admission records. follow perotte extending label also consider parents gold label codes hierarchy yields total vocabulary codes labels labels note average. split data training test sets perotte split training training validation. gives training validation testing notes. average sequence length maximum length truncated mimic-iii updated version mimic-ii dataset thousands patients admissions. dataset predict diagnosis well procedure codes. consider frequent diagnoses procedures label space giving diagnoses procedures note average. used training notes notes validation testing respectively. average length mimic-iii discharge summaries stack overﬂow website features large number computer programming questions answers. every question stack overﬂow tags deﬁned asker. used subset stack overﬂow data downloaded kaggle website evaluate grnn task predicting tags question text. pre-process data remove code blocks questions select questions words left. gave training validation testing samples. chose frequent tags label tags sample average. average sentence length much shorter words truncate maximum length baselines ﬁrst compare grnn bag-of-words baseline independent binary classiﬁers trained l-regularized logistic regression label early stopping based validation loss. regularization parameters tuned independently label using validation sets. considering recent advances attention-based models also devise neural bag-of-words approach uses soft attention words note adds embeddings. neural approaches used word embedding size attention scores words based local neighborhoods words shared labels. method ensures signiﬁcant words considered prediction labels. note representation computed weighted embeddings obtain predictions learn model parameters described section baseline deﬁned section hidden size either larger dimension corresponding number parameters corresponding grnn allows test whether difference model’s performance grounding simply comes increased capacity. also consider bidirectional grus dimension hidden state. furthermore mimic-ii dataset compare results perotte uses hierarchical task. example hierarchical node decision code trained parent code positive child code evaluated parent classiﬁed positive testing. predicts leaf codes independently builds extended predictions according hierarchy. unlike perotte learn predict entire extended label without considering hierarchy letting learning algorithm infer relevant label correlations. however also note authors baseline models could obtained better results ﬁne-tuning regularization parameters. comparisons early experiments grounded models without semi diagonal constraint recurrent transition matrices found models failed generalize large label spaces over-ﬁtting. also tried grounded recurrent models without control dimensions always performed worse since deny model capacity track history beyond current beliefs labels. addition investigated variant bidirectional closer formulation bidirectional grnn wherein outputs reverse concatenated inputs forward found always performed worse standard bidirectional described earlier. finally entity network mimic-iii text entity rnns. keys entities learned globally enabling network learn clusters related labels entity tracking cluster. predictions label performed attention entity network blocks determine value enabling label focus cluster part network took several days train performing similar worse baselines. curriculum learning maintain invariance grounded dimensions representing likelihoods labels intermediate timesteps train model truncated documents. convenient start small sentence lengths increase maximum document length training progresses. smaller lengths helps learn overall statistics labels early evidence text. maximum document length increased model learns attribute labels evidence text appears later since never back smaller length enables model ﬁne-tune predictions useful information becomes available. also view strategy curriculum learning enabling model perform well long documents initially learning shorter documents. initial document length increased factor every training epoch. initial experiments without curriculum learning took much longer train performed worse models trained curriculum learning. found approach better results trained faster recurrent models thus decided strategy baselines well. quantitative evaluation tables report quantitative results model baselines mimic-ii mimic-iii stackoverﬂow data respectively. report measure area precision/recall curves. micro-averaged versions measures correspond considering pair independent prediction either true false computing statistics together. obtain macro-averaged measures statistics computed independently label averaged uniformly regardless label frequency data. compared micro-averaged versions macro-averaging puts much weight model’s ability accurately predict rare labels. also consider model’s performance actual health care applications. given speciﬁc requirements domain successful human-in-the-loop strategy consists using model scores show user highest scored labels highly multi-class application scores improve auto-complete system jernite greenbaum case important know many proposals correct indeed precision signiﬁcantly hurt user’s conﬁdence system. additionally want know many example’s labels covered proposed predictions provide measures tables show grnn performs signiﬁcantly better medical data measures combined. greater mimic-ii agrees intuition mimic-ii less data makes data efﬁcient model important target label space hierarchical structure makes able take advantage correlations useful. particular advantage grounded architecture noticeable precision recall measures correspond proposed case. finally give results stackoverﬂow data table show model also performs well different domain setting dataset much larger individual example text sequences signiﬁcantly shorter. grounded architectures best baselines still consistently out-perform measures. model introspection figure provides insight properties model architecture. left plot shows grnn outperforms baselines regardless label frequency. note logistic curve close grounded architectures corresponds similar macro-averaged shown table compared models grnn advantage concepts middle frequency spectrum labels appear dozen hundred times training data. also investigate model’s data efﬁciency training grnn subsets mimic-iii increasing size using data. grnn always outperforms standard larger less training data model access implies grounding indeed allow recurrent neural network learn less data. interpretable predictions also want model provide interpretable predictions. indeed better interpretability model decisions important improved quantitative performances medical setting practitioners need able trust system easily query decision process predictions surprising them. noted obtain limited insight decision process attention-based baseline instance looking global scores. however grnn actually track model’s belief presence speciﬁc label note read. mentioned section grnn simply needs look evolution corresponding grounded dimension possible obtain similar information applying projection deﬁned equation time step gives value figure left per-label function label frequency mimic-ii. labels ordered least frequent. right comparing performance grnn varying amount training data available. figure presents visualization extracts discharge summaries outlining time steps either increase decrease model’s belief presence concept. cases grounded architecture provides sharper interpretable signal focusing clinically meaningful passages become increasingly coagulopathic coagulation defect diagnosis). hand gru’s belief increase somewhat phrases effect similar distantly related phrases well seem especially relevant work introduce grounded recurrent neural network recurrent network architecture learns perform multi-label text classiﬁcation data efﬁcient tying concepts interest speciﬁc dimensions hidden state. time structural constraints recurrence matrices allow model remain tractable even presence large number labels. thus model able combine data efﬁciency simple bag-of-word text classiﬁcation methods rnn’s ability model linguistic structures tying labels interest speciﬁc dimensions hidden state. show model especially suited medical setting ability learn limited data model concept correlations provide interpretable predictions lead better performance improved trustworthiness practitioners several strong baselines. also demonstrate network’s ability match outperform baselines even case data efﬁciency less crucial. hope improve model future work introducing structured prediction objectives take better advantage proposed architecture’s ability represent interactions concepts. yacine jernite david sontag gratefully acknowledge support defense advanced research projects agency probabilistic programming advancing machine learning program force research laboratory prime contract fa--c-. opinions ﬁndings conclusions recommendations expressed material author necessarily reﬂect view darpa afrl government. merrienboer bahdanau bengio. properties neural machine translation encoder-decoder approaches. emnlp eighth workshop syntax semantics structure statistical translation doha qatar october pages merriënboer gülçehre bahdanau bougares schwenk bengio. learning phrase representations using encoder–decoder statistical machine translation. proceedings conference empirical methods natural language processing oct. greenbaum jernite halpern calder nathanson sontag horng. contextual autocomplete novel user interface using machine learning improve ontology usage structured data capture presenting problems emergency department. biorxiv page jernite halpern horng sontag. predicting chief complaints triage time emergency department. nips workshop machine learning clinical data analysis healthcare perotte pivovarov natarajan weiskopf wood elhadad. diagnosis code assignment models evaluation metrics. journal american medical informatics association pestian brew matykiewicz hovermale johnson cohen duch. shared task involving multi-label classiﬁcation clinical free text. bionlp biological translational clinical language processing bionlp saeed villarroel reisner clifford l.-w. lehman moody heldt kyaw moody mark. multiparameter intelligent monitoring intensive care public-access intensive care unit database. critical care medicine", "year": 2017}