{"title": "Revisiting Simple Neural Networks for Learning Representations of  Knowledge Graphs", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We address the problem of learning vector representations for entities and relations in Knowledge Graphs (KGs) for Knowledge Base Completion (KBC). This problem has received significant attention in the past few years and multiple methods have been proposed. Most of the existing methods in the literature use a predefined characteristic scoring function for evaluating the correctness of KG triples. These scoring functions distinguish correct triples (high score) from incorrect ones (low score). However, their performance vary across different datasets. In this work, we demonstrate that a simple neural network based score function can consistently achieve near start-of-the-art performance on multiple datasets. We also quantitatively demonstrate biases in standard benchmark datasets, and highlight the need to perform evaluation spanning various datasets.", "text": "address problem learning vector representations entities relations knowledge graphs knowledge base completion problem received signiﬁcant attention past years multiple methods proposed. existing methods literature predeﬁned characteristic scoring function evaluating correctness triples. scoring functions distinguish correct triples incorrect ones however performance vary across different datasets. work demonstrate simple neural network based score function consistently achieve near start-of-the-art performance multiple datasets. also quantitatively demonstrate biases standard benchmark datasets highlight need perform evaluation spanning various datasets. knowledge graphs nell freebase repositories information stored multi-relational graphs. used many applications information extraction question answering etc. osuch contain world knowledge form relational triples entity connected entity using directed relation example would indicate fact donald trump president usa. although current fairly large containing millions facts tend quite sparse overcome sparsity knowledge base completion link prediction performed infer missing facts existing ones. dimensional vector representations entities relations also called embeddings extensively used problem methods characteristic score function distinguishes correct triples incorrect triples methods scoring functions summarized table standard benchmarks datasets evaluating link prediction kgs. previous research shown datasets suffer inverse relation bias performance datasets largely dependent model’s ability predict inverse relations expense independent relations. fact simple rule-based model exploiting bias shown achieved state-of-the-art performance datasets table score functions well known knowledge graph embedding models. vector embeddings entities relation respectively. represent circular correlation component-wise product real part complex number respectively. overcome shortcoming several variants fbk- wnrr proposed literature. complex hole popular embedding techniques achieve state-of-the-art performance datasets. however observe methods perform well uniformly across datasets mentioned above. suggest using predeﬁned scoring function complex hole might best option achieve competitive results datasets. ideally would prefer model achieves near state-of-the-art performance given dataset. paper demonstrate simple neural network based score function adapt different datasets achieve near state-of-the-art performance multiple datasets. main contributions papers summarized follows. demonstrate er-mlp simple neural network based scoring function ability adapt different datasets achieving near state-of-the-art performance consistently. also consider variant er-mlp-d. several methods proposed learning embeddings. differ entities relations modeled score function used scoring triples loss function used training. example transe uses real vectors representing entities relations rescal uses real vectors entities real matrices relations. translational models initial models embeddings transe models relation translation vectors head entity tail entity given triple pair-wise ranking loss used learning embeddings. following basic idea translation vectors transe many methods improve performance. methods transh transr transa transg etc. multiplicative models hole complex recent methods achieve state-ofthe-art performance link prediction commonly used datasets hole models entities relations real vectors handle asymmetric relations. complex uses complex vectors handle symmetric asymmetric well anti-symmetric relations. methods representatives state-of-the-art experiments. neural models several methods neural networks scoring triples proposed. notable among conv conve r-gcn conv uses internal structure textual relations input convolutional neural network. learns tensor relation knowledge graph. conve uses convolutional neural networks reshaped input vectors scoring triples. r-gcn takes different approach uses graph convolutional networks obtain embeddings graph. distmult used embeddings obtain score. focus simple neural models rather using predeﬁned function score triples learn embeddings scoring function neural network jointly learn scoring function embeddings together dataset. simple feed-forward neural network single hidden layer approximator scoring function given triple. particular er-mlp previously proposed neural network model embedding er-mlp-d variant er-mlp propose. architectures models shown figure d-dimensional embeddings entities respectively. similarly embedding relation whose dimensions er-mlp er-mlp-d respectively shall explain below. er-mlp head relation tail embeddings concatenated input input layer size er-mlp-d concatenated head tail embeddings translated using relation embedding size input size er-mlp-d models single fully connected hidden layer. leads output node taken score given triple denote activation function denote concatenation vectors respectively hidden output layer weight matrices er-mlp. single bias value. equivalent parameters er-mlp-d triple scoring functions er-mlp er-mlp-d respectively given below. consider sigmoid function probability correctness triple. train model assign probability correct triples incorrect triples. positive negative triples label optimize cross-entropy loss given below replaced fer−mlp fer−mlp−d er-mlp er-mlp-d respectively. initialization embeddings uniform initialization range used. xavier initialization used weights. neural network-based models used dropout hidden layer prevent overﬁtting. regularization parameter weight decay chosen based cross validation. chose hidden layer size since size layer determines expressive power model datasets simple relations require smaller number hidden units difﬁcult datasets require higher number achieve optimal performance. relu activation used hidden layer achieve fast convergence. minimize objective function used adam learning rate dimensionality entity relation embeddings equal er-mlp i.e. er-mlp-d cross validated datasets. experiments using tensorﬂow single gpu. achieve maximum utilization batch size larger used previously literature choosing using cross validation. sampling negative triples used bernoulli method described experiments datasets listed table previous work noted benchmark datasets high number redundant reversible relations simple rule-based model exploiting deﬁciencies shown achieved state-of-the-art performance datasets suggests evaluation restricted datasets accurate indication model’s capability. order address issue evaluate model performance datasets summarized table knowledge graph pair relations said inverse relations correct triple implies existence another correct triple vice versa. trivial triple refers existence triple test dataset already present training dataset inverse relations. model learn inverse relations well expense types relations still achieve good performance datasets involving biased relations. undesirable since goal learn effective embeddings highly multi-relational graphs. quantitatively investigated bias various datasets towards inverse relations measuring fraction trivial triples present them. results summarized table using training dataset pair relations tested inversion. identiﬁed inverses triples contained relation appeared inverse triple involving relation. seen table standard benchmark datasets large number trivial triples. contrast four pre-existing datasets literature fbk- wnrr suffer bias. mentioned above perform experiments spanning datasets. chose hole complex comparison state-of-the-art current literature. models re-implemented fair comparison. able achieve better performance hole reported original paper. available results conve r-gcn taken comparison. evaluated models link prediction task results reported table surprisingly linear models complex hole perform better neural models wnrr dataset without trivial triples. behavior related pagerank central nodes different datasets found linear models perform better simpler datasets relation-speciﬁc indegree wordnet. easier optimize able better local minima. neural models show superior performance complex datasets higher relation-speciﬁc indegree. despite effectiveness simple neural model like er-mlp methods haven’t received much attention recent literature. even though er-mlp compared hole rigorous comparison involving diverse datasets missing. results paper address show simple models merit consideration future. work showed current state-of-the-art models achieve uniformly good performance across different datasets current benchmark datasets misleading evaluating model’s ability represent multi-relational graphs. recommend models henceforth evaluated multiple datasets ensure adaptability knowledge graphs different characteristics. also showed neural network single hidden layer learns scoring function together embeddings achieve competitive performance across datasets spite simplicity. future plan identify characteristics datasets determine performance various models. references kurt bollacker colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database structuring human knowledge. proceedings sigmod international conference management data. antoine bordes nicolas usunier alberto garcia-duran jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. advances neural information processing systems. dong evgeniy gabrilovich geremy heitz wilko horn kevin murphy thomas strohmann shaohua zhang. knowledge vault web-scale approach probabilistic knowledge fusion. proceedings sigkdd international conference knowledge discovery data mining. mitchell william cohen estevam hruschka partha pratim talukdar justin betteridge andrew carlson bhavana dalvi mishra matthew gardner bryan kisiel jayant krishnamurthy never ending learning.. aaai. richard socher danqi chen christopher manning andrew reasoning neural tensor networks knowledge base completion. advances neural information processing systems. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research kristina toutanova danqi chen patrick pantel hoifung poon pallavi choudhury michael gamon. representing text joint embedding text knowledge bases.. emnlp vol. robert west evgeniy gabrilovich kevin murphy shaohua rahul gupta dekang lin. knowledge base completion search-based question answering. proceedings international conference world wide web.", "year": 2017}