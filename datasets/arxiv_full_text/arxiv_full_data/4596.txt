{"title": "Planning by Prioritized Sweeping with Small Backups", "tag": ["cs.AI", "cs.LG"], "abstract": "Efficient planning plays a crucial role in model-based reinforcement learning. Traditionally, the main planning operation is a full backup based on the current estimates of the successor states. Consequently, its computation time is proportional to the number of successor states. In this paper, we introduce a new planning backup that uses only the current value of a single successor state and has a computation time independent of the number of successor states. This new backup, which we call a small backup, opens the door to a new class of model-based reinforcement learning methods that exhibit much finer control over their planning process than traditional methods. We empirically demonstrate that this increased flexibility allows for more efficient planning by showing that an implementation of prioritized sweeping based on small backups achieves a substantial performance improvement over classical implementations.", "text": "eﬃcient planning plays crucial role model-based reinforcement learning. traditionally main planning operation full backup based current estimates successor states. consequently computation time proportional number successor states. paper introduce planning backup uses current value single successor state computation time independent number successor states. backup call small backup opens door class model-based reinforcement learning methods exhibit much ﬁner control planning process traditional methods. empirically demonstrate increased ﬂexibility allows eﬃcient planning showing implementation prioritized sweeping based small backups achieves substantial performance improvement classical implementations. reinforcement learning agent seeks optimal control policy sequential decision problem initially unknown environment. environment provides feedback agent’s behavior form reward signal. agent’s goal maximize expected return discounted rewards future timesteps. important performance measure sample eﬃciency refers number environment interactions required obtain good policy. many solution strategies improve policy iteratively improving state-value action-value function provide estimates expected return given policy states state-action pairs respectively. diﬀerent approaches updating value functions exist. terms sample eﬃciency eﬀective approaches estimate environment model using observed samples compute time step value function optimal respect model estimate using planning techniques. popular planning technique used value iteration performs sweeps backups state state-action space value function converged. drawback using computationally expensive making infeasible many practical applications. fortunately eﬃcient approximations obtained limiting number backups performed timestep. effective approximation strategy prioritized sweeping prioritizes backups expected cause large value changes. paper introduces backup enables dramatic improvement eﬃciency prioritized sweeping. estimates updated recomputed redoing backup. alternatively know received signiﬁcant value change might want update indicate value used construct current value updated subtracting value adding value prediction task consists determining value function gives expected return policy followed starting state found making bellman equations state values state following model-based methods samples update estimates transition probabilities reward function ˆrs. estimates iteratively improve estimate performing full backups derived equation control task methods often optimal policy maximizes expected return. policy greedy policy respect optimal action-value function gives expected return taking action state following thereafter. function solution bellman optimality equation action-values trade-oﬀ that general memory required storing estimates associated planning estimates correspond state-value estimates corresponds state state-action estimate serious restriction full model stored already. additional memory required order complexity memory required storage model. core advantage small backups full backups enable ﬁner control planning process. allows eﬀective update strategies resulting improved trade-oﬀs computation time quality approximation solution demonstrate empirically showing prioritized sweeping implementation based small backups yields substantial performance improvement classical implementations addition demonstrate relevance small backups domains severe constraints computation time showing method performs small backup time step equal computation time complexity classical method performs sample backup timestep. since sample backups introduce sampling variance require step-size parameter tuned optimal performance. small backups hand introduce sampling variance allowing parameter-free implementation. empirically demonstrate performance method performs small backup time step similar optimal performance achieved carefully tuning step-size parameter. problems often formalized markov decision processes described tuples consisting states; actions; transition probability state state action taken; e{r|s reward function giving expected reward action taken state discount factor controlling weight future rewards versus immediate reward. actions selected discrete timesteps according policy deﬁnes action selection probability conditioned state. general goal improve policy order increase return section introduce small backup version full backup prediction introduction showed small backup requires storage component values make current value variable. case small value backup component values correspond values successor states. indicate values function value estimate associated algorithm shows pseudo-code general class prediction methods based small backups. surpisngly planning method never explicitly computed saving time memory. note computation time step fully independent number successor states. members class need specify number iterations well strategy selecting state-successor pairs theorem relies relation hold. model gets updated relation longer holds. section discuss restore relation computation-eﬃcient commonly used model estimate full action-value backup. backup computation time complexity eﬃcient implementation obtained storing statevalues besides action-values according maxa backup implemented backup similar form prediction backup. hence make small backup version similar prediction case. theorems control versions theorems prediction case. proven similar prediction theorems. small action-value backup ﬁner-grained version backup performing small backup successor state equivalent performing backup once. principle backup performed small backup eﬃcient since small backups make many small changes. eﬃcient planning obtained backup performed while. small backup common sample backup update state value based current value successor states. addition share computation time complexity eﬀect general smaller full backup. disadvantage sample backup respect small backup introduces sampling variance caused stochastic environment. requires step-size parameter enable averaging successor states small backup introduce sampling variance since implicitly based expectation successor states. hence require tuning step-size parameter optimal performance. second disadvantage sample backup affects perceived distribution action outcomes places restrictions reusing samples. example model-free technique like experience replay stores experience samples order replay later time introduce bias reduces performance samples replayed often others. small backups hold since process learning model independent backups based model. allows small backups combined eﬀective selection strategies like prioritized sweeping. prioritized sweeping makes planning step model-based eﬃcient using heuristic selecting backups favours backups expected cause large value change. priority queue maintained determines values next line receiving backups. main implementations moore atkeson peng williams methods common perform backups call update cycles. adjusting number update cycles performed time step computation time time step controlled. below discuss detail occurs update cycle classical implementations. moore atkeson implementation elements queue states backups full value backups. control full value backup diﬀerent backup instead equivalent performing backup action followed backup hence associated computation time complexity update cycle consists following steps. first state removed queue receives full value backup. state value change caused backup. then predecessor state-action pairs priority computed using peng williams implementation diﬀers moore atkeson implementation backup full value backup. instead backup eﬀect small action-value backup computational complexity cheaper backup full backup value change smaller. backup requires state-action-successor triple. hence triples elements queue. predecessors added queue priorities estimate actionvalue change. natural small backup implementation might appear implementation similar peng williams main backup implemented eﬃciently. computational cost small backup however allows much powerful implementation. pseudo-code implementation shown algorithm below discuss characteristics algorithm. computation time small backup comparable priority computation classical implementations. therefore instead computing priority predecessor performing backup element highest priority next update cycle perform small backup predecessors. raises question priority queue type backup perform element. natural priority associated state based change action-value occurred small backups since last value backup. priority assures states large discrepancy state value action-values receive value backup ﬁrst. surprising aspect algorithm function forms essential part small action-value backups. reason speciﬁc backup strategy used algorithm equal state-action pairs successor states hence instead using used saving memory simplifying code. table computation time associated update cycle diﬀerent implementations. indicates number predecessors state-action pairs transition state whose value updated. compare performance performs sample backup time step version algorithm performs small backup time step. speciﬁcally number iterations selected state-successor pair pair corresponding recent transition. performance compared evaluation tasks consisting states laid circle. state transitions occur neighbours. transition probabilities tasks generated random process. speciﬁcally transition probability neighbour state generated random number normalized transition probabilities left right neighbour reward counter-clockwise transitions always reward clockwise transitions diﬀerent tasks. ﬁrst task clockwise transition results reward second task results reward discount factor initial state values time transition observed corresponding backup performed root-mean squared error states determined. average error ﬁrst transitions normalized initial error determines performance. figure shows performance averaged runs. standard error negligible maximum standard error ﬁrst task second task note performance equal performance should deﬁnition. normalized performance since learning occurs case. experiments demonstrate three things. first optimal step-size vary diﬀerent tasks. second selecting sub-optimal step-size cause large performance drops. third small-backup parameter-free performance similar performance optimized step-size. since computational complexity same small backup interesting alternative sample backup domains tight constraints computation time previously sample backups viable. keep mind sample backup require model estimate also tight constraints memory sample backup might still option. compare performance prioritized sweeping small backups classical implementations moore&atkeson peng&williams maze task depicted figure reward received time step discount factor agent take four actions corresponding four compass directions stochastically move agent diﬀerent square. bottom figure shows relative action outcomes ‘north’ action. free space action result possible successor states equal probability. agent close wall number decreases. exploration strategy agent select probability random action instead greedy one. that ‘optimism face uncertainty’ principle also used moore atkeson. means long state-action pair visited least times it’s value deﬁned classical implementations. results also show peng williams method performs considerably worse moore atkeson considered domain. explained different backups perform. eﬀect backup peng williams proportional transition probability cases contrast moore atkeson method performs full backup update cycle. small backup implementation also uses backups proportional transition probability performs backups update cycle. speciﬁcally number proportional number predecessors. general number increase stochasticity domain increases. prioritized sweeping viewed generalization idea replaying experience backward order related eligibility traces techniques common information propagated backwards. whereas backward replay eligibility traces recent trajectory backward performed experiments update cycles time step. figure shows average return ﬁrst episodes diﬀerent methods. results averaged runs. maximum standard deviation methods except method peng williams maximum standard deviation computation time update cycle three diﬀerent implementations small advantage small backup implementation shows computation dominant task. computation time observation value iteration method times high single update cycle. figure average error ﬁrst observations normalized initial error diﬀerent values step-size parameter case constant step-size diﬀerent values decay parameter case decaying step-size. graph corresponds ﬁrst evaluation task; bottom graph second. makes eﬃcient update strategies possible. addition small backups useful domains tight time constraints oﬀering parameterfree alternative sample backups often feasible option domains.", "year": 2013}