{"title": "Kernel machines with two layers and multiple kernel learning", "tag": ["cs.LG", "cs.AI"], "abstract": "In this paper, the framework of kernel machines with two layers is introduced, generalizing classical kernel methods. The new learning methodology provide a formal connection between computational architectures with multiple layers and the theme of kernel learning in standard regularization methods. First, a representer theorem for two-layer networks is presented, showing that finite linear combinations of kernels on each layer are optimal architectures whenever the corresponding functions solve suitable variational problems in reproducing kernel Hilbert spaces (RKHS). The input-output map expressed by these architectures turns out to be equivalent to a suitable single-layer kernel machines in which the kernel function is also learned from the data. Recently, the so-called multiple kernel learning methods have attracted considerable attention in the machine learning literature. In this paper, multiple kernel learning methods are shown to be specific cases of kernel machines with two layers in which the second layer is linear. Finally, a simple and effective multiple kernel learning method called RLS2 (regularized least squares with two layers) is introduced, and his performances on several learning problems are extensively analyzed. An open source MATLAB toolbox to train and validate RLS2 models with a Graphic User Interface is available.", "text": "paper framework kernel machines layers introduced generalizing classical kernel methods. learning methodology provide formal connection computational architectures multiple layers theme kernel learning standard regularization methods. first representer theorem two-layer networks presented showing ﬁnite linear combinations kernels layer optimal architectures whenever corresponding functions solve suitable variational problems reproducing kernel hilbert spaces input-output expressed architectures turns equivalent suitable single-layer kernel machines kernel function also learned data. recently so-called multiple kernel learning methods attracted considerable attention machine learning literature. paper multiple kernel learning methods shown speciﬁc cases kernel machines layers second layer linear. finally simple eﬀective multiple kernel learning method called introduced performances several learning problems extensively analyzed. open source matlab toolbox train validate models graphic user interface available. learning minimizing costs functional spaces proven important approach better understand many estimation problems. indeed functional analytic point view theoretical core many successful learning methodologies smoothing splines gaussian processes support vector machines collectively referred kernel methods. appealing properties kernel methods optimality according variety representer theorems. results usually presented within theory rkhs formalize intuition optimal learning machines trained ﬁnite number data must expressed ∗francesco dinuzzo department mathematics university pavia pavia italy ﬁnite number parameters even hypothesis space inﬁnite dimensional. representer theorems generalized analyzed many forms since ﬁrst appearance recently pointed standard kernel machines somehow limited ability approximate complex functional classes consequence shallow architectures. addition existing representer theorems apply single-layer architectures though extension theory include multi-layer networks would useful better understand behavior current multi-layer architectures characterize methods ﬂexible approximation capabilities. extension also suggested complexity theory circuits well biological motivated learning models ﬁeld kernel methods need complex hypothesis spaces reﬂecting broad prior knowledge idea learning kernel empirical data simultaneously predictor indeed diﬃculty choosing good hypothesis space little available a-priori knowledge signiﬁcantly reduced kernel also learned data. ﬂexibility algorithms implementing framework kernel learning makes also possible address important machine learning issues feature selection learning heterogeneous sources data multiscale approximation. major extension framework classical kernel methods based concept hyper-kernels introduced encompassing many convex kernel learning algorithms. paper connection learning kernel standard kernel machines learning multi-layer architectures analyzed. introduce framework kernel machines layers also encompasses classical single layer kernel methods well many kernel learning algorithms. generic hilbert spaces. problem learning simultaneously layers input-output data pairs indeed section shown representer theorem holds even searched inﬁnite dimensional function spaces optimal learning architectures ﬁnite linear combination kernel functions layer optimality measured according general regularization functionals. remarkably representer theorem also imply that upon training architecture equivalently regarded standard kernel machine kernel function learned data. discussing general result solution representation non-linear layers attention focused case second layer linear. section introduce regularization framework turns equivalent general class methods perform multiple kernel learning simultaneous supervised learning predictor associated kernel convex combination basis kernels. general problem learning kernel receiving attention recent years functional analytic point view pure optimization perspectives. since earlier works many improved optimization schemes proposed section method called based regularized multiple kernel learning square loss function introduces studied. along line recent advances multiple kernel learning shown involved optimization eﬃciently carried using two-step procedure. two-step optimization turns especially simple computationally appealing alternating solution linear system constrained least squares problem. application variety learning problems analyzed section state generalization performances achieved several datasets including multi-class classiﬁcation genomic data. open source matlab toolbox train validate models graphic user interface available http//www.mloss.org. proof theorems lemmas given appendix. here loss functions measuring approximation training data {+∞} extended-valued nondecreasing functions play role regularization presence composition nevertheless still holds theorem existence theorem since existence minimizers hypotheses. shown next sections existence ensured mild additional conditions general hypothesis theorem uniqueness minimizers problem also guaranteed even loss functions strictly convex. notice also theorem admit presence optimal solutions form ﬁnite kernel expansions. however solutions exist projections ﬁnite dimensional span kernel sections optimal well restrict attention kernel machines layers also case. finally strictly increasing holds every optimal solution problem expressed kernel machine layers. theorem shows training architecture layers equivalent train simultaneously single-layer kernel network kernel function equation section shown multiple kernel learning consisting simultaneous learning ﬁnite linear combination kernels associated predictor interpreted speciﬁc instance kernel architecture let’s brieﬂy discuss choice regularizers. first notice convex functions. since convex loss functions linear problem separately convex apparently regularizing equivalent impose constraint lemma shows minimization unitary ball thanks scaling properties coming linearity second layer indicator function introduction additional regularization parameter avoided thus signiﬁcantly reducing complexity model selection. next theorem characterizes optimal solutions problem standard formulations multiple kernel learning problems feature equality instead inequality nevertheless lemma shows always exist optimal solutions problem satisfying equality optimization problems equivalent. might diﬀerent rc). remarkably introduction change variable makes also possible derive addition formulation problem shown convex optimization problem. indeed rewriting problem function following problem obtained lemma completes equivalence speciﬁc kernel machines layers obtained solving problem multiple kernel learning algorithms. lemma also gives another important insight structure problem local minimizers also global minimizers property directly transfer problem change variable plays important role. optimization algorithms linear machines subject renewed attention literature important experimental ﬁndings. first turns linear models already enough ﬂexible achieve state classiﬁcation performances application domains text document classiﬁcation word-sense disambiguation drug design e.g. second linear machines trained using extremely eﬃcient scalable algorithms finally linear methods also used solve certain non-linear problems thus ensuring good trade-oﬀ ﬂexibility computational convenience. algorithms simultaneously perform regularization linear feature selection. property apparently linked introduction additional layer architecture since standard kernel machines layer able perform kind automatic feature selection. user’s point view linear kernel machines layers behave similarly sparse regularization methods lasso performing feature selection varying continuity shrinking parameter. however seems regularization methods cannot interpreted kernel machines classes algorithms thus distinct. instance linear regularization methods layers proposed subsection analyzed experimental section previous section general class convex optimization problems learn ﬁnite linear combinations kernels shown equivalent two-layer kernel machine. standard kernel machines diﬀerent choices loss functions lead variety learning algorithms. instance results previous section follows two-layer version standard support section attention focussed square loss functions associated kernel machine layers. shall show coeﬃcients deﬁning architecture well equivalent inputoutput kernel computed solving simple optimization problem. problem features minimization quartic functional separately quadratic worth noticing square loss function used solve regression problems well classiﬁcation ones. indeed generalization performances regularized least squares classiﬁers shown comparable support vector machines many dataset references therein. along lines recent developments multiple kernel learning optimization propose two-step minimization procedure alternates kernel predictor optimization. speciﬁc structure problem allows exact optimization phases optimization process. optimal coeﬃcients computed using iterative two-step procedure algorithm alternates minimization respect obtained solution linear system solution simplex-constrained least squares problem non-negativity constraint induce sparsity vector thus also input-output kernel expansion. understand initialization coeﬃcients algorithm consider limiting solution optimization problem regularization parameter tends inﬁnity. solution natural starting point regularization path optimal coeﬃcients computed closed form. equivalence regularization problem multiple kernel learning optimization problem readily exploited give bayesian interpretation rls. specify probabilistic model need prior distribution functions deﬁne data generation model following denote real gaussian distribution mean variance gaussian measure functions mean covariance function uniform distribution positive ﬁnite measure. distributed according gaussian measure zero mean covariance function random vector independent distributed according uniform distribution ellipsoid function space gaussian measure prior admit probability density. nevertheless regularization problem still recovered understanding estimate maximal point posterior probability measure described linear regularized least squares layers described subsection input subset linear choice basis kernels produces linear machines feature selection capabilities. first recall standard regularized least squares linear kernel boils ﬁnite-dimensional tikhonov regularization also known ridge regression result lemma also used give interesting interpretation linear rls. fact coeﬃcient sidi interpreted quantity proportional inverse variance i-th coeﬃcient hence problem seen bayesian estimation gaussian residuals gaussian prior coeﬃcients uniform prior suitable simplex vector inverse coeﬃcients’s variances. useful introduce notion degrees freedom e.g. degrees freedom index interpretable regularization parameter also used choose regularization parameter according tuning criteria general expression eﬀective degrees freedom non-linear kernel regression methods layer based sure approximation recently derived linear following quantity seems appropriate approximation degrees freedom regularized estimator regressors ﬁxed diagonal regularization notice neglects non-linear dependence matrix freedom. nevertheless property holds ability select features basis kernels highly inﬂuenced scaling second layer. subsection analyze certain scaling rules connected popular statistical indices often used ﬁlters feature selection since issue scaling still needs investigation excluded rules diﬀerent mentioned subsection work better speciﬁc problems. observation following according lemma heavy regularization favors basis kernels maximizes quantity represents kind alignment kernel outputs. means tends select kernels highly aligned outputs. since alignment proportional scaling factor eﬀective choose scaling makes alignment meaningful quantity maximize. first discuss choice scaling linear algorithm introduces subsection generalization case non-linear basis kernels easily follows analyzing associated feature maps. linear case skxkxkt k-th feature particular outputs features centered zero mean coincides squared pearson correlation coeﬃcient outputs k-th feature also known coeﬃcient determination. means heavy regularization selects features mostly without changing proﬁle solutions along regularization path observe rule also make sense data centered centered around values mean. fact datasets performances better without centering notice also uses training inputs whereas possible variation replace vector containing values k-th feature training test data latter procedure sometimes work better scaling using training inputs only referred choice without centering still make sense rules also possible. denote number samples positive negative class respectively denote within-class mean values k-th feature rules generalized case non-linear basis kernels observing non-linear kernels always seen linear upon mapping data suitable feature space. rule generalizes following ]rakotomamonjy amounts scale basis kernel trace kernel matrix reduces exactly linear case. also applied without centering. typical centering normalization feature space transductive scaling rule obtained extending training test inputs namely computing inverse trace overall kernel matrix finally non-linear generalization given following section behavior linear non-linear several learning problems analyzed. subsection illustrative analysis linear proposed whose goal study feature selection capabilities dependence regularization parameter algorithm simple experimental settings. non-linear kernels analyzed subsection extensive benchmark several regression classiﬁcation problems repository carried out. finally multi-class classiﬁcation microarray data considered subsection computations carried matlab environment sub-problem solved using smo-like algorithm current implementation features conjugate gradient solve linear systems sophisticated variable shrinking technique reduce gradient computations. stopping criterion algorithm used experiments following test normalized residual linear system choice turns suﬃcient make coeﬃcients stabilize good approximation ﬁnal values. full discussion optimization details outside scope paper. experiments core cache ram. subsection perform experiments analyze behavior linear rls. ﬁrst experiment synthetic dataset used investigate ability linear perform feature selection. dependence generalization performances learning algorithms training size analyzed means learning curves. goal second experiment illustrate qualitative dependence coeﬃcients regularization parameter give idea predictive potentiality algorithm. standard deviation outputs depend ﬁrst three bits input binary string. dataset divided training input output pairs test containing remaining data pairs. compare rmse learning curves obtained varying training size using diﬀerent methods figure binary strings data lower bounds rmse learning curves. plot shows test rmse training sizes diﬀerent methods ideal kernel linear kernel gaussian kernel linear lasso. bottom plot zoomed version plot training sizes ideal kernel linear lasso. goal assess overall quality regularization paths associated diﬀerent regularization algorithms independently model selection procedures. compute rmse test data function training size evaluate lower bounds learning curves respect variation regularization parameter. results shown figure whose plot reports lower bounds learning curves algorithms training sizes notice methods able learn asymptotically underlying concept precision limit imposed noise methods exploits coeﬃcients sparsity faster reach asymptotic error rate. surprisingly best method ideal kernel incorporates strong prior knowledge dependence outputs ﬁrst three bits only. though knowing advance optimal features realistic method used reference. slowest learning curve associated gaussian kernel incorporates notion smoothness. good compromise linear kernel uses knowledge linearity underlying function reaches good approximation asymptotic error rate seeing strings. remaining methods incorporate knowledge linearity sparsity. able learn underlying concept seeing examples despite presence noise. since example lasso linear basically equivalent interesting happen small sample sizes. bottom plot figure zoomed version plot training sizes showing learning curves three methods impose sparsity. example lasso learning curve stays lower learning curve. example learning curve stays uniformly lower lasso indicating high eﬃciency learning noisy sparse linear combinations. since multiple kernel learning interpretation suggests algorithm learning ideal kernel simultaneously predictor might interesting analyze asymptotic values kernel coeﬃcients indeed ﬁrst training examples sets zero coeﬃcients except ﬁrst three approximately equal experiment linear applied prostate cancer dataset regression problem whose goal predict level prostate-speciﬁc antigen basis number clinical measures receive radical prostatectomy data figure prostate cancer data -fold cross-validation prediction error curves standard errors bands estimated linear rls. model complexity increases right left. vertical line corresponds least complex model within standard error best. table prostate cancer data comparison subset selection shrinkage methods. estimated coeﬃcients test error standard error reported. results methods taken blank entries corresponds variables selected. ridge lasso used textbook compare diﬀerent feature selection shrinkage methods obtained site http//www-stat. stanford.edu/elemstatlearn/. data preprocessed normalizing inputs zero mean unit standard deviation. dataset divided training examples test examples. choose regularization parameter -fold cross-validation score comscaling coeﬃcients chosen thus normalizing training feature unit norm. intercept term equal average training outputs subtracted outputs estimating coeﬃcients. dataset splits computed validation data. figure reports average standard error bands validation along regularization path. following pick value corresponding least complex model within standard error best validation score. second phase whole training used compute solution diﬀerent values figure reports proﬁle coeﬃcients equation along whole regularization path function degrees freedom deﬁned continuous feature selection resemble lasso. however dependence coeﬃcients regularization parameter rather complex proﬁle figure piecewise linear. correspondence value chosen validation phase selects input variables table reports value coeﬃcients estimated together test error standard error. comparison table also reports models results taken associated best subset regression ridge regression lasso regression best model data achieves second lowest test error using variables. subsection benchmark experiments four regression classiﬁcation problems repository illustrated random dataset splits diﬀerent training/test ratios dataset split approximate regularization path speed-up regularization path computation warm-start technique employed value iteratively decreased kernel-expansion coeﬃcients initialized optimal values obtained previous value performances measured accuracy classiﬁcation rmse regression. dataset split value regularization parameter following quantities computed prediction performance test number selected kernels training time seconds number iterations compute whole regularization table regression classiﬁcation number selected kernels correspondence optimal value number iterations training time seconds compute regularization path. kernels path. datasets pre-processed removing examples missing features converting categorical features binary indicators. datasets input features standardized zero mean equal mean training outputs subtracted training data. basis kernel matrices pre-computed scaling matrix chosen according rule transductive scaling. better compare results similar benchmarks multiple kernel learning e.g. basis kernels datasets chosen. remark agnostic approach representative proﬁles test prediction performance number kernels number iterations dataset split correspondence diﬀerent values regularization parameter reported figures plots seen test performances relatively stable variations regularization parameter around optimal value indicating robust respect diﬀerent model selection procedures. regression datasets servo housing optimal performances seems reached correspondence un-regularized solution lines light color associated single dataset splits thick lines averages diﬀerent dataset splits. vertical dotted line corresponds value regularization parameter best average test performance. average number selected kernels vary quite smoothly respect regularization parameter. large values chooses basis kernel. small values number selected kernels grows exhibits higher variability. bottom plots figures give idea computation burden required alternate optimization correspondence diﬀerent values correspondence high values regularization parameter algorithm converges single iteration. occurs also ﬁrst value regularization path meaning initialization rule eﬀective. values also converges single iteration since second layer doesn’t change much iteration next. test performances regression classiﬁcation summarized table average standard deviation respect dataset splits either rmse accuracy correspondence best value reported. performances kernel learning algorithms datasets found references therein. another benchmark study might useful comparison comparisons handled care diﬀerent experimental procedures optimization problems. instance uses dataset split ratio uses uses also diﬀerent numbers dataset splits used. individuating kind datasets better suited algorithm complex issue certainly worth further investigation. experiments shows results competitive complexity model well controlled regularization. particular state performances reached servo housing hearth pima ionosphere. finally observed that although multiple kernel learning machines used black methods basis kernels single features sometimes also selects subset relevant features. property remarkable since standard single-layer kernel methods able perform embedded feature selection. table reports average standard deviation number selected kernels correspondence optimal value number iterations training time needed compute regularization path regression classiﬁcation datasets studied subsection. columns selected kernels seen considerable fraction overall number basis kernels ﬁltered algorithm correspondence optimal value regularization parameter. looking number iterations needed compute path values regularization parameter average number iterations compute solution single value indicating warm-start procedure rather eﬀective exploiting continuity solutions respect regularization parameter. matter fact optimization work spent correspondence central interval values shown bottom plots figures finally last column reporting average standard deviation training times seen that current implementation regularization paths datasets subsection computed less minute average although current implementation designed eﬃcient believed there’s still considerable margin computational improvements. issue well subject future developments. figure cancers data proﬁles training error -fold validation error test error diﬀerent values regularization parameter. standard error bands validation error also shown. vertical line corresponds least complex model maximizing validation accuracy. applied multi-class classiﬁcation problems solving several binary classiﬁcation problems combining outcomes. possible combine binary classiﬁers approach class compared others test labels assigned class maximizing conﬁdence corresponding binary classiﬁer. linear applied cancers dataset delicate multi-class classiﬁcation problem whose goal discriminate diﬀerent types cancer basis microarray measurements gene expressions. gene measurements type cancer available patients dataset already divided training patients test patients. another important goal problem individuate small subset genes relevant discriminate diﬀerent kind cancer. reports several results data using variety classiﬁcation methods. algorithms support vector classiﬁer uses genes compute classiﬁcation boundaries others table cancers data average test error number selected genes diﬀerent classiﬁcation algorithms. number selected genes relative least complex model maximizing validation accuracy. results methods taken lasso elastic also able select subset relevant genes. since feature selection experiment subsection suggests eﬃcient selecting relevant features noisy examples microarray dataset seems appropriate choice testing algorithm. gene expressions patient ﬁrstly standardized zero mean variance one. binary classiﬁer coeﬃcients chosen within-class sample variances computed using training data. scaling gives weight genes whose expressions exhibits small within-class variability seems slightly improve classiﬁcation performances. validation accuracy proﬁle computed using stratiﬁed -fold cross validation folds organized approximately preserve class proportions. ﬁnal model pick highest value maximizing validation accuracy. figure reports proﬁles training accuracy cross-validation accuracy corresponding standard error bands test accuracy logarithmically spaced values regularization parameter. table reports number test errors selected genes correspondence value chosen validation phase methods test errors table averages test errors diﬀerent classiﬁers associated diﬀerent values regularization parameter maximizes cross-validation score linear procedure yields value correspondence least complex model maximizing cross-validation accuracy obtain test errors using genes. although test size small draw signiﬁcative conclusions comparison linear seems work rather well problem achieve best test performances. good performance also conﬁrm eﬀectiveness multi-class approach rls. connection learning two-layer network problem learning kernel analyzed. architectures layer justiﬁed representer theorem alternative perspective look problem kernel learning proposed. perspective makes clear methodologies increasing approximation power standard single layer methods using machines adaptively select functions variety shapes little prior knowledge available. particular multiple kernel learning framework shown important speciﬁc case general two-layer architecture. also introduce method perform multiple kernel learning based regularization square loss function alternate optimization. exhibits state performances several learning problems including multi-class classiﬁcation microarray data. open source matlab scripts linear available http//www.mloss.org also includes graphic user interface. problem speciﬁc instance problem functional minimize bounded below lower semi-continuous radially-unbounded respect existence minimizers follows weak-compactness unit ball subject matrices. problem seen jointly convex using argument term matrix-fractional function example jointly convex function pair easily follows noticing epi-graph convex since convex function overall functional convex. minimization subject linear constraints thus convex optimization problem. since linear function problem also convex. lemma minimization respect restricted standard simplex ﬁxed functional problem convex quadratic function satisfy equation gradient objective functional respect zero meaning optimal. dropping dependence equation rewritten", "year": 2010}