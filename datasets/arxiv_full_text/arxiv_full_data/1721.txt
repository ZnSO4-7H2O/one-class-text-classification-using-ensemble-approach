{"title": "A random forest system combination approach for error detection in  digital dictionaries", "tag": ["cs.CL", "cs.LG", "stat.ML", "I.2.7; I.2.6; I.5.1; I.5.4"], "abstract": "When digitizing a print bilingual dictionary, whether via optical character recognition or manual entry, it is inevitable that errors are introduced into the electronic version that is created. We investigate automating the process of detecting errors in an XML representation of a digitized print dictionary using a hybrid approach that combines rule-based, feature-based, and language model-based methods. We investigate combining methods and show that using random forests is a promising approach. We find that in isolation, unsupervised methods rival the performance of supervised methods. Random forests typically require training data so we investigate how we can apply random forests to combine individual base methods that are themselves unsupervised without requiring large amounts of training data. Experiments reveal empirically that a relatively small amount of data is sufficient and can potentially be further reduced through specific selection criteria.", "text": "turfah instead usage note frequency third sense rare. figure shows tree corresponding representation. corrected digital representation depicted figure corresponding corrected tree shown figure zajic presented method repairing digital dictionary format using dictionary markup language called dml. remains time-consuming error-prone however human read manually correct digital version dictionary even languages available. therefore investigate automating detection errors. digitizing print bilingual dictionary whether optical character recognition manual entry inevitable errors introduced electronic version created. investigate automating process detecting errors representation digitized print dictionary using hybrid approach combines rulebased feature-based language modelbased methods. investigate combining methods show using random forests promising approach. isolation unsupervised methods rival performance supervised methods. random forests typically require training data investigate apply random forests combine individual base methods unsupervised without requiring large amounts training data. experiments reveal empirically relatively small amount data sufﬁcient potentially reduced speciﬁc selection criteria. digital versions bilingual dictionaries often errors need ﬁxed. example figures show example error occurred development dictionaries error corrected. figure shows entry word turfah appeared original print copy word three senses slightly different meanings. third sense rare. original digitized version depicted figure misrepresented meaning paper published within proceedings workshop innovative hybrid approaches processing textual data pages avignon france april association computational linguistics method replicates method presented third simple rule inference method. three individual methods different performances. investigate combine methods effectively. experiment majority vote score combination random forest methods random forest combinations work best. many dictionaries training data available large quantities priori therefore methods require small amounts training data desirable. interestingly automatically detecting errors dictionaries unsupervised methods performance rivals supervised feature-based method trained using svms. moreover combine methods using random forest method combination unsupervised methods works better supervised method isolation alwell combination available methods. potential drawback using random forest combination method however requires training data. investigated much training data needed amount training data required modest. furthermore selecting training data labeled speciﬁc selection methods reminiscent active learning possible train random forest system combination method even less data without sacriﬁcing performance. section discuss previous related work section explain three individual methods application. section explain three methods explored combining methods; section present discuss experimental results section conclude discuss future work. classiﬁer combination techniques broadly classiﬁed categories mathematical behavioral ﬁrst category functions rules combine normalized classiﬁer scores individual classiﬁers. examples techniques category include majority voting well simple score combination rules rule rule rule product rule second category output individual classiﬁers combined form feature vector random forest method described ensemble classiﬁer consisting collection decision trees output random forest mode classes output individual trees. single tree trained follows random samples initial training selected training node tree random subset features selected locally optimal split based feature subset. tree fully grown without pruning. used random forests combining scores several biometric devices identity veriﬁcation shown encouraging results. fully supervised methods. contrast explore minimizing amount training data needed train random forest unsupervised methods. active learning order reduce training data requirements without sacriﬁcing model performance reported extensively literature training random forest combination individual methods unsupervised explore select data small amounts training data needed many dictionaries gathering training data expensive labor-intensive. first supervised approach train model using svmlight linear kernel default regularization parameters. depth ﬁrst traversal tree unigrams bigrams tags occur features subtree make classiﬁcation decision. ﬁrst unsupervised approach learns rules classify nodes errors not. rulebased method computes anomaly score based probability subtree structures. given structure probability event occurs anomaly score event occur anomaly score basic idea certain structure happens rarely i.e. small occurrence high anomaly score. hand occurs frequently absence indicates anomaly. obtain anomaly score tree simply take maximal scores events induced subtrees within tree. second unsupervised approach uses reimplementation language modeling method described brieﬂy methods works calculating probability ﬂattened branch occur given probability model trained branches original dictionary. used generate bigram models using good turing smoothing katz back evaluated probability branches ranking likelihood. ﬁrst branches submitted hybrid system marked error remaining submitted non-error. results individual classiﬁers presented section investigate three methods combining three individual methods. baseline investigate simple majority vote. method takes classiﬁcation decisions three methods assigns ﬁnal classiﬁcation classiﬁcation majority methods predicted. drawback majority vote weight votes all. however might make sense weight votes according factors strength classiﬁcation score. example classiﬁers make binary decisions output scores indicative conﬁdence classiﬁcations. therefore also explore score combination method considers scores. since measures different systems different ranges normalize measurements combining z-score computes arithmetic mean standard deviation given data score normalization. take summation normalized measures ﬁnal measure. classiﬁcation performed thresholding ﬁnal measure. another approach would weight performance level various constituent classiﬁers ensemble. weighting based performance level individual classiﬁers difﬁcult would require extra labeled data estimate various performance levels. clear translate different performance estimates weights weights interact weights based strengths classiﬁcation. therefore weigh based performance level explicitly. believe third combination method implicitly captures weighting based performance level strengths classiﬁcations. random forest approach uses three features individual systems use. random forests strengths classiﬁcation taken account form values three features use. addition performance level taken account training data used train decision trees form forest help guide binning feature values appropriate ranges classiﬁcation decisions made correctly. discussed section section explains details experiments conducted testing performance various individual combined systems. subsection explains details data experiment subsection provides summary main results experiments; subsection discusses results. experimental setup obtained data experiments using digitized version urdu-english dictionary zajic used. zajic presented programming language used errors documents contain lexicographic data. team language experts used correct errors digital representation kitabistan urdu dictionary. current research compared source document commands identify elements language experts decided modify. consider elements errors. ground truth used training evaluation. evaluate tiers corresponding node types representation dictionary entry sense. example depicted figures shows example sense. intuition tier errors detectable observing elements within tier cross tier boundaries. tiers speciﬁc kitabistan urdu dictionary selected observing data. limitation work know time whether generally useful across dictionaries. future work automatically discover meaningful evaluation tiers dictionary. process dataset entries marked errors senses marked errors. perform tenfold cross-validation experiments. random forest experiments decision trees feature. tables show performance language modeling-based method rule-based method supervised feature-based method different tiers. seen entry tier rule obtains highest fmeasure accuracy sense tier performs best. section show applying random forests output individual systems gains accuracy gains f-measure tables show experimental results entry sense tiers applying random forests rule-based method. results obtained iterations experiments different partitions training data chosen iteration. mean values different evaluation measures standard deviations shown tables. change percentage training data repeat experiments amount training data affects performance. might surprising gains performance achieved using random forest decision trees created using rule-based scores features. shed light show distribution rule-based output scores anomaly nodes clean nodes figure well separated explains rule alone good performance. recall rule classiﬁes nodes anomaly scores larger errors. however figure many clean nodes anomaly scores larger thus simple thresholding strategy bring errors. applying random forest help identify errorful regions improve performance. another method helping identify errorful regions classify correctly apply random forest rule combined methods even boost performance. section explore different methods combining measures three systems. table shows results majority voting score combination entry tier. seen majority voting performs poorly. fact performances three systems different. rule signiﬁcantly outperforms systems discussed section neither majority voting score combination weights higher performance appropriately. tables show results combining rule particular interest since systems unsupervised. combining unsupervised systems works better individual methods including supervised methods. tables show results combinations available systems. yields highest performance slightly higher combination unsupervised base methods. random forest combination technique require labeled data even underlying base methods unsupervised. based observation figure study whether choosing training data errorful regions help improve performance. experimental results table show choice training data affects performance. appears weak trend toward higher performance force selection majority training data entry nodes whose rule anomaly scores larger however magnitudes observed differences performance within single standard deviation remains future work determine ways select training data random forest combination ways substantially improve upon random selection. majority voting performs poorly since performance three individual systems different majority voting weight votes all. score combination type weighted voting. takes account conﬁdence level output different systems enables perform better majority voting. however score combination take account performance levels different systems believe limits performance compared random forest combinations. random forest combinations perform best cost supervised combination method. investigated amount training data affects performance found small amount labeled data random forest needs order successful. moreover although requires exploration weak evidence size labeled data potentially reduced choosing carefully region expected errorful. application rule-based system high-anomaly scoring region although true anomalies often errors examination examples marked errors ground truth detected errors systems suggests examples decided basis features considered system. example figure second form well-formed structurally urdu text ﬁrst form beginning phrase transliterated second form. automatic systems detected ﬁrst form error however mark second form error whereas ground truth marked errors. examination false negatives also revealed cases systems correct error ground truth wrongly indicated error. semi-automated method producing ground truth considers elements mentioned commands errors. discovered instances merely mentioning element command imply element error. cases useful making reﬁnements ground truth generated commands. table effect choice training data based rule based method choose data training ﬁrst column table speciﬁes percentage training data chosen entries anomalous score larger deleted therefore command ever mentioned smaller element lexicographers upon inspection agree smaller element indeed errorful. category actual errors dictionary editors didn’t repair repaired. explored hybrid approaches application automatically detecting errors digitized copies dictionaries. base methods explored consisted variety unsupervised supervised methods. combination methods explored also consisted methods required labeled data not. found base methods different levels performance scenario majority voting score combination methods though appealing since require labeled data perform well since weight votes well. found random forests decision trees best combination method. hypothesize nature task base systems. random forests able help tease apart high-error region drawback random forests combination method require labeled data. however experiments reveal empirically relatively small amount data sufﬁcient amount might able reduced speciﬁc selection criteria. states government. opinions ﬁndings conclusions recommendations expressed material author necessarily reﬂect views university maryland college park and/or agency entity united states government. nothing report intended shall treated construed endorsement recommendation university maryland united states government authors product process service subject report. information contained based report advertisements promotional materials related company product process service support commercial purposes.", "year": 2014}