{"title": "Generative Adversarial Source Separation", "tag": ["cs.SD", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Generative source separation methods such as non-negative matrix factorization (NMF) or auto-encoders, rely on the assumption of an output probability density. Generative Adversarial Networks (GANs) can learn data distributions without needing a parametric assumption on the output density. We show on a speech source separation experiment that, a multi-layer perceptron trained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders trained with maximum likelihood, and variational auto-encoders in terms of source to distortion ratio.", "text": "limited. authors train de-noising networks using adversarial framework. paper propose using gans learn generative model magnitude spectrogram frames used speech source separation task. source separation task goal decompose given signal additive components approximates original sources accurately possible. generative source separation train generative models recover sources observed mixture. experimentally show speech mixtures adversarially trained layer perceptron outperforms ml-trained autoencoders terms source-to-distortion ratio experiments observed original formulation hard train. therefore showed performance improvement standard audio models recent wasserstein-gan formulation latent variable lower dimensionality pforward forward model sources. given sources mixture assumed distributed according conditional distribution pmixture conditioned sources. note assumed parametric forms distributions above. also note that experiments consider case sources although methods discussed generalized sources. give context audio application mixture corresponds column magnitude spectrogram. generative source separation methods non-negative matrix factorization auto-encoders rely assumption output probability density. generative adversarial networks learn data distributions without needing parametric assumption output density. show speech source separation experiment that multilayer perceptron trained wasserstein-gan formulation outperforms auto-encoders trained maximum likelihood variational auto-encoders terms source distortion ratio. many popular audio modeling/source separation algorithms non-negative matrix factorization autoencoders tensor factorization models generative models trained maximum likelihood require speciﬁcation output distributions. instance models different loss functions actually underlying mapping latent space observed space performances typically differ given dataset. output distribution/loss function therefore biases model. generative adversarial networks offer generative model learning framework require speciﬁcation output distribution. gans able learn processes implicitly deﬁned transformation random variable. namely generative process deﬁned random latent variable mapped data domain getting transformed deterministic neural network. removes bias comes assuming parametric output distribution leads accurate modeling distributions. generative source separation approach ﬁrst train forward models pforward pforward source distributions psource psource approximated best possible. given trained models sources testing common approximating source distributions assuming sources generated transforming dimensional latent variable platent non-linear mapping parameters adding noise transformed variable. corresponds following generative model pout output distribution models noise output mapping e.g. modeling spectrograms pout usually taken poisson distribution corresponds unnormalized divergence. under modeling assumptions optimization problem approximating source distribution written follows integral latent variable intractable general case. using variational auto-encoder framework objective expression maximized computing variational lower bound. practice however especially audio modeling integral latent variable computed conditional forward model pforward learnt simultaneously optimizing forward model parameters latent variables. written following optimization problem formulation corresponds widely used non-negative matrix factorization model also possible include latent variable estimation part model autoencoder. results following optimization problem conceptual problem training objectives discussed section picking speciﬁc output distribution sacriﬁcing generality approximated source distributions. remove assumption paper generative adversarial networks neural network framework learning generative models withexplicitly specifying output distribution training generator network. seen previous section maximum likelihood training involves parametric assumption output distribution. alternative paper propose using implicit generative model training require explicit loss function. implicit generative model sources speciﬁed follows source deterministically related latent variable unlike source model previous section. process implies intractable density function pmodel general case complicated nonlinear mapping neural network. learning implicit generative models currently active ﬁeld research attack problem discriminator function aims distinguish between samples generated model training instances. goal training becomes generate samples using process expression that discriminator becomes unable distinguish generated samples training data. described setup known generative adversarial network corresponding minimax game speciﬁed follows expression recognized bernoulli log-likelihoods tries maximize outputting training data outputting generated samples generator however tries minimize expression fooling discriminator. shown assumptions scheme minimizes jensenshannon divergence actual source distribution psource model distribution pmodel. however practice scheme unstable usually suffers mode collapse problem learnt distribution pmodel captures subset actual sample space. unfortunately acceptable source separation application. alternate formulation known wasserstein-gan alleviates mode collapse problem minimizing wasserstein- distance learnt data distributions results smooth gradients authors show experiment set-up follows form mixtures male female speaker utterances corresponding training data timit speech corpus form training/test data pairs randomly pick male female speaker pairs train folder timit corpus. speaker available utterances. speakers available utterances training testing. resulting training source around seconds long. selected test utterances around seconds mixture signal obtained mixing test utterances form mixtures/training sets test algorithm randomly selected sets preprocessing step compute fourier spectrograms utterances. point size learning source separation performed columns magnitude spectrograms. reconstructing separated sources time domain wiener ﬁltering equation division multiplication element wise istft designates inverse short time fourier transform operation time domain signal complex fourier spectrogram. obtain results following models standard gaussian random inputs. wasserstein gaussian random inputs. autoencoding wasserstein instead gaussian random inputs feed training samples generator network. denotes parameters γ-lipschitz continuous algorithm provided paper constraint achieved clipping weights experiments wasserstein gans showed signiﬁcant improvement original formulation. note referred critic formulation. extra beneﬁt training generative models gans that addition generator networks fbθk therefore separation stage score much obtained source looks like instances training set. also noticed using smoothing term enforce smooth ﬁrst difference across time improves quality estimated sources gans maximum likelihood based auto-encoders. therefore optimization separating sources given mixture spectrogram columns becomes following trade-off scalar reconstruction quality discriminator/critic score. experiments ﬁxed also potentially optimized validation set. smoothing term used finally note magnitude spectrograms common poisson distribution pmixture. fig. distributions eval scores speech source separation experiment. acronyms algorithm indicated violin plot. order left right algorithms ordered standard wasserstein gaussian inputs autoencoding wasserstein autoencoder trained maximum likelihood variational autoencoder. violin shows distribution corresponding score different speaker pairs. subplots organized follows left scores middle scores right scores. dimensionality data items since point fft) hidden units. therefore size size autoencoding auto-encoder trained maximum likelihood network architecture generator/forward model exactly same except inputs data items instead random variable used encoder wrelu mean variance terms latent variable size size encoder used size training testing neural network models rmsprop algorithm learning rate training gans iterations discriminator/critic updates generator update. neural network models training iterations test iterations. wasserstein-gan clip critic parameters lower upper limits respectively. report bss-eval scores obtained recovering sources mixture signals. bss-eval scores source distortion ratio source interference ratio source artifacts ratio summary measure good separation speaker pair averaged bss-eval scores recovered sources figure violin plots experiments indicate that wasserstein gaussian noise input outperforms auto-encoder variational auto-encoder terms source distortion ratio. note obtaining results similar underlying networks. models except kept exact generator architecture deﬁned equation also observed standard formulation reliable. although occasionally seen good sdrs observed inspecting outputs able capture variety source spectrogram distribution good wasserstein therefore source separation performance standard good. also experimented training auto-encoder adversarial training seen although less reliable wasserstein gaussian inputs sometimes able give great sdrs. general adversarial methods give great sirs losing especially compared ml-autoencoder. finally note code experiments available https//github.com/ycemsubakan/sourceseparation_misc. paper experimentally shown wasserstein gans obtain good performance generative source separation. addition requiring speciﬁcation output distribution gans source separation task nicely since discriminator/critic functions help source separation. believe exists many research opportunities gans audio domain. natural next step paper extend results showed paper end-to-end generative adversarial audio model. paris smaragdis judith brown non-negative matrix factorization polyphonic music transcription ieee workshop applications signal processing audio acoustics paris smaragdis shrikant venkataramani neural network alternative non-negative audio models ieee international conference acoustics speech signal processing icassp orleans march taylan cemgil umut simsekli yusuf s¨ubakan tensor factorization framework audio modeling ieee workshop applications signal processing audio acoustics waspaa paltz october goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio generative adversarial nets nips emmanuel vincent gribonval cdric fvotte performance measurement blind audio source separation. ieee trans. audio speech language processing vol. paris smaragdis c´edric f´evotte gautham mysore nasser mohammadiha matthew hoffman static dynamic source separation using nonnegative factorizations uniﬁed view ieee signal process. mag. vol.", "year": 2017}