{"title": "A Hybrid Latent Variable Neural Network Model for Item Recommendation", "tag": ["cs.LG", "cs.IR", "cs.NE", "stat.ML"], "abstract": "Collaborative filtering is used to recommend items to a user without requiring a knowledge of the item itself and tends to outperform other techniques. However, collaborative filtering suffers from the cold-start problem, which occurs when an item has not yet been rated or a user has not rated any items. Incorporating additional information, such as item or user descriptions, into collaborative filtering can address the cold-start problem. In this paper, we present a neural network model with latent input variables (latent neural network or LNN) as a hybrid collaborative filtering technique that addresses the cold-start problem. LNN outperforms a broad selection of content-based filters (which make recommendations based on item descriptions) and other hybrid approaches while maintaining the accuracy of state-of-the-art collaborative filtering techniques.", "text": "collaborative ﬁltering used recommend items user without requiring knowledge item tends outperform techniques. however collaborative ﬁltering suffers cold-start problem occurs item rated user rated items. incorporating additional information item user descriptions collaborative ﬁltering address cold-start problem. paper present neural network model latent input variables hybrid collaborative ﬁltering technique addresses cold-start problem. outperforms broad selection content-based ﬁlters hybrid approaches maintaining accuracy state-of-the-art collaborative ﬁltering techniques. modern technology enables users access abundance information. deluge data makes difﬁcult sift desired. problem particular concern companies trying sell products recommend movies lessen severity information overload recommender systems help user looking for. commonly used classes recommender systems content-based ﬁlters collaborative ﬁlters. content-based ﬁlters make recommendations based item/user descriptions users’ ratings items. creating item/user descriptions predictive user rate item however trivial process. hand collaborative ﬁltering techniques correlations users’ ratings infer rating unrated items user make recommendations without understand item user itself. depend item descriptions tends produce higher accuracies cbf. however suffers coldstart problem occurs item cannot recommended unless rated user rated items particularly important domains items frequently added items users interested items. example many users interested likely purchase styles shoes rather out-dated styles many users interested watching newly released movies rather older movies. recommending items potential drive away customers. addition making inappropriate recommendations users built proﬁle also drive away users. approach addressing cold-start problem using hybrid recommender system leverage advantages multiple recommendation systems. developing hybrid models significant research direction many hybrid approaches combine content-based ﬁlter collaborative ﬁlter methods averaging predicted ratings combining recommendations techniques paper present neural network model latent input variables hybrid recommendation algorithm addresses cold-start problem. uses matrix item ratings item/user descriptions simultaneously train weights neural network induce latent input variables matrix factorization. using neural network allows ﬂexible architecture conﬁgurations model higher-order dependencies data. based idea generative backpropagation expands upon unsupervised backpropagation genbp neural network methods induce latent input variables. latent input variables form internal representation observed values. latent input variables fewer observed variables methods dimensionality reduction techniques. genbp adjusts latent inputs holding network weights constant. used generate labels images natural language differs genbp trains network weights simultaneously latent inputs instead training weights pre-processing step. development incorporates input features among latent input variables. incorporating user/item descriptions input features able address cold-start problem. outperforms content-based ﬁlters hybrid ﬁlters cold-start problem. additionally outperforms predecessor maintains accuracy similar matrix factorization non-cold-start recommendations. matrix factorization become popular technique part effectiveness data used netflix competition widely considered state-of-the-art recommendation technique. linear dimensionality reduction technique factors rating matrix much-smaller matrices. smaller matrices combined predict missing ratings original matrix. previously shown could represented neural network model involving hidden layer linear activation functions using non-linear activation functions unsupervised backpropagation viewed non-linear generalization related nonlinear used means imputing missing values utilizes three phases training initialize latent variables weights model update weights latent variables simultaneously. builds nlpca integrating item user descriptions latent input variables. pure collaborative ﬁltering techniques able handle cold-start problem items users. result several hybrid methods developed incorporate item and/or user descriptions collaborative ﬁltering approaches. common surveyed burke involves using separate techniques combining outputs using output technique input another. content-boosted collaborative ﬁltering uses missing values ratings matrix dense ratings matrix passed collaborative ﬁltering method work addresses cold-start problem build user/item descriptions later recommendation system section formally describe latent neural networks high-level neural network latent input variables induced using generative backpropagation. simply using generative backpropagation compute gradient respect latent inputs less commonly used provide derivation here. compute presentation single element since assume typically high-dimensional sparse. signiﬁcantly efﬁcient train presentation known element individually. begin deﬁning error signal individual element express gradient partial derivative error signal respect latent input represent respectively output values input values output nodes backpropagation algorithm calculates ∂erc network unit) error term associated network unit. thus calculate additional calculation backpropagation algorithm needs made ∂βjc single layer perceptron initialize element small random values weights single-layer perceptron initialize element small random values train epoch s/s′ weights multi-layer perceptron hidden layers initialize element small random values train epoch s/s′ train epoch s/s′ return equation strict generalization equation equation considers output unit known target value presented whereas equation sums unit intrinsic value feeds. integrate generative backpropagation training process uses three phases train ﬁrst phase computes initial estimate intrinsic vectors second phase computes initial estimate network weights third phase reﬁnes together. three phases train using stochastic gradient descent. phase intrinsic vectors induced hidden layers form nonlinear separations among them. likewise phase gives weights chance converge without train moving inputs. preprocessing phases initialize system good initial starting point gradient descent likely local optimum higher quality. empirical results comparing three-phase single-phase training show three-phase training produces accurate results single-phase training reﬁnes together pseudo-code algorithm trains three phases given algorithm calls train epoch function performs single epoch training. detailed description follows. matrices containing known data values item descriptions passed along parameters returns ragged matrix containing weight values maps approximation lines perform ﬁrst phase training computes initial estimate lines initialize model variables. represents weights single-layer perceptron elements initialized small random values. implementation draws values normal distribution mean deviation single-layer perceptron temporary model used phase initial training learning rate used store previous error score. error measured initialized lines train convergence detected. discarded. note many techniques could used detect convergence. implementation decays learning rate whenever predictions fail improve sufﬁcient amount. convergence detected learning rate falls η′′. speciﬁes amount improvement expected epoch else learning rate decayed. regularization term used train epoch. lines perform second phase training. phase differs ﬁrst phase ways multilayer perceptron used instead temporary single-layer perceptron held constant phase. completeness describe train epoch given algorithm performs single epoch training stochastic gradient descent. algorithm similar epoch traditional backpropagation except presents element individually instead presenting vector conditionally reﬁnes latent variables well weights given current note efﬁcient implementations line propagate values output unit lines compute error term output unit hidden unit network. activation output units computed error units lines reﬁne gradient descent. line speciﬁes reﬁned phases lines reﬁne gradient descent. line computes root-meansquared-error known element section present results experiments. examine using movielens data hetrec workshop data provides descriptions movies addition ratings matrix. data sets provide user/item descriptions addition ratings matrix data sets provide unstructured data twitter information friends last.fm input variables could created. paper focuses performance rather feature creation unstructured data chose movielens data set. also running state-of-the-art recommendation systems take long time reported running bayesian probabilistic took hours netﬂix data using smaller data allows extensive evaluation facilitates cross-validation. movielens data contains users movies ratings. average ratings user ratings movie. item descriptions genre movie binary variables indicating movie belongs genres. without three phase training. equivalent hybrid hybrid nlpca technique. three phase training denoted lnnpt. compare several recommendation systems content-boosted collaborative ﬁltering content-based ﬁltering nonlinear principle component analysis unsupervised backpropagation matrix factorization recommendation system test several parameter settings. uses single learning algorithm learn rating preferences user. experiment using na¨ıve bayes linear regression decision tree neural network trained backpropagation. learning algorithms also used cbcf number neighbors ranges number latent variables ranges regularization term addition values used number latent variables regularization term number nodes hidden layer ranges nlpca lnnpt. experiment randomly select ratings test set. using training validation parameter selection. using selected parameters test test using -fold cross-validation. results comparing recommendation approaches shown table report mean absolute error approach. bold values represent lowest means within algorithms latent variables signiﬁcantly lower thus demonstrating predictive power using latent variables item recommendation. latent inputs also allows bypass feature engineering often difﬁcult process. count containing count many times rating predicted initialize element count numn eighbors distt hresh neighbors getneighbors numn eighbors return maxindex addition item descriptions nlpca improves performance compared using latent variables. performance lnnpt similar matrix factorization widely considered state-of-the-art recommendation systems comparing mae. power lnnpt comes faced cold-start problem address following section. discussed previously pure collaborative ﬁltering techniques able address cold-start problem despite able perform well items rated previously certain number times. lnnpt capable addressing cold-start problem still obtaining similar performance matrix factorization. examine cold-start problem remove ratings rated movies individually collectively. number removed ratings single movie ranged ratings removed recommendation systems trained using remaining ratings using parameter setting found previous experiments. predicting item poses additional challenge since latent variables items induced. using values latent inputs often produced worse results cbf. creates model user based item descriptions corresponding user ratings. hand produces single model beneﬁcial using ratings mutual information users items shared. shared information contained latent variables. quality latent variables depends number ratings user given and/or item received. compensate lack latent variables items utilize item prediction function takes vector anewitem representing description item outlined algorithm high level item prediction uses anewitem nearest neighbors. induced latent input variables neighbor concatenated anewitem trained predict rating item. weighted mode predicted ratings item returned. rating neighbor weighted according many times rated. weighting mean selecting mode numbers predicted rating added times number times neighbor item rated. chose mode rather mean mode robust outliers achieves better empirical results validation sets experimentation. next describe item prediction detail. lines initializes counter keeps track many times rating predicted item initializes values line initializes number nearest neighbors search sets distance threshold chose neighbors generally enough neighbors produce good results. used binary item descriptions movie genres considered using latent variables items genre values come play line item used distance greater distt hresh item rated least times. value chosen based evaluation content-based predictor number times item rated helps determine quality induced latent variables item provides conﬁdence level latent variables. line ﬁnds closest neighbors inserts indexes array. lines count number times rating predicted weighted number times item rated. linear rating prediction item rated times count ratings predicted value. helps discount items rated times whose latent variables good values. line returns index count results recommending items using item prediction provided table values table correspond movie movielens data set. bold values represent lowest value obtained. single recommendation system produces lowest items suggesting recommendation systems better others given user and/or item suggested previously lnnpt score lowest several movies individually. exception movie lnnpt produce lowest movies previously rated. holding items lnnpt produces lowest value. shows importance using latent variables. cbcf uses create dense matrix uses collaborative ﬁltering technique dense matrix recommend items user. thus emphasis given generally produces poorer item recommendations collaborative ﬁltering approach. hand utilizes latent variables predictive power. efﬁciency precise case neural network models since based number iterations convergence. experiments always converges regardless parameter settings. however parameter settings require longer reach convergence others. average time seconds required algorithm using parameter settings found previous experiments shown table additional complexity requires time train. however beneﬁt model induced order recommend unrated items case nlpca ubp. recommending items uses tree nearest neighbor search search insert complexities. paper presented neural network latent input variables capable recommending unrated items users items users call latent neural network latent variables input variables allow information correlations among rated items represented also incorporating item descriptions recommendation. thus empirically able achieve similar results state-of-the-art collaborative ﬁltering techniques matrix factorization also addressing cold-start problem. compared hybrid ﬁlters content-based ﬁltering achieves much lower error recommending previously unrated items. achieves similar error rates state-of-the-art ﬁltering techniques make recommendations previously unrated items retrained items rated order recommend them. built neural network capable modeling higher-order dependencies nonlinearities data. however data movielens data many similar data sets well suited using linear models matrix factorization. part fact many data sets inherently sparse nonlinear models could overﬁt reduce generalization. direction future work examining better incorporate nonlinear component lnn. also looking integrating user item descriptions latent input variables address user problem item problem single model.", "year": 2014}