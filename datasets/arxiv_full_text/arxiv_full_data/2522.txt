{"title": "Optimization of anemia treatment in hemodialysis patients via  reinforcement learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Objective: Anemia is a frequent comorbidity in hemodialysis patients that can be successfully treated by administering erythropoiesis-stimulating agents (ESAs). ESAs dosing is currently based on clinical protocols that often do not account for the high inter- and intra-individual variability in the patient's response. As a result, the hemoglobin level of some patients oscillates around the target range, which is associated with multiple risks and side-effects. This work proposes a methodology based on reinforcement learning (RL) to optimize ESA therapy.  Methods: RL is a data-driven approach for solving sequential decision-making problems that are formulated as Markov decision processes (MDPs). Computing optimal drug administration strategies for chronic diseases is a sequential decision-making problem in which the goal is to find the best sequence of drug doses. MDPs are particularly suitable for modeling these problems due to their ability to capture the uncertainty associated with the outcome of the treatment and the stochastic nature of the underlying process. The RL algorithm employed in the proposed methodology is fitted Q iteration, which stands out for its ability to make an efficient use of data.  Results: The experiments reported here are based on a computational model that describes the effect of ESAs on the hemoglobin level. The performance of the proposed method is evaluated and compared with the well-known Q-learning algorithm and with a standard protocol. Simulation results show that the performance of Q-learning is substantially lower than FQI and the protocol.  Conclusion: Although prospective validation is required, promising results demonstrate the potential of RL to become an alternative to current protocols.", "text": "objective anemia frequent comorbidity hemodialysis patients successfully treated administering erythropoiesis-stimulating agents esas dosing currently based clinical protocols often account high interintra-individual variability patient’s response. result hemoglobin level patients oscillates around target range associated multiple risks side-eﬀects. work proposes methodology based reinforcement learning optimize therapy. methods data-driven approach solving sequential decision-making problems formulated markov decision processes computing optimal drug administration strategies chronic diseases sequential decision-making problem goal best sequence drug doses. mdps particularly suitable modeling problems ability capture uncertainty associated outcome treatment stochastic nature underlying process. algorithm employed proposed methodology ﬁtted iteration stands ability make eﬃcient data. results experiments reported based computational model describes eﬀect esas hemoglobin level. performance proposed method evaluated compared well-known q-learning algorithm standard protocol. simulation results show performance q-learning substantially lower protocol. comparing protocol achieves increment proportion patients within targeted range hemoglobin period treatment. addition quantity drug needed reduced indicates eﬃcient esas. conclusion although prospective validation required promising results demonstrate potential become alternative current protocols. anemia common complication characterized reduced concentration hemoglobin occurs patients undergoing hemodialysis hemodialysis common treatment patients advanced stages chronic kidney disease particularly state commonly referred end-stage renal disease last years prevalence esrd increased substantially reaching million population developed countries countries japan current prevalence million esrd involves gradual loss kidney function time produces among health problems poor production erythropoietin hormone regulates blood cell production class cells rich levels associated heart disease poorer overall quality life increased mortality response kind drugs known large interintra-interindividual variability diﬀerences background characteristics disease severity comorbidities concurrent medications although exist protocols help physicians determine appropriate dose achieving stable levels within target range complex often requires dose titration. results several studies suggest phenomenon known cycling common occurrence esa-treated patients cycling deﬁned cyclical repeated movement levels treatment. exact causes cycling completely understood; however number possible reasons proposed. fishbane berns suggested management practices major causes. first rigid dose adjustment protocols account high heterogeneity patient response. second narrow target ranges recommended clinical guidelines need frequent dose changes. eﬀect dose change reach steady state days doses changed frequently diﬃcult take account long-term eﬀects dose often ignored link cycling development several diseases together high cost treatment justiﬁes need improve current protocols. widespread electronic medical records giving rise large amounts data could useful reduce medical errors improve treatments minimize side eﬀects costs work proposes methodology based reinforcement learning optimize therapy. data-driven approach solving sequential decision-making problems formulated markov decision processes computing optimal drug administration strategies chronic diseases sequential decision-making problem goal best sequence drug doses. mdps particularly suitable modeling problems ability capture uncertainty associated outcome treatment stochastic nature underlying process standard approach solve mdps dynamic programming however practical application limited cannot deal large-scale problems requires full knowledge model including transition probability function. contrast uses function approximation address large-scale problems data sampled process implicitly represent transition function exploit information contained medical records compute policies administration tailored individual characteristics patient. addition optimization process made sequences doses instead isolated doses crucial include drug long-term eﬀects. methodology proposed work uses algorithm ﬁtted iteration learn policy administration medical records. features employed deﬁne model extracted part laboratory tests part clustering procedure patient’s main attributes. order test methodology series experiments conducted using computational model simulates response patients. performance assessed algorithm q-learning standard protocol dose adjustment. rest paper organized follows. next section provides brief review related work domain. section introduces necessary background brieﬂy explains algorithms employed experiments namely qlearning ﬁtted iteration. latter algorithm makes extremely randomized trees supervised learning method described section section discusses computational model used experiments simulate patients’ response esa. anemia management formulation using framework presented section experiments carried detailed section section shows discusses achieved results. finally conclusions proposals work given section idea using data-driven method optimize administration new. artiﬁcial neural networks used several authors last decade individualize doses general methods used current previous levels doses variables describe patient’s condition order predict next level. goal previous works select optimal dose order achieve given level. approach suitable optimization horizon next time step. contrary therapy long-term stabilization. idea applied using machine learning techniques fuzzy logic support vector machines bayesian networks model predictive control method process control whose main advantage incorporates ﬁnite timehorizon optimization process. gaweda showed result improved anemia management. major diﬃculty requirement accurate system model. even system model available shown competitive context anemia management previously studied gaweda mart´ın-guerrero agree potential become alternative currently used protocols. algorithm employed works popular q-learning algorithm widely used ﬁelds robotics requires little computation work real time. however qlearning makes ineﬃcient data thus suitable problems acquiring data costly fitted q-iteration relatively algorithm signiﬁcantly reduces quantity data required learn useful policies. recently growing interest applying optimize treatment several diseases including hiv/aids psychiatric disorders epilepsy schizophrenia smoking addiction authors’ knowledge ﬁrst work applies optimization anemia treatment. reinforcement learning general class algorithms ﬁeld machine learning solving decision-making problems decisions made stages problems present wide range ﬁelds including operations research artiﬁcial intelligence automatic control medicine standard setting consists agent environment decision produces immediate reward. agent learns perform actions order maximize reward collected time. goal deﬁned user reward function. contrary approaches rely mathematical model system based experience agent obtains experience interacting environment. fig. represents main elements interact. stage discrete time-point evaluates immediate eﬀect transition provide information long-term eﬀects. reward function deﬁned user implicitly codiﬁes goal agent. notice reward function describe achieve goal agent must learn experience. suppose patient suﬀering certain disease requires long term treatment particular drug. usually administrate suitable sequence doses order control variable related severity disease. example anemic patients level used measure degree anemia. framework applied problem modeling patient environment. case state contain information relevant choose proper treatment. addition current level state include factors inﬂuence effect treatment physical characteristics patients nutritional condition. actions possible treatments administered patient. treatment patient status evolves state. state part consequence treatment part consequence aspects cancontrolled agent like example presence inﬂammation blood losses. objective treatment maintain level within range reward function deﬁned provide positive reward level limits target range negative reward otherwise. objective agent learn policy maximizes rewards received time quantity known return. maximizing policy denoted said optimal. return usually computed using inﬁnitediscounted horizon. case return initial state policy discount factor. parameter intuitively interpreted balance immediate reward future rewards. future rewards relevant calculation return approaches optimal policy agent must explore environment probability attempting actions must always non-zero. otherwise areas state-action space never visited learning process become stuck local optimum. tradeoﬀ greedy action choices exploration necessary performance algorithm exist several strategies include exploration agent’s agent receives environment’s state basis selects action. consequence action next time step agent receives numerical reward environment evolves state. agent selects actions depending environment state using policy assigns action every state. typically agent modiﬁes policy result interactions environment. elements problem formalized using markov decision processes framework. next section mdps used formally introduce basic components then algorithms used work q-learning ﬁtted iteration brieﬂy described section states discrete time-point environment occupies state state usually composed vector whose components describe current situation environment. time passes vector values evolve part consequence actions applied agent part stochastically. state simply variable observed directly environment complex structure variables highly processed combines information current past situations environment. actions agent applies action state. state next instant inﬂuenced current action. actions mechanism employed agent control guide evolution environment. transition probability function action taken current state transition function gives probability next state i.e. function describes state evolves. remaining section describes algorithms solving mdps. hand well-known q-learning algorithm q-learning probably popular algorithm used many applications including treatment anemia hemodialysis patients hand ﬁtted iteration recent algorithm forms part methodology proposed paper. oﬄine algorithms means require interacting environment learning phase instead learn solution using data collected advance. data experience usually stored transitions form sampled process. data representative state-action space i.e. contain certain degree exploration. given agent cannot interact environment algorithms applied oﬄine case necessary include exploration strategy. consider q-function approximator denoted parameterized d-dimensional vector every possible vector parameters provides approximated representation corresponding q-function symbol denotes approximation. general approximator nonlinear parameters. however algorithms linear approximators often preferred provide better convergence stability properties linearly parameterized approximation q-function expressed vector basis functions combined using d-dimensional vector parameters common approach deﬁne basis functions consists using regular grid gaussian radial basis functions spanned stateaction space case state-action pair vector basis functions q-learning algorithm starts arbitrary approximation optimal q-function i.e. arbitrary vector parameters then uses data transition update parameters using following rule solving means optimal policy. several methods solving mdps grouped classes dynamic programming reinforcement learning methods require knowledge full model. since transition probability function rarely available class methods employed limited number practical problems. hand methods completely based experience makes useful full model unknown diﬃcult estimate. algorithms q-functions optimal policy. given policy q-function particular pair deﬁned expected return encountered starting taking action thereafter following policy maxx stands argument attains maximum value function general given q-function policy maximizes said greedy therefore solving done ﬁrst ﬁnding using compute greedy policy mdps small enough number states actions q-functions exactly stored tables entry state-action pair. unfortunately many practical problems contain large inﬁnite number states; case qfunctions must represented approximately reasons. first suppose contains states large table entries would intractable computational memory limitations. contrary typical approximate representation necessary store vector parameters second state space large continuous agent probably never exactly state once. therefore experience acquired states generalized states seen before. principle function approximation method used represent qfunctions. however practice algorithms impose restrictions structure approximator. index corresponds number transitions data learning rate. learning rule updates estimation optimal q-function incrementally. moreover update requires little computation memory resources makes possible apply qlearning real-time. hand algorithm presents possible drawbacks generally requires many transitions obtain useful policies function approximator parametric typically linearly parameterized fitted iteration batch algorithm whose main feature lies handles experience unlike incremental algorithms uses complete transitions time updates estimation optimal qfunction. although process involves computation allows extract information stored experience. consequently data-eﬃcient algorithms. feature makes suitable algorithm many application domains. example problem tackled here patients modeled environment agent estimate optimal drug doses. acquiring experience context entails administering dose waiting takes eﬀect measuring variables deﬁne patient’s condition. process expensive time money. thus reducing quantity data required algorithm crucial. iterative process stopped simply establishing maximum number iterations. another possibility threshold value stop loop distance consecutive estimations optimal q-function similarly algorithms requires function approximator represent large continuous q-functions. however contrary q-learning impose constraint kind approximator. fact iteration possible change approximator order adapt resolution model reach best bias/variance tradeoﬀ although successfully combined many approximation methods linear regression technique known extremely randomized trees shown better performance approaches therefore treebased method whose details introduced next section used experiments. despite data-eﬃcient algorithms number transitions required learn optimal policies grows quickly state space dimensionality curse dimensionality feature common algorithms including also q-learning. thus reducing number state variables much possible important issue. similarities among patients exploited k-means clustering analysis applying learning algorithm extremely randomized trees tree-based ensemble method supervised classiﬁcation regression problems. considered improved version popular tree ensemble method tree bagging algorithm developed compute kind ensembles called extra-trees like tree bagging works building several trees. diﬀerence approaches lies sample used compute trees. tree bagging uses bootstrap sample extra-trees uses complete data built tree. similarly standard regression trees tree composed decision nodes node contains split attribute. value attribute split known cut-point. order deﬁne decision node extra-trees generates splits choosing attributes random attribute cut-point random. then calculates score candidate splits selects split obtained maximum score. process repeated number elements node less parameter lmin algorithm three parameters need speciﬁed number trees build ensemble number candidate tests node minimal leaf size lmin. ability compute treatment policies assessed simulations. experiments based computational model describes eﬀect darbepoetin alfa level. section presents main characteristics model order provide insight tackled clinical problem. appendix gives detailed description model. several theoretical pharmacodynamic models describing hematological response diﬀerent kinds esas developed last decades model introduced section focused patients undergoing hemodialysis treated intravenous darbepoetin alfa second generation drug. hematopoietic cell populations natural examples biological systems governed lifespan-based processes cell proliferation diﬀerentiation maturation senescence. level proportional number erythrocytes produced primarily stem cells bone marrow. process maturation stem cells undergo series diﬀerentiations. reach stage reticulocyte begin circulate blood days ultimately become mature rbcs patients erythrocyte lifespan approximately days process erythrocytes production regulated hormone produced kidneys. darbepoetin alfa synthetic form stimulates erythropoiesis mechanism. concentration dynamics following administration darbepoetin described multi-compartment model diﬀerent cell types involved erythropoiesis grouped population classes according characteristic properties respect interaction epo. number cells compartments depends plasma concentration consists naturally produced exogenous administered case intravenous administration total amount darbepoetin alfa injected vein within short time interval that without loss generality assumed amount exogenous hormone plasma time administration takes place exactly equal amount exogenous hormone administered. result sudden rise hormone plasma concentration followed exponential decay described ﬁrst order ordinary differential equation constant elimination rate appendix addition endogenous mean corpuscolar hemoglobin concentration individual response patient darbepoetin alfa deﬁned parameters constant determines cells ﬁrst compartment model plays similar role last compartment parameters adjusted using clinical laboratory data patients. matlab employed adjust parameters perform simulations. matlab solver used compute solutions system differential equations. computational model model simpliﬁcation real problem. makes basic assumptions ﬁrst patient maintains stable level inﬂammation second availability iron erythropoiesis constant. generally assumptions patients treatment. nevertheless model able capture heterogeneity response darbepoetin alfa long-term eﬀects factors suggested principal causes cycling. therefore model useful evaluate performance preventing cycling. symptoms anemia response treatment often vary depending several factors physical characteristics patient degree kidney disease comorbidities treatment period clinical evolution patient typically monitored monthly reviews. review consists laboratory test measures several variables assess patient’s condition drug prescription next review. therefore patient generates sequence observations treatments form sequential decision-making problem easily derived sequences actions correspond doses prescribed patient diﬀerent months applying method necessary deﬁne function assigns state patient’s current history scalar reward function evaluates transition taking action result applying functions sequence states actions rewards framework assumes state transition dynamics reward distribution markov property given current state action next state reward conditionally independent previous states actions rewards obvious deﬁning state representation holds markov property include past history practice feasible reasons stated section amount experience needed learn useful policies grows exponentially number state dimensions curse dimensionality space required store experience would grow indeﬁnitely grows. therefore state contain enough features provide good summary past history time compact possible allow learning useful policies limited amount experience. despite theory assumes markov property practical problems fully satisfy condition solved successfully applying methods cases framework extended general framework using partially observable mdps measures degree anemia current month ∆hbk deﬁned diﬀerence current level previous indicates trend. dose darbepoetin alfa. long-term eﬀects darbepoetin alfa month inﬂuenced last drug dose also doses previous months therefore dak− dak− also included state deﬁnition. hand given treatment tailored individual characteristics patient information characteristics must included state deﬁnition. computational model employs four variables describe patient namely variables could directly introduced state variables however prior knowledge problem used reduce state space dimensionality. dichotomous variable depends patient’s gender. therefore decided remove variable compute policy another women. remaining variables analyzed using k-means clustering objective groups patients respond treatment similar then introducing state deﬁnition information kind response instead patient’s characteristics directly. thus output clustering algorithm patientgroup also formed part state space. underlying values correspond doses administered patients. noted weightnormalized doses diﬀerent patient depending body weight. practice using discrete doses useful fact darbepoetin alfa supplied preﬁlled syringes syringe partially injected rest wasted. reward function deﬁnes agent’s goal. treatment maintain levels healthy range. according european best practice guidelines treatment anemia chronic kidney disease concentration g/dl patients higher g/dl patients comorbidities cardiovascular disease diabetes etc. thus target range used work g/dl. hand abrupt changes avoided. speciﬁcally changes close g/dl month increase risk cardiovascular episodes order satisfy goals reward designed piecewise function depends current level hbk. idea close target policy reach target range next month. however diﬀerence target large change required reach target next month abrupt avoided. parameter controls slope function. function assigns maximum reward hbk+ equal g/dl decreases smoothly zero hbk+ diﬀers g/dl indicating levels near target preferable. fig. shows shape ρhb. purpose experiments assess proposed method comparing quality learned policy popular q-learning algorithm standard protocol dose adjustment. illustrated fig. experiments carried three stages. first computational model used generate experience necessary apply algorithm. speciﬁcally hemodialysis patients treated darbepoetin alfa months simulated. second data processed order obtain states actions reward. then drug administration policy learned data using fqi. comparison reasons learning process also carried using q-learning. finally third stage cohort patients simulated accordance three treatment strategies policy learned using policy learned using q-learning treatment recommended protocol. computational model used generate data mimics medical data gathered common clinical practice. model uses four parameters describe patient’s individual response anemia treatment. chosen indicated remaining parameters adjusted employing data extracted medical records patients undergoing hemodialysis receiving darbepoetin alfa. data collected january december three hemodialysis centers located italy. table shows baseline characteristics patients used adjust model parameters table shows summary adjusted parameters. proposed method applied separately women data split gender. high level similarity results obtained groups rest work focused men’s group. data splitting adjusted parameters available male patients. order increase population size linear interpolation applied parameters patient patient randomly selected among nearest neighbors. process allows generate artiﬁcial patients similar real ones time introduces certain degree randomness order capture variability characteristics among diﬀerent patients. process repeated parameters corresponding patients generated making total patients. then patients’ progress time simulated during months treatment dose darbepoetin alfa administered month randomly selected among discrete introduced high variability doses. patients followed speciﬁc treatment protocol learned policy would strongly biased protocol. actual case physicians’ prescriptions based clinical protocols experience therefore common diﬀerent doses patients similar conditions. nevertheless general doses prescribed physicians closer optimal ones facilitates learning process. simulations generated total observations treatments. randomness treatment observations level greater g/dl. data removed represent unrealistic situations. restriction data consisted observations. data generated previous stage used input learning algorithms q-learning. sequences observations treatments transformed transitions following methodology introduced section state variables extracted using clustering algorithm k-means. rest given prevalence total population size simulated population typical region million inhabitants. hand patients often treated extended periods time chronic nature however months enough generate amount data required learn useful policies figure reward function. subﬁgures represent piecewise reward three separate subfunctions. function applied varies depending hbk+. functions applied vary depending ∆hbk+. finally represents complete reward function versus hbk+ hbk. fig. shows convergence versus number iterations observed approximated qfunction remains almost stable iterations. view this number iterations ﬁxed hand discount factor empirically value enough incorporate long-term eﬀects dose optimization process. k-means algorithm employed clustering analysis starts initial estimation centroids uses iterative method optimize cluster quality. typically number clusters selected empirically. work clustering analysis repeated using values value obtained clusters analyzed using silhouette plots ﬁnally selected suitable maximizes within-cluster compactness separability among clusters. based iterative procedure designer must decide stop iterations. algorithm starts arbitrary approximation optimal q-function improved iteration. thus number iterations enough allow convergence optimal q-function. convergence measured terms distance consecutive approximations q-function deﬁned figure experiments carried three stages. first computational model used simulate cohort patients treated period time gathering records generated patient database. secondly records processed produce transitions form algorithm applied. comparison reasons learning process also repeated using q-learning. third policies learned data evaluated compared standard protocol using cohort simulated patients. extra-tree algorithm parameters chosen following procedure described number trees ensemble parameter equal dimension input space minimal leaf size selected among values lmin using cross-validation iteration fqi. experiments showed default values good choice q-learning also based iterative procedure estimates optimal q-function. however transition used time number transitions limited algorithm simply iterates complete transitions. thus necessary implement strategy stop iterative procedure. gaussian network ﬁxed bases employed approximate q-function requires deﬁnition number gaussian functions centers standard deviations. process typically requires trial error experimentation various conﬁgurations. number functions enough provide smooth interpolation entire stateaction space. gaussians peaked necessary employ large number functions cover entire space. hand network ﬂexible enough approximate abrupt changes q-function. thus necessary reach compromise locality smoothness. experimenting several conﬁgurations selected network architecture employed gaussian functions whose centers distributed regular grid state space. inputs normalized range standard deviations discount factor ﬁxed algorithm additionally q-learning introduces parameter tuned learning rate large value usually results faster convergence exceeds certain critical value algorithm becomes unstable. fig. shows convergence q-learning ﬁgure suggests q-learning converged iterations. policy learned proposed methodology based evaluated cohort patients using computational model. similar section parameters corresponding patients generated linear interpolation. evolution patient simulated months treatment using drug doses indicated policy. comparison purposes cohort patients simulated according policy learned qlearning treatment recommended standard protocol. protocol extracted european medicines agency describes dosage regimen aranesptm follows figure plot representation levels corresponding simulated patients months treatment. patients treated according πq-learning πfqi πprotocol. horizontal lines indicate target range. related superiority ﬂexibility approximate q-functions. q-learning necessary deﬁne priori approximator structure remains ﬁxed learning process. contrary iteration selects approximator structure provides best approximation current q-function. limitation plot representation temporal information lost. order compare performance policies along time better approach consists representing levels treatment. fig. shows information πfqi πprotocol. sake simplicity fig. represents subset patients randomly selected. ﬁgure also includes horizontal lines g/dl indicate target range. ﬁrst months treatment shifted towards target range either policies. sense πfqi eﬀective πprotocol because general required less time reach target. seven months treatment several signs cycling appeared patients treated πprotocol. contrary πfqi able prevent drastically reduce cycling. expected extreme values previously shown outliers plot mainly concentrated ﬁrst months treatment. fig. also shows variation time case terms mean standard deviation complete group patients. again observed πfqi stabilizes levels within target range whereas protocol produces oscillations. large standard deviation corresponding πprotocol indicates patients dangerous levels months. although cycling produced πprotocol diminishes slightly time months still noticeable. variability level greater g/dl dose reduced reduction level continues rising treatment temporarily interrupted level starts decline moment treatment resumed dose approximately lower previous dose. computational model simulates patients initialized level determined initial conditions. conditions equal patients i.e. month zero patients level. order cohort patients heterogeneous initial state patients treated four months random treatment starting evaluation process. thus protocol recommendation initial dose never applied patients already received previous doses beginning evaluation. section assesses proposed methodology comparing policy learned using policy learned using q-learning extracted protocol results shown correspond cohort simulated patients used validation purposes months treatment. ﬁrst approach compare behavior three policies levels obtained policy represented plot. kind representation graphically depicts data values quartiles allowing visually estimate degree dispersion skewness. fig. shows plot corresponding πq-learning πfqi πprotocol. three cases median falls within desired range levels approximately symmetrically distributed. main diﬀerence observed among three policies large dispersion πq-learning means many patients treated policy levels away target range. policies also levels substantially diﬀerent target range; however values considered outliers complete observations likely initial states patients. performance q-learning probably consequence limited amount transitions available estimate policy. principle algorithms able similar policies given problem. fact authors found that given enough data policy learned q-learning superior protocol nonetheless advantage makes efﬁcient data crucial problems obtaining data non-trivial task. second factor figure level evolution months treatment simulated patients randomly chosen test group. patients treated according policy learned proposed method πfqi. treated according policy extracted protocol πprotocol. horizontal lines indicate target range. second metric employed compare policies number months observations patients presented adequate level. fig. presents information means chart. five ranges grouped three categories deﬁned category suitable includes target range g/dl; unsuitable consists ranges contiguous target range g/dl; dangerous contains rest levels. fig. shows bars corresponding πfqi πprotocol ranges. ideally patient observations within category suitable. patients treated πfqi level within target range observations represents important improvement compared achieved πprotocol. rest patient observations mainly located category unsuitable speciﬁcally πfqi πprotocol. percentage indicates πprotocol also achieved reasonable outcome. finally minor percentage patient observations category dangerous; however figs. shows observations corresponds mainly addition stabilizing level inside target range treatment must avoid abrupt changes. diﬀerence πfqi πprotocol concerning aspect negligible. policies really eﬀective avoiding changes greater g/dl month state transitions condition. high costs darbepoetin alfa another point take account comparing policies quantity drug used. highly signiﬁcant diﬀerence mean dose recommended πprotocol πfqi thus treatment recommended policy produced better outcomes also generated lower costs. finally particular case studied detail obtain insight behavior policy. table shows level patient dose drug administered varies along months treatment. goal table compare treatment recommended policies case πprotocol caused cycling. months level inclusion model parameters deﬁnition state space likely important diﬀerence applying proposed system simulated real patients. parameters related patients’ individual characteristics used groups patients respond treatment similar way. practice parameters cannot directly measured. option would adjust simulated case; however eﬀective solution employ variables commonly measured monthly reviews provide information. example level c-reactive protein serum albumin leukocytes used indicator inﬂammation level. thus clustering analysis performed using variables inputs instead model parameters. elements reward function discount factor carefully chosen provide desired outcomes. using computational model trialand-error procedures used adjust them. contrary approach viable real domains. thus despite valuable experience gained simulated patients expertise advice physicians still necessary design real case. finally third issue noticed assumptions made model stable level inﬂammation constant iron availability. previously mentioned assumptions real case. requirement overcome assumptions clinical data employed learn policy must contain enough examples patients assumptions violated variable levels inﬂammation iron availability. within target range highlighted bold. observed that ﬁfth month πprotocol started increase darbepoetin dose g/dl continued increasing dose months level reached target range. then reaching target level continued rising despite drug dose decreased phenomenon delay drug administration eﬀects hand doses administered πfqi suggest long-term eﬀects taken account. example months dose maintained constant despite level. consequence level next months increases stabilized within target. work proposed methodology based optimize anemia treatment hemodialysis patients. formulating problem using framework able learn automatically near-optimal treatments clinical data. contrary techniques solve mpds require complete knowledge system dynamics feature crucial medical problems. speciﬁcally methodology uses algorithm stands ability make eﬃcient data. combined function approximator based regression trees order deal continuous state space generalize learned policy cases covered data set. state variables extracted partly laboratory tests partly clustering analysis patient’s main attributes. proposed methodology evaluated computational model describes eﬀect darbepoetin alfa concentration addition experiments also performed using well-known q-learning algorithm. standard protocol dose adjustment used baseline comparison. quality policy obtained q-learning πq-learning considerably inferior comparison policies comparing πfqi πprotocol policy obtained increased proportion patients adequate level time reduced amount drug used simulation results suggest policy deal long-term eﬀects darbepoetin alfa high variability patient’s response. features absent standard dosing protocols suggested major cause cycling. result proposed methodology eﬀective standard-use tested protocol maintaining stable levels preventing cycling. hand drug prescribed eﬃcient since treatment achieves better outcomes less amount darbepoetin alfa. computational model used experiments several limitations owing assumptions grounded therefore represent possible patients. nevertheless reproduces important diﬃculties present actual cases cause cycling. thus although prospective validation required experiments shown potential beneﬁts anemia treatment. positive results obtained work using simulated patients motivated research applying proposed methodology actual patients. currently tool clinical decision support based validated clinical evaluation hemodialysis centers three european countries. although work focused renal anemia methodology extended types anemia. example oncology patients also receive darbepoetin alfa treatment even complex problems drug administration warfarin therapy prevent venous thromboembolism. another interesting line future research include optimization aspects reward function addition related patients’ health like cost treatment. many mathematical models developed simulate process erythropoiesis diﬀerent physiological scenarios common approach age-structured models model presented section focused patients undergoing hemodialysis treated intravenous darbepoetin alfa. addition incorporates eﬀects iron availability level inﬂammation although considered constant. similar stimulating action drug described multicompartment model. three diﬀerent cell classes compartments considered comprises remaining erythroblasts marrow reticulocytes iron-responsive. assumed patients suﬃcient iron available compartment serves delay diﬀerentiation next compartment. plasma concentration blood cells. equations governing evolution number cells population simply balance equations compartment cells entering fresh cells emptied outgoing diﬀerentiated apoptotic cells. fundamental assumption every cell population lives period time constant denoted cells cells assumption determines cell elimination rate since number cells lost time must equal number cells born time delayed appropriate lifespan. loss process modeled means weighted averages previous incoming rates order take consideration cells actually diﬀerent exposition times drug. exposition time varies according internal maturity level time administration. entering compartment depends progenitors response stimulatory eﬀect erythropoietin. according response described hill function etot/ etot deﬁned figure level simulated using model measured laboratory tests corresponding four patients approximately months treatment. four cases model’s assumptions approximately met.", "year": 2015}