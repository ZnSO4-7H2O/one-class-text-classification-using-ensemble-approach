{"title": "Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent  Space Alignment", "tag": ["cs.LG", "cs.AI", "I.5.1;I5.3"], "abstract": "Nonlinear manifold learning from unorganized data points is a very challenging unsupervised learning and data visualization problem with a great variety of applications. In this paper we present a new algorithm for manifold learning and nonlinear dimension reduction. Based on a set of unorganized data points sampled with noise from the manifold, we represent the local geometry of the manifold using tangent spaces learned by fitting an affine subspace in a neighborhood of each data point. Those tangent spaces are aligned to give the internal global coordinates of the data points with respect to the underlying manifold by way of a partial eigendecomposition of the neighborhood connection matrix. We present a careful error analysis of our algorithm and show that the reconstruction errors are of second-order accuracy. We illustrate our algorithm using curves and surfaces both in  2D/3D and higher dimensional Euclidean spaces, and 64-by-64 pixel face images with various pose and lighting conditions. We also address several theoretical and algorithmic issues for further research and improvements.", "text": "abstract. nonlinear manifold learning unorganized data points challenging unsupervised learning data visualization problem great variety applications. paper present algorithm manifold learning nonlinear dimension reduction. based unorganized data points sampled noise manifold represent local geometry manifold using tangent spaces learned ﬁtting aﬃne subspace neighborhood data point. tangent spaces aligned give internal global coordinates data points respect underlying manifold partial eigendecomposition neighborhood connection matrix. present careful error analysis algorithm show reconstruction errors second-order accuracy. illustrate algorithm using curves surfaces higher dimensional euclidean spaces -by- pixel face images various pose lighting conditions. also address several theoretical algorithmic issues research improvements. introduction. many high-dimensional data real-world applications modeled data points lying close low-dimensional nonlinear manifold. discovering structure manifold data points sampled manifold possibly noise represents challenging unsupervised learning problem discovered low-dimensional structures used classiﬁcation clustering outlier detection data visualization. example low-dimensional manifolds embedded high-dimensional input spaces include image vectors representing objects diﬀerent camera views lighting conditions document vectors text corpus dealing speciﬁc topic vectors encoding test results multiple choice questions group students observation dimensions embedding spaces high vocabulary text corpus number multiple choice questions test) intrinsic dimensionality data points however rather limited factors physical constraints linguistic correlations. traditional dimension reduction techniques principal component analysis factor analysis usually work well data points close linear subspace input space general discover nonlinear structures embedded data points. recently much renewed interests developing eﬃcient algorithms constructing nonlinear low-dimensional manifolds sample data points high-dimensional spaces emphasizing simple algorithmic implementation avoid∗ department mathematics zhejiang university yuquan campus hangzhou china. zyzhangmath.zju.edu.cn. work author done visiting penn state university supported part special funds major state basic research projects foundation university teacher ministry education china grants ccr-. department computer science engineering pennsylvania state university university park zhacse.psu.edu. work author supported part grants ccr-. optimization problems prone local minima lines research manifold learning nonlinear dimension reduction emerged exempliﬁed pairwise geodesic distances data points respect underlying manifold estimated classical multi-dimensional scaling used project data points low-dimensional space best preserves geodesic distances. another line research follows long tradition starting self-organizing maps principal curves/surfaces topology-preserving networks idea information global structure nonlinear manifold obtained careful analysis interactions overlapping local structures. particular local linear embedding method constructs local geometric structure invariant translations orthogonal transformations neighborhood data points seeks project data points low-dimensional space best preserves local geometries approach draws inspiration improves upon work opens directions nonlinear manifold learning many fundamental problems requiring investigated. starting point consider nonlinear dimension reduction isolation merely constructing nonlinear projection rather combine process reconstruction nonlinear manifold argue processes interact mutually reinforcing way. paper address inter-related objectives nonlinear structure ﬁnding construct so-called principal manifold goes middle data points; global coordinate system characterizes data points low-dimensional space. basic idea approach tangent space neighborhood data point represent local geometry align local tangent spaces construct global coordinate system nonlinear manifold. section formulate problem manifold learning dimension reduction precise terms illustrate intricacy problem using linear case example. section discuss issue learning local geometry using tangent spaces section show align local tangent spaces order learn global coordinate system underlying manifold. section discusses construct manifold global coordinate system available. call algorithm local tangent space alignment algorithm. section present error analysis ltsa especially illustrating interactions among curvature information embedded hessian matrices local sampling density noise level regularity jacobi matrix. section show partial eigendecomposition used global coordinate construction eﬃciently computed. present collection numerical experiments section section concludes paper addresses several theoretical algorithmic issues research improvements. inter-related solution leads solution other. situations dimension reduction means itself necessary learn manifold. paper however promote notion problems really sides coin best approach consider isolation. tackle algorithmic details ﬁrst want point diﬃculty manifold learning nonlinear dimension reduction sample data points data points unorganized i.e. adjacency relationship among known beforehand. otherwise learning problem becomes well-researched nonlinear regression problem techniques computational geometry used solve error-free manifold learning problems). ease discussion follows call space data points live input space space data points projected feature space. illustrate concepts problems introduced consider example linear manifold learning linear dimension reduction. assume data points sampled d-dimensional aﬃne subspace i.e. i.e. qdσdv diag largest singular values ¯xet matrices corresponding left right singular vectors respectively. optimal given learned linear manifold represented linear function function unique sense reparametrized i.e. coordinate replaced global aﬃne transformation change basis matrix interested respect dimension reduction low-dimensional representation linear manifold feature space. therefore without loss generality assume feature vectors uniformly distributed. given data amounts assuming coordinate matrix orthonormal i.e. hence take linear case discussed problem dimension reduction solved computing right singular vectors done without help linear function similarly construction linear function done general global nonlinear structure come local linear analysis alignment local linear structure data extracted representing point weighted linear combination neighbors local weight vectors preserved feature space order obtain global coordinate system. linear alignment strategy proposed aligning general local linear structures. type local geometric information tangent space given point constructed neighborhood given point. local tangent space provides low-dimensional linear approximation local geometric structure nonlinear manifold. want preserve local coordinates data points neighborhood respect tangent space. local tangent coordinates aligned dimensional space diﬀerent local aﬃne transformations obtain global coordinate system. alignment method similar spirit proposed next section discuss local tangent space global alignment applied data points sampled noise section dimension reduction reconstruct τi’s corresponding function values without explicitly constructing assume function smooth enough using ﬁrst-order taylor expansion ﬁxed tangent space spanned column vectors therefore dimension i.e. span). vector gives coordinate aﬃne subspace without knowing function explicitly compute jacobi matrix however know terms matrix forming orthonormal basis write equation shows aﬃne transformation align local coordinate global coordinate naturally seek global coordinate local aﬃne transformation minimize amounts matching local geometry feature space. notice deﬁned known function value unknown orthogonal basis matrix tangent space. turns however approximately determined certain function values. discuss approach next section. clearly linear approach readily applicable obviously feature extraction alignment. consider construct global coordinates local aﬃne transformation given data sampled noise underlying nonlinear manifold matrix consisting k-nearest neighbors including terms euclidean distance. consider computing best d-dimensional aﬃne subspace approximation data points consider constructing global coordinates lowdimensional feature space based local coordinates represents local geometry. speciﬁcally want satisfy following equations i.e. global coordinates respect local geometry determined remark. brieﬂy discuss nonlinear alignment idea mentioned particular neighborhood data point consisting data points ﬁrst order taylor expansion ltsa algorithm considered approach approximate solution minimization problem. however seek optimal solution using alternating least squares approach minimize respect minimize respect initial value start alternating least squares obtained ltsa algorithm. details algorithm presented separate paper. remark. minimization problem needs certain constraints well-posed otherwise choose zero. however impose normalization conditions. selected i.e. possibilities. illustrate issue look following minimization problem constructing principal manifolds. global coordinates computed data points apply non-parametric regression construct principal manifold underlying points component functions constructed separately example used simple loess function experiments generating principal manifolds. next section give detailed error analysis estimate errors alignment tangent space approximation terms noise geometric properties generating function density generating coordinates matrix consisting k-nearest neighbors including terms euclidean distance. similar deﬁned denote lowdimensional embedding coordinate matrix computed ltsa algorithm denon-singularity matrix requires jacobi matrix full column rank subspaces span largest left singular vector space span orthogonal other. give quantitative measurement non-singularity degree non-singularity determined curvature manifold rotation singular subspace mainly aﬀected sample noises ǫj’s neighborhood structure xi’s. error bounds clearly show reconstruction accuracy suﬀer manifold underlying data singular near-singular points. phenomenon illustrated numerical examples section finally give error upper bound tangent subspace approximation. results show accurate determination local tangent space dependent several factors curvature information embedded hessian matrices local sampling density noise level regularity jacobi matrix. numerical computation issues. major computational cost ltsa involves computation smallest eigenvectors symmetric positive semideﬁned matrix deﬁned general quite sparse local nature construction neighborhoods. algorithms computing subset eigenvectors large and/or sparse matrices based computing projections onto sequence krylov subspaces form initial vectors hence computation matrix-vector multiplications needs done eﬃciently. special nature computed neighborhood neighborhood without explicitly forming right factor orthogonal projector onto subspace spanned rows ¯xiet qσ)t available orthogonal projector given submatrix ﬁrst columns otherwise compute decomposition clearly rewrite orthogonal projector onto null space spanned rows /√k. therefore matrix-vector product siwiw easily computed follows denote {i··· indices nearest-neighbors j-th element zero form matrix locally summing direct eigen-solver used. otherwise implement routine computes matrix-vector multiplication arbitrary vector experimental results. section present several numerical examples illustrate performance ltsa algorithm. test data sets include curves euclidean spaces surfaces euclidean spaces. especially take closer look eﬀects singular points manifold interaction noise levels sample density. show algorithm also handle data points high-dimensional spaces also consider curves surfaces euclidean spaces dimension equal image data dimension dimension input space randn matlab’s standard normal distribution. figure ﬁrst left right plot colorcoded sample data points corresponding following three one-variable functions also important better understand failure modes ltsa ultimately identify conditions ltsa truly uncover hidden nonlinear structures data points. shown error analysis section diﬃcult align local tangent information pi’s deﬁned close singular. eﬀect computed coordinates neighbors compressed together. clearly demonstrate phenomenon consider following function equal zero case θ-vector deﬁned computed poorly presence noise. usually corresponding small also results small neighbors also small. ﬁrst column plot computed results curve. clearly near singular point computed τi’s become small compressed small interval around zero. second column examine another curve deﬁned notice similar phenomenon also occurs near point curvature curve large computed τi’s near corresponding point also become small clustering around zero. next look issues interaction sampling density noise levels. large noises around relative sampling density near resulting centered local data matrix able provide good local tangent space i.e. singular values close other. result nearly singular matrix plotting phenomenon computed coordinates getting compressed similar case generating function singular and/or near-singular points. however case result usually improved increasing number neighbors used producing shifted matrix normally distributed ﬁrst three columns correspond noise levels respectively. three data sets number neighbors increasing noise level computed τi’s expressed points relatively large noise. quality computed τi’s improved increase number neighbors shown column improved result data column used. shown diﬀerent neighborhood size produce diﬀerent embedding results. general chosen match sampling density noise level curvature data points extract accurate local tangent space. neighbors used result rank-deﬁcient tangent space leading over-ﬁtting large neighborhood introduce much bias computed tangent space match local geometry well. therefore worthy considering variable number neighbors adaptively chosen data point. fortunately ltsa algorithm seems less sensitive choice shown later. random orthogonal matrix resulting orthogonal transformation matrix singular values uniformly distributed resulting aﬃne transformation. figure plots coordinates advantage ltsa using ltsa potentially detect intrinsic dimension underlying manifold analyzing local tangent space structure. particular examine distribution singular values local data matrix consisting data points neighborhood data point manifold dimension close rank-d matrix. illustrate point below. data points peak manifold space. local data matrix j-the singular value next discuss issue global coordinates τi’s means clustering data points xi’s. situation illustrated figure data consists three bivariate gaussians covariance matrices mean vectors located sample points gaussian. thick curve right panel represents principal curve computed ltsa thin curve lle. seen thick curve goes gaussians turn corresponding global coordinates clearly separate three gaussians. perform well mixing gaussians. last look results applying ltsa algorithm face image data data consists sequence -by- pixel images face rendered various pose lighting conditions. image converted dimensional image vector. apply ltsa neighbors constructed coordinates plotted middle figure also extracted four paths along boundaries coordinates display corresponding images along path. seen computed coordinates capture well pose lighting variations continuous way. conclusions feature works. paper proposed algorithm nonlinear manifold learning nonlinear dimension reduction. techniques used construction local tangent spaces represent local geometry global alignment local tangent spaces obtain global coordinate system underlying manifold. provide careful error analysis exhibit interplay approximation accuracy sampling density noise level curvature structure manifold. following list several issues deserve investigation. make ltsa robust noise need resolve issue several smallest eigenvalues magnitude. clearly seen manifold consists several disjoint components. case needs break matrix several diagonal blocks apply ltsa block. however noise situation becomes complicated several eigenvectors corresponding near-zero eigenvalues together information global coordinates seems contained eigen-subspace unscramble eigenvectors extract global coordinate information needs careful analysis eigenvector matrix various models noise. preliminary results problem presented selection points estimate local tangent space crucial success algorithm. ideally want points close tangent space. however noise and/or points curvature manifold large easy task. line ideas preprocessing data points construct restricted local neighborhoods. example ﬁrst compute minimum euclidean spanning tree data restrict neighbors point linked branches spanning tree. idea applied self-organizing another idea iterative-reﬁnement combining computed τi’s xi’s neighborhood construction another round nonlinear projection. rationale τi’s computed global coordinates nonlinear manifold give better measure local geometry. discrete version manifold learning formulated considering data points vertices undirected graph quantization global coordinates speciﬁes adjacency relation vertices manifold learning becomes discovering whether edge created pair vertices resulting vertex neighbors resemble manifold. need investigate proper formulation problem related optimization methods. statistical point view also great interest investigate precise formulation error model associated consistency issues convergence rate sample size goes inﬁnity. learn-ability nonlinear manifold also depends sampling density data points. results non-parametric regression statistical learning theory helpful pursue donoho grimes. isomap recover natural parametrization families articulated images? technical report department statistics stanford university donoho grimes. local isomap perfectly recovers underlying parametrization families occluded/lacunary images. appear ieee computer vision pattern recognition polito perona. grouping dimensionality reduction locally linear embedding. advances neural information processing systems eds. dietterich becker ghahramani press j.o. ramsay b.w. silverman. applied functional data analysis. springer roweis saul. nonlinear dimension reduction locally linear embedding. science", "year": 2002}