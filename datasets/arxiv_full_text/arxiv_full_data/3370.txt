{"title": "Integrated perception with recurrent multi-task neural networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for \"all\" perceptual problems together, solving them efficiently and coherently in an \"integrated manner\". In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call \"MultiNet\", in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.", "text": "modern discriminative predictors shown match natural intelligences speciﬁc perceptual tasks image classiﬁcation object part detection boundary extraction etc. however major advantage natural intelligences still work well perceptual problems together solving efﬁciently coherently integrated manner. order capture advantages machine perception questions whether deep neural networks learn universal image representations useful single task them solutions different tasks integrated framework. answer proposing architecture call multinet deep image features shared tasks tasks interact recurrent manner encoding results analysis common shared representation data. manner show performance individual tasks standard benchmarks improved ﬁrst sharing features then signiﬁcantly integrating solutions common representation. natural perception extract complete interpretations sensory data coherent efﬁcient manner. contrast machine perception remains collection disjoint algorithms solving speciﬁc information extraction sub-problems. recent advances modern convolutional neural networks dramatically improved performance machines individual perceptual tasks remains unclear could integrated seamless natural perception does. paper consider problem learning data representations integrated perception. ﬁrst question whether possible learn universal data representations used solve sub-problems interest. computer vision ﬁne-tuning retraining show effective method transfer deep convolutional networks different tasks show that fact possible learn single shared representation performs well several sub-problems simultaneously often well even better specialised ones. second question complementary feature sharing different perceptual subtasks combined. since subtask extracts partial interpretation data problem form coherent picture data whole. consider incremental interpretation scenario subtasks collaborate parallel sequentially order gradually enrich shared interpretation data contributing dimension informally many computer vision systems operate stratiﬁed manner different modules running parallel sequence question done end-to-end systematically. paper develop architecture multinet provides answer questions. multinet builds idea shared representation called integration space reﬂects figure multinet. propose modular multi-task architecture several perceptual tasks integrated synergistic manner. subnetwork encodes data producing representation shared different tasks. task estimates different labels using decoder functions dec. task contributes back shared representation means corresponding encoder function enc. loop closed recurrent conﬁguration means suitable integrator functions statistics extracted data well result analysis carried individual subtasks. loose metaphor think integration space canvas progressively updated information obtained solving sub-problems. representation distills information makes available task resolution recurrent conﬁguration. multinet several advantages. first learning latent integration space automatically synergies tasks discovered automatically. second tasks treated symmetric manner associating encoder decoder integrator functions making system modular easily extensible tasks. third architecture supports incremental understanding tasks contribute back latent representation making output available tasks processing. finally multinet applied image understanding setting architecture general could applied numerous domains well. architecture described detail section instance specialized computer vision applications given section empirical evaluation section demonstrates beneﬁts approach including sharing features different tasks economical also sometimes better accuracy integrating outputs different tasks shared representation yields accuracy improvements. section summarizes ﬁndings. multiple task learning multitask learning methods studied decades machine learning community. methods based idea tasks share common low-dimensional representation jointly learnt task speciﬁc parameters. trains many tasks parallel mitchell thrun propose sequential transfer method called explanation-based neural nets exploits previously learnt domain knowledge initialise constraint parameters current task. breiman freidman devise hybrid method ﬁrst learns separate models improves generalisation exploiting correlation predictions. multi-task learning computer vision shown improve results many computer vision problems. typically researchers incorporate auxiliary tasks target tasks jointly train parallel achieve performance gains object tracking object detection facial landmark detection differently propose multi-task network cascades convolutional layer parameters shared three tasks tasks predicted sequentially. unlike method train multiple tasks parallel require speciﬁcation task execution. recurrent networks work also related recurrent neural networks successfully used language modelling speech recognition hand-written recognition semantic image segmentation human pose estimation related work carreira propose iterative segmentation model progressively updates initial solution feeding back error signal. najibi propose efﬁcient grid based object detector iteratively reﬁne predicted object coordinates minimising training error. methods also based iterative solution correcting mechanism main goal improve generalisation performance multiple tasks sharing previous predictions across learning output correlations. propose recurrent neural network architecture address simultaneously multiple data labelling tasks. symmetry drop usual distinction input output spaces consider instead label spaces label α-th space denoted symbol following used input network inferred whereas labels estimated neural network reason useful keep notation symmetric possible ground label treat input instead. task associated corresponding encoder function vectorial representation given information extracted data different tasks encoders integrated shared representation using integrator function since update operation incremental associate iteration number update equation written note that equation above constant corresponding variable input network grounded updated. overall task speciﬁed triplet contribution update rule full task modularity achieved decomposing integrator function sequence task-speciﬁc updates task quadruplet shared representation initialized symbol denotes initial value variable given output enc) task computed. step corresponds ordinary multi-task prediction discussed later idea feeding back network output processing exists several existing recurrent architectures however cases used process sequential data passing back output obtained last process element sequence; here instead feedback used integrate different complementary labelling tasks. model also reminiscent encoder/decoder architectures however case encoder decoder functions associated output labels rather input data. ordinarily multiple-task learning based sharing features parameters different tasks. multinet reduces ordinary multi-task learning recurrence. dec◦φ ﬁrst iteration fact multinet simply evaluates predictor functions task share common subnetwork multi-task learning representation sharing conceptually simple practically important allows learning universal representation function works well tasks simultaneously. possibility learning polyvalent representation veriﬁed empirically non-trivial useful fact. particular experiments image understanding that certain image analysis tasks possible efﬁcient learn shared representation cases feature sharing even improve performance individual sub-problems. section instantiate multinet three complementary tasks computer vision object classiﬁcation object detection part detection. main advantage multinet compared ordinary multi-task prediction that sharing parameters across related tasks improve generalization enough capture correlations task input spaces. example computer vision application ordinary multi-task prediction would able ensure detected parts contained within detected object. multinet instead capture interactions different labels potentially learn enforce constraints. latter done soft distributed manner integrating back output individual tasks shared representation. next discuss detail speciﬁc architecture components used application. starting point consider standard image classiﬁcation. powerful networks exist choose good performing model time reasonably efﬁcient train evaluate namely vgg-m- network model pre-trained image classiﬁcation imagenet ilsvrc data extended object detection; follow blueprints particular fast r-cnn method design subnetworks three tasks. components described detail below ﬁrst focusing components corresponding ordinary multi-task prediction moving ones used multiple task integration. ordinary multiple-task components. ﬁrst several layers vgg-m network grouped convolutional sections comprising linear convolution non-linear activation function cases pooling normalization. followed three fullyconnected sections convolutional ones ﬁlter support size corresponding input. last layer softmax computes posterior probability vector imagenet ilsvrc classes. vgg-m adapted different tasks follows. clarity symbolic names tasks rather numeric indexes consider {img part} instead convolutional sections vgg-m used image encoder φimg hence compute initial value shared representation. cutting vgg-m level last convolutional layer motivated fact fully-connected layers remove least dramatically blur spatial information whereas would like preserve object part localization. hence shared representation tensor rh×w×c spatial dimensions number feature channels determined vgg-m conﬁguration next φimg task image classiﬁcation object detection part detection image classiﬁcation rest original vgg-m network image classiﬁcation. branch choose words decoder function ψcls image-level labels initialized fully-connected layers original vgg-m φvgg-m however differences. ﬁrst last fully-connected layer reshaped reinitialized randomly predict different number possible objects instead imagenet classes. second difference ﬁnal output vector binary probabilities obtained using sigmoid instead softmax. object part detection decoders instead based fast r-cnn architecture classify individual image regions belonging object classes background. selective search windows method used generate shortlist region proposals image ximg; inputted extracts subsets spatial pyramid pooling layer ψspp feature correspondence region using pooling. object detection decoder given ψdet contains fully connected layers initialized manner classiﬁcation decoder exception last layer reshaped reinitialized needed whereas softmax still used regions class. described image encored φimg three tasks. components sufﬁcient ordinary multi-task learning corresponding initial multinet iteration. next specify components allow iterate multinet several times. recurrent components integrating multiple tasks. task integration need construct encoder functions φcls task well integrator function several constructions possible experiment simple ones. takes vector ccls binary order encode image label xcls encoder rcls φcls probabilities xcls rccls ccls possible object classes broadcasts corresponding values spatial locations formally rcls rh×w×ccls encoding object detection label xdet similar reﬂects geometric information captured labels. particular bounding extracted associated vector rccls+. ccls probabilities xdet decoded heat rcls rh×w× pooling across boxes part label xpart encoded entirely analogous manner. lastly need construct integrator function experiment simple designs. ﬁrst simply stacks evidence different sources stack. update equation given r×××c ﬁlter bank whose purpose reduce stacked representation back original channels. useful design maintains representation dimensionality regardless number tasks added. however compression perform less well. image encoder φimg initialized pre-trained vgg-m model using sections conv conv. input network image ximg rhimg×w img× then downsampling spatial dimension rimg φimg himg/ img/. number feature channels noted above decoders contain respectively part comprising layers vgg-m followed subnetworks ψdec randomly-initialized linear predictor output dimension equal respectively ccls ccls cpart pooling performed grid spatial bins task encoders φcls training task associated corresponding loss function. classiﬁcation task objective minimize negative posterior log-probabilities whether image contains certain object type combined fact classiﬁcation branch uses sigmoid binary logistic regression. object part detection tasks decoders optimized classify target regions ccls cpart classes background furthermore also train branch performing bounding reﬁnement improve selective search region proposed fully connected layers used softmax classiﬁcation bounding-box regression object part detection tasks initialized zero-mean gaussian distributions standard deviations respectively. fully connected layers used object classiﬁcation task adaptation layer initialized zero-mean gaussian standard deviation. layers learning rate ﬁlters biases. used optimize parameters learning rate epochs lower another epochs. observe running iterations recursion sufﬁcient reach performance although marginal gains possible more. publicly available toolbox matconvnet experiments. pascal parts dataset contains training validation images object categories ground truth bounding annotations target categories. pascal-part dataset obtain bounding annotations object parts consists annotated part categories aeroplane engine bicycle back-wheel bird left-wing person right-upper-leg. removing annotations smaller pixels side categories less training samples number part categories reduces dataset provides annotations training validation splits thus train models train split report results validation split tasks. follow standard pascal evaluation report average precision intersection-over-union detected boxes ground ones object classiﬁcation detection respectively. part detection follow report relaxed threshold. results tasks reported table order establish ﬁrst baseline train independent network task. network initialized vgg-m model last classiﬁcation regression layers initialized random noise layers ﬁne-tuned respective task. object part detection implementation fast-rcnn note that consistency baselines method minimum dimension image scaled pixels tasks including object classiﬁcation. layer employed scale feature dimensionality. second baseline train multi-task network shares convolutional layers across tasks observe table multi-task model performs comparable better independent networks efﬁcient shared convolutional computations. since training images cases shows combining multiple labels together improves efﬁciency cases even performance. finally test full multinet model settings deﬁned update rules corresponding respectively. ﬁrst models outperforms independent networks multi-task network well. remarkable model consists smaller number parameters three independent networks best model consistently outperforms roughly points mean furthermore multinet improves ordinary multi-task prediction exploiting correlations solutions individual tasks. addition observe update performs better update constraints shared representation space dimensions regardless number tasks expected larger capacity. nevertheless even bottleneck observe improvements compared ordinary multi-task prediction. also test case verify whether multinet learns information extracted various tasks presumed. exploit predictions performed task able improve ground truth labels test time. test time ground classiﬁcation label rcls ﬁrst iteration multinet ground truth class labels read predictions iteration. performances expectedly three tasks improve respectively. shows that feedback class information strong effect class prediction itself modest nevertheless signiﬁcant effect tasks well. pascal dataset consists training validation test images containing bounding annotations object categories. part annotations available dataset thus exclude part detection task baselines best model object classiﬁcation detection. results reported test split depicted table note rcnn individual networks obtains detection score paper presented multinet recurrent neural network architecture solve multiple perceptual tasks efﬁcient coordinated manner. addition feature parameter sharing common multi-task learning methods multinet combines output different tasks updating shared representation iteratively. results encouraging. first shown architectures successfully integrate multiple tasks sharing large subset data representation matching even outperforming specialised network. second shown iterative update common representation effective method sharing information different tasks improve performance.", "year": 2016}