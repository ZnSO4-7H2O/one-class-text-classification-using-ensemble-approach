{"title": "The Consciousness Prior", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A new prior is proposed for representation learning, which can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by the phenomenon of consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought, i.e., consciousness as awareness at a particular time instant. This provides a powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true, highly probable, or very useful for taking decisions. The fact that a few elements of the current state can be combined into such a predictive or useful statement is a strong constraint and deviates considerably from the maximum likelihood approaches to modelling data and how states unfold in the future based on an agent's actions. Instead of making predictions in the sensory (e.g. pixel) space, the consciousness prior allows the agent to make predictions in the abstract space, with only a few dimensions of that space being involved in each of these predictions. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in the form of facts and rules, although the conscious states may be richer than what can be expressed easily in the form of a sentence, a fact or a rule.", "text": "prior proposed representation learning combined priors order help disentangling abstract factors other. inspired phenomenon consciousness seen formation low-dimensional combination concepts constituting conscious thought i.e. consciousness awareness particular time instant. provides powerful constraint representation low-dimensional thought vectors correspond statements reality either true highly probable useful taking decisions. fact elements current state combined predictive useful statement strong constraint deviates considerably maximum likelihood approaches modeling data states unfold future based agent’s actions. instead making predictions sensory space consciousness prior allow agent make predictions abstract space dimensions space involved predictions. consciousness prior also makes natural conscious states natural language utterances express classical knowledge form facts rules although conscious states richer expressed easily form sentence fact rule. propose kind prior top-level abstract representations inspired understanding consciousness form awareness i.e. deﬁned locke consciousness perception passes man’s mind awareness external object something within oneself proposal based regularization term encourages top-level representation sparse attention mechanism focuses elements state representation small variables agent aware given moment combined make useful statement reality usefully condition action policy. refer elusive meanings attributed word consciousness sticking instead notion attentive awareness moment ability focus information minds accessible verbal report reasoning control behaviour. call representation encoder representation state. core objective learn good representations disentangles abstract explanatory factors sense exist simple transformation select information single factor think representation content almost whole brain time i.e. representation state high-dimensional vector abstract representation full current information available agent thus summarizing current recent past observations. contrast deﬁne conscious state low-dimensional vector derived form attention mechanism applied taking account previous conscious state context. random noise source. cognitive interpretation value corresponds content thought small subset information available unconsciously brought awareness particular form attention picks several elements projections function consciousness random noise inputs produces random choice elements attention gets focused. useful think consciousness tool exploring interpretations plans sample predictions future. also think consciousness tool isolate particular high-level abstraction extract information would happen think single factor general aggregate factors complex composed thought. capture assumption conscious thought encapsulate statement future introduce veriﬁer network match current representation state past conscious state ct−k generally would like deﬁne objective function embodies idea attended elements used whose value quantiﬁed optimized i.e. representation trained optimize objective function well possibly objectives able reconstruct input supervised unsupervised objective want throw independently controllable factors prior distinct mechanisms play contribute high-level state representation objective function attention mechanism selects combines elements high-level state representation low-dimensional conscious substate object predictions actions derived sequence conscious sub-states. second mechanism easy grasp frame standard practice either deep learning e.g. supervised unsupervised tasks. example attention mechanism could select elements current representation state choose make prediction future elements improve quality prediction mechanism want maximize logp proxy e.g. using variational auto-encoder objective conditional wants sample accurately note objective function used learn mapping also drives learning representation function itself i.e. back-propagated representation rnn). however part objective function sufﬁcient fact appropriate train attention mechanism indeed driving objective attention learner would always pick trivially predictable remains open question objectives would appropriate learning attend useful elements ultimately able actual reward learning agent purpose form entropy diversity needed would convenient consciousness attention mechanism veriﬁer network able refer names variables prediction made. models already distinguish keys values variations memory augmented neural networks conscious state must indirectly refer aspects dimensions computed representation wether done explicitly implicitly remains determined. key-value mechanism also makes easier veriﬁer network must match predicted variable instances future representation state value mixed predicted value differs substantially observed value simple associative process might miss opportunity match thus provide strong training signal linked interesting property conscious state fairly simple transformation natural language sentence externally provided sentence could also elicit associated conscious state although postulate conscious state generally richer object uttered sentence i.e. mapping conscious states sentences loses information sentence could thus interpreted differently depending context particulars agent reads sentence. formally could another conscious state utterance learning agent uses language could thus beneﬁt additional regularization term currently consciously attended elements often mapped something like sentence natural language uttered another agent human teacher. sentence focuses handful elements concepts unlike full internal state. imposes constraints representation function individual elements dimensions likely correspond concepts typically expressed single word phrase. based arguments reasonable hypothesize language actually help humans build sharper internal representations well facilitate learning arguments around curriculum learning cultural learning enable collaborative task-solving. along line research opens door possibility better connecting deep learning classical symbolic cognitive science move deep learning perception higher-level cognition knowledge representation example knowledge classically represented facts rules sharp statement reality involving concepts. nugget information knowledge seems well conscious state. combining conscious states sequentially order make complex predictions inferences actions basically reasoning about. however implying return symbolic forms knowledge representation well-known limitations. instead consider form regularization representations captured deep learning agents could many attributes classical facts rules keeping richer representation needed inference planning presence uncertainty non-discrete aspects world. progress direction would also address often expressed concern obtaining explanations deep nets since approach proposed would make easier trained agent communicate verbally high-level state. novel theory developped many different ways important start simple experiments allowing test evaluate qualitatively different approaches turnaround time experiment short analysis representations learned although working natural language input would likely help agent learn better abstract representations would better start experiments linguistic input make sure training objective training framework alone leading discovery appropriate high-level features. example learning form intuitive physics done babies without need linguistic guidance. similarly although consciousness prior could used supervised learning task-oriented testing ability alone discover high-level abstractions would best done context unsupervised e.g. using intrinsic reward favours discovery environment works. would interesting learning task involve meaningful abstractions high predictive power. example consider predicting whether pile blocks fall table. involves high-level discrete outcome predicted easily even details blocks fall difﬁcult even humans predict. case predicting future pixel level would extremely difﬁcult future states high entropy highly multi-modal distribution. however aspects future entropy. addition aspects impact predicting come next consciousness prior useful. terms experimental comparisons would good compare systems based consciousness prior systems based common approaches policy gradient deep hand model-based hand even better problems compute oracle solution upper bound best achievable performance. author wants thank philippe beaudoin gerry william fedus devon hjelm anirudh goyal preliminary discussions consciousness prior well funding nserc cifar canada research chairs open philanthropy project.", "year": 2017}