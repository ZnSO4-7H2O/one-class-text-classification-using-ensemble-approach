{"title": "Simple random search provides a competitive approach to reinforcement  learning", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.", "text": "common belief model-free reinforcement learning methods based random search parameter space policies exhibit signiﬁcantly worse sample complexity explore space actions. dispel beliefs introducing random search method training static linear policies continuous control problems matching state-ofthe-art sample eﬃciency benchmark mujoco locomotion tasks. method also ﬁnds nearly optimal controller challenging instance linear quadratic regulator classical problem control theory dynamics known. computationally random search algorithm least times eﬃcient fastest competing model-free methods benchmarks. take advantage computational eﬃciency evaluate performance method hundreds random seeds many diﬀerent hyperparameter conﬁgurations benchmark task. simulations highlight high variability performance benchmark tasks suggesting commonly used estimations sample eﬃciency adequately evaluate performance algorithms. model-free reinforcement learning aims oﬀer oﬀ-the-shelf solutions controlling dynamical systems without requiring models system dynamics. methods successfully produced agents surpass human players video games games although results impressive model-free methods successfully deployed control physical systems outside research demos. several factors prohibiting adoption model-free methods controlling physical systems methods require much data achieve reasonable performance ever-increasing assortment methods makes diﬃcult choose best method speciﬁc task many candidate algorithms diﬃcult implement deploy unfortunately current trend research impediments odds other. quest methods sample eﬃcient general trend develop increasingly complicated methods. increasing complexity lead reproducibility crisis. recent studies demonstrate many methods robust changes hyperparameters random seeds even diﬀerent implementations algorithm algorithms fragilities cannot integrated mission critical control systems without signiﬁcant simpliﬁcation robustiﬁcation. small number independent trials popular continuous control benchmarks mujoco locomotion tasks humanoid model considered challenging continuous control problems solvable state-of-the-art techniques principle video games simulated control problems beta testing ideas simple baselines established thoroughly evaluated moving towards complex solutions. determine simplest model-free method solve standard benchmarks. recently diﬀerent directions proposed simplifying salimans introduced derivative-free policy optimization method called evolution strategies authors showed that several tasks method easily parallelized train policies faster methods. method proposed salimans simpler previously proposed methods employs several complicated algorithmic elements discuss section second simpliﬁcation model-free rajeswaran shown linear policies trained natural policy gradients obtain competitive performance mujoco locomotion tasks showing complicated neural network policies needed solve continuous control problems. work combine ideas work salimans rajeswaran obtain simplest model-free method derivativefree optimization algorithm training linear policies. demonstrate simple random search method match exceed state-of-the-art sample eﬃciency mujoco locomotion benchmarks. moreover method least times computationally eﬃcient fastest competing method. ﬁndings contradict common belief policy gradient techniques rely exploration action space sample eﬃcient methods based ﬁnite-diﬀerences detail contributions follows section present classical basic random search algorithm solving derivative-free optimization problems. application continuous control augment basic random search method three simple features. first scale update step standard deviation rewards collected computing update step. second normalize system’s states online estimates mean standard deviation. third discard computation update steps directions yield least improvement reward. refer method augmented random search section evaluate performance benchmark mujoco locomotion tasks. method learn static linear policies achieve high rewards mujoco tasks. control action linear current states alone. neural networks used state-of-the-art performance still uniformly achieved. example humanoid model ﬁnds linear policies achieve average rewards highest reward reported literature. equal footing competing methods evaluated required sample complexity solve mujoco locomotion tasks three random seeds uniformly sampled interval. compare measured performance method results reported haarnoja rajeswaran salimans schulman matches exceeds state-of-the-art sample eﬃciency mujoco locomotion tasks. section report time computational resources required train policies humanoid-v task. measure time required reach average reward more results reported hundred random seeds. machine cpus takes minutes random seeds takes minutes random seeds. training policies humanoid-v task reach reward threshold takes modern hardware popular trust region policy since method eﬃcient previous approaches able explore variance method many random seeds. algorithms exhibit large training variances hence evaluations small number random seeds accurately capture performance. henderson islam already discussed importance measuring performance algorithms many random seeds sensitivity methods choices hyperparameters. thorough evaluation method measured performance hundred random seeds also evaluated sensitivity hyperparameter choices. though successfully trains policies mujoco locomotion tasks large fraction time hyperparameters random seeds varied note still exhibits large variance still frequently learned policies uniformly yield high rewards. order simplify streamline evaluation continuous control argue important baselines extensible reproducible. section argue using linear quadratic regulator unknown dynamics benchmark. evaluate performance hundred random seeds diﬃcult instance problem. although sample eﬃcient model-based methods ﬁnds nearly optimal solutions instance considered. recent adoption standard benchmark suites large body recent research applied methods continuous control inside simulation environments. levine koltun among ﬁrst mujoco testbed learning based control able achieve walking complex simulators without special purpose techniques. since then simulation engine used variety diﬀerent researchers diﬀerent contexts compare techniques. list many approaches here highlighting beneﬁts comparisons approaches listed assessed small random seeds often using unclear methods hyperarameter selection. henderson islam pointed methodology accurately capture performance methods sensitive choice random seed choice hyperarameters. mnih showed actor-critic methods popular variance reduction policy gradient algorithms asynchronously parallelized fast training policies atari video games mujoco models. previously schulman introduced generalized advantage estimation method estimating advantages oﬀering variance reduction less bias previous methods. popular trust region policy optimization algorithm related natural gradient method. trpo introduced schulman maximizes iteration approximate average reward objective regularized kl-divergence penalty. scalable trust region method proposed actor critic method uses kronecker-factor trust regions recently schulman introduced proximal policy optimization successor trpo easier implement better sample complexity. training policies locomotion tasks obstacles heess proposed distributed version ppo. diﬀerent direction towards sample eﬃciency oﬀ-policy methods q-learning designed data collected system regardless policies used data generation. silver expanding work degris combined ideas actor-critic framework method training deterministic policies relying exploratory polihigh variance gradient estimation hurdle policy gradients methods need surpass. optimization problems occurring highly non-convex leading many methods suboptimal local optima. address issue haarnoja proposed soft q-learning algorithm learning multi-modal stochastic policies entropy maximization leading better exploration environments multi-modal reward landscapes. recently haarnoja combined idea actor-critic framework soft actor-critic algorithm oﬀ-policy actor-critic method actor aims maximize expected reward entropy stochastic policy. diﬀerent direction rajeswaran used linear policies simplifying search space. used natural gradients policy gradients adapted metric parameter space policy train linear policies mujoco locomotion tasks. methods rely exploration action space model-free methods perform exploration parameter space policies. traditional ﬁnite diﬀerence gradient estimation model-free uses coordinate aligned perturbations policy weights linear regression measurement aggregation method based ﬁnite diﬀerences along uniformly distributed directions; inspired derivative free optimization methods analyzed nesterov spokoiny similar evolution strategies algorithm convergence random search methods derivative free optimization understood several types convex optimization jamieson oﬀer information theoretic lower bound derivative free convex optimization show coordinate based random search method achieves lower bound nearly optimal dependence dimension. although eﬃciency ﬁnite diﬀerence random search methods derivative free convex optimization proven theoretically methods perceived ineﬃcient applied nonconvex problems oﬀer evidence contrary. solving problems reinforcement learning requires ﬁnding policies controlling dynamical systems goal maximizing average reward given tasks. problems abstractly formulated environment i.e. random initial states stochastic transitions. value reward achieved policy trajectory generated system. general could stochastic policies proposed method uses deterministic policies. note problem formulation aims optimize reward directly optimizing policy parameters consider methods explore parameter space rather action space. choice renders training equivalent derivative-free optimization noisy function evaluations. simplest oldest optimization methods derivative-free optimization random search random search chooses direction uniformly random sphere parameter space optimizes function along direction. primitive form random search simply computes ﬁnite diﬀerence approximation along random direction takes step along direction without using line search. method i.i.d. random variables positive real number zero mean gaussian vector. known update increment unbiased estimator gradient respect eδeξ smoothed version objective close original objective small function evaluations noisy minibatches used reduce variance gradient estimate. basic random search algorithm outlined algorithm evolution strategies version algorithm several complicated algorithmic enhancements called bandit gradient descent flaxman note many names algorithm least years rediscovered variety diﬀerent optimization communities. introduce oracle model quantify information system used many methods. algorithm query oracle sending proposed policy then oracle samples random variable independent past generates trajectory system according policy randomness then oracle returns represent trajectory generated system according policy query called episode rollout. goal algorithms approximately solve problem making calls oracle possible. number oracle queries needed solving problem called oracle complexity sample complexity. note policy gradient methods ﬁnite diﬀerence methods implemented oracle model. approaches access information system rollouts ﬁxed policies associated states rewards. question whether approach making better information other? ﬁrst version method obtained scaling update steps standard deviation rewards collected iteration motivate scaling oﬀer intuition section shown section train linear policies achieve reward thresholds previously proposed literature swimmer-v hopper-v halfcheetah-v walkerd-v ant-v tasks. however requires larger number episodes training policies tasks cannot train policies humanoid-v task. address issues algorithm also propose trains policies linear maps states normalized mean standard deviation computed online. explain procedure section enhance performance introduce third algorithmic enhancement shown algorithm v-t. versions drop perturbation directions yield least improvement reward. motivate algorithmic element section training policies progresses random search parameter space policies lead large variations rewards observed across iterations. result diﬃcult choose ﬁxed step-size allow harmful changes large small steps. salimans address issue transforming rewards rankings using adaptive optimization algorithm adam computing update step. techniques change direction updates obfuscating behavior algorithm making diﬃcult ascertain objective evolution strategies actually optimizing. steps standard deviation rewards collected iteration understand eﬀect scaling plot standard deviations obtained training policy humanoid-v model figure standard deviations increasing trend training progresses. behavior occurs perturbations policy weights high rewards cause humanoid-v fall early yielding large variations rewards collected. therefore without scaling method iteration would taking steps thousand times larger beginning training. eﬀect scaling could probably obtained tuning step-size schedule. however goal minimize amount tuning required thus opted scaling standard deviation. normalization states used akin data whitening used regression tasks intuitively ensures policies equal weight diﬀerent components states. gain intuition might help suppose state coordinate takes values range control gain respect ﬁrst state coordinate would lead larger changes actions sized changes respect second state component. hence whitening allows isotropic exploration random search equal inﬂuence various state components. previous work also implemented state normalization ﬁtting neural network model several mujoco environments similar normalization used part virtual batch normalization neural network policies course implement eﬃcient require storage states. also keep track diagonal σj+. finally ensure ratio treated diagonal entry smaller make equal case state normalization seen form non-isotropic exploration parameter space linear policies. particular policy weights perturbation direction main empirical motivation version method comes humanoid-v task. able train linear policy task without normalization states described algorithm moreover measured sample complexity better mujoco locomotion tasks well shown section hand note impractical linear quadratic regulator problem discussed section size states grows exponentially fast function trajectory length policy stabilize system. section show matches exceeds state-of-the-art performance tasks swimmer-v hopper-v halfcheetah-v humanoid-v. however training walkerd-v ant-v models requires three times rollouts competing methods. improve performance propose v-t. update steps used perturbation direction weighted diﬀerence rewards rewards obtained queries oracle described section using policies update steps push policy weights update steps push policy direction weights direction −δk. however since noisy evaluations performance policies parametrized might push weights direction even better vice versa. moreover perturbation directions updating policy weights either direction would lead sub-optimal performance. example rewards small compared observed rewards might suggest moving either direction would decrease avperturbation directions according max{r algorithmic enhancement intuitively improves update steps update steps average directions obtained high rewards. however without theoretical investigation cannot certain eﬀect using algorithmic enhancement versions equivalent versions therefore certain tuning hyperparameters perform worse section show exceeds matches state-of-the-art performance mujoco locomotion tasks included openai gym. uses ranks compute update steps. rankings used make training robust. instead method scales update steps standard deviation rewards. bins action space swimmer-v hopper-v encourage exploration. implemented parallel version algorithm using python library avoid computational bottleneck communicating perturbations created shared noise table stores independent standard normal entries. then instead communicating perturbations workers communicate indices shared noise table. approach used implementation moritz similar approach proposed salimans code sets random seeds random generators workers copies openai environments held workers. random seeds distinct function single integer refer random seed. furthermore made sure states rewards produced evaluation rollouts used form training. evaluate performance mujoco locomotion tasks included openai gymv.. openai provides benchmark reward functions diﬀerent mujoco locomotion tasks. used default reward functions evaluating performance linear policies trained ars. reported rewards obtained policy averaged independent rollouts. hopper-v walkerd-v ant-v humanoid-v tasks default reward functions include survival bonus rewards agents constant reward timestep long termination condition reached. example environment humanoid-v awards reward time steps long humanoid model fall. hence humanoid model stands still timesteps receive reward minus small penalty actions used maintain vertical position. furthermore humanoid falls forward rollout receive reward higher common practice report sample complexity method showing number episodes required reach reward threshold example chose threshold rajeswaran chose threshold however given survival bonus awarded humanoid-v believe reward thresholds meaningful locomotion. table section reward threshold evaluate performance humanoid-v task threshold also used salimans survival bonuses awarded openai discourage exploration policies cause falling early needed discovery policies achieve locomotion. bonuses cause policies make mujoco models stand still thousand timesteps; policies likely local optima. bonuses probably included reward functions help training stochastic policies since policies cause constant movement stochastic actions. resolve local optima problem training deterministic policies ﬁrst evaluated performance three random seeds hyperparameter tuning. evaluation three random seeds widely adopted literature hence wanted equal footing competing methods. then evaluated performance random seeds thorough estimation performance. finally also evaluated sensitivity method changes hyperparameters. three random seeds evaluation compared diﬀerent versions following methods trust region policy optimization deep deterministic policy gradient natural gradients evolution strategies proximal policy optimization soft actor critic soft q-learning cross entropy method performance methods used values reported rajeswaran salimans schulman haarnoja rajeswaran schulman evaluated performance algorithms three random seeds salimans haarnoja used random seeds respectively. methods equal footing evaluation sampled three random seeds uniformly interval ﬁxed them. popular mujoco locomotion tasks chose grid hyperparameters shown appendix hyperarameters three times three ﬁxed random seeds. table shows average number episodes required trpo reach prescribed reward threshold using values reported rajeswaran trpo. version mujoco task chose hyperparameters minimize average number episodes required reach reward threshold. corresponding training curves shown figure mujoco tasks except humanoid-v used reward thresholds rajeswaran choice increase reward threshold humanoid-v motivated presence survival bonuses discussed section table comparison trpo mujoco locomotion tasks. task show average number episodes required achieve prescribed reward threshold averaged three random seeds. estimated number episodes required reach reward humanoid-v based learning curves presented rajeswaran table shows train policies mujoco locomotion tasks except humanoid-v successfully solved secondly note reaches prescribed thresholds swimmer-v hopper-v halfcheetah-v tasks faster figure evaluation four versions mujoco locomotion tasks. training curves averaged three random seeds shaded region shows standard deviation. shown tasks oﬀered improvement trpo matches performance humanoid-v task. walkerd-v ant-v tasks outperformed nonetheless note surpasses performance tasks. although trpo hits reward threshold walkerd-v faster metrics surpasses trpo. table shows maximum reward achieved trpo million timesteps simulator collected averaged three ﬁxed random seeds. hyperparameters chosen based evaluations performed table figure schulman report performance trpo ant-v humanoid-v tasks openai gym. table shows surpasses four methods swimmer-v hopper-v halfcheetah-v tasks. walkerd-v task achieves higher average maximum reward achieves similar maximum reward trpo. table comparison trpo mujoco locomotion tasks. task show maximum rewards achieved prescribed number simulator timesteps used averaged three random seeds. values trpo approximated based ﬁgures presented schulman chosen based evaluations performed table figure table shows surpasses ddpg trpo hopper-v walkerd-v tasks surpassed ddpg halfcheetah-v taks. however performs better trpo task. ant-v task surpassed performs similarly outperforms ddpg trpo. include values swimmer-v humanoid-v haarnoja openai versions tasks evaluation. instead evaluated rllab version tasks. authors indicated humanoid-v challenging rllab version parametrization states used openai swimmer-v challenging reward function used. table comparison ddpg trpo mujoco locomotion tasks. task show maximum rewards achieved prescribed number simulator timesteps used. values averaged three random seeds. values ddpg trpo approximated based ﬁgures presented haarnoja evaluated methods random seeds. table shows number timesteps required reach prescribed reward threshold averaged three ﬁxed random seeds. hyperparameters chosen based evaluations performed table figure compare trpo. methods show values reported salimans used random seeds evaluation. salimans report sample complexity results ant-v humanoid-v tasks. table shows trpo requires fewer timesteps reach prescribed reward threshold walkerd-v. however requires fewer timesteps trpo swimmer-v hopper-v halfcheetah-v tasks. table comparison trpo methods mujoco locomotion tasks. task show average number timesteps required reach prescribed reward threshold averaged three random seeds. swimmer-v used tasks used v-t. values trpo averaged random seeds taken salimans evaluate ant-v specify exact number timesteps required train humanoid-v. hundred seeds evaluation evaluating three random seeds shows overall method sample eﬃcient ddpg trpo methods mujoco locomotion tasks. however well known algorithms exhibit high training variance thorough evaluation sampled distinct random seeds uniformly random interval then using hyperparameters selected table figure mujoco locomotion tasks random seeds. thorough evaluation feasible small computational footprint discussed section results shown figure figure shows time trains policies mujoco locomotion tasks exception walkerd-v succeeds time. moreover succeeds training policies large fraction time using competitive number episodes. figure evaluation random seeds mujoco locomotion tasks. dotted lines represent median rewards shaded regions represent percentiles. swimmer-v used hopper-v walkerd-v ant-v used v-t. halfcheetah-v humanoid-v used types random seeds used figure cause reach high rewards. random seeds eventually ﬁnds high reward policies suﬃciently many iterations performed random seeds lead discover locally optimal behaviors. humanoid model found numerous distinct gait including ones humanoid hopes walks backwards moves swirling motion. gaits found random seeds cause slower training multiple gaits humanoid models previously observed evaluation better emphasizes prevalence. results emphasize importance evaluating algorithms many random seeds since evaluations small numbers seeds cannot correctly capture ability algorithms good solutions highly non-convex optimization problems. finally figure shows least sensitive random seed used applied halfcheetah-v problem. achieved higher reward task haarnoja evaluated sensitivity random seeds halfcheetah-v. sensitivity hyperparameters correctly noted literature methods sensitive hyperparameter choices hopes apply practice example ddpg known highly sensitive hyperparameter choices making diﬃcult practice evaluations presented used hyperparameters chosen tuning three ﬁxed random seeds. determine sensitivity choice hyperarameters figure plot median performance hyperparameters considered tuning three ﬁxed random seeds. recall grids hyperparameters used diﬀerent mujoco tasks shown appendix interestingly success rates depicted figure similar shown figure figure shows decrease median performance ant-v humanoid-v. similarity figures shows success inﬂuenced choice hyperparameters choice random seeds. another highly sensitive choice hyperparameters success rate varying hyperarameters similar success rate performing independent trials good choice hyperparameters. finally figure shows performance halfcheetah-v task problem often used evaluations sensitivity least sensitive choice hyperparameter. figure evaluation sensitivity choice hyperparameters. dotted lines represent median average reward shaded regions represent percentiles. used learning curves collected hyperparameter tuning performed evaluation three ﬁxed random seeds. swimmer-v used rest environments used linear policies suﬃciently expressive mujoco evaluation random seeds discussed linear policies produce diverse gaits mujoco models showing linear policies suﬃciently expressive capture diverse behaviors. moreover table shows linear policies achieve high rewards mujoco locomotion tasks. particular humanoid-v walkerd-v found policies achieve signiﬁcantly higher rewards results encountered literature. results show linear policies perfectly adequate mujoco locomotion tasks reducing need expressive computationally expensive policies. mujoco locomotion tasks considered popular benchmarks literature shortcomings. maximal achievable awards unknown optimal policies current state-of-the-art indeed suboptimal. methods exhibit high variance making diﬃcult distinguish quality learned policies. since hard generate instances community overﬁtting small suite tests. section propose simpler benchmark obviates many shortcomings classical linear quadratic regulator unknown dynamics. control theory known dynamics fundamental problem thoroughly understood. problem goal control linear dynamical system minimizing quadratic cost. problem formalized states actions matrices appropriate dimensions. noise process i.i.d. guassian. dynamics known mild conditions problem admits optimal policy form unique matrix computed eﬃciently solution algebraic riccati equation. moreover ﬁnite horizon version problem eﬃciently solved dynamic programming. unknown dynamics considerably less well understood oﬀers fertile ground research. note still trivial produce varied instances always compare best achievable cost dynamics known. natural model-based approach consists estimating transition matrices data solving plugging estimates riccati equation. controller computed fashion called nominal controller. though method ideally robust nominal control provide useful baseline compare methods. comparison relative cost controllers produced nominal synthesis procedure lspi method. points along dashed line denote median cost shaded region covers percentile trials. figure shows require signiﬁcantly samples lspi stabilizing controller note lspi requires initial controller stabilizes discounted version problem require special initialization. however figure also shows nominal control method orders magnitude sample eﬃcient lspi ars. hence much room improvement pure model-free approaches. would blow slowly. therefore long trajectories required evaluating performance controller. however variance policy gradient methods grows length trajectories used even standard variance reduction techniques used. small computational footprint linear policies embarrassingly parallel structure make method ideal training policies small amount time computational resources. tables show wall-clock time required reach average reward evaluated random seeds. requires median time minutes reach prescribed reward threshold trained m.xlarge instance cpus. evolution strategies method salimans took median time minutes evaluated trials. however authors clarify trails means multiple trials random seed multiple trials diﬀerent random seeds. moreover table shows trains policy minutes seeds. also table shows requires times less time finally would like point method could scaled workers. case computational bottleneck aggregation statistics across workers. successful training policies require update statistics occur iteration. example implementation moritz allowed worker independent estimate choice authors used scale cores reaching reward humanoid-v minutes. could tune update schedule statistics order reduce communication time workers reduce sample complexity ars. sake simplicity refrained tuning schedule. table evaluation wall-clock time required reach average reward humanoid-v task. median time required evaluated random seeds. values taken work salimans evaluated independent trials. stands unknown. attempted simplest algorithm model-free performs well continuous control benchmarks used literature. demonstrated algorithmic augmentations basic random search could used train linear policies achieve state-of-theart sample eﬃciency mujoco locomotion tasks. showed linear policies match performance complex neural network policies found simple algorithm. since algorithm policies simple able perform extensive sensitivity studies observed method good solutions highly nonconvex problems large fraction time. variance algorithms method achieves state-of-the-art performance mujoco locomotion tasks hyperparameters random seeds varied. results emphasize high variance intrinsic training policies mujoco tasks. therefore clear gained evaluating algorithms small numbers random seeds common literature. evaluation small numbers random seeds capture performance adequately high variance. results point problems common methodology used evaluation algorithms. though many researchers concerned minimizing sample complexity make sense optimize running time algorithm single instance. running time algorithm meaningful notion either evaluated family instances clearly restricting class algorithms. algorithm reaches target reward collecting samples. sample complexity method reported number samples required reach target reward threshold given hyperparameter conﬁguration. however number hyperparameter conﬁgurations tried. number algorithmic enhancements added discarded tested simulation. fair measurement sample complexity count number rollouts used every tested hyperparameters? method optimizing objective dimensional function functions minimizer. iteration algorithm queries oracle stochastic gradient. oracle samples returns algorithm derivative then according current methodology evaluation sequence random variables sampled oracle ﬁxing random seed proceed tune step-size stochastic gradient method. since step-size ﬁrst random variable sampled oracle same call tries determine step-size ensures reaching \u0001-close minimizer iteration algorithm. since functions using oracle calls behind scene elicit step-size optimizes objective iteration. however would sample complexity algorithm algorithm would require sample optimize objectives. better measure sample complexity would total number samples required tuning tasks simple dimensional convex objective considered above methods evaluated random seed. however arguments relevant. optimal hyperparameter tuning artiﬁcially improve perceived sample eﬃciency method. indeed work. adding third algorithmic enhancement basic random search able improve sample eﬃciency already highly performing method. considering prior work uses algorithms tunable parameters neural nets whose architectures hyperparameters signiﬁcance reported sample complexities methods clear. issue important meaningful sample complexity algorithm inform number samples required solve previously unseen task. simulation task thought instance problem problem itself. light issues empirical results make several suggestions future work simple baselines established moving forward complex benchmarks methods. simpler algorithms easier evaluate empirically understand theoretically. propose reasonable baseline task well-understood model known instances generated variety diﬀerent levels diﬃculty little overhead required replication. games physics simulators used evaluation separate problem instances used tuning evaluation methods. moreover large numbers random seeds used statistically signiﬁcant evaluations. however since distribution problems occurring games physics simulators diﬀers distribution problems hopes solve methodology ideal either. particular diﬃcult algorithm better another evaluations performed simulation since algorithms might exploiting particularities simulator used. rather trying develop algorithms applicable many diﬀerent classes problems might better focus speciﬁc problems interest targeted solutions. emphasis development model-based methods. many problems methods observed require fewer samples model-free methods. moreover physics systems inform parametric classes models used diﬀerent problems. model-based methods incur many computational challenges themselves quite possible tools deep improved tree search provide paths forward tasks require navigation complex uncertain environments. thank orianna demasi moritz hardt eric jonas robert nishihara rebecca roelofs esther rolf vaishaal shankar ludwig schmidt nilesh tripuraneni stephen many helpful comments suggestions. thanks robert nishihara vaishaal shankar sharing expertise parallel computing. part rise generally supported part cise expeditions award ccf- award hshqdc--- gifts alibaba amazon services financial capitalone ericsson google huawei intel microsoft scotiabank splunk vmware. generously supported part award ccf- awards n--- n--- darpa fundamental limits learning program amazon research award.", "year": 2018}