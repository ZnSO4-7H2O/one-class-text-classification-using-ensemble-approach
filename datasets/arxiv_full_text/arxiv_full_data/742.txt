{"title": "Feature learning in feature-sample networks using multi-objective  optimization", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Data and knowledge representation are fundamental concepts in machine learning. The quality of the representation impacts the performance of the learning model directly. Feature learning transforms or enhances raw data to structures that are effectively exploited by those models. In recent years, several works have been using complex networks for data representation and analysis. However, no feature learning method has been proposed for such category of techniques. Here, we present an unsupervised feature learning mechanism that works on datasets with binary features. First, the dataset is mapped into a feature--sample network. Then, a multi-objective optimization process selects a set of new vertices to produce an enhanced version of the network. The new features depend on a nonlinear function of a combination of preexisting features. Effectively, the process projects the input data into a higher-dimensional space. To solve the optimization problem, we design two metaheuristics based on the lexicographic genetic algorithm and the improved strength Pareto evolutionary algorithm (SPEA2). We show that the enhanced network contains more information and can be exploited to improve the performance of machine learning methods. The advantages and disadvantages of each optimization strategy are discussed.", "text": "abstract—data knowledge representation fundamental concepts machine learning. quality representation impacts performance learning model directly. feature learning transforms enhances data structures effectively exploited models. recent years several works using complex networks data representation analysis. however feature learning method proposed category techniques. here present unsupervised feature learning mechanism works datasets binary features. first dataset mapped feature– sample network. then multi-objective optimization process selects vertices produce enhanced version network. features depend nonlinear function combination preexisting features. effectively process projects input data higher-dimensional space. solve optimization problem design metaheuristics based lexicographic genetic algorithm improved strength pareto evolutionary algorithm show enhanced network contains information exploited improve performance machine learning methods. advantages disadvantages optimization strategy discussed. good representation encoded knowledge machine learning model fundamental success. several data structures used purpose instance matrices weights trees graphs sometimes learning models lack data structure store knowledge storing input recent years several works using complex networks data representation analysis complex networks graphs nontrivial topology represent interactions dynamical system advances science complex systems bring several tools understand systems. describe dataset binary features bipartite complex-network. network called feature–sample network. using representation solve semi-supervised learning task called positive-unlabeled learning dealing machine learning problems often need pre-process input data. feature learning transforms enhances data structures effectively exploited learning models. autoencoders manifold learning examples feature learning methods paper propose feature learning process information feature–sample networks. summary include network limited number vertices based non-linear function preexisting ones. vertices determined multi-objective objective problem goal maximize number features maintaining properties original data. multiobjective approaches designed lexicographic genetic algorithm implementation improved strength pareto evolutionary algorithm show enhanced feature–sample networks improve performance learning methods major machine learning paradigms supervised unsupervised learning. also expose pros cons optimization approach. rest paper organized follows. sections describe enhance feature–sample networks optimization problem. section computer simulations illustrate optimization process assess performance improvements machine learning tasks. finally conclude paper section section describe enhance feature– sample network adding network constrained number features. connections samples feature depend nonlinear function combination preexisting features. chosen features result optimization process maximizes number features also distributes along samples. process similar projection associated data higher-dimensional space preserving important characteristics data. following subsections ﬁrst review feature– sample networks describe creation features. moreover elaborate optimization problem enhance feature–sample networks. assume input dataset whose members binary feature vectors feature vectors sparse number elements value much lower dimension feature–sample network bipartite complexnetwork whose edges connect samples features disproportion zero number connections sample proportional initial sparsity. sparsity sample might change keep shape degree distribution samples. section ii-b number possible andfeatures scales exponentially number features consequence storing every possible and-feature feasible. number candidate solutions also exponential number possible features. precisely three common approaches solving multi-objective optimization problems weighted-formula lexicographic pareto ﬁrst strategy transforms problem single-objective usually weighting objective adding lexicographic approach assigns priority objective optimizes objectives order. hence comparing candidate solutions highest-priority objective compared equivalent second objective compared. second objective also equivalent solutions third used weighted-formula lexicographic strategies return solution problem. pareto methods different mathematical tools evaluate candidate solution ﬁnding non-dominated solutions. solution said non-dominated worse solution concerning criteria according cover’s theorem given not-densely populated space classiﬁcation problem chance linearly separable increased cast highdimensional space nonlinearly. since assume input feature–sample network sparse synthesize features nonlinearly exploit properties theorem. produce features using operator nonlinear boolean function. call and-feature feature links samples connected given preexisting features. given feature–sample network samples features produce and-feature combination features +d}. call order and-feature thus number possible and-features rest paper index every possible and-feature using parameter element indicates feature network. feature part combination thus using notation and-feature connects sample holds. problem enhancing network viewed optimization problem. given input feature–sample network samples features denote enhanced network original adding every andfeature number features enhanced network excluding and-features connect given thus disadvantage approach and-features might well distributed. thus samples feature others many. overcome limitation introduce disproportion network enhanced version although methods different approaches share many properties individual representation population initialization operators mutation recombination selection explained section iii-d. main difference ﬁtness evaluation. individuals ordered lexicographically ordered primarily ﬁrst objective function case second objective. spea however consider pareto front also density solutions. words random population candidate solutions generated ﬁrst step. then stop condition next generation individuals comprises best individuals previous generation individuals originated recombining mutating parents selected previous generation. improved strength pareto evolutionary algorithm review spea works similarly major difference keeps archive candidate solutions pareto set. number non-dominated solutions greater limit archive solutions discarded. operation called truncation. truncation operator tries maintain candidate solutions uniformly distributed along pareto front heuristic exposed next items. individual representation problem solution zero and-features. enumerate every possible and-feature solution also viewed binary vector entries present and-features. population initialization given sample without replacement random and-features compose candidate solution andfeatures sampled probability order selection operator binary tournament method chosen select parent recombination step. mutation operator formulated following speciﬁc mutation operator exploit characteristics problem. given candidate solution apply random changes individual. change equal probability either trying and-feature; trying remove and-feature trying modify and-feature trying feature and-feature sampled solution updated {v}. note candidate solution change and-feature previously present. trying remove feature and-feature selected. case probability candidate solution updated {v}. note that probability finally last case and-feature order selected uniformly modiﬁed. and-feature selected modiﬁed and-feature index produced. cases happen chance selected uniformly ﬁrst case include term andfeature second swaps elements effectively takes effect candidate solutions updated {v}. size never increased candidate solution preserved performance considerations although view solutions and-features binary vectors representation practical high spacecomplexity problem. moreover need store entries and-features lack connections. instead ignoring them exploit evaluation step determine and-features useless remove set. crossover candidate solutions yparent implemented efﬁciently steps ychild yparent yparent yparent ychild ychild ychild ychild stands symmetric difference operator. finally conform requirements section iii-d sample candidate solution efﬁciently steps sampleandfeature loop loop first illustrate optimization process famous iris dataset. example also conduct community detection original feature–sample network obtained dataset enhanced one. iris dataset contains samples describe sepals petals individual iris ﬂowers. ﬂowers either iris setosa iris virginica iris versicolor. construct feature–sample network dataset discretizing features. figure shows generated network. color represents different class. circles represent samples squares features. network input algorithms using spea lexicographic evolution candidate solutions execute optimization process strategy. population individuals. spea archive size lexicographic keep best solutions generations. initial population figure evolution number and-features disproportion along generations spea algorithm iris dataset input network. solid lines average disproportion number and-features non-dominated solution given generation. shadows cover range measurements. figures describe obtained result. disproportion number discovered and-feature depicted along generations. solid lines average result population shadows cover range minimum maximum measurement. results include non-dominated solutions spea best solutions lexicographic using strategies could reach optimal solution and-features least connection disproportion. however optimization strategies differ achieve this. figure evolution number and-features disproportion along generations lexicographic iris dataset input network. solid lines average disproportion number and-features best solutions given generation. shadows cover range measurements. figure feature–sample network iris dataset possible andfeatures circles vertices associated samples squares features. colors represent classes. lexicographic disproportion considered number discovered and-features same. result algorithm greedily produce and-features disregarding disproportion cannot features add. enables faster convergence case solutions high disproportion unfeasible reach maximum number and-feature common practice. solve issue larger problems limit number discovered features mmax. also apply proposal classiﬁcation tasks table presents datasets along number samples features possible and-features highlight unfeasible list every possible combination among features even small datasets. networks generated shown bins. optimization process executed times strategy spea population size archive size elitism solutions. initial population recombination rate mutations performed candidate solution. limit number andfeature mmax execution stopped generation. table summarizes number and-features disproportion obtained optimization process. lexicographic show average standard deviation measurements among best individuals. spea non-dominated solutions considered. expected considering previous study lexicographic achieved better count and-features maximum allowed worse values disproportion. candidate solutions spea present wide variation consistent lower disproportion. cover geometrical statistical properties systems linear inequalities applications pattern recognition ieee transactions electronic computers vol. m˜unoz mar´ı bovolo g´omez-chova bruzzone campvalls semisupervised one-class support vector machines classiﬁcation remote sensing data ieee trans. geosci. remote sens. vol. zitzler laumanns thiele spea improving strength pareto evolutionary algorithm multiobjective optimization evolutionary methods design optimization control applications industrial problems. athens greece international center numerical methods engineering best results conﬁguration shown table improvements highlighted bold. using solution highest number and-features improved classiﬁcation results cases. using less and-features lower disproportion improvements almost good using higher and-feature count. paper presented unsupervised feature learning mechanism works datasets binary features. first dataset mapped feature–sample network. then multi-objective optimization process selects vertices correspond features produce enhanced version network. solve optimization problem opted design population-based metaheuristics. used lexicographic spea algorithm candidate solutions. experiments conclude produces features fewer generations. however candidate solutions spea besides less features also improved performance machine learning methods. fact suggests disproportion good measurement quality selected and-features. future works correlate improvement disproportion solutions number features. learning techniques used fast-greedy community detection k-nearest neighbors take full advantage features. subsequent studies elaborate learning models exploit enhanced feature– sample network explicitly.", "year": 2017}