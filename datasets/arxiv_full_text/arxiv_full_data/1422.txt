{"title": "Like What You Like: Knowledge Distill via Neuron Selectivity Transfer", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Despite deep neural networks have demonstrated extraordinary power in various applications, their superior performances are at expense of high storage and computational costs. Consequently, the acceleration and compression of neural networks have attracted much attention recently. Knowledge Transfer (KT), which aims at training a smaller student network by transferring knowledge from a larger teacher model, is one of the popular solutions. In this paper, we propose a novel knowledge transfer method by treating it as a distribution matching problem. Particularly, we match the distributions of neuron selectivity patterns between teacher and student networks. To achieve this goal, we devise a new KT loss function by minimizing the Maximum Mean Discrepancy (MMD) metric between these distributions. Combined with the original loss function, our method can significantly improve the performance of student networks. We validate the effectiveness of our method across several datasets, and further combine it with other KT methods to explore the best possible results. Last but not least, we fine-tune the model to other tasks such as object detection. The results are also encouraging, which confirm the transferability of the learned features.", "text": "knowledge transfer network pruning iteratively prunes neurons weights importance based certain criteria network quantization tries reduce precision weights features. nevertheless worth noting approaches able fully exploit modern deep learning frameworks. accelerations need speciﬁc hardwares implementations. contrast based methods directly train smaller student network accelerates original networks terms wall time without bells whistles. best knowledge earliest work could dated trained compressed model pseudo-data labeled ensemble strong classiﬁers. however work limited shallow models. recently hinton brought back introducing knowledge distillation basic idea distill knowledge large teacher model small learning class distributions provided teacher softened softmax. despite simplicity demonstrates promising results various image classiﬁcation tasks. however applied classiﬁcation tasks softmax loss function. subsequent works tried tackle issue transferring intermediate representations teacher model. work explore type knowledge teacher models transfer student models. speciﬁcally make selectivity knowledge neurons. intuition behind model rather straightforward neuron essentially extracts certain pattern related task hand input. thus neuron activated certain regions samples implies regions samples share common properties relate task. clustering knowledge valuable student network since provides explanation ﬁnal prediction teacher model. result propose align distribution neuron selectivity pattern student models teacher models. despite deep neural networks demonstrated extraordinary power various applications superior performances expense high storage computational costs. consequently acceleration compression neural networks attracted much attention recently. knowledge transfer aims training smaller student network transferring knowledge larger teacher model popular solutions. paper propose novel knowledge transfer method treating distribution matching problem. particularly match distributions neuron selectivity patterns teacher student networks. achieve goal devise loss function minimizing maximum mean discrepancy metric distributions. combined original loss function method signiﬁcantly improve performance student networks. validate effectiveness method across several datasets combine methods explore best possible results. last least ﬁne-tune model tasks object detection. results also encouraging conﬁrm transferability learned features. recent years deep neural networks renewed state-of-the-art performance various ﬁelds computer vision neural language processing. generally speaking given enough data deeper wider networks would achieve better performances shallow ones. however larger larger networks also bring high computational memory costs. still great burden deploy state-of-the-art models real-time applications. problem motivates researches acceleration compression neural networks. last years extensive work proposed ﬁeld. attempts roughly categorized three types network pruning network quantization figure architecture neuron selectivity transfer student network trained ground-truth labels also mimics distribution activations intermediate layers teacher network. triangle ﬁgure denotes corresponding activation ﬁlter. teacher. maximum mean discrepancy used loss function measure discrepancy between teacher student features. test method cifar- cifar- imagenet datasets show neuron selectivity transfer improves student’s performance notably. deep network compression acceleration many works proposed reduce model size computation cost network compression acceleration. early development neural network network pruning proposed pursuit balance accuracy storage. recently brought back modern deep structures main idea weights small magnitude unimportant removed. however strategy yields sparse weights needs speciﬁc implementations acceleration. pursue efﬁcient inference speed-up without dedicated libraries researches network pruning undergoing transition connection pruning ﬁlter pruning. several works evaluate importance neurons different selection criteria others formulate pruning subset selection sparse optimization problem. beyond pruning quantization low-rank approximation also widely studied. note acceleration methods complementary combined method improvement. knowledge transfer deep learning knowledge distill pioneering work apply knowledge transfer deep neural networks. knowledge deﬁned softened outputs teacher network. compared one-hot labels softened outputs provide extra supervisions intra-class inter-class similarities learned teacher. one-hot labels project samples class single point label space softened labels project samples continuous distribution. hand softened labels could represent sample class distribution thus captures intra-class variation; hand inter-class similarities compared relatively among different classes soft target. formally soft target network deﬁned vector teacher logits temperature. increasing inter-class similarity retained driving prediction away student network trained combination softened softmax original softmax. however drawback also obvious first assume neural network compressed convolutional neural network refer teacher network student network let’s denote output feature layer rc×hw channels spatial dimensions better illustration denote column feature maps certain layers teacher student network respectively. without loss generality assume spatial dimensions. feature maps interpolated dimensions match. subsection review maximum mean discrepancy regarded distance metric probability distributions based data samples sampled suppose given sets samples {xi}n sampled distributions respectively. squared distance formulated effectiveness limits softmax loss function relies number classes. example binary classiﬁcation problem could hardly improve performance since almost additional supervision could provided. subsequent works tried tackle drawbacks transferring intermediate features. lately romero proposed fitnet compress networks wide shallow thin deep. order learn intermediate representations teacher network fitnet makes student mimic full feature maps teacher. however assumptions strict since capacities teacher student differ greatly. certain circumstances fitnet adversely affect performance convergence. recently zagoruyko proposed attention transfer relax assumption fitnet transfer attention maps summaries full activations. discussed later work seen special case framework. deﬁned novel type knowledge flow solution procedure knowledge transfer computes gram matrix features different layers. claimed matrix could reﬂect teachers solve problem. domain adaptation belongs ﬁeld transfer learning mostly popular setting goal domain adaptation improve testing performance unlabeled target domain model trained related different source domain. since labels available target domain core domain adaptation measure reduce discrepancy distributions domains. literature maximum mean discrepancy widely used criterion compares distributions reproducing kernel hilbert space several works adopted solve domain shift problem. examples source domain re-weighted selected minimize source target distributions. works like measured explicit low-dimensional latent space. applications deep learning model used regularize learned features source domain target domain. note that domain adaptation limited traditional supervised learning problem. example recently casted neural style transfer domain adaptation problem demonstrated neural style transfer essentially equivalent match feature distributions content image style image. special case second order polynomial kernel mmd. paper explore novel application knowledge transfer. section present neuron selectivity transfer method. start intuitive example explain motivation present formal deﬁnition discussions proposed method. fig. shows images blended heat selected neuron conv easy neurons strong selectivities neuron left image sensitive monkey face neuron right image activates characters strongly. activations actually imply selectivities neurons namely kind inputs neuron. words regions high activations neuron share task related similarities even though similarities intuitive human interpretation. order capture similarities also neurons mimic activation patterns student networks. observations guide deﬁne type knowledge teacher networks neuron selectivities called coactivations transfer student networks. wrong directly matching feature maps? natural question cannot align feature maps teachers students directly? did. considering activation spatial position feature ﬂattened activation ﬁlter sample space neuron selectivities dimension sample distribution reﬂects interpret input image focus type activation pattern emphasize more? distribution matching good choice directly match samples since ignores sample density space. consequently resort advanced distribution alignment method explained below. figure different knowledge transfer methods cifar cifar. test errors bold train errors dashed lines. improves ﬁnal accuracy observably fast convergence speed. best view color. activation maps relu layer already non-negative equivalent except form normalization. represent neurons high responses namely attention teacher network. thus special case framework. item gram matrix roughly represents similarity region guides student network learn better internal representation explaining task driven region similarities embedding space. greatly enriches supervision signal student networks. following sections evaluate several standard datasets including cifar- cifar- imagenet lsvrc cifar datasets extremely deep network resnet- used teacher model simpliﬁed version inception-bn adopted student model. imagenet lsvrc adopt pre-activation version resnet- original inception-bn teacher model student model respectively. validate effectiveness method compare several state-of-the-art knowledge transfer methods including fitnet temperature softened softmax following fitnet value following mapping function adopted reimplementation square performs best experiments linear polynomial gaussian kernel respectively. experiments conducted mxnet make implementation publicly available paper accepted. start cifar dataset evaluate method. cifar- cifar- datasets consist training images testing images classes respectively. data augmentation take random crop zero-padded image ﬂipping following weight decay optimization mini-batch size single gpu. train network epochs. learning rate starts divided epochs. fitnet single transfer loss convolutional layer output inception-bn output last group residual block resnet-. also multiple transfer losses different layers improvement single loss minor methods. table summarizes experiment results. achieves higher accuracy original student network demonstrates effectiveness feature maps distribution matching. choice kernel inﬂuences performance nst. experiments polynomial kernel yields better result linear gaussian kernels. comparing knowledge transfer methods also competitive. cifar- transfer methods achieve higher accuracy original student network. among them polynomial kernel performs best. cifar- achieves best performance. consistent explanation would perform better classiﬁcation task classes since classes provide accurate information intra-class variation softened softmax target. plore best possible results. table shows results kd+fitnet kd+nst kd+fitnet+nst. surprisingly matching ﬁnal predictions intermediate representations improve individual transfers. particularly combined performs best three settings. speciﬁc improve performance student network absolutely reduce relative error respectively. training testing curves experiments found fig. transfer methods converge faster student network. among them kd+nst fastest. section conduct large-scale experiments imagenet lsvrc classiﬁcation task. dataset consists training images another validation images. optimize network using nesterov accelerated gradient mini-batch size gpus weight decay momentum nag. data augmentation weight initialization follow publicly available implementation fb.resnet train network epochs. initial learning rate divided epoch respectively. report top- top- validation errors standard single test center-crop setting. according previous section evaluate best setting method second order polynomial kernel. value settings cifar experiments. results imagenet experiments found table fig. method achieves top- top- improvements compared stuinterestingly different also dent network. experiments fitnet improve convergence accuracy inception-bn. caused choice weak teacher network among methods performs best. combined achieves best accuracy improves top- top- accuracy respectively. results verify effectiveness proposed large scale application complementarity state-of-the-art knowledge transfer methods. pascal detection network engineering plays increasingly important role visual recognition. researchers focus designing better network architectures learn better representations. several works demonstrated improvement feature learning image classiﬁcation could successfully transferred recognition tasks object detection semantic segmentation. however gain knowledge transfer image classiﬁcation task transferred high level vision tasks? provide preliminary investigation object detection task. evaluation based faster-rcnn system pascal dataset. following settings train models union trainval trainval evaluate test set. since goal validate effectiveness base models make comparisons varying pretrained imagenet classiﬁcation models keeping parts unchanged. backbone network inception-bn different methods. extract features layer whose stride pixels. standard evaluation metrics average precision mean reported evaluation. table summarizes detection results. models achieve higher baseline. comparing transfer techniques improves higher map. combined kd+nst yields gain. results demonstrate could beneﬁt object detection task without modiﬁcations extra computations original student model testing. consequently powerful tools improve performance wide range applications practitioners. interestingly though performs best large-scale image classiﬁcation task feature based mimicking methods including fitnet greater advantages object detection task. importance spatial information object detection. totally ignores methods exploit certain extent. section ﬁrst analyze strengths weaknesses several closely related works based results experiment discuss possible extenstions proposed method. transfer cifar experiment using ﬁgure denotes activation pattern neuron. expected loss signiﬁcantly reduces discrepancy teacher student distributions makes student network like teacher network. achieves best performance large number classes. case softened softmax depict data sample embedded label space accurate case number class small. however drawback fully based softmax loss limits applications broader applications regression ranking. compared methods meet constraints. fitnet assumption strict sense forces student network match full activations teacher model mentioned before. discussed directly matching samples ignores density space. certain circumstances training fitnet inﬂuenced noise seriously makes hard converge. propose novel view knowledge transfer treating distribution matching problem. although select distribution matching method matching methods also incorporated framework. formulate distribution parametric form simple moment matching used align distribution. complex cases drawing idea generative adversarial network solve problem interesting direction pursue. goal train generator network generates samples speciﬁc data distribution. training discriminator network used distinguish whether sample comes real data generated framework student network seen generator. trained distinguish whether features generated sucessfully confuses student network teacher. domain discrepancy minimized. similar ideas already exploited domain adaptation area believe also used application. paper propose novel method knowledge transfer casting distribution alignment problem. utilize unexplored type knowledge neuron selectivity. represents task related preference neuron cnn. detail match distributions spatial neuron activations teacher student networks minimizing distance them. technique successfully improve performance small student networks. experiments show effectiveness method various datasets demonstrate complementary existing methods speciﬁcally combination yields state-of-the-art results. furthermore analyze generalizability knowledge transfer methods tasks. results quite promising thus further conﬁrm knowledge transfer methods could indeed learn better feature representations. successfully transferred high level vision tasks object detection task. believe novel view facilitate design knowledge transfer methods. future work plan explore applications methods especially various regression problems superresolution optical prediction etc.", "year": 2017}