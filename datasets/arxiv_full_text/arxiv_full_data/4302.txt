{"title": "Convex Approaches to Model Wavelet Sparsity Patterns", "tag": ["cs.CV", "stat.ML"], "abstract": "Statistical dependencies among wavelet coefficients are commonly represented by graphical models such as hidden Markov trees(HMTs). However, in linear inverse problems such as deconvolution, tomography, and compressed sensing, the presence of a sensing or observation matrix produces a linear mixing of the simple Markovian dependency structure. This leads to reconstruction problems that are non-convex optimizations. Past work has dealt with this issue by resorting to greedy or suboptimal iterative reconstruction methods. In this paper, we propose new modeling approaches based on group-sparsity penalties that leads to convex optimizations that can be solved exactly and efficiently. We show that the methods we develop perform significantly better in deconvolution and compressed sensing applications, while being as computationally efficient as standard coefficient-wise approaches such as lasso.", "text": "measured data linear observation operator image reconstructed additive gaussian noise. throughout paper assume standard matrix-vector representation image represented column vector linear operators represented matrices. images typically approximately sparse representations wavelet domain many approaches image reconstruction attempt exploit property example standard lasso reconstruction problem written lasso penalty reﬂects fact wavelet coefﬁcients approximately sparse reality patterns sparsity equally plausible/probable. commonly observed effect persistence large wavelet coefﬁcient across scales localized nature edges. many models proposed represent patterns particular tree-structured models among successful widely used tree-structured models admit efﬁcient estimation procedures based pruning message-passing algorithms denoising applications identity operator identity simple strategies longer applied. fact general optimization problem resulting tree models non-convex above) exact solutions difﬁcult almost impossible obtain. issue addressed resorting greedy suboptimal iterative reconstruction procedures motivates main idea contribution paper. tree models represent patterns sparsity wavelet coefﬁcients natural images capture effects. particularly interested modeling so-called parent-child dependency used reference persistence large/small wavelet coefﬁcients across scales. speciﬁcally wavelet coefﬁcient certain spatial location scale large/small neighboring coefﬁcients roughly location ﬁner coarser scales tend large/small. term parent-child refers pair coefﬁcients certain location adjacent scales. goal exploit statistical dependencies among wavelet coefﬁcients commonly represented graphical models hidden markov trees however linear inverse problems deconvolution presence sensing observation matrix produces linear mixing simple markovian dependency structure. leads reconstruction problems non-convex optimizations. past work dealt issue resorting greedy suboptimal iterative reconstruction methods. paper propose modeling approaches based group-sparsity penalties leads convex optimizations solved exactly efﬁciently. show methods develop perform signiﬁcantly better deconvolution compressed sensing applications computationally efﬁcient standard coefﬁcient-wise approaches lasso. statistical dependencies among wavelet coefﬁcients commonly represented trees graphical models hidden markov trees hmts provide superior denoising results compared independent coefﬁcient-wise thresholding/shrinkage methods like lasso fast exact and/or approximate inference algorithms exist many situations all. linear inverse problems presence sensing/observation matrix linearly markovian dependency structure simple exact inference algorithms longer exist. past work dealt issue resorting greedy suboptimal iterative reconstruction methods based belief propagation iterative reweighting variants orthogonal matching pursuit paper propose modeling approach based group-sparsity penalties leads convex optimizations solved exactly efﬁciently. results show approach performs much better deblurring compressed sensing applications computationally efﬁcient standard coefﬁcient-wise approaches. work uses group lasso overlap formulation introduced modify better represent dependencies among wavelet coefﬁcients. note norm formulation. similar work could performed using norm motivate problem section section explain model wavelet transform coefﬁcients overlapping groups. section outlines experiments performed results obtained. conclude paper section coefﬁcients group group lasso penalty enforces group sparsity setting whole groups zero norm group small relative importance data-ﬁtting term. applications group lasso groups assumed disjoint case overlapping hence penalty terms coupled. coupling standard group lasso optimization strategies cannot directly applied. offer approaches deal issue. deal overlapping groups introduce replicates coefﬁcient group involving certain coefﬁcient copy decouples overlapping groups other. approach proposed analyzed replication variables results formulation expressed tained replicating corresponding columns penalty function separable computationally efﬁcient iterative shrinkage/thresholding methods applied overlap group lasso replication strategy treats group independently other. means that grandparent parent coefﬁcient group selected parent child group violating persistence wavelet transforms across multiple scales motivates penalty tends cause coefﬁcients location across scales similar value. modify equation master copy i-th coefﬁcient denote copies appear group penalty terms. replicated variables setting large forces replicated copies agree yielding solution group lasso encourages stronger degree persistence across scales. best knowledge approach previously proposed overlapping group lasso problem. note additional quadratic penalty combined quadratic data-ﬁtting term obtain quadratic plus separable group sparsity penalty directly apply standard solvers henceforth denote lasso denote overlap group lasso equation oglr denote overlap group lasso replication fact coefﬁcients pair typically large small magnitude. accomplished using overlapping-group penalty function generalizes lasso captures parent-child dependencies time retaining convex nature. fig. depicts wavelet quadtree structures example parentchild groups. coefﬁcient associated orientation four child coefﬁcients orientation ﬁner scale many options exist grouping parents children options depicted ﬁgure. many grouping schemes also possible framework explore extensions paper. fig. quadtree corresponding dwt. scale parent coefﬁcients grouped child coefﬁcients. four children grouped together parent parent grouped child individually main contributions paper threefold introduce approach representing wavelet coefﬁcient sparsity patterns commonly observed natural images; adapt extend recently proposed methods group lasso overlaps order take advantage sparsity patterns linear inverse problems using simple convex optimization techniques; demonstrate fast efﬁcient reconstruction image deblurring compressed sensing signiﬁcant improvements reconstruction error relative lasso. encourage solutions wavelet coefﬁcient sparsity patterns reﬂective parent-child group structure persistence large/small coefﬁcients across scales apply group lasso regularization image. fig. shows wavelet coefﬁcients image standard organization randomized organization norm invariant randomization group penalties change parent-child dependencies preserved. quantify degree group penalties encourage parent-child persistence compute ratio group penalties left image randomly organized image right. group penalties larger parent-child relationships lost randomization ratios less group penalties. penalty ratio smaller oglr penalty indicating penalty strongly favors structure image compared oglr penalty penalty lasso. next show experimental evidence produces better reconstructions. evaluate proposed approaches -dimensional signals images real image. used sparsa modiﬁed suit overlapping groups scenario associated modiﬁed case solve equations used haar wavelet basis sparsity inducing transform. groups deﬁned according methods explained section illustrate potential proposed methods ﬁrst consider compressed sensing deconvolution results cameraman image resized fig. fig. show compressed sensing results. image undersampled using random gaussian matrix using samples every subimage. fig. fig. show results deblurring image blurred gaussian kernel variance samples cases corrupted variance evaluate improvements group penalties ﬁrst consider performance relative signal-to-noise ratio. ﬁrst experiments involved testing recovery scheme compressed sensing framework noise variance varied steps measured mean reconstruction error ||θ∗ ˆθ|| results obtained using images plotted fig. fig. results averaged independent trials noise level. random ‘toy’ image similar fig. generated trial. employed grid search pick best value note oglr produces better results standard lasso. models coefﬁcients lead convex optimizations. non-separable nature group lasso addressed devising optimization criterion solved standard methods based separable penalties. experiments demonstrate performance gains group penalty methods compared lasso. duarte m.f. wakin m.b. baraniuk r.g. wavelet-domain compressive signal reconstruction using hidden markov tree model. proceedings ieee international conference acoustics speech signal processing pages crouse m.s. nowak r.d. baraniuk r.g. wavelet based statistical signal processing using hidden markov models. transactions signal processing schniter turbo reconstruction structured sparse signals. proc. conference information sciences systems fig. effect varying number measurements taken. note overlap lasso needs fewer measurements lasso achieve errors. number measurements increased beyond methods stop improving signiﬁcantly. group penalties also reduce number compressed sensing measurements needed reconstruct images. study effect varied number rows matrix. inputs used random piecewise constant signals length jumps assigned uniformly random associated groups determined using binary tree structure group corresponding single parent-child pair observed need fewer measurements robust recovery signals implicit group structure assumed opposed needed conventional lasso proposed group penalties match sparsity patterns wavelet coefﬁcients natural images better simple coordinate-wise penalties penalty. like penalty linear inverse problems group penalties convex optimizations solved efﬁciently exactly. traditional markov tree", "year": 2011}