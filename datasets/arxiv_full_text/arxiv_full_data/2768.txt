{"title": "Budget-Constrained Multi-Armed Bandits with Multiple Plays", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We study the multi-armed bandit problem with multiple plays and a budget constraint for both the stochastic and the adversarial setting. At each round, exactly $K$ out of $N$ possible arms have to be played (with $1\\leq K \\leq N$). In addition to observing the individual rewards for each arm played, the player also learns a vector of costs which has to be covered with an a-priori defined budget $B$. The game ends when the sum of current costs associated with the played arms exceeds the remaining budget.  Firstly, we analyze this setting for the stochastic case, for which we assume each arm to have an underlying cost and reward distribution with support $[c_{\\min}, 1]$ and $[0, 1]$, respectively. We derive an Upper Confidence Bound (UCB) algorithm which achieves $O(NK^4 \\log B)$ regret.  Secondly, for the adversarial case in which the entire sequence of rewards and costs is fixed in advance, we derive an upper bound on the regret of order $O(\\sqrt{NB\\log(N/K)})$ utilizing an extension of the well-known $\\texttt{Exp3}$ algorithm. We also provide upper bounds that hold with high probability and a lower bound of order $\\Omega((1 - K/N)^2 \\sqrt{NB/K})$.", "text": "ad-exchange platforms channel selection radio networks documents acknowledged taking action practice inherently costly vast majority existing bandit-related work used analyze examples forgoes notion cost. furthermore above-mentioned applications rarely proceed strictly sequential way. realistic scenario setting which round multiple actions taken among possible choices. shortcomings motivate theme paper investigate problem budget constraint setting time-varying rewards costs multiple plays. precisely given a-priori deﬁned budget round decision maker selects combination distinct arms available arms observes individual costs rewards corresponds semi-bandit setting. player pays materialized costs remaining budget exhausted point algorithm terminates cumulative reward compared theoretical optimum deﬁnes weak regret expected difference payout best ﬁxed choice arms rounds actual gain. paper investigate stochastic adversarial case. stochastic case derive upper bound expected regret order utilizing algorithm ucb-mb inspired upper conﬁdence bound algorithm ﬁrst introduced adversarial case algorithm exp.m.b upper loweristing results also provide upper bound holds high probability. best knowledge ﬁrst case addresses adversarial budgetconstrained case therefore consider main contribution paper. study multi-armed bandit problem multiple plays budget constraint stochastic adversarial setting. round exactly possible arms played addition observing individual rewards played player also learns vector costs covered a-priori deﬁned budget game ends current costs associated played arms exceeds remaining budget. firstly analyze setting stochastic case assume underlying cost reward distribution support respectively. derive upper conﬁdence bound algorithm achieves regret. secondly adversarial case entire sequence rewards costs ﬁxed advance derive upper bound regret order utilizing extension well-known algorithm. also provide upper bounds hold high probability lower bound order b/k). multi-armed bandit problem extensively studied machine learning statistics means model online sequential decision making. classic setting popularized decision-maker selects exactly given round given observations realized rewards arms maximize cumulative reward ﬁxed horizon equivalently minimize regret deﬁned difference cumulative gain achieved decision-maker always played best realized cumulative gain. analysis setting reﬂects fundamental tradeoff desire learn better arms possibility play arms believed high payoff variety practical applications problem include placement online advertising maximize click-through rate particular online sponsored search auctions assume time-invariant costs cast setting knapsack problem rewards stochastic. contrast proposed algorithm ucb-bv per-round costs rewards sampled fashion unknown distributions derive upper bound regret order papers closest setting former investigates stochastic case resource consumption. unlike case however authors allow existence null tantamount skipping rounds obtain upper bound introduced however analysis based original bandit formulation introduced regret bounds hold asymptotically rely hard-to-compute index policies distribution-dependent. inﬂuenced works popularized usage easy-tocompute upper conﬁdence bounds recent line work investigated combinatorial bandit setting. example derived regret bound stochastic semi-bandit setting utilizing policy termed learning linear rewards similarly utilize framework decision-maker queries oracle returns fraction optimal reward. other less relevant settings paper setting adversarial bandit losses selected arms observed. furinvestithermore gate bandit slate problems take account ordering arms selected round. lastly utilize thompson sampling model stochastic problem. section formally deﬁne budgeted multiple play multi-armed bandit setup present main theorems whose results provided table together comparison existing results literature. ﬁrst describe stochastic setting proceed adversarial illuminating proofs theorems section presented section technical proofs relegated supplementary document. notice minimum cmin support cost distributions. assumption made practical reasons many applications bandits come minimum cost also guarantee well-deﬁned bang-per-buck ratios analysis paper relies goal design deterministic algorithm expected payout maximized given denotes sample mean observed bang-perbuck ratios time exploration term deﬁned algorithm round arms associated largest conﬁdence bounds played. initialization purposes allow arms played exactly prior while-loop. consider adversarial case makes assumptions reward cost distributions whatsoever. setup case ﬁrst proposed analyzed single play case ﬁxed horizon oblivious adversary. entire seqence rewards arms ﬁxed advance particular cannot adaptively changed runtime. proposed randomized algorithm enjoys maker play exactly arms. unlike previous settings additionally cost cmin stochastic setting player given budget costs incurred algorithm terminates rounds materialized costs round exceeds remaining budget. gain algorithm observed rewards including round expected regret deﬁned gain algorithm compared best arms omniscient algorithm upper bounds regret begin upper bounds regret budget constrained multiple plays later transition towards lower bounds upper bounds hold high probability. algorithm call exp.m.b provides randomdue assumption directly comparable bound theorem case outperforms occurs whenever loose upper bound gmax exists whenever gmax return best selection arms small. ized algorithm achieve sublinear regret. similar original algorithm developed algorithm exp.m.b maintains time-varying arms probabilities played time calculated noted requires weights capped value probabilities {pi}n kept range round player draws distinct arms cardinality |at| probability included done employing algorithm dependentrounding introduced runs time space. round observed rewards costs played arms turned estimates arms updated according ˆci) assigns larger weights increases decreases might expect. replaces original update step line algorithm algorithm exp. adaptation algorithm call exp..m.b algorithm algorithm deﬁne cumulative expected gains losses assumption natural assumption motivated individual rationality reasons. words user play bandit algorithm reward given round possible choice arms least large cost incurs playing. caveat proof theorem inﬂuenced proof methods algorithms exp.m main challenge absence well-deﬁned time horizon time-varying costs. remedy problem deﬁne proof follows existing procedures deriving lower bounds adversarial bandit settings main challenges found generalizing single play setting multiple play setting well incorporating notion cost associated bandits. select exactly arms random arms good subset arms round bernoulli distributed bias cost attain cmin probability respectively speciﬁed later. arms assigned rewards costs cmin independently random. denote expectation random variable conditional good arms. denote expectation respect uniform assignment costs {cmin rewards arms. lemma extension lemma denotes arbitrary time-invariant subset unique elements denotes upper conﬁdence bound cumulative optimal gain minus cumulative cost incurred rounds lemma gmax denotes optimal cumulative reward optimal chosen denote cumulative expected reward cost after exhaustion budget rounds) respectively. discussed budget-constrained multi-armed bandit problem arms multiple plays a-priori deﬁned budget explored stochastic well adversarial case provided algorithms derive regret bounds budget stochastic setting algorithm ucb-mb enjoys regret adversarial case showed algorithm exp.m.b enjoys work extended several dimensions future research. example incorporation budget constraint paper leads believe logical extension integrate ideas economics particular mechanism design multiple plays setting lemma whose proof relegated supplementary document allows bound gain existence optimal arms treating problem uniform assignment costs rewards arms. technical parts proof also found supplementary document. proof strategy acknowledge algorithm exp.p.m uses upper conﬁdence bounds pi√n update weights lemma asserts conﬁdence bounds valid namely upper bound possible idea investigate extent regret varies number plays increases. further believe settings repeated interactions customers give rise strategic considerations customers misreport preferences ﬁrst rounds maximize long-run surplus. works investigate repeated interactions single player only believe extension pool buyers worth exploring. setting would expect extent strategic behavior decreases number plays round increases since decision-maker could simply ignore users future rounds previously declined offers.", "year": 2017}