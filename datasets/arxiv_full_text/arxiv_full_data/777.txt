{"title": "Attention with Intention for a Neural Network Conversation Model", "tag": ["cs.NE", "cs.AI", "cs.HC", "cs.LG"], "abstract": "In a conversation or a dialogue process, attention and intention play intrinsic roles. This paper proposes a neural network based approach that models the attention and intention processes. It essentially consists of three recurrent networks. The encoder network is a word-level model representing source side sentences. The intention network is a recurrent network that models the dynamics of the intention process. The decoder network is a recurrent network produces responses to the input from the source side. It is a language model that is dependent on the intention and has an attention mechanism to attend to particular source side words, when predicting a symbol in the response. The model is trained end-to-end without labeling data. Experiments show that this model generates natural responses to user inputs.", "text": "conversation dialogue process attention intention play intrinsic roles. paper proposes neural network based approach models attention intention processes. essentially consists three recurrent networks. encoder network word-level model representing source side sentences. intention network recurrent network models dynamics intention process. decoder network recurrent network produces responses input source side. language model dependent intention attention mechanism attend particular source side words predicting symbol response. model trained end-toend without labeling data. experiments show model generates natural responses user inputs. conversation process process communication thoughts words. considered structural process stresses role purpose processing discourse essentially discourse structure intimately connected nonlinguistic notions intention attention. processing utterance attention explicates processing utterances example paying attention particular words sentence. hand intention higher level attention primary role explaining discourse structure coherence. clearly conversation process inherently complicated levels structures. conversation process cast sequence-to-sequence mapping task. task source side conversation person target side conversation another person. sequence-to-sequence mapping task includes machine translation graphemeto-phoneme conversion named entity tagging etc. however apparent difference dialogue process tasks dialogue process involves multiple turns whereas usually tasks involve turn mapping source sequence target sequence. neural network based approaches successfully applied sequence-to-sequence mapping tasks. made signiﬁcant progresses machine translation language understanding speech recognition among neural network-based approaches particular approach called encoder-decoder framework aims relaxing much requirement human labeling. conversation models typically designed domain speciﬁc much knowledge rules recent methods relax requirement extent whole systems ∗presented nips workshop machine learning spoken language understanding interaction still trained manual labels sub-components require manual labels error prone expensive. therefore appealing train system end-to-end without manual labels. recent works approach. general however using knowledge helpful. example alignment information source target side critical grapheme-to-phoneme conversion outperform strong baseline using n-gram models neural network based machine translation system alignment information used outperform strong phrase-based baseline context modeling conversation process neural network model built knowledge structural information conversation processes. particular network incorporate notion intention attention. test this developed model consists three recurrent neural networks source side encoder network encodes source side inputs. target side decoder network uses attention mechanism attend particular words source side predicting symbol response source side. importantly attention target side conditioned output intention rnn. model structural knowledge conversation process trained end-to-end without labels. experimented model observed generates natural responses user inputs. theory discourse discourse structure composed three separate related components. ﬁrst linguistic structure structure sequence utterance. linguistic structure consists segments discourse utterances naturally aggregate. second structure intentional structure captures discourse-relevant purposes expressed linguistic segments well relationships among them. third attentional state dynamic records objects properties relations salient point discourse. example table clear intentions. user states problem user’s intention conveying problem agent. agent receives words processes them communicates back user. user responds agent afterwards. therefore whole conversation process consists three intentions processed sequentially. ﬁrst intention communication problem. second intention process resolving issue. third intention acknowledgment. processing intentions user agent attention particular words. example resolving issue agent pays attention words virus. figure attention intention model. model unrolled three turns. turn rnns encoder network decoder network. session represented ﬁxed-dimension vector hidden state intention network. propose model attempts represent structural process intentions associated attentions. figure illustrates model. shows three layers processing encoder network intention network decoder network. encoder network inputs current source side input. source side current turn also dependent previous turn source side encoder network linked output previous target side. encoder network creates representation source side current turn. intention network dependent past state memories history intentions. therefore recurrent network taking representation source side current turn updating hidden state. decoder recurrent network language modeling outputs symbol time. output dependent current intention intention network. also pays attention particular words source side. details conversation totoal turns. turn user source side denoted length agent superscript input sequence target side denoted superscript responds user length proposed model conditional model target given source py|x). confusion omit session index following. encoder network reads input sentence converts ﬁxed-length variable length representation source side sequence. many choices encode source side. approach form output encoder last hidden state activity used representation source side current turn intention network. form variable-length representation used attention model described sec. general description variable length representation follows signal encoder network intention network model intention process. following intention process dynamic process model intrinsic dynamics conversation intention turn dependent intention previous turn. property might modeled using markov model choose rnn. interestingly hidden state certain turn considered distributed representation intention. different usual process training distributed representation words distribution representation intentions trained previous turns context. ﬁrst order model hidden state dependent explicitly previous state. using weighted averz summerizes variable-length source side representations age. weight computed using content-based alignment model produces high scores target side hidden state previous time similar. formally weight context alignment model enables attention particular words represented vector source side. since decoder network generates responses condition attention also intention model called attention intention model. recurrent networks implemented using recently proposed depth-gated long-shortterm memory network context vector embedding vector source side word time used in-house dialogue dataset. dataset consists dialogues helpdesk chat service. service costumers seeks helps computer related issues human agents. training consists dialogues turns conversations. number tokens source side target side. vocabulary size including words side. development data dialogues turns. test data dialogues turns. sentence-level without momentum. learning rate initialized development used control learning rate. learning rate halved perplexity development increased. epoch training pass training data. order training dialogues randomly shufﬂed beginning epoch. order turns dialogue however kept. objective comparison different models conversation still open question. report perplexity though drawbacks compare different models. table presents results perplexity models different hiden layer sizes. results show larger model hidden layer dimension lower model dimension. table lists example conversation process human trained model. model layers lstms setups used similarly observed model produces natural responses user inputs. intentions clearly seen example. work related recent works encoder-decoder framework model conversation. work model single turn conversation. work simple encoder-decoder method using ﬁxed-dimension representation source side. work also uses ﬁxed-dimension representaiton source side additional model dialogue context. additional similar intention model. however model differs incorprates concept attention intention based theory therefore attention mechanism essential awi. model doesn’t attention model. user agent user computer responding agent user agent recent changes made computer user agent user agent user agent user agent ahead start working issue within ofﬁce premium software support service user agent agent user agent user agent user agent user agent user agent user agent user agent user agent agent agent user agent user agent user agent allright case issue comes back within next days perform service free ahead prepare offer start working issue alright please sure phone number please xxx-xxx-xxxx thank problem thank waiting problem next step ahead prepare offers okay thank problem thank contacting answer desk great thanks working like send quick survey starts yes/no question questions experience today. survey takes minute really helps improve service send sure thank problem thank contacting answer desk great goodbye presented model incorporates attention intention processes neural network model. preliminary experiments show model generates natural responses user inputs. future works include experiments common dataset compare different models incorporating objective functions goals.", "year": 2015}