{"title": "Boltzmann Machines and Denoising Autoencoders for Image Denoising", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Image denoising based on a probabilistic model of local image patches has been employed by various researchers, and recently a deep (denoising) autoencoder has been proposed by Burger et al. [2012] and Xie et al. [2012] as a good model for this. In this paper, we propose that another popular family of models in the field of deep learning, called Boltzmann machines, can perform image denoising as well as, or in certain cases of high level of noise, better than denoising autoencoders. We empirically evaluate the two models on three different sets of images with different types and levels of noise. Throughout the experiments we also examine the effect of the depth of the models. The experiments confirmed our claim and revealed that the performance can be improved by adding more hidden layers, especially when the level of noise is high.", "text": "image denoising based probabilistic model local image patches employed various researchers recently deep autoencoder proposed burger good model this. paper propose another popular family models ﬁeld deep learning called boltzmann machines perform image denoising well certain cases high level noise better denoising autoencoders. empirically evaluate models three different sets images different types levels noise. throughout experiments also examine effect depth models. experiments conﬁrmed claim revealed performance improved adding hidden layers especially level noise high. numerous approaches based machine learning proposed image denoising tasks time. dominant approach perform denoising based local statistics whole image. instance method denoises small image patch extracted whole noisy image reconstructs clean image denoised patches. approach possible pixels image patch representations another domain instance wavelet domain case using pixels sparse coding method choice. hyv¨arinen proposed independent component analysis estimate dictionary sparse elements compute sparse code image patches. subsequently shrinkage nonlinear function applied estimated sparse code elements suppress elements small absolute magnitude. sparse code elements used reconstruct noise-free image patch. recently elad aharon also showed sparse overcomplete representation useful denoising images. researchers claimed better denoising performance achieved using variant sparse coding methods essence approaches build probabilistic model natural image patches using layer sparse latent variables. posterior distribution noisy patch either exactly computed estimated noise-free patch reconstructed expectation conditional distribution posterior distribution. based interpretation researchers proposed recently utilize model layers latent variables image denoising. burger showed deep multi-layer perceptron learns mapping noisy image patch corresponding clean version perform good state-of-the-art denoising methods. similarly proposed variant stacked denoising autoencoder effective image denoising. also able show denoising approach based deep neural networks performed good sometimes better than conventional state-of-the-art methods. along line research propose another type deep neural networks image denoising paper. gaussian-bernoulli restricted boltzmann machines deep boltzmann machines empirically shown perform well image denoising compared stacked denoising autoencoders. furthermore extensively evaluate effect number hidden layers boltzmann machine-based deep models autoencoder-based ones. empirical evaluation conducted using different noise types levels three different sets images. originally proposed boltzmann machine especially structural constrained version restricted boltzmann machine become increasingly important machine learning since showed powerful deep neural network trained easily stacking rbms other. recently another variant called deep boltzmann machine proposed shown outperform conventional machine learning methods many tasks although update rules based gradients log-likelihood function well deﬁned intractable exactly compute them. hence approach uses variational approximation together markov chain monte carlo sampling proposed salakhutdinov hinton however found training gdbm using approach starting randomly initialized parameters trivial hence salakhutdinov hinton proposed pretraining algorithms initialize parameters dbms. paper pretraining algorithm proposed gaussian-bernoulli special case gdbm number hidden layers restricted restriction possible compute posterior distribution hidden units conditioned visible units exactly tractably. conditional probability hidden unit hence positive part gradient needs approximated variational approach case gdbms computed exactly efﬁciently. negative part computed model distribution still relies mcmc sampling approximate methods contrastive divergence denoising autoencoder special form multi-layer perceptron network hidden layers sets tied weights. tries learn network reconstructs input vector optimally minimizing following cost function encoding decoding functions l-th layer component-wise nonlinearity function weights l-th layers shared encoder decoder. notational simplicity omit biases units. unlike ordinary autoencoder explicitly sets components input vector randomly zero learning explicitly adds noise input vector. usual combine different types noise using additive isotropic gaussian noise masking noise ﬁrst type adds zero-mean gaussian noise input component masking noise sets randomly chosen input components zeros. then trained denoise corrupted input. training straightforward using backpropagation algorithm computes gradient objective function using chain-rule dynamic programming. vincent proposed training deep becomes easier weights deep initialized greedily pretraining layer deep single-layer dae. following experiments section follow approach initialize weights subsequently ﬁnetune network stochastic backpropagation. mentioned earlier noisy large image denoised denoising small patches image combining together. deﬁne binary matrices rp×d extract small image patches given large whole image product width height number color channels image size several ﬂexibilities constructing matrix obvious size image patch. although standard approach many previous attempts tend patch sizes small another called stride number pixels consecutive patches. taking every possible patch option overlapping patches pixels would reduce computational complexity. popular choices construct probabilistic model latent variables describe natural image patches. instance sparse coding essence probabilistic model single layer latent variables common choice. hyv¨arinen used nonlinear shrinkage function compute sparse code image patch elad aharon used k-svd build sparse code dictionary. approach denoising considered two-step reconstruction. initially posterior distribution latent variables computed estimated given image patch. given estimated posterior distribution conditional distribution mean visible units computed used denoised image patch. noisy input patch. words mean conditional distribution visible units respect posterior distribution hidden units given visible units ﬁxed corrupted input image patch. however since taking expectation posterior distribution usually tractable exactly computable often easier approximate quantity. approximate marginal conditional distribution given unlike grbm posterior distribution hidden units gdbm neither tractably computable analytical form. salakhutdinov hinton proposed utilize variational approximation fully-factored distribution convergence variational parameters take much time suitable practice. hence experiments initialize variational parameters feed-forward propagation using doubled weights perform ﬁxed-point update iterations only. turned good enough compromise least sacriﬁces performance reducing computational complexity signiﬁcantly. encoder part considered performing approximate inference fullyfactorial posterior distribution top-layer hidden units i.e. bottleneck given input image patch hence similar approach taken used daes. recently burger tried deep manner perform image denoising. reported denoising performance achieved daes comparable sometimes favorable conventional image denoising methods k-svd bayes least squares-gaussian scale mixture order answer questions vary depth models level noise injection type noise–either white gaussian additive noise salt-and-pepper noise size image patches. also interest lies generalization capability models completely separate data training models apply trained models three distinct sets images different properties. used three sets images textures aerials miscellaneous usc-sipi image database test images. tab. lists details image sets fig. presents sample images test sets. datasets terms contents properties images different other. instance images texture highly repetitive patterns present images sets. images aerials coarse structures example lake nearby road time single image. also sizes images vary quite across test sets across images set. aiming evaluate performance denoising general image used large separate data natural image patches train models. extracted random image patches sizes cifar- dataset image training samples cifar- dataset patches randomly selected locations collected. tried three different depth settings boltzmann machines denoising autoencoders; single hidden layer hidden layers four hidden layers. sizes hidden layers number hidden units constant factor multiplied number pixels image patch. denote boltzmann machines four hidden layers grbm gdbm gdbm respectively. denoising autoencoders denoted respectively. model structure model trained image patches sizes grbms trained using enhanced gradient persistent contrastive divergence gdbms trained initializing parameters two-stage pretraining algorithm daes trained stochastic backpropagation algorithm hidden layers pretrained layer single-layer sparsity target images type level noise assumed training deep neural networks. words separate training done different types levels noise injected test images. unlike this instance trained speciﬁcally noise level changing accordingly. furthermore boltzmann machines propose image denoising require prior knowledge level type noise. types noise tested; white gaussian salt-and-pepper. white gaussian noise simply adds zero-mean normal random value predeﬁned variance image pixel salt-and-pepper noise sets randomly chosen subset pixels either black white. furthermore three different noise levels tested. case white gaussian noise used standard deviations case salt-and-pepper noise used noise probability. noise injected image preprocessed pixel-wise adaptive wiener ﬁltering following approach hyv¨arinen width height pixel neighborhood chosen small enough remove much detail input image. obvious observation deep neural networks including daes show improvement shallow counterparts low-noise regime however deeper models signiﬁcantly outperformed corresponding shallow models level another notable phenomenon gdbms tend behind daes even grbm noise regime except textures set. possible explanation rather poor performance gdbms noise regime approximate inference posterior distribution used experiment might good enough. instance mean-ﬁeld iterations might improved overall performance dramatically increasing computational time would allow gdbms practical value. gdbms however outperformed performed comparably models level injected noise higher. noticed performance depended type test images. instance although images aerials corrupted salt-and-pepper noise best denoised four hidden layers gdbms outperformed case textures set. emphasize deeper neural networks showed less performance variance depending type test images suggests better generalization capability deeper neural networks. visual inspection denoised images provides intuition performances deep neural networks. fig. denoised images sample image test image displayed. shows tend emphasize detailed structure image daes especially ones hidden layers tend capture global structure. additionally tried experiments using models trained random image patches extracted berkeley segmentation dataset martin case patches randomly chosen locations images collected form training set. obtained results similar presented paper. results presented appendix table performance models trained image patches level injected noise standard deviations shown inside parentheses best performing models marked bold. paper proposed that addition daes boltzmann machines grbms gdbms also used denoising images. furthermore tried empirical evidence supporting deep neural networks image denoising tasks. case daes experiments clearly show hidden layers improve performance especially level noise high. always apply found grbms outperformed performed well gdbms cases. regardlessly high noise regime always beneﬁcial hidden layers. figure images corrupted salt-and-pepper noise noise probability denoised various deep neural networks trained image patches. number denoised image psnr. deep neural networks trained completely separate dataset applied three test sets different image properties. turned performance depended test however small differences. also trend deeper models performing better could observed almost cases especially high level noise. suggests well-trained deep neural network perform blind image denoising prior information target noisy images available well. four hidden layers turned best performer general beating gdbms number hidden layers. however level noise high boltzmann machines grbm gdbm able outperform daes suggests boltzmann machines robust noise. noticeable observation grbm outperformed many cases hidden layers twice many parameter. potentially suggests better inference approximate posterior distribution hidden units might make gdbms outperform comparable daes number hidden layers units. work required future make deﬁnite answer question. although difﬁcult make general conclusion experiments evident deep models regardless whether daes performed better robust level noise shallow counterparts. future might appealing investigate possibility combining multiple deep neural networks various depths achieve better denoising performance.", "year": 2013}