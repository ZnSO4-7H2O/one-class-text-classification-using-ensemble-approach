{"title": "Playing SNES in the Retro Learning Environment", "tag": ["cs.LG", "cs.AI"], "abstract": "Mastering a video game requires skill, tactics and strategy. While these attributes may be acquired naturally by human players, teaching them to a computer program is a far more challenging task. In recent years, extensive research was carried out in the field of reinforcement learning and numerous algorithms were introduced, aiming to learn how to perform human tasks such as playing video games. As a result, the Arcade Learning Environment (ALE) (Bellemare et al., 2013) has become a commonly used benchmark environment allowing algorithms to train on various Atari 2600 games. In many games the state-of-the-art algorithms outperform humans. In this paper we introduce a new learning environment, the Retro Learning Environment --- RLE, that can run games on the Super Nintendo Entertainment System (SNES), Sega Genesis and several other gaming consoles. The environment is expandable, allowing for more video games and consoles to be easily added to the environment, while maintaining the same interface as ALE. Moreover, RLE is compatible with Python and Torch. SNES games pose a significant challenge to current algorithms due to their higher level of complexity and versatility.", "text": "nadav bhonker* shai rozenberg* itay hubara department electrical engineering technion israel institute technology indicates equal contribution {nadavbhshairoz}tx.technion.ac.il itayhubaragmail.com mastering video game requires skill tactics strategy. attributes acquired naturally human players teaching computer program challenging task. recent years extensive research carried ﬁeld reinforcement learning numerous algorithms introduced aiming learn perform human tasks playing video games. result arcade learning environment become commonly used benchmark environment allowing algorithms train various atari games. many games state-of-the-art algorithms outperform humans. paper introduce learning environment retro learning environment games super nintendo entertainment system sega genesis several gaming consoles. environment expandable allowing video games consoles easily added environment maintaining interface ale. moreover compatible python torch. snes games pose significant challenge current algorithms higher level complexity versatility. controlling artiﬁcial agents using high-dimensional input data image sound difﬁcult important task ﬁeld reinforcement learning recent breakthroughs ﬁeld allow utilization real-world applications autonomous driving navigation more. agent interaction real world usually either expensive feasible real world complex agent perceive. therefore practice interaction simulated virtual environment receives feedback decision made algorithm. traditionally games used environment dating back chess checkers backgammon recent modern games often present problems tasks highly correlated real-world problems. example agent masters racing game observing simulated driver’s view screen input usefull development autonomous driver. high-dimensional input leading benchmark arcade learning environment provides common interface dozens atari games presents different challenge. provides extensive benchmarking platform allowing controlled experiment setup algorithm evaluation comparison. main challenge posed successfully play many atari games possible without providing algorithm game-speciﬁc information work tackle problem deep q-networks algorithm made breakthrough ﬁeld deep reinforcement learning achieving human level performance games. work present environment retro learning environment sets challenges providing uniﬁed interface atari games well advanced gaming consoles. start focused super nintendo entertainment system snes games tested using state-of-the-art algorithms able outperform expert human player. additional feature supports research multi-agent reinforcement learning tasks utilize feature training evaluating agents other rather pre-conﬁgured in-game conducted several experiments feature discovered agents tend learn overcome current opponent rather generalize game played. however agent trained ensemble different opponents robustness increases. main contributions paper follows arcade learning environment software framework designed development algorithms playing atari games. interface provided allows algorithms select action receive atari screen reward every step. action equivalent human’s joystick button combination reward difference scores time stamp diversity games atari provides solid benchmark since different games signiﬁcantly different goals. atari games currently implemented commonly used algorithm comparison. inﬁnite mario remake classic super mario game levels randomly generated. levels mario competition held. competition several algorithms trained inﬁnite mario performances measured terms number stages completed. opposed training based screen data rather indication mario’s location objects surrounding. environment longer poses challenge state algorithms. main shortcoming fact provides single game learnt. additionally environment provides hand-crafted features extracted directly simulator algorithm. allowed planning algorithms highly outperform learning based algorithm. openai open source platform purpose creating interface environments algorithms evaluation comparison purposes. openai currently popular large number environments supported example mouintaincar vizdoom environment learning ﬁrst-person-shooter game doom. openai gym’s recent appearance wide usage indicates growing interest research done ﬁeld universe platform within openai framework algorithms train thousand games. universe includes advanced games portal well tasks unlike universe doesn’t games locally requires interface server runs games. leads lower frame rate thus longer training times. malmo artiﬁcial intelligence experimentation platform famous game minecraft. although malmo consists single game presents numerous challenges since minecraft game conﬁgured differently time. input algorithms include speciﬁc features indicating state game current reward. deepmind ﬁrst-person platform environment allows training algorithms several different challenges static/random navigation collect fruit laser-tag challenge objective opponents controlled in-game agent observations game screen velocity character. supports four games work used several variant deep q-network algorithm algorithm whose goal optimal policy state game simply game screen action combination joystick buttons game responds learns trial error trying estimate q-function predicts cumulative discounted reward episode given current state action following policy q-function represented using convolution neural network receives screen input predicts best possible action it’s output. q-function weights updated according current next states action chosen step size discounting factor reward received applying represents previous weights network updated periodically. examined leading algorithms double deep q-learning based algorithm modiﬁed network update rule. dueling double modiﬁcation d-dqn’s architecture q-function modeled using state dependent estimator action dependent estimator. super nintendo entertainment system home video game console developed nintendo released total games released among them iconic super mario world donkey kong country legend zelda. table presents comparison atari sega genesis snes game consoles clear snes genesis games complex. allow easier integration current platforms algorithms based environment maintaining much interface possible. highly coupled atari emulator stella takes different approach separates learning environment emulator. achieved incorporating interface named libretro allows communication front-end programs game-console emulators. currently libretro supports game consoles containing hundreds games estimated total games potentially supported using interface. examples supported game consoles include nintendo entertainment system game sega genesis saturn dreamcast sony playstation. chose focus snes game console implemented using snesx it’s games present interesting plausible overcome challenges. additionally utilized genesis-plus-gx emulator supports several sega consoles genesis/mega drive master system game gear sg-. fully available open source software gnu’s general public license. environment implemented interface algorithms python lua. adding game environment relatively simple process. provides uniﬁed interface games supported consoles acting rl-wrapper libretro interface. initialization environment done providing game gaming-console upon initialization ﬁrst state initial frame game skipping menu selection screens. cores provided installed together environment. actions bit-wise representation controller button represented one-hot vector. therefore combination several buttons possible using bit-wise operator. number valid buttons combinations larger therefore meaningful combinations provided. environments observation game screen provided array pixel dimensions vary depending game. reward deﬁned differently game usually score difference consecutive frames. setting different conﬁguration environment possible alter in-game properties difﬁculty characters levels etc. integrating snes genesis presents challenges ﬁeld visual information form image state available agent. obviously snes games signiﬁcantly complex unpredictable atari games. example sports games player controls single player nine players’ behavior determined pre-programmed agents exhibiting random behavior. addition many snes games exhibit delayed rewards course play similarly snes games agent obtain reward indirectly related imposed task. example platform games super mario reward received collecting coins defeating enemies goal challenge reach level requires move keep moving right. moreover upon completing level score bonus given according time required completion. therefore collecting coins defeating enemies necessarily preferable consumes much time. analysis games presented section moreover unlike atari consists eight directions action button snes eight-directions actions buttons. since combinations buttons allowed required times actual actions space larger compared maximum actions atari. furthermore background snes rich ﬁlled details move locally across screen effectively acting non-stationary noise since provided little information regarding state itself. finally note snes utilized ﬁrst games. game wolfenstein player must navigate maze ﬁrst-person perspective dodging attacking enemies. snes offers plenty games ﬂight racing games exhibit similar challenges. games much realistic thus inferring snes games real world tasks case self driving cars might beneﬁcial. visual comparison games atari snes presented figure figure atari snes game screen comparison left boxing atari ﬁghting game right mortal kombat snes ﬁghting game. note exceptional difference amount details games. therefore distinguishing relevant signal noise much difﬁcult. evaluation methodology used benchmarking different algorithms popular method proposed examined algorithm trained either reached convergence epochs thereafter evaluated performing episodes every game. episode ends either reaching terminal state minutes. results averaged game compared average result human player. game human player given hours training performances evaluated episodes. various algorithms don’t game audio learning process audio muted agent human. both humans agents score random agent score subtracted assure learning indeed occurred. important note dqn’s \u0001-greedy approach present testing thus assuring sequence actions isn’t repeated. screen dimensions snes larger atari experiments maintained pre-processing argue downscaling image size doesn’t affect human’s ability play game therefore suitable algorithms well. handle large action space limited algorithm’s actions minimal button combinations provide unique behavior. example many games action buttons don’t therefore combinations omitted. thorough comparison four different agents’ performances snes games seen figure full results found table game mortal kombat trained agent able surpass expert human player performance opposed atari games algorithms surpassed human player vast majority games. example wolfenstein game ﬁrst-person shooter game requires solving vision tasks navigating maze detecting object. evident ﬁgure agents produce poor results indicating lack required properties. using \u0001-greedy approach agents weren’t able explore enough states algorithm’s ﬁnal policy appeared random walk space. exploration based visited states presented bellemare might help addressing issue. interesting case gradius side-scrolling ﬂight-shooter game. trained agent able master technical aspects game includes shooting incoming enemies dodging projectiles it’s ﬁnal score still human’s. hidden game mechanism form power-ups accumulated signiﬁcantly increase players abilities. power-ups collected without larger ﬁnal impact game-mechanism evident human agent acts myopically uses power-up straight away. part environment algorithm evaluation process investigated case studies. first game failed achieve better-than-random score second game training duration signiﬁcantly longer games. ﬁrst case study used back-view racing game f-zero. game required complete four laps track avoiding race cars. reward deﬁned score game received upon completing lap. extreme case reward delay. last long seconds span states reward received. since dqn’s exploration simple \u0001-greedy approach able produce useful strategy. approached issue using reward shaping essentially modiﬁcation reward function reward observation rather reward alone. here deﬁne reward score agent’s speed indeed reward deﬁned such agents learned ﬁnish race ﬁrst place within short training period. second case study famous game super mario. game agent mario required reach right-hand side screen avoiding enemies collecting coins. found case interesting involves several challenges once dynamic background change drastically within level sparse delayed rewards multiple tasks surprise able reach level without reward shaping possible since agent receives rewards events tend appear right player causing agent prefer moving right. however training time required convergence signiﬁcantly longer games. deﬁned reward in-game reward bonus granted according player’s position making moving right preferable. reward proved useful training time required convergence decreased signiﬁcantly. games seen figure figure ddqn duel-ddqn performance. results normalized subtracting random agent’s score dividing human player score. thus represents human player zero random agent. figure illustrates agent’s average value function though able complete stage trained upon convergence rate reward shaping signiﬁcantly quicker immediate realization agent move rightwards. figure left game super mario added bonus moving right enabling agent master game less training time. right game f-zero. granting reward speed agent able master game oppose using solely in-game reward. section describe experiments rle’s multi-agent capabilities. consider case number agents goals agents opposite scheme known fully competitive used simple singleagent approach section apply single agent approach multi-agent case. approach proved useful crites barto matari´c elaborate schemes possible minimax-q algorithm explored future works. conducted three experiments setup ﬁrst train different agents in-game done previous sections evaluate performance letting compete other. here rather achieving highest score goal tournament consist rounds common human-player competitions. second experiment initially train agents in-game resume training competing other. case evaluated agent playing in-game separately. finally last experiment boost agent capabilities alternated it’s opponents switching in-game trained agents. chose game mortal kombat character side viewed ﬁghting game testbed above exhibits favorable properties players share screen agent’s optimal policy heavily dependent rival’s behavior unlike racing games example. order evaluate agents fairly trained using characters maintaining identity rival agent. furthermore remove impact starting positions agents performances starting positions initialized randomly. ﬁrst experiment evaluated combinations d-dqn dueling d-dqn. agent trained in-game convergence. matches performed agents. lost games dueling d-dqn d-dqn. d-dqn lost time dueling d-dqn. balance isn’t random case since algorithms converged policy movement towards opponent required rather generalize game. therefore many episodes little interaction agents occur leading semi-random outcome. second experiment continued training process d-dqn network letting compete dueling d-dqn network. evaluated re-trained network playing episodes in-game training d-dqn able games faced in-game performance deteriorated drastically objective measure generalization we’ve conﬁgured in-game difﬁculty very hard metric alternating version achieved compared dueling d-dqn trained default setting. thus proving agent learned generalize policies weren’t observed training. demonstrated presents numerous challenges answered. addition able learn available games task learning games reward delay extreme f-zero without reward shaping remains unsolved challenge. additionally games super mario feature several stages differ background levels structure. task generalizing platform games learning stage tested other another unexplored challenge. likewise surpassing human performance remains challenge since current state-of-the-art algorithms still struggling many snes games. introduced rich environment evaluating developing reinforcement learning algorithms presents signiﬁcant challenges current state-of-the-art algorithms. comparison environments provides large amount games access screen ingame state. modular implementation chose allows extensions environment consoles games thus ensuring relevance environment algorithms years come we’ve encountered several games learning process highly dependent reward deﬁnition. issue addressed explored reward deﬁnition done easily. challenges presented consist interpretation delayed reward noisy background stochastic behavior more. although algorithms able play successfully part games fully overcome challenges agent must incorporate technique strategy. therefore believe great platform future research. goodfellow mirza xiao courville bengio. empirical investigation catastrophic forgetting gradient-based neural networks. arxiv preprint arxiv. johnson hofmann hutton bignell. malmo platform artiﬁcial intelligence experimentation. international joint conference artiﬁcial intelligence page littman. markov games framework multi-agent reinforcement learning. proceedings eleventh international conference machine learning volume pages silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search. nature", "year": 2016}