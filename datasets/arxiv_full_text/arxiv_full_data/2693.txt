{"title": "Robust and Efficient Transfer Learning with Hidden-Parameter Markov  Decision Processes", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings. Our new framework correctly models the joint uncertainty in the latent parameters and the state space. We also replace the original Gaussian Process-based model with a Bayesian Neural Network, enabling more scalable inference. Thus, we expand the scope of the HiP-MDP to applications with higher dimensions and more complex dynamics.", "text": "introduce formulation hidden parameter markov decision process framework modeling families related tasks using lowdimensional latent embeddings. framework correctly models joint uncertainty latent parameters state space. also replace original gaussian process-based model bayesian neural network enabling scalable inference. thus expand scope hip-mdp applications higher dimensions complex dynamics. world ﬁlled families tasks similar identical dynamics. example consider task training robot swing unknown length mass task member family bat-swinging tasks. robot already learned swing several bats various lengths masses robot learn swing length mass efﬁciently learning scratch. grossly inefﬁcient develop control policy scratch time unique task encountered. hidden parameter markov decision process developed address type transfer learning optimal policies adapted subtle variations within tasks efﬁcient robust manner. speciﬁcally hip-mdp paradigm introduced low-dimensional latent task parameterization that combined state action completely describes system’s dynamics wb). however original formulation account nonlinear interactions latent parameterization state space approximating dynamics required states visited training. addition original framework scaled poorly used gaussian processes basis functions approximating task’s dynamics. present hip-mdp formulation models interactions latent parameters state transitioning state taking action including latent parameters state action input bayesian neural network learns common transition dynamics family tasks models unique variations particular instance impact instance’s overall dynamics. embedding latent parameters allows accurate uncertainty estimation robust transfer learning control policy possibly unique task instance. formulation also inherits several desirable properties bnns model multimodal heteroskedastic transition functions inference scales data large dimension number samples output dimensions jointly modeled reduces computation increases predictive accuracy herein capture complex dynamical systems highly non-linear interactions state dimensions. furthermore model uncertainty easily quantiﬁed bnn’s output variance. thus scale larger domains previously possible. improved hip-mdp formulation develop control policies acting simple twodimensional navigation domain playing acrobot designing treatment plans simulated patients hip-mdp rapidly determines dynamics instances enabling quickly near-optimal instance-speciﬁc control policies. model-based reinforcement learning consider reinforcement learning problems agent acts continuous state space discrete action space assume environment true transition dynamics unknown agent given reward function provides utility taking action state model-based reinforcement learning setting goal learn approximate transition function based observed transitions learn γtrt] governs hip-mdps hip-mdp describes family markov decision processes deﬁned tuple states actions reward function. transition dynamics task instance depend value hidden parameters instance parameters drawn prior hip-mdp framework assumes ﬁnite-dimensional array hidden parameters fully specify variations among true task dynamics. also assumes system dynamics invariant task agent signaled task ends another begins. bayesian neural networks bayesian neural network neural network parameters random variables prior place independent exact bayesian inference posterior parameters a)}) intractable several recent techniques developed scale inference bnns probabilistic models bnns reduce tendency neural networks overﬁt presence amounts data—just general training computationally efﬁcient still providing coherent uncertainty measurements. speciﬁcally predictive distributions calculated taking averages samples approximated posterior distribution parameters. such bnns adopted estimation stochastic dynamical systems dimension basis transition function ˆtkad using input linearly combined instance-speciﬁc weights wbk. inference involves learning parameters basis functions weights instance. robustly approximate stochastic state transitions present simpliﬁed version omits ﬁltering variables zkad make parallels formulation original explicit; simpliﬁcation change properties. continuous dynamical systems model-based reinforcement learning also widely used transfer learning outside formulation expressive limitations. primary limitation uncertainty latent parameters modeled independently agent’s state uncertainty. hence model account interactions latent parameterization state result doshi-velez konidaris required task instance performed state-action combinations training. training sometimes possible—e.g. robots driven identical positions—it onerous best impossible systems human patients. secondary limitation output dimension modeled separately collection basis functions ˆtkad}k basis functions output dimension independent basis functions output dimension hence model account correlation output dimensions. modeling correlations typically requires knowledge dimensions interact approximated dynamical system choose constrain hip-mdp priori knowledge since provide basis functions ascertain relationships observed transitions. overcome limitations include instance-speciﬁc weights input transition function model dimensions output jointly critical modeling change eliminates limitations learn directly data observed—which abundant many industrial health domains—and longer require highly constrained training procedure. also capture correlations outputs domains occur many natural processes. finally computational demands using transition function limited application original hip-mdp formulation relatively small domains. following rather model transition function. computational requirements needed learn gp-based transition function makes direct comparison bnn-based formulation infeasible within experiments demonstrate appendix bnn-based transition model exceeds gp-based transition model computational predictive performance. addition bnns naturally produce multi-dimensional outputs without requiring prior knowledge relationships dimensions. allows directly model output correlations state dimensions leading uniﬁed coherent transition model. inference larger input space large number samples tractable using efﬁcient approaches us—given distribution input-output tuples )—estimate distribution latent embedding enables robust scalable transfer. demonstration present domain agent tasked navigating goal region. state space continuous action space discrete task instances vary following domain aspects location wall blocks access goal region orientation cardinal directions direction nonlinear wind effect increases agent moves away start region. ignoring wall grid boundaries transition dynamics figure demonstration hipmdp modeling joint uncertainty latent parameters state space. left blue dots show exploration blue instances. latent parameters learned instance used predict transitions taking action area state space either unexplored explored instance. prediction variance provides estimate joint uncertainty latent parameters state. penalized trying cross wall step incurs small cost agent reaches goal region encouraging agent discover goal region shortest route possible. episode terminates agent enters goal region time steps. linear function state latent parameters would struggle model classes instances domain state transition resulting taking action nonlinear function interactions state hidden parameter contrast hip-mdp model allows nonlinear interactions state latent parameters well jointly models uncertainty. figure produces measurable differences transition uncertainty regions related observed transitions even many observations unrelated instances. here hip-mdp trained instances distinct classes left). display uncertainty transition function using latent parameters wred inferred instance regions domain area explored instances area explored instances explored blue instances. transition uncertainty three times larger region instances been—even many blue instances there—than regions instances commonly explored demonstrating latent parameters different effects transition uncertainty different states. algorithm summarizes inference procedure learning policy task instance facilitated pre-trained task similar structure prior work procedure involves several parts. speciﬁcally start instance global replay buffer observed transitions posterior weights transition function learned data ﬁrst objective quickly determine latent embedding current instance’s speciﬁc dynamical variation transitions observed current instance. transitions instance stored global replay buffer instance-speciﬁc replay buffer second objective develop optimal control policy using transition model learned latent parameters transition model latent embedding separately updated mini-batch stochastic gradient descent using adam using planning increases sample efﬁciency reduce interactions environment. describe parts detail below. updating embedding parameters instance latent weighting sampled prior preparation estimating unobserved dynamics introduced next observe transitions task instance initial exploratory episode given input global replay buffer transition function initial state draw randomly init. policy init. instance replay buffer init. ﬁctional replay buffer episodes data optimize latent parameters minimize α-divergence posterior predictions true state transitions here minimization occurs adjusting latent embedding holding parameters ﬁxed. initial update newly encountered instance parameters transition function optimized trained multiple instances task found additional data needed reﬁne latent instance provided initial exploratory episode. otherwise additional data subsequent episodes used improve latent estimates mini-batches used optimizing latent network parameters sampled squared error prioritization found switching small updates latent parameters small updates parameters best transfer performance. either network latent parameters updated aggressively disregards latent parameters state inputs respectively. completing instance parameters latent parameters updated using samples global replay buffer speciﬁc modeling details number epochs learning rates etc. described appendix construct ε-greedy policy select actions based approximate action-value function model action value function double deep network ddqn involves training networks primary qnetwork informs policy target q-network slowly annealed copy primary network providing greater stability updating policy updated transition function approximate environment developing control policy simulate batches entire episodes length using approximate dynamical model storing transition ﬁctional experience replay buffer primary network parameters updated every time steps minimize temporal-difference error primary network’s target network’s q-values. mini-batches used update sampled ﬁctional experience replay buffer using td-error-based prioritization demonstrate performance hip-mdp embedded latent parameters transferring learning across various instances task. revisit demonstration problem section well describe results acrobot complex healthcare domain prescribing effective treatments patients varying physiologies. domains compare formulation hip-mdp embedded latent parameters four baselines demonstrate efﬁciency learning policy instance using hip-mdp. comparisons made across ﬁrst handful episodes encountered task instance highlight advantage provided transferring information hip-mdp. ‘linear’ baseline uses learn basis functions linearly combined parameters equation allow interactions states weights. ‘model-based scratch’ baseline considers task instance unique; requiring transition function trained observations made current task instance. ‘average’ model baseline constructed assumption single transition function used every instance task; trained observations task instances together. model-based approaches replicated hip-mdp procedure closely possible. trained observations single episode used generate large batch approximate transition data policy learned. finally model-free baseline learns ddqn-policy directly observations current instance. information experimental speciﬁcations long-run policy learning appendix respectively. figure demonstration model-free control policy comparison learning policy outset task instance using hip-mdp versus four benchmarks. hip-mdp embedded outperforms four benchmarks. hip-mdp average model supplied transition model trained previous instances class updated according procedure outlined sec. newly encountered instance. ﬁrst exploratory episode hip-mdp sufﬁciently determined latent embedding evidenced figure developed policy clearly outperforms four benchmarks. implies transition model adequately provides accuracy needed develop optimal policy aided learned latent parametrization. hip-mdp linear also quickly adapts instance learns good policy. however hip-mdp linear unable model nonlinear interaction latent parameters state. therefore model less accurate learns less consistent policy hip-mdp embedded single episode data model trained scratch current instance accurate enough learn good policy. training scratch requires observations true dynamics necessary hip-mdp learn latent parameterization achieve high level accuracy. model-free approach eventually learns optimal policy requires signiﬁcantly observations represented figure model-free approach improvement ﬁrst episodes. poor performance average model approach indicates single model cannot adequately represent dynamics different task instances. hence learning latent representation dynamics speciﬁc instance crucial. first introduced sutton barto acrobot canonical control problem. common objective domain agent swing two-link pendulum applying positive neutral negative torque joint links actions must performed sequence bottom link reaches predetermined height pendulum. state space consists angles angular velocities hidden parameters corresponding masses lengths links. appendix details hidden parameters varied create different task instances. policy learned setting acrobot generally perform poorly settings system noted thus subtle changes physical parameters require separate policies adequately control varied dynamical behavior introduced. provides perfect opportunity apply hip-mdp transfer separate acrobot instances learning control policy current instance. figure shows hip-mdp learns optimal policy single episode whereas model-based benchmarks required additional episode training. example model-free approach eventually learns optimal policy requires time. determining effective treatment protocols patients introduced problem mathematically representing patient’s physiological response separate classes treatments model state patient’s health recorded separate markers measured blood test. patients given four treatments regular schedule. either given treatment classes drugs mixture treatments provided treatment hidden parameters system control patient’s speciﬁc physiology dictate rates virulence cell birth infection death. visual representation patient transitioning unhealthy steady state healthy steady state using proper treatment schedule comparison learning policy task instance using hip-mdp versus four benchmarks. details.) objective develop treatment sequence transitions patient unhealthy steady state healthy steady state small changes made parameters greatly effect behavior system therefore introduce separate steady state regions require unique policies transition them. figure shows hip-mdp develops optimal control policy single episode learning unmatched optimal policy shortest time. simulator complex three domains separation benchmark pronounced. modeling dynamical system scratch single episode observations proved infeasible. average model trained large batch observations related dynamical systems learns better policy. hip-mdp linear able transfer knowledge previous task instances quickly learn latent parameterization instance leading even better policy. however dynamical system contains nonlinear interactions latent parameters state space. unlike hip-mdp embedded hip-mdp linear unable model interactions. demonstrates superiority hipmdp embedded efﬁciently transferring knowledge instances highly complex domains. large body work solving single pomdp models efﬁciently contrast transfer learning approaches leverage training done task perform related tasks. strategies transfer learning include latent variable models reusing pre-trained model parameters learning mapping separate tasks work falls latent variable model category. using latent representation relate tasks particularly popular robotics similar physical movements exploited across variety tasks platforms chen latent representations encoded separate mdps accompanying index agent learns adapting observed variations environment. take closely related approach updated formulation hip-mdp incorporating estimates unknown partially observed parameters known environmental model reﬁning estimates using model-based bayesian core difference work learn transition model observed variations directly data assume given speciﬁc variations parameters learned. also related multi-task approaches train single model multiple tasks simultaneously finally many applications reinforcement learning transfer learning healthcare domain identifying subgroups similar response broadly bnns powerful probabilistic inference models allow estimation stochastic dynamical systems core functionality ability represent model uncertainty transition stochasticity recent work decomposes forms uncertainty isolate separate streams information improve learning. ﬁxed latent variables input helps account model uncertainty transferring pretrained instance task. approaches stochastic latent variable inputs introduce transition stochasticity view hip-mdp latent embedding methodology facilitate personalization robustly transfers knowledge prior observations current instance. approach especially useful extending personalized care groups patients similar diagnoses also extended control system variations present. present formulation transfer learning among related tasks similar identical dynamics within hip-mdp framework. approach leverages latent embedding—learned optimized online fashion—to approximate true dynamics task. adjustment hip-mdp provides robust efﬁcient learning faced varied dynamical systems unique previously learned. able virtue transfer learning rapidly determine optimal control policies faced unique instance. results work assume presence large batch already-collected data. setting common many industrial health domains months sometimes years worth operations data plant function product performance patient health. even large batches instance still requires collapsing uncertainty around instance-speciﬁc parameters order quickly perform well task. section used batch transition data multiple instances task—without artiﬁcial exploration procedure—to train learn latent parameterizations. seeded data diverse task instances latent parameters accounted variation instances. primarily interested settings batches observational data exist might also interested traditional settings ﬁrst instance completely second instance information ﬁrst etc. initial explorations found indeed learn online manner simpler domains. however even simple domains model-selection problem becomes challenging overly expressive overﬁt ﬁrst instances hard time adapting sees data instance different dynamics. model-selection approaches allow learn online starting scratch interesting future research direction. another interesting extension rapidly identifying latent exploration identify would supply dynamical model data regions domain largest uncertainty. could lead accurate latent representation observed dynamics also improving overall accuracy transition model. also found training requires careful exploration strategies. exploration constrained early quickly converges suboptimal deterministic policy––often choosing action step. training along bnn’s trajectories least certainty could lead improved coverage domain result robust policies. development effective policies would greatly accelerated exploration robust stable. could also hidden parameters learn policy directly. recognizing structure latent embeddings task variations enables form transfer learning robust efﬁcient. extension hip-mdp demonstrates embedding low-dimensional latent representation input approximate dynamical model facilitates transfer results accurate model complex dynamical system interactions input state latent representation modeled naturally. also model correlations output dimensions replacing basis functions original hip-mdp formulation bnn. transition function scales signiﬁcantly better larger complex problems. improvements hip-mdp provide foundation robust efﬁcient transfer learning. future improvements work contribute general transfer learning framework capable addressing nuanced complex control problems. acknowledgements thank mike hughes andrew miller jessica forde andrew ross helpful conversations. supported lincoln laboratory lincoln scholars depeweg hernández-lobato doshi-velez udluft. learning policy search stochastic dynamical systems bayesian neural networks. international conference learning representations doshi-velez konidaris. hidden parameter markov decision processes semiparametric regression approach discovering latent task parametrizations. proceedings twenty-fifth international joint conference artiﬁcial intelligence volume pages ernst stan goncalves wehenkel. clinical data based optimal strategies reinforcement learning approach. proceedings ieee conference decision control marivate chemali brunskill littman. quantifying uncertainty batch personalized sequential decision making. workshops twenty-eighth aaai conference artiﬁcial intelligence moore pyeatt kulkarni panousis padrez doufas. reinforcement learning closed-loop propofol anesthesia study human volunteers. journal machine learning research shortreed laber lizotte stroup pineau murphy. informing sequential clinical decision-making reinforcement learning empirical study. machine learning williams young. scaling pomdps dialog management composite summary point-based value iteration aaai workshop statistical empirical approaches spoken dialogue systems pages section demonstrate computational motivation replace basis functions original hip-mdp model single stand-alone discussed sec. using navigation domain. fully motivate replacement altered gp-based model accept latent parameters one-hot encoded action additional inputs transition model. done investigate performance would scale higher input dimension; original formulation hip-mdp uses input dimensions proposed reformulation hip-mdp uses figure show time episode approximated transition model latent parameters updated every episodes. navigation domain completion time relatively constant whereas gp’s completion time drastically increases data collected construct transition model. directly compare run-time performance training gp-based bnn-based hipmdp unique instances domain episodes instance. figure shows running times episode gp-based hip-mdp bnn-based hip-mdp transition model latent parameters updated every episodes. stark contrast increase computation gp-based hip-mdp bnn-based hip-mdp increase computation time data training encountered. training course separate episodes domain completed little hours. contrast gp-based hip-mdp trained domain took close hours complete training number episodes. signiﬁcant increase computation time using gp-based hip-mdp relatively simple domain prevented performing comparisons model domains. chose bnns going make many inference approximations seemed reasonable turn model easily capture heteroskedastic multi-modal noise correlated outputs.) previous section justiﬁed replacing basis functions hip-mdp favor bnn. section investigate prediction performance various models determine whether latent embedding provides desired effect robust efﬁcient transfer. models characterize presented sec. used baseline comparisons hip-mdp embedded latent parameters. models hip-mdp embedded hip-mdp linear learned scratch without latent characterization dynamics average model trained data without latent characterization dynamics. benchmarks except trained scratch batch transition data previously observed instances used pre-train method used learn dynamics previously unobserved instance. ﬁrst episode newly encountered instance updated. models latent estimation environment latent parameters also updated. seen models using latent parameterization improve greatly ﬁrst network latent parameter updates. also improve marginally. average model unable account different dynamics instance model trained scratch enough observed transition data instance construct accurate representation transition dynamics. superior predictive performance models learn utilize latent estimate underlying dynamics environment reinforces intent hip-mdp latent variable model. estimating employing latent estimate environment robustly transfer trained transition models previously unseen instances. further shown across domains represented fig. latent parametrization embedded input reliably accurate duration model interacts environment. hip-mdp embedded latent parameters model nonlinear interactions latent parameters state hipmdp linear latent parameters cannot. moreover navigation domain constructed true transition function nonlinear function latent parameter state. therefore accurate predictions made approximate transition function model nonlinear interactions. hence navigation section outlines nonlinear dynamical systems deﬁne experimental domains investigated sec. outline equations motion hidden parameters dictating dynamics motion procedures used perturb parameters produce subtle variations environmental dynamics. domain speciﬁc settings length episode also presented. hyperparameters restrict agent’s movement either laterally vertically depending hidden parameter domain hidden parameter simply binary choice classes agent force used counteract accentuate certain actions agent scaled nonlinearly distance agent moves away center region origin. agent accumulates small negative reward step taken large penalty agent hits wall attempts cross goal region wrong boundary. agent receives substantial reward successfully navigates goal region correct boundary. value purposefully large encourage agent rapidly enter goal region move force pushing agent away goal region. initialization episode class agent chosen uniform probability starting state agent randomly chosen region hidden parameters lengths masses links initially. order observe varied dynamics system perturb adding gaussian noise parameter independently initialization instance. possible state values angular velocities pendulum constrained initialization episode agent’s state initialized perturbed small uniformly distributed noise dimension. agent free apply torques hinge raises foot pendulum goal height time steps. dynamical system used simulate patient’s response treatments formulated equations highly nonlinear parameters used track evolution core markers used infer patient’s overall health. markers viral load number healthy infected t-lymphocytes number healthy infected macrophages number hiv-speciﬁc cytotoxic t-cells thus system equations deﬁned reward function parameters selected prescribed action. hidden parameters baseline settings shown fig. done acrobot initialization instance hidden parameters perturbed gaussian noise parameter independently. perturbations applied naively times would cause dynamical system lose stability otherwise provide non-physical behavior. ﬁlter instantiations domain deploy hip-mdp well-behaved controllable versions dynamical system. initialization episode agent started unhealthy steady state viral load number infected cells much higher number virus ﬁghting t-cells. episode characterized time steps where dynamically time step equivalent days. interval patient’s state taken prescribed treatment treatment period completed. hip-mdp architecture domains model dynamics using feed-forward bnn. example used fully connected hidden layers hidden units each acrobot domains used fully connected hidden layers units each. used rectiﬁer activation functions hidden layer identity activation function output layer. hip-mdp embedded input vector length |wb| consisting state one-hot encoding action latent embedding architecture hip-mdp linear uses different input layer output layer. input include latent parameters. hyperparameters training domains zero mean priors random input noise network weights variances respectively following procedure used hernández-lobato experiments found performed best initialized small prior variance network weights increases training rather using large prior variance. following hernández-lobato learn network parameters minimizing α-divergence using adam acrobot example domain. update performed epochs adam epoch sampled transitions prioritized experience buffer divided transitions mini batches size used learning rate acrobot learning rate example. latent parameters learned batch transition data gathered multiple instances across episodes instance. example acrobot data instances respectively. found performance improved standardizing observed states zero mean unit variance. latent parameters domains used |wb| latent parameters. latent parameters updated using update procedure updating network parameters learning rate learn policy task instance double deep network full connected hidden layers hidden units respectively. rectiﬁer activation functions used hidden layers identity function used output layer. domains update primary network weights every time steps using adam learning rate slowly update target network mirror primary network rate additionally clip gradients l-norm less \u0001-greedy policy starting decaying episode rate model-based approaches found learns robust policies training exclusively approximated transitions bnn. training ﬁrst episode train using initial batch approximated episodes generated using bnn. used td-error-based prioritized experience replay buffer store experiences used train dqn. model-based approaches used separate squared-error-based prioritized buffer store experiences used train learn latent parameterization. prioritized buffer large enough store experiences. used prioritization exponent importance sampling exponent demonstrate benchmark methods used learn good control policies unique instances acrobot domain treatment domain sufﬁcient number training episodes. however terms policy learning efﬁciency proﬁciency comparing performance hip-mdp benchmark ﬁrst episodes instructive—as presented experiments section main paper. format emphasizes immediate returns using embedded latent parameters transfer previously learned information encountering instance task. figure comparison learning policy task instance using hip-mdp versus four benchmarks episodes. mean reward episode runs shown benchmark. error bars omitted show results clearly.", "year": 2017}