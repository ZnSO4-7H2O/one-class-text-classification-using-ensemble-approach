{"title": "Gaussian Processes for Sample Efficient Reinforcement Learning with  RMAX-like Exploration", "tag": ["cs.AI", "cs.LG"], "abstract": "We present an implementation of model-based online reinforcement learning (RL) for continuous domains with deterministic transitions that is specifically designed to achieve low sample complexity. To achieve low sample complexity, since the environment is unknown, an agent must intelligently balance exploration and exploitation, and must be able to rapidly generalize from observations. While in the past a number of related sample efficient RL algorithms have been proposed, to allow theoretical analysis, mainly model-learners with weak generalization capabilities were considered. Here, we separate function approximation in the model learner (which does require samples) from the interpolation in the planner (which does not require samples). For model-learning we apply Gaussian processes regression (GP) which is able to automatically adjust itself to the complexity of the problem (via Bayesian hyperparameter selection) and, in practice, often able to learn a highly accurate model from very little data. In addition, a GP provides a natural way to determine the uncertainty of its predictions, which allows us to implement the \"optimism in the face of uncertainty\" principle used to efficiently control exploration. Our method is evaluated on four common benchmark domains.", "text": "abstract. present implementation model-based online reinforcement learning continuous domains deterministic transitions speciﬁcally designed achieve sample complexity. achieve sample complexity since environment unknown agent must intelligently balance exploration exploitation must able rapidly generalize observations. past number related sample eﬃcient algorithms proposed allow theoretical analysis mainly model-learners weak generalization capabilities considered. here separate function approximation model learner interpolation planner model-learning apply gaussian processes regression able automatically adjust complexity problem practice often able learn highly accurate model little data. addition provides natural determine uncertainty predictions allows implement optimism face uncertainty principle used eﬃciently control exploration. method evaluated four common benchmark domains. reinforcement learning agent interacts environment attempts choose actions externally deﬁned performance measure accumulated per-step reward maximized time. deﬁning characteristic environment unknown agent learn directly experience. practical applications e.g. robotics obtaining experience means physical system interact physical environment real time. therefore methods able learn quickly minimize amount time robot needs interact environment good optimal behavior learned highly desirable. paper interested online tasks continuous state spaces smooth transition dynamics typical robotic control domains. primary goal algorithm keeps sample complexity possible. maximize sample eﬃciency consider online model-based spirit rmax extended continuous state spaces similar rmax related methods algorithm gp-rmax consists parts model-learner planner. model-learner estimates dynamics environment sample transitions agent experiences interacting environment. planner used best possible action given current model. predictions model-learner become increasingly accurate actions derived become increasingly closer optimal. control amount exploration optimism face uncertainty principle employed makes agent visit unexplored states ﬁrst. algorithm model-learner implemented gaussian process regression; non-parametric give enhanced modeling ﬂexibility. allow bayesian model selection automatic relevance determination. addition provide natural determine uncertainty predictions allows implement optimism face uncertainty exploration rmax principled way. planner uses estimated transition function solve bellman equation value iteration uniform grid. point algorithm separate steps estimating function samples model-learner solving bellman equation planner. rationale behind that transition function relatively simple estimated accurately sample transitions. hand optimal value function inclusion operator often complex function sharp discontinuities. solving bellman equation however require actual samples; instead must able evaluate bellman operator arbitrary points state space. transition function learned samples large gains sample eﬃciency possible. competing model-free methods ﬁtted q-iteration policy iteration based lspi/lstd/lspe advantage need actual sample transitions estimate represent value function. conceptually approach closely related fitted r-max proposed uses instance-based approach model-learner related work uses grid-based interpolation model-learner. primary contribution paper instead. means willing trade theoretical analysis practical performance. example unlike recent pac-style performance bounds could derived much better able handle generalization consequence achieve much lower sample complexity. dimensionality state space. uniform grid number grid points solving bellman equation scales exponentially dimensionality. advanced methods sparse grids adaptive grids allow somewhat reduce exponential increase break curse dimensionality. alternatively nonlinear function approximation; however despite encouraging results unclear whether approach would really better general applications. today breaking curse dimensionality still open research problem. deterministic transitions. fundamental requirement approach since also learn noisy functions bellman operator evaluated resulting predictive distribution. rather taken convenience. known reward function. assuming reward function known transition function needs learned diﬀerent comparable work. fundamental requirement approach assumption think well justiﬁed reward performance criterion speciﬁes goal. type control problems consider here reward always externally deﬁned never something generated within environment. reward sometimes discontinuous function e.g. goal state elsewhere. makes amenable function approximation. consider reinforcement learning problem mdps continuous state space ﬁnite action space discounted reward criterion deterministic dynamics section assume dynamics rewards available ﬁnite action space transition function reward function. following theoretical argument require transition reward function lipschitz continuous actions; i.e. exist constants addition assume reward bounded rmax note practice ﬁrst condition continuity transition function usually fulﬁlled domains derived physical systems second condition continuity rewards often violated despite many cases outlined procedure still work well enough. order solve inﬁnite dimensional problem numerically reduce ﬁnite dimensional problem. done introducing discretization ﬁnite number elements applying bellman operator nodes interpolating between. ﬁxed action value state respect grid written convex combination vertices grid cell enclosing coeﬃcients example consider -dimensional case figure determine four vertices enclosing cell known function values etc. perform linear interpolations along x-coordinate auxilary points obtain denote successor state obtain apply transition function vertices using action i.e. denote action vector coeﬃcients respect grid thus written rows matrix coeﬃcients. contraction function q∗γh q-function obtained linear interpolation vector along states. function q∗γh used determine optimal control actions state simply determine order estimate well function q∗γh approximates true posteriori estimates deﬁned based local errors i.e. maximum residual grid cell. local error grid cell turn depends granularity grid modulus continuity details). model-based interested solving problem case transition function known. instead agent interact environment samples observes compute optimal behavior. goal paper develop learning framework remember working assumption reward performance criterion externally given need estimated agent. also note discretization likely feasible state spaces medium dimensionality. breaking curse dimensionality open research problem. sketch architecture shown figure gp-rmax consists parts model learning planning interwoven online learning. model-learner estimates dynamics environment sample transitions agent experiences interacting environment. planner used best possible action given current model. predictions model-learner become increasingly accurate actions derived planner become increasingly closer optimal. highlevel overview algorithm essence estimating samples regression problem. theory nonlinear regression algorithm could serve purpose believe particularly well-suited non-parametric means great modeling ﬂexibility; setting hyperparameters done automatically optimization marginal likelihood allows automatic determination relevant inputs; provide natural determine uncertainty predictions used guide exploration. furthermore uncertainty supervised depends target function estimated methods consider density data tend overexplore target function simple. assume observed number transitions given triplets state performed action resulting successor state e.g. xt+}t=... note d-dimensional function instead trying estimate directly estimate relative change eﬀect action state variable treated independently train multiple univariate combine individual predictions afterwards. individual trained respective subset data e.g. trained input output individual hyperparameters obtained optimizing marginal likelihood. details working found using learn model previously also studied characteristic functional form given terms parameterized covariance function. squared exponential also problem implementing eﬃciently dealing possible large number data points. lack space sketch particular implementation detailed information. implementation based subset regressors approximation. elements subset chosen stepwise greedy procedure aimed minimizing error incurred using rank approximation optimization likelihood done random subsets data ﬁxed size. avoid degenerate predictive variance projected process approximation used. optimization). note variant implement automatic relevance determination relevant inputs linear projections inputs automatically identiﬁed whereby model complexity reduced generalization sped trained testpoint provides distribution target values individual mean predicts change i-th coordinate state j-th action. individual variance interpreted associated uncertainty; close certain close uncertain depends hyperparameters stacking individual predictions together model-learner produces summary time planner receives input model state action model evaluated produce transition along normalized scalar uncertainty means maximally certain maximally uncertain proved converges ﬁxed point empirically demonstrated convergence occur signiﬁcantly fewer iterations. exact reduction problem-dependent savings greater small large cells self-transitions occur shown perform well. instead update rule becomes vmax rmax/. seen generalization binary uncertainty original rmax paper continuous uncertainty; whereas rmax state either known case unmodiﬁed update used unknown case value vmax assigned shift exploration exploitation gradual. finally take advantage fact planning function called many times process learning. since discretization kept ﬁxed reuse ﬁnal q-values obtained call plan initial values next call plan. since updates model often aﬀect states local neighborhood number necessary iterations call planning reduced. mountain mountain goal drive underpowered bottom valley hill. powerful enough climb hill directly instead build necessary momentum reversing throttle going hill opposite side ﬁrst. problem -dimensional state variable describes position velocity. possible actions learning episodic every step gives reward hill reached. experimental setup following exceptions maximal episode length steps discount factor every episode starts agent bottom valley zero velocity xstart inverted pendulum next task swing stabilize single-link inverted pendulum. mountain motor provide enough torque push pendulum single rotation. instead pendulum needs swung back forth gather energy pushed balanced. creates diﬃcult nonlinear control problem. state angular space -dimensional angle velocity. control force discretized held constant .sec. reward deﬁned remaining experimental setup task made episodic resetting system every steps initial state xstart discount factor bicycle next consider problem balancing bicycle rides constant speed problem -dimensional state variables roll angle roll rate angle handle angular velocity action space inherently -dimensional usually discretized actions. experimental setup similar allow conclusive comparison performance instead able keep bicycle falling deﬁne discriminating reward learning episodic every episode starts states close boundary recovery impossible xstart xstart proceeds steps bicycle fallen. discount factor acrobot ﬁnal problem acrobot swing-up task goal swing lower link underactuated two-link robot given height since lower link actuated rather challenging problem. state space -dimensional possible actions experimental setup implementation state transition dynamics similar objective learning reach goal state quickly possible thus every step. initial state every episode xstart episode ends either goal state reached steps passed. discount factor apply algorithm gp-rmax four problems. granularity discretization planner chosen dimensional problems loss performance discretization negligible. -dimensional problems oﬄine trials true transition function best compromise granularity computational eﬃciency. result grid mountain inverted pendulum grid bicycle balancing task grid acrobot. maximum number value iterations tolerance practice running full planning step took seconds small problems less large problems using planning module oﬄine true transition function computed best possible performance domain advance. obtained mountain inverted pendulum bicycle balancing acrobot gp-based model-learner maximum size subset tolerance hyperparameters covariance manually tuned found data likelihood optimiziation. since would computationally expensive update model perform full planning step every single observation planning frequency steps. gauge optimal behavior reached learning becomes unnessecary monitor change model predictions uncertainties successive updates stop fall threshold actively explores adjusting bellman updates according uncertainties produced prediction; gp-rmaxgrid uses binary uncertainty overlaying uniform grid state-action space keeping track cells visited; gprmaxnoexp actively explore comparison figure shows result online learning gp-rmax sarsa. short graphs show things particular gp-rmax learns quickly; gp-rmax learns behavior close optimal. comparison sarsa much higher sample complexity always learn optimal behavior direct comparison high performance algorithms ﬁtted value iteration policy iteration based lspi/lstd/lspe kernelbased methods diﬃcult either batch methods handle exploration ad-hoc respective results given literature clear domains examined gp-rmax performs relatively well. examining plots detail that gp-rmaxgrid somewhat less sample eﬃcient gp-rmaxexp gprmaxnoexp perform nearly same. initially appears contrast whole point rmax eﬃcient exploration guided uncertainty predictions. here believe behavior explained good generalization capabilities gps. figure illustrates model learning certainty propagation mountain domain state model-learner shown snapshots transitions transitions. shows value function results applying value iteration update modiﬁed uncertainty bottom shows observed samples associated certainty predictions. expected certainty high regions data observed. however generalization data-dependent hyperparameter selection certainty also high unexplored regions; particular constant along y-coordinate. understand this look state transition function mountain acceleration indeed depends position velocity. shows certainty estimates supervised take properties target function account whereas prior rmax treatments uncertainty unsupervised consider density samples decide state known. comparison also show gp-rmax grid-based uncertainty would produce situation. presented implementation model-based online reinforcement learning similar rmax continuous domains combining gp-based model learning value iteration grid. algorithm separates problem function approximation model-learner problem function approximation/interpolation planner. transition function easier learn i.e. requires samples relative representation optimal value fig. learning curves algorithm gp-rmax standard method sarsa tile coding four benchmark domains. curve shows online learning performance plots total reward function episode black horizontal line denotes best possible performance computed oﬄine. note diﬀerent scale x-axis gp-rmax sarsa. fig. model-learning propagation knownness state-action pairs gps. shows value function results applying value iteration update modiﬁed uncertainty bottom shows actual samples induced uncertainty states black perfectly known white perfectly unknown. panels show certainty model predictions rapidly propagated whole state space leading strong generalization targeted exploration. turn allows optimal value function learned sample transitions panel shows transitions approximated value function already resembles true panel shows counter-based binary uncertainty; grid cells unvisited thus approximate value function zero parts state space. function large savings sample-complexity gained. related modelfree methods ﬁtted q-iteration take advantage situation. fundamental limitation approach relies solving bellman equation globally state space. even advanced discretization methods adaptive grids sparse grids curse dimensionality limits applicability problems moderate dimensionality. other minor limitations concern simplifying assumptions made deterministic state transitions known reward function. however conceptual limitations rather simplifying assumptions made present paper; could easily addressed future work. work taken place learning agents research group artiﬁcial intelligence laboratory university texas austin. larg research supported part grants national science foundation", "year": 2012}