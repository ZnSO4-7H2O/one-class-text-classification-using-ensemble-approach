{"title": "On the Compression of Recurrent Neural Networks with an Application to  LVCSR acoustic modeling for Embedded Speech Recognition", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We study the problem of compressing recurrent neural networks (RNNs). In particular, we focus on the compression of RNN acoustic models, which are motivated by the goal of building compact and accurate speech recognition systems which can be run efficiently on mobile devices. In this work, we present a technique for general recurrent model compression that jointly compresses both recurrent and non-recurrent inter-layer weight matrices. We find that the proposed technique allows us to reduce the size of our Long Short-Term Memory (LSTM) acoustic model to a third of its original size with negligible loss in accuracy.", "text": "paper study techniques compressing recurrent neural networks speciﬁcally acoustic models. demonstrate generalization conventional inter-layer matrix factorization techniques jointly compress recurrent inter-layer weight matrices allows compress acoustic models third original size negligible loss accuracy. focus acoustic modeling techniques presented applied rnns domains e.g. handwriting recognition machine translation inter alia. technique presented paper encompasses traditional recurrent neural networks well long short-term memory neural networks. section review previous work focussed techniques compressing neural networks. proposed compression technique presented section examine effectiveness proposed techniques sections finally conclude discussion ﬁndings section noted previous work large amount redundancy parameters neural network. example denil show entire neural network reconstructed given values small number parameters. caruana colleagues show output distribution learned larger neural network approximated neural network fewer parameters training smaller network directly predict outputs larger network approach termed model compression closely related recent distillation approach proposed hinton redundancy neural network also exploited hashnet approach chen imposes parameter tying study problem compressing recurrent neural networks particular focus compression acoustic models motivated goal building compact accurate speech recognition systems efﬁciently mobile devices. work present technique general recurrent model compression jointly compresses recurrent non-recurrent inter-layer weight matrices. proposed technique allows reduce size long short-term memory acoustic model third original size negligible loss accuracy. neural networks multiple feed-forward recurrent hidden layers emerged state-of-theart acoustic models automatic speech recognition tasks. advances computational capabilities coupled availability large annotated speech corpora made possible train nn-based large number parameters great success. speech recognition technologies continue improve becoming increasingly ubiquitous mobile devices voice assistants apple’s siri microsoft’s cortana amazon’s alexa google enable users search information using voice. although traditional model applications recognize speech remotely large servers growing interest developing technologies recognize input speech directly on-device promise reduce latency enabling user interaction even cases mobile data connection either unavailable slow unreliable. main challenges regard disk memory computational constraints imposed devices. since number operations neural †equal contribution. authors would like thank has¸im raziel alvarez helpful comments suggestions work chris thornton yu-hsin chen comments earlier draft. context previous approaches acoustic model compression focused mainly case feedforward dnns. popular technique based sparsifying weight matrices neural network example setting weights whose magnitude falls certain threshold zero based second-derivative loss function optimal brain damage procedure fact seide demonstrate two-thirds weights feed-forward network zero withincurring loss performance. although techniques based sparsiﬁcation decrease number effective weights encoding subset weights ‘zeroed out’ requires additional memory. further weight matrices represented dense matrices efﬁcient computation parameter savings disk translate savings runtime memory. techniques reduce number model parameters based changing neural network architecture e.g. introducing bottleneck layers low-rank matrix factorization layer also note recent work wang uses combination singular value decomposition vector quantization compress acoustic models. methods investigated work similar previous work examined using reduce number parameters network context feedforward dnns describe section methods thought extension techniques proposed wherein jointly factorize recurrent inter-layer weight matrices network. describe approach general setting standard rnn. denote activations l-th hidden layer consisting nodes time inputs layer time turn activations previous layer input features denoted write following equations deﬁne output activations l-th layers standard denoted rrl×n rank that note sharing across recurrent interlayer matrices allows efﬁcient parameterization weight matrices; shown section result signiﬁcant loss performance. thus degree compression model controlled setting ranks projection matrices layers network. determine recurrent projection matrix ﬁrst computing recurrent weight matrix truncate retaining singular values corresponding singular vectors where denotes frobenius norm matrix. pilot experiments found proposed svd-based initialization performed better training model recurrent projection matrices random initialization network weights. generalizing procedure described context standard rnns case lstm rnns straightforward. using notation note recurrent-weight matrix case lstm concatenation four gate weight matrices obtained stacking vertically represent respectively recurrent connections input gate output gate forget gate cell state. concatenation similarly inter-layer matrix matrices correspond input gate forget gate output gate cell state deﬁnitions compression applied described section note compress peep-hole weights since already narrow single column matrices contribute signiﬁcantly total number parameters network. mentioned section primary motivations behind investigating acoustic model compression build compact acoustic models deployed mobile devices. recent work demonstrated deep lstm-based trained predict either contextindependent phoneme targets context-dependent phoneme targets approach state-of-the-art performance speech tasks. systems important characteristics addition phoneme labels system also hypothesize blank label unsure identity current phoneme systems trained optimize connectionist temporal classiﬁcation criterion maximizes total probability correct label sequence conditioned input sequence. details found following baseline model thus model hidden layer lstm cells layer predicts phonemes point comparison also present results obtained using much larger state-of-the-art ‘server-sized’ model large deploy embedded devices nonethless serves upper-bound performance models dataset. model consists hidden layers lstm cells layer trained predict context-dependent phonemes systems trained using distributed asynchronous stochastic gradient descent parameter server systems ﬁrst trained convergence optimize criterion following discriminatively sequence trained optimize state-level minimum bayes risk criterion discussed section applying proposed compression scheme ﬁne-tune network ﬁrst criterion followed sequence discriminative training smbr criterion. additional ﬁne-tuning step found necessary achieve good performance particularly amount compression increased. language model used work -gram model trained sentences in-domain data entropybased pruning applied reduce size roughly n-grams vocabulary. since goal build recognizer efﬁciently mobile devices minimize size decoder graph used recognition following approach outlined perform additional pruning step generate much smaller ﬁrst-pass language model composed lexicon transducer construct decoder graph. perform on-the-ﬂy rescoring larger resulting models compressed on-device total thus enabling many times faster real-time recent mobile devices parameterize input acoustics computing dimensional mel-ﬁlterbank energies range computed every windowed speech segments. server-sized system uses -dimensional features computed range since resulted slightly improved performance. following stabilize training stacking together consecutive speech frames every third stacked frame presented input network. training evaluation data systems trained hand-transcribed anonymized utterances extracted google voice search trafﬁc create multi-style training data synthetically distorting utterances simulate background noise reverberation using room simulator noise samples extracted youtube videos environmental recordings everyday events; distorted examples created utterance training set. systems additionally adapted using smbr criterion anonymized hand-transcribed dictation utterances extracted google trafﬁc processed generate multi-style training data described above improves performance dictation task. results reported hand-transcribed anonymized utterances extracted google trafﬁc open-ended dictation domain. experiments seek determine impact proposed joint svd-based compression technique system performance. particular interested determining system performance varies function degree compression controlled setting ranks recurrent projection matrices described section notice since proposed compression scheme applied hidden layers baseline system numerous settings ranks projection matrices layer result number total parameters compressed network. order avoid ambiguity various projection ranks using following criterion given threshold layer rank corresponding projection matrix corresponds retaining fraction explained variance truncated speciﬁcally singular values sorted non-increasing order choosing projection ranks using allows control degree compression thus compressed model size varying single parameter pilot experiments found scheme performed better setting ranks equal layers projection ranks determined various projection matrices ﬁne-tune compressed models ﬁrst optimizing criterion followed sequence training smbr criterion adaptation in-domain data described section results experiments presented table seen table baseline system predicts phoneme targets relative worse larger server-sized system although half many parameters. since ranks chosen retain given fraction explained variance operation also note earlier hidden layers network appear lower ranks later layers since variance accounted smaller number singular values. seen table word error rates increase amount compression increased although performance compressed systems close baseline moderate compression using value enables model compressed third original size small degradation accuracy. however performance begins degrade signiﬁcantly future work consider alternative techniques setting projection ranks order examine impact system performance. presented technique compress rnns using joint factorization recurrent inter-layer weight matrices generalizing previous work proposed technique applied task compressing lstm acoustic models embedded speech recognition found could compress baseline acoustic model third original size negligible loss accuracy. proposed techniques combination weight quantization allow build small efﬁcient speech recognizer many times faster real-time recent mobile devices hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. schalkwyk beeferman beaufays byrne chelba cohen kamvar strope your word command google search voice case study advances speech recognition springer graves liwicki fern´andez bertolami bunke schmidhuber novel connectionist system unconstrained handwriting recognition ieee transactions pattern analysis machine intelligence vol. sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets proc. icassp graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proc. icml vinyals heigold senior mcdermott monga sequence discriminative distributed training long short-term memory recurrent neural networks proc. interspeech mcgraw prabhavalkar alvarez gonzalez arenas rybach alsharif gruenstein beaufays parada personalized speech recognition mobile devices proc. icassp", "year": 2016}