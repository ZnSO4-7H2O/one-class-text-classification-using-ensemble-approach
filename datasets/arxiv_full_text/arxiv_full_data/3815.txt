{"title": "Depth-Gated LSTM", "tag": ["cs.NE", "cs.CL"], "abstract": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.", "text": "short note present extension long short-term memory neural networks using depth gate connect memory cells adjacent layers. introduces linear dependence lower upper layer recurrent units. importantly linear dependence gated gating function call depth gate. gate function lower layer memory cell input past memory cell layer. conducted experiments veriﬁed architecture lstms able improve machine translation language modeling performances. deep neural networks successfully applied many areas including speech vision natural language processing tasks recurrent neural networks widely used ability memorize long-term dependency. typical problem training deep networks including rnns gradient diminishing explosion. problem apparent training simple rnn. long short-term memory neural networks extension simple lstm memory cell linear dependence current activity past activity. importantly forget gate used modulate information past current activities. lstms also input output gates modulate input output. perhaps introduction gating functions signiﬁcant improvement recurrent neural networks recently gated recurrent unit also adopted concept using gates. lstms grus widely used many natural language processing tasks construct deep neural networks standard stack many layers neural networks. however problem building simple recurrent networks. difference error signals instead last time instance back-propagated many layers nonlinear transformations therefore error signals might either diminished exploded. short note investigates extension lstms uses depth-gate connect memory cells lower upper layers. review recurrent neural networks sec. section presents extension. experiments sec. relate extension works sec. conclude sec. lstm initially proposed later modiﬁed follow implementation illustrated fig. lstm introduces linear dependence memory cells past ct-. additionally lstm input output gates. specially lstm written input gate forget gate output gate lstm. output lstm. logistic function. denotes element-wise product. application lstm forget gate input gate share parameters computed note bias terms omitted equations applied default. typically lstms stacked form deep recurrent neural networks illustrated left ﬁgure figure output lower layer lstm layer output used input connection connections layers. depth-gated lstm illustrated right ﬁgure fig. depth gate connects memory cells lower layer depth-gate controls much lower memory cell directly upper layer memory cell. gate function layer time logistic function applied dglstms datasets. ﬁrst btec chinese english machine translation task. training consists sentence pairs. devset devset validation total sentence pairs. devset test sentence pairs. conducted preliminary experiments observed attention model performed better encoder-decoder method therefore applied attention model experiments. encoder decoder used recurrent neural networks however experiment used recurrent neural networks decoder. encoder used word embedding learned training set. preliminary experiment showed simple performed worst. therefore don’t include simple results paper. compared dglstm lstm. models used -dimension hidden layer. varied depth rnns. results table show dglstm outperforms lstm tested depths. another experiment machine translation experiment used attention model dglstm rescore test k-best lists. ﬁrst trained attention models layers dglstms layers dglstms training set. used -dimension hidden layers. trained reranker model using development data -best lists translation pair. best lists generated baseline. features reranker models scores attention model. -best lists test reranked using trained reranker model. described reranking processes times averaged bleu scores obtained using reference. bleu scores listed table compared baseline dglstm improved bleu scores points test set. conducted experiments dataset. trained layer dglstm. layer dimension vector. test perplexity results shown table compared previously published results dataset dglstm obtained lowest perplexity test knowledge. called transform gate carry gate respectively. output nonlinear path. matrices. therefore highway network output direct linear connection albeit gated input. allows highway networks train extremely deep networks easily. dglstm related highway networks using idea linear gated connection input. differs highway networks using speciﬁc gate non-linear path; dglstm keeps input output gates applied non-linear transformations lstms. however overall effect using input output gates similar transfer gate highway networks. additional important difference dglstm linearly connects memory cells lower upper layers. this memory cell dglstm errors back-propagated future layer linearly albeit gated. might biggest difference highway networks current implementation. perhaps closet work research grid lstm uses lstms different dimensions connects using gated linear connections. dimensions include time typical recurrent neural networks also depth others grid lstm general dglstm considers time depth. also grid lstm uses generic form input memory output. allows memory cell gated linear dependence past memory cell also past observations. therefore consider dglstm speciﬁc simple case grid lstm gate applied time depth memory cells. however dglstm grid lstm highway networks share idea stacking networks linear gated connections nonlinear paths. idea applied fully connected convolutional recurrent layers. presented depth-gated lstm architecture uses depth-gate gated linear connection lower upper layer memory cells. observed better performances using architecture machine translation language modeling tasks. architecture related highway networks grid lstm using additional linear connection gates regulate information across layers.", "year": 2015}