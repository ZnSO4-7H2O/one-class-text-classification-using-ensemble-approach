{"title": "The Computational Theory of Intelligence: Information Entropy", "tag": ["cs.AI", "cs.LG", "68T27", "I.2.1"], "abstract": "This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications.", "text": "paper presents information theoretic approach concept intelligence computational sense. introduce probabilistic framework computational intelligence shown entropy minimizing process local level. using scheme develop simple data driven clustering example discuss applications. paper attempts introduce computational approach study intelligence researcher accumulated years study. approach takes account data psychology neurology artiﬁcial intelligence machine learning mathematics. central framework fact goal intelligent agent reduce randomness environment meaningful way. course formal deﬁnitions context paper terms like intelligence environment agent follow. approach draws multidisciplinary research many applications. utilize construct discussions paper. applications follow future works. implementations framework apply many ﬁelds study including general artiﬁcial intelligence machine learning optimization information gathering clustering data extend outside applied mathematics computer science realm even areas including sociology psychology neurology even philosophy. cannot begin discussion philosophy artiﬁcial intelligence without deﬁnition word intelligence ﬁrst place. panopoly deﬁnitions available understandable disagreement typically school thought generally shares common process entails skills problem solving enabling individual resolve genuine problems diﬃculties encounters appropriate create eﬀective product must also entail potential ﬁnding creating problems thereby providing foundation acquisition knowledge. vernon’s hierarchical model intelligence hawkins’ intelligence great resources topic. consider following working deﬁnition paper regard information theory computation computational intelligence information processing algorithm mapping called learning endemic entity facilitating process refer agent particular connotation entity autonomous. surrounding infrastructure encapsulates agent together ensuing events called environment. paper organized follows. section provide brief summary concept information entropy used purposes. section provide mathematical framework intelligence show discuss relation entropy. section discusses global ramiﬁcations local entropy minimization. section present simple application framework data analytics available free download. sections discuss relavent related research future work. although terms like shannon entropy pervasive ﬁeld information theory insightful review formulation context. arrive deﬁnition entropy must ﬁrst recall meant information content. information content random variable denoted given concept entropy deeply rooted heart physical reality. central concept thermodynamics governing everything chemical reactions engines refrigerators. relationship entropy known information theory however mapped straightforwardly thermodynamics. connection thermodynamic information theoretic versions entropy relate information needed detail exact state system speciﬁcally amount shannon information needed deﬁne microscopic state system remains ambiguous given macroscopic deﬁnition terms variables classical thermodynamics. boltzmann constant serves constant proportionality. input set. cardinality sets need match mapping need bijective even surjective. iterative process denoted index finally represent collection time mapping converge intended element function measure distance assuming function exists called learning rate. applications process abstract topological spaces distance function commodity open question. qualify intelligence process must unsupervised supervised learning consideration given sets power mapping unsupervised mapping. otherwise mapping supervised. ramiﬁcations distinction follows. supervised learning agent given distinct sets trained form mapping explicitly. unsupervised learning agent tasked learning subtle relationships single data succinctly develop mapping power discussed satisfying deﬁnitions assumptions section mean mapping necessarily meaningful. could make completely arbitrary consistent mapping prescription above although would satisfy deﬁnitions assumptions would complete memorization part agent. this fact exactly deﬁnition overtraining common pitfall training stage machine learning must diligent avoid. nature whenever system taken state higher entropy state lower entropy always amount energy involved transition increase entropy rest environment greater equal entropy loss. words consider system composed subsystems second thermodynamics veriﬁed time virtually areas physics extended general principal context information theory. fact conclude paper postulate intelligence here implement discussions paper practical examples. first consider simple example unsupervised learning; clustering algorithm based shannon entropy minimization. next look simple behavior intelligent agent acts maximize global entropy environment. consider data consisting number elements organized rows. take example data found particular example contains samples vector simple proof concept group data like neighborhoods minimizing entropy across elements respective index data set. data driven example essentially genetic algorithm perturb juxtaposition members neighborhood global entropy reaches minimum time avoiding trivial cases neighborhood element. prerequisite running code must python framework installed also freely available many operating systems clustering source code freely available simply downplease note simple prototype proof concept used exemplify concept. optimized latency memory utilization optimized performance tested algorithms comparative class although dramatic improvements could easily acheived integrating information content elements algorithm. speciﬁcally would move elements high information content clusters element would otherwise information content. furthermore observe eﬃcacy preprocessing layer beneﬁcial especially topological data sets like iris data set. nevertheless applications concept applied clustering small large scales discussed future work. visualize progression algorithm ﬁnal results respectively graphs pictured below. simplicity ﬁrst dimensions plotted. accuracy clustering algorithm error rate iterations average simulation time seconds. observe although ’blemishes’ ﬁnal clustering results proper choice parameters including maximum computational epochs clustering algorithm eventually succeed accuracy. also pictured ﬁgure results clustering algorithm applied data containing four additional ﬁelds pseudo-randomly genconﬁne note range respective surface algorithm proceeds follows. first agent initialized starting position updates incrementing decrementing coordinates small value agent meanders surface data collected position z−axis. partition range surface equally spaced intervals form histogram agent’s positional information. construct discrete probability function thus calculate renyi entropy. agent feedback entropy determined using calculate appropriate upates position cycle continues. overall goal maximize entropy timeout predetermined number iterations. many sources claim computational theories intelligence part theories merely describe certain aspects intelligence example meyer suggests performance multiple tasks dependent adaptive executive control makes claim emergence characteristics. others discuss data aggregated. type analysis especially relevant computer vision image recognition inspired physics cosmology wissner-gross asserts autonomous agents maximize entropy environment speciﬁcally proposes path integral formulation derives gradient analogized causal force propelling system along gradient maximum entropy time. using idea created startup called entropica applies principal ingenious ways variety diﬀerent applications ranging anything teaching robot walk upright maximizing proﬁt potential stock market. essentially wissner-gross start global principal worked backwards. paper arrive similar result diﬀerent perspective namely entropy minimization. purpose paper groundwork generalization concept intelligence computational sense. discussed entropy minimization utilized facilitate intelligence process disparities agent’s prediction reality training used optimize agents performance. also showed concept could used produce meaningful albeit simpliﬁed practical demonstration. future work includes discussing underlying principals data collected hierarchically discussing computational processes implement discussions paper evolve work together form processes greater complexity discussing relevance contributions abstract concepts like consciousness self awareness.", "year": 2014}