{"title": "On the Differential Privacy of Bayesian Inference", "tag": ["cs.AI", "cs.CR", "cs.LG", "math.ST", "stat.ML", "stat.TH"], "abstract": "We study how to communicate findings of Bayesian inference to third parties, while preserving the strong guarantee of differential privacy. Our main contributions are four different algorithms for private Bayesian inference on proba-bilistic graphical models. These include two mechanisms for adding noise to the Bayesian updates, either directly to the posterior parameters, or to their Fourier transform so as to preserve update consistency. We also utilise a recently introduced posterior sampling mechanism, for which we prove bounds for the specific but general case of discrete Bayesian networks; and we introduce a maximum-a-posteriori private mechanism. Our analysis includes utility and privacy bounds, with a novel focus on the influence of graph structure on privacy. Worked examples and experiments with Bayesian na{\\\"i}ve Bayes and Bayesian linear regression illustrate the application of our mechanisms.", "text": "study communicate ﬁndings bayesian inference third parties preserving strong guarantee differential privacy. main contributions four different algorithms private bayesian inference probabilistic graphical models. include mechanisms adding noise bayesian updates either directly posterior parameters fourier transform preserve update consistency. also utilise recently introduced posterior sampling mechanism prove bounds speciﬁc general case discrete bayesian networks; introduce maximum-a-posteriori private mechanism. analysis includes utility privacy bounds novel focus inﬂuence graph structure privacy. worked examples experiments bayesian naïve bayes bayesian linear regression illustrate application mechanisms. consider problem faced statistician analyses data communicates ﬁndings third party wants learn much possible data doesn’t want learn individual datum. example case insurance agency data medical records wants convey efﬁcacy drugs agency without revealing speciﬁc illnesses individuals population. requirements privacy growing interest learning chaudhuri duchi jordan wainwright theoretical computer science dwork smith mcsherry talwar databases communities barak zhang impact individual privacy real-world data analytics. setting assume using bayesian inference draw conclusions observations system random variables updating prior distribution parameters posterior. goal release approximation posterior preserves privacy. adopt formalism differential privacy characterise easy discover facts individual data aggregate posterior. releasing posterior permits external parties make inferences will. example third-party pharmaceutical might released posterior prior efﬁcacy drugs update patient data. could form predictive posterior classiﬁcation regression preserving differential privacy original data. focus paper bayesian inference probabilistic graphical models popular tool modelling conditional independence assumptions. similar effect statistical computational efﬁciency non-private inference central tenet paper independence structure impact privacy. mechanisms theoretical bounds ﬁrst establish link graph structure privacy. main contributions. develop ﬁrst mechanisms bayesian inference ﬂexible framework propose posterior perturbation mechanisms networks likelihood functions exponential families conjugate priors laplace noise dwork posterior parameters preserve privacy. latter achieves stealth consistent posterior updates. general bayesian networks posteriors non-parametric. case explore mechanism dimitrakakis samples posterior answer queries—no additional noise injected. complement study maximum posteriori estimator leverages exponential mechanism mcsherry talwar utility privacy bounds connect privacy graph/dependency structure complemented illustrative experiments bayesian naïve bayes linear regression. related work. many individual learning algorithms adapted maintain differential privacy including regularised logistic regression chaudhuri monteleoni rubinstein chaudhuri monteleoni sarwate chaudhuri sarwate sinha functional mechanism zhang trees jagannathan pillaipakkamnatt wright probabilistic graphical models used preserve privacy. zhang learned graphical model data order generate surrogate data release; williams mcsherry model response private mechanisms clean output improve accuracy. xiao xiong similarly used bayesian credible intervals increase utility query responses. little attention paid private inference bayesian setting. seek adapt bayesian inference preserve differential privacy releasing posteriors. dimitrakakis introduce differentially-private mechanism bayesian inference based posterior sampling—a mechanism build— zheng considers reﬁnements. wang fienberg smola explore monte carlo approaches bayesian inference using mechanism ﬁrst establish differential privacy gibbs estimator mcsherry talwar minimising risk bounds. paper ﬁrst develop mechanisms differential privacy general framework bayesian inference multiple dependent r.v.’s. mechanisms consider graph structure include purely bayesian approach places conditions prior. show lipschitz assumptions dimitrakakis lift graphs r.v.’s bound kl-divergence releasing empirical posterior based modiﬁed prior. chaudhuri monteleoni sarwate achieve privacy regularised empirical risk minimisation objective randomisation conditions priors. develop alternate approach uses additive-noise mechanism dwork perturb posterior parameterisations; apply techniques barak released marginal tables maintain consistency addition privacy adding laplace noise fourier domain. motivation novel wish guarantee privacy omniscient attackers stealth unsuspecting third parties. consider bayesian statistician estimating parameters family distributions system r.v.’s index observations denoted sample space prior distribution reﬂecting prior belief updates observation obtain posterior goal communicate posterior distribution third party limiting information revealed original data. point view data provider trusted party. however still inadvertently reveal information. assume computationally unbounded knowledge prior family guarantee gain little additional information communication uses bayesian inference learn data differentially-private posterior ensure disclosure carefully controlled. parents i-th variable bayesian network—a directed acyclic graph r.v.’s nodes. example concreteness illustrate mechanisms systems bernoulli r.v.’s case represent conditional distribution given parents bernoulli parameters differential privacy communicates releasing information posterior distribution randomised mechanism maps dataset response dwork characterise mechanism private deﬁnition randomised mechanism neighbouring measurable deﬁnition requires neighbouring datasets induce similar response distributions. consequently impossible identify true dataset bounded mechanism query responses. differential privacy assumes bounds adversarial computation auxiliary knowledge. approach differential privacy additive laplace noise previous work focused addition noise directly outputs non-private mechanism. ﬁrst apply laplace noise posterior parameter updates. setting example laplace noise posterior parameters. algorithm releases perturbed parameter updates beta posteriors calculated simply counting. adds zero-mean laplace-distributed noise updates ﬁnal dependence finally perturbed updates truncated zero rule invalid beta parameters upper truncated yields upper bound updates facilitates application mcdiarmid’s bounded-differences inequality utility analysis. note truncation improves utility affect privacy. corollary algorithm preserves \u0001-differential privacy. proof. based lemma intermediate preserve \u0001-differential privacy dwork since truncation depends preserves privacy. utility updates. bounding effect posterior laplace mechanism demonstrate utility bound posterior update counts. proposition probability least update counts computed algorithm close non-private counts utility posterior. derive main utility bounds algorithm terms posteriors proved appendix. abuse notation refer prior density; meaning apparent context. given priors beta posteriors observations natural measure utility kl-divergence joint product posteriors component-wise divergences known closed form. analysis divergence random quantity randomness added noise. laplace mechanism fourier domain algorithm follows kerckhoffs’s principle kerckhoffs security obscurity differential privacy defends mechanism-aware attacker. however additional stealth required certain circumstances. oblivious observer tipped privacy-preserving activities independent perturbations likely inconsistent one-another achieve differential privacy stealth turn barak study consistent marginal contingency table release. section presents particularly natural application bayesian posterior updates. contingency table r.v.’s induced i.e. combination variables }|i| component cell non-negative count observations characteristic geometrically real-valued function |i|-dimensional boolean hypercube. parameter delta’s ﬁrst mechanism correspond cells -way marginal contingency tables vector πi+ei projection/marginalisation operator deﬁned basis structure linearity projection operator marginal contingency table must span projections fourier basis vectors barak theorem table r{}|i| states marginal lies span basis vectors contained number values needed update |πi|+ potentially less suggested release updates r.v.’s well signiﬁcant overlap need release once coefﬁcients downward closure variable neighbourhoods compares favourably algorithm noise scale provided r.v. child half graph. moreover denser graph—the overlap nodes’ parents less conditional independence assumed—the greater reduction scale. intuitively appealing. consistency. gained passing fourier domain perturbed marginal tables corollary consistent anything span projected fourier basis vectors corresponds valid contingency table real-valued cells barak non-negativity. described ﬁrst stage algorithm remainder yields stealth guaranteeing releases non-negative w.h.p. adapt idea barak increase coefﬁcient fourier basis vector affecting small increment cell contingency table. exact minimal amount would guarantee non-negativity data dependent. thus efﬁcient -time approach randomised. corollary adding t|ni|\u0001−−k/ coefﬁcient induces nonnegative table w.p. exp. parameter trades probability non-negativity resulting loss utility. rare event negativity re-running algorithm affords another chance stealth cost privacy budget could alternatively truncate achieve validity sacriﬁcing stealth privacy. utility. analogous proposition perturbed marginal close unperturbed version w.h.p. theorem perturbed tables algorithm satisfy probability least general bayesian networks release samples posterior dimitrakakis instead perturbed samples posterior’s parametrisation. develop calculus building lipschitz properties systems r.v.’s locally lipschitz. given smoothness entire network differential privacy utility posterior sampling follow. distance function absolute log-ratio consider general bayesian network. following lemma shows individual lipschitz continuity conditional likelihood every implies global lipschitz continuity network. lemma exists liρi note lipschitz continuity holds uniformly families e.g. exponential distribution many useful distributions bernoulli. cases relaxed assumption requires prior concentrated smooth regions. exists constants that e−cl. lemma conditional likelihood node deﬁne parameters lipschitz continuity holds lipschitz constant e−cil e−cl mini∈i ln|i|/l mini∈i application posterior sampler turn releasing point estimates exponential mechanism mcsherry talwar samples responses likelihood exponential score function. selecting utility function maximised target non-private mechanism exponential mechanism used privately approximate target high utility. natural select utility posterior likelihood maximised estimate. lipschitz coefﬁcient sup-norm responses pseudo-metric datasets previous section. providing base measure non-trivial general discrete ﬁnite outcome spaces uniform mcsherry talwar mechanism broadly applicable safely take proposed number mechanisms approximating exact bayesian inference general framework probabilistic graphical models demonstrate approaches simple well-known pgms naïve bayes classiﬁer linear regression. section derivations appendix illustrates approaches applied supports extensive theoretical results experimental observation. focus trade-off privacy utility involves posterior predictive posterior distribution case studies. illustrative example mechanisms bayesian naïve bayes model bernoulli class attribute variables full conjugate beta priors. directly specialises running example synthesised data generated naïve bayes model features examples. trained mechanisms examples uniform beta priors. formed predictive posteriors thresholded make classiﬁcation predictions remaining unseen test data evaluate classiﬁcation accuracy. results reported figure average performance taken repeats account randomness train/test split randomised mechanisms. small size data represents challenge setting since privacy difﬁcult preserve smaller samples dwork expected privacy incurs sacriﬁce accuracy private mechanisms. laplace mechanisms perturb posterior updates note boolean attributes class label yields nodes downward closure size |ni| following generic mechanisms noise added sufﬁcient statistics independent training size similar scale. fourier approach stealth achieved time—those times contributed plot. small increments cell counts fourier necessary achieve additional stealth property expect small decrease utility borne figure beta pair obtain generalised form -differential privacy wish compare \u0001-differentially-private mechanisms choose route satisﬁes assumption detailed appendix. trim posterior sampling probabilities lower-bounded. figure demonstrates small minimal probability trim relatively large resulting poor approximate posterior. past certain threshold posterior sampler eventually outperforms private mechanisms. next explore system continuous r.v.’s bayesian linear regression posterior sampler appropriate. model label i.i.d. gaussian known-variance mean linear function features linear weights endowed multivariate gaussian prior zero mean spherical covariance. satisfy assumption conservatively truncate gaussian prior sample resulting truncated posterior; form predictive posterior; compute mean squared error. evaluate approach used u.s. census records dataset integrated public microdata series minnesota population center records demographic features. predict annual income train data remainder testing. figure displays varying prior weights bounded norm conservatively). expected concentrated prior leads worse mechanisms stronger priors reduce data inﬂuence. compared linear presented suite mechanisms differentially-private inference graphical models bayesian framework. ﬁrst perturb posterior parameters achieve privacy. achieved either performing perturbations original parameter domain frequency domain fourier transform. third mechanism relies choice prior combination posterior sampling. complement mechanisms releasing posterior private point estimators. throughout proved utility privacy bounds mechanisms cases depend graph structure bayesian network naturally conditional independence affects privacy. support mechanisms analysis applications concrete models experiments exploring privacyutility trade-off. proof theorem follow proof laplace union bound {xj}j∈ni since follows probability additional increment comes additional cost t|ni|/\u0001. putting everything together note |πi|+ fourier coefﬁcients represent including therefore satisfying stochastic lipschitz continuity conditional likelihood every bayesian network subset satisfying global stochastic lipschitz continuity e−cil. modify slightly sample truncated posterior. allows assume minimal probability assigned sub-event naïve bayes network joint distribution satisﬁes assumption trivially particular yields differential privacy level given log. given desired privacy budget therefore select exp. simply rejection sample sampling above obtain samples truncated posterior. posterior sampler algorithm used naïve bayes experiments. bayesian linear regression denote observations model assume independent given recall normal linear regression model i.i.d gaussian noise given follows", "year": 2015}