{"title": "Learning Simple Algorithms from Examples", "tag": ["cs.AI", "cs.LG"], "abstract": "We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using $Q$-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by $Q$-learning.", "text": "present approach learning simple algorithms copying multidigit addition single digit multiplication directly examples. framework consists interfaces accessed controller. typical interfaces tapes grids hold input output data. controller explore range neural network-based models vary ability abstract underlying algorithm training instances generalize test examples many thousands digits. controller trained using q-learning several enhancements show bottleneck capabilities controller rather search incurred q-learning. many every tasks require multi-step interaction world. example picking apple tree requires visual localization apple; extending muscle control guided visual feedback pluck tree. individual procedure complex task nevertheless requires careful sequencing operations across visual motor systems. paper explores machines learn algorithms involving similar compositional structure. since emphasis learning correct sequence operations consider domain arithmetic operations simple. example although learning digits straightforward solving addition multi-digit numbers requires precise coordination operation movement sequence recording carry. explore variety algorithms domain including complex tasks involving addition multiplication. approach formalizes notion central controller interacts world interfaces appropriate task hand. controller neural network model must learn control interfaces discrete actions produce correct output given input patterns. speciﬁcally train controller large sets examples input output patterns using reinforcement learning. reward signal sparse received model emits correct symbol output tape. consider separate settings. ﬁrst provide supervision form ground truth actions. second train input-output pairs able solve tasks latter case supervised setting provides insights model limitations upper bound performance. evaluate model sequences longer present training. surprisingly controllers even modest capacity recall previous states easily overﬁt short training sequences generalize test examples even correct actions provided. even appropriate controller off-the-shelf q-learning fails majority tasks. therefore introduce series modiﬁcations dramatically improve performance. include novel dynamic discount term makes reward invariant sequence length; extra penalty aids generalization deployment watkins q-lambda figure input tape grid interfaces. single head reads character time response read action controller. also move location head left right actions. overview model showing abstraction controller interfaces example model applied addition task. time step controller form reads symbol input grid outputs no-operation symbol output tape action input interface well passing hidden state next timestep. model consists rnn-based controller accesses environment series pre-deﬁned interfaces. interface speciﬁc structure actions perform. interfaces manually selected according task controller part system learns prior knowledge interfaces operate. thus controller must learn sequence actions various interfaces allow solve task. make three different interfaces input tape provides access input data symbols stored inﬁnite tape. read head accesses single character time read action. head moved left right actions. input grid version input tape read head moved actions down left right. output tape similar input tape except head writes single symbol time tape provided controller. vocabulary includes no-operation symbol enabling controller defer output desires. training written target symbols compared using cross-entropy loss. provides differentiable learning signal used addition sparse reward signal provided q-learning. controller explore several recurrent neural network architectures different sizes -layer lstm gated-recurrent unit vanilla feed-forward network. note rnn-based models able remember previous network state unlike feed-forward network. important tasks explicitly require form memory e.g. carry addition. simple algorithms consider deterministic solutions expressed ﬁnite state automata. thus training hope controller implicitly learn correct automata training samples since would ensure generalization sequences arbitrary length. tasks like reverse observe higher-order form over-ﬁtting model learns solve training tasks correctly generalizes successfully test sequences length however presented longer test sequences model fails completely. suggests model converged incorrect local minima corresponding alternate automata implicit awareness sequence length trained. fig. example reverse task. note behavior results controller learning scheme since present supervised q-learning settings experiments show need carefully adjust controller capacity prevent learning dependencies length training sequences ensuring enough state implement algorithm question. illustrated fig. controller passes signals output tape discrete action symbol vocabulary. symbol produced taking softmax output controller. training different signals computed this cross-entropy loss used compare softmax output target symbol discrete reward symbol correct/incorrect. ﬁrst signal gives continuous gradient update controller parameters backpropagation. leveraging reward requires reinforcement learning since many actions might occur symbol written output tape. thus action output controller trained reinforcement learning symbol output trained backpropagation. consider different tasks copy reverse walk multi-digit addition number addition single digit multiplication. input interface copy reverse input tape input grid others. tasks output tape interface. unless otherwise stated arithmetic operations base examples tasks shown fig. copy task involves copying symbols input tape output tape. although simple model still learn correspondence input output symbols well executing move right action input tape. reverse goal reverse sequence symbols input tape. provide special character indicate sequence. model must learn move right multiple times hits symbol move left copying symbols output tape. walk goal copy symbols according directions given arrow symbol. controller starts moving right reaching symbols ↑↓←. change it’s direction accordingly copy symbols encountered output tape. addition goal multi-digit sequences provided input grid. sequences provided adjacent rows right edges aligned. initial position read head last digit number model memorize addition table pairs digits; learn move input grid discover concept carry. number addition addition task three numbers added. challenging reward signal less frequent also carry take three states compared number addition task. single digit multiplication involves multiplying single digit long multi-digit number. similar complexity number addition task except carry take values figure examples tasks presented initial state. yellow indicates starting position read head input interface. gray characters output tape target symbols used training. variety recent work explored learning simple algorithms. many different embodiments controller-interface abstraction formalized model. neural turing machine uses modiﬁed lstm controller three inferences sequential input delayed output differentiable memory. model able learn simple algorithms including copying sorting. stack controller three interfaces sequential input stack memory sequential output. learning simple binary patterns regular expressions demonstrated. closely related work recently extended neural deque list instead. end-to-end memory networks feed-forward network controller interfaces consisting soft-attention input plus delayed output model applied simple tasks involve logical reasoning. contrast model automatically determines produce output uses general interfaces. however approaches continuous interfaces permit training backpropagation gradients. approach differs uses discrete interfaces thus challenging train since must rely reinforcement learning instead. notable exception reinforcement learning neural turing machine version discrete interfaces. stack-rnn also uses discrete search procedure interfaces unclear would scale larger problems. problem learning algorithms origins ﬁeld program induction liang wineberg oppacher solomonoff domain model infer source code program solves given problem. similar goal ours quite different setting. i.e. produce computer program rather neural operate interfaces tapes implements program without humanreadable. relevant work learns algorithms hanoi tower problem using simple form program induction incremental learning components. genetic algorithms goldberg also considered form program induction mostly based random search strategy rather learned one. similar train controller approximate q-function. however introduce several modiﬁcations classical q-learning. first watkins sutton barto helps overcome non-stationary environment. unaware prior work uses watkins purpose. second reparametrized function become invariant sequence length. finally penalize ||q|| might help remove positive bias understand behavior model provide upper bound performance train model supervised setting i.e. ground truth actions provided. note controller must still learn symbol output. done purely backpropagation since actions known. facilitate comparisons difﬁculty tasks common measure complexity corresponding number time steps required solve task instance reserve task involving sequence length requires time-steps conversion factors sequence lengths complexity follows copy=; reverse=; walk=; addition=; addition= single digit multiplication=. task train separate model starting sequences complexity incrementing achieves accuracy held-out examples current length. training stops model successfully generalizes examples complexity three different cores controllers explored unit -layer lstm; unit -layer model fig. show accuracy different controllers tasks test instances increasing complexity time-steps. simple feed-forward controller generalizes perfectly copy reverse walk tasks completely fails remaining ones lack required memory†. rnn-based controllers succeed varying degrees although variability performance observed. insight obtained examining internal state controller. this compute autocorrelation matrix‡ network state time model processing reverse task example length trained sequences length shorter. problem distinct states move right reached move left start. fig. plots models three different controllers. larger controller capacity less similar states within phases execution showing captured correct algorithm. ﬁgure also shows conﬁdence actions time. case high capacity models initial conﬁdence move left action high drops moving along sequence. controller learned training change direction steps. consequently unexpectedly long test sequence makes unsure correct action contrast simple feed-forward controller show behavior since stateless thus capacity know within sequence. equivalent automata shown fig. fig. shows incorrect time-dependent automata learned over-expressive rnn-based controllers. note argument empirically supported results table well related work found limited capacity controllers effective. example latter case counting memorization tasks used controllers units respectively. figure test accuracy tasks supervised actions runs feed-forward lstm controllers. setting optimal policy provided. complexity number time steps required compute solution. every task slightly different conversion factor complexity sequence length complexity copy walk would mean input symbols; input symbols; addition would involve reverse would correspond long numbers; addition would involve three long numbers single digit multiplication would involve single long number. previous section assumed optimal controller actions given training. meant output symbols need predicted could learned backpropagation. consider setting actions also learned test true capabilities models learn simple algorithms pairs input output sequences. q-learning standard reinforcement learning algorithm learn sequence discrete actions solves problem. function estimated future rewards updated †ammending interfaces allow reading writing interface would provide mechanism long-term memory even feed-forward controller. lack generalization issues would become issue. ‡let controller state time autocorrelation time-steps given number table three models different controllers trained reverse task applied digit test example. shows conﬁdence values actions input tape move left move right function time. correct model equivalent two-state automata thus expect controller hidden state occupy distinct values. autocorrelation matrices show case feed-forward model distinct blocks high correlation. however lstm controllers structure loosely present matrix indicating failed learn correct algorithm. figure automata describing correct solution reverse problem. model ﬁrst right suppressing prediction. then left predict sees given moment another automata solves reverse problem short sequences generalize arbitrary length sequences unlike expressive models like lstms tend learn incorrect automata. training according taking action state causes transition state case deterministic. reward experienced state discount factor learning rate. another commonly considered quantity maxa called value function expected future rewards starting state moreover function values optimal policy. controller receives reward every time correctly predicts digit since overall solution task requires digits correct terminate training episode soon incorrect prediction made. learning environment non-stationary since even model initially picks right actions symbol prediction unlikely correct model receives reward. training symbol prediction reliable correct action rewarded§. important reinforcement learning algorithms assume stationarity environment true case. learning non-stationary environments well understood deﬁnitive methods deal however empirically non-stationarity partially addressed watkins detailed section reinforcement train symbol output well actions environment would stationary. however would mean ignoring reliable signal available direct backpropagation symbol output. purpose reinforcement learning learn policy yields highest future rewards. q-learning indirectly learning q-function. optimal policy extracted taking argmax note shifting scaling induces policy. propose dynamically rescale independent length episode within small range making easier predict. deﬁne reparametrization. roughly range correspond close could decomposed multiplicatively however practice access thus instead estimate future rewards based total number digits left sequence. since every correct prediction yields reward optimal policy achieve future rewards equal number remaining symbols predict. number remaining symbols predict known denote note form supervision albeit weak one. therefore normalize q-function remaining rewards left task watkins update eqn. comes parts observed reward estimated future reward setting factors make former reliable latter rewards deterministic non-stationarity means estimates unreliable environment evolves. consequently single action recurrence used eqn. improved upon on-policy actions chosen. precisely at+t consecutive actions induced special form watkins classical applications watkins suggest choosing small trades-off estimates based various numbers future rewards. rolls back classical q-learning. reliability rewards found better however needs study. note unrolling rewards take place non-greedy action taken. using \u0001-greedy policy means would expect able unroll steps average. value used experiments corresponds steps average. penalty q-function reparameterizing q-function optimal correct action zero otherwise. encourage estimate converge this introq effect introducing margin correct incorrect actions greatly improving generalization. commence training make non-zero good accuracy reached short samples apply enhancements tasks series experiments designed examine contribution them. unless otherwise speciﬁed controller -layer model units. selected basis mean performance across tasks supervised setting performance reinforcement learning methods tend highly stochastic repeat experiment times different random seed. model trained using characters takes hrs. model considered successfully solved task able give perfect answer test instances digits length. model trained batch size learning rate using initialization multiplied tasks trained curriculum used supervised experiments whereby sequences initially complexity accuracy achieved increased model able solve validation sequences length show results various combinations terms table experiments demonstrate standard q-learning fails tasks additions penalty term) give signiﬁcant improvements. three used model able succeed tasks providing appropriate curriculum controller used. reverse walk tasks default controller failed completely. however using feed-forward controller instead enabled model succeed dynamic discount watkins used. noted above -row addition required careful curriculum model able learn successfully. increasing capacity controller hurts performance echoing fig. last columns table show results test sequences length except multiplication models still generalized successfully. fig. shows accuracy function test example complexity standard q-learning enhanced version. difference performance clear. high complexity corresponding digits accuracy starts drop complicated tasks. note trends essentially observed supervised setting suggesting q-learning blame. instead inability controller learn automata seems cause. potential solutions might include noise injection discretization state state error correction mechanism regularizing learned automata using principles. however issue inability perfectly represent automata examined separately setting actions learnt results found appendices. addition task model able discover multiple correct solutions different movement pattern input tape table appendix sheds light trade-off errors actions errors symbol prediction varying base used arithmetic operations hence size table success rates classical q-learning versus enhanced q-learning. gru-based controller used tasks except reverse walk feed-forward network. curriculum learning also used -row addition task dynamic discount watkins penalty term used model consistently succeeds tasks. model still performs well test sequences length apart multiplication task. increasing capacity controller results worse performance figure test accuracy function task complexity standard q-learning enhanced version penalty term). accuracy corresponds fraction correct test cases explored ability neural network models learn algorithms simple arithmetic operations. experiments supervision reinforcement learning shown able successfully albeit caveats. q-learning shown work well supervised case. disappointingly able single controller could solve tasks. found tasks generalization ability sensitive memory capacity controller little would unable solve complex tasks rely carrying state across time; much resulting model would overﬁt length training sequences. finding automatic methods control model capacity would seem important developing robust models type learning problem. wish thank jason weston marc’aurelio ranzato przemysław mazur useful discussions comments. also thank christopher olah lstm ﬁgures used paper accompanying video. kyunghyun merrienboer bart gulcehre caglar bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. sreerupa giles guo-zheng. learning context-free grammars capabilities limitations recurrent neural network external stack memory. proceedings fourteenth annual conference cognitive science society glorot xavier bengio yoshua. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics holland john adaptation natural artiﬁcial systems introductory analysis applications biology control artiﬁcial intelligence. press cambridge isbn mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. arxiv preprint arxiv. wineberg mark oppacher franz. representation scheme perform program induction canonical genetic algorithm. parallel problem solving natureppsn springer examination models learned addition task notice three different solutions discovered. give correct answer differ actions input grid shown fig. explore learning time varies size target vocabulary varied. trades reward frequency reliability. small vocabularies reward occurs often less reliable since chance wrong action sequence yielding correct result relatively high copying reverse tasks altering vocabulary size alters variety symbols tape. however arithmetic operations involves change base inﬂuences task complex way. instance addition base requires memorization digit-to-digit addition table size instead base table shows median training time function vocabulary size. results suggest infrequent reliable reward preferred frequent noisy one. table median training time runs vary base used different problems. training stops model successfully generalizes test sequences length results show relative importance reward frequency versus reliability latter important. reward reinforcement learning systems drives learning process. setting control rewards deciding when much give. examine various kinds rewards inﬂuence learning time system. vanilla setting gives reward every correct prediction reward every incorrect one. refer setting reward. consider settings addition this rely probabilities correct prediction. target symbol probability predicting label setting discretized reward sort gives order indices i.e. discretized reward yields reward reward otherwise environment gives reward continuous reward table gives results three different reward structures showing training time tasks might expect continuous reward would convey information discrete thus result faster training. however results support hypothesis training seems harder continuous reward discrete one. hypothesize continuous reward makes environment less stationary might make q-learning less efﬁcient although needs veriﬁcation. table median training time tasks three different reward structures. reward model gets reward every correct prediction otherwise. discretized reward provides values reward prediction sufﬁciently close correct one. continuous reward gives probability correct answer reward. text details.", "year": 2015}