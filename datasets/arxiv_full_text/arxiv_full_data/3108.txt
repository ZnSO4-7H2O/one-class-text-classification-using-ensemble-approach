{"title": "Learning to encode motion using spatio-temporal synchrony", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We consider the task of learning to extract motion from videos. To this end, we show that the detection of spatial transformations can be viewed as the detection of synchrony between the image sequence and a sequence of features undergoing the motion we wish to detect. We show that learning about synchrony is possible using very fast, local learning rules, by introducing multiplicative \"gating\" interactions between hidden units across frames. This makes it possible to achieve competitive performance in a wide variety of motion estimation tasks, using a small fraction of the time required to learn features, and to outperform hand-crafted spatio-temporal features by a large margin. We also show how learning about synchrony can be viewed as performing greedy parameter estimation in the well-known motion energy model.", "text": "consider task learning extract motion videos. show detection spatial transformations viewed detection synchrony image sequence sequence features undergoing motion wish detect. show learning synchrony possible using fast local learning rules introducing multiplicative gating interactions hidden units across frames. makes possible achieve competitive performance wide variety motion estimation tasks using small fraction time required learn features outperform hand-crafted spatio-temporal features large margin. also show learning synchrony viewed performing greedy parameter estimation well-known motion energy model. classic motion energy model turns frames video representation motion summing squares gabor ﬁlter responses motivations computation fact sums squared ﬁlter responses allow detect oriented energies spatiotemporal frequency bands. this turn makes possible encode motion independently phase information thus represent motion degree independent moving. related models proposed binocular disparity estimation also involves estimation displacement local features across multiple views. supervised deep learning techniques become popular learn features videos interest learning-based models motion fueled part observation activity recognition hand-crafted features tend perform uniformly well across tasks suggests learning features instead designing hand. unlike images videos somewhat resistant feature learning many standard models work well. images example models like autoencoder even k-means clustering known yield highly structured gabor-like ﬁlters perform well recognition tasks seem true videos neither autoencoders k-means shown work well notable exceptions feature learning models like inference involves search ﬁlters sparse time minimize squared reconstruction error shown learn least visually good ﬁlters references chapter exception energy models compute sums squared ﬁlter responses inference shown work well activity recognition tasks work propose possible explanation models work well videos models not. show linear encoding permits detection transformations across time supports detection temporal synchrony video features. makes possible interpret motion energy models combine independent contributions motion encoding namely detection synchrony encoding invariance. show disentangling contributions provides different perspective onto energy model suggests approaches learning. particular show learning linear encoding viewed learning presence multiplicative gating interactions allows learn competitive motion features conventional cpu-based hardware small fraction time required previous methods. consider task computing representation motion given frames video. classic energy model solves task detecting subspace energy. amounts computing squared quadrature fourier gabor coefﬁcients across multiple frequencies orientations motivation behind energy model fourier amplitudes independent stimulus phase yield representation motion degree independent image content. shall show below view confounds independent contributions energy model disentangled practice. alternative computing squares originally proposed stereopsis crosscorrelation model computes products ﬁlter-responses across frames. shown products ﬁlter responses quadrature encodes angles invariant subspaces associated transformation. representation angles thereby also yields phaseinvariant representation motion like energy model also confounds invariance representing transformations shall show. shall discuss synchrony detection allows compute motion content-invariance achieved pooling afterwards desired. consider ﬁlters shall encode transformation images restrict attention transformations represented orthogonal transformation pixel space words orthogonal image warp. include permutations include particular common spatial transformations local translations combinations assumption orthogonality transformations made implicitly also motion energy model. true. shall call synchrony condition. amounts choosing ﬁlter pair example transformation want detect determine whether ﬁlters yield equal responses applied sequence frames shall later relax exact equality approximate equality. last equation follows shows presence transformation implies synchrony ﬁlters themselves related order detect presence thus look synchrony condition using ﬁlters transformed inductive reasoning step accumulate evidence transformation looking synchrony across multiple ﬁlters. absence transformation implies ﬁlter pairs violate synchrony condition. interesting note gabor ﬁlters phase shifts position shifts locally global fourier features phase shifts position shifts exactly identical. thus synchrony inputs sequence phase-shifted fourier features example allows detect transformations local translations. shall discuss learning ﬁlters video data section synchrony condition extended sequence frames follows denote input frames corresponding ﬁlters. detect transformations relates adjacent frames xi+) condition presence sequence transformations turns check synchrony condition practice necessary detect equality transformed ﬁlter responses across time current deep learning models based layers weighted summation followed nonlinearity. detection synchrony unfortunately cannot performed layer weighted summation plus nonlinearity shall discuss now. fact ﬁlter responses attain maximum inputs match ﬁlters seems suggest thresholding would allow detect synchrony. case however thresholding works well inputs similar feature vectors themselves inputs practice normalized superpositions multiple feature vectors. thus detect synchrony thresholded would need threshold small enough represent features explain fraction variability assume example features account variance inputs would reduce threshold half maximum attainable response able detect synchrony. however level distinguish stimuli satisfy synchrony condition stimuli image perfect match ﬁlter zero overlap ﬁlter situation become worse feature vectors account less variability. important note multiplicative interactions allow check synchrony condition using entirely local operations figure illustrates deﬁne neuron detect synchrony allowing gating interactions within dendritic tree. model consisting multiple synchrony detector units single-layer model cross-talk required between units. shall show fact allows fast local update rules learning synchrony data. stark contrast learning energy models bi-linear models rely non-local computations back-prop learning although multiplicative interactions common ingredient models motivation allow computation subspace energies subspace angles rather synchrony usefulness intra-dendritic gating discussed lengths neuroscience literature example work colleagues besides multi-layer bilinear models discussed above received much attention machine learning. dendritic gating reminiscent also pisigma neurons applied supervised prediction tasks past. willing abandon weighted sums allowable type module constructing deep networks simple detect synchrony allowing multiplicative interactions ﬁlter responses product take large negative) values. sufﬁciently small response shut response either regardless ﬁlter response image. even threshold sacriﬁce ability differentiate presence feature images partial presence transformed feature images related less formal argument product interactions synchrony detection amounts operation akin logical and. odds observation weighted sums accumulate information resemble figure shows illustration product response using example. ﬁgure shows product transformed ﬁlters inputs yields large response whenever input well-represented ﬁlter input evolves time similar ﬁlter ﬁgure also illustrates failing satisfy either yield small product response need satisfy condition makes product response dependent input. dependency alleviated pooling multiple products involving multiple different ﬁlters top-level pooling unit ﬁres subset synchrony detectors ﬁres. classic energy model example pools ﬁlter pairs quadrature eliminate dependence phase practice however phase also frequency position orientation determine whether image aligned ﬁlter not. investigate pooling separately trained pooling layer section discuss learn ﬁlters allow detect synchrony condition. principle many ways achieve practice introduce temporal variant k-means algorithm learn synchrony. appendix present another model based contractive autoencoder call synchrony autoencoder following denote images rq×n denote matrices whose rows contain feature vectors denote ﬁrst note that given cluster centers performing online gradient-descent standard k-means clustering objective equivalent updating cluster centers using local competitive learning rule note computing multiplication equivalent replacing k-means winner-takes-all units gating units allows redeﬁne k-means objective function reconstruction error input assigned prototype vector gated projection input similar online-kmeans rule obtain hebbian term active forgetting term )ty)) enforces competition among hiddens. hebbian term contrast standard k-means gated involves pre-synaptic input projected pre-synaptic input coming input. similarly update rule deﬁned section denote input frames corresponding ﬁlters. even-symmetric nonlinearity global minimum zero square detector synchrony condition too. reason binomial identity states square terms contains pair-wise products individual terms plus squares individual terms. latter change preferred stimulus unit high individual terms equal high value synchrony condition case sequences squaring non-linearities applied phase-shifted gabor ﬁlter responses cornerstone energy model figure filters learned synthetic translations natural image patches. filters learned natural videos. columns frames learned ﬁlters. column filter groupings learned separate layer k-means column shows ﬁlters contributing cluster center. even-symmetric non-linearities implicitly compute pair-wise products implemented using multiplicative interactions consider unit figure using tied inputs assume contain video sequence rather single image. also tied weights output unit practice model equal square learn weights required. enable model section encode motion across multiple frames thus proceed follows concatenation frames rq×n denote matrix containing feature vectors stacked row-wise. feature composed frame features spans frame input video. inference case sk-means model sigmoid activation function squared features experiments instead winner-takes-all case object classiﬁcation relaxing harsh sparsity induced k-means tends yield codes better suited recognition tasks. example ﬁlters learned contractive sequences shown figure ﬁrst ﬁgure columns show ﬁlters learned synthetic movies generated translating image patches natural image dataset columns second show ﬁlters learned blocks sampled videos broadcast database obtained similar ﬁlters using skmeans model. study dependencies features performed kmeans clustering using centroids hiddens extracted training sequences. column figure shows active clusters across training data features contribute cluster centers. shows pooling units group together features similar orientation position arbitrary frequency phase. expected translation direction affect frequencies phase angles nearby orientations positions. note particular pooling across phase angles alone done classic energy model would sufﬁcient fact solution found pooling. activity recognition common task evaluating models motion understanding. allow fair comparison pipeline described using features learned models. train models pca-whitened input patches size number training samples number product units ﬁxed inference blocks size patch size cropped super blocks size blocks cropped stride axis giving blocks super block. feature responses blocks concatenated dimensionally reduced using form local feature. using separate layer k-means vocabulary spatio-temporal words learned samples training. experiments super blocks cropped densely video overlap. finally χ-kernel histogram spatio-temporal words used classiﬁcation. sports action classes. total number videos dataset increase data horizontally ﬂipped version video dataset. like train multi-class classiﬁcation leaveone-out evaluation. original video tested videos training except ﬂipped version tested itself. hollywood twelve activity classes. consists test samples train samples video samples belonging multiple classes. hence binary used compute average precision class mean classes reported yupenn dynamic scenes fourteen scene categories videos category. gray-scale version videos experiments. leave-one-out cross-validation used performance evaluation results shown tables show sk-means competitive stateof-the-art although learning simpler existing methods. evaluate importance element-wise products hidden units also evaluated k-means well standard autoencoder contraction regularization hollywood dataset. models achieved average precision respectively much lower performance skmeans. also tested covariance auto-encoder learns additional mapping layer pools squared simple cell responses. table shows performance model also considerably lower single-layer models showing learning pooling layer along features help. show models learn features generalize across datasets trained random samples datasets used feature extraction report performance others. performances using metrics shown table seen performance gets reduced fairly small fraction compared training samples respective dataset. case training dataset performance hollywood considerably lower. probably less diverse activities compared hollywood. training times learning motion features shown table show sk-means orders magnitude faster models. implementations used theano library also calculated inference times using similar metric computed time required extract descriptors videos hollywood dataset resolution pixels average inference times making models feasible practical possibly real-time applications. experiments performed system .ghz gpu. work shows learning motion videos simpliﬁed signiﬁcantly sped disentangling learning spatio-temporal evolution signal learning invariances inputs. allows achieve competitive performance activity recognition tasks fraction computational cost learning motion features required existing methods motion energy model also showed learning motion possible using entirely local learning rules. computing products using dendritic gating within individual competing units viewed efﬁcient compromise bi-linear models expensive encode interactions pairs pixels factored models multi-layer models rely complicated training schemes back-prop work well recognition. acknowledgments work supported part german federal ministry education research project contractive regularization well-known regularization important extract useful features learn sparse representations. here contraction regularization amounts adding frobenius norm jacobian extracted features i.e. squares partial derivatives respect training regularization term reconstruction cost using hyperparameter contractive regularization possible bi-linear models computational complexity computing contraction gradient multiple layers single layer model synchrony autoencoder makes application contractive regularization feasible. contraction parameter cross-validation. qtxt also accounts synchrony explained earlier. reconstruction error regularization term model derived replacing appropriate terms equations respectively. present additional approach encoding motion across frames based contractive autoencoder like synchrony k-means algorithm extended sequences frames using analogous construction given images ﬁrst compute linear ﬁlter responses wyy. given derivations section encoding motion inherent image sequence deﬁned element-wise multiplication sigmoid nonlinearity deﬁnition makes sense only features vectors related transformation wish detect. shall discuss deﬁne reconstruction criterion enforces criterion. standard train autoencoder images decoder minimize reconstruction error. case presence multiplicative interactions encoder encoding loses information sign input. however note interpret multiplicative interactions gating discussed previous section. suggests deﬁning reconstruction error input given other. decoder thus perform element-wise multiplication hiddens factors input reconstruct other. also view re-introducing sign information reconstruction time. assuming autoencoder tied weights reconstructed inputs deﬁned learning amounts minimizing reconstruction error wrt. ﬁlters contrast bi-linear models trained using similar criteria representation motion dependent image content fourier phase translational motion. dependence removed using separately trained pooling layer shall show. absence pooling feature learning allows much efﬁcient learning show section note that practice bias terms deﬁnition hiddens reconstructions. bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. scipy hyv¨arinen aapo hoyer patrik. emergence phaseshift-invariant features decomposition natural images independent feature subspaces. neural comput. july hyvarinen aapo hurri jarmo hoyer patrick natural image statistics probabilistic approach early computational vision. springer publishing company incorporated martin fowlkes malik database human segmented natural images application evaluating segmentation algorithms measuring ecological statistics. iccv olshausen b.a. learning sparse overcomplete represenimage protations time-varying natural images. cessing icip proceedings. international conference volume vol. sept olshausen bruno cadieu charles culpepper jack warland david. bilinear models natural images. spie proceedings human vision electronic imaging jose rumelhart zipser parallel distributed processing explorations microstructure cognition vol. chapter feature discovery competitive learning press shin yoan ghosh joydeep. pi-sigma network efﬁcient higher-order neural network pattern classiﬁcation function approximation. international joint conference neural networks taylor graham fergus lecun yann bregler christoph. convolutional learning spatiotemporal features. proceedings european conference computer vision part eccv’ wang heng ullah muhammad muneeb kl¨aser alexander laptev ivan schmid cordelia. evaluation local spatio-temporal features action recognition. university central florida u.s.a", "year": 2013}