{"title": "Adversarial Examples that Fool Detectors", "tag": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "abstract": "An adversarial example is an example that has been adjusted to produce a wrong label when presented to a system at test time. To date, adversarial example constructions have been demonstrated for classifiers, but not for detectors. If adversarial examples that could fool a detector exist, they could be used to (for example) maliciously create security hazards on roads populated with smart vehicles. In this paper, we demonstrate a construction that successfully fools two standard detectors, Faster RCNN and YOLO. The existence of such examples is surprising, as attacking a classifier is very different from attacking a detector, and that the structure of detectors - which must search for their own bounding box, and which cannot estimate that box very accurately - makes it quite likely that adversarial patterns are strongly disrupted. We show that our construction produces adversarial examples that generalize well across sequences digitally, even though large perturbations are needed. We also show that our construction yields physical objects that are adversarial.", "text": "linear feature constructions without strong mathematical constraints constructions taking position means cannot methods largely accurate effective. detectors classiﬁers. classiﬁer accepts image produces label. contrast detector like faster rcnn identiﬁes bounding boxes worth labelling generates labels box. ﬁnal label generation step employs classiﬁer. however statistics bounding boxes cover objects detector complex well understood. modern detectors like yolo predict boxes labels using features ﬁxed grid resulting fairly complex sampling patterns space boxes means pixels outside participate labelling box. another important difference detectors usually pooling feature resizing might effective disrupting adversarial patterns. date successful adversarial attack detector demonstrated. paper demonstrate successful adversarial attacks faster rcnn generalize yolo also discuss generalization ability adversarial examples. adversarial perturbation generalizes circumstances change corresponding images remain adversarial. example perturbation stop sign generalizes different distances remains adversarial camera approaches stop sign. example generalizes better remains adversarial cases adversarial example cannot generalize threat majority real world systems. contributions paper follows demonstrate method construct adversarial examples fool faster rcnn digitally; examples produced method reliably either missed mislabeled detector. examples without modiﬁcation also fool yolo indicating construction produces examples transfer across models. adversarial example example adjusted produce wrong label presented system test time. date adversarial example constructions demonstrated classiﬁers detectors. adversarial examples could fool detector exist could used maliciously create security hazards roads populated smart vehicles. paper demonstrate construction successfully fools standard detectors faster rcnn yolo. existence examples surprising attacking classiﬁer different attacking detector structure detectors must search bounding cannot estimate accurately makes quite likely adversarial patterns strongly disrupted. show construction produces adversarial examples generalize well across sequences digitally even though large perturbations needed. also show construction yields physical objects adversarial. adversarial example example adjusted produce wrong label presented system test time. literature adversarial examples imperceivable perturbations unexpected properties biggest mysteries neural networks. range constructions yield adversarial examples image classiﬁers good evidence small imperceivable adjustments sufﬁce. furthermore athalye show possible build physical object visible perturbation patterns persistently misclassiﬁed standard image classiﬁers different view angles roughly ﬁxed distance. good evidence adversarial examples built classiﬁer fool others success attacks seen warning highly adversarial examples physically created successfully still fool detectors suitable circumstances. also slip recent strong image processing defenses adversarial examples. practice adversarial examples require quite large disruptions pattern object order fool detectors. physical adversarial examples require bigger disruptions digital examples succeed. adversarial examples interest mainly adjustments required seem small easy obtain numerous search procedures generate adversarial examples searches look example near correctly labelled example mislabelled. printing adversarial images photographing retain adversarial property suggests adversarial examples might exist physical world. existence could cause great deal mischief. evidence difﬁcult build physical examples fool stop sign detector particular actually takes video existing adversarial stop sign adversarial pattern appear affect performance detector much. speculated might because adversarial patterns disrupted viewed different scales rotations orientations. created discussion. openai demonstrated search procedure could produce image misclassiﬁed viewed multiple scales blurring texture generate would likely imperceptible observers. openai also demonstrated adversarial image misclassiﬁed viewed multiple scales orientations however signiﬁcant visible artifacts image; would think obviously tampered with. recent work demonstrated physical objects persistently misclassiﬁed different angles roughly ﬁxed distance search procedure manipulates texture object. procedure samples viewing conditions object renders obtain images finally procedure adjusts texture obtain images close original images high probability misclassiﬁcation. adversarial properties resulting objects robust inevitable errors color etc. producing physical objects digital representations. dence attack likely misclassiﬁed) current procedures build adversarial examples deep networks appear subvert feature construction implemented network produce patterns activation late stage relu’s; exploited build form defence evidence feature constructions admit adversarial attacks however adversarial attacks typically introduce unnatural patterns images image processing methods remove patterns yield successful defenses. showed cropping rescaling depth reduction jpeg compression decompression resampling reconstructing using total variation criteria image quilting provide quite effective ways removing adversarial patterns detectors classiﬁers usual attack classiﬁers attacks aware attack classiﬁers. however many applications classiﬁers useful themselves. road sign good example. road sign classiﬁer would applied images consist largely road sign little application need road sign classiﬁer except component road sign detector unusual practice deal images consist largely road sign. instead usually deals images contain many things must label road sign. natural study road sign classiﬁers image classiﬁcation remains difﬁcult academic studies feature constructions important. particular threat posed attack road sign classiﬁer. attack road sign detector entirely different matter. example imagine danger could template that spray paint could ensure detector reads stop sign yield sign result important know whether examples could exist robust adversarial property practice. recently evtimov shown several physical stop signs misclassiﬁed cropped stop signs frames presenting classiﬁer. cropping proxied box-prediction process detector; however attack intended attack detector showed construction fool standard detector likely because cropping process proxy detector’s selection well suggested constructing adversarial example fools detector might hard. figure shows stop signs presented reliably detected faster rcnn. conditions. registration reconstruction based approach generates adversarial perturbations video sequences object moving cameras. require objects videos accurately aligned space. easily register stop signs polygons. moreover accurately register face images virtual face model. hence perform experiments types data. stop sign example demonstrate attack extends objects registered image domain root coordinate system search adversarial pattern looks like stop sign fools faster rcnn. select diverse frames training examples generate pattern. stop sign represented texture root coordinate system. construct correspondences eight vertices stop sign instances training frames vertices correspondences estimate viewing maps texture root coordinate system appropriate pattern training frame also incorporate illumination intensity estimated computing average intensity stop sign image. relative illumination intensities used scale adversarial perturbations. write image frame obtained superimposing frame using mapping stop sign bounding boxes obtained applying faster rcnn image score produced faster rcnn’s classiﬁer stop sign produce adversarial example minimize mean score stop sign produced faster rcnn training images possibly subject constraint close normal stop sign distance. also investigated minimizing maximum score stop sign proposals found minimizing mean score gives slightly better results. minimization procedure first compute computing gradients frame coordinate system mapped root coordinate system inverse view mapping cropped extent coordinate system. average gradients mapped training frames. however directly using gradients take large steps frequently stalls optimization process. instead computing descent direction sign gradients given pattern facilitates optimization process. optimization process usually takes hundreds even thousands steps. termination criterion stop optimization pattern fools detector cases validation set. another termination criterion ﬁxed number iterations. large steps hard experiments taking large steps unsigned gradients stalls optimization process believe large steps hard take reasons. first instance pattern occurs different scale meaning must updown-sampling gradients mapped root coordinate system. although register images subpixel accuracy bilinear method interpolate transformation process signal losses still inevitable. section show evidence effect make patterns more rather less robust. second structure network means gradient poor guide behavior large scales. particular relu network divides input space large number cells values layer softmax layer continuous piecewise linear function within cell. network trained constant output large pieces input space constraining distance original stop sign order create less perceivable adversarial perturbations constrain distance original stop sign small. distance loss added cost function minimize extend experiments onto faces complex geometries larger intra class variances demonstrate analysis generalizes classes. face setting search pattern fools faster rcnn based face detector looks like original face. root coordinate system faces virtual high quality face mesh generated morphable face model video sequences face reconstruct geometry face input frames using morphable face model built facewarehouse data. model produces face mesh function identity parameters expression parameters facetracker used detect landmarks face frames recover parameters poses face mesh minimizing distances projected landmark vertices corresponding landmark locations image planes. construction gives pixelto-mesh mesh-to-pixel dense correspondences face frames root face coordinate system projecting image pixels face meshes barycentric coordinates achieve subpixel accurate pixel-to-pixel registrations face frames correspondences used transfer gradients face image coordinates root coordinate system merge gradients multiple images reverse transfer merged gradients back face image coordinates. section describe depth experiments results got. supplementary materials include videos results downloaded http//jiajunlu.com/docs/ advdetector_supp.zip. high resolution paper downloaded http//jiajunlu.com/ figure modifying single stop sign image attack faster rcnn successful. original stop sign image stop sign reliably detected. second image small perturbations added whole image stop sign detected. last image small perturbations added stop sign region instead whole image stop sign detected vase. figure modifying face image attack faster rcnn successful. original face image face reliably detected. second image small perturbations added whole image face detected. last image slightly larger perturbations added face region instead whole image face detected. detectors affected internal thresholds. faster rcnn uses maximum suppression threshold conﬁdence threshold. stop signs used default conﬁgurations. faces found detector willing detect faces made less responsive faces used default yolo conﬁgurations. figure adversarial examples stop signs faster rcnn generalize across view conditions. original sequence ﬁrst test video sequence captured real stop sign stop sign detected frames. apply attack training videos generate cross view condition adversarial perturbation apply perturbation test sequence generate attacked sequence second row. digital attack stop sign either detected detected kite. figure adversarial examples faces faster rcnn based face detector generalize across view conditions. original sequence images ﬁrst sampled test video sequence faces reliably detected. apply attacking method training videos generate cross view condition adversarial perturbation apply perturbation test sequence generate attacked sequence second row. digital attack. easily adjust pattern single image fool detector change pattern tiny. practical signiﬁcance shows search method small adversarial perturbations. really interested produce pattern fails detected image. much harder pattern needs generalize different view conditions still adversarial patterns situation patterns found process involve signiﬁcant changes stop signs faces. stop sign dataset panasonic hc-vm camera take videos camera approaching stop signs extract diverse frames video. manually register stop signs attacking method generate uniﬁed adversarial perturbation frames. videos generating adversarial perturbations videos validation videos evaluation validation figure generate three adversarial stop signs attacking method. ﬁrst stop sign distance penalty termination criterion successfully attacking validation images. second stop sign uses distance penalty objective function terminates validation images attacked. last stop sign also adopts distance penalty performs large ﬁxed number iterations. three patterns reliably fool detectors mapped videos. however physical instances patterns equally successful. ﬁrst stop signs physical objects occasionally fool faster rcnn; third much extreme pattern effective. termination criteria described section figure gives example video sequence corresponding attacked video sequence. table shows stop sign detection rates different circumstances. plan release labelled figure print three adversarial stop signs figure stick real stop sign. took videos driving printed stop signs faster rcnn videos. notice adversarial perturbations generalize well digitally. render detection results stop signs make ﬁgures clean. three sequences ﬁgure correspond three stop signs order. ﬁrst sequences stop signs detected without trouble last sequence stop sign detected video physical adversarial stop sign. result poor texture contrast tree though sequence seen training. face dataset sony camera take videos still face different distances angles extract diverse frames video. morphable face model approach register faces attacking method generate uniﬁed adversarial perturbation. videos generating adversarial perturbations video validation video evaluation again validation termination criteria described section figure shows example video sequence corresponding attacked video sequence. experiments smallest perturbations faces could generalize. table shows face detection rates different circumstances. also plan release processed dataset. summary possible attack stop signs faces multiple images require generalize similar view condition images. however require strong perturbation patterns generalize. refer supplementary materials details. attacks digital world attacks physical world means adversarial perturbations generalize well digital world generalize physical world. suspect various practical concerns sensor properties view conditions printing errors lighting etc. paper print stop signs perform physical experiments them believe similar conclusions apply faces. performed physical experiments three adversarial perturbation patterns figure results table show less perturbed stop signs still detected faster rcnn large perturbations hard detect. frames physical experiments could found figure refer supplementary materials videos. performed analysis data table table regularized logistic regression used predict success many different cases. important variable detector whether adversarial example physical scale adversarial examples certain classiﬁer generalize across different classiﬁers. test whether adversarial examples faster rcnn generalize across detectors feed images yolo. categorize adversarial examples three categories single image examples small perturbations multiple image examples table table reports detection rates faster rcnn yolo multiple image digital attacks stop signs. cell ratio semicolon represents detection rate faster rcnn ratio semicolon represents detection rate yolo. tree means background stop sign tree contrast means background stop sign high contrast. following background means perturbations large means perturbations extremely large. three dark stop signs detection rates calculated three different distances train/val/test splits. attack faster rcnn multiple view conditions adversarial perturbations generalize view conditions. adversarial examples also generalize yolo especially background tree. table table reports detection rates faster rcnn based face detector multiple image digital attacks faces. means images experiments means images. means frontal face means side face. small perturbations applied attacks training images succeed generalize validation testing images. large perturbations applied attacks generalize different view conditions. table detection rates physical adversarial stop signs physical clean stop signs faster rcnn yolo different circumstances. table layout similar table stop signs different brightness large perturbations stop sign different brightness extremely large perturbations. report detection rates inches adversarial stop signs inches clean normal stop signs applicable large perturbations generalize across viewing conditions digitally physical examples large perturbations. experiments show small perturbations generalize yolo obvious perturbation patterns generalize yolo good probability. examples given figure detection rates found table table previous settings attack whole masked objects images however usually hard apply attacks physical world. example modifying whole stop sign patterns useless practice wearing whole face mask perturbation patterns hard too. would effective attack manufacture small stickers perturbation patterns sticker attached small region stop sign face foretable evaluate effectiveness simple defense methods. means downsample input image resolution half upsample original size. means total variation denoise removes high frequency information. physical attack numbers counted real stop sign near adversarial one. simple defense methods effective single image perturbations effective multiple image perturbations generalize. also cannot defeat physical adversarial perturbations. figure test whether adversarial examples generated faster rcnn generalize yolo. ﬁrst adversarial examples generated single image small perturbations. yolo detect stop signs without trouble. second adversarial examples generated multiple images digitally perturbed images fool yolo half times. last physically printed adversarial stop signs still fool yolo circumstances. detailed summary found table table figure localized attacks stop signs faces fail multiple view condition setting. applied attacks regions stop signs faces large number iterations introduced extremely large perturbations objects still detected. detail localized attacks stop signs sometimes digitally fool stop signs middle near stop signs; localized attacks faces cannot fool face detector. ﬁrst image example perturbed stop signs second image example perturbed faces. head detector would fail. evtimov showed example successfully attacked stop sign classiﬁers. generate adversarial patterns constrained ﬁxed region objects fool detectors however attacks occasionally successful wholly unsuccessful figure shows examples. simple defenses fail recently showed simple image processing could defeat majority imperceivable adversarial attacks. assume detectors frame rate exclude image quilting. investigated down-up sampling total variation smoothing defense. methods defeat attacks single image cannot figure apply simple defenses adversarial examples generated faster rcnn. up-down sample means sample image resolution half upsample original resolution. denoise means denoising image total variation regularization remove high frequency information keep frequency information. ﬁrst adversarial examples generated single image small perturbations detected simple image processing. second adversarial examples generated multiple view conditions still cannot detected simple defense. last physically printed adversarial stop signs still cannot detected simple defense. disrupt large patterns needed produce adversarial examples generalize figure table hypothesis phenomenon tiny perturbations work numerical accumulation mechanism robust changes obvious perturbations work pattern recognition mechanism robust better generalize. demonstrated ﬁrst adversarial examples fool detectors. construction yields physical objects fool detectors too. however adversarial perturbations able construct require large perturbations. suggests prediction step detector acts form natural defense. speculate better viewing models construction yield smaller physical digital results. patterns reveal something important detector.", "year": 2017}