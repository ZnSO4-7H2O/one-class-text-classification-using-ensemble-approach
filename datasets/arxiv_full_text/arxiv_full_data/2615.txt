{"title": "Faster variational inducing input Gaussian process classification", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Gaussian processes (GP) provide a prior over functions and allow finding complex regularities in data. Gaussian processes are successfully used for classification/regression problems and dimensionality reduction. In this work we consider the classification problem only. The complexity of standard methods for GP-classification scales cubically with the size of the training dataset. This complexity makes them inapplicable to big data problems. Therefore, a variety of methods were introduced to overcome this limitation. In the paper we focus on methods based on so called inducing inputs. This approach is based on variational inference and proposes a particular lower bound for marginal likelihood (evidence). This bound is then maximized w.r.t. parameters of kernel function of the Gaussian process, thus fitting the model to data. The computational complexity of this method is $O(nm^2)$, where $m$ is the number of inducing inputs used by the model and is assumed to be substantially smaller than the size of the dataset $n$. Recently, a new evidence lower bound for GP-classification problem was introduced. It allows using stochastic optimization, which makes it suitable for big data problems. However, the new lower bound depends on $O(m^2)$ variational parameter, which makes optimization challenging in case of big m. In this work we develop a new approach for training inducing input GP models for classification problems. Here we use quadratic approximation of several terms in the aforementioned evidence lower bound, obtaining analytical expressions for optimal values of most of the parameters in the optimization, thus sufficiently reducing the dimension of optimization space. In our experiments we achieve as well or better results, compared to the existing method. Moreover, our method doesn't require the user to manually set the learning rate, making it more practical, than the existing method.", "text": "background gaussian processes provide elegant eﬀective approach learning kernel machines. approach leads highly interpretable model allows using bayesian framework model adaptation incorporating prior knowledge problem. framework successfully applied regression classiﬁcation dimensionality reduction problems. unfortunately standard methods gp-regression gpbig data problems. variety methods proposed overcome limitation regression classiﬁcation problems. successful recent methods based concept inducing inputs. methods reduce computational complexity complex problems required number inducing points fairly making optimization method challenging. methods analyze structure variational lower bound appears inducing input classiﬁcation. first notice using quadratic approximation several terms bound possible obtain analytical expressions optimal values optimization parameters thus suﬃciently reducing dimension optimization space. provide methods constructing necessary quadratic approximations. based jaakkola-jordan bound logistic function derived using taylor expansion. results propose variational lower bounds inducing input classiﬁcation depend number parameters. propose several methods optimization bounds compare resulting algorithms state-of-the-art approach based stochastic optimization. experiments bunch classiﬁcation datasets show methods perform well better existing one. however methods don’t require tunable parameters work settings within range values thus signiﬁcantly simplifying training classiﬁcation models. keywords gaussian process; classiﬁcation; variational inference; data; inducing inputs; optimization; variational lower bound gaussian processes provide prior functions allow ﬁnding complex regularities data. gaussian processes successfully used classiﬁcation/regression problems dimensionality reduction work consider classiﬁcation problem only. dataset. complexity makes inapplicable data problems. therefore variety methods introduced overcome limitations paper focus methods based called inducing inputs. paper introduces inducing inputs approach training models regression. approach based variational inference proposes particular lower bound marginal likelihood bound maximized w.r.t. parameters kernel function gaussian process thus ﬁtting model paper shows apply approach gp-classiﬁcation problem. provides lower bound optimized w.r.t. kernel parameters variational parameters using stochastic optimization. however lower bound derived intractable approximated gauss-hermite quadratures integral approximation work develop approach training inducing input models classiﬁcation problems. analyze structure variational lower bound notice using quadratic approximation several terms bounds possible obtain analytical expressions optimal values optimization parameters thus suﬃciently reducing dimension optimization space. provide methods constructing necessary quadratic approximations based jaakkola-jordan bound logistic function derived using taylor expansion. paper organized follows. section describe standard gp-classiﬁcation framework main limitations. section introduce concept inducing inputs derive evidence lower bound section consists main contribution tractable evidence lower bounds diﬀerent methods optimization. section provides experimental comparison methods existing approach last section concludes paper. fig. shows example one-dimensional gaussian process. dark blue line mean function process light blue region σ-region diﬀerent color curves samples process. denote matrix comprised points rn×d vector corresponding class labels task predict class label point consider following model. first introduce latent function unfortunately integrals intractable since involve product sigmoid functions normal distributions. thus integral-approximation techniques estimate predictive distribution. back obtain tractable integral. predictive distribution remains intractable since one-dimensional integral easily estimated quadratures techniques. detailed derivation algorithm another algorithm based expectation propagation found previous section described gaussian process data classiﬁcation problem. however considered gaussian processes ﬁxed covariance functions. model rather limiting. order good model data good kernel hyperparameters bayesian paradigm provides tuning kernel hyper-parameters gp-model maximization model evidence given however integral intractable model since involves product sigmoid functions normal distribution. subsequent sections describe several methods construct variational lower bound marginal likelihood. maximizing lower bound respect kernel hyper-parameters could model data. number approximate methods proposed literature gpregression gp-classiﬁcation paper consider methods based concept inducing inputs. methods construct approximation based values process points. points referred inducing points. idea following. hidden gaussian process corresponds smooth low-dimensional surface surface fact well approximated another gaussian process properly chosen training points rm×d process values points predictions process training points used constructing approximate posterior distribution positions inducing inputs learned within training procedure. however simplicity following clusterize dataset clusters using k-means choose cluster centres. practice observe approach works well almost cases. following variational approach solving maximum evidence problem approach evidence lower bound introduced simpler compute evidence itself. lower bound maximized w.r.t. kernel hyperparameters additional variational parameters used constructing lower bound. approximation step inducing points approach gaussian processes. chosen family subsumes large enough information hidden process values training points successfully restored values inducing inputs evidence lower bound maximized respect variational parameters kernel hyper-parameters. using optimal distribution perform predictions data point follows lower bound however expectations one-dimensional gaussian integrals thus eﬀectively approximated range techniques. paper gauss-hermite quadratures used purpose. note lower bound form training objects. hence bound maximized using stochastic optimization techniques. paper suggests maximize lower bound respect variational parameters kernel hyper-parameters using stochastic optimization. refer method method lower bound algorithm following way. case regression lower bound analytically optimised respect variational parameters substituting optimal values back lower bound obtain lower bound marginal likelihood depends solely kernel hyper-parameters simpliﬁes optimization problem dramatically reducing number optimization parameters. unfortunately bound doesn’t form objects hence stochastic optimization methods longer applicable here. however experiments we’ve found even fairly datasets method outperforms despite lack stochastic optimization. following subsection devise approach similar method case classiﬁcation. provide tractable evidence lower bound analytically maximize respect variational parameters substituting optimal values parameters back lower bound obtain lower bound depends kernel hyper-parameters values deﬁned ﬁrst method analytical formulas recompute values gradient-based optimization maximize bound respect pseudocode given alg. refer method vi-jj stands jaakkola jordan authors note computational complexity second method uses gradient-based optimization maximize respect note method don’t recompute iteration makes methods iterations empirically faster values refer method vi-jj-full. finally vi-jj-hybrid combination methods described above. general scheme method vi-jj. vi-jj-hybrid method analytical formulas recompute vi-jj method stage stage gradient-based optimization respect virtues method described experiments section. experiments didn’t optimize lower bound respect positions inducing points. instead used k-means clustering procedure equal number inducing inputs took clusters centres also used squared exponential covariance function experiments gaussian noise term. stochastic method svi-adadelta requires user manually specify learning rate batch size optimization method. former method diﬀerent learning rates choose value resulted fastest convergence. used learning rates ﬁxed grid step always happened largest value grid method diverged smallest method converged slower medium value verifying optimal learning rate somewhere range. choose batch size used following convention. small german svmguide datasets we’ve batch size datasets used approximately batch size size training set. vi-jj vi-taylor vi-jj-hybrid experiments every iteration recomputed values times tune every iteration we’ve l-bfgs-b optimization method constrained evaluations lower bound it’s gradient. found values parameters work well datasets experimented with. svi-adadelta method used optimization w.r.t. cholesky factor matrix order maintain it’s positive deﬁniteness described used adadelta optimization method implementation climin toolbox done original paper. also preprocessed datasets normalizing features setting mean features variance datasets without available test data used data test train set. fig. provides results german svmguide datasets. small german dataset stochastic svi-adadelta method struggles takes longer achieve optimal quality methods show similar results. svmguide results magic telescope ijcnn datasets provided magic telescope dataset vi-jj vi-taylor show poor quality ﬁrst iterations still manage converge faster svi-adadelta. datasets vi-jj-hybrid method works similar vi-jj vi-taylor shows better quality ﬁrst iterations magic telescope data. vi-jj-full can’t converge reasonable quality datasets. finally results data provided rather amount inducing inputs. vi-jj-full vi-jj-hybrid fastest achieve optimal quality. svi-adadelta method also converges reasonably fast vi-jj vi-taylor struggle little bit. general vi-jj vi-taylor vi-jj-hybrid methods perform similar svi-adadelta method. dataset skin nonskin features vi-jj-hybrid little slower stochastic svi-adadelta datasets better. vi-taylor vi-jj struggle otherwise comparable vi-jj-hybrid. stochastic svi-adadelta method performs poorly small datasets even skin nonskin data doesn’t manage substantially outperform methods even provided good value learning rate. finally vi-jj-full works well small data datasets doesn’t manage achieve reasonable quality. paper presented approach training variational inducing input gaussian process classiﬁcation. derived tractable evidence lower bounds described several ways maximize them. resulting methods vi-jj vi-jj-full vi-jj-hybrid vi-taylor similar method gp-regression. provided experimental comparison methods current state-of-the-art method svi-adadelta experimental setting approach proved practical converges optimal quality fast svi-adadelta method without requiring user manually choose parameters optimization method. four described methods showed similar performance it’s hard distinguish them. however note vi-taylor approach general applied likelihood functions logistic. could also easily derive method similar vi-jj-hybrid vi-jj-full non-logistic case scope paper.", "year": 2016}