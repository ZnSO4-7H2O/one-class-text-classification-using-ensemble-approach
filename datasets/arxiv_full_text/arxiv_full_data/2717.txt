{"title": "Theoretical Foundation of Co-Training and Disagreement-Based Algorithms", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Disagreement-based approaches generate multiple classifiers and exploit the disagreement among them with unlabeled data to improve learning performance. Co-training is a representative paradigm of them, which trains two classifiers separately on two sufficient and redundant views; while for the applications where there is only one view, several successful variants of co-training with two different classifiers on single-view data instead of two views have been proposed. For these disagreement-based approaches, there are several important issues which still are unsolved, in this article we present theoretical analyses to address these issues, which provides a theoretical foundation of co-training and disagreement-based approaches.", "text": "suppose instance space label space {··· labeled data {xl+ xl+··· xl+|u|} unlabeled data. suppose labeled data independently identically come unknown distribution whose marginal distribution unlabeled data independently identically come denotes hypothesis space. suppose |hv| ﬁnite target concept perfectly consistent distribution belongs error rate disagreement hypotheses underlying distribution. reduce inﬂuence non-i.i.d. sample create small pool size drawing instances randomly large unlabeled data select conﬁdent instances label beled instance lemma process algorithm know unlabeled instance connected labeled example path form t··· tρtρ+ must exist unlabeled instances obtained label propagation graph lemma know unlabeled instance path labeled example measures insuﬃciency view correctly learning distribution examples view insufﬁciency i.e. view provides suﬃcient information correctly classify examples; examples view insuﬃciency i.e. view provides information correctly classify example. deﬁnition denote insuﬃciency view denote hypothesis space learning task view denote ﬁnite vc-dimension classiﬁcation rule induced hypothesis instance sign). error rate hypothesis distribution pxxy)∈d) maxfv∈fv optimal classiﬁers views. thus hypothesis optimal classiﬁer denote hypothesis minimizing empirical risk error rate optimal classiﬁer upper bound error rate hypothesis minimizing empirical risk i.i.d. sample size theorem suppose hypothesis space satisﬁes deﬁnition denote training i-th round algorithm denote hypotheses minimizing empirical risk deﬁnitions maxf n|l|−|l| hold monotonically decreasing function e.g. parameter probabilistic margin assumption allows small label noise examples labeled large margin. considering worst", "year": 2017}