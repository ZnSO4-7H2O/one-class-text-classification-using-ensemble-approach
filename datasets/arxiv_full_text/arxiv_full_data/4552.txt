{"title": "Gaussian Process Bandits for Tree Search: Theory and Application to  Planning in Discounted MDPs", "tag": ["cs.LG", "cs.AI"], "abstract": "We motivate and analyse a new Tree Search algorithm, GPTS, based on recent theoretical advances in the use of Gaussian Processes for Bandit problems. We consider tree paths as arms and we assume the target/reward function is drawn from a GP distribution. The posterior mean and variance, after observing data, are used to define confidence intervals for the function values, and we sequentially play arms with highest upper confidence bounds. We give an efficient implementation of GPTS and we adapt previous regret bounds by determining the decay rate of the eigenvalues of the kernel matrix on the whole set of tree paths. We consider two kernels in the feature space of binary vectors indexed by the nodes of the tree: linear and Gaussian. The regret grows in square root of the number of iterations T, up to a logarithmic factor, with a constant that improves with bigger Gaussian kernel widths. We focus on practical values of T, smaller than the number of arms. Finally, we apply GPTS to Open Loop Planning in discounted Markov Decision Processes by modelling the reward as a discounted sum of independent Gaussian Processes. We report similar regret bounds to those of the OLOP algorithm.", "text": "motivate analyse tree search algorithm gpts based recent theoretical advances gaussian processes bandit problems. consider tree paths arms assume target/reward function drawn distribution. posterior mean variance observing data used deﬁne conﬁdence intervals function values sequentially play arms highest upper conﬁdence bounds. give eﬃcient implementation gpts adapt previous regret bounds determining decay rate eigenvalues kernel matrix whole tree paths. consider kernels feature space binary vectors indexed nodes tree linear gaussian. regret grows square root number iterations logarithmic factor constant improves bigger gaussian kernel widths. focus practical values smaller number arms. finally apply gpts open loop planning discounted markov decision processes modelling reward discounted independent gaussian processes. report similar regret bounds olop algorithm. order motivate work presented here ﬁrst review problem tree search bandit-based approaches. motivate models dependencies bandit problems purpose searching trees. introduce approach based gaussian processes analyse rest paper. tree search consists looking optimal sequence nodes select starting root order maximise reward given leaf reached. introduce problem detail motivate bandit algorithms tree search review existing techniques. applications tree search important artiﬁcial intelligence games machine represents possible sequences moves tree looks ahead ﬁrst move likely yield win. rewards given monte carlo simulations randomly ﬁnish game current position return otherwise. tree search also used search optimum space sequences given length sequence labelling. generally used search topological space tree coverings deﬁned shown bubeck node corresponds region space. instance space search d-dimensional hyper-rectangle root node tree coverings whole hyper-rectangle children nodes deﬁned recursively splitting region current node region hyper-rectangle split middle along longest side. mdps agent takes sequence planning markov decision processes actions take sequence states gets rewards environment action takes aims maximising total reward. alternatively simpler objective maximise discounted rewards agent gets discount factor given beforehand weight applied reward obtained time generative model available represent possible sequences actions tree determine reward path tree idea using bandit algorithms search optimal action large state-space mdps introduced kocsis szepesv´ari also considered chang alternative costly dynamic programming approaches approximate optimal value function. challenges searching trees large branching factors computationally challenging applications game shown. requires eﬃciently select branches explore based estimated potential uncertainty estimations. similarly high depths unattainable lack computational time selection branches explore. tree search algorithm waste much time exploring sub-optimal branches still exploring enough order miss optimal. bandit algorithms used guide selection nodes exploration tree based knowledge acquired previous reward samples. however must cautious process selecting best nodes explore ﬁrst doesn’t become computationally expensive. work gelly wang search game trees bandit algorithms allow eﬃcient exploration tree compared traditional branch bound approaches bandit problem simple model trade-oﬀ exploration exploitation. multi-armed bandit analogy traditional slot machine known one-armed bandit multiple arms. stochastic bandit scenario player pulling selected ﬁnite arms receives reward. assumed reward obtained playing sample distribution unknown player samples iid. stochastic bandit problem characterised probability distributions ri≤i≤n measure performance objective player maximise collected reward iterative plays bandit. optimal selection policy i.e. policy yields maximum expected cumulative reward consists selecting argmaxi{eri} play iteration. expected cumulative reward time teri∗. performance policy assessed analysis cumulative regret time deﬁned diﬀerence expected cumulative reward time exploration exploitation good policy requires optimally balance learning distributions exploitation arms learnt high expected rewards. number arms ﬁnite smaller number experiments allowed possible explore possible options certain number times thus building empirical estimates ˆeri exploit best performing ones. number times play grows expect reward estimate improve. optimism face uncertainty popular strategy balancing exploration exploitation consists applying so-called optimism face uncertainty principle reward estimates uncertainty estimates maintained probability actual mean-reward values outside conﬁdence intervals drops quickly. played time step upper bound conﬁdence interval highest. strategy implemented algorithm shown auer achieve optimal regret growth-rate problems independent arms problem-speciﬁc upper bound problem-independent typically algorithms proceed iterations. iteration leaf node selected reward received. usually assumed exists mean-reward function noisy observation common assumptions highest value known always bigger algorithm stops convergence criterion computational/time budget exhausted maximum number iterations speciﬁed path tree given. simply path leads leaf node received highest reward. path selection sequence bandit problems algorithm developed kocsis szepesv´ari considers bandit problems node tree. children given node represent arms associated bandit problem rewards obtained selecting values obtained leaf. iteration start root select children nodes invoking bandit algorithms parents leaf reached reward received back propagated ancestors root. bandit algorithm used stands upper conﬁdence bounds implements principle optimism face uncertainty smooth trees although gelly wang reported performed well game trees shown coquelin munos behave poorly certain situations overly optimistic assumptions design upper conﬁdence bounds leading high lower bound cumulative regret. algorithm proposed bast parameterised adapt diﬀerent levels smoothness reward function leaves deal situations handles badly. bast diﬀerent deﬁnition ‘upper conﬁdence bounds’ time-independent regret upper bound derived however expressed terms sub-optimality values nodes thus problem speciﬁc. also quite paradoxically bound could become high smooth functions optimistic planning discounted mdps discount factor implies particular smoothness function tree paths starting point work bubeck munos open loop optimistic planning algorithm close spirit bast. olop proved minimax optimal logarithmic factor means upper bound growth rate simple regret matches lower bound. however olop requires knowledge time horizon regret bounds apply algorithm anytime fashion. measure performance tree search algorithm’s performance measured however although good objective achieve good exploration/exploitation balance might ultimately interested bound reward value best node would interest consider bandit problems arms possible number plays inﬁnity arms. refer manyarmed bandit problem. case need model dependencies arms order play information several arms played. show models applied online global optimisation. particular review gaussian processes modelling dependencies. bandit algorithms used focus exploration global optimisation. point space search rewards given select points want observe function. even though actual objective minimise cumulative regret minimise simple regret seen bound former give bound latter. cumulative regret also interesting forces algorithms waste samples. samples costly acquire certain applications might involve physical expensive action instance deploying sensor taking measurement particular location simply computationally costly less samples quicker maximum. modelling dependencies observations noisy. latter case bandit problem trivial search space less elements maximum number iterations perform. global optimisation search space usually continuous. case pointed wang assumption made smoothness search might arbitrarily hard. idea model dependencies arms smoothness assumptions information gained several arms playing arm. modelling dependencies also beneﬁcial problems ﬁnite numbers arms speeds learning. pandey developed algorithm exploits cluster structures among arms applied content-matching problem auer shawe-taylor kernelised version linrel ucbtype algorithm introduced auer linear optimisation analysed dani image retrieval task eye-movement feedback. linrel continuous spaces bandit problems continuous spaces studied notably kleinberg wang bubeck bandit problem corresponds mean-reward function space arms. kleinberg consider metric spaces lipschitz functions derive regret growth-rate time independent dimension input space. quite interestingly algorithm bubeck uses bast recursive splitting space node corresponds region space regions divided halves i.e. non-leaf nodes children. bast used select smaller smaller regions randomly sample algorithm developed wang ucb-air assumes probability chosen uniformly random ǫ-optimal scales thus many near-optimal arms choosing certain number arms uniformly random exists least good high probability. assumption global optimisation setting popular assumption bayesian community drawn gaussian process ﬂexibility power applicability practise engineering problems. optimisation sometimes referred kriging response surfaces probability distributions functions characterise belief smoothness functions. idea roughly similar inputs likely yield similar outputs. similarity deﬁned kernel/covariance function inputs. parameterising covariance function translates parametrisation smoothness assumption. note global smoothness assumption thus stronger bubeck like ucb-air assumption probabilistic assumption although stronger one. srinivas claim assumption neither weak strong practise. added beneﬁt bayesian framework possibility tuning parameters smoothness assumption maximising likelihood observed data written closed-form commonly used auto relevance determination kernel comparison parameter tuning critical perform well parameters need tuned hand. acquisition samples similarly bandit problems function samples acquired iteratively important ways eﬃciently focus exploration input space. acquisition function samples often based heuristics expected improvement probable improvement proved successful practise principled approach osborne considers ﬁxed number iterations fully exploits bayesian framework compute time step expected loss arms times times probability picking arms also broken recursively. similar spirit pioneering work gittins jones bandit problems dynamic allocation indices arms here computing optimal allocation samples extremely high computational cost warranted problems function samples expensive themselves. simple regret procedure analysed gr¨unewalder case observations noisy. heuristic acquiring samples approaches extended bandit setting gaussian process upper conﬁdence bound algorithm presented dorard theoretical regret bound given srinivas based rate decay eigenvalues kernel matrix kernels. seems match logarithmic factor times lower bound simple regret given gr¨unewalder lower bound cumulative regret. name gp-ucb indicates sample acquisition heuristic based optimism face uncertainty principle posterior mean variance used deﬁne conﬁdence intervals. better results bayesian acquisition criteria obtained sensor network applications presented srinivas still remains problem ﬁnding maximum upper conﬁdence function order implement algorithm brochu showed global search heuristics eﬀective. light this consider gp-based algorithm searching potentially large space tree paths ucb-type heuristic choosing arms play iteration. consider bandit problem whole tree arms tree paths kernel used algorithm therefore kernel paths deﬁned looking nodes common paths. assumption makes sense tree search similar paths nodes common expect nodes common likely similar rewards owing share information gained ‘playing’ path paths nodes common also able results srinivas derive problem-independent regret bounds practise rewards taken bandit problems convenient dealing gaussian processes output spaces centred around assume values within known interval. previously mentioned upper bound could known easy encode knowledge prior probably motivated graepel consider generalised linear model probit link function order learn click rates displayed search engines maximising number clicks assumptions similarly bast wish model diﬀerent levels smoothness response/reward function leaves/paths. this extend notion characteristic length-scale functions considering gaussian covariance function feature space paths. smoothness covariance/kernel translates quick eigenvalue decay rate used improve regret bound. already said parameter smoothness assumption learnt training data. note smoothness assumption global whereas bast assumes smoothness η-optimal nodes. examples tree search expect globally smooth planning discounted mdps even clearer deﬁned intermediate rewards thus lipschitz respect certain metric such also made smoother decreasing value discount factor finally allow model uncertainty results tight conﬁdence bounds also taken account outputting sequence actions tree search instead taking best observed action might take highest lower conﬁdence bound given threshold. main results derive regret bounds proposed gp-based tree search algorithm anytime fashion tight constants terms parameters tree search problem. regret bounded high probability gaussian kernel width regret improves kernel width increases. small constants terms size problem important since large practise computational budgets allow beyond value. upper conﬁdence function made eﬃcient tree case. theoretical analysis algorithm begins section analysis eigenvalues kernel matrix whole tree paths. followed section derivation upper bound cumulative regret tree search exploits eigenvalues’ decay rate. finally section compare algorithms namely bast tree search olop planning theoretical perspective. also show cumulative regret bound used derive regret bounds. propose ideas tree search algorithms based gaussian processes bandits bringing forward conclusions. deﬁnition probability distribution functions used formalise assumption smooth believe extension multi-variate gaussians inﬁnite number variables characterised mean function variables space think extension vector matrix inﬁnite number components. choosing inputs probability density outputs -variate gaussian covariance matrix holds extending inputs. role similarity measure arms taken covariance function specifying much outputs co-vary characterise likely think outputs ﬁnite inputs based similarities inputs thus expressing belief smoothness inference noise modelling reward observed noise which framework modelled additive gaussian white noise. variance noise characterises variability reward always playing arm. absence extra knowledge problem hand priori prior mean function model allows time receive sample probabilistic reasoning update belief come relatively close sample values time agree level smoothness dictated covariance function thus creating posterior belief. addition creating ‘statistical picture’ encoded posterior mean model gives error bars terms gives conﬁdence intervals value consider space arms kernel elements model reward playing given function drawn gaussian process zero mean covariance function arms played time rewards vector concatenated reward observations denoted posterior time seeing data mean variance algorithm plays sequence arms aims optimally balancing exploration exploitation. this select arms iteratively maximising upper conﬁdence function care much make inaccurate predictions elsewhere. term balances exploration algorithm greedy. original formula √log balance exploration exploitation choice corresponds however constants bounds optimised scaling constant speciﬁc problem hand might beneﬁcial practise. sensor network application tune scaling parameter cross validation. algorithm studied srinivas cases ﬁnite inﬁnite number arms assumption mean-reward function drawn gaussian process zero mean given covariance function agnostic setting complexity measured rkhs norm induced given kernel. work core regret bounds give section finite case analysis values within conﬁdence intervals relationship given regret algorithm information gain acquiring samples everything gaussian information gain easily written terms eigenvalues kernel matrix training arms played far. simplest case linear kernel dimensions. however general simple expression eigenvalues since know arms played. thanks result nemhauser fact information gain sub-modular function order bound information gain greedy information gain expressed terms eigenvalues kernel matrix whole arms instead kernel matrix training set. present analysis slightly detail section inﬁnite case analysis analysis requires discretise input space need additional regularity assumptions covariance function order values within conﬁdence intervals high probability. present main results srinivas needed section first show regret ucb-type algorithms bounded high probability based measure quick function learnt information theoretic sense maximum possible information gain iterations intuitively small growth rate means much information left gained time hence learn quickly result small regrets. infogain problem dependent quantity growth determined properties kernel input space. process selecting arms non-deterministic noise introduced responses. however could maybe determine probabilities arms selected analysis would problem-speciﬁc could done proof look probability select given number times selected recursion... gives problem-speciﬁc bound terms ∆i’s. following denote total number iterations performed notations algorithm cumulative regret iterations immediate regret time step number arms kernel matrix arms ai≤i≤n feature representation element feature space denote respectively information gain greedy algorithm algorithm. maximise immediate information gain time step. extended space linear combinations ai’s characterised index anymore weight vector. infogain maximisers arms maximise variance given greedy algorithm weight vector norm denote eigenvectors eigenvalues shown share eigenbasis greedy algorithm selects arms weight vectors among ﬁrst eigenvectors eigenvalue eigenvector given maximum possible infogain information-theoretic argument submodular functions gives relationship infogain gpucb algorithm time infogain greedy algorithm time based constant letting depend also provide tight bounds case improved constants. growth rate might become higher tight constants result tighter bounds. finally study constants improved smoother kernels. consider trees maximum branching factor depth announced introduction gaussian processes tree search algorithm considers tree paths arms bandit problem. number arms therefore drawing equivalent drawing n-dimensional vector values multi-variate gaussian. linear gaussian kernels linear kernel space simply counts number nodes common paths intuitively nodes common closer rewards nodes could model diﬀerent levels smoothness considering gaussian kernel feature space adapting width parameter. kernels generally could consider kernel functions characterised χ≤d≤d decreasing values value kernel product paths nodes common. χ≤d≤d chosen give explicit feature representation kernel based original feature space change components taking √χd−i χd−i+ instead node depth diﬃculty implementing algorithm maximum upper conﬁdence function computational cost exhaustive search prohibitive large number arms tree search applications. time look path maximises here beneﬁt tree structure order perform search only. ﬁrst deﬁne terminology prove result. terminology node said explored exists xii≤t training data contains said unexplored otherwise. sub-tree deﬁned nodes parent together descendants. sub-tree unexplored path training data goes sub-tree. maximum unexplored sub-tree sub-tree root belongs training data. proof procedure expressed function instead function argue paths given unexplored sub-tree value hence value. path deﬁned node explored ﬁrst nodes nodes matter kernel computations since haven’t visited. consequently need evaluate randomly chosen path goes unexplored sub-tree paths value represent maximum unexplored sub-trees dummy nodes similarly leaf nodes compute store values dummy nodes. number dummy nodes memory visited node unexplored siblings sub-tree containing unexplored siblings descendants. nodes path training data paths training data hence number dummy nodes less equal would mean number nodes examine order maximiser would search made eﬃcient examining nodes assign upper conﬁdence values recursively nodes taking maximum upper conﬁdence values children. maximiser thus found starting root selecting node highest upper conﬁdence value leaf dummy node reached. method selecting path cost only. playing would need update upper conﬁdence values leaf nodes dummy nodes method would also need update upper conﬁdence values nodes’ ancestors adding extra cost pseudo-code pseuco-code implements search argmax given algorithm sometimes talk kernel products leaves reward leaves paths identiﬁed leaf nodes. note algorithm might choose leaf node unless σnoise analysis ‘expand’ tree creating extra nodes branches branching factor construction purely theoretical algorithm doesn’t need representation whole tree expanded tree order run. write kernel matrix paths expanded tree branching factor depth matrix ones dimension completely characterise tree expressed terms expressed block matrix form kbd− jbd− blocks this must think -tree root pointing -trees. diagonal block kernel matrix paths ﬁrst -tree. kernel function normalised stays prepend nodes paths kbd−. similarly diagonal blocks kbd−. order complete block matrix representation need know paths diﬀerent -trees root common deﬁnition preliminary result eigenanalysis eigenvalues multiplicity multiplicity denote j...jb eigenvectors decreasing order corresponding eigenvalue. vector ones. coordinates notated ji...jib. deﬁne concatenation vectors sequence ﬁrst need reverse order eigenvalues thus consider sequence ¯ld−i’s. obtain lambda hats repeating lambda bars many times multiplicities. ¯ld−i bi+. hence bi+. log−r squared euclidian distance paths feature space twice number nodes diﬀer path contains nodes indexed i...id path doesn’t contain path contains nodes indexed j...jd path doesn’t contain i...id j...jd components feature vectors diﬀer. components diﬀerence feature decreases decreases. result increases increases hence csqs decreases. also since tends tends inﬁnity limit csqs tends inﬁnity. upper-bound improves linear kernel enough csqs information gain bounds turned high probability regret bounds seen section section give information gain bounds valid kernel values bound independent time interesting asymptotically. however interesting tree search problems extremely large number iterations smaller bound linear involves constants small compared interesting section derive better bounds take advantage decay kernel matrix eigenvalues. bounds combined give function linear time becomes logarithmic number iterations. bound becomes independent time. show that case gaussian kernel constants improve kernel width increases. gaussian kernel bound better linear kernel bound large enough values algorithm time could feature representation described previous section case dimension would bounded number visited nodes bounded alternatively could consider feature representations indicator vectors case dimension would bounded equates changing kernel matrix identity matrix worsen regret since arms become independent. algorithm runs linear kernel feature space consider gaussian kernel slightly diﬀerent feature space dimensionality corresponds case theorem srinivas linear kernel kernel matrix training equal denote diagonal matrix eigenvalues ¯ldim information gained training expressed terms could bound instead bounding ﬁrst occurrence constant would huge would make result less interesting could even bound independent time bounding occurrences remedy this split ˆlt>t∗ acceptable noisemtˆlt. thus consider tail-sum eigenvalues bound decreasing function bound tail-sum times tail integral quicker decreases lower integral hence lower information gain bound lower regret. gaussian kernel know greedy procedure chooses eigenvectors among highest associated eigenvalue. however might picked eigenvectors picked several times ones look smallest that increasing decrease eigenvalues tail-sum bound increase log-eigenvalues bound. conversely decreasing increases former bound decreases latter. know picked optimal cost ﬁrst would higher tail doesn’t depend kernel smoother kernel smaller using log-eigenvalues bound resulting linear information gain bound. combine bounds. ﬁrst bound becomes constant replacing second bound dictates rate growth constant otherwise. thus regret rewritten regret improve smoother kernels i.e. bigger check case bounds gave. already noted csqs decreases hence eigenvalues tail-sum bound clearly improving larger values derive log-eigenvalues bound terms coquelin munos extend deﬁnition nodes call function coincides leaf nodes which node equal maximum value tree paths maximum value also extend deﬁnition suboptimality leaf node node depth bast smoothness assumption that η-suboptimal node depth exists child local regularity assumption ancestors respectively) assumption lies high probability within ellipse determined kernel thus close values siblings thus bound terms depth high probability order give rough comparison bast smoothness assumption. although bound high probability would always hold bast makes extra assumption values would distributed. bast assumes reward leaf always given probability distribution mean equal value leaf whereas assumes reward distribution gaussian standard deviation σnoise. however srinivas also extended regret analysis general case rewards given sequence noise variables arbitrary martingale diﬀerence sequence uniformly bounded σnoise. resulting regret growth rate theorem coquelin munos gives regret bound decreases exponentially δγd. bound written terms parameters smoothness assumption independent time. however bound problembast bound interesting asymptotically number iterations tree search algorithm unlikely past interesting values issue /∆min term that smoother bigger /∆min bigger regret whereas actually would like take advantage smoothness iterative-deepening trees search usually represented memory grow iteratively adding nodes needed implementation algorithm. method growing tree iterativedeepening used tree search coulom current iteration stopped creating node reward obtained function visited path function randomly completed path length resulting tree asymmetric contains paths diﬀerent numbers nodes. hopefully helps deeper tree regions high values keeps paths short rest tree. saves time memory stopping exploration depth smaller creating nodes would belong sub-optimal paths. depth-ﬁrst consider tree paths arms bandit problem gpts need paths length results depth-ﬁrst tree growing method. depth-ﬁrst means that iteration nodes sequentially reaching maximum depth start another iteration tree search algorithm root. unlike gpts bast either iterative-deepening depth-ﬁrst mode. supposedly algorithm eﬃcient iterative-deepening version regret bound given version. compare gpts open loop optimistic planning algorithm. tree search applied planning mdps reward discounted intermediate rewards. bubeck munos assume intermediate rewards bounded model belief expect intermediate reward functions considering node sequence actions explored random variables intermediate reward function values possible actions node realisation this. assume random variables follows normalised gaussian distribution independent. determine tree paths kernel function follows assumption. path list nodes root corresponding list indices ...id actions taken environment. belief function value path represented paths action indices common represented ...ih ...id ...ih i′h+ ...i′d. kernel product problem-speciﬁc bound problem-independent bound used bilinearity covariance independence random variables fact variances always characterises belief discounted rewards note kernel normalised reﬂects fact signal variance higher deeper trees. although olop bast similar spirit olop exploits fact globally smooth owing discount factor again smoothness assumption weaker sense intermediate rewards bounded stronger since make assumption distributed. however previous studies bayesian optimisation gives reasons think reasonable practise. measure performance planning algorithm obtain bound dividing cumulative regret bound since algorithm outputs best observed path might actually interesting give bound empirical simple regret i.e. |f∗− ybest|. relationship empirical simple regret cumulative regret given high probability. first deﬁne empirical cumulative regret coquelin munos give relationship cumulative cumulative regret considered bubeck munos measured function number calls generative model equal immediate regret given policy deﬁned diﬀerence inﬁnite discounted rewards sequence nodes chosen optimal policy sequence nodes given following policy actions switching optimal policy consider path exploration. write node diﬀerent node following policy actions. would optimal policy. reason n∗d+ available implies sequences nodes follow diﬀerent even though simple regret. clearly ﬁxing implies simple regret poor choice. necessary deeper tree number iterations ﬁxed advance increases. olop builds tree depth depends gpts also used similar fashion ﬁxing choosing function makes depend bounded constant anymore. adapt previous results determining current kernel. paths diﬀer nodes action indices common. mentioned introduction could conﬁdence intervals built change output algorithm instead outputting best observed path could output path highest lower conﬁdence bound instance. outputting optimal action take root planning case could output action highest estimated reward long term. variants might robust variability rewards regret bounds them. conﬁdence intervals also used determine stopping criterion e.g. stop width conﬁdence interval best observed action smaller certain threshold. automatically performance guarantee algorithm guarantee runtime. gpts manner similar maximum function space given tree coverings. leaf node tree corresponds region search space learning average values regions. search space cartesian product arbitrary family discrete continuous sets. hierarchical optimistic optimisation advantage choice point interesting lines research upper conﬁdence-type tree search algorithms generalise nodes tree according gelly domain knowledge incorporated encoding heuristics prior mean also labelling nodes incorporating kernel nodes kernel tree paths. tree search searching game trees could simply label nodes corresponding boards. would kernel boards applied leaves paths. nodes would selected sequence starting root computing upper conﬁdence values possible next board order select next node. instance algorithm used time need search optimal move knowledge gained algorithm transferred game other. trees labelled nodes nodes labelled feature vectors natural kernel sequences nodes would product gaussian kernels feature vectors depth creating feature representation path concatenating feature vectors nodes taking gaussian kernel paths feature space. write kernel matrix children node method eigenanalysis could adapted ﬁrst writing block matrix coeﬃcients taken here takes role takes role however regard implementation algorithm clear search maximum upper conﬁdence function would performed. planning mdps generative models available could learn immediate reward functions node functions children’s labels. would instance node exploration path would train instance node along path corresponding immediate reward observed selection nodes would performed similar using sequence ucb-type bandit algorithms. simple regret bounds node immediate mean-reward function node take advantage spectral properties combine simple regret bounds whole paths discounted immediate mean-rewards. finally work might extended closed-loop planning communicating mdps deterministic transitions considering cycles graph states instead paths tree given depth cycles length smaller diameter ﬁnite communicating mdps. closed-loop planning diﬀers open-loop fact chosen actions depend current states time. generative model available would directly interact environment would therefore cumulative regret measure performance since every interaction environment would cost. dependencies actions inference could used derive tighter upper conﬁdence bounds used algorithm ucycle paper presented bandit-based tree search algorithm makes gaussian processes framework model reward function leaves. resulting assumption smoothness function easily conﬁgurable covariance functions parameters learnt maximum likelihood known advance. analysed regret algorithm provided problemindependent bounds tight constants expressed terms χd≤d≤d parameters covariance function paths. comparing algorithms shown particular gpts applied planning mdps achieves regret growth rate recent olop algorithm. believe analysis presented paper provide groundwork studying theoretical properties extensions mentioned previously. also possible extend results agnostic setting function ﬁnite norm given rkhs using another bound derived srinivas whether optimal. also interest derive bounds noisy observations analyse number times play sub-optimal arms order problem-speciﬁc bounds finally complement theoretical analysis algorithm investigate performance practical tree search problems. particular would certainly interesting kernel boards could beneﬁcial compared techniques used uct-rave.", "year": 2010}