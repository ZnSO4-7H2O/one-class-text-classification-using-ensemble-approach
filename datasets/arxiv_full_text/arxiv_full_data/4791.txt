{"title": "Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive  Transfer from multiple sources in the same domain", "tag": ["cs.AI", "cs.LG"], "abstract": "Transferring knowledge from prior source tasks in solving a new target task can be useful in several learning applications. The application of transfer poses two serious challenges which have not been adequately addressed. First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of helping it. Second, the agent should be able to selectively transfer, which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task. We propose A2T (Attend, Adapt and Transfer), an attentive deep architecture which adapts and transfers from these source tasks. Our model is generic enough to effect transfer of either policies or value functions. Empirical evaluations on different learning algorithms show that A2T is an effective architecture for transfer by being able to avoid negative transfer while transferring selectively from multiple source tasks in the same domain.", "text": "transferring knowledge prior source tasks solving target task useful several learning applications. application transfer poses serious challenges adequately addressed. first agent able avoid negative transfer happens transfer hampers slows learning instead helping second agent able selectively transfer ability select transfer different multiple source tasks different parts state space target task. propose attentive deep architecture adapts transfers source tasks. model generic enough effect transfer either policies value functions. empirical evaluations different learning algorithms show effective architecture transfer able avoid negative transfer transferring selectively multiple source tasks domain. goals artiﬁcial intelligence build autonomous agents learn adapt environments. reinforcement learning technique achieving adaptability. goal algorithms learn optimal policy choosing actions maximize notion long term performance. transferring knowledge gained tasks solved earlier solve target task help either terms speeding learning process terms achieving better solution among performance measures. applied transfer could accomplished many ways good survey ﬁeld). could value function source task initial estimate target task exploration alternatively could policies source task target task. take forms derived policies used initial exploratory trajectories niekum target task derived policy could used deﬁne macro-actions used agent solving target task brunskill transfer much explored crucial issues adequately addressed literature. ﬁrst negative transfer occurs transfer results performance worse compared learning scratch target task. severely limits applicability many transfer techniques cases measure relatedness source target tasks guaranteed beforehand. brings second problem transfer issue identifying appropriate source task transfer. scenarios different source tasks might relevant useful different parts state space target task. real world analogy consider multiple players good different aspects game example player good playing backhand shots player good playing forehand shots. consider case player wants learn tennis selectively learning experts. handle situation architecture allowing agent learn pick solutions multiple different source tasks solving target task selectively applicable different parts state space. call selective transfer. agent transfer knowledge player required play backhand shots player playing forehand shots. further consider consider situation player player playing drop shots. apart source tasks maintain base network learns scratch target task. agent pick solution base network solving target task parts state space transferring source tasks negative. situation could arise source task solutions irrelevant solving target task speciﬁc portion state space transferring source tasks negative speciﬁc portion state space situation also entails ﬁrst problem avoiding negative transfer. framework allows agent avoid transferring players learning play drop shots rather acquire drop shot skill learning base network. architecture trained base network uses experience obtained usage solutions target task overall experience acquired using combined knowledge source tasks itself. enables base network solutions closer behavior overall architecture makes easier base network assist architecture tune useful source task solutions suit target task perfectly time. contribution architecture deep attention network decides solutions attend given input state. network learns solutions function current state thereby aiding agent adopting different solutions different parts state space target task. propose attend adapt transfer attentive deep architecture adaptive transfer avoids negative transfer performing selective transfer multiple source tasks domain. addition tennis example fairly generic framework used selectively transfer different skills available different experts appropriate situation. instance household robot appropriately skills different experts different household chores. would require skill transfer manipulation skills across objects tasks robotic actuators. well developed attention mechanism appropriate helpful combination object-skill-controller identiﬁed aiding learning related task. further generic enough effect transfer either action policies actionvalue functions case also adapt different algorithms reinforcement learning appropriate different settings empirically demonstrate effective transfer learning setting. mentioned earlier transfer learning approaches could deal transferring policies value functions. example banerjee stone describe method transferring value functions constructing game tree. similarly sorg singh value function source task initial estimate value function target task. another method achieve transfer reuse policies derived source task target task. probabilistic policy reuse discussed fern´andez veloso maintains library policies selects policy based similarity metric random policy max-policy knowledge obtained. different proposed approach proposed approach transfer policies granularity individual states possible policy-reuse rendering unable learn customized policy granularity.atkeson schaal niekum evaluated idea transferred policy source tasks explorative policies instead random exploration policy. provides better exploration behavior provided tasks similar. talvitie singh promising policy candidate policies generated using different action mapping single solved task. contrast make source tasks selectively transfer policies granularity state. apart policy transfer value transfer discussed above ferguson mahadevan discuss representation transfer using proto value functions. idea negative selective transfer discussed earlier literature. example lazaric restelli address issue negative transfer transferring samples related task multi-task setting. konidaris discuss idea exploiting shared common features across related tasks. learn shaping function used later tasks. recent works relevant proposed architecture discussed parisotto rusu parisotto explore transfer learning across atari games trying learn multi-task network source tasks available directly ﬁnetune learned multi-task network target task. however ﬁne-tuning transfer paradigm cannot address issue negative transfer observe many experiments. rusu address negative transfer issue proposing sequential learning mechanism ﬁlters network learned ongoing task dependent lateral connections lower level ﬁlters networks learned already previous tasks. idea ensure dependencies characterize similarity across tasks could learned lateral connections. even though observe better transfer results direct ﬁne-tuning still able avoid negative transfer experiments. source tasks solutions source tasks respectively. solution learn target task source tasks refer tasks already learnt perform target task refers task interested learning now. solutions could example policies state-action values. source tasks domain target task state action spaces. propose setting learned function solution base network starts learning scratch acting target task. work convex combination solutions obtain weight given solution state agent uses target task. figure shows proposed architecture. source task solutions remain ﬁxed base network solutions learnt hence change time. central network learns weights given input state refer network attention network. weights determine attention solution gets allowing agent selectively accept reject different solutions depending input state. adopt soft-attention mechanism whereby weight non-zero opposed hard-attention mechanism forced non-zero weight. here deep neural network could consist convolution layers fully connected layers depending representation input. parametrised takes input state outputs vector length gives attention scores solutions state normalises score weights follow eq.. source task solution useful state high value attention network. working granularity states allows attention network attend different source tasks different parts state space target task thus giving ability perform selective transfer. parts state space target task source task solutions cause negative transfer source task solutions relevant attention network learns give high weight base network solution thus avoiding negative transfer. depending feedback obtained environment upon following attention network’s parameters updated improve performance. mentioned earlier source task solutions remain ﬁxed. updating source task’s parameters would cause signiﬁcant amount unlearning source tasks solutions result weaker transfer observed empirically. also enables source task solutions long outputs alone irrespective come from. even though agent follows update parameters base network produces action taken agent based special updating apart experience unique individual contribution parts state space source task solutions relevant also uses valuable experience using uses solutions source tasks well. also means that source task whose solution useful target task parts state space tries replicate parts state space. practise source task solutions though useful might need modiﬁed suit perfectly target task. base network takes care modiﬁcations required make useful source task solutions perfect target task. special training base network assists architecture achieving faster. note agent could follow/use even attain replication corresponding parts state space. allows good performance agent earlier stages training itself useful source task available identiﬁed. since attention soft model ﬂexibility combine multiple solutions. deep neural networks allow model work even large complex problems. deep attention network allows agent learn complex selection functions without worrying representation issues priori. summarise given state learns attend speciﬁc solutions adapts attention different states hence attaining useful transfer. general used transfer solutions policy value. solutions transfer source task policies taking advantage which learn policy target task. thus represents stochastic policy probability distribution actions. agent acts target task sampling actions probability distribution target task policy described eq.. attention network produces weights different solutions trained feedback taking action following base network produces trained sampled action came implications discussed previous section. attention network’s weight policy high mixture policy dominated base network learning nearly on-policy. cases undergoes off-policy learning. look closely even latter case since moves towards tries nearly on-policy time. empirically observe converges. architecture policy transfer used alongside algorithm explicit representation policy. describe instantiations policy transfer direct policy search using reinforce algorithm another actor-critic setup. reinforce algorithms used direct policy search making weight adjustments direction lies along gradient expected reinforcement. full architecture shown fig.a direct policy search parameters updated using reinforce. attention network parametrized base network outputs parametrized updates given non-negative factors return obtained episode baseline length episode. action sampled agent state following note used update attention network used update base network. actor-critic methods temporal difference methods separate components viz. actor critic. actor proposes policy whereas critic estimates value function critique actor’s policy. updates actor happens td-error step estimation error helps reinforcing agent’s behaviour. actor part actor-critic. architecture shown fig.b. actor aware previous learnt tasks tries solution policies beneﬁt. critic evaluates action selection basis performance target task. notations reinforce action dictated lead agent next state reward represent value state discount factor. then update equations actor below case solutions transferred source tasks’ action-value functions call functions. thus represent discrete action space tasks agent acts using target task described eq.. attention network base network updated described architecture. state-action value function used guide agent selecting optimal action state measure long-term return obtained taking action state learn optimal policies agent estimate optimal task. q-learning off-policy temporal difference learning algorithm q-values updated iteratively bellman optimality equation rewards obtained task below high dimensional state spaces infeasible update q-value possible state-action pairs. address issue approximating parametrized function approximator qthereby generalizing states actions operating higher level features approximates q-value function deep neural network able predict actions states here represents expected error corresponding current parameter estimate represents parameters separate target network represents parameters online network. usage target network improve stability learning updates. gradient descent step shown below avoid correlated updates learning transitions current network simulates experience replay used experiences pooled fifo fashion. learn experts source tasks. q-learning used ensure driven good estimate functions target task. taking advantage offpolicy nature q-learning learned experiences gathered \u0001-greedy behavioral policy based attention network outputs parametrised represent base network outputting parametrised parameters respective target networks. note usage target signify parameters used calculate target value q-learning update different usage context target task. update equations updated gradients using rmsprop. note q-learning updates attention network base network target value generated target networks stabilize updates reduce nonstationarity training. parameters target networks periodically updated online networks. evaluate performance architecture policy transfer using simulated worlds viz. chain world puddle world described below. main goal experiments test consistency results algorithm motivation. chain world figure shows chain world goal agent point chain another point least number steps. state agent choose either move position left right. reaching goal state agent gets reward inversely proportional number steps taken reach goal. puddle worlds figures show discrete version standard puddle world widely used reinforcement learning literature. world goal agent speciﬁed start position goal position maximising return. state agent choose four actions move position north south east west.with probability agent moves chosen direction probability moves random direction irrespective choice action. reaching goal state agent gets reward reaching parts grid agent gets different penalties mentioned legend ﬁgures. evaluate performance architecture value transfer using arcade learning environment platform atari provides simulator atari games. commonly used benchmark tasks deep reinforcement learning algorithms mnih parisotto rusu perform adaptive transfer learning experiments atari game pong. section consider case multiple partially favorable source tasks available assist learning process different parts state space target task. objective ﬁrst show effectiveness attention network learning focus source task relevant state agent encounters trying complete target task evaluating full architecture additional randomly initialised base network. illustrated policy transfer setting using chain world shown consider target task start uniform probability reach least number steps. consider learned source tasks viz. available. source task agent learned reach left starting right contrast source task agent learned reach right starting left intuitively clear target task beneﬁt policies learnt tasks learn solve task using reinforce given policies learned figure shows weights given attention network source task policies different parts state space learning. observe attention network learned ignore left right half state space target task respectively. next base network evaluate full architecture task. figure shows weights given attention network different source policies different parts state space learning. observe attention network learned ignore left right half state space target task respectively. base network replicates time high weight throughout state space target task. also evaluate architecture relatively complex puddle world shown figure case task moving task moving target task agent learn move starting either chosen uniform probability. learn task using actor-critic method following available learned policy learned policy randomly initialized policy network figure shows performance results. observe actor-critic using able policies learned performs better network learning scratch without knowledge source tasks. similar evaluation attention network followed full architecture value transfer well. create partially useful source tasks modiﬁcation atari game pong. take inspiration real world scenario sport tennis could imagine different right-handed players ﬁrst expert player forehand weak backhand second expert player backhand weak forehand. someone learning play tennis style experts easy follow forehand expert player whenever receives ball forehand follow backhand expert whenever receives ball backhand. simulate scenario pong. trick blur part screen want force agent weak returning ball. blurring black pixels speciﬁc region required. make sure blurring doesn’t contrast background modify pong played black background instead existing gray construct partially helpful source task experts constructed figure visualisation attention weights selective transfer attention network experiment green blue bars signify attention probabilities expert- expert respectively. ﬁrst snapshots ball lower quadrant expected attention high expert- third fourth snapshots ball bounces back upper quadrant attention increases expert-. training pong upper quadrant blurred constructed training lower quadrant blurred. essentially results ball invisible upper quadrant lower quadrant therefore expect useful guiding return balls lower quadrant upper quadrant. goal attention network learn suitable ﬁlters parameters focus correct source task speciﬁc situation game. source task experts scored average respectively pong game play black background. attention network suitably weigh value functions average performance recorded single epoch training. clearly shows attention mechanism learned take advantage experts adaptively. fig. shows visualisation attention weights same. evaluate full architecture setting addition learning scratch setting. architecture take advantage knowledge source task experts selectively early training using expertise base network wherever required perform well target task. figure summarizes results clear learning partially useful experts better learning turn better learning scratch without additional knowledge. ﬁrst consider case learned source task available solution hamper learning process target task. refer source task unfavorable source task. scenario attention network shown figure learn assign weight also consider modiﬁcation setting adding another source task whose solution favorable target task. scenario attention network learn assign high weight ignoring deﬁne experiment using puddle world figure policy transfer. target task experiment maximize return reaching goal state starting states artiﬁcially construct unfavorable source task ﬁrst learning solve task negating weights topmost layer actor network. favorable task setting. artiﬁcially construct favorable source task figure avoiding negative transfer transferring value favorable task. speciﬁc training architecture details mentioned appendix. plots averaged runs different random seeds. simply learning solve target task using learned actor network. figure shows results. target task value transfer experiment reach expert level performance pong. construct kinds unfavorable source tasks experiment. inverse-pong pong trained negated reward functions reward provided emulator choosing action state freeway expert another atari game freeway range optimal value functions action space pong. empirically veriﬁed freeway expert leads negative transfer directly initialized ﬁne-tuned pong makes good proxy negative source task expert even though target task pong different state space. artiﬁcially construct favorable source task learning achieve expertise target task learned network. figure compares performance various scenarios unfavorable source task inverse-pong figure offers similar comparison negative expert freeway. results clearly hampered unfavorable source task learning ignore performs competitively randomly initialized learning target task without expert available. secondly presence additional source task favorable learns transfer useful knowledge ignoring unfavorable task thereby reaching expertise target task much faster scenarios. present evolution attention weights experiment described section focus efﬁcacy framework providing agent ability avoid negative transfer transfer favorable source task figure depicts evolution attention weights training framework. corresponding experiment case target task solve pong source task experts perfect pong playing trained inverse-pong trained negated reward functions additionally there’s also base network learns scratch using experience gathered attentively combined behavioral policy expert networks base network itself. train framework epochs plot illustrates attention weights every second epoch. clearly ﬁgure weird co-adaptation happens training attention negative expert uniformly throughout. initially framework needs collect level experience ﬁgure positive expert optimal till then attention mostly base network learning scratch. attention shifts positive expert turn provides rewarding episodes transition tuples learn from. finally attention drifts slowly base network positive expert again attention roughly random choosing between execution positive expert base network. base network acquired sufﬁcient expertise positive expert happens optimal target task. visualization clearly shows powerful framework ignoring negative expert throughout using positive expert appropriately learn quickly experience gathered acquire sufﬁcient expertise target task. experiments previous subsection dealing prevention negative transfer using favorable source task consider positive expert perfect expert task treat target task. raises question relying presence perfect expert positive expert. situation obvious solution execute experts target task vote probabilities proportional average performance each. framework however generic intended source task selection. illustrate additional baseline experiment positive source task imperfect expert target task. case weighted average voting among available source task networks based individual average rewards upper bounded performance best available positive expert happens imperfect expert target task. rather base network acquire skills present source task networks. choose partially trained network pong scores average graph ﬁgure clearly shows framework partial pong expert negative expert performs better learning scratch negative expert performs worse perfect positive expert negative expert. expected partial expert cannot provide much expert knowledge perfect expert still provides useful knowledge speeding process solving target task. important conclusion experiment framework capable discovering skills available among experts skills required optimally solving target task. maintain consistency perform number runs averaging scores experimented learning rates pick better performing paper present general deep neural network architecture transfer learning avoids negative transfer enabling selective transfer multiple source tasks domain. show simple ways using policy transfer value transfer. empirically evaluate performance different algorithms using simulated worlds games show indeed achieves stated goals. apart transferring task solutions also used transferring useful knowledge model world. work focused transfer tasks share state action spaces domain deep networks opens possibility going beyond setting. example deep neural network used learn common representations multiple tasks thereby enabling transfer related tasks could possibly different state-action spaces. hierarchical attention lower level ﬁlters across source task networks learning ﬁlters target task network another natural extension transfer across tasks different state-action spaces. setup progressive neural networks could borrowed ﬁlter transfer setup retained policy/value transfer. exploring setting continuous control tasks transfer modular controllers well avoid negative transfer also potential direction future research. nature tasks considered experiments naturally connected hierarchical reinforcement learning continual learning. instance blurring experiments inspired tennis based experts speciﬁc skills like forehand backhand could considered learning sub-goals like forehand backhand solve complex broader task like tennis invoking relevant sub-goals structure could useful build household robot general purpose navigation manipulation whereby speciﬁc skills manipulation different objects navigating across different source-destination points could invoked necessary. attention network framework essentially soft meta-controller hence presents powerful differentiable tool continual meta learning. meta-controllers typically designed discrete decision structure high level subgoals. paper presents alternate differentiable meta-controller soft-attention scheme. believe aspect exploited differentiable meta-learning architectures hierarchical reinforcement learning. believe novel approach different problems like transfer learning meta-learning hierarchical reinforcement learning reﬁnements design good direction explore. thanks anonymous reviewers iclr provided thoughtful remarks helped revise paper. would also like thank sherjil ozair john schulman yoshua bengio sarath chandar caglar gulchere charu chauhan useful feedback work. kimberly ferguson sridhar mahadevan. proto-transfer learning markov decision processes using spectral methods. computer science department faculty publication series fernando fern´andez manuela veloso. probabilistic policy reuse reinforcement learning agent. proceedings ﬁfth international joint conference autonomous agents multiagent systems shie mannor ishai menache amit hoze klein. dynamic abstraction reinforcement learning clustering. proceedings twenty-ﬁrst international conference machine learning volodymyr mnih koray kavukcuoglu david silver alex graves ioannis antonoglou daan wierstra martin riedmiller. playing atari deep reinforcement learning. arxiv preprint arxiv. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. scott niekum sachin chitta andrew barto bhaskara marthi sarah osentoski. incremental semantically grounded learning demonstration. robotics science systems volume andrei rusu neil rabinowitz guillaume desjardins hubert soyer james kirkpatrick koray kavukcuoglu razvan pascanu raia hadsell. progressive neural networks. corr abs/. jonathan sorg satinder singh. transfer soft homomorphisms. proceedings international conference autonomous agents multiagent systems-volume international foundation autonomous agents multiagent systems erik talvitie satinder singh. experts algorithm transfer learning. proceedings international joint conference artiﬁcal intelligence morgan kaufmann publishers inc. source task expert dqns architecture input convolution ﬁlters dimensions stride followed convolution ﬁlters dimensions stride followed convolution ﬁlters size stride followed fully connected layer units ﬁnally fully connected output layer many units number actions pong relu nonlinearity hidden layers. speciﬁcally nips architecture mnih takes batch inputs followed convolution ﬁlters dimensions stride convolution ﬁlters dimensions stride fully connected hidden layer units followed output layer. selective transfer blurring experiments described section second option above. experiments section additional experiments appendix ﬁrst option. attention network outputs number source tasks. experiments value transfer used rmsprop updating gradient. policy transfer since tasks simple stochastic gradient descent sufﬁcient provide stable updates. also reward clipping target networks experience replay value transfer experiments exactly training epoch frames training epoch evaluate networks testing epoch lasts frames. report average score completed episodes testing epoch. average scores obtained averaged runs different random seeds. testing epochs \u0001-greedy policy. experiments trained architecture using learning rates general lower learning rate provided stable training curves. comparing across algorithms picked best performing learning rate training curve. experts trained blurring black background illustrated appendix therefore compare learning random network without additional knowledge baseline pong black background too. black background provides rich contrast white ball black background thereby making training easier faster performance curves setting different settings reported inverse pong freeway negative transfer experiments blacking done pong played gray background. blurring mechanism pong illustrated appendix figure ﬁgures explain blurring mechanism selective transfer experiments pong. background screen made black. denote array containing pixels screen. paddle controlled agent right. focus quadrants pong screen relevant agent controlled paddle. simulate expert weak returning balls upper quadrant portion till horizontal location agent-paddle blacked similarly simulating weakness bottom quadrant blur portion till agentpaddle’s horizontal location figures illustrate scenarios blurring upper quadrant blurring; similarly blurring lower quadrant. effectively blurring black screen equivalent hiding ball appropriate quadrant weakness simulated. hence figures mechanisms used training pong hide ball respective quadrants create partially useful experts analogous forehand-backhand experts tennis. indicates subarray rows upto index columns upto column index similar blurring experiment pong additionally another experiment atari game breakout validate efﬁciency attention mechanism. consider setup experts along attention network. experts trained blurring lower left right quadrants breakout screen respectively. don’t make background black like case pong background already black breakout direct blurring sufﬁcient hiding ball respective regions without contrasts introduced. blur lower part make easy agent least anticipate ball based movement top. empirically observed blurring half makes hard learn meaningful partially useful experts goal experiment show attention network learn suitable ﬁlters dynamically adapt learn select expert appropriate situation task. expert blurred left bottom half bound weak returning balls region expected weak right. vein forehandbackhand example tennis synthetic simulation pong blurring upper lower quadrants. game play attention mechanism expected ignore ball bottom right half similarly ignore ball left bottom half. learn experts score respectively. using attention mechanism select correct expert able achieve score training epochs. training epoch corresponds decision steps scores averaged completed episodes decision steps. shows attention mechanism learns select suitable expert. though performance limited weaknesses respective experts goal show attention paradigm able take advantage experts appropriately. evident scores achieved standalone experts attention mechanism. additionally also present visualization attention mechanism weights assigned experts game play appendix weights assigned agreement expect terms selective attention. blurring mechanism visually illustrated appendix figure ﬁgures explain blurring mechanism used selective transfer experiments breakout. background screen already black. denote array containing pixels screen. focus quadrants perform blurring case ensuring pixels within training respectively. effectively equivalent hiding ball appropriate quadrants. blurring simulates weakness lower left quadrant blurring simulates weakness lower right quadrant. don’t blur upto last ensure paddle controlled agent visible screen. also don’t black rectangular border width pixels surrounding screen. figures illustrate scenarios blurring lower left quadrant blurring; similarly blurring lower right quadrant. figure visualisation attention weights selective transfer attention breakout green blue bars signify attention probabilities expert- expert- respectively scale ﬁrst snapshots ball lower right quadrant expected attention high expert- third fourth snapshots ball lower right quadrant hence attention high expert-. figure experiment case study target task performance limited data availability. focused experiments target task solve pong value transfer puddle worlds policy transfer. cases randomly initialized value network learning without expert network able solve target task within reasonable number epochs want illustrate case solving target task reasonable time hard presence favorable source task signiﬁcantly impacts speed learning. consider variant pong target task. variant small probability transition tuples non-zero reward added replay memory performance target task limited availability rewarding transitions replay memory. synthetically makes target task pong sparse reward problem replay memory largely ﬁlled transition tuples zero reward. prioritized sampling make sure sparsity negative effect learning solve target task. version pong black background faster experimentation. used plots illustrated above. figure clearly shows difference normal pong task without synthetic sparsity variant introduce. learning much slower clearly limited data availability even epochs reward sparsity. figure describes comparison setting positive expert expertly solves target task negative expert learning scratch direct ﬁne-tuning negative expert. clearly effect positive expert source tasks speeding learning process signiﬁcantly compared learning scratch also ﬁne-tuning negative expert severely limits learning even epochs training. also framework powerful work sparse reward settings avoids negative transfer even cases also clearly learning beneﬁt presence target task expert among source task networks. importantly experiment demonstrates transfer learning signiﬁcant effect tasks hard without expert available. further also beneﬁcial situations accessing weights expert network possible outputs expert used. synthetic sparse variants existing tasks good explore future directions intersection inverse reinforcement learning reward-based learning providing viable framework off-policy on-policy learning.", "year": 2015}