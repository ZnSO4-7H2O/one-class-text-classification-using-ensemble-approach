{"title": "Item Recommendation with Continuous Experience Evolution of Users using  Brownian Motion", "tag": ["cs.AI", "cs.CL", "cs.IR", "cs.SI", "stat.ML"], "abstract": "Online review communities are dynamic as users join and leave, adopt new vocabulary, and adapt to evolving trends. Recent work has shown that recommender systems benefit from explicit consideration of user experience. However, prior work assumes a fixed number of discrete experience levels, whereas in reality users gain experience and mature continuously over time. This paper presents a new model that captures the continuous evolution of user experience, and the resulting language model in reviews and other posts. Our model is unsupervised and combines principles of Geometric Brownian Motion, Brownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal progression of user experience and language model respectively. We develop practical algorithms for estimating the model parameters from data and for inference with our model (e.g., to recommend items). Extensive experiments with five real-world datasets show that our model not only fits data better than discrete-model baselines, but also outperforms state-of-the-art methods for predicting item ratings.", "text": "example experienced users often appreciate certain facets item differently novices amateurs. users gain experience mature later appreciate intricate facets. consider following reviews christopher nolan movies. facet interest narrative style. user memento story become interesting told ﬁrst user appreciate complex narratives. second user prefers simpler blockbusters. third user seems appreciate non-linear narration style inception memento. terms maturity would consider user experienced underlying facet assessment generating future recommendations similar users. state-of-the-art limitations evolution user experience affects ratings ﬁrst studied however works make simplifying assumption user experience categorical discrete levels users progress level next discrete manner. artifact assumption experience level user changes abruptly transition. also undesirable consequence discrete model users level experience treated similarly although maturity could still apart therefore assumption exchangeability reviews latent factor model users level experience hold language model changes. prior work assumes user activity play major role experience evolution biases model towards highly active users contrast prior work captures interpretable evidence user’s experience level using vocabulary cast language model latent facets. however approach also exhibits drawbacks discrete levels experience discussed above. current paper overcomes limitations modeling evolution user experience corresponding language model continuous-time stochastic process. model time explicitly work contrast prior works. prior work item recommendation considered review texts sole perspective learning topical similarities static snapshot-oriented manner without considering time all. abstract online review communities dynamic users join leave adopt vocabulary adapt evolving trends. recent work shown recommender systems beneﬁt explicit consideration user experience. however prior work assumes ﬁxed number discrete experience levels whereas reality users gain experience mature continuously time. paper presents model captures continuous evolution user experience resulting language model reviews posts. model unsupervised combines principles geometric brownian motion brownian motion latent dirichlet allocation trace smooth temporal progression user experience language model respectively. develop practical algorithms estimating model parameters data inference model extensive experiments real-world datasets show model data better discrete-model baselines also outperforms state-of-the-art methods predicting item ratings. concepts information systems recommender systems; mathematics computing dimensionality reduction; human-centered computing collaborative ﬁltering; social recommendation; keywords review community; user experience; language evolution; recommendation; topic modeling; brownian motion motivation review communities items like movies cameras restaurants beer newspapers asset recommender systems. state-of-the-art methods harness different signals predictions user-user item-item similarities addition user-item ratings. typically cast latent factor models exploit user-user interactions user bias bursty posting behavior community-level features none methods however consider role user experience evolution users mature time. dimensions recognized investigated incorporated recommender models recently permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights components work owned others must honored. abstracting credit permitted. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. request permissions permissionsacm.org. august francisco acm. isbn ----//. http//dx.doi.org/./. work develop continuous-time model user experience language evolution. unlike prior work rely explicit features like ratings number reviews. instead capture user’s experience latent language model learned user-speciﬁc vocabulary review texts. present generative model user’s experience language model evolve according geometric brownian motion brownian motion process respectively. analysis trajectory users offer interesting insights; instance users reach high level experience progress faster also exhibit comparatively higher variance. also number reviews written user strong inﬂuence unless written long period time. facets model generated using latent dirichlet allocation. user experience item facets latent variables whereas observables words explicit timepoints user reviews. parameter estimation inference model challenging since combine discrete multinomial distributions continuous brownian motion process language models’ evolution continuous geometric brownian motion process user experience. contributions solve technical challenge present inference method consisting three steps estimation user experience user-speciﬁc using metropolis hastings algorithm estimation language model evolution kalman filter estimation latent facets using gibbs sampling. experiments real-life data different communities movies food beer news media show three components coherently work together yield better data previously best models discrete experience levels. also achieve improvement mean squared error predicting user-speciﬁc ratings items compared baseline finally present use-case study news-media community experienceaware models used identify experienced citizen journalists method performs well capturing user maturity. summary contributions model devise probabilistic model tracing continuous evolution user experience combined language model facets explicitly captures smooth evolution time. experiments perform extensive experiments realword datasets together comprising million ratings million users million items demonstrate substantial improvements method state-of-the-art baselines. rest paper organized follows. section introduces fundamental components modeling continuous evolution user experience language. section presents generative model joint evolution facets experience language also inference methods based markov chain monte carlo sampling. section shows experimental results comparing model variety baselines. section discusses use-case study followed related prior work. review next. contrast current paper models time explicitly allows experience continuously evolve time able trace joint evolution experience vocabulary. challenging discrete multinomial distribution based language model needs combined continuous stochastic process experience evolution. levels temporal granularity. since experience naturally continuous beneﬁcial model evolution resolution hand language model much coarser granularity show section smoothly merge granularities using continuous-time models. model language evolution motivated seminal work wang blei major differences extensions. following subsections formally introduce components affected time experience evolution language model evolution. continuous experience evolution prior works model experience discrete random variable timepoint user allowed stay level move level result transition abrupt user switches levels. also model distinguish users level experience even though experience quite apart instance figure language model uses parameters long user stays level although language model changes. order address issues goal develop continuous experience evolution model following requirements experience value always positive. markovian assumption continuous-time process experience value time depends value recent observed time prior drift overall trend increase time. volatility evolution smooth occasional volatility. instance experienced user write series expert reviews followed sloppy one. natural continuous state alternative discrete-state space based hidden markov model used previous work figure shows real-world example evolution experienced amateur user beeradvocate community traced proposed model along discrete counterpart previous work. stochastic process used model population growth ﬁnancial processes like stock price behavior random noise. continuous time stochastic process logarithm random variable follows brownian motion volatility drift. formally stochastic process arbitrary initial value said follow geometric brownian motion satisﬁes following stochastic differential equation brownian motion trajectory µxtdt σxtdwt capture trend volatility required experience evolution. however real life communities user might show different experience evolution; therefore model considers multivariate version model trajectory per-user. correspondingly inference process learn user therefore future states depend future increment brownian motion satisﬁes requirement experience evolution. also process always positive. note start time user relative ﬁrst review community. experience-aware language evolution experience values user generated log-normal distribution develop language model whose parameters evolve according markov property experience evolution. movie’s protagonist whereas amateur movie lovers talk hero. similarly beer review community experts fruity words describe beer like caramel ﬁnish coffee roasted vanilla citrus hops. facet preferences users also evolve experience. example users high level experience prefer hoppiest beers considered bitter amateurs encoding explicit time model allows trace evolution vocabulary trends jointly temporal experience dimension. latent dirichlet allocation traditional process document assumed distribution facets facets distribution words ﬁxed vocabulary collection. per-facet word distribution drawn dirichlet distribution words generated multinomial. process assumes documents drawn exchangeably facets. however process neither takes experience evolution facets time account. discrete experience-aware previous work incorporates layer experience process. user experience manifested facets user chooses write vocabulary writing style used reviews. experience levels drawn hidden markov model reviews assumed exchangeable user level experience assumption generally hold; since language model user discrete experience level different different points time process considers time implicitly transition latent variable experience. continuous time seminal work blei captures evolving content instance scholarly journals news articles themes evolve time considering time explicitly generative process. language model evolution motivated continuous time dynamic topic model major difference facets case evolve time experience. continuous experience-aware since assumption exchangeability documents level experience user hold want language model explicitly evolve experience time. incorporate effect changing experience levels goal condition parameter evolution experience progression. variance linearly increase experience change successive timepoints. entails experience user change successive timepoints language model remains almost same. incorporate temporal aspects data model multiple distributions time facet furthermore capture smooth temporal evolution facet language model need chain different distributions sequentially evolve time distribution affect distribution βt+z. since traditional parametrization multinomial distribution mean parameters amenable sequential modeling inconvenient work gradient based optimization since gradient step requires projection feasible simplex follow similar approach instead operating mean parameters consider natural parameters multinomial. natural parameters unconstrained thus enable easier sequential modeling. denote natural parameters multinomial time facet identiﬁability parameters βtzw needs ﬁxed zero. applying following mapping obtain back mean parameters located simplex using natural parameters deﬁne facet-model evolution underlying idea strong changes users’ experience lead strong changes language model changes lead changes. capture effect denote average experience word time given average experience reviews containing word time ordered timestamps i.e. denote reviews written timepoint review consists sequence words denoted wnd} word drawn vocabulary unique words indexed number facets corresponds denote experience value review since review associated unique timestamp unique user experience value review refers experience user time writing model user follows geometric brownian motion trajectory starting time relative ﬁrst review user community parametrized mean variance starting experience value shown equation analytical form translates log-normal distribution given mean variance. user-dependent distribution generate experience value review written timestamp following standard facet proportion review drawn dirichlet distribution concentration parameter facet word drawn multinomial. generated experience values generate language model individual words review. here language model βtzw uses state-transition equation actual word based facet timepoint according multinomial) transformation given equation note technically distribution word generated simultaneously require terms depend experience words. thus joint distribution since however words observed inference dependence crucial i.e. computed experience values known using equation observation simplify notations illustrations algorithm outlines generative process figure depicts visually plate notation graphical models. here simply follow idea standard dynamic system gaussian noise mean value previous timepoint variance increases linearly increasing change experience. thereby desired properties language model evolution ensured. consider corpus review documents written users timestamps review denote user ﬁne-grained timestamp review timestamp coarser granularity reviews assumed contrast variable cannot integrated process normal multinomial distributions conjugate. therefore refer another approximation technique estimate work kalman filter model sequential language model evolution. widely used model linear dynamic systems series observed measurements time containing statistical noise produces robust estimates unknown variables single measurement. continuous analog hidden markov model state space latent variables continuous observed latent variables evolve gaussian noise. however unlike standard kalman filter observed measurement variables presence latent facets therefore resort inferred measurement gibbs sampling process. where inverse transformation given equation used smoothing. update equations kalman filter denote prediction error kalman gain time respectively. variance process noise measurement given difference experience value word observed successive timepoints. following standard kalman filter calculations predict equations given algorithm generative model continuous experienceaware language model. granularity language model evolution granularity experience evolution timestamp experience values reviews experience values words facets timestamps words corpus respectively. following denotes review indexes word denotes per-review facet distribution language model respectively. exact computation distribution intractable resort approximate inference. exploiting conjugacy multinomial dirichlet distributions integrate distribution. assuming integrated decompose joint distribution collapsed gibbs sampling standard estimate conditional distribution latent facets computed current assignment hidden variables integrating denote count topic appearing review following equation indicates summation counts possible subscript denotes value variable excluding data position. estimate facets using equation estimate using equations sort reviews timestamps estimate using equation metropolis hastings algorithm random subset reviews. amazon movie reviews yelp food restaurant reviews newstrust reviews news media table gives dataset statistics. total million reviews million users years communities combined. ﬁrst four communities used product reviews extract following quintuple model userid itemid timestamp rating review newstrust special community discuss section data likelihood smoothness inference model quite involved different markov chain monte carlo methods. imperative show resultant model stable also improves log-likelihood data. although several measures evaluate quality facet models report following higher likelihood indicates better model. http//snap.stanford.edu/data/ http//www.yelp.com/dataset_challenge/ http//resources.mpi-inf.mpg.de/impact/credibilityanalysis/data.tar.gz emits counts estimated gibbs sampling similar dynamic topic model intuitively kalman filter smoothing estimate gibbs sampling taking experience evolution account. experience value review depends user language model although state-transition model previous process estimation using kalman filter cannot applied case observed inferred value therefore resort metropolis hastings sampling. instead sampling complex true distribution proposal distribution sampling random variables followed acceptance rejection newly sampled value. iteration algorithm samples value random variable current estimate depends previous estimate thereby forming markov chain. assume reviews {··· ···} users sorted according timestamps. discussed section computational feasibility coarse granularity language model inference however need operate temporal resolution reviews’ timestamps note process deﬁned represents aggregated language model multiple ﬁne-grained timestamps. accordingly corresponding ﬁne-grained counterzw |edi edi−|) part operating review’s individual experience values. since language model given inference easily refer ﬁne-grained deﬁnition metropolis hastings sampling. time depends language model language model experience value difference |edi edi−| timepoints. therefore change experience value timepoint affects language model current next timepoint i.e. di+. numerator accounts modiﬁed distributions affected updated experience value denominator discounts ones. note since used proposal distribution factor cancels term figure contrasts log-likelihood data continuous experience model discrete counterpart continuous model stable smooth increase data log-likelihood iteration. attributed smoothly language model evolves time preserving markov property experience evolution. empirically model also shows fast convergence indicated number iterations. hand discrete model worse also less smooth. exhibits abrupt state transitions hidden markov model experience level changes leads abrupt changes language model coupled experience evolution. experience-aware item rating prediction ﬁrst task show effectiveness model item rating prediction. given user item time review words objective predict rating user would assign item based experience. prediction following features experience value user taken last experience attained user training. based learned language model construct language feature vector log) dimension word review consider value corresponding best facet assigned word time take log-transformation empirically gives better results. furthermore also done baseline works consider average rating community; offset average rating given user global average; rating bias item thus combining above construct feature vector review user-assigned ground rating training. support vector regression default parameters used discrete model rating prediction. consider baselines code experiments. baseline prior discrete experience model. standard latent factor recommendation model community uniform rate users products community evolve using single global clock different stages community evolution appear uniform time intervals. based preferences experience levels evolving time. model assumes uniform rate experience progression. user learned rate extends allowing experience user evolve personal clock time reach certain experience levels depends user reportedly best version experience evolution models. discrete experience model prior work discrete version experience-aware language model experience user depends evolution user’s maturing rate facet preferences writing style. table compares mean squared error rating predictions task generated model versus baselines. model outperforms baselines except newstrust community performing slightly worse prior work reducing improvements baselines statistically signiﬁcant level conﬁdence determined paired sample t-test. models used three recent reviews user withheld test data. experience-based models consider last experience value reached user training corresponding learned parameters rating prediction. similar setting consider users minimum reviews. users less reviews grouped background model treated single user. beeradvocate ratebeer yelp facets; amazon movies newstrust richer latent dimensions. discrete experience models consider experience levels. continuous model experience value initialize parameters joint model performance improvement strong beeradvocate community large number reviews per-user long period time newstrust converse. qualitative results user experience progression figure shows variation users’ recent experience along number reviews posted number years spent community. would expect user’s experience increases amount time spent community. contrary number reviews posted strong inﬂuence experience progression. thus user writes large number reviews short span time experience increase much; contrast reviews written long period time. figure shows variation users’ recent experience along mean variance geometric brownian motion trajectory learned inference. observe users reach high level experience progress faster not. experienced users also exhibit comparatively higher variance amateur ones. result also follows using process mean variance tend increase time. language model evolution figure shows variation frequency word used community learned experience value associated word. plots depict bell curve. intuitively experience value word increase general usage; increases used experienced users. highlighted words plot give interesting insights. instance words beer head place food movie story etc. used high frequency beer food movie community average experience value. figure variation experience years reviews user. stacked chart corresponds user recent experience number years spent number reviews posted community. figure variation experience mean variance trajectory user stacked chart corresponds user recent experience mean variance experience evolution. figure language model score variation sample words time. figure shows count sample words time beeradvocate community whose evolution traced figure figures show evolution yelp amazon movies. models continuous experience model discrete experience model user learned rate community learned rate community uniform rate user uniform rate latent factor model experience beeradvocate chestnut_hued near_viscous rampant_perhaps cherry_wood faux_foreign sweet_burning bright_crystal faint_vanilla boned_dryness woody_herbal citrus_hops mouthfeel amazon aﬁcionados minimalist underwritten theatrically unbridled seamless retrospect overdramatic diabolical recreated notwithstanding oblivious featurettes precocious yelp foie smoked marinated savory signature contemporary selections bacchanal delicate grits gourmet texture exotic balsamic newstrust health actions cuts medicare climate major jobs house vote congressional spending unemployment citizens events table shows words used experienced users amateur ones different communities learned model. note ranked list words numeric values experienced users interested ﬁne-grained facets like mouthfeel fruity ﬂavors texture food drinks; narrative style movies opposed popular entertainment themes; discussing government policies regulations news reviews etc. figure shows evolution sample words time experience different communities. score y-axis combines language model probability βtzw experience value associated word time figure illustrates frequency words beeradvocate evolution traced figure seen overall usage word increases time; evolution path different word. instance smell convention started aroma dominant; latter less used experienced users time slowly replaced smell. also reported different context. similarly caramel likely used experienced users ﬂavor. also contrast evolution bitterness used experienced users compared bitter. yelp certain food trends like grilled crispy increasing time; contrast decreasing feature like casino restaurants. amazon movies certain genres like horror thriller contemporary completely dominating genres recent times. focused traditional item recommendation items like beers movies. switch different kind items newspapers news articles analyzing newstrust online community features news stories posted reviewed members professional journalists content experts. stories reviewed based objectivity rationality general quality language present unbiased balanced narrative event focus quality journalism. unlike datasets newstrust contains expertise members used groundtruth evaluating model-generated experience values. framework story item rated reviewed user. facets underlying topic distribution reviews topics healthcare obama administration etc. facet preferences mapped polarity users news community. recommending news articles. ﬁrst objective recommend news readers catering viewpoints experience. apply model setting earlier datasets. mean squared error results reported section model clearly outperforms baselines; performs slightly worse regarding prior work task possibly high rating sparsity face large number model parameters. identifying experienced users. second task experienced members community potential citizen journalists. order evaluate quality ranked list experienced users generated model consider following proxy measure user experience. newstrust users member levels determined newstrust staff based community engagement time community users’ feedback reviews proﬁle transparency manual validation. member levels categorize users experienced inexperienced. treated ground truth assessing ranking quality model baseline models considering users model ranked experience. consider top-performing baseline models previous task. report normalized discounted cumulative gain normalized kendall distance ranked lists users generated models. better model exhibit higher ndcg lower kendall distance. table shows model outperforms baseline models capturing user maturity. state-of-the-art recommender systems harness user-user item-item similarities means latent factor models. timedependent phenomena bursts item popularity bias ratings temporal evolution user community investigated also prior work anomaly detection capturing changes social links linguistic norms none prior works take account evolving experience behavior individual users. prior work analyzed user review texts focused sentiment analysis learning latent aspects ratings user-user interactions however prior approaches operate static snapshot-oriented manner without considering time all. work baselines modeled studied inﬂuence evolving user experience rating behavior targeted recommendations. however disregards vocabulary users’ reviews. recent work addressed limitation means language models speciﬁc experience level individual user modeling transitions experience levels hidden markov model. however works limited discrete experience levels leading abrupt changes experience language model. address above related drawbacks current paper introduces continuous-time models smooth evolution user experience language model. wang modeled topics time. however topics constant time used better discover them. dynamic topic models introduced blei prior work developed generic models based brownian motion applied news corpora. argues continuous model avoids making choices discretization also tractable compared ﬁne-grained discretization. language model motivated latter. substantially extend capture evolving user behavior experience review communities using geometric brownian motion. conclusion paper proposed experience-aware language model trace continuous evolution user experience language explicitly time. combine principles geometric brownian motion brownian motion latent dirichlet allocation model smooth temporal progression user experience language model time. ﬁrst work develop continuous generalized version user experience evolution. experiments data domains like beer movies food news demonstrate model effectively exploits user experience item recommendation substantially reduces mean squared error predicted ratings compared stateof-the-art baselines demonstrate utility model use-case study identifying experienced members newstrust community users would candidates citizen journalists. another similar use-case model detect experienced medical professionals health community. references david blei john lafferty. dynamic topic models.", "year": 2017}