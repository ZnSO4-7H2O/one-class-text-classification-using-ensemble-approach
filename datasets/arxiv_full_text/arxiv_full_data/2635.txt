{"title": "Learning to Optimize Neural Nets", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100.", "text": "recently malik andrychowicz introduced different frameworks learning optimization algorithms. whereas andrychowicz focuses learning optimization algorithm training models particular task malik sets ambitious objective learning optimization algorithm training models task-independent. study latter paradigm paper develop method learning optimization algorithm highdimensional stochastic optimization problems like problem training shallow neural nets. learning optimize framework proposed malik problem learning optimization algorithm formulated reinforcement learning problem. consider general structure unconstrained continuous optimization algorithm shown algorithm iteration algorithm takes step uses update current iterate hand-engineered optimization algorithms computed using ﬁxed formula depends objective function current iterate past iterates. often simply function current past gradients. different choices yield different optimization algorithms optimization algorithm essentially characterized update formula hence learning learn optimization algorithm. malik observed optimization algorithm viewed markov decision process state includes current iterate action step veclearning optimize recently proposed framework learning optimization algorithms using reinforcement learning. paper explore learning optimization algorithm training shallow neural nets. high-dimensional stochastic optimization problems present interesting challenges existing reinforcement learning algorithms. develop extension suited learning optimization algorithms setting demonstrate learned optimization algorithm consistently outperforms known optimization algorithms even unseen tasks robust changes stochasticity gradients neural architecture. speciﬁcally show optimization algorithm trained proposed method problem training neural mnist generalizes problems training neural nets toronto faces dataset cifar- cifar. machine learning centred philosophy learning patterns automatically data generally better meticulously crafting rules hand. data-driven approach delivered today machine learning techniques found wide range application areas beyond. domain conspicuously left untouched machine learning design tools power machine learning itself. widely used tools machine learning optimization algorithms. grown accustomed seeing optimization algorithm black takes model design data collect outputs optimal model parameters. optimization algorithm largely stays static design reserved human experts must toil many rounds theoretical analysis empirical validation devise better paper build method proposed develop extension suited learning optimization algorithms high-dimensional stochastic problems. learn optimization algorithm training shallow neural nets show outperforms popular hand-engineered optimization algorithms like adam adagrad rmsprop optimization algorithm learned using supervised learning method proposed furthermore demonstrate optimization algorithm learned experience training mnist generalizes training datasets dissimilar statistics like toronto faces dataset cifar- cifar-. line work learning optimization algorithms fairly recent. malik andrychowicz ﬁrst propose learning general optimization algorithms. malik explored learning task-independent optimization algorithms used reinforcement learning learn optimization algorithm andrychowicz investigated learning task-dependent optimization algorithms used supervised learning. special case objective functions optimization algorithm trained loss functions training models methods used learning learn meta-learning. terms appeared time time literature used different authors refer disparate methods different purposes. methods share objective learning form meta-knowledge learning differ type meta-knowledge learn. divide various methods following three categories. methods category learn parameter values base-level learner useful across family related tasks. meta-knowledge captures commonalities shared tasks family enables learning task family done quickly. early methods fall category; line work blossomed area later become known transfer learning multi-task learning. methods category learn base-level learner achieves best performance task. meta-knowledge captures correlations different tasks performance different base-level learners tasks. challenge setting decide parameterization space base-level learners rich enough capable representing disparate base-level learners compact enough permit tractable search space. brazdil proposes nonparametric representation stores examples different base-level learners database whereas schmidhuber proposes representing baselevel learners general-purpose programs. former limited representation power latter makes search learning space base-level learners intractable. hochreiter views training procedure base-learner black function maps sequence training examples sequence predictions models recurrent neural net. formulation meta-training reduces training recurrent base-level learner encoded memory state recurrent net. hyperparameter optimization seen another example methods category. space base-level learners search parameterized predeﬁned hyperparameters. unlike methods above multiple trials different hyperparameter settings task permitted generalization across tasks required. discovered hyperparameters generally speciﬁc task hand hyperparameter optimization must rerun tasks. various kinds methods proposed based bayesian optimization random search gradientbased optimization methods category learn good algorithm training base-level learner. unlike methods previous categories goal learn outcome learning rather process learning. meta-knowledge captures commonalities behaviours learning algorithms achieve good performance. base-level learner task given user learned algorithm must generalize across base-level learners tasks. since learning cases equivalent optimizing objective function learning learning algorithm often reduces learning optimization algorithm. problem explored various work explored learning adjust hyperparameters hand-engineered optimization algorithms like step size damping factor levenbergmarquardt algorithm related line work stochastic meta-descent derives rule adjusting step size analytically. different line work parameterizes intermediate operands special-purpose solvers class optimization problems arise sparse coding learns using supervised learning. completely unrelated tasks used training optimization algorithm. therefore learned optimization algorithm must learn anything tasks used training. instead goal learn optimization algorithm exploit geometric structure error surface induced base-learners. example base-level model neural relu activation units optimization algorithm hopefully learn leverage piecewise linearity model. hence clear division responsibilities meta-learner base-learners. knowledge learned meta-level pertinent tasks whereas knowledge learned base-level task-speciﬁc. metalearner therefore generalize across tasks whereas base-learner generalize across instances. learning optimize framework given training objective functions drawn distribution optimization algorithm takes objective function initial iterate input produces sequence iterates solution found optimizer. also given distribution generates initial iterate meta-loss takes objective function sequence iterates produced optimization algorithm input outputs scalar measures quality iterates. goal learn optimization algorithm loss chosen penalize optimization algorithms exhibit behaviours undesirable like slow convergence excessive oscillations. assuming would like learn algorithm minimizes objective function given good choice meta-loss would simply interpreted area objective functions correspond loss functions training base-level learners case algorithm learns optimization algorithm viewed meta-learner. setting objective function loss function training particular baselearner particular task training objective functions loss functions training baselearner family base-learners different tasks. test time learned optimization algorithm evaluated unseen objective functions correspond loss functions training base-learners tasks goal reinforcement learning learn interact environment minimizes cumulative costs expected incurred time. environment formalized partially observable markov decision process deﬁned tuple states observations actions probability density initial states probability density subsequent state given current state action probability density current observation given current state function assigns cost state time horizon. often probability densities unknown given learning algorithm. policy conditional probability density actions given current observation time step policy independent known stationary policy. goal reinforcement learning algorithm learn policy minimizes total expected cost time. precisely optimization challenging. iteration performs policy optimization uses resulting policy supervision train precisely solves following constrained optimization problem algorithm constructs model transition probability density ﬁtted samples drawn trajectory induced essentially amounts local linearization true transition probability denote expectation taken respect trajectory induced practice explicit form observation probability usually known integral intractable compute. linear gaussian model ﬁtted samples used place true necessary. make learning tractable often constrained parameterized family. common assumption denotes density gaussian mean covariance functions possibly modelled using function approximators whose parameters learned. setting state consists current iterate features depend history iterates gradients objective values action step used update iterate. observation excludes consists features depend iterates gradient objective values recent iterations previous memory state learned optimization algorithm takes form recurrent neural net. memory state viewed statistic previous observations learned jointly policy. formulation initial probability density captures initial iterate gradient objective value tend distributed. transition probability density captures gradient objective value likely change given step taken currently; words encodes local geometry training objective functions. assuming goal learn optimization algorithm minimizes objective function objective value particular policy generates every time step corresponds particular update formula therefore particular optimization algorithm. therefore learning optimization algorithm simply reduces searching optimal policy. mean policy modelled recurrent neural fragment corresponds single time step takes observation features previous memory state input outputs step take. reinforcement learning method guided policy search policy search method designed searching large classes expressive non-linear policies continuous state action spaces. maintains policies former lies time-varying linear policy class optimal policy found closed form latter lies stationary non-linear policy class policy spaces. example case running time cubic dimensionality state space performing policy search even simple class linear-gaussian policies would prohibitively expensive dimensionality optimization problem high. fortunately many high-dimensional optimization problems underlying structure exploited. example parameters neural nets equivalent permutation among certain coordinates. concretely fully connected neural nets dimensions hidden layer corresponding weights permuted arbitrarily without changing function compute. because permuting dimensions adjacent layers permute weight matrix arbitrarily optimization algorithm invariant permutations rows columns weight matrix. reasonable prior impose algorithm behave manner coordinates correspond entries matrix. values coordinates current past gradients iterates identical step vector produced algorithm identical values coordinates. refer coordinates permutation invariance enforced coordinate group. purposes learning optimization algorithm neural nets natural choice would make coordinate group correspond weight matrix bias vector. hence total number coordinate groups twice number layers usually fairly small. case impose prior purposes updating ﬁrst impose blockdiagonal structure parameters ﬁtted transition probability density coordinate optimization problem dimensions correspond coordinate depend dimensions correspond coordinate. result decomposes multiple inde coordinate similarly also impose blockdiagonal structure ﬁtting parameter matrix ﬁtted model assumptions guaranteed blockdiagonal well. hence bregman divergence penalty term decomposes bregman divergence terms coordinate. constrain dual variables sub-vectors parameter vectors sub-matrices parameter matrices corresponding coordinate group identical across group. additionally replace weight individual weight bregman modelled transition probability additionally algorithm local quadratic approximations around samples drawn trajectory induced st’s near samples. assumptions subproblem needs solved update denotes previous iteration. because valid locally around trajectory induced constraint added limit amount updated. turns unconstrained problem solved closed form using dynamic programming algorithm known linear-quadratic-gaussian regulator time linear time horizon cubic dimensionality state space constrained problem solved using dual gradient descent uses subroutine solve primal variables iteration increments dual variable constraint satisﬁed. updating straightforward since expectations taken respect trajectory induced always conditioned outer expectations taken respect trajectory induced therefore essentially decoupled transition probability parameters updated without affecting distribution st’s. subproblem needs solved update therefore amounts standard supervised learning problem. since gaussian computed analytically. concretely assume ﬁxed simplicity subproblem solved updating problem learning high-dimensional optimization algorithms presents challenges reinforcement learning algorithms high dimensionality state action figure comparison various hand-engineered learned algorithms training neural nets input hidden units cifar- cifar- mini-batches size vertical axis true objective value horizontal axis represents iteration. best viewed colour. divergence term coordinate group. problem decomposes multiple independent subproblems coordinate group. dimensionality state subspace corresponding coordinate constant executed subproblem much efﬁciently. shares parameters similarly choose across different coordinates group. also impose block-diagonal structure constrain appropriate sub-matrices share entries. stochasticity gradients objective values state features deﬁned terms sumi= gradients deﬁne following statistics refer average recent iterate gradient objective value respectively state features consist relative change average recent objective value average recent gradient normalized magnitude previous average recent gradient previous change average recent iterate relative current change average recent iterate unlike state features used training optimization algorithm observation features used training test time. consequently noisier observation features computed efﬁciently require less memory overhead. observation features consist following meta-trained optimization algorithm single objective function corresponds problem training two-layer neural input units hidden units output units randomly projected normalized version mnist training dimensionality unit variance dimension. modelled optimization algorithm using recurrent neural figure comparison various hand-engineered learned algorithms training neural nets input units hidden units cifar- cifar- mini-batches size vertical axis true objective value horizontal axis represents iteration. best viewed colour. figure comparison various hand-engineered learned algorithms training neural nets input hidden units cifar- cifar- mini-batches size vertical axis true objective value horizontal axis represents iteration. best viewed colour. single layer lstm cells. used time horizon iterations mini-batch size computing stochastic gradients objective values. evaluate optimization algorithm ability generalize unseen objective functions correspond problems training neural nets different tasks/datasets. evaluate learned optimization algorithm three datasets toronto faces dataset cifar- cifar-. datasets chosen different characteristics mnist other contains grayscale images relatively little variation seven different categories whereas cifar- contains colour images varied appearance different categories. algorithms tuned training objective function. hand-engineered algorithms entails choosing best hyperparameters; learned algorithms entails meta-training objective function. compare seven hand-engineered algorithms stochastic gradient descent momentum conjugate gradient l-bfgs adam adagrad rmsprop. addition compare optimization algorithm meta-trained using method defirst examine performance various optimization algorithms similar objective functions. optimization problems consideration training neural nets number input hidden units used meta-training. number output units varies number categories dataset. mini-batch size used meta-training. shown figure optimization algorithm meta-trained using method consistently descends optimum fastest across datasets. hand algorithms consistent relative ranking algorithms varies dataset. suggests predicted step descent learned robust variations data distributions despite being trained objective function associated speciﬁc data distribution characterizes mnist. also interesting note figure comparison various hand-engineered learned algorithms training neural nets input units hidden units cifar- cifar- mini-batches size vertical axis true objective value horizontal axis represents iteration. best viewed colour. figure comparison various hand-engineered learned algorithms training neural nets input units hidden units cifar- cifar- iterations mini-batches size vertical axis true objective value horizontal axis represents iteration. best viewed colour. next change architecture neural nets predicted step descent generalizes architecture. increase number input units number hidden units number parameters roughly increased factor shown figure predicted step descent consistently outperforms algorithms dataset despite trained optimize neural nets architecture. interestingly exhibited oscillation initially cifar- quickly recovered overtook algorithms reminiscent phenomenon reported low-dimensional optimization problems. suggests learned detect performing poorly knows change tack accordingly. llbgdbgd experienced difﬁculties cifar- well slowly diverged. original architecture input hidden units enlarged architecture input units hidden units. shown figure original architecture predicted step descent still outperforms algorithms able handle increased stochasticity fairly well. contrast conjugate gradient llbgdbgd difﬁculty handling increased stochasticity lesser extent cifar-. former case diverged; latter case progressing slowly towards optimum. enlarged architecture predicted step descent experienced signiﬁcant oscillations cifar still managed achieve much better objective value algorithms. many hand-engineered algorithms also experienced much greater oscillations previously suggesting optimization problems inherently harder. llbgdbgd diverged fairly quickly datasets. finally doubling number iterations. shown figure despite trained time horizon iterations predicted step descent behaves reasonably beyond number iterations trained for. paper presented method learning optimization algorithms high-dimensional stochastic problems. applied method learning optimization algorithm training shallow neural nets. showed algorithm learned using method problem training neural mnist generalizes problems training neural nets unrelated tasks/datasets like toronto faces dataset cifar- cifar-. also demonstrated learned optimization algorithm robust changes stochasticity gradients neural architecture. references andrychowicz marcin denil misha gomez sergio hoffman matthew pfau david schaul freitas nando. learning learn gradient descent gradient descent. arxiv preprint arxiv. baxter jonathan caruana rich mitchell pratt lorien silver daniel thrun sebastian. nips workshop learning learn knowledge consolidation transfer inductive systems. https //web.archive.org/web// http//www.cs.cmu.edu/afs/cs.cmu.edu/ user/caruana/pub/transfer.html accessed bergstra james bardenet r´emi bengio yoshua k´egl bal´azs. algorithms hyper-parameter optimization. advances neural information processing systems bray koller-meier muller gool schraudolph hand tracking rapid stochasvisual gradient descent using skinning model. media production european conference daniel christian taylor jonathan nowozin sebastian. learning step size controllers robust neural network training. thirtieth aaai conference artiﬁcial intelligence hochreiter sepp younger steven conwell peter learning learn using gradient descent. international conference artiﬁcial neural networks springer hutter frank hoos holger leyton-brown kevin. sequential model-based optimization general algolearning intelligent optirithm conﬁguration. mization springer brazdil pavel carrier christophe giraud soares carlos vilalta ricardo. metalearning applications data mining. springer science business media snoek jasper larochelle hugo adams ryan practical bayesian optimization machine learning algorithms. advances neural information processing systems sprechmann pablo litman roee yakar bronstein alexander sapiro guillermo. supervised sparse analysis synthesis operators. advances neural information processing systems tieleman tijmen hinton geoffrey. lecture .rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning", "year": 2017}