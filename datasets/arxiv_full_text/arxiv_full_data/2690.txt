{"title": "Bayesian Conditional Generative Adverserial Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Traditional GANs use a deterministic generator function (typically a neural network) to transform a random noise input $z$ to a sample $\\mathbf{x}$ that the discriminator seeks to distinguish. We propose a new GAN called Bayesian Conditional Generative Adversarial Networks (BC-GANs) that use a random generator function to transform a deterministic input $y'$ to a sample $\\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, and naturally handle unsupervised learning, supervised learning, and semi-supervised learning problems. Experiments show that the proposed BC-GANs outperforms the state-of-the-arts.", "text": "traditional gans deterministic generator function transform random noise input sample discriminator seeks distinguish. propose called bayesian conditional generative adversarial networks random generator function transform deterministic input sample bc-gans extend traditional gans bayesian framework naturally handle unsupervised learning supervised learning semi-supervised learning problems. experiments show proposed bc-gans outperforms state-of-the-arts. generative adversarial nets class models developed tackle unsupervised learning long standing problem machine learning. algorithms work training neural networks generator discriminator–to play game minimax formulation generator network learns generate fake samples similar possible real ones. discriminator hand learns distinguish real samples fake ones. information-theoretic view discriminator measure learns evaluate close distribution real fake samples generator network deterministic function transforms input noise samples target distribution e.g. images. original algorithm extended conditional models addition input noise generator attribute vector label also provided. helps generating samples particular class adding vector layer generator network effect performance. paper propose replace deterministic generator function stochastic leads simpler uniﬁed model. shown figure omit need random vector input. furthermore generator network learns utilize uncertainty generating samples particular class leads activation certain weights class. representation uncertainty generator allows introduce bayesian conditional bayesian framework learning conditional gans. integrating generator discriminator functions bring figure difference original bayesian proposed paper. approach parameter generator original random variable itself. moreover deterministic label variable feed generator. sample data generated sample generator function. beneﬁts bayesian methods gans representing uncertainty models avoiding overﬁtting. dropout bernoulli gaussian build model. since training gans involve alternating training generator discriminator network saddle-point problem optimization unstable difﬁcult tune. believe utilizing bayesian methods monte carlo fashion average function values help stabilizing training. make following contributions unlike traditional methods using random noise variable generator random function takes deterministic input allows utilize uncertainty model rather noise input. provide bayesian framework learning gans capture uncertainty model samples taken generator. since bayesian methods integrate parameters less susceptible overﬁtting stable. bayesian conditional real data fake data respectively rn×n seem work supervised learning only actually works semi-supervised unsupervised learning problems gans. supervised learning setting number classes data. semi-supervised setting unlabelled data augment real assigning unlabelled data label unsupervised learning setting real labeled fake ones many gans wasserstein generator function transforms random noise input sample discriminator seeks distinguish approach hand model generator random function transforms deterministic input sample whose distribution resembles distribution real data deﬁne distribution generated samples generator parameter/weights discriminator network. resemble gaussian processes classiﬁcation problems. fact shown using dropout discriminator type network resembles posterior estimation advantage using bayesian approaches inference parameters include model uncertainty approach better equipped tackle convergence problem gans. using weights posterior taking advantage functional distribution learner navigate better complicated parameter space. observe helps general gan’s problem reaching saddle point alternation optimization generator discriminator. estimate expectations perform inference turn commonly used monte carlo methods. following discuss experiment methods. markov chain monte carlo gradient langevin dynamics. uncertainty model randomness generator function observed multiple rounds generator update performs better practice. words sample generator often updating discriminator. deﬁnitions distributions generator discriminator show learn below. simple approach using distribution transformation function uncertainty discriminator sample weight distribution perform updates. this sample functional values generator discriminator minimize network’s loss accordingly. approach performing thompson sampling used sequential decision making agent picks action iteratively minimize expected loss within bayesian framework. here generator discriminator play thompson sampling iteration based current observations distribution discriminator updated pdata true underlying distribution data pfake fake distribution data represented generator. loss function discriminator network loss generator network. describes discrepancy overall framework method shown figure since monte carlo unbiased estimator expectations perform parameters function sample functions follows call approach map-mc another perform inference model employ stochastic gradient langevin dynamics. inspired robbins-monro algorithms mcmc approach proposed perform efﬁcient inference large datasets. principle langevian dynamics takes updates parameters direction maximum posteriori injecting noise trajectory covers full posterior. thus updating discriminator generator network adding noise gradient model updates. particularly used losses give rise distributions e.g. case softmax loss. langevin dynamics allows perform full bayesian inference parameters minor modiﬁcations pervious approach. langevin dynamics update parameters added gaussian noise gradients i.e. added noise ensure parameters traversing towards mode distributions also sampling according density. practice improve convergence model smaller variance noise distribution. step algorithm need compute expectations respect generator discriminators. taking samples function according distributions. done using simple tricks like dropout allow sample neural network. shown dropout bayesian interpretation posterior approximated using variational inference connection gaussian processes dropout classiﬁcation made placing variational distribution variables model minimizing kldivergence variational distribution true distribution variables. dropout acts regularizer improves generalization performance neural nets reported such dropout means sampling various functions generator discriminator. discriminator variants dropout estimating uncertainty discriminator predictions using variance predictive distribution denotes activation function variance prior weights identity matrix. here denote test instance corresponding predicted label either real fake dataset. trick using variants dropout obtain samples generator function too. learning rate regularizer dropout probability discriminator generator respectively standard deviation weight prior discriminator generator respectively deﬁne distributions functions non-parametric bayesian manner analytically integrating parameters sample parameters monte carlo method estimate expectation. generator network generates high quality fake samples discrepancy fake samples real samples expected small. suitable discrepancy measure capture statistical properties real fake data. choose maximum mean discrepancy measure asserts dimensions data large moment matching input space possible difference empirical means distributions using nonlinear feature measure closeness two-sample problems. feature used measure bounded compact. property ensured constraining weights i.e. function deﬁned interesting note neural network implementation measure need ensure parameters normalized. value weight normalization neural nets already shown show used different manner model density comparison. evaluation generative models general gans particular typically difﬁcult. since approach classiﬁcation loss well target semi-supervised learning problems. particular small number training instances loss softmax layer output discriminator training model. also observed important output fake images loss one-hot vector labels input generator. deterministic input cause collapse whole generator network especially randomizations enough. layers dropout gaussian noise every layer input. experiments batch sizes stochastic gradient descent randomly select subset labeled examples rest unlabelled data training. perform experiments times report mean standard error. constant learning rate map-mc approach reduce learning rate inversely proportionate rate training epoch langevin dynamics. monte carlo samplings samples efﬁciency. evaluate approach datasets mnist cifar-. experiments map-mc performs better langevin dynamics terms accuracy conjecture nature inference method. intuitively langevin dynamics gradients noisy thus movement parameters complex space minimax objective difﬁcult. mnist dataset mnist dataset contains training test images size handwritten digits greyscale images. since image pixels binary generator network sigmoid activation output pixel. generator sample one-hot label vector input uniformly classes three layer generator network softplus activation units. between fully connected layers gaussian bernoulli dropout variance ratio respectively. also used batch normalization layer shown effective figure predictive variance discriminator randomly selected samples training mnist dataset using map-mc. x-axis show epoch y-axis variance. seen variance reduced training. discriminator fully connected network gaussian bernoulli dropout layers variance ratio respectively. weight normalization last layer shown figure generate samples real looking images mnist dataset label. small numbers generated images generated label discriminator’s prediction respectively. observed ﬁnal stages training discriminator powerful basically predict almost generated labels correctly. generator also trained match generated class corresponding image. noted samples exploit uncertainty generator network function desirable. test errors semi-supervised learning using approach compared state presented table furthermore predictive variance discriminator measure uncertainty using labelled instances class shown figure expected training variance reduced discriminator becomes conﬁdent predictions. cifar- dataset cifar- dataset composed classes natural images images training images testing. complexity images higher dimensions color variability make task harder. fully connected layer input three layers deconvolution batch normalized generate samples. again bernoulli gaussian dropout layers ratio induce uncertainty. discriminator network layers convolutional fully connected layers output. furthermore weight normalization layer. report performance approach dataset semi-supervised learning table samples generator shown figure observed case map-mc diverse images similarity across columns images label. langevin approach hand suffers mode collapse classes images seem similar suggests stopped earlier langevin used higher variance ratio dropout. since inception gans unsupervised learning various attempts made either better explaining models extending beyond unsupervised learning generating realistic samples. wasserstein loss-sensitive recently developed theoretically grounded approaches provide information-theoretic view models. objective methods constructed generic binary classiﬁcation done discriminator. fact discriminator acts measure closeness generated samples real ones similar approach taken paper. further gans successfully used latent variable modeling semi-supervised learning intuition generator assists discriminator number labelled instances small. instance infogan proposed learn latent variable represents cluster data learning generate images utilizing variational inference. directly used semi-supervised learning extension categorical utilized mutual information part loss developed good performance. furthermore developed heuristics better training achieving state results semi-supervised learning using gans. hand unlike approach conditional generate labelled images adding label vector input noise. furthermore measure used gans success however previously kernel function explicitly deﬁned approach learn part discriminator. bayesian methods gans generally limited combining variational autoencoders gans hand take bayesian view discriminator generator using dropout approximation variational inference. allows develop simpler intuitive model. unlike traditional gans proposed conditional bayesian model called bc-gan exploits uncertainty generator source randomness generating real samples. similarly evaluate uncertainty discriminator used measure performance. allows better analyze behavior good generator/discriminator functional perspective future shed light gans. also evaluated approach semisupervised learning problem showed effectiveness achieving state results. future plan explore inference methods hamiltonian monte carlo yield better performance. addition hope perform analysis method better explain internal behavior minimax model. chen chen duan rein houthooft john schulman ilya sutskever pieter abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems pages goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages curran associates inc. diederik kingma shakir mohamed danilo rezende welling. semi-supervised learning deep generative models. advances neural information processing systems pages sebastian nowozin botond cseke ryota tomioka. f-gan training generative neural samplers using variational divergence minimization. advances neural information processing systems pages curran associates inc. antti rasmus harri valpola mikko honkala mathias berglund tapani raiko. semi-supervised learning ladder networks. proceedings international conference neural information processing systems nips’ pages cambridge press. salimans goodfellow wojciech zaremba vicki cheung alec radford chen chen. improved techniques training gans. advances neural information processing systems pages curran associates inc. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. journal machine learning research", "year": 2017}