{"title": "Domain-Adversarial Training of Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.", "text": "introduce representation learning approach domain adaptation data training test time come similar diﬀerent distributions. approach directly inspired theory domain adaptation suggesting that eﬀective domain transfer achieved predictions must made based features cannot discriminate training test domains. approach implements idea context neural network architectures trained labeled data source domain unlabeled data target domain training progresses approach promotes emergence features discriminative main learning task source domain indiscriminate respect shift domains. show adaptation behaviour achieved almost feed-forward model augmenting standard layers gradient reversal layer. resulting augmented architecture trained using standard backpropagation stochastic gradient descent thus implemented little eﬀort using deep learning packages. demonstrate success approach distinct classiﬁcation problems state-of-the-art domain adaptation performance standard benchmarks achieved. also validate approach descriptor learning task context person re-identiﬁcation application. keywords domain adaptation neural network representation learning deep learning synthetic data image classiﬁcation sentiment analysis person re-identiﬁcation cost generating labeled data machine learning task often obstacle applying machine learning methods. particular limiting factor further progress deep neural network architectures already brought impressive advances state-of-the-art across wide variety machine-learning tasks applications. problems lacking labeled data still possible obtain training sets enough training large-scale deep models suﬀer shift data distribution actual data encountered test time. important example training image classiﬁer synthetic semi-synthetic images come abundance fully labeled inevitably distribution diﬀerent real images another example context sentiment analysis written reviews might labeled data reviews type product need classify reviews products learning discriminative classiﬁer predictor presence shift between training test distributions known domain adaptation proposed approaches build mappings source target domains classiﬁer learned source domain also applied target domain composed learned mapping domains. appeal domain adaptation approaches ability learn mapping domains situation target domain data either fully unlabeled labeled samples below focus harder unsupervised case although proposed approach generalized semi-supervised case rather straightforwardly. unlike many previous papers domain adaptation worked ﬁxed feature representations focus combining domain adaptation deep feature learning within training process. goal embed domain adaptation process learning representation ﬁnal classiﬁcation decisions made based features discriminative invariant change domains i.e. similar distributions source target domains. obtained feed-forward network applicable target domain without hindered shift domains. approach motivated theory domain adaptation suggests good representation cross-domain transfer algorithm cannot learn identify domain origin input observation. thus focus learning features combine discriminativeness domaininvariance. achieved jointly optimizing underlying features well discriminative classiﬁers operating features label predictor predicts class labels used training test time domain classiﬁer discriminates source target domains training. parameters classiﬁers optimized order minimize error training parameters underlying deep feature mapping optimized order minimize loss label classiﬁer maximize loss domain classiﬁer. latter crucially show three training processes embedded appropriately composed deep feed-forward network called domain-adversarial neural network uses standard layers loss functions trained using standard backpropagation algorithms based stochastic gradient descent modiﬁcations approach generic dann version created almost existing feed-forward architecture trainable backpropagation. practice non-standard component proposed architecture rather trivial gradient reversal layer leaves input unchanged forward propagation reverses gradient multiplying negative scalar backpropagation. provide experimental evaluation proposed domain-adversarial learning idea range deep architectures applications. ﬁrst consider simplest dann architecture three parts linear demonstrate success domain-adversarial learning architecture. evaluation performed synthetic data well sentiment analysis problem natural language processing dann improves state-of-the-art marginalized stacked autoencoders chen common amazon reviews benchmark. evaluate approach extensively image classiﬁcation task present results traditional deep learning image data sets—such mnist svhn well office benchmarks domain-adversarial learning allows obtaining deep architecture considerably improves previous state-of-the-art accuracy. finally evaluate domain-adversarial descriptor learning context person re-identiﬁcation application task obtain good pedestrian image descriptors suitable retrieval veriﬁcation. apply domainadversarial learning consider descriptor predictor trained siamese-like loss instead label predictor trained classiﬁcation loss. series experiments demonstrate domain-adversarial learning improve cross-data-set re-identiﬁcation considerably. general approach achieving domain adaptation explored many facets. years large part literature focused mainly linear hypothesis recently non-linear representations become increasingly studied including neural network representations notably state-of-the-art msda literature mostly focused exploiting principle robust representations based denoising autoencoder paradigm proaches perform reweighing selecting samples source domain others seek explicit feature space transformation would source distribution target important aspect distribution matching approach similarity distributions measured. here popular choice matching distribution means kernelreproducing hilbert space whereas gong fernando principal axes associated distributions. approach also attempts match feature space distributions however accomplished modifying feature representation rather reweighing geometric transformation. also method uses rather diﬀerent measure disparity between distributions based separability deep discriminatively-trained classiﬁer. note also several approaches perform transition source target domain changing gradually training distribution. among methods chopra deep layerwise training sequence deep autoencoders gradually replacing source-domain samples target-domain samples. improves similar approach glorot simply trains single deep autoencoder domains. approaches actual classiﬁer/predictor learned separate step using feature representation learned autoencoder. contrast glorot chopra approach performs feature learning domain adaptation classiﬁer learning jointly uniﬁed architecture using single learning algorithm therefore argue approach simpler method also achieves considerably better results popular office benchmark. approaches perform unsupervised domain adaptation approaches perform supervised domain adaptation exploiting labeled data target domain. context deep feed-forward architectures data used ﬁne-tune network trained source domain approach require labeled target-domain data. time easily incorporate data available. idea related described goodfellow goal quite diﬀerent measure minimize discrepancy distribution training data distribution synthesized data similar architecture measures minimizes discrepancy feature distributions domains. moreover authors mention problem saturating sigmoids arise early stages training signiﬁcant dissimilarity domains. technique circumvent issue directly applicable method. also recent concurrent reports tzeng long wang focus domain adaptation feed-forward networks. techniques measures minimizes distance data distribution means across domains approach thus diﬀerent idea matching distributions making indistinguishable discriminative classiﬁer. below compare approach tzeng long wang oﬃce benchmark. another approach deep domain adaptation arguably diﬀerent ours developed parallel chen sentations learned word tagging using posterior regularizer also inspired ben-david al.’s work. addition tasks diﬀerent—huang yates focus word tagging problems— would argue dann learning objective part paper published conference paper version extends ganin lempitsky considerably incorporating report ajakan brings terminology in-depth theoretical analysis justiﬁcation approach extensive experiments shallow dann case synthetic data well natural language processing task furthermore version beyond classiﬁcation evaluate domain-adversarial learning descriptor learning setting within person re-identiﬁcation application. domain adaptation consider classiﬁcation tasks input space possible labels. moreover diﬀerent distributions called source domain target domain unsupervised domain adaptation learning marginal distribution tackle challenging domain adaptation task many approaches bound target error source error notion distance source target distributions. methods intuitively justiﬁed simple assumption source risk expected good indicator target risk distributions similar. several notions distance proposed domain adaptation paper focus h-divergence relies capacity hypothesis class distinguish examples generated ben-david proved that symmetric hypothesis class compute empirical h-divergence samples given generalization error problem discriminating ben-david value called proxy a-distance adistance deﬁned chen i.e. train either linear deeper classiﬁer subset obtained classiﬁer error subset value equation details illustrations linear case provided section work ben-david also showed h-divergence upper bounded empirical estimate plus constant complexity term depends dimension size samples combining previous result tells term i.e. exists classiﬁer achieve risk distributions. also tells that classiﬁer small given class ﬁxed dimension learning algorithm minimize trade-oﬀ source risk empirical h-divergence ˆdh. pointed-out ben-david strategy control h-divergence representation examples source target domain indistinguishable possible. representation hypothesis source risk will according theorem perform well target data. paper present algorithm directly exploits idea. original aspect approach explicitly implement idea exhibited theorem neural network classiﬁer. learn model generalize well domain another ensure internal representation neural network contains discriminative information origin input preserving risk source examples. section detail proposed approach incorporating domain adaptation component neural networks. subsection start developing idea simplest possible case i.e. single hidden layer fully connected neural network. describe generalize approach arbitrary network architectures. using softmax function component vector denotes conditional probability neural network assigns class represented component. given source example natural classiﬁcation loss negative log-probability correct label consider class hyperplanes representation space. inspired proxy a-distance suggest estimating part equation domain classiﬁcation layer learns logistic regressor parameterized vector-scalar pair models probability given input source domain propose tackle problem simple stochastic gradient procedure updates made opposite direction gradient equation minimizing parameters direction gradient maximizing parameters. stochastic estimates gradient made using subset training samples compute averages. algorithm provides complete pseudo-code learning procedure. words training neural network domain regressor competing other adversarial objective equation reason refer networks trained according objective domain-adversarial neural networks dann eﬀectively attempt learn hidden layer maps example representation allowing output layer accurately classify source samples crippling ability domain regressor detect whether example belongs illustration purposes we’ve focused case single hidden layer dann. however straightforward generalize sophisticated architectures might appropriate data hand. example deep convolutional neural networks well known state-of-the-art models learning discriminative features images d-dimensional neural network feature extractor parameters also part dann computes network’s label prediction output layer parameters corresponds computation domain prediction output network parameters note preserving theoretical guarantees theorem hypothesis class generated domain prediction component include hypothesis class generated label prediction component thus figure proposed architecture includes deep feature extractor deep label predictor together form standard feed-forward architecture. unsupervised domain adaptation achieved adding domain classiﬁer connected feature extractor gradient reversal layer multiplies gradient certain negative constant backpropagation-based training. otherwise training proceeds standardly minimizes label prediction loss domain classiﬁcation loss gradient reversal ensures feature distributions domains made similar thus resulting domain-invariant features. predictor domain classiﬁer diﬀerence gradients class domain predictors subtracted instead summed since sgd— many variants adagrad adadelta main learning algorithm implemented libraries deep learning would convenient frame implementation stochastic saddle point procedure sgd. fortunately reduction accomplished introducing special gradient reversal layer deﬁned follows. gradient reversal layer parameters associated forward propagation acts identity transformation. backpropagation however takes gradient preceding layer. implementing layer using existing object-oriented packages deep learning simple requiring deﬁne procedures forward propagation implemented leads emergence features domain-invariant discriminative time. learning label predictor used predict labels samples target domain note release source code gradient reversal layer along usage examples extension caffe ﬁrst experiment section evaluate behavior simple version dann described subsection note results reported present subsection obtained using algorithm thus stochastic gradient descent approach consists sampling pair source target examples performing gradient step update parameters dann. crucially update regular parameters follows usual opposite direction gradient adversarial parameters step must follow gradient’s direction study adaptation capability dann comparing standard neural network experiments algorithms share network architecture hidden layer size neurons. train using procedure dann. keep updating domain regressor component using target sample disable adversarial back-propagation hidden layer. execute algorithm omitting lines numbered allows recovering learning algorithm—based source risk minimization equation without regularizer—and simultaneously train domain regressor equation discriminate source target domains. experience ﬁrst illustrate dann adapts decision boundary compared moreover also illustrate representation given hidden layer less adapted source domain task dann recall founding idea behind proposed algorithm. analysis experiment appears figure upper graphs relate standard lower graphs relate dann. looking lower upper graphs pairwise compare dann four diﬀerent perspectives described details below. column label classification figure shows decision boundaries dann problem predicting labels source target examples. expected accurately classiﬁes classes source sample fully adapted target sample contrary decision boundary dann perfectly classiﬁes examples source target samples. studied task dann clearly adapts target distribution. column representation studies domain adaptation regularizer aﬀects representation provided network hidden layer. graphs obsource target data points i.e. thus given trained network every point mapped -dimensional feature space hidden layer projected back two-dimensional plane transformation. dann-pca representation observe target points homogeneously spread among source points; nn-pca representation number target points belong clusters containing source points. hence labeling target points seems easier task given dann-pca representation. push analysis further graphs four crucial data points letters correspond moon extremities original space observe points close nn-pca representation clearly belong diﬀerent classes. happens points conversely four points opposite four corners dann-pca representation. note also target point —that diﬃcult classify original space—is located column domain classification shows decision boundary domain classiﬁcation problem given domain regressor equation precisely example classiﬁed source example classiﬁed domain example otherwise. remember that learning process dann regressor struggles discriminate source target domains hidden representation adversarially updated prevent succeed. without allowing inﬂuence learned representation hand dann domain regressor clearly fails generalize source target distribution topologies. hand domain regressor shows better generalization capability. inter alia seems roughly capture rotation angle target distribution. corroborates dann representation allow discriminating domains. ﬁfteen plot line corresponds coordinates i-th component observe standard neurons equals grouped three clusters allowing generate straight line zigzag decision boundary label classiﬁcation problem. however neurons also able capture rotation angle domain classiﬁcation problem. hence observe adaptation regularizer dann prevents kinds neurons produced. indeed striking predominant patterns neurons vanishing dann neurons. perform unsupervised domain adaption provide ways hyper-parameters unsupervised i.e. without referring labeled data target domain. following experiments sections select hyper-parameters algorithm using variant reverse cross-validation approach proposed zhong call reverse validation. split training sets respectively containing original examples) validation sets labeled unlabeled target learn classiﬁer then using algorithm learn reverse classiﬁer using self-labeled {)}x∈t unlabeled part target sample. finally reverse classiﬁer evaluated validation source sample. classiﬁer reverse validation risk process repeated multiple values hyper-parameters selected parameters corresponding classiﬁer lowest reverse validation risk. note train neural network architectures validation also used early stopping criterion learning self-labeled validation compare performance proposed dann algorithm standard neural network hidden layer described equation support vector machine linear kernel. compare algorithms amazon reviews data pre-processed chen data includes four domains composed reviews speciﬁc kind product reviews encoded dimensional feature vectors unigrams bigrams labels binary product ranked stars product ranked stars. perform twelve domain adaptation tasks. learning algorithms given labeled source examples unlabeled target examples. then evaluate separate target test sets note unlabeled target sample learning. original data part table shows target test accuracy algorithms table reports probability algorithm signiﬁcantly better others according poisson binomial test note dann signiﬁcantly better performance respective probabilities diﬀerence dann domain adaptation regularizer conclude approach successfully helps representation suitable target domain. investigate whether dann algorithm improve representation learned state-of-the-art marginalized stacked denoising autoencoders proposed chen brief msda unsupervised algorithm learns robust feature representation training samples. takes unlabeled parts source target samples learn feature input space representation space. denoising autoencoders algorithm ﬁnds feature representation reconstruct original features example noisy counterpart. chen showed using msda linear classiﬁer reaches state-of-the-art performance amazon reviews data sets. alternative propose apply shallow dann algorithm representations generated msda note that even msda dann representation learning approaches optimize diﬀerent objectives complementary. perform experiment amazon reviews data described previous subsection. source-target domain pair generate msda representations using corruption probability number layers execute three learning algorithms representations. precisely following experimental procedure chen concatenation output layers original input representation. thus example encoded vector dimensions. note grid results msda representation columns table conﬁrm combining msda dann sound approach. indeed poisson binomial test shows dann better performance probabilities respectively reported table note however standard best solution respectively second fourth tasks. suggests dann msda adaptation strategies fully complementary. theoretical foundation dann algorithm domain adaptation theory bendavid claimed dann ﬁnds representation source target example hardly distinguishable. experiment section already points evidence provide analysis real data. msda dann combined. recall described section metric estimating similarity source target representations. precisely obtain value following procedure construct data equation using source target representations training samples; randomly split subsets equal size; train linear svms ﬁrst subset using large range values; compute error obtained classiﬁers second subset lowest error compute value equation lastly figure presents sets results related section experiments. hand reproduce results chen noticed msda representations greater values original data. although msda approach clearly helps adapt target task seems contradict theory bendavid al.. hand observe that running dann msda obtained representations much lower values. observations might explain improvements provided dann combined msda procedure. perform extensive evaluation deep version dann number popular image data sets modiﬁcations. include large-scale data sets small images popular deep learning methods office data sets facto standard domain adaptation computer vision much fewer images. following baselines evaluated experiments subsection. source-only model trained without consideration target-domain data train-on-target model trained target domain class labels revealed. model serves upper bound methods assuming target data abundant shift domains considerable. addition compare approach recently proposed unsupervised method based subspace alignment simple setup test data sets also shown perform well experimental comparisons shallow methods. boost performance baseline pick important free parameter setting train source-only model consider activations last hidden layer label predictor descriptors/features learn mapping source target domains since baseline requires training classiﬁer adapting features order compared settings equal footing retrain last layer label predictor using standard linear four considered methods general compose feature extractor three convolutional layers picking exact conﬁgurations previous works. precisely four diﬀerent architectures used experiments. ﬁrst three shown figure office domains pre-trained alexnet caffe-package adaptation architecture identical tzeng hyper-parameters selected grid search small scale experiments section would computationally costly. instead learning rate adjusted stochastic gradient descent using following formula figure eﬀect adaptation distribution extracted features ﬁgure shows t-sne visualizations cnn’s activations case adaptation performed case adaptation procedure incorporated training. blue points correspond source domain examples ones correspond target domain. cases adaptation method makes distributions features much closer. t-sne projection visualize feature distributions different points network color-coding domains already observed shallow version dann strong correspondence discuss experimental settings results. case train source data test diﬀerent target domain data considerable shifts domains results summarized table table mnist mnist-m. ﬁrst experiment deals mnist data order obtain target domain blend digits original patches randomly extracted color photos bsds operation formally deﬁned images coordinates pixel channel index. words output sample produced taking patch photo inverting pixels positions corresponding pixels digit. human classiﬁcation task becomes slightly harder compared original data whereas trained mnist domain quite distinct background strokes longer constant. consequently source-only model performs poorly. approach succeeded aligning feature distributions successful adaptation results time improvement source-only model achieved subspace alignment quite modest thus highlighting diﬃculty adaptation task. synthetic numbers svhn. address common scenario training synthetic data testing real data street-view house number data svhn target domain synthetic digits source. latter consists images generated windowstm fonts varying text positioning orientation background stroke colors amount blur. degrees variation chosen manually simulate svhn however data sets still rather distinct biggest diﬀerence structured clutter background svhn images. proposed backpropagation-based technique works well covering almost training source data training target domain data known target labels. contrast results slight classiﬁcation accuracy drop indicating adaptation task even challenging case mnist experiment. mnist svhn. experiment increase distributions test mnist svhn signiﬁcantly diﬀerent appearance. training svhn even without adaptation challenging classiﬁcation error stays high ﬁrst epochs. order avoid ending poor local minimum therefore table classiﬁcation accuracies digit image classiﬁcations diﬀerent source target domains. mnist-m corresponds diﬀerence-blended digits nonuniform background. ﬁrst corresponds lower performance bound last corresponds training target domain data known class labels methods show much lower upper bounds covered cases approach outperforms fernando considerably covers portion gap. table accuracy evaluation diﬀerent approaches standard office data set. methods evaluated fullytransductive protocol method outperforms competitors setting state-of-the-art. feature distributions. observe quite strong separation domains feed trained solely mnist whereas svhn-trained network features much intermixed. diﬀerence probably explains method experiment except distribution features complex signiﬁcantly larger number classes source domain obtained synthetic images simulating various imaging conditions. target domain random training samples unsupervised adaptation rest evaluation. again method achieves sensible increase performance proving suitability synthetic-to-real data adaptation. additional experiment also evaluate proposed algorithm semi-supervised domain adaptation i.e. additionally provided small amount labeled target data. here reveal labeled examples training label predictor. figure shows change validation error throughout training. graph clearly suggests method beneﬁcial semi-supervised setting thorough veriﬁcation semi-supervised setting left future work. sets office rather small-scale labeled images spread across diﬀerent categories largest domain. amount available data crucial successful training deep model hence opted ﬁne-tuning pre-trained imagenet done recent works make approach comparable tzeng using exactly network architecture replacing domain mean-based regularization domain classiﬁer. following previous works assess performance method across three transfer tasks commonly used evaluation. training protocol adopted gong chopra long wang adaptation available labeled source examples unlabeled target examples also source domain data used training. fully-transductive setting method able improve previously-reported state-of-the-art accuracy unsupervised adaptation interestingly three experiments observe slight over-ﬁtting training progresses however doesn’t ruin validation accuracy. moreover switching domain classiﬁer branch makes eﬀect apparent conclude technique serves regularizer. section discuss application described adaptation method person re-identiﬁcation problem. task person re-identiﬁcation associate people seen diﬀerent camera views. formally deﬁned follows given sets images diﬀerent cameras person depicted probe image gallery image person probe image person gallery set. disjoint camera views diﬀerent illumination conditions various poses quality data make problem diﬃcult even humans unlike classiﬁcation problems discussed above re-identiﬁcation problem implies image mapped vector descriptor. distance descriptors used match images probe gallery set. evaluate results re-id methods cumulative match characteristic curve commonly used. plot identiﬁcation rate rank-k probability matching gallery image within closest images probe image. existing works train descriptor mappings evaluate within data containing images certain camera network similar imaging conditions. several papers however observed performance resulting re-identiﬁcation systems drops considerably descriptors trained data tested another. therefore natural handle cross-domain evaluation domain-adaptation problem camera network constitutes domain. figure matching non-matching pairs probe-gallery images diﬀerent person re-identiﬁcation data sets. three data sets treated diﬀerent domains experiments. recently several papers signiﬁcantly improved re-identiﬁcation performance presented reporting good results cross-data-set evaluation scenario. moment deep learning methods achieve state-of-the-art results probably limited size training sets. domain adaptation thus represents viable direction improving deep re-identiﬁcation descriptors. following prid viper cuhk target data sets experiments. prid data exists versions single-shot variant. contains images persons viewed camera images persons viewed camera persons appear cameras. viper data also contains images taken cameras total persons captured every person image camera views. cuhk data consists images pairs cameras images person cameras. refer subset data includes ﬁrst pair cameras cuhk/p figure samples data sets. perform extensive experiments various pairs data sets data serves source domain i.e. used train descriptor mapping supervised known correspondences probe gallery images. second data used target domain images data used without probe-gallery correspondence. detail cuhk/p used experiments cuhk serves target domain settings used experiments cuhk serves source domain. given prid target data randomly choose persons appearing camera views training set. images persons camera used probe images camera excluding used training used gallery test time. viper random persons training others testing. cuhk persons split training testing. unlike images ﬁrst pair cameras cuhk instead choosing image person camera view. also performed following augmented data mirror images test time calculate similarity score images mean four scores corresponding diﬀerent ﬂips compared images. case cuhk images camera views person combinations’ scores averaged. relu pooling fully-connected layer gives -dimensional descriptors output. three parallel ﬂows within processing three part image upper middle lower one. ﬁrst convolution layer shares parameters three parts outputs second convolution layers concatenated. training follow calculate pairwise cosine similarities -dimensional features within batch backpropagate loss pairs within batch. perform domain-adversarial training construct dann architecture. feature extractor includes convolutional layers discussed above. label predictor case replaced descriptor predictor includes fully-connected layer. domain classiﬁer includes fully-connected layers veriﬁcation loss function descriptor predictor used binomial deviance loss deﬁned similar parameters domain classiﬁer trained logistic loss subsection used learning rate ﬁxed momentum schedule adaptation similar described subsection used. also inserted dropout layer rate concatenation outputs second max-pooling layer. -sized batches used source data -sized batches target data. suﬃcient number iterations domain-adversarial training consistently improves performance re-identiﬁcation. pairs involve prid data dissimilar data sets improvement considerable. overall demonstrates applicability domain-adversarial learning beyond classiﬁcation problems. figure results viper prid cuhk/p without domain-adversarial learning. across eight domain pairs domain-adversarial learning improves reidentiﬁcation accuracy. domain pairs improvement considerable. paper proposes approach domain adaptation feed-forward neural networks allows large-scale training based large amount annotated data source domain large amount unannotated data target domain. similarly many previous shallow deep techniques adaptation achieved aligning distributions features across domains. however unlike previous approaches alignment accomplished standard backpropagation training. approach motivated supported domain adaptation theory ben-david main idea behind dann enjoin network hidden layer learn representation predictive source example labels uninformative domain input implement approach within shallow deep feed-forward architectures. latter allows simple implementation within virtually deep learning package introduction simple gradient reversal layer. shown approach ﬂexible achieves state-of-the-art convenient aspect approach domain adaptation component added almost neural network architecture trainable backpropagation. towards demonstrated experimentally approach conﬁned classiﬁcation tasks used feed-forward architectures e.g. descriptor learning person re-identiﬁcation. work supported national science engineering research council discovery grants well russian ministry science education grant rfmefix. computations performed colosse supercomputer grid universit´e laval auspices calcul qu´ebec compute canada. operations colosse funded nserc canada foundation innovation nanoqu´ebec fonds recherche qu´ebec nature technologies also thank graphics media faculty computational mathematics cybernetics lomonosov moscow state university providing synthetic road signs data set.", "year": 2015}