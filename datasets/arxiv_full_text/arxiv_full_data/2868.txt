{"title": "Composable Deep Reinforcement Learning for Robotic Manipulation", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "abstract": "Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.", "text": "fig. stack lego blocks together using maximum entropy reinforcement learning algorithm called soft qlearning. training policy scratch takes less hours learned policy extremely robust perturbations also demonstrate learned policies combined form compound skills stacking avoiding tower lego blocks hypothesize maximum entropy principle yield effective framework practical real-world deep reinforcement learning following properties. first maximum entropy policies provide inherent informed exploration strategy expressing stochastic policy boltzmann distribution energy corresponding reward-to-go q-function distribution assigns non-zero probability actions actions higher expected rewards likely sampled. consequence policy automatically direct exploration regions higher expected return. property thought soft combination exploration exploitation highly beneﬁcial real-world applications since provides considerably structure \u0001-greedy exploration shown experiments substantially improves sample complexity. second show paper independently trained maximum entropy policies composed together adding q-functions yielding policy combined reward function provably close corresponding optimal policy. composability controllers typically possible standard reinforcement learning especially important real-world applications reuse past experience greatly improve sample efﬁciency tasks naturally decomposed simpler sub-problems. instance policy pick-andplace task decomposed reaching speciﬁc x-coordinates reaching speciﬁc y-coordinates avoiding certain obstacles. decomposable policies therefore learned three stages yielding subpolicy later combined ofﬂine without need interact environment. abstract— model-free deep reinforcement learning shown exhibit good performance domains ranging video games simulated robotic manipulation locomotion. however model-free methods known perform poorly interaction time environment limited case real-world robotic tasks. paper study maximum entropy policies trained using soft q-learning applied real-world robotic manipulation. application method real-world manipulation facilitated important features soft q-learning. first soft q-learning learn multimodal exploration strategies learning policies represented expressive energy-based models. second show policies learned soft q-learning composed create policies optimality resulting policy bounded terms divergence composed policies. compositionality provides especially valuable tool real-world manipulation constructing policies composing existing skills provide large gain efﬁciency training scratch. experimental evaluation demonstrates soft q-learning substantially sample efﬁcient prior model-free deep reinforcement learning methods compositionality performed simulated real-world tasks. intersection expressive general-purpose function approximators neural networks general purpose model-free reinforcement learning algorithms used acquire complex behavioral strategies holds promise automating wide range robotic behaviors reinforcement learning provides formalism reasoning sequential decision making large neural networks provide representation principle used represent behavior minimal manual engineering. however applying model-free reinforcement learning algorithms multilayer neural network representations real-world robotic control problems proven difﬁcult practice sample complexity model-free methods tends quite high increased inclusion high-capacity function approximators. prior work sought alleviate issues parallelizing learning across multiple robots making example demonstrations training simulation relying accurate model enable transfer real world approaches carry additional assumptions limitations. instead devise model-free reinforcement learning algorithms efﬁcient enough train multilayer neural network models directly real world without reliance simulation demonstrations multiple robots? primary contribution paper framework based recently introduced soft q-learning algorithm learning robotic manipulation skills expressive neural network policies. illustrate framework provides efﬁcient mechanism learning variety robotic skills outperforms state-of-the-art model-free deep reinforcement learning methods terms sample efﬁciency real robotic system. empirical results indicate substantially outperforms deep deterministic policy gradient normalized advantage functions previously explored real world model-free robotic learning neural networks. also demonstrate novel extension algorithm enables composition previously learned skills present novel theoretical bound difference policy obtained composition optimal policy composed reward function applies reinforcement learning methods based soft optimality. experiments leverage compositionality maximum entropy policies simulated physical domains showing robust learning diverse skills outperforming existing stateof-the-art methods terms sample efﬁciency. crucial choices applying robotic manipulation choice representation. policies based dynamic movement primitives trajectory-centric representations splines particularly popular ease incorporating demonstrations stability relatively dimensionality however trajectory-centric policies limited expressive power particularly comes integrating rich sensory information reacting continuously environment. reason recent works sought explore expressive representations including deep neural networks cost higher sample complexity. number prior works sought address increased sample complexity employing model-based methods. example guided policy search learns model-based teacher supervises training deep policy network. works leverage simulation ghadirzadeh considers vision-based manipulation training perception behavior networks simulation learns low-dimensional intermediate layer real-world interaction. works learn vision-based policies completely simulation transfers real world approach work simulator needs model system accurately needs signiﬁcant variation appearance synthetic images order handle unavoidable domain-shift simulation real-world another common approach make reinforcement learning algorithms sample efﬁcient learn demonstrations comes cost additional instrumentation human supervision. perhaps closely related recent work aims apply deterministic model-free deep algorithms real-world robotic manipulation. used learn door opening reaching parallelizing across multiple real-world robots veˇcer´ık extended ddpg real-world robotic manipulation including example demonstrations. experiments show learn real-world manipulation proﬁciently fewer samples methods ddpg without requiring demonstrations simulated experience additional supervision. also show extended enable composition several previously trained skills. soft q-learning trains policies maximize reward entropy. maximum entropy policies discussed many contexts ranging applications inverse reinforcement learning connections kalman duality optimal control incorporation prior knowledge reinforcement learning recently several papers noted connection soft q-learning policy gradient methods prior works assume discrete action space nachum approximates maximum entropy distribution gaussian knowledge soft q-learning method approximates true maximum entropy policy expressive inference network essential tasks requiring diverse behaviour test time involving multimodal solutions. demonstrate that lead sample-efﬁcient real-world reinforcement learning also provide framework composing several previously trained policies initialize compound skills. derive novel bound section provides insight composition likely succeed. several prior works studied composition context soft optimality typically different framework instead considering composition reward functions constituent skills added considered settings soft maximum reward functions used reward composite skill argue former substantially useful latter robotic skills since allows decompose task multiple objectives need satisﬁed. soft q-learning extend work learning composable controllers real-world robotic manipulation based framework maximum entropy reinforcement learning section introduce formalism reinforcement learning describe soft q-learning algorithm proposed prior work discuss relates conventional reinforcement learning algorithms namely ddpg naf. action space assumed continuous unknown state transition probability represents probability density next state given current state current action environment emits bounded reward transition. denote state state-action marginals trajectory distribution induced policy drop time indices denote states actions consecutive time steps whenever inferred context. standard objective seeks policy maximizes e∼ρπ paper consider general maximum entropy objective favors stochastic policies augmenting objective expected entropy temperature parameter determines relative importance entropy term reward thus controls stochasticity optimal policy conventional objective recovered limit maximum entropy objective number conceptual practical advantages. first policy incentivized explore widely giving clearly unpromising avenues. second policy capture multiple modes optimal behavior. particular problem settings multiple actions seem equally attractive policy commit equal probability mass actions. lastly prior work observed substantially improved exploration property experiments observe considerably improves learning speed real-world robotic manipulation. wish extend objective inﬁnite horizon problems convenient also introduce discount factor ensure expected rewards entropies ﬁnite. writing precise maximum entropy objective inﬁnite horizon discounted case involved refer interested readers prior work using soft q-learning algorithm since knowledge existing deep algorithm continuous actions capture arbitrarily complex action distributions—a requirement policies composable discuss section soft q-learning optimizes q-function predict expected future return include future entropy values taking action state following optimal policy expressed terms optimal qfunction energy based model q-function takes role negative energy. unfortunately cannot evaluate action likelihoods since would require knowing partition function intractable general. however possible draw samples distribution resorting approximate sampling method. soft q-learning uses amortized stein variational descent learn stochastic neural network approximate samples desired main beneﬁt amortizing cost approximate sampling neural network producing samples test time amounts single forward pass fast enough done real-time. addition learning optimal policy also need learn optimal q-function. possible derive update rule maximum entropy objective resembles bellman backup used conventional qlearning. soft bellman operator updates q-function according possible show soft bellman backup contraction optimal q-function ﬁxed point iteration therefore transform bounded function optimal q-function iteratively applying soft bellman backup convergence. practice represent q-function parametric function approximator multilayer neural network therefore cannot perform soft bellman backup exact from. instead learn parameters minimizing squared soft bellman residual difference left right hand sides however make soft bellman backup section propose method composing policies existing policies. section brieﬂy introduce ddpg prior model-free reinforcement learning methods optimize conventional maximum return objective applied real-world robotic tasks discuss connection soft q-learning. deep deterministic policy gradient deep deterministic policy gradient frequently used prior method learns deterministic policy q-function jointly. policy trained maximize q-function whereas q-function trained represent expected return current policy. main difference soft q-learning ddpg replaces soft maximization approximate hard maximum estimated current policy. since ddpg uses deterministic policy exploration usually achieved adding independent noise policy output. contrast explicitly balances conventional approach solve compound tasks directly optimizing compound reward every possible combination unfortunately exponentially many combinations. compositionality makes learning process much faster instead training optimal policy reward later combining them. combine policies represents approximation true optimal q-function composed task extract policy approximate q-function using policy-extraction algorithm. conventional reinforcement learning without entropy regularization cannot make guarantees close however show next section that constituent policies represent optimal maximum entropy policies bound difference value approximate policy qπσc optimal value understand expect performance composed policy induced analyze value relates unknown optimal q-function corresponding composed reward simplicity consider special case compose optimal policies given q-functions reward functions extending proof policies values straightforward. start introducing lower bound optimal combined q-function terms ﬁxed point es∼p|sa) proof appendix lower bound tells simple additive composition q-functions never overestimates divergence constituent policies. interestingly constant obtained ﬁxed point conventional bellman equation divergence constituent policies acts reward function. thus exploration exploitation policy explore uncertain actions good take actions clearly sub-optimal leading informative exploration. normalized advantage functions normalized advantage functions consider q-function special form quadratic respect action. beneﬁt constraining form q-function allows closed form computation maximizer. therefore replaces soft maximum exact hard maximum. similarly policy represented deterministic maximum q-function making easy sample actions. contrast soft q-learning limited representational power cannot represent diverse multimodal solutions major motivation learning maximum entropy policies. compositionality powerful property naturally emerges maximum entropy reinforcement learning. compositionality means multiple policies combined create policy simultaneously solves tasks given constituent policies. feature desirable provides reusability enables quick initialization policies learning complex compound skills previously learned building blocks. related idea discussed todorov considers combining independent rewards soft maximization. however type composition corresponds solving constituent tasks time—a kind disjunction contrast approach composition corresponds conjunction tasks typically useful section discuss soft q-functions different tasks combined additively solve multiple tasks simultaneously. simply adding q-functions generally give q-function combined task show regret using policy obtained adding constituent q-functions together upper bounded difference policies composed. intuitively composed policies agree action indifferent towards other’s actions composed policy closer optimal one. multi-objective settings naturally emerge robotic tasks. example training robot move objects objective move objects quickly another objective avoid collisions wall person. generally assume tasks deﬁned reward functions interested solving subset tasks simultaneously. compound task expressed value adversarial policy seeks maximize divergence. bounded terms constituent q-functions necessarily mean composed policy high value. soft policy evaluation bound value composed policy qπσc ﬁxed point es∼p|sa) proof appendix analogously discussion lemma viewed ﬁxed point bellman equation role reward function. intuitively means bound becomes tight policies agree states visited composed policy. result show bound regret using policy formed composing optimal policies constituent rewards. bound likely quite loose indicate regret decreases divergence constituent policies decreases. interesting implications. first deterministic policies always inﬁnite divergence unless identical suggests poorly suited composition. highly stochastic policies produced maximum entropy algorithms like soft q-learning much lower divergences. intuitively policy wider distribution therefore policies overlap thus reducing regret composition. consequence expect maximum entropy methods produce much composable policies. section show soft q-learning obtain substantially better sample complexity existing modelfree deep methods. experiments demonstrate fast reliable learning simulated real-world robotic manipulation domains. also show compositionality maximum entropy policies discussed section provides practical tool composing compound skills previously trained components. evaluate method pushing task simulation well reaching lego block stacking combined stacking avoiding tasks real-world sawyer robot. videos experiments found website code available github. simulated tasks used mujoco physics engine real-world experiments used -dof sawyer robotic manipulator. actions torque commands joint observations joint angles angular velocities well end-effector position. simulated pushing tasks also include relevant object coordinates experiments sawyer include end-effector position forces estimated actuator currents observations. parameterize q-function policies -layer neural network units layer rectiﬁer linear activations. composing q-functions individual constituent policies allow quickly learn compound policies? answer question study compositionality policies simulated domain task move cylinder target location. simulated evaluation allows provide detailed comparison prior methods evaluating base performance performance additive composition. start learning qfunctions constituent tasks combining adding together q-functions extract policy requires training policy produce samples ddpg requires training network maximizes optimal action closed-form expression since constituent qfunctions quadratic actions. train combined ddpg policies reusing data collected training constituent policies thus imposing additional sample cost. constituent tasks simulated evaluation require planar push cylinder speciﬁc positions along axes. policy trained push disk speciﬁc position choose arbitrary position vice-versa. maximum entropy policy case would attempt randomly cover positions deterministic would pick arbitrarily. composed policy formed combining policy trained push cylinder speciﬁc position policy trained push cylinder speciﬁc location thus solving combined objective moving disk particular location table. illustration task depicted fig. shows compound policy formed combining q-function pushing disk blue line orange line ﬁnal disk locations episodes ﬁnal policies shown dots respective colors. green dots illustrate disk positions combined policy. note even though orange policy never moves disk point close intersection combined policy interpolates intended target correctly. trained four policies push cylinder following goals left middle right bottom using three algorithm goals gives three natural compositional objectives bottom-left bottom-middle bottom-right table summarizes error distance constituent policy policies trained optimize combined objective directly composed powerful tool allow efﬁcient build skills ones. next section complex real-world manipulation tasks substantially outperforms ddpg terms learning speed ﬁnal performance constituent policies trained also reused form compound skills real world. fig. comparison number iterations needed extract policy q-function training policy using off-policy on-policy data note x-axis scale. proposed method extracts policy additively composed q-function almost instantly ofﬂine fashion offpolicy data collected training constituent q-functions. contrast training policy scratch online takes orders magnitude gradient steps requires collecting experience environment. lastly training policy composed task using ofﬂine pre-collected data fails converge reasonable amount time test viability deep maximum entropy policies trained sawyer robot reaching different locations stacking lego blocks. also evaluated compositionality soft policies combining stacking policy policy avoids obstacle. reaching ﬁrst experiment explores compares ddpg terms sample complexity. trained robot move endeffector speciﬁed target location cartesian space. fig. shows learning curves three methods. experiment repeated three times analyze variability methods. since rely external exploration simply show training performance whereas ddpg show test performance regular intervals exploration noise switched provide fair comparison. solves task minutes whereas ddpg substantially slower exhibit large variation training runs. also trained policy reach randomly selected points provided input policy different target point selected episode. learning policy almost fast training ﬁxed target targets easier reach others policy exhibited larger variation terms ﬁnal error end-effector position. lego block stacking next experiment used train policy stacking lego blocks test ability exercise precise control presence contact dynamics goal position effector holds lego block position fig. independent policies trained push cylinder orange line blue line respectively. colored circles show samples ﬁnal location cylinder respective policies. policies combined resulting policy learns push cylinder lower intersection lines additional samples environment used train combined policy. combined policy learns satisfy original goals rather simply averaging ﬁnal cylinder location. task push left push middle push right push bottom push bottom-left merge bottom-left push bottom-middle merge bottom-middle push bottom-right merge bottom-right policies policies yielding lowest error individual tasks emphasized table. ddpg perform well across combined tasks whereas able combine policies better fails others note policies unimodal gaussian well suited compositions. also compared different methods learning composed task terms training time. example training curves targets shown fig. training policy given takes virtually time algorithms compared training q-function policy scratch fact combination policies closed form thus included graph. comparison also trained q-function scratch combined task ofﬂine fashion meaning samples collected policies trained individual tasks. however ofﬂine fails converge reasonable time. analysis shows that ddpg compound policies obtained quickly efﬁciently simply adding together q-functions individual constituent policies. bound suboptimality policies section ddpg analysis directly apply since ddpg limiting case nonetheless simple practical problems like experiments even ddpg q-functions composed reasonably. suggests composition fig. learning curve ddpg sawyer robot trained move effector speciﬁc location. learns much faster methods. also train reach randomly sampled end-effector locations concatenating desired location observation vector learns solve task quickly. curves show moving average training episodes. fig. lego stacking policy learned less hours. learned policy remarkably robust perturbations robot able recover successfully stack lego blocks together pushed state substantially different typical trajectories. orientation successfully stacks another block. reduce variance end-effectors pose augmented reward function additional negative log-distance term incentivizes higher accuracy block nearly inserted also included reward downward force overcome friction forces. half hour training robot able successfully insert lego block ﬁrst time hours policy fully converged. evaluated task visually inspecting four studs contact observed success rate trials ﬁnal policy. resulting policy solve task consistently also surprisingly robust disturbances. test robustness forced conﬁgurations states encounters normal execution policy able recover every time eventually solve task believe robustness attributed effective diverse exploration natural maximum entropy policies knowledge prior deep work shown similar results. composing policies last experiment evaluated compositionality sawyer robot. trained policy avoid ﬁxed obstacle combined policy stack lego blocks avoidance policy rewarded avoiding obstacle moving towards target block without shaping term required successfully stack blocks. evaluated avoidance policy stacking policy combined policy insertion task presence obstacle executing fig. illustrate compositionality physical hardware trained sawyer manipulator avoid obstacle stack lego blocks together stacking policy fails executed presence obstacle however combining policies combined skills solves tasks simultaneously able stack blocks avoiding obstacle. policy times counting number successful insertions. avoidance policy able bring lego block close target block never successfully stacked blocks together. stacking policy collided obstacle every time still able solve task time. hand combined policy successfully avoided obstacle stacked blocks time. experiment illustrates policy compositionality effective tool successfully employed also realworld tasks. paper discuss soft q-learning extended real-world robotic manipulation learning individual manipulation tasks learning constituent tasks composed policies. experiments show soft q-learning substantially outperforms prior model-free deep reinforcement learning. soft q-learning achieves substantially better performance simulated reaching task including case multiple policies composed reach locations outperforms ddpg real-world reaching task evaluated sawyer robot. method exhibits better stability convergence ability compose q-functions obtained soft q-learning make particularly useful studying composability maximum entropy policies derive bound error composed policy optimal policy composed reward function. bound suggests policies higher entropy easier compose. interesting avenue future work would study implications bound compositionality. example derive correction applied composed q-function reduce bias? answering questions would make practical construct robotic skills previously trained building blocks making easier endow robots large repertoires behaviors learned reinforcement learning. acknowledgements. thank haoran tang suggestions improve proof abhishek gupta providing simulated pusher environment. work supported berkeley deepdrive. holly lillicrap levine deep reinforcement learning robotic manipulation asynchronous off-policy updates robotics automation ieee international conference finn christiano abbeel levine connection between generative adversarial networks inverse reinforcement learning energy-based models arxiv preprint arxiv. veˇcer´ık hester scholz wang pietquin piot heess roth¨orl lampe riedmiller leveraging demonstrations deep reinforcement learning robotics problems sparse rewards arxiv preprint arxiv. peters schall reinforcement learning motor skills policy gradients neural networks vol. levine finn darrell abbeel end-to-end training deep visuomotor policies journal machine learning research vol. ghadirzadeh maki kragic bj¨orkman deep predictive policy training using reinforcement learning intelligent robots systems ieee/rsj international conference tobin fong schneider zaremba abbeel domain randomization transferring deep neural networks simulation real world intelligent robots systems ieee/rsj international conference ieee andrychowicz crow schneider fong welinder mcgrew tobin abbeel zaremba hindsight experience replay advances neural information processing systems james davison johns transferring end-to-end visuomotor control simulation real world multistage task corr vol. abs/. available http//arxiv.org/abs/. pastor hoffmann asfour schaal learning generalization motor skills learning demonstration robotics automation icra’. ieee international conference ieee theodorou buchli schaal reinforcement learning motor skills high dimensions path integral approach robotics automation ieee international conference ieee todorov general duality optimal control estimation ieee rawlik toussaint vijayakumar stochastic optimal control reinforcement learning approximate inference proceedings robotics science systems viii", "year": 2018}