{"title": "Determining the Number of Clusters via Iterative Consensus Clustering", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "We use a cluster ensemble to determine the number of clusters, k, in a group of data. A consensus similarity matrix is formed from the ensemble using multiple algorithms and several values for k. A random walk is induced on the graph defined by the consensus matrix and the eigenvalues of the associated transition probability matrix are used to determine the number of clusters. For noisy or high-dimensional data, an iterative technique is presented to refine this consensus matrix in way that encourages a block-diagonal form. It is shown that the resulting consensus matrix is generally superior to existing similarity matrices for this type of spectral analysis.", "text": "paper proposes solution fundamental problem using multiple algorithms multiple values determine appropriate value number clusters. begin brief theoretical motivation example provides intuition behind basic approach. follow discussion results real datasets demonstrate eﬀectiveness iterated approach. matrix column data. particular implementation iterated consensus clustering approach outlined herein assume data nonnegative relatively noisy. neither conditions necessary general scheme preferred algorithms dimension reduction nonnegative matrix factorization which name suggests requires nonnegative data. main focus falls realm document clustering demonstrate method works equally well types data. document clustering data matrix term-by-document matrix represents frequency term document data normalized weighted according term-weighting schemes like described matrix symmetric matrix pairwise similarities data measures notion similarity many clustering algorithms particularly spectral variety rely similarity matrix cluster data points many types similarity functions exist commonly used function literature gaussian similarity function parameter cluster ensemble determine number clusters group data. consensus similarity matrix formed ensemble using multiple algorithms several values random walk induced graph deﬁned consensus matrix eigenvalues associated transition probability matrix used determine number clusters. noisy high-dimensional data iterative technique presented reﬁne consensus matrix encourages block-diagonal form. shown resulting consensus matrix generally superior existing similarity matrices type spectral analysis. ensemble methods used various data mining ﬁelds improve performance single algorithm combine results several algorithms. data clustering strategies implemented techniques commonly referred consensus methods since single algorithm work best given class data natural approach several algorithms solve clustering problems. however vast majority clustering algorithms literature require user specify number clusters algorithm create. applied data mining problem unusual user know information before hand. fact number distinct groups user. discuss similarity matrix known consensus matrix section goal clustering create clusters objects high intra-cluster similarity inter-cluster similarity. thus similarity matrix rows columns ordered cluster nearly block-diagonal structure. nearly uncoupled markov chains similarity matrix viewed adjacency matrix nodes undirected graph. data points nodes graph edges drawn nodes weights similarity matrix. figure illustrates graph using thickness edge indicate weight. edges exist nodes separate clusters expect weights edges less weights within clusters. distribution markov chain given represents reversible markov chain satisﬁes detailed balance equations condition guarantees eigenvalues real since q/pq−/ q−/pt indicates similar trix q/pq−/ precisely normalized laplacian matrix used many spectral clustering algorithms computational considerations symmetric matrix compute spectrum algorithm. precise deﬁnition nearly uncoupled markov chain real eigenvalues exactly eigenvalues near number blocks diagonal. cluster eigenvalues near known perron cluster moreover decomposition diagonal blocks relatively large eigenvalues expected previously suggested observed determine number clusters data however demonstrate section common similarity matrices literature impart level uncoupling necessary visible perron cluster. main goal algorithm construct nearly uncoupled markov chain using similarity matrix cluster ensemble. next section fully motivates approach. build similarity matrix using results several diﬀerent clustering algorithms. previously mentioned clustering algorithms require user input number desired clusters. choose values denoted algorithms partition data clusters result clusterings. clusterings recorded consensus matrix setting equal number times observation clustered observation matrix become popular ensemble methods example observe motivate approach we’ll look brief fabricated example. vertices graph figure clearly separated clusters. figure illustrates diﬀerent clusterings points consensus similarity matrix resulting clusterings ﬁrst eigenvalues transition probability matrix sorted magnitude. using incorrect guess clusterings correct value discovered counting number eigenvalues perron cluster. authors chosen four diﬀerent algorithms form consensus matrix principal direction divisive partitioning k-means expectation-maximization gaussian mixtures round clustering k-means twice initialized randomly initialized centroids clusters found pddp. latter hybrid pddp-k-means considered algorithm. text data sets clustering symmetric matrices spherical k-means used opposed euclidean kmeans. three diﬀerent dimension reductions used alter data input algorithms. motivation focused along three objectives. ﬁrst objective merely reduce size data matrix speeds computation time clustering algorithms. second objective reduce noise data. ﬁnal objective decompose data components reveal underlying patterns features. ﬁrst dimension reduction ever popular principal components analysis second dimension reduction simple truncated singular value decomposition larson’s propack software eﬃciently compute third dimension reduction nonnegative matrix factorization algorithm used alternating constrained least squares algorithm sparsity parameters dimension reduction techniques require user input level dimension reduction choices parameter provide hundreds diﬀerent clusterings single algorithm. here choose three diﬀerent values smaller datasets feasible compute complete svd/pca data matrix chosen number principal components required capture variance data respectively. require values unique. larger document datasets illustrated figure section discusses adjustments algorithm meant combat eﬀect noise large datasets particularly document sets. document clustering although underlying topics deﬁne individual clusters quite distinct spatial concept well-separated clusters becomes convoluted uncoupling beneﬁt drop tolerance clear graph figure beneﬁt iteration apparent reader result visualized. next section noisy datasets illustrate. high dimensions. thus nearly uncoupled structure depicted figure rare practice. adjustments presented section meant reﬁne data iterative encourage uncoupling. drop tolerance necessarily similarity documents diﬀerent clusters. result clustering algorithms make errors. however algorithms independent reasonable expect majority algorithms make error. introduce drop iteration basic algorithm previously outlined several clusterings transform original data matrix similarity matrix rows/columns essentially variables describing original observations. thus used data input clustering algorithms procedure iterated follows documents clustered using clusters drop tolerance david gleich’s vismatrix tool allows visualize matrices heat maps. figure observe diﬀerence consensus matrix prior iteration iterations. non-zero entry matrix represented colored pixel. colorbar right indicates magnitude entries color. iterations magnitude intracluster similarites clearly larger magnitudes inter-cluster similarities noticeably diminished. note strong similarities clusters weaker clearly visible contains correct number eigenvalues. furthermore eigenvalue belonging perron-cluster smaller magnitude others type eﬀect eigenvalues cause user consider subclustering situation like caused topics labels atheism misc. religion topic could clearly considered subtopic another. complete discussion text dataset present figure eigenvalue plots markov chains induced similarity matrices cosine matrix gaussian matrix. evident illustration measures similarity fail yield nearly uncoupled markov chain. pendigits pendigits dataset subset used consists coordinate observations made handwritten digits. roughly instances digits drawn writers. considered diﬃcult dataset similarity digits number ways draw each. complete pendigits similarities clusters meaningful subcluster structure document collection categorical topics clusters atheism misc. religion respectively topics clusters computersgraphics computersos windows misc. fact beautiful aspects algorithm ability detect subcluster structure data. figure visualize uncoupling eﬀect iteration observing diﬀerence eigenvalues transition probability matrix. prior iteration perron cluster eigenvalues apparent still much inter-cluster noise matrix. however iterations perron cluster dataset available machine learning repositiory experiments used sequence drop-tolerance seen figure perron-cluster convincing prior iteration system almost completely uncoupled iterations. purposes comparison observe eigenvalues transition probability matrices associated graphs deﬁned cosine similarity matrix gaussian similarity matrix. figure clear similarity matrices inadequate determining number clusters. agblog undirected hyperlink network mined political blogs. dataset used described contains clusters pertaining liberal conservative division. algorithms clusters drop tolerance resulting eigenvalue plots displayed figure figure displays eigenvalues markov chain imposed similarity matrix used simply original hyperlink matrix. plot particularly interesting contain appears cation type analysis suggests communities dataset. encourage readers check visualizations graph www.ncsu.edu/∼slrace support hypothesis. believe uncoupling fact rather isolated blogs link sites frequency others example warns outliers misleading eﬀect eigenvalues aﬃnity matrix. unreasonable expect graph clustering algorithm like normalized power iteration jordan weiss accurately divide graph clusters eigenvalues associated markov chain indicate eleven potential groups formed rather two. fact accuracy purity clusters found algorithms increase dramatically consensus matrix used place original matrix. table shows purity clusterings found three spectral algorithms compared consensus matrix makes true clusters obvious ﬁrst algorithms little eﬀect third. cluster solutions three algorithms consensus matrix identical. type agreement important practice gives user additional level conﬁdence cluster solution paper demonstrates eﬀectiveness iterated consensus clustering task determining number clusters dataset. main contribution formation consensus matrix multiple algorithms dimension reductions without prior knowledge consensus matrix superior similarity matrices determining nearly uncoupled structure associated graph. graph initial consensus matrix nearly uncoupled adjustments iteration drop tolerance outlined section encourage structure. number clusters known previously shown obtain excellent clustering results encouraging underlying algorithms agree upon common solution iteration demonstrated using consensus similarity matrix instead existing similarity matrices stefano monti pablo tamayo jill mesirov todd golub. consensus clustering resampling-based method class discovery visualization gene expression microarray data. machine learning clustering dimensionreduction algorithm aggregation. master’s thesis north carolina state university iterated consensus clustering. thesis north carolina state university ﬂexible exploratory method determining number clusters. framework adapted clustering algorithms dimension reductions preferred user. ﬂexibility allows scalability given computation time method dependent upon computation time algorithms used. drop tolerance changed reﬂect conﬁdence user chosen clustering algorithms based upon level noise data. range values speciﬁed level dimension reduction also changed purposes investigation.", "year": 2014}