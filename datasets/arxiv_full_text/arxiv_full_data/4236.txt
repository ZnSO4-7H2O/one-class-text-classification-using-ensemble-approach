{"title": "Saliency-based Sequential Image Attention with Multiset Prediction", "tag": ["cs.CV", "cs.AI"], "abstract": "Humans process visual scenes selectively and sequentially using attention. Central to models of human visual attention is the saliency map. We propose a hierarchical visual architecture that operates on a saliency map and uses a novel attention mechanism to sequentially focus on salient regions and take additional glimpses within those regions. The architecture is motivated by human visual attention, and is used for multi-label image classification on a novel multiset task, demonstrating that it achieves high precision and recall while localizing objects with its attention. Unlike conventional multi-label image classification models, the model supports multiset prediction due to a reinforcement-learning based training process that allows for arbitrary label permutation and multiple instances per label.", "text": "humans process visual scenes selectively sequentially using attention. central models human visual attention saliency map. propose hierarchical visual architecture operates saliency uses novel attention mechanism sequentially focus salient regions take additional glimpses within regions. architecture motivated human visual attention used multi-label image classiﬁcation novel multiset task demonstrating achieves high precision recall localizing objects attention. unlike conventional multi-label image classiﬁcation models model supports multiset prediction reinforcement-learning based training process allows arbitrary label permutation multiple instances label. humans rapidly process complex scenes containing multiple objects despite limited computational resources. visual system uses various forms attention prioritize selectively process subsets vast amount visual input computational models various forms psychophysical neuro-biological evidence suggest process implemented using various \"maps\" topographically encode relevance locations visual ﬁeld models visual input compiled saliency-map encodes conspicuity locations based bottom-up features computed parallel feed-forward process top-down goal-speciﬁc relevance locations incorporated form priority used select next target attention thus processing scene multiple attentional shifts interpreted feed-forward process followed sequential recurrent stages furthermore allocation attention separated covert attention deployed regions without movement precedes movements overt attention associated movement despite evident importance human visual attention notions incorporating saliency decide attentional targets integrating covert overt attention mechanisms using multiple sequential shifts processing scene fully addressed modern deep learning architectures. motivated model itti propose hierarchical visual architecture operates saliency computed feed-forward process followed recurrent process uses combination covert overt attention mechanisms sequentially focus relevant regions take additional glimpses within regions. propose novel attention mechanism implementing covert attention. here architecture used multi-label image classiﬁcation. unlike conventional multi-label image classiﬁcation models model perform multiset classiﬁcation proposed reinforcement-learning based training. ﬁrst introduce relevant concepts biological visual attention contextualize work deep learning related visual attention saliency hierarchical reinforcement learning observe current deep learning models either exclusively focus bottom-up feed-forward attention overt sequential attention saliency traditionally studied separately object recognition. visual attention classiﬁed covert overt components. covert attention precedes movements intuitively used monitor environment guide movements salient regions particular functions covert attention motivate gaussian attention mechanism proposed below noise exclusion modiﬁes perceptual ﬁlters enhance signal portion stimulus mitigate noise; distractor suppression refers suppressing representation strength outside attention area inspiring proposed attention mechanism evidence cueing multiple object tracking fmri studies indicate covert attention deployed multiple disjoint regions vary size conceptually viewed multiple \"spotlights\". overt attention associated movement attentional focus coincides fovea’s line sight. planning movements thought inﬂuenced bottom-up saliency well top-down factors particular major view types maps saliency priority encode measures used determine target attention view visual input processed feature-agnostic saliency quantiﬁes distinctiveness location relative locations scene based bottom-up properties. saliency integrated include top-down information resulting priority map. saliency initially proposed koch ullman implemented computational model itti model saliency determined relative feature differences compiled \"master saliency map\". attentional selection consists directing ﬁxed-sized attentional region area highest saliency i.e. \"winner-take-all\" process. attended location’s saliency suppressed process repeats multiple attentional shifts occur following single feed-forward computation. subsequent research effort directed ﬁnding neural correlates saliency priority map. proposed areas salience computation include superﬁcial layers superior colliculus inferior sections pulvinar priority computation include frontal ﬁeld deeper layers superior colliculus here need assume existence maps conceptual mechanisms involved inﬂuencing visual attention refer reader recent review. explore aspects itti’s model within context modern deep learning-based vision bottom-up featureless saliency guide attention sequential shifting attention multiple regions. furthermore model incorporates top-down signals bottom-up saliency create priority includes covert overt attention mechanisms. visual attention major area interest deep learning; existing work separated sequential attention bottom-up feed-forward attention. sequential attention models choose series attention regions. larochelle hinton used classify images sequence fovea-like glimpses recurrent attention model mnih posed single-object image classiﬁcation reinforcement learning problem policy chooses sequence glimpses maximizes classiﬁcation accuracy. \"hard attention\" mechanism developed since widely used notably extension multiple objects made dram model dram limited datasets natural label ordering svhn recently cheung developed variable-sized glimpse inspired biological vision incorporating simple single object recognition. fovea-like attention shifts based task-speciﬁc objectives models seen overt top-down attention mechanisms. alternative approach alter structure feed-forward network convolutional activations modiﬁed image moves network i.e. bottom-up fashion. spatial transformer networks learn parameters transformation effect stretching rotating cropping activations layers. progressive attention networks learn attention ﬁlters placed layer progressively focus arbitrary subset input residual attention networks learn feature-speciﬁc ﬁlters. here consider attentional stage follows feed-forward stage i.e. saliency image representation produced feed-forward stage attention mechanism determines parts image representation relevant using saliency map. saliency typically studied context saliency modeling model outputs saliency image matches human ﬁxation data salient object segmentation separately several works considered extracting saliency understanding classiﬁcation network decisions zagoruyko formulate loss function causes student network similar \"saliency\" teacher network. model saliency reduction operation rc×h×w rh×w applied volume convolutional activations adopt simplicity. here investigate using saliency downstream task. recent work begun explore saliency maps inputs prominent object detection image captioning pointing uses saliency-based vision models. focus using reinforcement learning multiset prediction class labels annotation applied computer vision tasks including modeling movements based annotated human scan paths optimizing prediction performance subject computational budget describing classiﬁcation decisions natural language object detection finally architecture inspired works hierarchical reinforcement learning. model distinguishes upper level task choosing image region focus lower level task classifying object related region. tasks handled separate networks operate different time-scales upper level network specifying task lower level network. hierarchical modularity relates meta-controller controller architecture kulkarni feudal reinforcement learning here apply hierarchical architecture multi-label image classiﬁcation levels linked differentiable operation. architecture hierarchical recurrent neural network consisting main components meta-controller controller. components assume access saliency model produces saliency image activation model produces activation volume image. figure shows high level components appendix shows detailed views overall architecture individual components. short given saliency meta-controller places attention mask object controller takes subsequent glimpses classiﬁes object. saliency updated account processed locations process repeats. meta-controller controller operate different time-scales; step meta-controller controller takes steps. notation denote space images rhi×wi denote labels. denote space saliency maps rhs×ws denote space activation volumes rc×hv denote space covert attention masks rhm×wm denote space priority maps rhm×wm denote action space. activation model function mapping input image activation volume. example volume activation tensor ﬁnal conv layer resnet. meta-controller meta-controller function mapping saliency covert attention mask. here recurrent neural network deﬁned follows concatenation ﬂattened saliency one-hot encoding previous step’s class label prediction attn novel spatial attention mechanism deﬁned below. mask transformed interface layer priority directs controller’s glimpses towards salient region used produce initial glimpse vector controller. gaussian attention mechanism spatial attention mechanism inspired covert visual attention discrete convolution mixture gaussians ﬁlter. speciﬁcally attention mask matrix denotes number gaussian components importance width center component implement mechanism parameters output network layer k-dimensional vector elements transformed proper ranges softmax exp. formed applying coordinates note operations differentiable allowing attention mechanism used module network trained back-propagation. graves proposed version; version spatial attention. interface interface layer transforms meta-controller’s output priority glimpse vector used input controller priority combines top-down covert attention mask bottom-up saliency since inﬂuences region processed next also seen generalization \"winner-take-all\" step itti model; learned function chooses region high saliency rather greedily choosing maximum location. provide initial glimpse vector controller mask used spatially weight mijv·ij interpreted meta-controller taking initial possibly broad variable-sized glimpse using covert attention. weighting produced attention retains activations around centers attention down-weighting outlying areas effectively suppressing activations noise outside attentional area. since activations averaged single vector trade-off attentional area information retention. controller controller recurrent neural network runs steps maps priority initial glimpse vector interface layer parameters distribution action sampled. ﬁrst actions select spatial indices activation volume ﬁnal action chooses class label i.e. a...k speciﬁcally indexes meta-controller time-step indexes controller time-step action sampled categorical distribution parameter vector glimpse vectors formed extracting column activation volume location intuitively controller uses overt attention choose glimpse locations using information conveyed priority initial glimpse compiling information hidden state make classiﬁcation decision. recall covert attention priority maps known inﬂuence saccades appendix diagram. update mechanism step meta-controller takes saliency input focuses region using attention mask controller takes glimpses locations step saliency reﬂect fact regions already attended order encourage attending novel areas. metacontroller’s hidden state principle prevent repeatedly focusing regions explicitly update saliency function update suppresses saliency glimpsed locations locations nonzero attention mask values thereby increasing relative saliency remaining unattended regions mechanism motivated inhibition return effect human visual system; attention removed region increased response time stimuli region inﬂuence visual search encourage attending novel areas saliency model saliency model function mapping input image saliency map. here saliency model computes compressing activation volume using |vi| output activation model furthermore activation model ﬁne-tuned single-object dataset containing classes found multi-object dataset saliency model high activations around classes interest. multi-label classiﬁcation tasks categorized based whether labels lists sets multisets. claim multiset classiﬁcation closely resembles human’s free viewing scene; exact labeling order objects vary individual multiple instances object appear scene receive individual labels. speciﬁcally dataset images labels consider structure list-based classiﬁcation labels consistent order e.g. left right. sequential prediction problem exactly true label prediction step standard cross-entropy loss used prediction step labels y|yi|} approach sequential prediction impose ordering preprocessing step transforming set-based problem list-based problem. instance order labels based prevalence training data finally multiset classiﬁcation generalizes set-based classiﬁcation allow duplicate labels within example i.e. here propose training process allows duplicate labels permutation-invariant respect labels removing need hand-engineered ordering supporting three types classiﬁcation. saliency-based model permutation invariance labels especially crucial since salient object correspond ﬁrst label. solution frame problem terms maximizing non-smooth reward function encourages desired classiﬁcation attention behavior reinforcement learning maximize expected reward. assuming access trained saliency model activation model meta-controller controller jointly trained end-to-end. reward support multiset classiﬁcation propose multiset-based reward controller’s classiﬁcation action. speciﬁcally consider image labels ym}. metacontroller step multiset available labels corresponding class scores output controller. deﬁne softmax) short class label sampled controller controller receives positive reward label multiset available labels. label removed available labels. clearly reward sampled labels equals reward permutation elements. note list-based tasks supported setting controller’s location-choice actions simply receive reward equal priority value glimpse location encourages controller choose locations according priority map. locations sampled controller deﬁne riloc objective ...n index example ...m index meta-controller step ...k index controller step. goal choosing maximize total expected reward rewards rnti deﬁned above expectation distribution trajectories produced using model parameterized unbiased gradient estimator obtained using reinforce estimator within stochastic computation graph framework schulman follows. viewed stochastic computation graph input saliency passes path deterministic nodes reaching controller. controller’s steps produces categorical parameter vector pnti stochastic node introduced sampling operation pnti. ptirti stochastic computation graph. corollary gradient gives unbiased gradient estimator objective approximated using monte-carlo sampling standard reinforcement learning state-value function used rti). implementation controller outputs state-value estimate controller’s hidden state. validate classiﬁcation performance training process hierarchical attention set-based multiset-based classiﬁcation experiments. test effectiveness permutation-invariant training compare baseline model uses cross-entropy loss probabilities labels instead training similar training proposed datasets synthetic datasets mnist mnist multiset well real-world svhn dataset used. mnist multiset image dataset variable number digits varying sizes positions along cluttering objects introduce noise. label image mnist unique mnist multiset images contain duplicate labels. dataset split training examples testing examples metrics reported testing set. svhn multiset consists svhn examples label order randomized batch sampled. removes natural left-to-right order svhn labels thus turning classiﬁcation multiset task. evaluation metrics evaluate classiﬁcation performance macro-f exact match deﬁned used. evaluating hierarchical attention mechanism visualization well saliency metric controller’s glimpses deﬁned attnsaliency controller trajectory meta-controller time step averaged time steps examples. high score means controller tends pick salient points glimpse locations. implementation details activation saliency model resnet- network pre-trained imagenet. mnist experiments resnet ﬁne-tuned single object mnist dataset svhn ﬁne-tuned randomly selecting image’s labels time batch sampled. images resized ﬁnal convolutional layer used since label sets vary size model trained extra \"stop\" class inference greedy argmax sampling used \"stop\" class predicted. appendix details. section analyze model’s classiﬁcation performance contribution proposed training behavior hierarchical attention mechanism. classiﬁcation performance table shows evaluation metrics set-based multiset-based classiﬁcation tasks proposed hierarchical saliency-based model training cross-entropy baseline introduced above. hsal-rl performs well across metrics; multiset tasks model achieves high precision recall macro-f scores expected multiset task difﬁcult. conclude proposed model training process effective multiset image classiﬁcation tasks. contribution training seen table performance greatly reduced standard cross-entropy training used invariant label ordering. shows importance training assumes predictions permutation labels. controller attention based attnsaliency controller learns glimpse salient regions often training progresses starting ending baseline reward signal glimpses fails improve training demonstrating importance effectiveness controller’s glimpse rewards. hierarchical attention visualization figure visualizes hierarchical attention mechanism three example inference processes. appendix examples discuss here. general upper level attention highlights region encompassing digit lower level figure inference process showing hierarchical attention three different examples. column represents single meta-controller step controller glimpses classiﬁcation. glimpses near digit classifying. notice saliency update time priority map’s structure gaussian attention mechanism variable-sized focus priority followed ﬁner-grained glimpses. note predicted labels need order ground truth labels model predict multiple instances label illustrating multiset prediction. cases upper level attention sufﬁcient classify object without controller taking related glimpses glimpses blank region covert attention initially placed controller focuses interpreted using multiple spotlight capability covert attention directing overt attention single target. saliency input since saliency top-level input quality saliency model potential performance bottleneck. figure shows general guarantee objects interest high saliency relative locations around them. however modular architecture allows plugging alternative rigorously evaluated saliency models state-of-the-art saliency model trained human ﬁxation data activation resolution currently activation model returns highest-level convolutional activations spatial dimension image. consider case shown figure even controller acted optimally activations multiple digits would included glimpse vector resolution. suggests activations higher spatial resolution needed perhaps incorporating dilated convolutions using lower-level activations attended areas motivated covert attention’s known enhancement spatial resolution proposed novel architecture attention mechanism rl-based training process sequential image attention supporting multiset classiﬁcation. proposal ﬁrst step towards incorporating notions saliency covert overt attention sequential processing motivated biological visual attention literature deep learning architectures downstream vision tasks.", "year": 2017}