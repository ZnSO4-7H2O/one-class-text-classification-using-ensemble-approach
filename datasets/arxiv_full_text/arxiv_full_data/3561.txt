{"title": "Logo Synthesis and Manipulation with Clustered Generative Adversarial  Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Designing a logo for a new brand is a lengthy and tedious back-and-forth process between a designer and a client. In this paper we explore to what extent machine learning can solve the creative task of the designer. For this, we build a dataset -- LLD -- of 600k+ logos crawled from the world wide web. Training Generative Adversarial Networks (GANs) for logo synthesis on such multi-modal data is not straightforward and results in mode collapse for some state-of-the-art methods. We propose the use of synthetic labels obtained through clustering to disentangle and stabilize GAN training. We are able to generate a high diversity of plausible logos and we demonstrate latent space exploration techniques to ease the logo design task in an interactive manner. Moreover, we validate the proposed clustered GAN training on CIFAR 10, achieving state-of-the-art Inception scores when using synthetic labels obtained via clustering the features of an ImageNet classifier. GANs can cope with multi-modal data by means of synthetic labels achieved through clustering, and our results show the creative potential of such techniques for logo synthesis and manipulation. Our dataset and models will be made publicly available at https://data.vision.ee.ethz.ch/cvl/lld/.", "text": "designing logo brand lengthy tedious back-and-forth process designer client. paper explore extent machine learning solve creative task designer. this build dataset logos crawled world wide web. training generative adversarial networks logo synthesis multi-modal data straightforward results mode collapse state-of-the-art methods. propose synthetic labels obtained clustering disentangle stabilize training. able generate high diversity plausible logos demonstrate latent space exploration techniques ease logo design task interactive manner. moreover validate proposed clustered training cifar achieving state-of-theart inception scores using synthetic labels obtained clustering features imagenet classiﬁer. gans cope multi-modal data means synthetic labels achieved clustering results show creative potential techniques logo synthesis manipulation. dataset models made publicly available https//data.vision.ee.ethz. ch/cvl/lld/. figure original generated images four selected clusters lld-icon-sharp dataset. three rows consist original logos followed logos generated using iwgan-lc trained clusters. goal work provide framework towards system ability generate inﬁnitely many variations logos facilitate expedite process. prospective client able modify prototype logo according speciﬁc parameters like shape color shift certain amount towards characteristics anprototype. example interface system presented figure could help designer client idea potential logo designer could build upon even system able output production-quality designs. logo design designing logo brand usually lengthy tedious process client designer. ultimately unused drafts produced client selects favorites followed multiple cycles reﬁning logo match clients needs logo image data existing research literature focused mostly retrieval detection recognition reduced number logos consequently number datasets introduced. representative large public logo datasets shown tafigure logo generator interface. user able choose either vicinity sampling class transfer modify image chosen semantic direction. methods random variations arranged around current logo. upon selecting appropriate sample current logo modiﬁed variable amount using slider bottom window. conﬁrming selected modiﬁcation process starts newly modiﬁed logo desired appearance reached. addition vicinity sampling within across clusters pre-deﬁned semantic modiﬁcations made using sliders right hand side ﬁrst view. images used generated iwgan-lc trained pixels lld-logo clustered different classes explained section diversity contained logos datasets suitable learning validating automatic logo generators. time number pages allow access large number icons iconsdb.com icons.com iconﬁnder.com iconarchive.com thenounproject.com however diversity icons limited number sources namely designers/artists themes design patterns therefore crawl highly diverse dataset large logo dataset real logos wild’ internet. shown table proposes thousands times distinct logos largest public logo dataset date weblogo-m contrast popularly used natural image datasets imagenet cifar- lsun face datasets like celeba relatively easily modeled handwritten digits mnist logos artiﬁcial strongly multimodal thus challenging generative models; applied obvious real-world demand synthetically generated unique logos since expensive produce; hard label categorical properties manifest logo’s visual appearance. logos easily obtainable large quantities speciﬁcally designed unique ensures diversity large logo dataset. argue characteristics make logos attractive domain machine learning research generative models recent advances generative modeling provided viable frameworks making system possible. current state-of-the-art made mainly types generative models namely variational autoencoders generative adversarial networks models generate images high-dimensional latent space sort design space user able modify output structured way. vaes advantage directly providing embeddings given image latent space allowing targeted modiﬁcations reconstruction tend suffer blurry output owed nature pixel-wise loss used training. gans hand consist separate generator discriminator network trained simultaneously opposing objectives competitive manner known provide realistic looking crisp images notoriously unstable train. address difﬁculty number improvements architecture training methods gans suggested using deep convolutional layers modiﬁed loss functions e.g. based least-squares wasserstein distance probability distributions inception generating mnist digits conditioned class labels provided generator discriminator training. since shown supervised datasets class-conditional variants generative networks often produce superior results compared unconditional counterparts adding encoder real image latent space proven feasible generate modiﬁed version original image changing class attributes faces natural images notable applications include generation images high-level description various visual attributes text descriptions novel dataset logo images. methods successfully train models multimodal data. proposed clustered training achieves state-of-the-art inception scores cifar dataset. remainder paper structured follows. introduce novel large logo dataset section describe proposed clustered training clustering methods well architectures used perform quantitative experiments section demonstrate logo synthesis latent space exploration operations section finally draw conclusions section pixel) favicon subset higher-resolution twitter subset following brieﬂy describe acquisition properties possible use-cases each. versions made available https//data.vision.ee.ethz.ch/ cvl/lld/. lld-icon favicons generative models like gans difﬁculty keeping network stable training increases image resolution. thus starting work type data makes sense start variant inherently low-resolution. luckily domain logo images category inherently low-resolution lowcomplexity images favicons small icons representing website e.g. browser tabs favorite lists. decided crawl favicons using largest resource high quality website urls could alexa’s million website list. python package scrapy conjunction download script directly converts icons found standardized pixel resolution color space discarding nonsquare images. acquiring data remove exact duplicates visual inspection data reveals non-negligible number images comply initial dataset criteria often even remotely logo-like faces natural images. attempt unwanted data sort images png-compressed size image complexity indicator; manually inspect partition resulting sorted list three sections clean mostly clean data kept mostly unwanted data discarded; discard mostly clean images containing least amount white pixels. training generative networks increased resolution additional high-resolution data needed favicons cannot provide. possible option would crawl respective websites directly look website company logo. however might always straightforward logo distinguish images website aspect ratio resolution logos obtained varied would necessitate extensive cropping resizing potentially degrading quality large portion logos. crawling twitter instead websites able acquire standardized square pixel proﬁle images easily downloaded twitter without need scraping. python wrapper tweepy search domain names contained alexa list match original website provided twitter proﬁle make sure found right twitter user. images face detector reject personal twitter accounts remaining images saved together twitter meta data user name number followers description. part dataset original resolutions kept as-is pixels rest lower resolution acquired images analyzed sorted combination automatic manual processing order unwanted possibly sensitive images resulting usable high-resolution logos consistent quality rich meta data respective twitter accounts. logo images form lld-logo dataset small sample presented figure means clustering latent space autoencoder trained data feature space resnet classiﬁer trained imagenet. methods able produce semantically meaningful clusters improve training. section review architectures used study describe clustering methods based autoencoder latent space resnet features discuss quantitative experimental results. dcgan dcgan experiments taehoon kim’s tensorflow implementation train dcgan exclusively low-resolution lld-icon subset proved inherently unstable without using clustering approach. input blurring explained next section dcgan experiments. details hyper-parameters used refer interested reader supplementary material. iwgan iwgan experiments based ofﬁcial tensorflow repository gulrajani kept default settings provided authors. exclusively -pixel resnet architectures provided repository major modiﬁcations conditioning method described below. also linear learning rate decay experiments. mentioned introduction training conditional labels beneﬁcial terms improved output quality unsupervised setting. particular found dcgan unstable icon dataset resolutions higher able stabilize introducing synthetic labels described section. addition stabilizing training able achieve state-of-the-art inception scores cifar- using iwgan synthetic labels produced clustering described below thus demonstrate quantitative evidence quality improvement using approach section furthermore cluster labels subsequently allow additional control figure autoencoder used clustering. generator equivalent used encoder consists discriminator higher number outputs match dimensionality latent space trained using simple loss function. generated logos generating samples individual clusters transforming particular logo inherit speciﬁc attributes another cluster demonstrated section autoencoder clustering ﬁrst proposed method producing synthetic data labels means clustering latent space autoencoder. construct autoencoder consisting modiﬁed version discriminator outputs instead acting encoder latent space unmodiﬁed generator acting decoder reconstruction image latent representation illustrated figure autoencoder trained using simple loss original reconstructed image. images encoded latent vectors followed dimensionality reduction ﬁnally clustered using k-means. logo data produces clusters semantically meaningful based high-level features recognizable created using general network topology. resnet classiﬁer clustering second clustering method leverage learned features imagenet classiﬁer namely resnet- feed images classiﬁer extract output ﬁnal pooling layer network dimensional feature vector. dimensionality reduction cluster data feature space k-means. obtained clusters considerably superior produced clustering method cifar- could argue beneﬁting similarity categories imagenet cifar- thus indirectly using labeled data. however clustering meaningful also layer conditional layer-conditional models cluster label training sample convolutional linear layers generator discriminator. linear layers simply appended input one-hot vector. convolutional layers labels projected onto one-hot feature maps many channels clusters corresponding cluster number ﬁlled ones rest zero. additional feature maps appended input every convolutional layer every layer directly access label information. illustrated figure dcgan figure resnet used iwgan model. even though labels provided every layer explicit mechanism forcing network information. case labels random meaningless simply ignored network. however soon discriminator starts adjusting criteria cluster forces generator produce images comply different requirements class. experiments conﬁrm visually meaningful clusters always picked model network simply falls back unconditional state random labels. type class conditioning useful properties ability interpolate different classes less prone failure producing class-conditional samples compared conditioning described below. however come drawback adding signiﬁcant number parameters especially low-resolution networks large number figure layer conditional residual block used iwgan-lc. label information appended convolutional layer input described figure skip connections remain unconditional. figure generator network used layer conditional dcgan labels appended onehot vector latent vector. also projected onto feature maps consisting zeros except corresponding class number elements value one. additional feature maps appended input convolutional layer. auxiliary classiﬁer iwgan also auxiliary classiﬁer proposed odena implemented glurajani method allow interpolate clusters thus slightly limited application perspective avoid adding parameters convolutional layers general results network fewer parameters. iwgan-ac method choice cifar- delivers highest inception scores. gaussian blur experiments noticed blurring input image helps network remain stable training lead apply gaussian blur images presented discriminator like previously implemented susmelj method schematically illustrated figure upscaling images pixel resolution convolving gaussian kernel enables train blurred images preserving almost image’s sharpness scaled back original resolution pixels. generating image samples trained generator without applying blur ﬁlter noticeable noise images becomes imperceptible resizing original data resolution producing almost perfectly sharp output images. based experimental experience believe produce higher quality samorder quantitatively assess performance solutions commonly used cifar- dataset report inception scores diversity scores based ms-ssim suggested randomly generated images. table summarize results different conﬁgurations supervised unsupervised settings conditional modes including reported scores literature. cifar- lld-icon generative models obtained cornia scores equivalent original images dataset. result in-line ﬁndings studied gans also converge terms cornia scores towards data image quality convergence. show cornia ms-ssim scores lld-icon dataset complement inception scores cifar- table conditional gans ac-gan variants better counterparts terms inception scores comparable terms diversity cifar. believe owed fact ac-gan enforces generation images easily classiﬁed provided clusters turn could raise classiﬁer-based inception score. even though numbers indicate qualitative advantage aclc-gan prefer latter logo application allows smooth interpolations even in-between different clusters. possible standard ac-gan implementation since cluster labels discrete integer values thus desirable latent space operations would constrained performed within speciﬁc data cluster match intended use. mentioned previous section layer conditioning allows smooth transitions latent space class another critical logo synthesis manipulation exploration latent space. therefore work conﬁgurations experiments iwgan-lc clusters dcgan-lc clusters. inception diversity cornia scores comparable lld-icon dataset. generative models like gans vaes images generated high-dimensional latent vector also commonly referred z-vector. training component vector randomly sampled uniform gaussian distribution generator trained produce reasonable output random vector sampled distribution. space spanned latent vectors called latent space often highly structured latent vectors deliberately manipulated order achieve certain properties output using dcgan-lc clusters data figure contains samples speciﬁc cluster next sample respective original data. shows layer conditional dcgan able pick data infusion training impr.gan egan-ent-vi iwgan iwgan iwgan-lc clustering iwgan-lc clustering iwgan-lc clustering iwgan-ac clustering iwgan-ac clustering iwgan-ac clustering iwgan-ac clustering table comparison inception diversity scores cifar-. unsupervised methods cifar- class labels. note unsupervised methods achieve state-of-the-art performance comparable best supervised approaches. table cornia scores diversity scores models trained lld-icon. starred models trained subset lld-icon-sharp. lower values mean higher quality cornia higher diversity ms-ssim. performance state-of-the-art best inception score achieved iwgan-ac clusters signiﬁcantly higher salimans improved method best score reported literature unsupervised methods. surprisingly best result achieved unsupervised synthetic labels provided clustering comparable stacked gans approach huang best score reported supervised methods. image quality complementary inception diversity scores also measured image quality using cornia robust no-reference image quality assessment method proposed doermann figure ﬁrst four clusters lld-icon attained ae-clustering method using cluster centers. half example contains random selection original images bottom half consists samples generated dcgan-lc corresponding cluster. strong visual correspondence demonstrates network’s ability capture data distributions inherent classes produced clustering method. figure interpolation selected logos distinct classes using dcgan-lc clusters lld-icon showcasing smooth transitions interesting intermediate samples in-between them. distribution produce samples easy attribute corresponding cluster often hard distinguish originals ﬁrst glance. comparison also show results iwgan-lc clusters trained lld-icon-sharp dataset figure show generator simply learn reproduce samples training fact able produce smooth variations output images common practice perform interpolations points latent space show outcome smooth transition corresponding generated images intermediate images exhibiting distribution quality. interpolation also provides effective tool logo generator application output image manipulated controlled manner towards certain direction latent space. interpolation experiments distrifigure continuous interpolation random points within cluster in-between distinct clusters latent space using iwgan-lc clusters icon-sharp. observe smooth transitions logo-like samples sampled subspace. bution matching methods order preserve prior distribution sampled model trained example interpolation steps showcase smoothness interpolation given figure interpolate sample points producing believable logos every step. case example interpolation works well even logos different clusters even though generator never trained mixed cluster attributes. one-hot class vector representing logo cluster separate latent vector also possible keep latent space representation constant change cluster generated logo. figure contains logos transformed particular cluster figure logo class transfer using dcgan-lc lldicon clusters. logos transferred class logos column hereby latent vector kept constant within column class label kept constant within original samples hand-picked illustrative purposes. models trained lld-icon data generated icons blurry since roughly half logos dataset upscaled lower resolution. however averaging z-vector number blurry samples subtracting average number sharp samples possible construct sharpening vector added blurry logos transform sharp ones. works well even directional vector calculated exclusively samples cluster applied samples another showing blurriness fact nothing feature embedded latent space. result transformation shown figure sharpening vector calculated sharp blurry samples manually selected random batches cluster. resulting vector applied equally blurry samples. quality result already visually convincing could optimized adding individually adjusted fractions sharpening vector logo. example adding sharpening vector latent representation many latent space operations could think directed manipulation form color performed supplementary material. class subsequent row. shows general appearance color contents encoded z-vector cluster label transforms attributes form conforms contents respective cluster. here again interpolation could used create intermediate versions desired. another powerful tool explore latent space vicinity sampling perturb given sample random directions latent space. could useful present user logo generator application choice possible variants allowing modify logo step step directions choice. figure present example -step vicinity sampling process order cope high multi-modality stabilize training data proposed clustered gans gans conditioned synthetic labels obtained clustering. performed clustering latent space autoen quantitatively validated clustered approaches cifar- benchmark clear state-of-the-art inception score unsupervised generative models showcasing beneﬁts meaningful synthetic labels obtained clustering feature space imagenet classiﬁer. showed latent space networks trained logo data smooth highly structured thus interesting properties exploitable performing vector arithmetic space. showed synthesis manipulation inﬁnitely many variations logos possible latent space exploration equipped number operations interpolations sampling class transfer vector arithmetic latent space like sharpening example.", "year": 2017}