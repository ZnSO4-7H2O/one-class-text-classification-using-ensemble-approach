{"title": "Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace  Clustering", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "State-of-the-art subspace clustering methods are based on expressing each data point as a linear combination of other data points while regularizing the matrix of coefficients with $\\ell_1$, $\\ell_2$ or nuclear norms. $\\ell_1$ regularization is guaranteed to give a subspace-preserving affinity (i.e., there are no connections between points from different subspaces) under broad theoretical conditions, but the clusters may not be connected. $\\ell_2$ and nuclear norm regularization often improve connectivity, but give a subspace-preserving affinity only for independent subspaces. Mixed $\\ell_1$, $\\ell_2$ and nuclear norm regularizations offer a balance between the subspace-preserving and connectedness properties, but this comes at the cost of increased computational complexity. This paper studies the geometry of the elastic net regularizer (a mixture of the $\\ell_1$ and $\\ell_2$ norms) and uses it to derive a provably correct and scalable active set method for finding the optimal coefficients. Our geometric analysis also provides a theoretical justification and a geometric interpretation for the balance between the connectedness (due to $\\ell_2$ regularization) and subspace-preserving (due to $\\ell_1$ regularization) properties for elastic net subspace clustering. Our experiments show that the proposed active set method not only achieves state-of-the-art clustering performance, but also efficiently handles large-scale datasets.", "text": "state-of-the-art subspace clustering methods based expressing data point linear combination data points regularizing matrix coefﬁcients nuclear norms. regularization guaranteed give subspace-preserving afﬁnity broad theoretical conditions clusters connected. nuclear norm regularization often improve connectivity give subspace-preserving afﬁnity independent subspaces. mixed nuclear norm regularizations offer balance subspacepreserving connectedness properties comes cost increased computational complexity. paper studies geometry elastic regularizer uses derive provably correct scalable active method ﬁnding optimal coefﬁcients. geometric analysis also provides theoretical justiﬁcation geometric interpretation balance connectedness subspace-preserving properties elastic subspace clustering. experiments show proposed active method achieves state-of-the-art clustering performance also efﬁciently handles large-scale datasets. many computer vision applications including image representation compression motion segmentation temporal video segmentation face clustering high-dimensional datasets well approximated union low-dimensional subspaces. case problem clustering high-dimensional dataset multiple classes categories reduces problem assigning data point subspace recovering underlying low-dimensional structure data problem known literature subspace clustering problem received attention literature many methods developed. among them spectral clustering based methods become extremely popular details). methods usually divide problem steps learning afﬁnity matrix characterizes whether points likely subspace applying spectral clustering afﬁnity. arguably ﬁrst step important success spectral clustering depends appropriate afﬁnity matrix. state-of-the-art methods constructing afﬁnity matrix based self-expressiveness model model data point expressed linear comxicij coefﬁcient used deﬁne afﬁnity between points vector captures deviations self-expressive model. coefﬁcients typically found solving optimization problem form data matrix vector coefﬁcients properly chosen regularizer coefﬁcients properly chosen regularizer noise corruption parameter balances regularizers. main difference among state-of-the-art methods lies choice regularizer sparse subspace clustering method searches sparse representation using broad theoretical conditions representation produced guaranteed subspace preserving afﬁnity matrix lack connectedness recently proposed sparsity based methods orthogonal matching pursuit nearest subspace neighbor also suffer connectivity issue. lem. proposed algorithm exploits fact nonzero entries elastic solution fall oracle region deﬁne efﬁciently update active set. proposed update rule leads iterative algorithm shown converge optimal solution ﬁnite number iterations. provide theoretical conditions afﬁnity generated ensc subspace preserving well clear geometric interpretation balance subspace-preserving connectedness properties. conditions depend local characterization distribution data improves prior global characterizations. notice oracle point unique since unique oracle point cannot computed optimal solution computed. next result gives critical relationship involving oracle point exploited active-set method. method uses regularizer beneﬁt representation matrix generally dense alleviates connectivity issue sparsity based methods. however representation known subspace preserving subspaces independent signiﬁcantly limits applicability. nuclear norm regularization based methods rank representation rank subspace clustering also suffer limitation bridge subspace preserving connectedness properties propose mixed norms. example rank sparse subspace clustering method uses mixed nuclear norm regularizer shown give subspace preserving representation conditions similar stronger ssc. however justiﬁcation improvements connectivity given lrssc merely experimental. likewise propose mixed norm given controls trade-off regularizers. however provide theoretical justiﬁcation beneﬁts method. subspace clustering regularizers studied trace lasso k-support norm respectively. however theoretical justiﬁcation provided beneﬁt methods. another issue aforementioned methods provide efﬁcient algorithms deal large-scale datasets. address issue proposes representation anchor points sampled perform spectral clustering anchor graph. authors propose cluster small subset original data classify rest data based learned groups. however strategies suboptimal sacriﬁce clustering accuracy computational efﬁciency. paper contributions. paper exploit mixture norms balance subspace preserving connectedness properties. speciﬁcally method thus combination reduces respectively. statistics literature optimization program using regularization called elastic used variable selection regression problems thus refer method elastic subspace clustering figure illustration structure solution data matrix containing randomly generated points shown blue dots plane. direction shows magnitude coefﬁcient represents oracle point direction denoted dashed line. value ﬁxed value varies depicted. figure depict dimensional example solution elastic problem different values tradeoff parameter expected solution becomes denser decreases. moreover predicted theorem magnitude coefﬁcient decaying function angle corresponding dictionary atom oracle point enough |haj holds true corresponding coefﬁcient zero. call region containing nonzero coefﬁcients oracle region. formally deﬁne oracle region using quantity denote coherence vectors i.e. oracle region composed antipodal pair spherical caps unit ball located symmetric locations ±δ/kδk angular radius arccos deﬁnition oracle region theorem follows words support solution vectors oracle region. oracle region also captures behavior solution columns matrix removed columns added. provides insight designing active-set method solving optimization. interpretation proposition solution change columns added dictionary long columns inside oracle region another perspective change removes columns dictionary oracle region although elastic optimization problem recently introduced subspace clustering prior work provide efﬁcient algorithm handle large-scale datasets. fact prior work figure conceptual illustration orgen algorithm. dots unit circle illustrate dictionary active step illustrated dots. oracle region illustrated arcs. active illustrated green indices points solves elastic problem using existing algorithms require calculations involving full data matrix used linearized alternating direction method used here propose solve elastic problem active-set algorithm efﬁcient ladm handle largescale datasets. call method oracle guided elastic solver orgen short. basic idea behind orgen solve sequence reduced-scale subproblems deﬁned active determined oracle region. active iteration then next active selected contain indices columns oracle region denotes submatrix columns indexed figure conceptual illustration. figure show columns correspond active labeling corresponding columns red. oracle region union arcs figure notice bottom left thus must included blue dots oracle region thus must included tk+. figure illustrate green dots. iterative procedure terminated contain points i.e. time support result follows lemma implies active never repeated. since ﬁnitely many distinct active sets algorithm must eventually terminate remaining part proof establishes gives nonzero entries solution. orgen solves large-scale problems solving sequence reduced-size problems step algorithm active small step small-scale problem efﬁciently solved. however procedure algorithm explicitly controls size address concern propose alternative step small number points—the ones correlated δ—are added. speciﬁcally holds indices largest entries ideally {|a⊤ chosen size bounded predetermined value nmax represents maximum size subproblem handled step nmax chosen large enough second union non-empty convergence result still holds. initialization. suggest following procedure computing initial active first compute solution closed form solution computed efﬁciently ambient dimension data big. then largest entries solution pre-speciﬁed value added experiments suggest strategy promotes fast convergence algorithm although elastic recently introduced subspace clustering works provide conditions afﬁnity guaranteed subspace preserving potential improvements connectivity. section give conditions afﬁnity subspace preserving balance subspacepreserving connectedness properties. best knowledge ﬁrst time theoretical guarantees established. problem ird×n real-valued matrix whose columns drawn union dimension ℓ-th subspace satisﬁes goal subspace clustering segment columns representative subspaces. j-th column removed. section focus given vector suppose denote submatrix columns except removed. since goal entries construct afﬁnity graph points subspace connected desire nonzero entries subset columns connections built points different subspaces. case solution subspace preserving. hand also want nonzero entries dense possible within cluster afﬁnity graph wellconnected. extent conﬂicting goals connections likely solution subspace preserving afﬁnity graph cluster well connected. conversely builds connections likely false connectivity improved. next sections give geometric interpretation tradeoff subspace preserving connectedness properties provide sufﬁcient conditions representation subspace preserving. analysis built upon optimization problem −j). note solution trivially subminc figure structure solution example associated point lies -dimensional subspace blue dots illustrate columns union regions oracle region denoted circles. green dots points dictionary. lemma says subspace preserving green dots outside region. ensure solution subspace preserving desires small oracle region ensure connectedness desires large oracle region. facts highlight trade-off properties. recall elastic balances regularization regularization thus expect oracle region decrease size increased towards theorem formalizes claim ﬁrst need following deﬁnition characterizes distribution data difference result theorem used instead characterizing show lemma distribution points makes theorem general theorem geometrically large subspace well-covered large neighborhood oracle closest well-covered i.e. point close thus condition theorem requires subspace global coverage data condition theorem allows data biased requires local region well-covered. addition condition checked membership data points known. advantage allows check tightness condition studied details appendix. contrast condition previous work inradius generally np-hard calculate upper bound size oracle region figure also notice right hand side range monotonically increasing thus provides upper bound area oracle region decreases increases. highlights trade-off subspace-preserving connectedness properties controlled notice theorem quantity determined lies within subspace deﬁnix −j). thus left-hand-side charaction increasing function showing solution likely subspace preserving weight placed regularizer relative regularizer. theorem close relationship sufﬁcient condition give subspace preserving solution speciﬁcally shows maxkxk /∈sℓ gives subspace preserving solution. observe condition approaches condition conducted synthetic experiments illustrate computational efﬁciency proposed algorithm orgen. three popular solvers exploited regularized feature sign search active type method lasso version lars algorithm implemented sparse modeling software gradient projection sparse reconstruction algorithm proposed three solvers used solve subproblem step orgen resulting three implementations orgen. also used three solvers stand-alone solvers comparison purposes. experiments vector columns generated independently uniformly random unit sphere results averages trials. ﬁrst experiment test scaling behavior orgen varying results shown figure active-set scheme improves computational efﬁciency three solvers. moreover grows improvement becomes signiﬁcant. next test performance orgen various values parameter controls tradeoff subspace preserving connectedness properties; running times sparsity level shown figures respectively. performance spams reported since performs poorly even moderately small values methods computational efﬁciency decreases becomes smaller. versions orgen expected since solution becomes denser becomes smaller thus active sets become larger leads directly larger time consuming subproblems step section orgen solve optimization problems arising ensc subproblem step solved using rfss method. compute coefﬁcient vectors parameter parameter hyperparameter smallest value nonzero. algorithm iterations observe sufﬁcient purpose subspace clustering subsequent iterations boost performance. measure clustering performance clustering accuracy calculated best matching rate label predicted algorithm ground truth. datasets. test method four datasets presented table coil- dataset contains grayscale images different objects. object images taken pose intervals degrees images size dataset contains images faces people taken different poses different illuminations different expressions. experiments near frontal poses images different illuminations expressions. image manually cropped normalized pixels. mnist dataset contains images handwritten digits image extract feature vector dimension scattering convolution network project dimension using pca. finally covtype database collected predict forest cover type cartographic variables. methods. compare method several state-ofthe-art subspace clustering methods categorized three groups. ﬁrst group contains variant k-nearest neighbors method sparse greedy methods convex optimization method. algorithms build sparse afﬁnity matrices computationally efﬁcient therefore perform large-scale clustering. code provided respective authors. note code optimized computational efﬁciency considerations. implementation optimized subspace clustering. spams solver described previous section. second group consists lrsc code provided respective authors uses alternating direction method multipliers solve optimization problems. distinguish versions refer ssc-admm previous ssc-spams. ﬁnal group consists ensc closest spirit method. method ensc balance regularizations ensc uses penalize noise linearized alternating direction method minimize objective. k-support norm used blend regularizers. implemented ensc according descriptions original papers. results. best knowledge comparison methods large scale datasets reported prior work. thus experiments tune parameters method give best clustering accuracy. results reported table table performance different clustering algorithms. running time includes time computing afﬁnity matrix performing spectral clustering. sparsity number nonzero coefﬁcients representation averaged value means memory limit exceeded value means time limit seven days reached. proposed method achieves best clustering performance every dataset. method also among efﬁcient terms computing time. methods ssc-admm ensc lrsc cannot handle large-scale data perform calculations full data matrix entire kernel matrix memory infeasible large datasets. method ssc-spams uses active method deal massive data however computationally much less efﬁcient solver orgen. understanding advantages method table report sparsity representation coefﬁcients number nonzero entries averaged sparsity directly provided parameter algorithms. method ensc-orgen sparsity indirectly controlled parameters models. method usually gives nonzero entries sparsity based methods ssc. shows beneﬁt method number correct connections built general upper-bounded dimension subspace method limit capable constructing correct connections producing well-connected afﬁnity graphs. hand afﬁnity graph lrsc dense although cluster self-connected abundant wrong connections. highlights advantage method ﬂexible controlling number nonzero entries adjusting trade-off parameter results illustrate tradeimproves clustering accuracy. regularizations. drawback works solvers optimization problems effective orgen algorithm cannot deal large datasets memory requirements. moreover observe algorithms converge modest accuracy iterations slow giving high precision solution. explain clustering accuracy good ensc-orgen. especially ensc gives dense solutions although true solution expected sparser explained fact solution paths solver dense solutions. investigated elastic regularization scalable provable subspace clustering. speciﬁcally presented active algorithm efﬁciently solves elastic regularization subproblem capitalizing geometric structure elastic solution. gave theoretical justiﬁcations—based geometric interpretation trade-off subspace preserving connectedness properties—for correctness subspace clustering elastic net. extensive experiments veriﬁed proposed active method achieves state-of-the clustering accuracy handle large-scale datasets. acknowledgments. robinson vidal supported national science foundation under grant c.-g. partially supported national natural science foundation china grants project grant authors thank haeffele insightful comments design orgen algorithm. appendix organized follows. section present proofs geometric properties elastic solution. section show convergence algorithm orgen. section prove relevant results properties ensc. section synthetically generated data verify results properties ensc. section study special case ensc method reduces ssc. show properties ensc well orgen algorithm also apply minor modiﬁcations thus work also offers additional understanding ssc. section report parameters algorithms used real data experiments. finally section clarify contribution paper comparison several prior works elastic based subspace clustering. ∂kˆck. then taking soft-thresholding sides proof reverse implication suppose satisﬁes considering three cases separately establish j-th satisﬁed corresponding holds. inradius introduced deﬁnition characterizes distribution points. next lemma interpreted giving equivalent deﬁnition inradius certain convex sets. result used interpreting differences theorem theorem well proving theorem remains show inequality strict. show arguing appears second line optimal solution optimization problem stated third line. denote solution optimization problem proof. ﬁrst prove algorithm terminates ﬁnite number iterations. ﬁrst observe objective strictly decreasing iteration termination occurs since ﬁnitely many different active sets must conclude algorithm terminates ﬁnite number iterations prove algorithm terminates output vector optimal. construct vector compleˆ ment theorem holds termination condition know tk+. thus step thus solution i.e. also construction seen support precisely tk+. interpretation lemma follows searches vector furthest away points inradius coherence {±aj}n words charclosest neighbor {aj}n acterizes covering property points {±aj}n inradius large point space exists close only part oracle region proposition shows coefﬁcient vector corresponds points outside nonzero. therefore solution correct identifying l-th subspace. result theorem follows bound norm oracle point given lemma relation revealed lemma lemma consider problem deﬁne maxj coherence oracle point closest neighbor among columns theorem theorem give conditions guarantee correctness representation given ensc purpose subspace clustering. section synthetic experiments verify theoretical analysis. speciﬁcally verify tradeoff parameter increases representation likely correct. moreover examine tightness bound predicting correctness. pair randomly generate subspaces data samples speciﬁed caption figure ensc generated data matrix representation vectors figure report percentage {cj}n vectors correct identifying subspace. seen easier correct representations larger. consistent intuition becomes larger solution sparser likely correct. moreover consistent predicted theoretical analysis theorem theorem condition correctness easier satisﬁed increases. result theorem figure speciﬁcally solve using ground truth labels comc∗ consequently quantities condition theorem computed consequently whether condition holds. figure plot percentage points satisfy condition. since condition sufﬁcient necessary expect percentage figure larger corresponding percentage figure reveals tightness result theorem clearly illustrated figure plot selected rows figure correspond seen condition becomes tighter approaches finally notice condition theorem checked ground truth known condition theorem cannot since generally np-hard compute inradius advantage theorem addition fact weaker requirement guarantee correctness ensc. figure correctness solution ensc different values generate subspaces dimension ambient space dimension uniformly random. subspace sample uniformly random equal number points varies report percentage representations correct identifying subspaces. percentage correct representations different values produced experimental results predicted theorem respectively. plots selected rows help clarify difference. analyses results paper section discuss case turns geometric structure elastic solution slightly different. result many theorems discussions apply need separate discussion results. oracle point oracle region. deﬁnitions oracle point oracle region before. oracle point unique since unique strong convexity problem argument apply case however sill establish uniqueness oracle point. whose corresponding coefﬁcients nonzero i.e. case argument longer holds. actually theorem still holds left-hand-side becomes zero means column oracle region understand structure solution need following result. support notice operands union disjoint sets. modiﬁcation show orgen converges optimal solution ﬁnite number iterations. proof essentially omitted here. case solution unique solution orgen converges depends upon initialization well speciﬁc solution given solver step using relationship condition weaker requirement previous work. specifically condition requires entire subspace large. well-covered columns contrast condition requires neighborhood oracle point well-covered i.e. exists column close another advantage condition veriﬁed ground truth known. contrast condition cannot veriﬁed since computation generally np-hard purpose reproducible results report parameters used methods real data experiments. parameter controls number nonzero coefﬁcients representation. parameter sparsity reported table additional parameters. maximum subspace dimension default value suggested original paper. controls post-processing step. purpose fair comparison methods essentially disables post-processing step. ssc-spams uses model hyperparameter speciﬁed table smallest value nonzero. parameters solver spams default values. ssc-admm code solving optimization problem presented value ssc-spams. lrsc code model parameters provided table ensc three parameters model remaining parameters suggested authors. implemented number iterations parameter times lipschitz constant reported table finally proposed algorithm ensc-orgen parameter controls trade-off norms parameter controls value deﬁnition smallest value nonzero. parameters summarized table elastic formulation originally proposed subsequently introduced subspace clustering works regularization combination penalty function proposes penalty uses joint penalty. works existing methods solving optimization problem uses accelerated proximal gradient uses linearized alternating direction method optimization model studied slightly different prior works since penalty suggested original elastic paper. despite difference modeling noise three models elastic regularization. major contributions work comparison related works threefold design active-set algorithm solving optimization problem. comparison ladm used related works method computationally efﬁcient able handle larger datasets. although using elastic subspace clustering balance correctness connectivity provide ﬁrst detailed argument based geometric interpretation solution elastic net. deepens understanding approach.", "year": 2016}