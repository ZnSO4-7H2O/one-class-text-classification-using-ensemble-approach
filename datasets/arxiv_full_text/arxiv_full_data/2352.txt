{"title": "Statistical Modeling in Continuous Speech Recognition (CSR)(Invited  Talk)", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Automatic continuous speech recognition (CSR) is sufficiently mature that a variety of real world applications are now possible including large vocabulary transcription and interactive spoken dialogues. This paper reviews the evolution of the statistical modelling techniques which underlie current-day systems, specifically hidden Markov models (HMMs) and N-grams. Starting from a description of the speech signal and its parameterisation, the various modelling assumptions and their consequences are discussed. It then describes various techniques by which the effects of these assumptions can be mitigated. Despite the progress that has been made, the limitations of current modelling techniques are still evident. The paper therefore concludes with a brief review of some of the more fundamental modelling work now in progress.", "text": "automatic sufficiently real world applications including interactive views evolution elling systems n-grams. scription terisation consequences describes fects assumptions despite limitations still evident. cludes fundamental statistical formulation problem assumes speech represented sequence sequence encodes specific chosen minimise encoding butional modeling. typically computed cated cosine foundations laid fred jelinek time initial development whole word small vocabulary early attention independent word resource developed rapidly accuracy acoustic efficiently estimated terances baseforms constructed. used compute state imised simple weighted note practice parameters transition ther likelihood decoding piling within network allowable language tween words bigram compiled conventional viterbi state sequence unknown input utterance. trace back likely practice however decoding complex. least trigram equate performance means recognition must expanded maintained. next section inadequate language common choice model context-dependent. thus decoding search research simple approach composing vocabulary base phones fails capture context-dependent variation example base form pronunciations \"mood\" \"cool\" would vowel \"oo\" practice different influence preceding simple mitigate following problem possible context. avoid resulting data spar­ sity problems would otherwise result hypotheses. logical costs uses mul­ scheme reducing shared physical rnorlels ependent phone decomposition data. output pass comes output solutions pruning limit number active search standard tiple passes lattice single recognition pass. initial search space large later passes fined models search multipass research lattices putational process tation context context-dependence spreads across aries essential tant phonological processes. example \"stop that\" burst suppressed following consonant choice trees. binary carries question regarding context. duster states models derived root node tree. depending node pool states states states node selected maximize final based clustering. aw+n t-aw+n hence share central representative physical partitioning decision logical models training disadvantage coarse. soft-tying. objective function optimised using extended form baum-welch reestim ation algorithm. summations numerator denominator terms computed decoder gener­ ated lattices forced recognition denominator recognition large number gaussians component variance ability mixtures diagonal covariance gaus­ structure limited. sians model correlation improve covariance modelling without incurring unacceptable requirements tor. much studied discriminant analysis variants. also used prune nuisance dimen­ sions feature vectors thereby reducing com­ putation potentially per­ formance. however practice gains achievable variable robust improvements obtained using semi-tied covariances scheme decomposes component covariance product transformation matrix diagonal property logp logp+ hence global applied transform speech data diagonal covariance computed normal sians estimated transformed data. although direct based reestimation terative scheme ystems many gaussians available single global capture different correlation structures. case transformations h{¢{m)) used effective form speaker norm alisation warp shared amongst gaussians partition frequency axis used front-end spectrum anal­ chosen maximise training ysis order pensate variations recently optimal warping found tract length. whereas normalisation seeks modify closer models adaptation models make better speech. main approaches adaptation. firstly model parameters treated random vari­ ables estimated using traditional bayesian techniques. parameters tion data updated also determining suitable pri­ difficult. second widely used approach estimate transformations odel parameters likelihood linear regression mllr seeks find affine transform gaussian means maximises likelihood adapta­ tion data i.e. power adaptation approach single transformation gaussian mixture components. amount adaptation data limited single trans­ form shared across gaussians system. amount data increases ponents grouped classes class transform. creases further number classes therefore transforms better adaptation. number transforms tomatically node represents gaussian components share single transform. given adaptation data tree descended specific nodes lected sufficient data minimum word error rate decoding achieved estimating done converting confusion matrix. terior probability backward algorithm clustered parallel graph pass nodes order lattice. pothesis found selecting confusion confusion graphs also used compute confi­ dence scores direct word posteriors tends overestimate nition lattices cover fraction complete hypothesis hence practice form mapping used sections described range tech­ niques used improve performance system techniques often found bined various ways. empirically even different system combinations lead similar performance system different. scription applications several decoders. simple voting. confusion described section core current lan­ guage models word n-gram. n-gram cap­ tures local syntactic semantic dependencies many languages sufficient cover large fraction useful constraints. ness inevitably mit­ though smoothing igate effect data sparsity always problem. easy argue case perfor­ assumptions major limitation mance systems. nevertheless next sec­ tremely resistent tions provide pointers recent current work attempts improve assumptions. previous section techniques sumptions hmm-based recognition furthermore tegrated result terms performance first pass competitive. large vocabulary recogniser tied-state cross-word triphones likelihood applied march evaluation word error rate however addition vtln mllr adaptation quinphone acoustic models soft-tied training gram class model confusion graph model combination i.e. relative demanding chitecture performance. beads-on-a-string dependent found carefully significant spontaneous speech. many phonological processes naturally parallel nasality vowel timing allophone ciation variability ture changes chronous level move synchronously separate chosen ensure expected val­ matches known arginals. example bigram constraint would iffw ends required marginal constraint would given count computed train­ data. within corporated explicit semantic parsers uncover head words used predictors. using combined conventional gram constraints. work especially since models longer range dependencies principled growing synergy speech dends area. odell valtchev woodlanti young. one· pass decoder design large vocabulary recognition. proc ffuman t\"anguage technology work.<hop pages plainsboro organ kaufman puhlishers paper reviewed statistical used build continuous speech recognitio briefly needed endow system state-of-the-art performance. current beads-on-a-string latter part paper language modelling. described work progess aims improve upon assumptions. significant gains mance resulting lacking. however argued important even mean increas­ work continues error rates short term. modern start-of­ the-art systems further. nevertheless three fundamental assump­ tions based must surely mean climbing local maximum somewhere better solutions", "year": 2013}