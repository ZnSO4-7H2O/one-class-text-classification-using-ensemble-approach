{"title": "Parametrizing filters of a CNN with a GAN", "tag": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "abstract": "It is commonly agreed that the use of relevant invariances as a good statistical bias is important in machine-learning. However, most approaches that explicitly incorporate invariances into a model architecture only make use of very simple transformations, such as translations and rotations. Hence, there is a need for methods to model and extract richer transformations that capture much higher-level invariances. To that end, we introduce a tool allowing to parametrize the set of filters of a trained convolutional neural network with the latent space of a generative adversarial network. We then show that the method can capture highly non-linear invariances of the data by visualizing their effect in the data space.", "text": "commonly agreed relevant invariances good statistical bias important machine-learning. however approaches explicitly incorporate invariances model architecture make simple transformations translations rotations. hence need methods model extract richer transformations capture much higherlevel invariances. introduce tool allowing parametrize ﬁlters trained convolutional neural network latent space generative adversarial network. show method capture highly non-linear invariances data visualizing effect data space. machine-learning solving classiﬁcation task typically consists ﬁnding function rather large data space much smaller space labels function therefore necessarily invariant transformations input data. clear able characterize transformations greatly help learning procedure striking examples perhaps convolutional neural networks image classiﬁcation built-in translation invariance convolutions subsequent pooling operations. convolutional layer essentially fully connected layer constraint tying weights together could expect invariances encoded weights training. indeed empirical perspective cnns observed naturally learn invariant features depth theoretical perspective proven conditions satisﬁed weights convolutional layer layer could re-indexed performing convolution bigger group transformations translations exciting note recently interest theoretically extending successful invariant computational structures general groups transformations notably group invariant scattering operators deep symmetry networks group invariant signal signatures group invariant kernels group equivariant convolutional networks however practical applications mostly remained limited linear afﬁne transformations. indeed challenge parametrize complicated non-linear transformations preserving labels especially need depend dataset. work seek answer fundamental question following brief summary method considering already trained labeled dataset train generative adversarial network produce ﬁlters given layer ﬁlters’ convolution output indistinguishable obtained real cnn. combine infogan discriminator prevent generator producing always ﬁlters. result generator provides smooth data-dependent non-trivial parametrization ﬁlters characterizing complicated transformations irrelevant classiﬁcation task. finally generative adversarial network consists players generator discriminator playing minimax game tries produce samples indistinguishable given true distribution pdata tries distinguish real generated samples. typically maps random noise generated sample transforming noise distribution distribution supposed match pdata. objective function minimax game maximum likelihood given infogan generator takes input noise also another variable called latent code. make generated samples depend structured instance choosing independent ci’s modelling order avoid trivial correspondence infogan procedure mutual information random variables deﬁned entropy symmetric measures amount information known value random variable value known equal zero independent. hence maximizing mutual information prevents independent already trained whose th-layer representation image denoted goal learn kind ﬁlters would learn layer could tempting simply train match distribution ﬁlters cnn’s layer. however ﬁlters small dataset train would cause generator massively overﬁt instead extracting smooth hidden structure lying behind discrete ﬁlters. cope problem instead feeding discriminator alternatively ﬁlters produced real ﬁlters propose feed activations ﬁlters caused data passing i.e. alternatively conv− corresponding respectively real fake samples. here image sampled data sampled latent space conv− activation obtained passing layer replacing ﬁlters th-layer short step generator produces ﬁlters th-layer cnn. next different samples data passed using real ﬁlters th-layer ﬁlters replaced fake ﬁlters produced discriminator guess activation produced using real generated ﬁlters th-layer generator produce ﬁlters making subsequent activations indistinguishable obtained real ﬁlters. however even though formulation allows train dataset reasonable size saving otherwise unavoidable overﬁtting could priori still always produce ﬁlters fool ideally simply reproduces real ﬁlters cnn. overcome problem augment model infogan discriminator whose goal predict noise used produce conv− prevents always producing ﬁlters preventing indenpendent random variables. note that discriminator infogan discriminator directly output ﬁlters activation output ﬁlters produce. setting noise generator plays role latent code infogan. original infogan paper train neural network predict latent code shares layers last discriminator finally modelling latent codes independent gaussian random variables term variational bound log-likelihood actually given l-reconstruction error. joint training three neural networks described algorithm illustrated figure figure illustration different neural networks interact other. layers depicted light gray. data shown green generation ﬁlters generative model shown red. discriminator part shown blue. note discriminator direct access generated ﬁlters observe data passed them. ﬁxed trained jointly. sample minibatch noise samples noise prior pnoise. generate ﬁlters sample minibatch examples data distribution pdata. pass data real generated ﬁlters obtain cnn)’s feed update discriminator ascending stochastic gradient using method parameterize ﬁlters trained thus characterize learned invariances. order assess actually learned need visualize invariances trained. speciﬁcally given data sample would like know transformations regards invariant. following manner take latent noise vector obtain generated ﬁlters using ﬁlters pass data sample network obtain conv− call activation proﬁle given next select dimensions random construct grid noise vectors {zk|zi moving around dimensions small neighborhood. gradient descent start data point gives activation proﬁle ﬁlters generated using data point gave ﬁlters generated using formally want result grid z-vectors obtain grid x-points. grid data space represents manifold traversal small neighborhood manifold learned invariances. method successful expect sensible continuous transformations original data point along axes grid. apply method extracting invariances convolutional neural network trained mnist dataset. particular train standard architecture featuring convolutional layers relu nonlinearities max-pooling batch normalization epochs -class classiﬁcation task. results seen figure sample learned ﬁlters seen figure expectations clearly resulting outputs fact ensemble highly nonlinear high-level transformations data. even visualizations found appendix. hypothesize apply method ﬁlters ﬁrst layers network transformations learn much low-level pixel-local. test this method cnn’s second convolutional layer. results seen figure expected transformations much low-level simple brightness changes changes stroke width. figure invariance transformations extracted cnn’s layer. middle sample grid represents original data sample rest grid found matching original sample’s activation proﬁle. figure invariance transformations extracted cnn’s layer. middle sample grid represents original data sample rest grid found matching original sample’s activation proﬁle. order assess quality generator need sure that ﬁlters produced generator would yield good accuracy original classiﬁcation task generator produce variety different ﬁlters different latent noises. ﬁrst part randomly drew noise vectors computed corresponding ﬁlters them data sample going cnnl− passed th-layer averaged them signal next layer becomes next layers re-trained. averaging seen average pooling w.r.t. transformations deﬁned generator which transformations learned indeed irrelevant classiﬁcation task induce loss accuracy. expectations conﬁrmed test accuracy obtained following procedure test accuracy real cnn. second part figure shows multi-dimensional scaling original ﬁlters generated ﬁlters randomly sampled noise vectors. observe different noise vectors produce variety different ﬁlters conﬁrms generator overﬁtted real ﬁlters. further since generator learned produce variety ﬁlters real ﬁlter retaining classiﬁcation accuracy means truly captured invariances data regard cnn’s classiﬁcation task. figure multi-dimensional scaling ﬁlters produced gan. individual colors represent different samples ﬁlter true cnn. large cluster sizes shows producing wide variety different ﬁlters corresponding real ﬁlter. introducing invariance irrelevant transformations given task known constitute good statistical bias translations cnns. although work already done regarding implement known invariances computational structure practical applications mostly include simple linear afﬁne transformations. indeed characterizing complicated transformations seems challenge itself. work provided tool allowing extract transformations w.r.t. trained invariant transformations visualized image space potentially re-used computational structures since parametrized generator. generator shown extract smooth hidden structure lying behind discrete possible ﬁlters. ﬁrst time method proposed extract symmetries learned explicit parametrized manner. applications work likely include transfer-learning data augmentation. future work could apply method colored images. suggested last subsection parametrization irrelevant transformations ﬁlters could also potentially deﬁne another type powerful pooling operation. references fabio anselmi joel leibo lorenzo rosasco mutch andrea tacchetti tomaso poggio. unsupervised learning invariant representations hierarchical architectures. theoret. comput. sci. dx.doi.org/./j.tcs... chen duan rein houthooft john schulman ilya sutskever pieter abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. advances neural information processing systems goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems karel lenc andrea vedaldi. understanding image representations measuring equivariance equivalence. proceedings ieee conference computer vision pattern recognition aravindh mahendran andrea vedaldi. understanding deep image representations inverting them. proceedings ieee conference computer vision pattern recognition youssef mroueh stephen voinea tomaso poggio. learning group invariant features kernel perspective. advances neural information processing systems figure invariance transformations extracted cnn’s layer. middle sample grid represents original data sample rest grid found matching original sample’s activation proﬁle. figure invariance transformations extracted cnn’s layer. middle sample grid represents original data sample rest grid found matching original sample’s activation proﬁle.", "year": 2017}