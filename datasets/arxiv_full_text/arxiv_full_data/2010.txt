{"title": "Tradeoffs between Convergence Speed and Reconstruction Accuracy in  Inverse Problems", "tag": ["cs.NA", "cs.LG", "cs.NE", "math.OC", "stat.ML", "65B99, 90C59", "G.1.6; F.1.1; I.2.6; G.1.3"], "abstract": "Solving inverse problems with iterative algorithms is popular, especially for large data. Due to time constraints, the number of possible iterations is usually limited, potentially affecting the achievable accuracy. Given an error one is willing to tolerate, an important question is whether it is possible to modify the original iterations to obtain faster convergence to a minimizer achieving the allowed error without increasing the computational cost of each iteration considerably. Relying on recent recovery techniques developed for settings in which the desired signal belongs to some low-dimensional set, we show that using a coarse estimate of this set may lead to faster convergence at the cost of an additional reconstruction error related to the accuracy of the set approximation. Our theory ties to recent advances in sparse recovery, compressed sensing, and deep learning. Particularly, it may provide a possible explanation to the successful approximation of the l1-minimization solution by neural networks with layers representing iterations, as practiced in the learned iterative shrinkage-thresholding algorithm (LISTA).", "text": "abstract—solving inverse problems iterative algorithms popular especially large data. time constraints number possible iterations usually limited potentially affecting achievable accuracy. given error willing tolerate important question whether possible modify original iterations obtain faster convergence minimizer achieving allowed error without increasing computational cost iteration considerably. relying recent recovery techniques developed settings desired signal belongs low-dimensional show using coarse estimate lead faster convergence cost additional reconstruction error related accuracy approximation. theory ties recent advances sparse recovery compressed sensing deep learning. particularly provide possible explanation successful approximation ℓ-minimization solution neural networks layers representing iterations practiced learned iterative shrinkage-thresholding algorithm often recovery ill-posed problem. example fewer rows columns rendering underdetermined linear system equations. case impossible recover without introducing additional assumptions structure. popular strategy popular technique solving using iterative programs proximal methods include iterative shrinkage-thresholding algorithm alternating direction method multipliers strategy particularly useful large dimensions many applications impose time constraints limit number computations performed recover measurements. minimize time computations reduce number iterations without increasing computational cost iteration. different approach momentum methods random projections accelerate convergence. another alternative keep number iterations ﬁxed reducing cost iteration. example since complexity iterative methods rely among things common technique save computations sub-sample measurements removing redundant information amount still allows reconstruction series recent works suggest obtaining measurements beneﬁt simple efﬁcient methods cannot applied smaller number measurements. generalization properties large-scale learning systems studied showing tradeoff number measurements target approximation. work showed possible make run-time optimization decrease size training data increases. shown problem supervised learning halfspaces -sparse vectors trinary values number training examples exceeds certain limit. similar phenomena encountered context sparse recovery efﬁcient algorithms guaranteed reconstruct sparsest vector number samples larger certain quantity shown larger number training examples possible design efﬁcient optimization problems projecting onto simpler sets. idea studied changing amount smoothing applied convex optimization. authors show measurements allow increasing step-size projected gradient algorithm thus accelerating convergence. works studied tradeoff convergence speed number available measurements paper takes different route. consider case time constraints need stop iterations achieve desired reconstruction accuracy. original algorithm result recovery optimum. important question whether modify original iterations method convergences improved solution fewer iterations without adding complexity them. introduces tradeoff recovery error willing tolerate computational cost. demonstrate goes beyond trivial relationship approximation error number iterations exists various iterative methods k·k. technique learns neural network several layers layer modiﬁed version ista iteration. achieves virtually accuracy original ista using orders magnitude less iterations. acceleration iterative algorithms neural networks unique sparse recovery problem analysis cosparse low-rank matrix models poisson noise acceleration eulerian ﬂuid simulation feature learning however proper theoretical justiﬁcation phenomena still lacking. contribution. work provide theoretical foundations elucidating tradeoff allowed minimization error number simple iterations used solving inverse problems. formally show allow certain reconstruction error solution possible change iterative methods modifying linear operations applied iteration complexity number steps required attain certain error reduced. tradeoff seems natural working real data data assumed models noisy approximate; searching exact solution optimization problem variables affected measurement model noise unnecessary valuable computational resources. formally prove relation iterative projection algorithms. interestingly related tradeoff exists also context sampling theory allowing error reconstruction fewer samples and/or quantization levels argue tradeoff analyze explain smaller number iterations required lista compared ista. parallel efforts work also provide justiﬁcation success lista. fast convergence lista justiﬁed connecting convergence speed factorization gram matrix convergence speed ista lista analyzed using restricted isometry property showing lista reduce leads faster convergence. relation lista approximate message passing strategies drawn paper differs previous contributions three main points goes beyond case standard lista sparse signals considers variants apply general lowdimensional models; theory relies concept inexact projections relation tradeoff convergence-speed recovery accuracy differs signiﬁcantly attempts explain success lista; besides exploring lista provide acceleration strategies programs model-based compressed sensing sparse recovery side-information. organization. paper organized follows. section present preliminary notation deﬁnitions describe ista lista techniques. section introduces theory non-convex cones. section shows possible tradeoff convergence speed reconstruction accuracy introducing inexact projected gradient descent method using spectral compressed sensing motivating example. reconstruction error ipgd analyzed function iterations section section discusses relation between theory model-based compressed sensing sparse recovery side information section proposes lista version ipgd learned ipgd demonstrates usage task image super-resolution. section viii relates approximation minimization problems studied neural networks deep learning providing theoretical foundation success lista suggesting mixture-model extension technique. section concludes paper. write euclidian norm vectors spectral norm matrices norm sums absolute values vector pseudo-norm counts number non-zero elements vector. conjugate transpose denoted orthogonal projection onto original algorithm whose iterations almost identical ones ista orthogonal projection instead proximal mapping. propose acceleration technique similar lista accompanied theoretical analysis. value varies depending projected vector note similarity proximal mapping ista ℓ-norm also soft thresholding operation ﬁxed threshold similarity unique ℓ-norm case happens also types pseudo-norm nuclear norm. step size assumed constant sake simplicity methods vary iterations. sparse vectors important method analyzed various works. example standard sparsity sparsity patterns belong certain model general union subspaces nonlinear measurements recently form therefore advantage ista iterations require application matrix multiplications simple non-linear function. nonetheless main drawback ista large number iterations typically required convergence. many acceleration techniques proposed speed convergence ista partial list works). prominent strategy lista structure ista different linear operations empirically observed able attain solution close ista signiﬁcantly smaller ﬁxed number iterations lista iterations given learned training examples back-propagation objective ℓ-distance ﬁnal ista solution lista minimization objectives used e.g. training lista minimize directly notice lista structure recurrent neural network seen fig. acceleration techniques ista proposed together thorough theoretical analysis powerful lista method introduced without mathematical justiﬁcation success. work focus second equality deﬁnitions minkowski difference. therefore projecting onto equivalent projection onto {x}. lemma nonempty closed cone respectively variants measure generally used. cone gaussian mean width measures dimensionality tangent cone gaussian mean width related directly minkowski difference cone gaussian mean width relies speciﬁc target point gaussian mean width considers hand dependence indirect descent point series works developed convergence reconstruction guarantees various methods based others rely ﬁrst mainly employed case convex functions used relax non-convex resides. setting often entry non-zero parent node nonzero value change. although definition similar yields different results. k-sparse vectors ﬁrst result similar expression ball k-sparse vector second provides better measure examples included ksparse vectors corresponding pseudo-norm proper function. even ignore condition result theorem case random gaussian matrix face problem. using relationship gaussian mean width characterizes therefore leads smaller resulting faster convergence. improvement signiﬁcant; smaller improves convergence rate allows using larger step-size example consider case k-sparse vector whose sparsity pattern obeys tree structure. ignore structure choose norms mean widths respectively. however take tree structure close approach taken context model-based compressed sensing shown faster convergence achieved projecting onto k-sparse vectors tree structure instead standard k-sparse set. related study showed enough small number gaussians represent patches natural images instead using dictionary spans much larger union subspaces. work relied gaussian mixture models whose mean width scales proportionally number gaussians used signiﬁcantly smaller mean width sparse model. motivate algorithm consider problem spectral compressed sensing wants recover sparse representation dictionary high local coherence. shown non-zeros representation easier obtain good recovery times redundant dictionary k-sparse vector sparsity dimension minimal distance non-zero neighboring coefﬁcients greater value non-zero coefﬁcient generated normal distribution. construct vector adding random gaussian values zero mean variance mentioned above better reconstruction achieved estimating estimating highly correlated columns common practice improve recovery case force recovery algorithm select solution separated coefﬁcients. context simply using ipgd projection onto therefore reaches slightly higher ﬁnal error projection onto ball. compared ipgd projects onto simpler smaller gaussian mean width thus attaining faster convergence ﬁrst iterations approximation error still signiﬁcantly larger seen fig. coherence larger advantage ipgd signiﬁcant. cases ipgd even attain lower ﬁnal recovery error compared pgd. example consider case four times redundant dictionary generated larger redundancy dictionary coherence larger case. thus recovery harder. here projection onto ball converges slower reaches large error high correlations atoms. using ipgd ball projection keeps dominant turn analyze performance ipgd. simplicity discussion analyze convergence technique linear operator noiseless setting i.e. extension types operators noisy case straightforward arguments similar used treating noise term classes matrices. present theorems convergence ipgd. ﬁrst result provides bound terms case convex corresponding theorem second provides bound terms shows ipgd accelerate convergence compared cases even achieve lower recovery error. signiﬁcantly smaller smaller maps smaller becomes. time maps smaller sets usually provides coarser estimate thus approximation error increases. thus ipgd allows tradeoff approximation error improved convergence error term theorems iteration comprised components. ﬁrst goes zero increases second increases iterations order fewer iterations perform larger allow. alternative perspective larger reconstruction error tolerate larger thus require fewer iterations. therefore projection introduces tradeoff. hand leads increase reconstruction error. hand simpliﬁes projected leads faster convergence works similar concept nearoptimal projection assumes exact projections). main difference contributions papers focus speciﬁc models present general framework speciﬁc certain low-dimensional prior. addition papers projection performed make possible recover vector certain low-dimensional work main purpose inexact projections accelerate convergence within limited number iterations. larger number iterations projections lead good reconstruction error. fig. reconstruction error function iterations running time recovering sparse vector tree structure. since initialize algorithms zero vector error iteration/time zero kxk. zoomed version ﬁrst iterations ﬁrst appears bottom row. ﬁgure demonstrates convergence rate projections onto sparse sparse tree compared ipgd projects onto certain number levels tree ipgd changing projects onto increasing number levels iterations proceed. note projection onto tree structure converges faster ipgd function number iterations converges slower ipgd take account actual time iteration shown right ﬁgure higher complexity projections. model-based compressed sensing projection requires additional computations iteration technique suggests approximate linear projection onto ﬁrst levels tree followed levels projection smaller approximation error turns speciﬁcally easy show bounded times energy entries eliminated divided total energy i.e. kp−xk clearly layers smaller becomes. assuming nodes layer selected equal probability probability proposed ipgd projects onto different number levels tree. algorithms step size interesting note projects onto ﬁrst layer algorithm converge resulting approximation error large. however starting second layer faster convergence ﬁrst iterations projects onto smaller yields smaller number iterations increases accurate projections achieve lower reconstruction error plateau attained proportional approximation error predicted theory. tradeoff used accelerate convergence changing projection ipgd iterations. thus ﬁrst iterations enjoy fast convergence coarser projections later ones accurate projections allow achieving lower plateau. last line fig. demonstrates strategy ﬁrst iteration projection onto ﬁrst levels every four iterations another tree level added projection becomes projection onto tree levels note ipgd converges faster also projection becomes onto tree levels. explained fact typically convergence non-linear optimization techniques depends initialization point arbitrarily chose another level every ﬁxed number iterations general control used setting number iterations performed training level. demonstrate strategy section vii. jection projects onto precise achieves smallest recovery error throughout iterations. projection computationally demanding converges slower ipgd take account time iteration seen fig. clearly demonstrates advantage using simple projections ipgd compared accurate complex projections pgd. demonstrate approach combination proposed framework recovery sparse vector discrete cosine transform given information representation haar transform. sampling matrix dictionary. random patches size normalized unit norm standard house image. note patch exactly sparse either haar domains. fig. example patch house image. without considering side information haar transform recover using assume someone gives oracle side information haar columns corresponding largest coefﬁcients contain energy patch many ways incorporate side information recovery show ipgd used purpose. denoting poracle linear projection onto columns apply ipgd dporacle kdxk fig. reconstruction error function iterations sparse recovery side information. demonstrates convergence rate projection onto ball compared ipgd oracle side information columns representation haar basis; ipgd oracle side information projects onto increasing number columns haar basis ordered according signiﬁcance representing ipgd projection onto ﬁrst columns haar basis; ipgd changing projects onto increasing number columns haar basis. number columns haar contain energy roughly thus gaussian mean width case roughly width tangent cone norm k-sparse vector space dimension smaller thus smaller clearly less energy preserved need less columns haar implies smaller since projections onto smaller sets lead faster convergence suggest previous example apply oracle projection uses less columns haar basis ﬁrst iteration adds columns gradually throughout iterations. third line fig. demonstrates option ﬁrst iterations projection onto columns contain energy every iterations next columns correspoding coefﬁcients largest energy added. continue columns span energy signal. thus ipgd changing projections converges reaches faster ipgd constant dporacle plateau. typically oracle information coefﬁcients haar basis accessible. even though still possible common statistics data accelerate convergence. example case known energy signal concentrated low-resolution haar ﬁlters. fig. loss function iterations ista lista listamm applied patches house image. demonstrates faster convergence proposed lista-mm compared lista fast convergence lista compared ista. therefore propose ipgd projection projects onto ﬁrst columns haar basis. before possible accelerate convergence projecting ﬁrst smaller number columns increasing number iterations proceed options presented fourth ﬁfth line fig. respectively. options provide faster convergence ipgd ﬁxed projection incurs higher error uses less accurate projections last iterations compared ipgd changing projections. plateau latter regular achieved much smaller number iterations. apply method replace sparse coding step super-resolution algorithm proposed pair high resolution dictionaries used reconstruct patches high-resolution image low-resolution one. code provided authors orthogonal matching pursuit sparsity used. complexity strategy corresponds iterations. target sparsity higher observed provide better reconstruction results. note number iterations different sparsity level. optimal hyperparameter selection training used training dictionary contains images. since converge iterations apply lipgd accelerate convergence. high resolution dictionaries respectively) train lipgd network infer sparse code image patches low-resolution dictionary. training weights performed stochastic gradient descent batch-size nesterov momentum adaptively setting learning rate. train network using ﬁrst images training keeping last validation set. reduce training rate factor validation error stops decreasing. initial learning rate nesterov parameter sparse representations training data calculated lipgd generate high-resolution dictionary table summarizes reconstruction results regular bicubic interpolation omp-based super-resolution technique version lipgd seen clearly leads inferior results compared since converge iterations. lipgd improves training network allows provide good sparse approximation iterations. demonstrates efﬁciency proposed lipgd technique computational complexity iht. indirect learning linear operators linear operator together proximal mapping approximates accurate proximal mapping true unknown function leads much faster convergence. model similar gaussian mixture model proposed train several lista networks part dataset. then vector apply networks parallel chose attains smallest value objective minimization problem patches size adding random gaussian noise variance removing normalizing each. take patches training validation testing. train lista minimize directly objective stop optimization error validation increases. lista-mm lista networks train ﬁrst whole data. remove data whose objective value closest ista attains iterations. lista network initialization next trained rest data. repeat process removing part data smallest relative error train next network. training networks cluster data points selecting patch network leads smallest objective error tune network corresponding group patches. repeat process times. objective error function number iterations/depth networks presented fig. indeed seen partitioning data leads better approximation accelerates convergence. proposed lista-mm strategy bears resemblence recently proposed rapid accurate image super resolution algorithm method different ﬁlters trained different types patches natural images. leads improved quality attained upscaled images minor overhead computational cost leading efﬁcient super-resolution technique. work suggested approach trade-off approximation error convergence speed. accomplished approximating complicated projections inexact ones computationally efﬁcient. provided theory convergence iterative algorithm uses approximate projection showed cost error projection achieve faster convergence ﬁrst iterations. larger error smaller number iterations enjoy fast convergence. suggests budget small number iterations worthwhile inexact projections result worse solution long term make better given computational constraints. moreover showed even afford larger number iterations worthwhile inexact projections ﬁrst iterations change accurate ones latter stages. theory offers explanation recent success neural networks approximating solution certain minimization problems. networks achieve similar accuracy iterative techniques developed problems much smaller computational cost. demonstrate usage method problem image super-resolution. addition analysis provides technique estimating solution minimization problems using multiple networks fewer layers them. i-./ erc-stg grant partially supported grant -erc-cogbnyq. partially supported erc-stg rapid. partially supported aro. thank pablo sprechmann early work insights line research prof. kimmel prof. gilles blanchard insightful comments. andrychowicz m.and denil ´omez hoffman pfau schaul freitas. learning learn gradient descent gradient descent. advances neural information processing systems pages beck teboulle. fast gradient-based algorithms constrained total variation image denoising deblurring problems. ieee transactions image processing boyd parikh peleato eckstein. distributed optimization statistical learning alternating direction method multipliers. found. trends mach. learn. january bruckstein donoho elad. sparse solutions systems equations sparse modeling signals images. siam review bruer tropp cevher becker. designing statistical estimators balance sample size risk computational cost. ieee journal selected topics signal processing june g.-h. chen tang leng. prior image constrained compressed sensing method accurately reconstruct dynamic images highly undersampled projection data sets. medical physics mar. combettes j.-c. pesquet. proximal splitting methods signal processing. bauschke burachik combettes elser luke wolkowicz editors fixed-point algorithms inverse problems science engineering pages springer york daniely linial shalev-shwartz. data speeds training time learning halfspaces sparse vectors. international conference neural information processing systems pages daubechies defrise mol. iterative thresholding algorithm linear inverse problems sparsity constraint. communications pure applied mathematics duarte baraniuk. spectral compressive sensing. appl. boyd cand`es. differential equation modeling nesterovs accelerated gradient method theory insights. advances neural information processing systems tompson schlachter sprechmann perlin. accelerating eulerian ﬂuid simulation convolutional networks. proceedings international conference machine learning zeyde elad protter. single image scale-up using sparse-representations. proceedings international conference curves surfaces pages berlin heidelberg springer-verlag. duchi shalev-shwartz singer chandra. efﬁcient projections onto ℓ-ball learning high dimensions. proceedings annual international conference machine learning elad matalon zibulevsky. coordinate subspace optimization methods linear least squares non-quadratic regularization. appl. comput. harmon. anal. foucart rauhut. mathematical introduction compressive sensing. springer publishing company incorporated edition friedlander mansour saab yilmaz. recovering compressively sampled signals using partial support information. ieee trans. inf. theory giryes elad gribonval m.e. davies. greedylike algorithms cosparse analysis model. linear algebra applications special issue sparse approximate solution linear systems. hero piramuthu fessler titus. minimax emission computed tomography using high-resolution anatomical side information b-spline models. ieee trans. inf. theory apr. khajehnejad avestimehr hassibi. weighted minimization sparse recovery prior information. ieee international symposium information theory pages june oymak recht soltanolkotabi. sharp time–data tradeoffs linear inverse problems. appear ieee trans. inf. theory newton sketch linearconvergence. pilanci wainwright. randomized sketches convex programs sharp guarantees. ieee trans. inf. theory iterative hessian sketch fast accurate solution approximation constrained least-squares. journal machine learning research remez litany bronstein. picture worth billion bits real-time image reconstruction dense binary pixels. international conference computational photography", "year": 2016}