{"title": "Enhancing Multi-Class Classification of Random Forest using Random  Vector Functional Neural Network and Oblique Decision Surfaces", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Both neural networks and decision trees are popular machine learning methods and are widely used to solve problems from diverse domains. These two classifiers are commonly used base classifiers in an ensemble framework. In this paper, we first present a new variant of oblique decision tree based on a linear classifier, then construct an ensemble classifier based on the fusion of a fast neural network, random vector functional link network and oblique decision trees. Random Vector Functional Link Network has an elegant closed form solution with extremely short training time. The neural network partitions each training bag (obtained using bagging) at the root level into C subsets where C is the number of classes in the dataset and subsequently, C oblique decision trees are trained on such partitions. The proposed method provides a rich insight into the data by grouping the confusing or hard to classify samples for each class and thus, provides an opportunity to employ fine-grained classification rule over the data. The performance of the ensemble classifier is evaluated on several multi-class datasets where it demonstrates a superior performance compared to other state-of- the-art classifiers.", "text": "data instead single feature recent exhaustive comparison among classiﬁers ensemble random vector functional link networks popular single layer feed forward neural network also ranks amongst top-. decision trees random forest employ recursive partitioning training data smaller subsets classiﬁcation optimizing impurity criteria information gain gini index classical rafs achieve using single feature node partition training partitions generates axis-parallel orthogonal hyperplane node. hyperplanes always approximate complex decision boundaries variant random forest known oblique random forest oblique hyperplane used node. decision boundary uses linear combination features split training data. decision trees random forest exhaustively search single feature among random subset features node. however exhaustive search best oblique hyperplane computationally expensive thus search oblique splits generally based heuristic approaches non-optimal. circumvent issue present oblique random forest searches optimal linear hyperplane node ﬁnite search space optimizing gini-impurity criteria similar raf. random vector functional link network hand randomized variant functional link neural network weight bias vectors hidden layer rvfl randomly generated thus making learning algorithm less complicated faster train conventional back-prop based slfn generally cumbersome learn large datasets. classiﬁers random forest support vector machine scale well datasets large sample size feature dimension number classes. divide conquer strategies dimension reduction techniques common techniques employed cases. however number classes still pose constraint application decision trees based classiﬁers. abstract—both neural networks decision trees popular machine learning methods widely used solve problems diverse domains. classiﬁers commonly used base classiﬁers ensemble framework. paper ﬁrst present variant oblique decision tree based linear classiﬁer construct ensemble classiﬁer based fusion fast neural network random vector functional link network oblique decision trees. random vector functional link network elegant closed form solution extremely short training time. neural network partitions training root level subsets number classes dataset subsequently oblique decision trees trained partitions. proposed method provides rich insight data grouping confusing hard classify samples class thus provides opportunity employ ﬁne-grained classiﬁcation rule data. performance ensemble classiﬁer evaluated several multi-class datasets demonstrates superior performance compared stateof-the-art classiﬁers. performance classiﬁers signiﬁcantly improved aggregating decisions several classiﬁers instead using single classiﬁer. generally known ensemble classiﬁers multiple classiﬁer systems. ensemble obtained perturbing combining several individual classiﬁers speciﬁcally obtained perturbing training injecting randomness classiﬁer aggregating outputs classiﬁers suitable way. decision trees neural networks generally used ensemble generation. decision trees randomized neural networks unstable classiﬁers whose performance greatly vary even small perturbation training classiﬁer parameters. thus ideal candidates base classiﬁer ensemble framework. random forest ensemble decision trees exemplar ensembles. ranked classiﬁer based comparisons amongst classiﬁers datasets standard random forest however superseded oblique random forest ensemble decision trees employing linear hyperplanes node split approaches used handle scenarios trade performance computational complexity. rvfl hand effectively utilized large datasets. paper extend idea proposed fusing rvfl proposed oblique random forest show ensembles improve accuracy incurring less computational cost random forest based ensembles. rvfl partitions training dataset several subsets confusing difﬁcult samples grouped subset. technique allows employ ﬁner classiﬁcation rules focusing confusing difﬁcult-to-classify samples. experiments several datasets varying sample size number classes demonstrate proposed oblique random forest ensemble superior standard random forest oblique variant terms performance computational requirements. create hybrid ensemble rvfl oblique random forest performance oblique random forest classiﬁer. rest paper organized follows present brief review related works following section. section elucidate approach hybrid ensemble. section present experimental results comparison proposed hybrid ensemble different classiﬁers. finally present conclusions section proposing hybrid ensemble classiﬁer based decision trees neural networks section brieﬂy review decision trees random forest random vector functional link networks hybridization based classiﬁcation techniques. decision tree consists nodes edges. nodes either internal leaf internal nodes split child nodes stopping criterion become leaf. internal node associated test function deﬁned where threshold. outcome determines child node routed. instance represents left child node represents right child node. node chooses best test function pool potential test functions optimizing metric known gini-impurity. objective make resulting child nodes pure possible containing training samples single class only. based nature test functions decision trees categorized types univariate multivariate univariate decision tree parameter test function based single feature i.e. node selects single feature random subset features best minimizes gini-impurity. ﬁnal decision boundary produced tree fig. classiﬁcation boundaries generated axis-parallel oblique decision tree example binary classiﬁcation problem. axis-parallel staircase decision boundaries fail approximate optimal decision boundary. however oblique trees linear hyperplanes better perform task. staircase type shown fig. multivariate oblique decision trees depends linear combination features. since decision boundary orient direction axes trees hyperplanes also known oblique trees thus reformulated weight coefﬁcient feature random forest ensemble decision trees trained independently several instances training data obtained using bagging optimal split node chosen possible splits unique feature values typically square root feature dimension means worst possible scenario exhaustive search ‘optimal’ split grows linearly number training samples node search optimal oblique hyperplanes computationally expensive many heuristic search methods forest-lc proposed literature. generally linear classiﬁers mpsvm logistic regression used generate oblique hyperplanes however approaches used without optimizing impurity criteria search optimal oblique hyperplanes. multi-class classiﬁcation problem ususally reformulated binary problem deﬁning hyperclasses either clustering distance metric possible combinations linear decision boundary learned. approaches either optimize impurity criteria computationally expensive. however still integrate impurity optimization techniques optimal oblique hyperplane search obraf without incuring great computational costs implementing simple network decision trees. authors decision trees empirically determine number neurons needed three layers neural network. richmond extends idea mapping stacked convolutional neural networks semantic segmentation. similarly jerez decision trees identify important variables breast cancer data variables input neural network architecture. work integrates replacing ﬁnal softmax layer authors multi-layer perceptrons split functions node trees. different works present hybrid heterogeneous ensemble classiﬁers exploit probability like outputs neural network quickly partition training data efﬁcient multi-class classiﬁcation decision trees apart hybridization technique discussed above also review ideas relevant proposed method. generally binary splits popular decision trees researches multi-way splits. multi-way splits decision trees previously studied correlation used best single feature thresholds split training data multiple branches computed svm. however multi-way splits cumbersome determine improve performance decision trees another closely related work ours. uses deep neural network perform hierarchical partition data decision trees creating clusters confusing classes. classes clustered employing spectral coclustering algorithm confusion matrix computed validation dataset. thus computationally expensive requires large dataset. however employ simple fast neural network partition data without incurring large computational complexities. extend idea proposing improved oblique random forest. method based ensemble framework boosts multi-class classiﬁcation handling capacity random forest. rvfl followed oblique decision trees base classiﬁer ensemble framework. original training data obtained using bagging carefully partitioned rvfl several subsets decision trees employed afterwards improve classiﬁcation performance learning separate confusing training samples. decision trees speciﬁcally ensembles decision trees best classiﬁcation algorithm terms generalization ability robustness. partitions obtained using rvfl enables employ ﬁne-grained classiﬁcation rule decision trees classiﬁcation algorithm focuses difﬁcult classify samples. section ﬁrst describe data partitioning step rvfl present oblique random forest. fig. structure rvfl. lines direct links input output layer. weights blue links randomly generated certain range. output weights needs computed black links. best viewed color. rvfl single layer feed-forward neural network mainly characterized absence backpropagation presence direct links input output nodes weights input hidden neurons rvfl randomly generated suitable range. direct links rvfl regularize network effects randomization leading simpler model small number hidden neurons improving generalization performance neural network output layer rvfl consists nodes corresponding number classes node assigning score class. predicted class sample class represented node max) score given output node since hidden layer parameters randomly generated kept ﬁxed output weights need computed. learning objective rvfl closed form solution obtained using either least squares moore-penrose pseudoinverse. using moorepenrose pseudoinverse solution given using least squares closed form solution given employ rvfl node divide data partitions number classes dataset. proposed oblique decision tree trained thereafter partition separately improve accuracy. partition class distribution samples unique i.e. majority samples class rest classes. samples classes hard classify rvfl. partitioning possible utilizing output scores given rvfl. training phase training sample passed rvfl. output rvfl probability like score class particular data set. generally class highest score predicted class rvfl. however case classes highest second-highest scores selected potential classes indicate confusing classes particular training sample. partition rvfl corresponds class. thus used training data oblique decision trees associated classes/subsets. procedure repeated training samples creating training decision tree. ﬁnal model ensemble base classiﬁers. cases true class neither highest second-highest class training sample still placed true class. details readers refer decision trees employ recursive partitioning child nodes purer parent node. objective separate training samples different partitions partitions contain samples class only. partitions obtained exhaustive search best orthogonal hyperplane. problem however reformulated using information class labels training samples. many popular binary classiﬁers support vector machine one-vs-all approach breakdown multiclass classiﬁcation problem several binary classiﬁcation problems. speciﬁcally class single trained samples class positive samples negative samples. caveat always best method deal multi-class problems methods however integrated intactly internal nodes decision trees. stated earlier linear classiﬁer node need always perfect classiﬁer simply classiﬁcation. thus objective random forest restated separating class classes node. means instead performing exhaustive search search hyperplanes transforming multi-class classiﬁcation problems binary classiﬁcation problems number classes node restricts hyperplane search space ways node linear classiﬁer selected best optimizes impurity criteria. best scenario linear classiﬁer decision boundary result samples class child node rest training samples another child node exactly objective decision trees i.e. make child nodes purer parent nodes. thus employing linear hyperplanes impurity optimization technique help better capture geometric structure data axis-parallel hyperplanes. oblique decision trees employ mpsvm based linear classiﬁer. mpsvm generates non-parallel planes based proximity class ﬁnal decision boundary employed node based angle bisector planes proximal planes found eigenvectors corresponding smallest eigenvalues following generalized eigenvalue problems ginip ginic values gini impurity parent node child nodes respectively number data samples parent node number data samples reach left right child nodes current parent node number samples class left right child nodes respectively. issues one-vs-all technique computationally voracious number classes large. thus generating hyperplanes node computationally expensive cases. however also offers advantage. since linear hyperplane node selected pool hyperplanes based impurity criteria results pure child nodes faster previous exhaustive approaches non-impurity optimization approaches check stopping criteria node depth create partitions using one-vs-all approach. train linear classiﬁer partition. compute eqs. return hyperplane maximizes thus trees proposed oblique random forest variant generally shallow compared standard trees. negate complexity associated generating many hyperplanes node. section validate experiments several datasets. employing rvfl node subset partition contains majority samples classes rest others. linear classiﬁers trained greater number training samples likely better optimize impurity criteria compared classiﬁers trained samples avoid classes instead choose best hyperplane pool hyperplanes obtained using classes larger training data only. intuitive since hyperplane generated using one-vs-all method attempts separate class rest classes thus favours classes larger training data better optimizes observation also based experiments best hyperplane usually trained larger number classes. thus number classes large employ onevs-all approach classes hyperparamater. based experiments decreases computational complexity model without incuring signiﬁcant loss performance. method particularly suitable multi-core distributed environment partitioning rvfl partition parallel distributed across different cores. similarly hyperplane generation operation using one-vs-all also distributed. method also suitable large datasets. however caveat associated ensemble classiﬁer employed many class classiﬁcation problems. binary classes partitions provided rvfl duplicated versions. section compare performance random forests variants hybrid ensemble. compare four classiﬁers multi-class datasets. number classes datasets vary datasets selected based size number classes performance oblique random forest hybrid ensemble follow experimental setup fair comparison ensemble methods values common parameters. thus classiﬁer ensemble size number trees number random features node feature dimension. feature vector normalized feature normalized removing mean dividing variance. ensembles trees fully grown terminating criteria rvfl conﬁguration. objective proposed method obtain diverse rvfl models base classiﬁer turn results diverse data partitions hence diverse decision trees. parameter setting rvfl randomly picks activation function network parameters parameter settings listed below table present classiﬁcation accuracies classiﬁer dataset. first compare performance random forest variants obraf obraf. almost datasets except plant margin proposed oblique random forest outperforms standard random forest oblique random forest. although obraf obraf employ linear decision boundary node using mpsvm obraf performs search optimal linear boundary. suggests oblique random forests tables show training time average number nodes comparison random forest classiﬁers. comparison select datasets plant shape classes pendigits medium size dataset. even though proposed oblique random forest employs one-vsall approach still offers computational advantages obraf evident shorter training time less number nodes. average classiﬁcation accuracies obraf obraf obrafl respectively. hybrid ensemble highest accuracy followed proposed oblique random forest obraf. however comparing classiﬁers using average accuracy susceptible outliers atone classiﬁer’s poor performance dataset excellent performance other. thus follow procedure rank classiﬁer assess performance. approach classiﬁer ranked based performance means highest performing classiﬁer ranked second highest rank mean ranks classiﬁer datasets presented table iii. hybrid ensemble obrafl ranked classiﬁer followed proposed oblique random forest obraf. thus experimental results infer employing obraf partitions provided rvfl improves performance. rvfl provides partitions confusing samples classiﬁcation rules focus difﬁcult-to-classify samples employed. rules easily implemented decision trees random forest owing superior robust performance. furthermore proposed oblique random forest improves random forest employing rvfl node obtain robust superior performance. tree depth number features randomly selected node number trees common parameters random forest based methods. evaluate inﬂuence parameter proposed oblique random forest hybrid classiﬁer pendigits dataset. similar conclusions pertain datasets well. analysis maximum depth number trees number random features varied ranges respectively. tree depth. generally trees random forest fully grown. fig. sharp increase accuracy tree depth increases implies obtain good performance trees deeper. trees hybrid classiﬁer grown reduced data trees usually shallow. however gives good performance oblique random forest even number features. number features randomly selected node controls diversity trees forest. small value results uncorrelated trees whereas large values result correlated trees. generally number trees. number trees increases generalization ability random forest based methods also increases. however large number trees ensemble size also increases computational cost. slight improvement performance beyond ensemble size fig. observe even small tree depth ensemble size hybrid ensemble classiﬁer provides near maximal performance. large datasets random forest require large number deep trees provide acceptable performance. computationally intractable. however employing hybrid ensemble shallow trees small ensemble size eschew expensive computational requirements still obtain good performance. paper ﬁrst proposed oblique decision tree uses impurity optimization techniques similar trees random forests. employed oblique decision trees fast rvfl network create hybrid ensemble. base classiﬁer decision trees trained samples partitioned rvfl. marriage decision trees fast neural network enhances capability decision trees handle multi-class classiﬁcation problem evident performance hybrid ensemble several machine learning datasets. even small tree depth ensemble size hybrid ensemble achieve superior performance compared standard random forest classiﬁers. signiﬁcantly preserve computational resources time dealing large datasets. interesting traits hybrid ensemble parallelization distributed computation several jobs effectively distributed several cores machines. however gain seized reduce communication overhead. even though proposed oblique random forest faster training time partitioning application rvfl adds complexity ensemble. thus future work improve training time hybrid ensemble efﬁcient distributed computing. fern´andez-delgado cernadas barro amorim need hundreds classiﬁers solve real world classiﬁcation problems? journal machine learning research vol. available http//jmlr.org/papers/v/delgadoa.html zhang suganthan benchmarking ensemble classiﬁers novel co-trained kernal ridge regression random vector functional link ensembles ieee computational intelligence magazine vol. criminisi shotton konukoglu decision forests uniﬁed framework classiﬁcation regression density estimation manifold learning semi-supervised learning foundations trends® computer graphics vision vol. murthy singh chen manmatha comaniciu deep decision network multi-class image classiﬁcation computer vision pattern recognition ieee conference menze kelm splitthoff koethe hamprecht oblique random forests joint european conference machine learning knowledge discovery databases. springer zhang varadarajan suganthan ahuja moulin robust visual tracking using oblique random forests ieee international conference computer vision pattern recognition. ieee richmond kainmueller yang myers rother relating cascaded random forests deep convolutional neural networks semantic segmentation arxiv preprint arxiv. jerez-aragon´es g´omez-ruiz ramos-jim´enez mu˜nozp´erez alba-conejo combined neural network decision trees model prognosis breast cancer relapse artiﬁcial intelligence medicine vol. sadoghi yazdi salehi moghaddami poostchi mohammadabadi correlation based splitting criterionin multi branch decision tree central european journal computer science vol.", "year": 2018}