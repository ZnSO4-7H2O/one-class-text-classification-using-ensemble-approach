{"title": "Convolutional Kernel Networks", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.", "text": "important goal visual recognition devise image representations invariant particular transformations. paper address goal type convolutional neural network whose invariance encoded reproducing kernel. unlike traditional approaches neural networks learned either represent data solving classiﬁcation task network learns approximate kernel feature training data. approach enjoys several beneﬁts classical ones. first teaching cnns invariant obtain simple network architectures achieve similar accuracy complex ones easy train robust overﬁtting. second bridge neural network literature kernels natural tools model invariance. evaluate methodology visual recognition tasks cnns proven perform well e.g. digit recognition mnist dataset challenging cifar- stl- datasets accuracy competitive state art. recently seen revival attention given convolutional neural networks high performance large-scale visual recognition tasks architecture cnns relatively simple consists successive layers organized hierarchical fashion; layer involves convolutions learned ﬁlters followed pointwise non-linearity downsampling operation called feature pooling. resulting image representation empirically observed invariant image perturbations encode complex visual patterns useful properties visual recognition. training cnns remains however difﬁcult since high-capacity networks involve billions parameters learn requires high computational power e.g. gpus appropriate regularization techniques exact nature invariance cnns exhibit also precisely understood. recently invariance related architectures characterized; case wavelet scattering transform hierarchical models work revisits convolutional neural networks adopt signiﬁcantly different approach traditional one. indeed kernels natural tools model invariance inspired hierarchical kernel descriptors propose reproducing kernel produces multi-layer image representations. main contribution approximation scheme called convolutional kernel network make kernel approach computationally feasible. approach type unsupervised convolutional neural network trained approximate kernel map. interestingly network uses non-linear functions resemble rectiﬁed linear units even though handcrafted naturally emerge approximation scheme gaussian kernel map. label information used subsequently support vector machine achieve competitive results several datasets mnist cifar- stl- simple architectures parameters learn data augmentation. open-source code learning convolutional kernel networks available ﬁrst author’s webpage. arc-cosine kernels. kernels building deep large-margin classiﬁers introduced multilayer arc-cosine kernel built successive kernel compositions layer relies integral representation. similarly kernels rely integral representation enjoy multilayer construction. however contrast arc-cosine kernels build sequence kernels convolutions using local information spatial neighborhoods propose training procedure learning compact representation kernel data-dependent manner. multilayer derived kernels. kernels invariance properties visual recognition proposed kernels built parameterized neural response function consists computing maximal response base kernel local neighborhood. multiple layers built iteratively renormalizing response kernels pooling using neural response functions. learning performed plugging obtained kernel svm. contrast propagate information lower upper layers using sequences convolutions. furthermore propose simple effective data-dependent learn compact representation kernels show obtain near state-of-the-art performance several benchmarks. hierarchical kernel descriptors. kernels proposed produce multilayer image representations visual recognition tasks. discuss details kernels next section paper generalizes establishes strong link convolutional neural networks. convolutional multilayer kernel generalization hierarchical kernel descriptors introduced computer vision kernel produces sequence image representations built multilayer fashion. layer interpreted non-linear transformation previous additional spatial invariance. call layers image feature maps formally deﬁne follows deﬁnition image feature function subset representing normalized coordinates image hilbert space. practical examples paper two-dimensional grid corresponds different locations two-dimensional image. words pixel coordinates. given point represents characteristics image location neighborhood instance color image size three channels green blue represented initial feature regular grid euclidean space provides color pixel values. multilayer scheme non-trivial feature maps obtained subsequently encode complex image characteristics. terminology hand introduce convolutional kernel ﬁrst single layer. deﬁnition consider images represented image feature maps respectively pixel locations hilbert space. one-layer convolutional kernel deﬁned kernel literature feature denotes mapping data points representation reproducing kernel hilbert space here feature maps refer spatial maps representing local image characteristics everly location usual neural network literature easy show kernel positive deﬁnite consists pairwise comparisons image features computed spatial locations signiﬁcant comparison needs corresponding close normalized features close feature space parameters respectively control deﬁnitions closeness. indeed large kernel invariant positions small features placed location compared other. therefore role control much kernel locally shift-invariant. next show beyond single layer that present concrete examples simple input feature maps gradient map. assume provides two-dimensional gradient image pixel often computed ﬁrst-order differences along dimension. then quantity kϕkh gradient intensity orientation characterized particular angle—that exists sin]. resulting kernel exactly kernel descriptor introduced natural image patches. patch map. setting associates location image patch size centered then space simply rm×m contrast-normalized version patch useful transformation visual recognition according classical ﬁndings computer vision image encoded three color channels patches size deﬁne multilayer convolutional kernel generalizing ideas deﬁnition consider hilbert space hk–. build hilbert space follows choose patch shape deﬁned bounded symmetric subset coordinates location patch {zk} subset ωk–; words coordinate corresponds valid patch centered deﬁne convolutional kernel patch feature maps replacing appropriate smoothing parameters denote hilbert space positive deﬁnite kernel reproducing. image represented feature layer encoded k-th layer representation patch feature concretely kernel patches kz−z′k xz∈pk xz′∈pk hilbertian norm hk–. figure illustrate interactions sets coordinates patches feature spaces across layers. two-dimensional grids typical patch shape square example {−/n {−/n patch image size information encoded k-th layer differs aspects ﬁrst point layer contains information several points layer possibly represent larger patterns; second feature locally shift-invariant previous term involving parameter multilayer convolutional kernel slightly differs hierarchical kernel descriptors exploits similar ideas. deﬁne indeed several kernels representing local information images gradient color shape. kernels close deﬁned variations. normalized features kernels different weighting strategies summands specialized image modality e.g. color gradient whereas weight kϕkh kϕ′kh kernels. generic formulation propose useful main contribution comes next section kernel tool learning convolutional neural networks. figure left concrete representation successive layers multilayer convolutional kernel. right layer convolutional neural network approximates kernel. generic schemes proposed approximating non-linear kernel linear nystr¨om method variants random sampling techniques fourier domain shift-invariant kernels context convolutional multilayer kernels approximation critical computing full kernel matrix database images computationally infeasible even moderate number images moderate number layers. reason nystr¨om method hierarchical kernel descriptors. section show coordinate sets two-dimensional regular grids natural approximation multilayer convolutional kernel consists sequence spatial convolutions learned ﬁlters pointwise non-linearities pooling operations illustrated figure precisely scheme approximates kernel deﬁned layer ﬁnite-dimensional spatial maps coordinates related positive integer controlling quality approximation. consider indeed images represented layer image feature maps call approximation scheme convolutional kernel network comparison cnns approach enjoys similar beneﬁts efﬁcient prediction test time involves ﬁlters hyper-parameters number layers numbers ﬁlters layer shape sizes feature maps. parameters automatically chosen discussed later. training argued simple training unsupervised manner since show main difference cost function optimized. component formulation gaussian kernel. start approximating linear operation learned ﬁlters followed pointwise non-linearity. starting point next lemma obtained simple calculation. lemma gives mapping function kernel linear constant front integral. obtain ﬁnite-dimensional representation need approximate integral weighted ﬁnite classical problem arising statistics chapter then consider different cases. small dimension data lives compact integral approximated uniform sampling large enough set. choose strategy types kernels spatial kernels ˜ϕ′k gradient presented section latter case gradient orientation. typically sample orientations explained section higher dimensions. prevent curse dimensionality learn approximate kernel training data intrinsically low-dimensional. optimize importance weights interestingly already draw links neural networks. applied unit-norm vectors problem produces sampling points whose norm close one. learning unit-norm point mapped vector kx−wlk assuming norm always written therefore ﬁnite-dimensional representation involves linear operation followed non-linearity typical neural networks. figure show shape resembles rectiﬁed linear unit function tools hand build convolutional kernel network. start making assumptions input data present learning scheme approximation principles. zeroth layer. assume input data ﬁnite-dimensional extracts patches formally exists patch shape patch centered then property described beginning section satisﬁed choosing examples input feature maps given earlier satisfy ﬁnite-dimensional assumption gradient gradient image along direction patch patch input image data. convolutional kernel network. zeroth layer characterized present algorithms subsequent layers learn parameters feedforward manner. interesting note input parameters algorithm exactly cnn—that number layers ﬁlters sizes patches feature maps ultimately cnns ckns differ cost function optimized learning ﬁlters choice non-linearities. show next exists link parameters convolutional multilayer kernel. vector representing patch centered shape vector ˜ψk– ℓ-normalized version ψk–. operation interpreted spatial convolution ﬁlters followed pointwise non-linearities; approximation principles. proceed recursively show kernel approximation property satisﬁed; assume holds layer then show also hold layer sufﬁcient purpose since previously assumed zeroth layer. given images feature maps replacing constant comes multiplication constant giving property result right-hand side exactly remains show property also holds speciﬁcally quantity approximated euclidean inner-product patches shape applied layer replacing {zk}+pk. substitution immediately obtain approximation then gaussian terms negligible other—say ku−zk thus replace sumspu∈ω′ pzz′∈ωk– non-negligible terms. yields exactly approximation stochastic gradient descent used since potentially inﬁnite amount training data available. however preferred l-bfgsb pairs randomly selected training data points initialize k-means algorithm. l-bfgs-b parameter-free state-of-the-art batch method fast much easier use. always l-bfgs-b algorithm iterations seems ensure convergence stationary point. goal demonstrate preliminary performance type convolutional network leave future work speed improvement. present experiments performed using matlab l-bfgs-b solver interfaced stephen becker. image represented last used linear implemented software package liblinear representations centered rescaled unit ℓ-norm average regularization parameter always selected validation -fold cross-validation range patches typically small; tried sizes ﬁrst layer upper ones. number ﬁlters experiments downsampling factor always chosen consecutive layers whereas last layer downsampled produce ﬁnal maps small size—say gradient approximate gaussian kernel maxk unsupervised learning ﬁrst used discovering underlying structure natural image patches olshausen field without making priori assumption data except parsimony principle method able produce small prototypes resemble gabor wavelets—that spatially localized oriented basis functions. results found impressive scientiﬁc community work received substantial attention. also known results also achieved cnns show section also case convolutional kernel networks even though explicitly trained reconstruct data. following randomly select database whitened natural image patches size learn ﬁlters using formulation initialize gaussian random noise without performing k-means step order ensure output obtain artifact initialization. figure display ﬁlters associated top- largest weights among ﬁlters exhibit interpretable gabor-like structures rest less interpretable. best knowledge ﬁrst time explicit kernel gaussian kernel whitened natural image patches shown related gabor wavelets. mnist dataset consists images handwritten digits training testing. types initial maps networks patch denoted cnkpm gradient denoted cnk-gm. follow evaluation methodology table test error various approaches mnist dataset without data augmentation. numbers parentheses represent size feature maps layer. comparison varying training size. select regularization parameter -fold cross validation training size smaller otherwise keep examples training validation. report table results obtained four simple architectures. ckn-gm simplest second layer uses patches ﬁlters resulting network parameters. achieves outstanding performance error full dataset. best performing ckn-gm similar ckn-gm uses ﬁlters. working patches layers gives better results layer. details network architectures provided supplementary material. general method achieves state-of-the-art accuracy task since lower error rates reported using data augmentation move challenging datasets cifar- stl- select best architectures validation examples training cifar- -fold cross-validation stl-. report table results ckn-gm deﬁned previous section without exploiting color information ckn-pm working patches whose mean color subtracted. best selected models always layers ﬁlters layer. since ckn-pm ckn-gm exploit different information also report combination models ckn-co concatenating normalized image representations together. standard deviations stl- always approach appears competitive state especially stl- method better ours despite fact models layers require learning parameters. note better results reported table obtained literature using either data augmentation external data planning investigate similar data manipulations future. paper proposed methodology combining kernels convolutional neural networks. show mixing ideas concepts fruitful since achieve near state-of-the-art performance several datasets mnist cifar- simple architectures data augmentation. challenges regarding work left open future. ﬁrst supervision better approximate kernel prediction task. second consists leveraging kernel interpretation convolutional neural networks better understand theoretical properties feature spaces networks produce.", "year": 2014}