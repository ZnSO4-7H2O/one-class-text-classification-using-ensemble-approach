{"title": "Robust Report Level Cluster-to-Track Fusion", "tag": ["cs.AI", "cs.NE", "I.2.3; I.2.6; I.4.8; I.5.3"], "abstract": "In this paper we develop a method for report level tracking based on Dempster-Shafer clustering using Potts spin neural networks where clusters of incoming reports are gradually fused into existing tracks, one cluster for each track. Incoming reports are put into a cluster and continuous reclustering of older reports is made in order to obtain maximum association fit within the cluster and towards the track. Over time, the oldest reports of the cluster leave the cluster for the fixed track at the same rate as new incoming reports are put into it. Fusing reports to existing tracks in this fashion allows us to take account of both existing tracks and the probable future of each track, as represented by younger reports within the corresponding cluster. This gives us a robust report-to-track association. Compared to clustering of all available reports this approach is computationally faster and has a better report-to-track association than simple step-by-step association.", "text": "abstract paper develop method report tracking based dempster-shafer clustering using potts spin neural networks clusters incoming reports gradually fused existing tracks cluster track. incoming reports cluster continuous reclustering older reports made order obtain maximum association within cluster towards track. time oldest reports cluster leave cluster fixed track rate incoming reports fusing reports existing tracks fashion allows take account existing tracks probable future track represented younger reports within corresponding cluster. gives robust report-to-track association. compared clustering available reports approach computationally faster better report-to-track association simple step-by-step association. paper develop robust incremental clusterto-track fusion algorithm ability take report-to-track second associations. using dempster-shafer clustering potts spin neural networks incremental manner incoming reports gradually fused existing tracks. definition. dempster-shafer method clustering uncertain data using conﬂict dempster’s rule distance measure. section describe memory management used spin dempster-shafer clustering process described section case shortlong-term memory. finally series test runs performed evaluate clustering performance computation time different sizes shortlong-term memory. previously used intelligence management process high number intelligence reports clustered opportunity intelligence arrived ongoing clustering could immediately classiﬁed comparison previous cluster result older intelligence impossible constantly recluster newly arrived intelligence time intelligence older complexity. wait ongoing clustering process terminated could start clustering intelligence arrived latest clustering process. here develop dynamic clustering process reclustering made continuously intelligence arrive process. organized current intelligence three different memories. first recently arrived intelligence short-term memory. secondly older intelligence maintained long-term memory ﬁnally intelligence longer used clustering kept history memory used evaluation clustering performance. potts model handle data pairwise terms. conﬂict therefore necessary simplify function write conﬂict pairwise conﬂicts linearize conﬂict function taking logarithm. rewrite minimization follows changing states sia’s means report cluster model serve clustering method used penalty factor report cluster; reports different clusters penalty. dempster-shafer clustering conﬂict dempster’s rule elements within subset combined indication whether reports belong together. higher conﬂict less credible belong together. order apply potts model dempstershafer clustering −log basic probability numbers reports minimizing energy function also minimize overall conﬂict. minimization carried simulated annealing. simulated annealing temperature important parameter. process starts high temperature change state less random begin lower temperature gradually. temperature lowered spins become biased interactions reaching minimum energy function. also gives best partition intelligence clusters minimal overall conﬂict. moved long-term memory intelligenceto-cluster associations maintained future. similarly oldest report long-term memory moved history kept investigation clustering performance purposes figure complete reclustering intelligence short-term memory made using potts spin dempster-shafer clustering take account conﬂicts intelligence reports short-term long-term memories updating cluster association intelligence short-term memory. thus intelligence long-term memory short-term remains ﬁxed memory report-to-cluster association unchanged time. clustering short-term memory short-term long-term memories together history jointly make current view tracks report track association. long-term memory history together make permanent view reportto-track association question report-to-track association short-term memory change intelligence becomes available. perform clustering reports short-term memory using dempster-shafer clustering potts spin mean ﬁeld theory. method minimize distance measure weight conﬂict neural clustering. summary potts spin method application information fusion intelligence analysis presented section provide short sketch method sake completeness. clustering process separate intelligence subsets tracked objects. combine dempster-shafer theory potts spin neural network model powerful solver large scale dempster-shafer clustering problems. sake clustering algorithm could content ﬁrst reports ﬁlling short-term memory successively building long-term memory order measure clustering performance consistent manner need equal number reports shortterm memory long-term memory history together. order minimize energy function eqs. used recursively belong short-term memory stationary equilibrium state reached temperature. then temperature lowered step step constant factor stationary equilibrium state figure create different data sets reports data drawn different random order without replacement reports given random basic probability number uniformly represent uncertainty report. tests different sizes short-term long-term memory. short-term memory varies size increment ﬁve. long-term memory varies zero minus size short-term memory. thus short-term long-term memories never greater total perform test different sizes short-term long-term memories. test start number reports equal size short-term memory cluster these. receive reports one-by-one recluster short-term memory report. step oldest short-term memory moved long-term memory. goes total reports drawn. memories ﬁlled additional reports transferred history short-term memory size long-term memory size reports respectively memories reports transferred history. basic probability number report test uniformly distributed random number thus expected conﬂict pieces evidence known conﬂict i.e. expected weight conﬂict −log therefore expected weight conﬂict pieces evidence drawn randomly becomes thirdly weight conﬂict cluster misclassiﬁcation yield number misclassiﬁcations target result divided average number reports cluster yield classiﬁcation error rate. note high conﬂict reports proof misclassiﬁcations conﬂict proof perfect classiﬁcation allowed representation reports dempster-shafer theory. reports refer targets e.g. reports propositions misclassiﬁed appear cluster regardless cluster since nonempty intersection zero conﬂict. thus error rates directly comparable error rates bayesian theory sensors must report single targets. error rates paper used compare performance different conﬁgurations short-term long-term memories. used building long-term memory history ﬁrst reports. order measure performance conﬁguration data perform clustering processes different sizes short-term long-term memories clustering processes conﬁguration figure plotted average classiﬁcation error rate different conﬁgurations shortterm long-term memory. notice figure roughly speaking obtain good classiﬁcation rate joint size short-term long-term memory maximal i.e. equal obtain error rate good conﬁguration would fairly small short-term memory large long-term memory order clustering performance decent computation time. closer look made figure classiﬁcation error rate average computation time plotted three joint sizes short-term long-term memories different sizes short-term memory. observe crucial clustering performance maximal joint size shortterm long-term memory especially since computation time virtually unchanged size long-term memories decreased ﬁxed size short-term memory. much important actual sizes memories. choice actual conﬁguration always domain dependent test example good performances found sizes short-term memory classiﬁcation errors computation time reasonable many problems. short-term memories sizes classiﬁcation error rates respectively computation times seconds respectively table lowest classiﬁcation computation time seconds fastest computation time seconds classiﬁcation error rate average computation time conﬁgurations. notice computation time mostly dependent size short-term memory. fairly small short-term memories receive computation time second. short-term memory size might expected. apparently reduction dimensionality yields smaller energy landscape ﬁnding good minimum easier. compensates risk using frozen long-term memory reclustering must made. however reduction short-term memory size carried rate increases expected classiﬁcation error simultaneous clustering. demonstrated incremental cluster-totrack fusion algorithm based dempster-shafer cluster using potts spin neural network robust. robustness achieved second thought report-to-track association made last arrives. reclustering process small classiﬁcation error rate report-to-track association. bengtsson schubert dempster-shafer clustering using potts spin mean ﬁeld theory soft computing vol. june schubert specifying nonspeciﬁc evidence int. intell. syst. schubert clustering belief functions based attracting conﬂicting metalevel evidence proc. ninth int. conf. information processing management uncertainty knowledge-based systems annecy france appear. smets practical uses belief functions proc. fifteenth conf. uncertainty artiﬁcial intelligence k.b. laskey prade stockholm sweden jul− morgan kaufmann francisco", "year": 2003}