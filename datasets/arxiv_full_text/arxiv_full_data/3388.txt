{"title": "A Probabilistic Optimum-Path Forest Classifier for Binary Classification  Problems", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Probabilistic-driven classification techniques extend the role of traditional approaches that output labels (usually integer numbers) only. Such techniques are more fruitful when dealing with problems where one is not interested in recognition/identification only, but also into monitoring the behavior of consumers and/or machines, for instance. Therefore, by means of probability estimates, one can take decisions to work better in a number of scenarios. In this paper, we propose a probabilistic-based Optimum Path Forest (OPF) classifier to handle with binary classification problems, and we show it can be more accurate than naive OPF in a number of datasets. In addition to being just more accurate or not, probabilistic OPF turns to be another useful tool to the scientific community.", "text": "abstract probabilistic-driven classiﬁcation techniques extend role traditional approaches output labels only. techniques fruitful dealing problems interested recognition/identiﬁcation only also monitoring behavior consumers and/or machines instance. therefore means probability estimates take decisions work better number scenarios. paper propose probabilistic-based optimum path forest classiﬁer handle binary classiﬁcation problems show accurate na¨ıve number datasets. addition accurate probabilistic turns another useful tool scientiﬁc community. havior. additionally expected learned function generalize well unseen data. depending amount information used concerning learning process decision functions usually divided three main categories supervised semi-supervised unsupervised former approaches make fully-labeled training semi-supervised approaches consider partial-labeled data only. finally unsupervised techniques knowledge training samples. techniques commonly referred clustering. classiﬁcation techniques usually divided according output well abstract ranking conﬁdence. abstract-based classiﬁers refer great majority techniques output label sample classiﬁed. ranking-driven approaches also output labels possible outputs considered given sample queued using sort heuristic applied diﬀerent purposes. finally conﬁdenceoriented techniques output conﬁdence value related probability sample assigned given label. last category concerns so-called probabilistic classiﬁers. probabilistic techniques play important role machine learning since extend classiﬁcation process greater range simply labels. often face problems desirable obtain probability label itself. consider problem theft identiﬁcation energy distribution systems. electrical power companies consider much fruitful monitor probability certain user become thiefer along time instead purely identifying user. probabilities time hands company take preventive approach much cost-eﬀective punishing user. fortunately considerable number probabilistic-driven techniques literature. seminal work conducted platt extended well-known support vector machines ﬁrst designed handle abstract outputs probabilistic classiﬁcation. idea quite simple svms’ outputs feed logistic function. therefore initial outputs mapped within range however order cope problems related diﬀerent quantities author considered optimization process whole training order variables regularize label-probability mapping process. technique often referred platt scaling. later niculescu-mizil caruana presented interesting comparison platt scaling isotonic regression obtain probabilistic outputs concerning svms. work motivated fact logistic functions work well several situations appropriate others. roughly speaking isotonic regression aims learning function constrained monotonically increasing svms’ real-valued outputs authors concluded platt scaling works better small-sized datasets since isotonic regression prone overﬁtting recommended applied large datasets. zadrozny elkan proposed obtain probability estimates considering decision trees na¨ıve bayesian classiﬁers. authors adopted smoother probability estimates i.e. adjust less extreme. smoothing interesting tool dealing probability estimation since methods push probabilities away range others adjust probabilities closer interesting classes equiprobable soon after group authors extended work handle multiclass-oriented problems recent works referred well mainly focus application probabilistic classiﬁers comparison studies only theories approaches. years group authors introduced optimum-path forest classiﬁer framework design graph-based classiﬁers comprises supervised semi-supervised unsupervised versions roughly speaking classiﬁer models problem pattern recognition graph partition task samples compete among order conquer remaining samples means rewardcompensation process. ﬁnal optimum-path forest essentially collection optimum-path trees rooted prototype sample. demonstrated suitable results number applications usually faster svms training tough similar even better accuracy. however na¨ıve works abstract outputs only. also know recent work considered conﬁdence-based probability estimates work proposed learn conﬁdence level training sample classifying others. additionally cost-function used conquering purposes adapted consider reliability level. authors showed proposed conﬁdence-based works better datasets high concentration overlapped samples. furthermore best knowledge probabilistic-driven date turns main contribution work i.e. lack research regarding conﬁdence-based outputs respect classiﬁers. proposed approach initially designed cope binary-oriented classiﬁcation problems compared na¨ıve diﬀerent scenarios showing suitable results. remainder paper organized follows. sections present theoretical background probabilistic-driven approach respectively. section discusses methodology section presents experiments. finally section states conclusions future works. λ-labeled dataset stand training testing sets respectively. additionally n-dimensional sample encodes features extracted certain data function computes distance samples graph derived training node connected every node dtr\\{v} i.e. deﬁnes adjacency relation known complete graph arcs weighted function also deﬁne path sequence adjacent distinct nodes terminus node dtr. notice trivial path denoted i.e. single-node path. path-cost function essentially assigns real positive value given path prototype nodes. roughly speaking aims solving following optimization problem since prototypes play major role papa proposed position regions highest probabilities misclassiﬁcation i.e. boundaries among samples diﬀerent classes. fact looking nearest samples diﬀerent classes computed means minimum spanning tree gtr. interesting properties ensure errorless training arc-weights diﬀerent finally respect path-cost function requires smooth previous experience image segmentation authors chain code-invariant path-cost function basically computes maximum arc-weight along path denoted fmax given stands concatenation path short computing equation every sample obtain collection optimum-path trees rooted originate optimum-path forest. sample belongs given means strongly connected gtr. roughly speaking training step aims solving equation order build optimum-path forest. next step concerns testing phase sample classiﬁed individually follows connected training nodes optimumpath forest learned training phase evaluated node conquers i.e. satisﬁes following equation worth noting distance-based classiﬁer instead uses power connectivity among samples. complete graph degenerates nearest neighbor classiﬁer training samples prototypes. actually situation considerably diﬃcult face thus indicating high degree overlapping among samples means features used speciﬁc problem adequate enough describe probabilistic inspired platt scaling approach basically ends mapping svms’ output probability estimates. therefore introducing proposed approach must master platt scaling mechanism. considering labeled dataset described section assume sample assigned class label |d|. platt proposed approximate posterior class probability follows rationale behind proposed approach assume lower cost assigned sample i.e. higher probability sample correctly classiﬁed. similar idea used platt since greater likely sample belongs +exp catastrophic cancellation close one. term arises fact need subtract relatively close number already results previous ﬂoating-point operations. described interesting example suppose case returns equivalent formulation +exp gives accurate result. also group authors stated aforementioned catastrophic cancellation induces occurrences. however even using equations overﬂow problem still occur. order cope problem proposed apply equation otherwise equation similarly adopted procedure concerning probabilistic hereinafter called p-opf. short implement p-opf changing yici equations learning parameters compute probability sample belong class i.e. p-opf assigns label sample; otherwise sample assigned class work adopted since models single chance. however easily ﬁne-tune threshold using linear-search optimization algorithm. section present methodology used compare p-opf nai¨ıve opf. although could consider probabilistic classiﬁer comparison purposes main idea work concern outperforming techniques propose probabilistic technique instead. order ﬁne-tune parameters employed four diﬀerent optimization methods three based meta-heuristics another purely mathematical. regard meta-heuristic-driven techniques opted algorithm fireﬂy algorithm particle swarm optimization respect another mathematical method used nelder-mead main reason aforementioned techniques concerns good eﬀectiveness number problems literature. order study behavior p-opf diﬀerent scenarios used three synthetic datasets datasets concerning energy theft detection well nine public benchmarking datasets. datasets frequently used evaluation diﬀerent classiﬁcation methods. table presents main characteristics dataset. dataset australian comercial industrial breast colon cancer diabetes fourclass heart ionosphere ionosphere scale liver synthetic synthetic synthetic addition randomly divided dataset disjoint sets training testing training testing sizes deﬁned respectively. experimental setup conducted using cross-validation procedure runnings. order compare p-opf computed mean accuracy execution time usage wilcoxon signed rank test signiﬁcance order justify application conventional optimization method plotted ﬁtness landscape optimization function built gridsearch search space figure depicts ﬁtness landscape concerning industrial diabetes datasets. smoothness apparently quasi-convexity opted employ conventional technique optimization purposes section present experimental results regarding probabilistic opf. tables present mean accuracy computational load concerning compared methods. accurate techniques considering wilcoxon test highlighted bold. dataset australian comercial industrial breast cancer colon cancer diabetes fourclass heart ionosphere ionosphere scale liver synthetic synthetic synthetic proposed p-opf obtained best results nine datasets na¨ıve achieved best result eleven datasets. three nine datasets p-opf obtained results. results quite interesting since p-opf able improve datasets besides able output probability estimates. considering datasets although p-opf outperform former achieved considerably close results somehow interesting since popf obtain similar accuracies compared able output probabilities well. regard optimization techniques obtained best results eight datasets closely followed obtained best results seven datasets. however consider trade-oﬀ computational load accuracy best optimization approach since lower computational cost. good performance mainly smoothness objective functions. table presents mean computational load seconds concerning na¨ıve p-opf parameters ﬁne-tuned optimization techniques. since swarm-based techniques means update possible solutions iteration much costly finally conducted extra round experiments assess inﬂuence threshold parameter figures display accuracy values diﬀerent thresholds considering four datasets breast cancer comercial industrial ionosphere. clearly observe datasets contain certain plateau accuracies considering diﬀerent threshold values dataset australian industrial industrial breast cancer colon cancer diabetes fourclass heart ionosphere ionosphere scale liver synthetic synthetic synthetic probabilistic classiﬁcation topic great interest concerning machine learning community mainly lack ﬂexible information rather labels only. work cope problem proposing probabilistic binary classiﬁcation problems namely p-opf. results proposed p-opf compared na¨ıve number datasets achieving suitable results several them. also compared four optimization techniques minimize cost function aiming learning best parameters whole training set. regard future works extending p-opf multi-class classiﬁcation problems well consider optimization techniques ﬁne-tune parameters help minimizing cost function. also shall consider using derivative cost function together optimization techniques require computation explicitly.", "year": 2016}