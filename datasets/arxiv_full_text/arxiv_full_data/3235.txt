{"title": "Domain Adaptations for Computer Vision Applications", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "A basic assumption of statistical learning theory is that train and test data are drawn from the same underlying distribution. Unfortunately, this assumption doesn't hold in many applications. Instead, ample labeled data might exist in a particular `source' domain while inference is needed in another, `target' domain. Domain adaptation methods leverage labeled data from both domains to improve classification on unseen data in the target domain. In this work we survey domain transfer learning methods for various application domains with focus on recent work in Computer Vision.", "text": "bergamo torresani although related ﬁeld covariate shift longer history. perhaps indicative ﬁeld proposed methods different characteristic. best knowledge previous survey domain adaptation focused learning theory natural language processing applications. also thorough survey related ﬁeld transfer learning. mentioned introduction shortage labeled data fundamental problem applied machine learning. important enough several areas research devoted various aspects problem. active learning paradigm labels acquired interactive fashion maximize beneﬁt label related approaches include ‘human-in-the-loop’ determines labels update thus making ‘most’ acquired labels. crowd sourcing through e.g. amazon mechanical turk allows rapid collection large amounts labels much research devoted efﬁcient distribution tasks interpretation weighting retrieved labels areas include weakly supervised method e.g. multiple instance learning latent structureal svms level supervision lower given task demands. approaches include semi-supervised learning make small amounts labelled data together large amounts unlabeled data. notably concept co-training popular approach. closely related domain adaptation transfer learning transfer learning marginal distribution source target data similar different tasks considered. make problem tractable typically assumed common prior model parameters across tasks. computer vision example ‘one-shot learing’ visual categories leaned using single training example basic assumption statistical learning theory train test data drawn underlying distribution. unfortunately assumption doesn’t hold many applications. instead ample labeled data might exist particular ‘source’ domain inference needed another ‘target’ domain. domain adaptation methods leverage labeled data domains improve classiﬁcation unseen data target domain. work survey domain transfer learning methods various application domains focus recent work computer vision. shortage labeled data fundamental problem applied machine learning. huge amounts unlabeled data constantly generated made available many domains cost acquiring data labels remains high. even worse sometimes situation makes highly impractical even impossible acquire labelled data domain adaptation approach problem leveraging labelled data related domain hereafter referred ‘source’ domain learning classiﬁer unseen data ‘target’ domain. domains assumed related identical situation occur many domains. examples event detection across video corpora different domains named entity recognition across different text corpora object recognition images acquired different domains domain adaptation recently started receiving signiﬁcant attention particular computer vision applications also applied computer vision problems methods used ﬁeld adapted gaussian mixture models could trivially used domain transfer setting discarding source data labels letting source data constitute background model. cross-modal classiﬁcation retrieval makes similar assumptions data compared assume instance rather class level relationship domains. cross-modal classiﬁcation ample data domains available train time unknown sample come modalities. denote input output random variable. denote joint probability distribution similarly denote marginal probability distributions. domain adaptation scenario mentioned introduction distinct distributions. denote source distribution where typically access ample labelled data target distribution seek estimate. also refer joint probability thus differentiating represents probability distribution. mentioned introduction domain adaptation relatively ﬁeld. also relatively loosely deﬁned regards e.g. ‘related’ domains ‘few’ labelled samples exist target domain. also general problem statement applies several application domains natural language processing computer vision. reasons variety proposed methods. inspired categorization proposed begin considering instance weighting methods relaxation assumptions sec. consider methods utilizing source data regularize target models sec. survey method seeking common representation across domains sec. section make connections transfer learning sec. brieﬂy survey method multi-modal learning. following ﬁrst consider relaxations problem. analysis empirical risk minimization framework proposed standard supervised data. model parameter given parameter space optimal parameter choice distribution loss function. framework want minimize drawn source domain labelled data target domain unlabeled data commonly assumed target source labels generally assumed belong space e.g. k-class classiﬁcation task further data matrix domain data sample column goal domain adaptation thus summarized learning function figure mega model proposed model assumes data fact generated three distributions target common source. mega model learns classiﬁer space. left standard logistic regression model. again well founded solution identiﬁed appropriate instance weighting loss function. explored approach show weighted model better estimate data given biased sampling function. quantity estimated using e.g. nonparametric kernel estimation proposed directly estimate ratio i.e. difference distributions. kernel mean match often simplifying assumptions previous section doesn’t hold. section discuss method prior probabilities estimated source data regularize model. ﬁrst cover priors bayesian sense examples discriminative methods. bayesian priors weighing loss training sample provides solution consistent empirical risk minimization framework. clearly good estimate would already done doesn’t really help formulation useful discussion below. following consider relaxations problem formulation. class imbalance covariate shift class imbalance relax problem formulation assume called class imbalance population drift sampling bias. consider example training data sampled remote sensing application. test data collected later occasion different class distribution changed landscape. taking assumptions account ratio becomes covariance shift another relaxation here given observation class distributions source target domains marginal data distributions different. situation arise example active learning tend biased near margin classiﬁer. ﬁrst glance situation appears present problem since estimate data. becomes problem practice. assuming ﬁrst model family mismatched data i.e. regardless parameter choose model won’t underlying distribution. assumption covariate shift becomes problem following reason. optimal source data minimize model error dense area since different learned model optimal target data propose adaptive support vector machine basic idea learn decision boundary close learned source domain. source data thus acting regularizer ﬁnal model. method presume existence model trained source domain data. ﬁnal decision function ﬁnal classiﬁer attained solving following constrained optimization problem. problem formulation doesn’t strive large margin rather solution close source solution. reasonable situation similar address proposed cross-domain cdsvm relax constraints ﬁnal model need similar enforcing proximity support vectors close target data. introducing additional constraints support vectors like target data points correctly classiﬁed. constraints active support vectors close part target data. speciﬁcally solve following constrained optimization problem figure figure illustrating adapted model. left ﬁgure shows universal estimated background data together speakerspeciﬁc train data. right shows adapted model. pursued approach adapting maximum entropy capitalizer. argued step process estimating non-intuitive suggested ensemble model considered three classiﬁers simultaneously target source joint portion data. generative model denote mega shown fig. estimated unlabeled data problem technically longer domain adaptation rather model adaptation. adapted gaussian mixture models successfully applied speaker veriﬁcation recently also computer vision figure show schematic illustration adapted gmm. right ‘required’ query word likely noun. this then helps disambiguate words ’signal’ noun adjective. algorithm works unsupervised data therefore doesn’t maximize correspondence directly rather related tasks. several recent papers computer vision community pursue idea. later proposed variations metric learning formulation learn mapping aligns feature spaces also maximize class separation. b||q mahalanobis distance respect matrix constraints formulations known information theoretic metric learning algorithmic contribution paper enforce pair datapoints source target domain respectively. state crucial ensure domain transfer transform learned. authors note since deﬁned positive deﬁnite matrices decompose mapping therefore symmetric since address changing regularizer squared frobenius norm. also changed constraints encode similarity data samples rather mahalanobis distance. formulation becomes functions might equal related even identity depending method. note entropy likely increase compared since feature representation usually simpler mapping thus encode less information. means bayes error likely increase good algorithm domain alignment take account. simple straight forward feature selection. proposed method remove features minimize approximated distance function source target distributions. took another aproach towards goal. follow maximum mean discrepancy criterion compare data distributions based distance means samples domains reproducing kernel hilbert space thus jointly ﬁnding kernel minimize distk decision function separate data kernel space. make tractable iteratively solve parameterized mixture kernel functions loss function. show improvements trecvid dataset related approaches. proposed structural correspondence learning ﬁnds feature representation maximize correspondence unlabeled data source target domain leveraging pivot feature behave similarly domains. example word ‘frustratingly easy’ method show promising performance named-entity recognition several text datasets. linear classiﬁcation algorithm equivalent decomposing model parameters class shared domains. formulation basically identical proposed purpose transfer learning. authors provide different analysis paper argue similarity method section discuss concept multi-modal learning. setting correspondences assumed instance rather category level. also commonly assumed ample train data available domains. similarly sec. common goal methods estimate transformations done letting e.g. thus mapping target domain source domain vice versa. could also consider mapping spaces common space. begin section reviewing canonical correlation analysis principal component analysis linear discriminant analysis. consider recent work utilizing methods principal component analysis popular dimensionality reduction method projects data direction maximum variance. derived follows. input data. desired mean projection direction. also projected data variance projected data another recent method computer vision also propose mapping common representation motivate incremental learning create intermediate representation source domain data viewing generative subspaces created domains points grassmanian manifold. intermediate representations recovered sampling geodesic path. ﬁnal feature representation stacked feature vector location along path. partial least squares learn model extended feature representation. table show evaluation several discussed methods. transfer learning sometimes called multi-task learning different transfer learning joint probability task different marginal data distribution normally state space assumed different e.g. learning class conditional models typically assumed common prior distribution variables formally different thought special case transfer learning tasks source target a-svm source a-svm webcam dslr dslr amazon table evaluation discussed method dataset introduced naive method train methods trained images category source domain category target domain. best result experiment marked bold. linear discriminant analysis popular unsupervised data dimensionality reduction purposes agnostic class might project data directions unsuitable class discrimination. linear discriminant analysis ﬁnds projection directions minimizing within class scatter matrix maximizing class scatter. derived follows. samples different classes. data projection direction given solving developed data analysis dimensionality reduction method though multimodal extension pca. ﬁnds basis vectors sets variables correlations projections variables onto basis variables mutually maximized. using notation sec. matrix rows similarly. covariance matrix data domains note formulation requires number samples domain dimensionality. also note optimization written constrained optimization optimization problem formulated generalized eigenvalue problem solved efﬁciently regular eigenvalue problems details provides excellent analysis kernelized version kcca. generalized multiview analysis proposed unifying framework learning multi modal discriminative linear projections. argue methods handle multi-view data. hand methods supervised. methods svm-k cdsvm asvm meet criteria generalize well unseen classes. unifying framework using framework propose methods generalized multiview generalized multiview marginal fisher analysis recap gmlda. noted above i’th view achieved setting setting matrix columns class means enforce class mean alignment across classes. authors also note step process vice versa needs considered baseline method. similar approaches include introduced semantic correlation matching uses logistic regression combine semantic matching. also introduce wikitext data face recognition data containing face images different pose illumination expression item represented using text image. image-tag pairs object amazon webcam john blitzer ryan mcdonald fernando pereira. domain adaptation structural correspondence learning. conference empirical methods natural language processing steve branson pietro perona serge belongie. strong supervision weak annotation interactive training deformable part models. ieee international conference computer vision", "year": 2012}