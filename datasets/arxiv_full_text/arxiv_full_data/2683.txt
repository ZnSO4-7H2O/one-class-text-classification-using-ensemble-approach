{"title": "Contextual Explanation Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We introduce contextual explanation networks (CENs)---a class of models that learn to predict by generating and leveraging intermediate explanations. CENs are deep networks that generate parameters for context-specific probabilistic graphical models which are further used for prediction and play the role of explanations. Contrary to the existing post-hoc model-explanation tools, CENs learn to predict and to explain jointly. Our approach offers two major advantages: (i) for each prediction, valid instance-specific explanations are generated with no computational overhead and (ii) prediction via explanation acts as a regularization and boosts performance in low-resource settings. We prove that local approximations to the decision boundary of our networks are consistent with the generated explanations. Our results on image and text classification and survival analysis tasks demonstrate that CENs are competitive with the state-of-the-art while offering additional insights behind each prediction, valuable for decision support.", "text": "introduce contextual explanation networks class models learn predict generating leveraging intermediate explanations. cens deep networks generate parameters context-speciﬁc probabilistic graphical models used prediction play role explanations. contrary existing post-hoc model-explanation tools cens learn predict explain jointly. approach offers major advantages prediction valid instance-speciﬁc explanations generated computational overhead prediction explanation acts regularization boosts performance low-resource settings. prove local approximations decision boundary networks consistent generated explanations. results image text classiﬁcation survival analysis tasks demonstrate cens competitive state-of-the-art offering additional insights behind prediction valuable decision support. model interpretability long-standing problem machine learning become quite acute accelerating pace widespread adoption complex predictive algorithms. high performance often supports belief predictive capabilities system perturbation analysis reveals black-box models easily broken unintuitive unexpected manner therefore machine learning system used social context imperative provide sound reasoning decision. restricting class models human-intelligible potential remedy often limiting modern practical settings. alternatively complex model explain predictions post-hoc e.g. searching linear local approximations decision boundary approaches achieve goal explanations generated posteriori require additional computation data instance importantly never basis predictions made ﬁrst place lead erroneous interpretations. explanation fundamental part human learning decision process inspired fact introduce contextual explanation networks class deep neural networks generate parameters probabilistic graphical models. generated models play role explanations used prediction encode arbitrary prior knowledge. data often consists representations low-level unstructured features high-level human-interpretable features ensure interpretability cens deep networks process low-level representation construct explanations context-speciﬁc probabilistic models high-level features importantly explanation mechanism integral part models trained predict explain jointly. motivating example. consider diagnosing risk developing heart arrhythmia causes condition quite diverse ranging smoking diabetes injury previous heart attacks carry different effects risk arrhythmia figure illustration arrhythmia risk diagnosis. shades denote strength association variables. graphical model context encoder crf-based explanations. model parameterized graphical model context autoencoding inference network generator network crf-based explanations. different contexts. assume data patient consists medical notes form text number speciﬁc attributes further assume access parametric class expert-designed models relate attributes condition. maps medical notes parameters model class produce context-speciﬁc hypothesis used make prediction. sequel formalize intuitions refer example discussion illustrate different aspects framework. main contributions paper follows prove post-hoc approximations cen’s decision boundary consistent generated explanations show that practice methods tend produce virtually identical explanations cens construct orders magnitude faster. implement cens extending number established domain-speciﬁc deep architectures image text data design architectures survival analysis. experimentally demonstrate value learning explanations prediction model diagnostics. moreover explanations regularizer improve sample efﬁciency. deep graphical models. idea combining deep networks graphical models explored extensively. notable threads recent work include replacing task-speciﬁc feature engineering task-agnostic general representations discovered deep networks representing potential functions energy functions neural networks encoding learnable structure gaussian process kernels deep recurrent networks learning state-space models nonlinear embeddings observables goal body work design principled structured probabilistic models enjoy ﬂexibility deep learning. difference cens previous latter directly integrate neural networks graphical models components ﬂexible resulting deep graphical models could longer clearly interpreted terms crisp relationships speciﬁc variables interest. cens hand preserve simplicity contextual models shift complexity process conditioning context variables. case consider simple graphical model given figure relates input variables targets using linear pairwise potential functions. linearity allows directly interpret parameters model associations pairs variables. substituting inputs deep latent representations representing pairwise potentials neural networks would result powerful model. however precise relationships variables longer directly readable model parameters. meta-learning. cens operate resembles meta-learning setup. meta-learning goal learn meta-model which given task produce another model capable solving task representation task seen context produced task-speciﬁc models similar cen-generated explanations. meta-training deep network generates parameters another network successfully used zero-shot few-shot learning cold-start recommendations scenarios suitable interpretability purposes. contrast cens generate parameters models restricted class attention mechanism improve interpretability. using explanations based domain knowledge known improve generalization could used powerful mechanism solving complex downstream tasks program induction solving algebraic word problems context representation. generating probabilistic model conditioning context aspect approach. previous work context-speciﬁc graphical models represented contexts discrete variable enumerated ﬁnite number possible contexts cens hand designed handle arbitrary complex context representations. also note context-speciﬁc approaches widely used language modeling context typically represented trainable embeddings interpretability. many ways deﬁne interpretability discussion focuses explanations deﬁned simple models locally approximate behavior complex model. methods allow construct explanations post-hoc manner proposed recently contrast cens learn generate explanations along predictions. multiple complementary approaches interpretability ranging variety visualization techniques explanations example natural language rationales finally framework encompasses class so-called personalized instance-speciﬁc models learn partition space inputs local sub-models consider problem learning collection data instance represented three random variables context attributes targets goal learn model parametrized predict deﬁne contextual explanation networks models assume following form predictor parametrized call predictors explanations since explicitly relate interpretable variables targets example targets scalar binary explanations take form linear logistic models; targets complex dependencies components represented graphical model e.g. conditional random ﬁeld cens assume explanation context-speciﬁc deﬁnes conditional probability explanation valid context make prediction marginalize θ’s; interpret prediction given data instance infer posterior main advantage approach allow modeling conditional probabilities black-box fashion keeping class explanations simple interpretable. instance context given text choose represented recurrent neural network class linear models. implications assumptions made discussed appendix here move describing number practical choices learning inference those. contextual explanation networks practice represent neural network encodes context parameter space explanations. multiple ways construct encoder consider below. deterministic encoding. consider delta-function network maps collapsing conditional distribution delta-function makes figure example architecture. context represented image transformed convnet encoder attention vector used construct contextual hypothesis dictionary sparse atoms. uses similar attention mechanism combining predictions model dictionary. since posterior also collapses hence inference done single forward pass. constrained deterministic encoding. downside deterministic encoding lack constraints generated explanations. multiple reasons might issue context encoder unrestricted might generate unstable overﬁtted local models explanations guaranteed human-interpretable often require imposing additional constraints sparsity want reason patterns data whole local explanations enough. address issues constrain space explanations introducing global dictionary {θk}k atom dictionary sparse. encoder generates context-speciﬁc explanations using soft attention dictionary i.e. explanation becomes convex combination sparse atoms attention dictionary. previously encoder delta-distribution model trained learning weights dictionary log-likelihood given learning inference done forward pass. mixtures experts. represented delta-function centered around output encoder. natural extend mixture delta-distributions case cens recover mixtures experts particular {θk}k pwδ. log-likelihood case note also represented soft attention dictionary used combining predictions expert given context instead constructing single context-speciﬁc explanation. learning done either directly optimizing log-likelihood infer explanation given context compute posterior contextual variational autoencoders. modeling form avoids representing joint distribution good decision data abundant. however incorporating generative model context provides beneﬁts better regularization low-resource settings coherent bayesian framework allows imposing additional priors parameters explanations accomplish representing note deterministic encoding dictionary constraint assume explanations graphical structure parameterization. hierarchical structured space explanations possible using ideas amortized inference leave direction future work. encoder decoder respectively. consider encoders also make global learnable dictionary represent form logistic normal distribution simplex spanned atoms prior dirichlet distribution parameters induce sharp attention. derivations deferred appendix section analyze relationship cen-generated lime-generated post-hoc explanations. lime constructs explanations local linear approximations decision boundary model neighborhood given point optimization measures quality linear model approximation neighborhood regularizer. typical choice losses respectively. neighborhood deﬁned distribution concentrated around point interest. given trained lime approximate decision boundary compare explanations produced methods. question case binary classiﬁcation turns context encoder deterministic space explanations linear local approximations obtained solving recover original cen-generated explanations formally result stated following theorem. theorem explanations local approximations class linear models −→t→∞ intuitively sampling distribution sharply concentrated around ensure recover high probability. proof given appendix result establishes equivalence explanations generated produced lime post-hoc approximating cen. note lime applied model equivalence explanations guaranteed. moreover show experimentally certain conditions incomplete noisy interpretable features lead lime producing inconsistent erroneous explanations. lime generate similar explanations case simple classiﬁcation structured constructing coherent local approximations post-hoc manner non-trivial. time cens naturally represent using arbitrary graphical models. demonstrate approach consider survival time prediction task interpretability uniquely valuable survival analysis re-formulated sequential prediction problem design cens crf-based explanations suitable sequentially structured outputs. setup follows. again data instances represented contexts attributes targets difference targets sequences binary variables indicate occurrence event. event occurred time event censored represent targets latent variables. note sequences valid i.e. assigned non-zero probability model. deﬁne crf-based note explanations time point rnn-based encoder potentials attributes targets linear functions parameterized pairwise potentials targets ensure conﬁgurations improbable learnable parameters). given constraints likelihood uncensored event time joint log-likelihood data consists parts non-censored instances compute censored instances provide much elaborate discussion survival time prediction setup architectures appendix empirical evaluation consider applications involve different data modalities context image text time-series. case cens based deep architectures designed learning given type context. ﬁrst part focus classiﬁcation tasks linear logistic models explanations. second part apply cens survival analysis structured explanations form conditional random ﬁelds design experiments around following questions explanation part learning prediction process affect performance predictive model? learning become less efﬁcient terms convergence sample complexity? cens stand vanilla deep nets? explanations good features explain predictions. noisy interpretable features affect explanations generated post-hoc lime whether help detect avoid situations. table performance models classiﬁcation tasks subscripts denote features linear models built pixels bag-or-words topics embeddings discrete attributes figure validation error size dictionary. training error iteration baselines cens. test error models trained random subsets data different sizes. classical datasets. consider classical image datasets mnist cifar text dataset sentiment classiﬁcation imdb reviews mnist cifar full images used context; imitate high-level features original images cubically downscaled pixels gray-scaled normalized descriptors computed using blocks imdb context represented sequences words; highlevel features bag-of-words representation -dimensional topic representation produced separately trained off-the-shelf topic model. neither data augmentation pre-training unsupervised techniques used. remote sensing. also consider problem poverty prediction household clusters uganda satellite imagery survey data household cluster represented collection satellite images categorical variables living standards measurement survey task binary classiﬁcation households poor poor. follow original study jean pre-trained vgg-f network compute -dimensional embeddings satellite images build contextual models. note datasets fairly small hence keep vgg-f part model frozen avoid overﬁtting. models. task linear regression vanilla deep nets baselines. mnist cifar networks simple convnet vgg- architecture respectively. imdb following johnson bi-directional lstm pooling. satellite ﬁxed vgg-f followed multi-layer perceptron hidden layer. models used baseline deep architectures context encoders three types cens constrained deterministic encoding mixture experts cens variational context autoencoding models dictionary constraint sparsity regularization. part compare cens baselines terms performance. task cens trained simultaneously generate predictions construct explanations using global dictionary. dictionary size become equivalent linear models. larger dictionaries cens become ﬂexible deep nets adding small sparsity penalty dictionary helps avoid overﬁtting large dictionary sizes model learns dictionary atoms prediction shrinking rest zeros. overall cens show competitive performance able approach surpass baselines number cases especially imdb data thus forcing model produce explanations along predictions limit capacity. additionally explanation layer cens somehow affects geometry optimization problem notice often causes faster convergence models trained subset data explanations play role regularizer strongly improves sample efﬁciency models becomes even evident results satellite dataset training points contextual explanation networks signiﬁcantly improved upon sparse linear models survey features note training satellite image features survey variables beneﬁcial come close result achieved contextual explanation networks figure effect feature quality explanations. explanation test error level noise added interpretable features. explanation test error total number interpretable features. regularization useful aspect main case explanations model diagnostics. linear explanation assign weights interpretable features hence quality depends select features. consider cases features corrupted additive noise selected features incomplete. analysis mnist imdb datasets. question trust explanations noisy incomplete features? effect noisy features. experiment inject noise features lime explanations corrupted features. note injecting noise data point noiseless representation noisy lime constructs explanations approximating decision boundary baseline model trained predict features only. trained construct explanations given make predictions applying explanations predictive performance produced explanations noisy features given figure since baselines take inputs performance stays regardless noise level lime successfully overﬁts explanations—it able almost perfectly approximate decision boundary baselines using noisy features. hand performance gets worse increasing noise level indicating model fails learn selected interpretable representation quality. effect feature selection. here setup instead injecting noise construct randomly subsampling dimensions. figure demonstrates result. performance cens degrades proportionally size that again lime able explanations decision boundary original models despite loss information. figure qualitative results satellite dataset weights given subset features models discovered cen. frequently selected areas marked rural urban average proportion tenement-type households urban/rural area selected. models selected different areas uganda map. tends selected urbanized areas picked rest. nightlight intensity different areas uganda. experiments indicate major drawback explaining predictions post-hoc constructed poor noisy incomplete features explanations overﬁt decision boundary predictor likely misleading. example predictions perfectly valid model might getting absurd explanations unacceptable decision support point view. here focus poverty prediction task analyze cen-generated explanations qualitatively. detailed discussion qualitative results visualization learned explanations mnist imdb datasets given appendix training dictionary size discover encoder tends sharply select explanations different household clusters uganda survey data household cluster marked either urban rural. notice that conditional satellite image tends pick urban areas rural notice explanations weigh different categorical features reliability water source proportion houses walls made unburnt brick quite differently. visualized selects frequently around major city areas also correlates high nightlight intensity areas high performance model makes conﬁdent produced explanations allows draw conclusions causes model classify certain households different neighborhoods poor. finally apply cens survival analysis showcase networks structured explanations. survival analysis goal learn predictor time occurrence event well able assess risk occurrence. classical models task aalen’s additive model proportional hazard model linearly regress attributes particular patient hazard function. shown survival analysis formulated structured prediction problem solved using variant. here propose cens deep nets encoders crf-structured explanations details architectures models baselines well background survival analysis provided appendix datasets. publicly available datasets survival analysis intense care unit patients support data physionet challenge. data preprocessed used follows table performance classical aalen models crf-based models cens lstm context embedding explanations. numbers averages -fold cross-validation; std. order least signiﬁcant digit. denotes temporal quantile i.e. time point patients data died censored point. figure weights cen-generated explanations patients support dataset inﬂuential features dementia avtisst slos hday ca_yes sfdm_coma intub sfdm_sip higher weight values correspond higher feature contributions risk death given time. support data patient records variables. selected variables features categorical features one-hot encoded. values features non-negative ﬁlled missing values crf-based predictors survival timeline capped years converted discrete intervals days each. used patient records training validation testing. physionet data patient records represented -hour irregularly sampled -dimensional time-series different measurements taken patient’s stay icu. resampled mean-aggregated time-series frequency. resulted large number missing values ﬁlled resampled time-series used context attributes took values last available measurement variable series. crf-based predictors survival timeline capped days converted discrete intervals. apply crfs outputs neural encoders similar models show successful natural language applications note parameters layer assign weights latent features longer interpretable terms attributes interest. metrics. following metrics speciﬁc survival analysis accuracy correctly predicting survival patient times correspond populationlevel temporal quantiles relative absolute error predicted actual time death non-censored patients. quantitative results. results models given table implementation baseline reproduces performance reported mlp-crf lstm-crf improve upon plain crfs noted longer interpreted terms original variables. hand cens outperform neural models certain metrics providing explanations risk prediction patient point time. qualitative results. inspect predictions cens qualitatively given patient visualize weights assigned corresponding explanation respective attributes. figure explanation weights subset inﬂuential features patients support dataset predicted survivor non-survivor. explanations allow better understand patient-speciﬁc temporal dynamics contributing factors survival rates predicted model paper introduced contextual explanation networks class models learn predict generating leveraging intermediate context-speciﬁc explanations. formally deﬁned cens class probabilistic models considered number special cases derived learning inference procedures within encoder-decoder framework simple sequentially-structured outputs. shown that explanations generated cens provably equivalent generated post-hoc certain conditions cases post-hoc explanations misleading. cases hard detect unless explanation part prediction process itself. besides learning predict explain jointly turned number beneﬁts including strong regularization consistency ability generate explanations computational overhead. would like point limitations approach potential ways addressing future work. firstly prediction made comes explanation process conditioning context still uninterpretable. ideas similar context selection rationale generation help improve interpretability conditioning. secondly space explanations considered work assumes graphical structure parameterization explanations uses simple sparse dictionary constraint. might limiting could imagine using hierarchically structured space explanations instead bringing bear amortized inference techniques nonetheless believe proposed class models useful improving prediction capabilities also model diagnostics pattern discovery general data analysis especially machine learning used decision support high-stakes applications. thank willie neiswanger mrinmaya sachan many useful comments early draft paper ahmed hefny shashank reddy bryon aragam russ salakhutdinov helpful discussions. also thank neal jean making available code helped conduct experiments satellite imagery data. work supported afrl/darpa", "year": 2017}