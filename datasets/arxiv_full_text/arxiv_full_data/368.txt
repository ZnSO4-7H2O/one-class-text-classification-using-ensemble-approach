{"title": "Emergence of Structured Behaviors from Curiosity-Based Intrinsic  Motivation", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML", "68"], "abstract": "Infants are experts at playing, with an amazing ability to generate novel structured behaviors in unstructured environments that lack clear extrinsic reward signals. We seek to replicate some of these abilities with a neural network that implements curiosity-driven intrinsic motivation. Using a simple but ecologically naturalistic simulated environment in which the agent can move and interact with objects it sees, the agent learns a world model predicting the dynamic consequences of its actions. Simultaneously, the agent learns to take actions that adversarially challenge the developing world model, pushing the agent to explore novel and informative interactions with its environment. We demonstrate that this policy leads to the self-supervised emergence of a spectrum of complex behaviors, including ego motion prediction, object attention, and object gathering. Moreover, the world model that the agent learns supports improved performance on object dynamics prediction and localization tasks. Our results are a proof-of-principle that computational models of intrinsic motivation might account for key features of developmental visuomotor learning in infants.", "text": "interactions child pushes boundaries worldmodel-prediction systems achieve giving useful data improve develop systems. worldmodeling capacity improves used novel becomes cycle starts again. related conception scientist crib account aside play behaviors extremely useful highly structured driving self-supervised learning variety representations underlying sensory judgments motor planning capacities building recent work artiﬁcial intelligence make computational model agent driven curiosity-based intrinsic motivation. present simple simulated interactive environment agent move around physically objects sees world interesting interactions sparse unless actively sought after. describe neural network architecture agent learns world model seeks predict consequences actions. addition agent optimizes accuracy world model separate neural network simultaneously learns agent action policy seeks take actions adversarially challenge current state world model. demonstrate architecture stably engages virtuous reinforcement learning cycle described above spontaneously learning understand self-generated motion selectively attention localize interact objects without concepts built learning occurs emergent self-curricularization capacities arise distinct developmental milestones. work computational proof-of-principle intrinsic motivation could drive aspects visuomotor learning play infants young children ﬂexible intrinsically motivated autonomous robots might constructed. infants experts playing amazing ability generate novel structured behaviors unstructured environments lack clear extrinsic reward signals. seek replicate abilities neural network implements curiosity-driven intrinsic motivation. using simple ecologically naturalistic simulated environment agent move interact objects sees agent learns world model predicting dynamic consequences actions. simultaneously agent learns take actions adversarially challenge developing world model pushing agent explore novel informative interactions environment. demonstrate policy leads self-supervised emergence spectrum complex behaviors including motion prediction object attention object gathering. moreover world model agent learns supports improved performance object dynamics prediction localization tasks. results proof-of-principle computational models intrinsic motivation might account features developmental visuomotor learning infants. keywords development learning curiosity neural network models within ﬁrst year life humans exhibit wide range interesting apparently spontaneous visuomotor behaviors including navigating environment seeking attending objects engaging physically objects novel surprising ways short young children excellent playing ability make sense structure environments sets apart even advanced autonomous robots. play capacity period likely interacts infants’ powerful abilities understand model environment. months younger infants orient complex environment account presence number visual properties objects interact with sense objects behave dynamically spelke stahl feigenson baillargeon exactly young children know play? behaviors relate world-model building abilities? natural idea infants’ worldmodeling capacities result built-in core systems including e.g. object attention permanence selflocalization number sense intuitive physics operational systems would naturally give infant basis make judgments sequences actions would interesting perform. related alternative idea intrinsic motivation curiosity drive development worldmodel making idea relies virtuous cycle which seeking novel replicable figure self-supervised curiosity model. train dynamical world model simultaneously learning loss model predicts world model’s loss choose actions lead novel surprising events environment place agent physically realistic simulated environment built unity simulation framework. agent consists world model loss model. world model tasked learn dynamics visuomotor inputs. loss model tries estimate world model’s losses several time steps future choose actions antagonize world model’s learning. self-supervised curiosity system depicted figure emphasize initialize model pretrained weights explore world representation behaviors emerge simple antagonistic setup physically embodied environment. interaction environment environment consists simple square room agent several objects initially placed randomly. agent modeled invisible sphere move around discrete time steps receives image forward-facing camera. order model interaction objects requiring attention proximity agent apply forces torques three dimensions objects view within ﬁxed distance refer state agent object play state object play. although remaining environment static agent objects collide every part. deﬁne state state space consist images captured times agent. state agent speciﬁes action leads next state st+. action space continuous. ﬁrst dimensions specify motion restricting agent movement forward/backward motion horizontal planar rotation remaining dimensions specify forces torques τxτyτz applied objects sorted lowerleftmost upper-rightmost object relative agent’s ﬁeld view. representation unambiguous objects acted view objects world model given slice history describe generalized dynamical problem input true-value require world model regardless whether well-deﬁned. denote world model prediction loss lwm)η) incurred. theory future prediction makes attractive dynamical problem st+. practice inverse dynamical prediction useful ﬁlling missing action instead predicting future concretely case excluding zeros force torque components corresponding objects play observable effect bins action classes softmax cross-entropy loss. dimension −.−. train convolutional neural network task scratch stochastic gradient descent randomly initialized parameters twelve convolutional layers two-stride pools every layer hidden layer encode states lower-dimensional latent space shared weights. latent states {λt+i} concatenated given actions {at+ input two-layer predict loss model agent’s goal antagonize world model could predict loss incurred future time steps function options policy could made. practice explicitly except predicting discretization loss ease training. given proposed next action loss model predicts probability distribution discrete classes world model loss number future time steps. penalized softmax cross-entropy loss ωφ)) separate convolutional neural network parameters twelve convolutional layers two-stride pools every layer hidden layer encode state concatenated proposed representation used input two-hidden-layer infer prediction. note future losses aside ﬁrst depend state world model also future actions taken loss model hence needs predict expectation future policy. loss predictions usefully interpreted loss prediction maps action space given current state depicted figure action policy given loss prediction model agent simple mechanism choose actions. loss model provides given proposed next action probability distributions .cl}. given real-valued function probability distributions deﬁne policy distribution hyperparameter purposes taking expectation loss class time steps sufﬁcient. practice execute policy evaluating uniform random samples sample k-way discrete distribution probabilities proportional equation choosing policy mechanism start na¨ıve approach using sophisticated reinforcement learning standards effort focus studying structure proposed self-supervision signal. explicitly predicting loss separately several time steps future results admit easy visualization interpretation. randomly situate agent objects square unit room play distance agent trains blue objects different shapes i.e. cones cylinders cuboids pyramids spheres varied aspect ratios. reinitialize play scene every steps. ﬁrst place agent object room show learns predict motion attend localize navigate towards objects evaluating world model’s training loss agent’s play state frequency inverse dynamics prediction performance ﬁxed validation set. second experiment increase number objects demonstrate agent learns gather objects prefers play objects object looking frequency object play states object-agent distances. overarching results compare learned world model curious policy baseline world model ﬁxed random weights following random policy baseline world model weights learned random policy motion learning figure shows training loss curves lw/cp- baselines. rw/rp learn well since weights ﬁxed random. lw/rp quickly converges value learns constant random distribution without antagonistic policy. lw/cp- loss dips before increasing loss model ﬁrst needs learn actions lead higher loss able antagonize world model effectively. ﬁrst loss corresponds world model learning motion. motion error reported table close error reached point. emergence object attention lw/cp- loss increases initial motion agent starts attend objects reﬂected increase object interactions shown figure ﬁnal stage agent interacts object time indicates learns localize attend object time increasing world model loss shows object interactions much harder predict motion. baselines almost never interact object thus experience lower motion losses. improved inverse dynamics prediction evaluate inverse dynamics prediction performance held validation gathered environment following random action policy. measure models’ performance predicting motion object actions separately divide validation sets. ﬁrst contains examples agent play state. second consists remaining examples. figure table lw/cp- lw/rp perform well predicting motion antagonistic policy necessary encounter motion. however policy outperforms baselines predicting object interactions signiﬁcant margin showing focusing object interactions indeed improve inverse dynamics prediction performance seen figure table improved object detection localization quantify world model’s object presence localization performance train linear regression/logistic regression elastic regularization features various world model layers ofﬂine dataset generated gathering data online following random action policy. half object presence training data contains object. localization experiment second image guaranteed contain object. training datasets consists figure object experiments. learned world model curious policy compared learning world model following random policy world model random weights random policy world model cross-entropy loss training. object play state frequency motion prediction cross-entropy loss held validation set. object interaction prediction cross-entropy loss held validation set. table performance comparison. learned world model curious policy compared learned world model random policy random world model random policy motion interaction accuracy compared play non-play states. object frequency presence measured localization mean pixel error. image pairs labeled object’s presence pixel-wise position centroid respectively. respective validation test sets comprise image pairs each. table model outperforms baselines object presence localization task indicating learns better visual features. addition object localization agent also exhibits navigation planning abilities. figure give visualizations loss maps projected onto agent’s position respective time. loss prediction maps generated uniformly sampling actions action space evaluating applying postprocessing smoothing algorithm. truncate ﬁgure time steps loss model predicts. loss maps show agent predicting higher loss actions moving towards object reach play state. consequently curious policy take actions navigate agent closer object. emergence multi-object interactions beginning training object experiment observe similar stages object experiment loss dips agent learns predict motion rises attention shifts towards objects interacts with. stage followed another loss increase corresponds agent gathering playing objects simultaneously. reﬂected increase object play time object play time consequently average distance agent objects decreases time seen figure drops units equals maximum interaction distance. lw/rp baseline quickly drops ﬂattens out. lw/cp- instead learns interact object objects simultaneously. observe simple general intrinsic motivation mechanism based adversarially antagonizing loss dynamically constructed model world allows agent stably generate spectrum emergent naturalistic behaviors. self-curricularization active learning setfigure navigation planning behavior. loss model predicts higher loss agent turns towards object. colors correspond high blue colors loss predictions. center heat corresponds agents position. figure object experiments. learned world model curious policy compared setup learned world model following random policy world model cross-entropy loss training. object play state frequency object object play state frequency objects average distance agent objects unity units. tles process agent achieves several developmental milestones suitably increasing complexity learns play. starting random actions quickly learns dynamics motion. then without given explicit supervision signal presence location object discards motion prediction boring begins focus attention objects interesting. lastly multiple objects available gathers objects bring interaction range other. throughout agent ﬁnds towards challenging data distribution moment hard enough expose agent situations still understandable exploitable agent. intrinsically motived policy leads performance gains understanding object dynamics well tasks system explicitly learning. visual system world model intentionally initialized ﬁlter weights pretrained imagenet classiﬁcation. result constitutes partial progress replacing training visual backbone task large-scale image classiﬁcation interactive self-supervised task proof-of-concept complex milestones potentially reached developing understanding object categories physical relations. machine learning perspective combination spontaneous behavior leading improved world model well suited designing agents must effectively many real-world reinforcement learning scenarios rewards sparse potentially unknown. here ultimately seek develop algorithms control autonomous robots learn operate complex unpredictable environments. cognitive science perspective results suggest route toward using intrinsically motivated learning systems however truly achieve either goals variety limitations current work need overcome future work. first make connection cognitive science realistic environment agent need realistic. hand better graphics physics varied interesting visual objects important allow better transfer learned behavior real-world visuomotor interactions. also important create properly embodied agent visible arms tactile feedback allowing realistic interactions. current work stress proof-of-principle full cognitive model fail address fact infants severely limited mobility motor control. able make realistic predictions actual infant developmental milestones especially critical realistically model known developmental trajectory motor system. addition including animate agents another complex interactions potentially also better learning imitation. second important improve reinforcement learning techniques used better handle complex interactions beyond demonstrated here. interactions part larger experiment e.g. placing object table ramp watching fall sophisticated policies used likely necessary better ability handle temporally extended reward schedules. also likely necessary recurrent networks meet working memory demands scenarios. third world model needs better representations improve predicting complex interactions. current approach especially suffers degenerate cases inverse dynamical prediction problem problem correspond well-deﬁned map. example given sequence images object rests ground action sequence under-determined agent could pushing object not. though latent-space approach meant part ameliorate issue found entirely effective solution context. choices blend components generalized inverse dynamical prediction task interaction loss terms seem resolving this. taking next steps help understand infants learn also develop systems learn without human supervision. work supported understanding human cognition award james mcdonnell foundation simons collaboration global brain grant berry foundation postdoctoral fellowship hardware donation nvidia corporation.", "year": 2018}