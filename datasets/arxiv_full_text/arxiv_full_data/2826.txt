{"title": "Pseudo-Recursal: Solving the Catastrophic Forgetting Problem in Deep  Neural Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In general, neural networks are not currently capable of learning tasks in a sequential fashion. When a novel, unrelated task is learnt by a neural network, it substantially forgets how to solve previously learnt tasks. One of the original solutions to this problem is pseudo-rehearsal, which involves learning the new task while rehearsing generated items representative of the previous task/s. This is very effective for simple tasks. However, pseudo-rehearsal has not yet been successfully applied to very complex tasks because in these tasks it is difficult to generate representative items. We accomplish pseudo-rehearsal by using a Generative Adversarial Network to generate items so that our deep network can learn to sequentially classify the CIFAR-10, SVHN and MNIST datasets. After training on all tasks, our network loses only 1.67% absolute accuracy on CIFAR-10 and gains 0.24% absolute accuracy on SVHN. Our model's performance is a substantial improvement compared to the current state of the art solution.", "text": "hypothesis tries reduce signiﬁcant changes important weights would impair functionality network thus performance previous task. however constraining important neurons network retain similar weights number disadvantages. predominantly group neurons’ function network compressed smaller group neurons solution. another disadvantage task speciﬁc parameters needed unit allow method work challenging problems playing atari games. pseudo-rehearsal proposed solution thus life long learning neural networks. network needs trained task pseudorehearsal protects prior learning generating samples network’s behaviour captures structure original task/s. done randomly generating input samples assigning target outputs passing network. input-output pairs rehearsed learning items task. pseudorehearsal constrains changes network’s function remains approximately input space previously learnt tasks function changes input space task. pseudo-rehearsal aligned synaptic plasticity hypothesis weights individual neurons encoding previous task change long network’s overall functionality remains consistent. let’s begin formalising pseudo-rehearsal. neural network chosen architecture function given trainable parameters. one-hot encoded vector indicating assignment class task zero vectors classes across tasks given parameters minimises loss function task want next train abstract—in general neural networks currently capable learning tasks sequential fashion. novel unrelated task learnt neural network substantially forgets solve previously learnt tasks. original solutions problem pseudo-rehearsal involves learning task rehearsing generated items representative previous task/s. effective simple tasks. however pseudo-rehearsal successfully applied complex tasks tasks difﬁcult generate representative items. accomplish pseudo-rehearsal using generative adversarial network generate items deep network learn sequentially classify cifar- svhn mnist datasets. training tasks network loses absolute accuracy cifar- gains absolute accuracy svhn. model’s performance substantial improvement compared current state solution. deep neural networks currently state solution many machine learning problems. however cannot trained task retaining knowledge previously trained tasks. known catastrophic forgetting problem needs solved continuously learning artiﬁcial agents called lifelong learners. solutions aligned biological learning notions synaptic stability plasticity synaptic stability hypothesis states memory retained ﬁxing weights units encode synaptic plasticity hypothesis states weights units change long memory retained output units still recreate correct pattern activity. recent focus overcoming introduced elastic weight consolidation loss function augmented importance weights previously learnt tasks. measure encourages weights important previous task retain similar values whereas less important weights signiﬁcantly altered learn task. aligned synaptic stability fig. average accuracy classiﬁcation network using pseudorehearsal pseudo-images generated uniform distribution x-axis represents task learnt lines represent network’s test accuracy various tasks trained far. error bars represent standard deviation data point across trials. nonvisible error bars smaller standard deviations data point. generative adversarial network neural network model uses unsupervised learning generate random images representative input dataset. achieved creating network models; discriminative model generative model. goal discriminative model identify whether input image real image generated image whereas goal generative model create images fool discriminator. results generator learning create images represent input images. conﬁrm used generate pseudoimages look similar real images applied deep convolutional generative adversarial network cifar- dataset. figure illustrates generated images look similar real cifar- images distance although differences emerge close inspection. nevertheless generated images still contain class speciﬁc features network learn retain. pseudo-rehearsal achieved generating pseudoitems using random distribution. however deep learning problems much harder thus pseudo-vectors generated purely random likely good representations training data. particularly obvious images generating pseudo-images uniform distribution produces static images represent natural images furthermore static images poorly represent distribution classes generations network believed almost static images either birds frogs. static images used pseudo-rehearsal trained images previous task used generate pseudo-images representative task without storing actual dataset. means classiﬁcation network trained ﬁrst task also trained images. then next task learnt used generate pseudo-images rehearsed along network task optimal weights performing well previous task i.e. hwt+ hwt. rehearse previous tasks learning task would need minimise following fig. real cifar- images. pseudo-images generated trained cifar- images. pseudo-images generated trained images cifar- followed images svhn along pseudo-images representing cifar-. images labelled classiﬁcation network. generates images representing svhn cifar- however images representing cifar- included ﬁgure. representative tasks. simple solution train second generate images effective requires allocation memory every task. elegant solution pseudo-rehearsal well. allows produce pseudo-items without requiring extra memory task. pseudo-rehearsal model easily achieved generating pseudo-images mixing current task’s images. procedure repeated recursively every time task present thus term process pseudo-recursal. conﬁrm pseudo-rehearsal used still generate pseudo-images look similar real images train cifar- svhn rehearsing generated images represent cifar-. figure figure illustrates using pseudo-rehearsal causes generate images represent recent task previously trained task/s furthermore case svhn pseudo-images appear noticeably different real svhn images. originally pseudo-rehearsal applied simple tasks pseudo-items generated random distribution covered whole input space. means pseudoitems represent network’s function whole input space changes made accommodate task local possible input space task however using generate pseudo-items near actual previous inputs much larger sparser space thus network retains mapping near previous inputs parts space network free vary. summary achieve continuous learning applying pseudo-rehearsal classiﬁcation model model. allows overcome problem without requiring extra memory task presented. develop general purpose algorithm overcoming problem sequential task learning. believe algorithm general purpose scalable large number tasks following criteria incremental learning ﬁeld research neural network required learn task’s classes sequentially. incremental learning impairs networks’ ability remember previously trained classes. paper focuses sequentially learning different tasks. tasks generally dissimilar necessarily modality although learning problems differ many similarities therefore methods ﬁeld often applicable other. recently pseudo-rehearsal applied incremental learning models. pseudo-rehearsal used sequentially train convolutional neural network classify ﬁrst digits mnist followed remaining digits. achieved using recurrent neural network generate random images representative ﬁrst mnist digits. fig. real svhn images. pseudo-images generated trained images cifar- followed images svhn along pseudo-images representing cifar-. images labelled classiﬁcation network. generates images representing svhn cifar- however images representing svhn included ﬁgure. images could rehearsed learning later digits. work number major limitations. generative model requires pixel’s mean value standard deviation stored classes learnt. results memory requirements model scaling number classes learnt. furthermore must check generated images discard network conﬁdent belonging learnt class. finally generating random images representative mnist trivial task images class similar another case difﬁcult tasks cifar- lesser extent svhn. pseudo-rehearsal also applied incremental learning model called fearnet. model components short-term memory system long-term memory system system determines whether short-term long-term system used classifying item. model stores recent items short-term system pseudo-rehearsal periodically used train long-term system items without forgetting previously learnt classes. long-term memory system implemented autoencoder output encoder passed ﬁnal classiﬁcation layer. decoder used generate pseudo-items however also requires storage mean covariance matrix encoder’s representation previous classes. generated decoder rehearsed items. generative model isolated classiﬁcation network whereas fearnet combines models. means long-term system trained classiﬁcation loss reconstruction loss minimised concurrently. difference reconstruction loss minimised across layer autoencoder. limitation means function neurons intermediate layers network constrained. fearnet shown counteract supposedly complex tasks cifar-. however fearnet never trained cifar-’s images rather output ﬁrst weight layers resnet pre-trained imagenet. means classiﬁcation done much smaller multi-layer perceptron majority work pre-trained resnet architecture. subsequently autoencoder learning reproduce cifar- images rather items’ output resnet architecture. although difﬁcult task training mnist method incomparable training convolutional neural network classify task input. recent review methods overcoming concluded current algorithms solve also found performed best learning multiple tasks thus compare pseudo-recursal ewc. main contributions paper are; show pseudo-recursal used overcome problem dnns sequentially learning cifar- svhn mnist demonstrate learning rate used network training ﬁrst task’s dataset. learning rate used network trained later task. number items trained mini-batch. training stopped network improved validation error number epochs. first moment decay rate adam optimiser. second moment decay rate adam optimiser. epsilon value adam optimiser. number pseudo-items training portion pseudo-dataset. number pseudo-items validation portion pseudo-dataset. pseudo-rehearsal applied recursively separate classiﬁcation generative model. architecture also satisﬁes previously mentioned criteria. although limit image classiﬁcation paper techniques applicable problems. dataset size items mini-batch discrimination layer mini-batch discrimination layer reduces training time needed generator produce visually appealing images helps stop network converging point outputs image. experiments train classiﬁer model sequentially cifar- svhn mnist datasets. datasets chosen comprise similar sized images number classes range similarities differences datasets’ tasks. cifar- contains animals types transport dissimilar svhn mnist contain digits datasets divided training validation testing items. classiﬁcation network tasks’ pseudodatasets’ validation test images center cropped standardised. training images distortions applied every epoch randomly cropping images ﬂipping images left right adjusting brightness adjusting contrast standardising images. classiﬁcation network based network passes input convolutional layers max-pooling layer convolutional layers max-pooling layer fully-connected layers. convolutional layers ﬁlters respectively applied ﬁlters stride max-pooling layers applied window stride fully-connected layers units respectively ﬁnal unit layer softmax layer. layers except softmax layer apply relu activation function. classiﬁer trained using hyper-parameters speciﬁed table ﬁrst task trained mini-batch’s training examples come task’s dataset. however later tasks half examples come task’s dataset remaining pseudo-dataset. validation error recorded epoch current task’s dataset pseudo-dataset training completed network weights epoch lowest validation loss reloaded network evaluated real test items current task previously learnt tasks. network correctly told task classifying correct task speciﬁc weights always applied. gives best possible chance outperforming pseudo-recursal. controller consistent across games classiﬁcation problem make sense output neurons shared tasks separate network would required determine current task. however determine whether effective scenario create condition assumes current task known thus output neurons shared tasks. ewc’s parameter random search trials. rote learn conﬁrm using generate pseudo-items effective using allocation memory rote learn subset previous tasks’ items tested model’s retention rehearsal applied subset items. number free parameters generative model approximately thus images randomly selected memorised past tasks. condition learns datasets sequentially still rehearsing memorised items. images split training validation sets ratio datasets distortions also applied training images every epoch. although classiﬁcation network test state network datasets still trained respectable accuracy tasks without using special tricks. results experimental conditions displayed figure condition clearly shows task learnt network correctly classify previous tasks’ images. fact accuracy drops straight previous tasks seems dramatic however logical classiﬁcation network trains using cross-entropy training previous tasks’ images appear thus output neurons representing classes quickly learn never activate. expected condition demonstrate ﬁnal task accuracies increased slightly initial values. condition demonstrates network capacity learn three tasks high accuracy without needing additional units. pseudo condition also overcomes problem experience dramatic drop previous tasks’ accuracy learns task. fact loses accuracy every time task presented loses cifar- test accuracy rehearsing tasks. differences accuracy absolute differences remain consistent throughout paper. svhn pseudo-recursal resulted increase accuracy learning mnist. conveys network capability retain almost knowledge previous tasks without needing store condition barely resistant problem managing correctly classify cifar svhn datasets three tasks learnt. however hypothesised could tasks’ classes represented separate output units. output units represent ﬁrst task never active later tasks thus pressure units never activate later tasks likely greater pressure units remember previous task therefore condition allowed tasks share output units units active ﬁrst task also active subsequent tasks. found lead dramatic improvement ewc’s ability retain knowledge previous tasks could classify cifar- svhn accuracy learning tasks. suggests ineffective learning tasks share output representations moderately effective still pseudo-recursal clearly outperforms loses cifar-’s accuracy compared ewc’s furthermore noted trials ewc’s retention varied fairly dramatically however pseudorecursal consistently outperformed across trials large margin. compared standard pseudo-rehearsal main disadvantage method generative network required pseudo-rehearsal work deep network. however also apply pseudo-rehearsal generative model size network constant. another disadvantage pseudo-recursal takes considerably training time generator must also trained. furthermore number pseudo-items novel task’s items trained classiﬁer generator results twice many mini-batches epoch. however attempted optimise number pseudo-items required large increase necessary. results rote learn condition conveys classiﬁer retain majority knowledge past tasks however pseudo-recursal still clearly outperforms retaining accuracy cifar- svhn. demonstrates using model effective simply remembering past items. demonstrated combining gans pseudorehearsal effective method solving problem. pseudo-recursal major advantages methods require network grow task network hard constraints individual neurons learn task. future research apply pseudo-recursal reinforcement learning task network could learn play novel atari games retaining ability play previously trained games. fig. average accuracy classiﬁcation network pseudo rote learn conditions. x-axis represents task learnt lines represent network’s test accuracy various tasks trained far. error bars represent standard deviation data point across trials. non-visible error bars smaller standard deviations data point. kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska overcoming catastrophic forgetting neural networks proceedings national academy sciences goodfellow pouget-abadie mirza warde-farley ozair courville bengio generative adversarial nets advances neural information processing systems robins frean local learning algorithms sequential tasks neural networks journal advanced computational intelligence intelligent informatics vol. mellado saavedra chabert salas pseudorehearsal approach incremental learning deep convolutional neural networks latin american workshop computational neuroscience. springer", "year": 2018}