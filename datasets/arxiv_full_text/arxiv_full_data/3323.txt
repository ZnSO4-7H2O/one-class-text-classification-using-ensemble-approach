{"title": "Fast Metric Learning For Deep Neural Networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Similarity metrics are a core component of many information retrieval and machine learning systems. In this work we propose a method capable of learning a similarity metric from data equipped with a binary relation. By considering only the similarity constraints, and initially ignoring the features, we are able to learn target vectors for each instance using one of several appropriately designed loss functions. A regression model can then be constructed that maps novel feature vectors to the same target vector space, resulting in a feature extractor that computes vectors for which a predefined metric is a meaningful measure of similarity. We present results on both multiclass and multi-label classification datasets that demonstrate considerably faster convergence, as well as higher accuracy on the majority of the intrinsic evaluation tasks and all extrinsic evaluation tasks.", "text": "abstract. similarity metrics core component many information retrieval machine learning systems. work propose method capable learning similarity metric data equipped binary relation. considering similarity constraints initially ignoring features able learn target vectors instance using several appropriately designed loss functions. regression model constructed maps novel feature vectors target vector space resulting feature extractor computes vectors predeﬁned metric meaningful measure similarity. present results multiclass multi-label classiﬁcation datasets demonstrate considerably faster convergence well higher accuracy majority intrinsic evaluation tasks extrinsic evaluation tasks. many machine learning information retrieval systems rely distance metrics accurately estimate semantic similarity objects interest. functions euclidean distance cosine similarity applied directly complex input domains like images audio poor measures semantic relatedness. hence machine learning applied problem constructing metrics suited speciﬁc domains. recent advances ﬁeld taken advantage superior modelling capacity deep neural networks learn complex relations data objects. methods based deep learning able represent intricate metrics compared linear mahalanobis methods also take considerably longer train. objective work method accelerate training deep learning approaches similarity metric learning. current metric learning algorithms based deep neural networks rely propagating multiple training examples network simultaneously order compute gradient objective functions deﬁned network output. example could attempt maximise euclidean distance network outputs dissimilar instances also trying minimise euclidean distance computed representations instances similar. forces train network pairs examples— something greatly increases eﬀective training size therefore time taken network converge towards good solution. many techniques evaluated classiﬁcation benchmark datasets instance labelled class instances considered similar share class. evaluating metrics implicit assumption transitivity desired—which tasks justiﬁed—but always case. example consider three images could case considered similar similar backgrounds. could also case similar object appears foreground both despite rest scene vastly diﬀerent. transitivity enforced would considered similar even though reason case. interested transitive non-transitive relations multi-label classiﬁcation datasets. elaborating previous example; images would given label indicating type background have would given label indicating foreground object appears image. case labels hence task multilabel classiﬁcation paradigm. however work take even general view problem deﬁnition. assume information provided consists pairwise similarity dissimilarity constraints. generalised view enables diverse data collection strategies relevance feedback methods commonly used information retrieval systems. approach similarity metric learning acknowledge latent classes within data however explicit knowledge classes required. taking advantages existence latent classes ﬁrst learn structure target vector space embedding function subsequently learn model performs embedding. compute intensive part algorithm operate pairs feature vectors hence results computationally cheaper approach learning instance similarity. ﬁrst review related methods section section describe problem deﬁned terms binary relations. section describes similarity metric learning method detail. section empirically demonstrate method converges considerably faster conventional methods ﬁnal accuracy higher intrinsic extrinsic evaluation tasks. metric learning well established area much research developing sophisticated algorithms learn instance similarity. metric learning techniques relevant work roughly divided categories mahalanobis based methods neural network based methods. notable general form mahalanobis distance metric parameterised matrix deﬁned given equation algorithms based model primarily diﬀer linear transform optimised. large margin nearest neighbours algorithm employs semidefinite programming optimise loss function composed terms. specifically evaluations equation term draws similar instances together encourages margin formed dissimilar instances. name suggests motivation development algorithm improve accuracy k-nearest neighbours classiﬁers. information-theoretic metric learning another mahalanobis technique introduced criterion aims minimise kullback-leibler divergence multivariate gaussians inverse covariance matrices used deﬁne mahalanobis distance metrics. gaussians deﬁned advance acts regulariser treated free parameter optimised subject constraints derived similarity information. neighbourhood components analysis another method developed used conjunction k-nn classiﬁers. technique attempts matrix minimising diﬀerentiable loss function approximates behaviour k-nn transformed feature space. ﬁrst application neural networks metric learning introduction siamese networks original authors initially applied models signature veriﬁcation however others since used technique many domains face recognition veriﬁcation siamese networks composed standard feed forward neural networks topology share parameters. output subnetworks compared cosine similarity function produce ﬁnal output network indicating whether instances propagated subnetworks similar not. training network presented pairs instances labelled either positive negative. positive pairs cosine similarity maximised whereas negative pairs minimised. improved resulting contrastive loss function siamese networks given equation function averaged training pairs create objective function training network. instances label indicating whether similar dissimilar performs forward propagation subnetworks margin. value simply sets scale resulting vector space typically generalisation siamese networks class methods consider three instances loss function evaluation called triplet loss functions methods attempt minimise distance target instance another positive example simultaneously maximising distance target instance negative example. variants approach allow deﬁne ground truth labels terms relative similarity. rather hard constraints specifying similarity dissimilarity similarity deﬁned instance similar instance another. also extension nonlinear transformations input data method viewed probabilistic variant siamese networks. nonlinear transformation models used original exposition method stacked restricted boltzmann machines initialised using unsupervised pretraining subsequently ﬁne-tuned using loss function. common theme uniﬁes approaches described thus need train pairs instances. increases eﬀective size training quadratically greatly slowing training time. proposal decouple process learning embeddings learning functions performs embedding entirely new. work utilises similar step process learning hashing function. work embeddings fact strings function used generate hash codes takes form boosted decision trees. also greedy discrete optimisation procedure target hash codes rather numerical optimisation method real valued vectors. similarity metric learning algorithms typically trained pairs objects labelled either positive pairs similar objects negative pairs dissimilar objects. formally objects call also contains pairs objects coupled labels indicating whether similar not. represents binary relation entries relation matrix unknown. convenient problem formulation naturally gives rise models trained pairs objects time. problem approach lies eﬃciency pairwise training. training size eﬀectively squared resulting representation training data eﬃciently encode useful information required construct accurate model. even though information available model ineﬃcient encoding signiﬁcantly slows training procedure. must inevitably train pairs point metric learning process however goal work demonstrate method modifying pre-existing loss functions pairwise training comprises negligible fraction runtime. advantage posing similarity learning task inferring binary relation ability decompose training data classes. idea often studied conjunction equivalence relations equivalence classes form disjoint partition original training set. context relation must exhibit reﬂexivity symmetry transitivity—the last somewhat limiting. drop requirement transitivity instead considering binary tolerance relations reﬂexive symmetric. equivalence relations possible decompose training data classes however classes need disjoint. rather partitioning training into potentially overlapping subsets correspond tolerance class target vector instance. vectors constrained targets instances related close target vector space unrelated instances apart. rationale behind tolerance class mapped cluster target vector space however employ technique explicitly determine tolerance classes instance belongs. instead training large siamese network pairs of—potentially quite large— feature vectors training compute target vector training instance. this completely disregard information provided features associated instance instead solve optimisation problem encourages target vectors similar instances clustered together. target vectors computed multi-target regression model trained embed instances target vector space. provided suitable loss function chosen learning target vectors predeﬁned distance metric applied target vector space result system capable determining instance similarity. assumption underlies method confusability latent classes dataset provide information useful constructing metric. describe method formally still enough abstraction generality obvious. consider diﬀerentiable loss function pair instances. similarity metric learning algorithms rely embedding data space semantic similarity salient usually rely several components objective function model ﬁxed distance euclidean manhattan distance. particular objective function generally implies certain ﬁxed distance metric used embedded data. example using contrastive loss given equation fairly obvious euclidean distance intended metric. leaves components; objective function embedding model. class metric learning algorithms embedding function primary component write loss function yij) embedding function parameterised consider scenarios diﬀerentiable problem could solved large variety numerical optimisation algorithms. trivial systems used practice soon novel feature vector encountered label unknown model capable making prediction. however take learned target vectors train second model implements generalisable model. model created multi-target regression algorithm focus deep neural networks paper. although original model must trained pairs instances composition model require pairwise training. investigate options contrastive loss given equation represents product vectors. using loss function equivalent factoring adjacency matrix underlying binary relation deﬁned training set. adam optimiser minimise loss functions matter seconds dataset consider empirical evaluation. corresponding target vector shown equation multi-target regression model. technique rely speciﬁc regression algorithm instead general. possible multi-target regression method learn mapping features learned target vectors however focus performance neural networks metric learning hence regression algorithm choice. original siamese network introduced applied cosine similarity function embeddings instances computed branches network order determine whether instances related. case value means instances similar value negative indicates instances dissimilar. networks trained contrastive loss replace cosine similarity function euclidean distance interpretation resulting real value also changed. value zero indicates high degree similarity value becomes larger instances considered increasingly dissimilar. target vectors learned using technique presented herein found either optimising loss function based squared euclidean distance product. using euclidean distance small values indicate similar instances large values indicate dissimilar instances. important correct ﬁxed metric used resulting embeddings ensure optimal performance. motivation behind introducing method accelerate training process neural network based metric learning algorithms. firstly show optimisation problem used compute target vectors solved matter seconds thus comprising negligible fraction overall training time. demonstrate time taken method siamese networks converge trained datasets. finally perform extrinsic evaluation show learned metrics perform well k-nn classiﬁcation tasks. standard image classiﬁcation datasets summarised table used demonstrate capabilities method. similarity metric learning methods consider involve pairwise training point necessitates datasets contain pairs instances binary similarity constraints. words dataset must deﬁne binary relation represented manner described section element datasets positive negative pairs generated. pairs labelled positive instances least class common. process performed separately training testing instances prevent overlap train test subsets. basic datasets considered mnist cifar- consist balanced classes similar number total instances. primary diﬀerence mnist contains easily discriminated hand written digits cifar- contains downsampled photographs natural objects. public figures dataset large collection photos spanning diﬀerent identities. unfortunately originators dataset supply urls images dataset several years many links dead. fortunately subset called pubfig scraped made available download version pubfig dataset created faces aligned eyes image always position. version dataset colour images pixels. networks trained dataset supplied central pixels image order reduce overﬁtting caused background clutter surrounding faces. nus-wide multi-label dataset included order simulate tolerance relation. although classes dataset includes unique label vectors consisting diﬀerent combinations classes. results highly complex metric learning problem. diﬃculty dataset compounded presence label noise labels determined using method relies user speciﬁed tags. original dataset consists image urls associated labels however urls unavailable. managed collect original instances. image resized smallest dimension central pixels used training testing. dataset requires diﬀerent network varying complexity associated task diﬀerent number features contained within instance. table provides overview architectures. case siamese network architectures given table describe single branch network. additionally training networks cifar- pubfig nus-wide also train horizontal ﬂips images training set. size output layer determines length learned target vectors dataset. table diﬀerent network architectures used dataset throughout experiments. table dense indicates fully connected layer hidden units convolutionall means convolutional layer feature maps ﬁlters size lastly pool represents pooling layer pool size stride hidden layer relu activation function used output layers nonlinearity. dropout applied hidden layers consisting fully connected units. weight initialisation procedure used setting starting values weights network. applied slightly modiﬁed standardisation procedure target vectors training regression networks. mean subtraction performed however rather dividing individual standard deviation component target vectors scale vectors mean standard deviation components one. prevents distortion target vector space would eﬀectively change objective function weighted squared error variant. target vector optimisation performed using single threaded program written programming language intel deep networks trained using implementation takes advantage functions provided cudnn executed nvidia titan gpu. ﬁrst point order show ﬁrst optimisation problem consumes negligible fraction overall training time constructing learned similarity metric. table contains empirical estimates expected runtime loss functions applied dataset. although product based loss function takes longer ﬁnish contrastive loss still ﬁnish relatively short amount time. table conﬁdence intervals expected runtime ﬁrst phase optimisation problem loss function dataset combination. values seconds. fml-c denotes targets trained contrastive loss function fml-dp denotes targets trained loss function given equation demonstrate method converges signiﬁcantly faster conventional siamese network trained contrastive loss function still achieving competitive accuracy intrinsic evaluation task. range various loss functions used train diﬀerent models evaluating quite diﬀerent must select single metric proxy model performance. chosen area receiver operating characteristic curve binary classiﬁcation task determine whether instances similar according previously deﬁned binary relation. figure shows fast three methods converge dataset. variants fast metric learning method proposed converge signiﬁcantly faster conventional siamese network. however dataset siamese network outperform method. perform extrinsic evaluation shows useful method practice k-nn classiﬁers created utilise euclidean distance applied embeddings make predictions. validation used determining many epochs network trained for. noted models trained product based loss function given equation disadvantage case. multiclass classiﬁers trained using weka multi-label classiﬁers created using binary relevance problem transformation scheme implemented meka. table shows performance k-nn classiﬁers classiﬁers. table performance k-nn classiﬁers applied dataset model combination. accuracy reported three multiclass datasets jaccard index reported multi-label dataset seen fast metric learning methods presented work outperform conventional siamese network trained contrastive loss function evaluating multiclass classiﬁcation tasks. particularly surprising performance pubfig accuracy siamese network signiﬁcantly worse methods. also interesting performance nus-wide dataset. quite surprising fml-c achieves higher jaccard index siamese network despite intrinsic evaluation showing siamese network converges towards accurate solution. fig. plots auroc test training time dataset. plots denotes fast metric learning methods presented work. suﬃx means targets found loss function given equation means found contrastive loss. siamese network convergence methods double number epochs taken train siamese network. siamese network considers pairs images epoch takes twice long regular regression network. nus-wide experiment stopped methods earlier time constraints lack extra information would obtained running duration siamese network. paper presented fast method learning similarity metrics backed deep neural networks. shown convergence time techniques presented work signiﬁcantly faster majority cases result superior performance intrinsic extrinsic evaluation tasks. method coupled contrastive loss function appears good choice learning specialised distance metrics k-nn. would interesting investigate well methods work information retrieval tasks. formalisation given section well suited scenarios wish explicity assign ground truth class data collection even though likely large number latent classes topics. would also interesting investigate eﬀective step training phase speeding networks trained triplet loss functions especially since loss functions popular information retrieval tasks", "year": 2015}