{"title": "Gated Feedback Recurrent Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GF-RNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.", "text": "work propose novel recurrent neural network architecture. proposed gated-feedback extends existing approach stacking multiple recurrent layers allowing controlling signals ﬂowing upper recurrent layers lower layers using global gating unit pair layers. recurrent signals exchanged layers gated adaptively based previous hidden states current input. evaluated proposed gf-rnn different types recurrent units tanh long short-term memory gated recurrent units tasks character-level language modeling python program evaluation. empirical evaluation different units revealed tasks gf-rnn outperforms conventional approaches build deep stacked rnns. suggest improvement arises gfrnn adaptively assign different layers different timescales layer-to-layer interactions learning gate interactions. recurrent neural networks widely studied used various machine learning tasks involve sequence modeling especially input output variable lengths. recent studies revealed rnns using gating units achieve promising results classiﬁcation generation tasks bengio hochreiter successful promising approaches solve issue modifying architecture e.g. using gated activation function instead usual state-to-state transition function composing afﬁne transformation point-wise nonlinearity. gated activation function long short-term memory gated recurrent unit designed persistent memory capture long-term dependencies easily. sequences modeled contain fast changing slow changing components underlying components often structured hierarchical manner which ﬁrst pointed hihi bengio help extend ability learn model longer-term dependencies. conventional encode hierarchy stack multiple levels recurrent layers recently koutn´ık proposed explicit approach partition hidden units groups group receives signal input groups separate predeﬁned rate allows feedback information partitions propagated multiple timescales. stollenga recently showed importance feedback information across multiple levels feature hierarchy however feedforward neural networks. paper propose novel design rnns called gated-feedback deal issue learning multiple adaptive timescales. proposed multiple levels recurrent layers like stacked rnns however uses gated-feedback connections upper recurrent layers lower ones. makes hidden states across pair consecutive timesteps fully connected. encourage recurrent layer work different timescales proposed gf-rnn controls strength temporal connection adaptively. efempirically evaluated proposed model conventional stacked usual single-layer task language modeling python program evaluation experiments reveal proposed model signiﬁcantly outperforms conventional approaches different datasets. able process sequence arbitrary length recursively applying transition function internal hidden states symbol input sequence. activation hidden states timestep computed function current input symbol previous hidden states factorize probability sequence arbitrary length pp··· then train model distribution letting predict probability next symbol given hidden states function previous symbols x··· current symbol difﬁculty training capture long-term dependencies known long previously successful approaches fundamental challenge modify state-to-state transition function encourage hidden units adaptively maintain long-term memory creating paths time-unfolded gradients many timesteps. long short-term memory proposed hochreiter schmidhuber speciﬁcally address issue learning long-term dependencies. lstm maintains separate memory cell inside updates exposes content deemed necessary. recently proposed gated recurrent unit adaptively remembers forgets state based input signal unit. units central proposed model describe details remainder section. lstm unit consists memory cell input gate forget gate output gate memory cell carries memory content lstm unit gates control amount changes exposure memory content. content memory cell j-th lstm unit timestep updated similar form gated leaky neuron i.e. weighted content previous memory content modulated input forget gates respectively respectively vectors input forget gates recurrent layer composed lstm units. element-wise logistic sigmoid function. input vector previous hidden states lstm units respectively. words gates memory cell allow lstm unit adaptively forget memorize expose memory content. detected feature i.e. memory content deemed important forget gate closed carry memory content across many timesteps equivalent capturing long-term dependency. hand unit decide reset memory content opening forget gate. since modes operations happen simultaneously across different lstm units multiple lstm units capture fast-moving slow-moving components. recently proposed like lstm designed adaptively reset update memory content. thus reset gate update gate reminiscent forget input gates lstm. however unlike lstm fully exposes memory content timestep balances previous memory content memory content strictly using leaky integration albeit adaptive time constant controlled update gate timestep state respectively correspond previous memory content candidate memory content. update gate controls much previous memory content forgotten much memory content added. update gate computed based previous hidden states current input element-wise multiplication. major difference traditional transition function states previous step modulated reset gates behavior allows ignore previous hidden states whenever deemed necessary considering previous hidden states current input update mechanism helps capture longterm dependencies. whenever previously detected feature memory content considered important later update gate closed carry current memory content across multiple timesteps. reset mechanism helps model capacity efﬁciently allowing reset whenever detected feature necessary anymore. although capturing long-term dependencies sequence important difﬁcult goal rnns worthnotice sequence often consists slowmoving fast-moving components former corresponds long-term dependencies. ideally needs capture long-term short-term dependencies. hihi bengio ﬁrst showed capture dependencies different timescales easily efﬁciently hidden units explicitly partitioned groups correspond different timescales. clockwork implemented allowing i-th module operate rate positive integer meaning module updated makes module operate different rates. addition precisely deﬁned connectivity pattern modules allowing i-th module affected j-th module here propose generalize cw-rnn allowing model adaptively adjust connectivity pattern between hidden layers consecutive timesteps. similar cw-rnn partition hidden units multiple modules module corresponds different layer stack recurrent layers. unlike cw-rnn however explicit rate module. instead module operate different timescales hierarchically stacking them. module fully connected modules across stack itself. words deﬁne connectivity pattern across pair consecutive timesteps. contrary design cw-rnn conventional stacked rnn. recurrent connection modules instead gated logistic unit computed based current input previous states hidden layers. call gating unit global reset gate opposed unit-wise reset gate applies single unit figure illustrations conventional stacking approach gated-feedback approach form deep architecture. bullets correspond global reset gates. skip connections omitted simplify visualization networks. number hidden layers j−→j weight matrices current input previous hidden states i-th module respectively. compared difference previous hidden states multiple layers controlled global reset gates. long short-term memory gated recurrent unit. cases lstm global reset gates computing unit-wise gates. words eqs. lstm eqs. modiﬁed. global reset gates computing state lstm gru). concatenation hidden states previous timestep superscript index associated parameters transition layer timestep layer timestep wi→j ui→j respectively weight vectors current input previous hidden states. controlled words signal single scalar gi→j depends input previous hidden states call fully-connected recurrent transitions global reset gates gated-feedback fig. illustrates difference conventional stacked proposed gf-rnn. models information ﬂows lower recurrent layers upper recurrent layers. gf-rnn however allows information upper recurrent layer corresponding coarser timescale ﬂows back lower recurrent layers corresponding ﬁner timescales. compared three different architectures singlelayer stacked proposed gf-rnn. architecture evaluated three different transition functions tanh afﬁne long short-term memory gated recurrent unit fair comparison constrained number parameters model roughly similar other. used rmsprop momentum tune model parameters according preliminary experiments results validation used learning rate momentum coefﬁcient training models either lstm units. necessary choose much smaller learning rate case tanh units ensure stability learning. whenever norm gradient explodes halve learning rate. update done using minibatch subsequences length each avoid memory overﬂow problems unfolding time backprop. approximate full back-propagation carrying hidden states computed previous update initialize hidden units next update. every update hidden states used dataset made available part human knowledge compression contest refer dataset hutter dataset. dataset built english wikipedia contains mbytes characters include latin alphabets non-latin alphabets markups special characters. closely following protocols used ﬁrst mbytes characters train model next mbytes validation remaining test vocabulary characters including token unknown character. used average number bits-per-character measure performance model hutter dataset. zaremba sutskever recently showed speciﬁcally stacked lstm able execute short python script. here compared proposed architecture conventional stacking approach model task refer python program evaluation. scripts used task include addition multiplication subtraction for-loop variable assignment logical comparison if-else statement. goal generate predict correct return value given python script. input program output result print statement every input script ends print statement. input script output sequences characters input output vocabularies respectively consist symbols. advantage evaluating models task artiﬁcially control difﬁculty sample difﬁculty determined number nesting levels input sequence length target sequence. ﬁner-grained analysis model observing behavior examples different difﬁculty levels. figure validation learning curves three different architectures; stacked gf-rnn number model parameters gf-rnn number hidden units. curves represent training epochs. best viewed colors. table test models trained gated-feedback hutter dataset epochs. global reset gates ﬁxed bold indicates statistically signiﬁcant winner column task python program evaluation used encoder-decoder based approach learn mapping python scripts corresponding outputs done sutskever machine translation. training models python scripts encoder hidden state encoder unfolded timesteps. prediction performed decoder whose initial hidden state initialized last hidden state encoder rnn. ﬁrst hidden state encoder always initialized zero vector. task used lstm units either without gated-feedback connections. encoder decoder three hidden layers. hidden layer contains units lstm hidden layer contains units. curriculum strategy training model training example random difﬁculty sampled uniformly. generated examples using script provided zaremba sutskever nesting randomly sampled target length used adam train models update using minibatch sequences. used learning rate trained model epochs early stopping based validation performance prevent over-ﬁtting. test time evaluated model multiple sets test examples generated using ﬁxed target length number nesting levels. test contains examples ensured overlap training set. clear table proposed gated-feedback architecture outperforms baseline architectures tried used together widely used gated units lstm gru. however proposed architecture failed improve performance vanillarnn tanh units. addition ﬁnal modeling performance fig. plotted learning curves models wall-clock time rnns trained proposed gatedfeedback architecture tends make much faster progress time. behavior observed number parameters constrained number hidtable generated texts trained models. given seed left-most column models predict next characters. tabs spaces new-line characters also generated models. observing superiority proposed gatedfeedback architecture single-layer conventional stacked ones trained another gf-rnn lstm units time ﬁxing global reset gates validate need global reset gates. without global reset gates feedback signals upper recurrent layers inﬂuence lower recurrent layer fully without control. test gf-lstm without global reset gates results conventional stacked lstm gf-lstm global reset gates conﬁrms importance adaptively gating feedback connections. qualitative analysis text generation qualitatively evaluate stacked lstm gflstm trained earlier generating text. choose subsequence characters test initial seed. model ﬁnishes reading seed text model generate following characters sampling symbol softmax probabilities timestep provide symbol next input. given seed snippets selected randomly test generated sequence characters times model show generated samples model seed snippet table observe stacked lstm failed close tags </username> </contributor> trials. however gf-lstm succeeded close trained larger gf-rnn recurrent layers lstm units. makes possible compare performance proposed architecture previously reported results using types rnns. table present test multiplicative stacked lstm gf-rnn lstm units. performance proposed gf-rnn comparable better than previously reported best results. note sutskever used vocabulary characters result directly comparable ours. experiment used adam instead rmsprop optimize rnn. used learning rate respectively. figure heatmaps stacked gf-rnn difference obtained substracting heatmaps models using grus bottom represents heatmaps models using lstm units. best viewed colors. length target sequences number nesting levels difﬁculty complexity python program increases. observed test sets gf-rnns outperforming stacked rnns regardless type units. fig. represents gaps test accuracies stacked rnns gf-rnns computed subtracting fig. yellow colors indicating large gains concentrated right regions easily gf-rnn outperforms stacked especially number nesting levels grows length target sequences increases. proposed novel architecture deep stacked rnns uses gated-feedback connections different layers. experiments focused challenging sequence modeling tasks character-level language modeling python program evaluation. results consistent different datasets clearly demonstrated gated-feedback architecture helpful models trained complicated sequences involve longterm dependencies. also showed gated-feedback architecture faster wall-clock time training achieved better performance compared standard stacked amount capacity. large gflstm able outperform previously reported best results character-level language modeling. suggests gf-rnns also scalable. gf-rnns able outperform standard stacked rnns best previous records python program evaluation task varying difﬁculties. noticed deterioration performance proposed gated-feedback architecture used together tanh activation function unlike used sophisticated gated activation functions. thorough investigation interaction gatedfeedback connections role recurrent activation function required future. authors would like thank developers theano pylearn also authors thank yann dauphin laurent dinh insightful comments discussion. acknowledge support following agencies research funding computing support nserc samsung calcul qu´ebec compute canada canada research chairs cifar. references bahdanau dzmitry kyunghyun bengio yoshua. neural machine translation jointly learning align translate. technical report arxiv preprint arxiv. bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas bengio yoshua. theano features speed improvements. deep learning unsupervised feature learning nips workshop kyunghyun merri¨enboer bart gulcehre caglar bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. hihi salah bengio yoshua. hierarchical recurrent neural networks long-term dependencies. advances neural information processing systems citeseer goodfellow warde-farley david lamblin pascal dumoulin vincent mirza mehdi pascanu razvan bergstra james bastien fr´ed´eric bengio yoshua. pylearn machine learning research library. arxiv preprint arxiv. untersuchungen dynamischen institut f¨ur inneuronalen netzen. diploma thesis formatik lehrstuhl prof. brauer technische universit¨at m¨unchen http//www. informatik.tu-muenchen.de/˜ehochreit. hochreiter sepp. vanishing gradient problem during learning recurrent neural nets problem soluinternational journal uncertainty fuzziness tions. knowledge-based systems stollenga marijn masci jonathan gomez faustino schmidhuber j¨urgen. deep networks internal selective attention feedback connections. advances neural information processing systems sutskever ilya martens james hinton geoffrey generating text recurrent neural networks. proceedings international conference machine learning", "year": 2015}