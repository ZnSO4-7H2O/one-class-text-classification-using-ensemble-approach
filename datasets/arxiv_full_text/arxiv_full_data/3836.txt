{"title": "Numerically Grounded Language Models for Semantic Error Correction", "tag": ["cs.CL", "cs.NE"], "abstract": "Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements.", "text": "semantic error detection correction important task applications fact checking speech-to-text grammatical error correction. current approaches generally focus relatively shallow semantics account numeric quantities. approach uses language models grounded numbers within text. groundings easily achieved recurrent neural language model architectures further conditioned incomplete background knowledge bases. evaluation clinical reports shows numerical grounding improves perplexity semantic error correction points compared ungrounded approaches. conditioning knowledge base yields improvements. many real world scenarios important detect potentially correct semantic errors inconsistencies text. example clinicians compose reports statements text inconsistent measurements taken patient error rates clinical data range many number-based errors likewise blog writer make statistical claims contradict facts recorded databases numerical concepts constitute contradictions wikipedia googlenews contradictory pairs entailment datasets inconsistencies stem oversight lack reporting guidelines negligence. fact even errors point interesting outliers errors reference database. cases important spot possibly correct inconsistencies. task known semantic error correction paper propose approach support clinicians writing patient reports. system reads patient’s structured background information knowledge base clinical report. recommends improvements text report semantic consistency. example inconsistency shown figure system trained dataset records learnt phrases dilated severely dilated correspond high values respectively. system presented phrase dilated context value detect semantic inconsistency correct text severely dilated. contributions straightforward extension recurrent neural network language models grounding numbers available text; simple method modelling text conditioned incomplete lexicalising evaluation semantic error correction task clinical records shows method achieves improvements percentage points grounding conditioning respectively ungrounded approach approach semantic error correction starts training language model grounded numeric quantities mentioned inline text and/or conditioned potentially incomplete given document semantic checking hypothesis generator proposes corrections scored using trained language model ﬁnal decision step involves accepting best scoring hypothesis. numerically grounded language modelling denote document one-hot representation t-th token vocabulary size. neural uses matrix rd×v derive word embeddings einwt. hidden state previous time step current word embedding sequentially rnn’s recurrence function produce current hidden state conditional probability next word estimated softmax eout output embeddings matrix. out-of-vocabulary numbers. straightforward ﬂoat representation deﬁning ﬂoat numeric conversion function returns ﬂoating point number constructed string input. conversion fails returns zero. proposed mechanism numerical grounding shown figure probability next word depends numbers appeared earlier text. treat numbers separate modality happens share medium natural language convey exact measurements properties real world. training time numeric representations mediate ground language model real world. proposed extension also used conditional language modelling documents given knowledge base. consider tuples accompanying document describing attributes form attribute value attributes deﬁned schema. lexicalise converting tuples textual statements form attribute value. example lexicalise shown figure generated tokens interpreted word embeddings numeric representations. approach incorporate tuples ﬂexibly even evaluate generate corrupted dataset semantic errors test part trusted dataset manually build confusion sets searching development words related numeric quantities grouping appear similar contexts. then document trusted test generate erroneous document sampling substitution confusion sets. documents possible substitution excluded. resulting corrupted dataset balanced containing correct incorrect documents. base single-layer long short-term memory network latent dimensions extend baseline conditional variant conditioning lexicalised also derive numerically grounded model concatenating numerical representation token inputs base model finally consider model grounded conditional vocabulary contains frequent tokens training set. out-of-vocabulary tokens substituted <num unk> numeric <unk> otherwise. extract numerical representations masking grounded models generalise out-ofvocabulary numbers. models trained minimise token cross-entropy epochs backpropagation adaptive mini-batch gradient detable statistics clinical dataset. counts non-numeric numeric tokens reported percentage counts tokens. out-of-vocabulary rates vocabulary frequent words train data. semantic error correction statistical model chooses likely correction possible correction choices. model scores corrected hypothesis higher original document correction accepted. takes original document input generates candidate corrected documents hm}. simple hypothesis generator uses confusion sets semantically related words produce possible substitutions. scorer model assigns score hypothesis scorer based likelihood ratio test original document candidate correction i.e. assigned score represents much probable correction original document. dataset comprises clinical records london chest hospital patient record consists text report accompanying structured tuples. latter describe possible numeric attributes also partly contained report. average tuples oracle hypothesis generator access groundtruth confusion sets estimate scorer using trained base conditional grounded g-conditional lms. additional baselines consider scorer assigns random scores uniform distribution always scorers assign lowest score original document uniformly random scores corrections. report perplexity adjusted perplexity test tokens token classes adjusted perplexity sensitive oov-rates thus allows meaningful comparisons across token classes. perplexities high numeric tokens form large proportion vocabulary. grounded g-conditional models achieved improvement perplexity respectively base model. conditioning without grounding yields slight improvements numerical values lexicalised out-of-vocabulary. qualitative example figure demonstrates numeric values inﬂuence probability tokens given history. select document development substitute numeric values vary selected exact values unseen training data. calculate probabilities observing document different word choices {non mildly severely} grounded dilated associated higher values. shows captured semantic dependencies numbers. detection report precision recall scores table g-conditional model achieves best results total improvement points base model points best baseline. conditional model without grounding performs slightly worse metric base note hypotheses random baseline behaves similarly always. hypothesis generator generated average hypotheses document. results never zero fails detect error. correction report mean average precision addition metrics detection former measures position ranking correct hypothesis. always baseline ranks correct hypothesis again g-conditional model yields best results achieving improvement points points base model improvement points points best baseline. conditional model without grounding worst performance among lm-based models. grounded language models represent relationship words non-linguistic context refer previous work grounds language vision audio video colour olfactory perception however previous approach explored in-line numbers source grounding. language modelling approach inspired approaches grammatical error detection similarly derive confusion sets semantically related words substitute target words alternatives score existing semantic error correction approaches correcting word error choices collocation errors semantic anomalies adjective-noun combinations approaches focus short distance semantic agreement whereas approach detect errors require resolve long-range dependencies. work shows language models useful error correction however neither ground numeric quantities incorporate background kbs. paper proposed simple technique model language relation numbers refers well conditionally incomplete knowledge bases. found proposed techniques lead performance improvements tasks language modelling semantic error detection correction. numerically grounded models make possible capture semantic dependencies content words numbers. future work plan apply numerically grounded models tasks numeric error correction. explore alternative ways deriving numeric representations accounting verbal descriptions numbers. trainable hypothesis generator potentially improve coverage system. authors would like thank anonymous reviewers insightful comments. also thank steffen petersen providing dataset advising clinical aspects work. research supported farr institute health informatics research.", "year": 2016}