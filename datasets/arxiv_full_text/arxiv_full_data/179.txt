{"title": "Handwritten Digit Recognition with a Committee of Deep Neural Nets on  GPUs", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "abstract": "The competitive MNIST handwritten digit recognition benchmark has a long history of broken records since 1998. The most recent substantial improvement by others dates back 7 years (error rate 0.4%) . Recently we were able to significantly improve this result, using graphics cards to greatly speed up training of simple but deep MLPs, which achieved 0.35%, outperforming all the previous more complex methods. Here we report another substantial improvement: 0.31% obtained using a committee of MLPs.", "text": "idsia joint institute university lugano university applied sciences southern switzerland founded dalle molle foundation promoted quality life. competitive mnist handwritten digit recognition benchmark long history broken records since recent substantial improvement others dates back years recently able signiﬁcantly improve result using graphics cards greatly speed training simple deep mlps achieved outperforming previous complex methods. report another substantial improvement obtained using committee mlps. current automatic handwriting recognition algorithms already pretty good learning recognize handwritten digits. decade multilayer perceptrons mlps among ﬁrst classiﬁers tested famous mnist handwritten digit recognition benchmark. layers artiﬁcial neurons layer apparently back biggest feasible mlps trained cores least times slower today. recent single hidden layer units achieved error latest substantial improvement others occurred obtained convolutional neural network using novel elastic training image deformations. ranzato pre-trained hidden layer unsupervised fashion used supervised learning achieve error rate. recently able signiﬁcantly improve result obtaining error rate using graphics cards greatly speed training plain deep mlps deformations proved essential prevent mlps million free parameters overﬁtting. deformation process keep network training speed port onto well. stage classiﬁer design process usually collected possible classiﬁers. often yields best performance. intriguingly however sets patterns misclassiﬁed diﬀerent classiﬁers necessarily overlap. information could harnessed committee. context handwritten recognition chellapilla already showed combination various classiﬁers trained quickly single classiﬁer yielding error rate. focus improving recognition rate using committee mlps. goal produce group classiﬁers whose errors various parts training diﬀer much possible show handwritten digit recognition achieved training identical classiﬁers data normalized diﬀerent ways prior training. mnist consists datasets training testing many studies divide training sets consisting images training validation. however best results mnist obtained deforming training images thus greatly increasing number. allows training networks many weights making insensitive in-class variability. network also trained numerous slightly deformed images continually generated online fashion; hence whole un-deformed training validation without wasting training images. pixel intensities original gray scale images range obtained applying displacement ﬁeld digit. displacement ﬁeld built convolving randomly initialized ﬁeld gaussian kernel whose standard deviation deﬁned scaling factor controlling amplitude applied elastic deformations; takes seconds deform mnist training images elastic distortions. time-consuming part latter—convolution gaussian kernel—is ported gpu. mnist training split sequentially processed batches. mnist digits scaled original pixels pixels proper center simpliﬁes convolution. batch grid cells zero-padded thus avoiding margin eﬀects applying gaussian convolution kernel size kernel parts random ﬁeld. blocks contain threads generating elastic displacement ﬁeld takes seconds. deforming whole training times faster taking instead original seconds. optimizations would possible porting deformations onto using hardware’s interpolation capabilities perform ﬁnal bilinear interpolation. omitted since deformations already pretty fast preprocessing original mnist data mainly motivated practical experience. mnist digits normalized width height bounding equals pixels. variation aspect ratio various digits quite large normalize width bounding range pixels step-size pixels prior training digits except ones. normalizing original mnist training data results normalized training sets. total perform experiments seven diﬀerent data sets training procedure network summarized figure network trained separately normalized original data. normalization done digits training prior training training epoch digits distorted diﬀerent way. network trained original mnist data normalization step omitted. figure training committee member. original mnist training data normalized prior training normalized data distorted training epoch used input network depicted digit represents whole training set. perform experiments analyze performance improvements committees. committee consists seven randomly initialized one-hidden-layer mlps hidden units trained algorithm randomly selected batches. committees diﬀer data preprocessed prior training data deformed ﬁrst experiments performed undeformed original mnist images. train committee seven mlps original mnist also form committee mlps trained preprocessed data. table error rates listed individual nets committees. improvement committee respect individual nets marginal ﬁrst experiment. adding preprocessing individual experts well corresponding committee second experiment achieve substantially better recognition rates. study combined eﬀect preprocessing deformation perform four additional experiments deformed mnist unless stated otherwise default elastic deformation parameters used. experiments deformed images independent experiment randomly reselect training validation sets individual experts simulating bootstrap aggregation technique resulting committee performs slightly better experiment last experiment train seven mlps preprocessed images also continually deformed training. error rate committee best result ever reported simple architecture. table error rates individual nets resulting committees. experiment seven nets trained original mnist whereas experiment seven nets trained preprocessed data width normalization bounding pixels wide; orig original mnist. table error rates individual nets resulting committees. experiments seven nets trained deformed mnist whereas experiment training validation sets reselected. experiment seven nets trained deformed mnist experiment seven nets trained normalized deformed mnist. width normalization bounding pixels wide; orig original mnist. implementation framework explained ciresan architecture error rate mnist. train additional nets architecture preprocessed data form committee averaging predictions individual nets. mnist data already preprocessed width height digit pixels. variations writing style result diﬀerent aspect ratios handwritten digits. therefore normalize width digits pixels. results nets trained normalized data together resulting committee listed table interestingly error committee considerably lower individual nets. best result ever reported mnist. misclassiﬁed digits shown figure many ambiguous and/or uncharacteristic obviously missing parts strange strokes etc. interestingly second guess network correct misclassiﬁed digits. benchmark single precision ﬂoating-point gpu-based committees neural nets outperform previously published methods including complex ones involving specialized architectures unsupervised pre-training combinations machine learning classiﬁers etc. avoid overﬁtting training sets suﬃcient size obtained appropriately distorting images.", "year": 2011}