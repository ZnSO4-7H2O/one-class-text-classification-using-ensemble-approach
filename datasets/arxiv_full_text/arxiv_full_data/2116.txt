{"title": "Local Rademacher Complexity Bounds based on Covering Numbers", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "This paper provides a general result on controlling local Rademacher complexities, which captures in an elegant form to relate the complexities with constraint on the expected norm to the corresponding ones with constraint on the empirical norm. This result is convenient to apply in real applications and could yield refined local Rademacher complexity bounds for function classes satisfying general entropy conditions. We demonstrate the power of our complexity bounds by applying them to derive effective generalization error bounds.", "text": "paper provides general result controlling local rademacher complexities captures elegant form relate complexities constraint expected norm corresponding ones constraint empirical norm. result convenient apply real applications could yield reﬁned local rademacher complexity bounds function classes satisfying general entropy conditions. demonstrate power complexity bounds applying derive eﬀective generalization error bounds. ples generalization analysis learning algorithms stands central place machine learning since important understand factors inﬂuencing models’ behavior well suggest ways improve seminar example found multiple kernel learning context cortes established framework showing generalization analysis could motivate novel algorithms. errors supremum empirical process supf∈f associated loss class induced hypothesis space true probability measure empirical probability measure respectively. indicated supremum closely connected acquired projecting function class onto sample. quantities like covering numbers measure number balls required cover original class introduced capture ﬁner scale size real-valued function classes recent development concentration inequalities empirical process theory possible obtain slightly tighter however mentioned approaches provide global estimates complexity function classes reﬂect learning algorithm explores function class interacts examples moreover bound control deviation empirical errors true errors simultaneously whole class quantity primary importance deviation particular function picked learning algorithm reaching supremum therefore analysis based global complexity would give rather conservative estimate. hand learning algorithms inclined towards choosing functions possessing small empirical errors hopefully also small generalization errors functions also admit small variances. obtained prediction rule likely fall subclass small variances seminar work koltchinskii panchenko massart turns notion rademacher complexity naturally modiﬁed take account yielding so-called local rademacher complexity since local rademacher complexity always smaller global counterpart discussion based local rademacher complexities always yields signiﬁcantly better learning rates variance-expectation conditions. mendelson initiated discussion estimating local rademacher complexities covering numbers complexity bounds eﬀective establishing fast learning rates. however discussions somewhat dispersed sense author provided general result applicable function classes. indeed mendelson derived local rademacher complexity bounds several function classes satisfying diﬀerent entropy conditions case-by-case involved deduction also relies speciﬁc entropy conditions. mendelson also derived general reproducing kernel hilbert space interesting local rademacher complexity bound based eigenvalues associated integral operator later generalized ℓp-norm context results exclusively developed rkhss still remains unknown whether could extended general function classes. paper reﬁne discussions providing general sharp results controlling local rademacher complexities covering numbers. distinguished property result captures elegant form relate local rademacher complexities associated empirical local rademacher complexities allows improve existing local rademacher complexity bounds function classes diﬀerent entropy conditions systematic manner. also demonstrate eﬀectiveness complexity bounds applying reﬁne existing learning rates. paper organized follows. section formulates problem. section provides general local rademacher complexity bound well applications diﬀerent function classes. section applies complexity bounds generalization analysis. proofs presented section conclusions presented section class consisting elements represented minus elements real number indicates least integer less represents natural logarithm denote quantity constant multiple involved arguments deﬁnition metric space called ǫ-cover every element satisfying ǫ-cover called proper ǫ-cover covering number cardinality minimal proper ǫ-cover deﬁnition probability measure examples independently drawn. independent rademacher random variables equal probability class functions introduce paper concentrate attention local rademacher complexities. word local means class rademacher process deﬁned subset original class. consider local rademacher complexities following form refer former local rademacher complexity latter empirical local rademacher complexity. parameter used ﬁlter functions large variances little signiﬁcance learning process since learning algorithms unlikely pick them. section devoted establishing general local rademacher complexity bound. purpose ﬁrst show control empirical local rademacher complexities. empirical radii connected true radii contraction property rademacher averages examples illustrating power result also presented. generally trivial control integral since random variable appears upper limit integral trivially used control r.h.s. mendelson’s idea diﬀerent entropy conditions construct diﬀerent upper bounds involved integral random variable appears relatively simple term. mendelson established controlled applying standard upper bound although deductions elegant allow general bounds local rademacher complexities sometimes yield unsatisfactory results looseness introduced constructing additional artiﬁcial upper bound integral overcome drawbacks providing general result controlling local rademacher complexity bounds. step stone following lemma controlling local rademacher complexity sub-class involving random radius local rademacher complexity sub-class involving deterministic adjustable parameter plus linear function allows direct standard upper bound e√ˆr excludes necessity constructing non-trivial bounds remark advantage theorem existing local rademacher complexity bounds consists fact provides general framework controlling local rademacher complexities which show section trivially derive explicit local rademacher complexity bounds entropy information available. furthermore since theorem involve artiﬁcial upper bound integral could yield sharper local rademacher complexity bounds compared results demonstrate eﬀectiveness theorem applying interesting classes satisfying general entropy conditions. discussion based reﬁned entropy integral used tackle situation standard entropy integral diverges. ization performance learning algorithms. learning context given input space output space along probability measure given sequence examples independently drawn goal prediction rule perform prediction accurately possible. error incurred using minimization principle ﬁrstly establishes so-called empirical error approximate searches prediction rule minimizing speciﬁed class called hypothesis space. argminh∈h denoting argminh∈h best prediction rule attained generalization analysis aims relate excess generalization error empirical behavior sample. error bound. call function sub-root nonnegative nondecreasing ψ/√r nonincreasing sub-root function checked equation unique positive solution referred ﬁxed point exist three positive constants satisfying logp. suppose variance-expectation condition holds functions i.e. exists constant then satisﬁes following inequality probability least remark possible derive generalization error bounds using local rademacher complexity bounds given entropy condition. obstacle applying lemma r.h.s. sub-root function. trick towards problem consider local rademacher complexity slightly larger function class sub-root application study generalization performance also requires trick star-hull argument. indeed trick show bound could yield following generalization proof lemma temporarily ﬁxed minimal proper ǫ-cover class respect metric according deﬁnition covering numbers know furthermore lemma shows |f△| using similar deduction strategy also prove corollary local rademacher complexity bounds entropy number grows polynomial simplicity omit proof here. solving equality gives cdn− logp. directly checked also satisﬁes kfk∞ consequently apply lemma show particular function following inequality holds probability least paper provides systematic approach estimating local rademacher complexities covering numbers. local rademacher complexity eﬀective concept learning theory recently received increasing attention since captures property prediction rule picked learning algorithm always lies subset original class. provide general local rademacher complexity bound captures elegant form relate complexities constraint norm corresponding ones constraint norm. bound convenient calculate easily applicable practical learning problems. show general result could yield local rademacher complexity bounds superior mendelson applied function classes satisfying general entropy conditions. also apply derived local rademacher complexity bounds generalization analysis. since deﬁnition covering numbers requires ǫ-cover belong original class covering numbers sub-class necessarily smaller whole class. however following structural result tackling covering numbers sub-class.", "year": 2015}