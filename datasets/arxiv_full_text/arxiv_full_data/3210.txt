{"title": "Large Scale, Large Margin Classification using Indefinite Similarity  Measures", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Despite the success of the popular kernelized support vector machines, they have two major limitations: they are restricted to Positive Semi-Definite (PSD) kernels, and their training complexity scales at least quadratically with the size of the data. Many natural measures of similarity between pairs of samples are not PSD e.g. invariant kernels, and those that are implicitly or explicitly defined by latent variable models. In this paper, we investigate scalable approaches for using indefinite similarity measures in large margin frameworks. In particular we show that a normalization of similarity to a subset of the data points constitutes a representation suitable for linear classifiers. The result is a classifier which is competitive to kernelized SVM in terms of accuracy, despite having better training and test time complexities. Experimental results demonstrate that on CIFAR-10 dataset, the model equipped with similarity measures invariant to rigid and non-rigid deformations, can be made more than 5 times sparser while being more accurate than kernelized SVM using RBF kernels.", "text": "despite success popular kernelized support vector machines major limitations restricted positive semi-deﬁnite kernels training complexity scales least quadratically size data. many natural measures similarity pairs samples e.g. invariant kernels implicitly explicitly deﬁned latent variable models. paper investigate scalable approaches using indefinite similarity measures large margin frameworks. particular show normalization similarity subset data points constitutes representation suitable linear classiﬁers. result classiﬁer competitive kernelized terms accuracy despite better training test time complexities. experimental results demonstrate cifar- dataset model equipped similarity measures invariant rigid non-rigid deformations made times sparser accurate kernelized using kernels. linear support vector machine become classiﬁer choice many large scale classiﬁcation problems. main reasons success linear margin property achieved convex optimization training time linear size training data testing time independent although linear classiﬁer operating input space usually ﬂexible linear classiﬁer operating mapping data higher dimensional feature space become arbitrarily complex. mixtures linear classiﬁers proposed increase non-linearity linear classiﬁers seen feature mappings augmented non-linear gating functions. training mixture models usually scales bilinearly respect data number mixtures. drawback non-convexity optimization procedures need know number components beforehand. kernelized maps data possibly higher dimensional feature space maintains convexity become arbitrarily ﬂexible depending choice kernel function. kernels however limiting. firstly kernelized signiﬁcantly higher training test time complexities compared linear svm. number support vectors grows approximately linearly training data training complexity becomes approximately somwehere testing time complexity scales linearly number support vectors bounded secondly positive deﬁnite kernels sometimes expressive enough model various sources variation data. recent study argues metric constraints necessarily optimal recognition. example image classiﬁcation problems considering kernels similarity measures cannot align exemplars model deformations measuring similarities. response this invariant kernels introduced generally indefinite. indeﬁnite similarity measures plugged solvers result non-convex optimizations unless explicitly made mainly using eigen decomposition methods alternatively latent variable models proposed address alignment problem e.g. cases dependency latent variables parameters model learnt mainly drawbacks optimization problem cases becomes non-convex cost training becomes much higher case without latent variables. paper aims address problems using explicit basis expansion. show resulting model better training test time complexities kernelized models make indeﬁnite similarity measures without need removal negative eigenvalues requires expensive eigen decomposition make multiple similarity measures without losing convexity cost linear number similarity measures. contributions proposing analyzing basis expanding regarding aforementioned three properties investigating suitability particular forms invariant similarity measures large scale visual recognition problems. background given dataset based classiﬁers learn margin binary classiﬁers. classiﬁer learnt hinge loss. minimizing positive semi deﬁnite kernel associated reproducing kernel hilbert space vice versa implicitly deﬁned feature mapping associated consequently representer particular case namely linear kernel associated euclidean space linear classiﬁer given minimizing primal objective generally given arbitrary kernel kernelized classiﬁer αtykyα diag need positiveness evident dual objective quadratic regularizing term depends eigenvalues case indeﬁnite problem becomes non-convex inner products need re-deﬁned associating rkhs indeﬁnite similarity measures. various workarounds indeﬁnite similarity measures exist involve expensive eigen decomposition gram matrix kernel learnt similarity matrix constraints e.g. close similarity matrix closeness usually measured frobenius norm. case frobenius norm closed form solution spectrum clipping namely setting negative eigenvalues gram matrix pointed guarantee resulting kernels optimal classiﬁcation. nevertheless jointly optimizing kernel classiﬁer impractical large scale scenarios. details possible re-formulations regarding indeﬁnite similarity measures refer reader information. linear kernelized different properties. linear training cost testing cost dimensionality kernelized training complexity cost evaluating kernel pair data number resulting support vectors. testing cost kernelized therefore signiﬁcant body research dedicated reducing training test costs kernelized svms approximating original problem. common approach approximating kernelized problem restrict feature mapj= βjψh methods direction either learn synthetic samples restrict training data methods essentially exploit rank approximations gram matrix rank approximations result speedups training testing complexities kernelized svm. methods learn basis coordinates outside training data e.g. usually involve intermediate optimization overheads thus prohibitive large scale scenarios. contrary nystr¨om method gives rank approximation cost. refers matrix indexed similarly approximation derived deﬁning eigenfunctions expansions numerical eigenvectors consequence data embedded euclidean space real. rest paper case indeﬁnite similarity measures refer indeﬁnite version similarity matrix refer normalization nystr¨om normalization. order approximation indeﬁnite indeﬁnite ˜kmm needs made psd. spectrum clipping spectrum spectrum shift spectrum square possible solutions based eigen decomposition ˜kmm. latter achieved without eigen decomposition step goal matrix closest original indeﬁnite respect reduced basis spectrum clip gives closed form solution. therefore negative eigenvalues spectrum clip technique gives good rank approximations ˜kmm used rank approximation however considerable number negative eigenvalues case similarity measures consider later section guarantee resulting matrix optimal classiﬁcation. true specially eigenvectors associated negative eigenvalues contain discriminative information. experimentally verify section negative eigenvalues contain discriminative information. seek normalizations assume require eigen-decompositions. example replace covariance columns kmn. experimentally found simple embedding presented next section competitive nystr¨om embedding similarity measures outperforming case indeﬁnite ones studied. similar nystr¨om feature space different normalization scheme pointed section centralization better conditions linear solver normalization average norm useful combining multiple similarity measures. be-svm classiﬁer regularizer results sparser solutions cost expensive optimization regularization. therefore large scale scenarios regularization combined reduced basis preferred regularizer combined larger basis set. using multiple similarity measures straightforward be-svm. concatenated feature lack expressibility kernels argued e.g. example similarity measures based vectorial representations data likely indeﬁnite. particularly computer vision considering latent information results lack ﬁxed vectorial representation instances therefore similarity measures based latent information likely indeﬁnite. applications indeﬁnite similarity measures computer vision pointed below. proposes jitter kernels building desired invariances classiﬁcation problems. uses indeﬁnite pairwise similarity measures latent positions objects clustering. considers deformation models image matching. deﬁnes indeﬁnite similarity measure based explicit correspondences pairs images image classiﬁcation. similarity measure representation given latent variable regularization term latent variable possible latent variables associated speciﬁcally involves latent positions similarity measure becomes similar involves latent positions local deformations becomes similar zero order model finally prior combination latent positions local deformations gives similarity measure similar proposed similarity measure picks latent variables maximal similarity values contrast latent variables suggested minimize metric distance based kernel advantage metric based latent variable selection clear works argue unnecessary restrictions metrics also deriving metric best expensive. therefore latent variables selected according similarity values instead metric distances. note similar approaches kernel ﬁxed vectorial representation data given latent information. latent informations turn updated using alterantive minimization approach. makes optimization non-convex differs similarity measures directly model latent informations. table complexity analysis kernelized be-svm. number samples classes assumed equal number kernels/similarity measures dimensionality representations required evaluating kernels/similarity measures cost evaluating kernels/similarity measures. well sophisticated formulations particular concludes case kernelized svms terms accuracy competitive terms training testing complexities superior. therefore consider approach kernelized svm. case linear svms however results unnecessary overhead algorithm choice. be-svm expected faster generalize better be-svm bases classes used binary classiﬁers. case be-svm bases classes consideration used binary classiﬁer clear advantage terms training complexity. however reduction size basis algorithm generalizes less comparison approach. therefore consider formulation be-svm. table summarizes memory computational complexity analysis kernelized be-svm. shown upper bounds complexities considered upper bound nsv. kernelized be-svm margin classiﬁers feature spaces. feature space kernelized implicitly deﬁned kernel function feature space be-svm explicitly deﬁned empirical kernel maps. order derive margin function data ﬁrst need derive dual be-svm objective assume non-squared hinge loss unnormalized feature mappings borrowing representer yiβi consequently derive be-svm dual objective similar dual objective refer similarity data bases. theorem considering conditions primal derive margin be-svm given optimal dual variables isβtyst opposed toαtykyα− kernelized given optimal dual variables parameter kernel said solution besvm even derived large eigenpairs even less small ones. straightforward therefore contribution large eigenpairs {|λi ampliﬁed. similarly contribution small eigenpairs dampened. figure demonstration kernelized be-svm using gaussian kernels based equally weighted kernels. without normalization. normalization data randomly selected bases. fold cross validation accuracy number support vectors averaged scenarios based problem different spatial noises. noise model scenario zero mean gaussian visualization noiseless data clarity. best viewed electronically. comparing margin nystr¨omized method difference nystr¨omized method be-svm normalization covariance kernels nystr¨omized method suitable covariance basis feature space. therefore said normalization essentially de-correlates bases feature space. although appealing property associating rkhs indeﬁnite similarity measures de-correlation cases non-trivial. case covariance kernels said be-svm assumes un-correlated bases bases always correlated feature space. larger sets bases usually result covariances un-correlated assumption violated large bases. consequence cases covariance kernels large bases be-svm expected perform worse nystr¨omized method. however sufﬁciently small bases case indeﬁnite similarity measures reason superiority nystr¨omized method. cases practice be-svm competitive better nystr¨omized method. dual objective be-svm tends result sparser solutions measured nonzero support vector coefﬁcient believe main reason modiﬁcation eigenvalues described section note however order classify sample similarity training data needs evaluated irrespective sparsity be-svm solution sense be-svm dual objective results completely dense solutions similar primal be-svm objective without basis reduction. however solution made sparse construction reducing basis similar case primal be-svm objective. demonstrate here mainly main focus primal objective. deﬁnition kernel be-svm solution be-svm inherent bias respect distribution class labels. words contribution class norm consequently value directly depends number bases class. consequently decision boundary be-svm shifted towards class less bases compare decision boundaries left exists body work regarding proximity data similarity dissimilarity measures classiﬁcation problems. uses similarity ﬁxed samples features kernel classiﬁer. uses proximities data features linear classiﬁer. uses proximities data features proposes linear program machine based representation. contrast normalization similarity points subset data features linear classiﬁer. present experimental results cifar- dataset dataset comprised tiny images images classes involved divided folds inequal distribution class labels fold. ﬁrst folds used training fold used testing. modiﬁed version feature described experiments cell sizes result normalization cells namely normalizing gradient/contrast information neighboring cells cells border images normalized properly. believe negative effect results paper best results possible model rely consistency normalization images address problem. possible up-sample images ignore hog-cells boundaries provide results ﬁxes. experiments center feature vector scale feature vectors inversely average norm centered feature vectors similar normalization be-svm results easier selection parameters formulations. unless stated otherwise kernelized gaussian kernels rest. liblinear optimize primal linear objectives squared hinge loss similar kernelized libsvm report multi-class classiﬁcation results test used formulation kernelized formulation methods. figure shows performance linear kernelized gaussian kernel function number parameters models. number parameters linear input dimensionality kernelized dimensionality feature vector corresponding kernel operates numbers model results model trained folds training data figure shows performance kernelized function support vectors trained folds. except linear cell size pixels saturates performance folds models consistently beneﬁt training data. general form invariant similarity measures consider given particular consider rigid deformable similarity measures smallest unit deformation/translation cell. figure performance be-svm function different similarity measures trained ﬁrst fold. refers cell size pixels. refer linear gaussian kernels respectively refers similarity measure rigid local deformations {|zx {−hr hr}} allows maximum cells displacements directions indices cells direction dimensional cell located position zero cells outside maximal cross correlation {|zx {−hl hl}} allows maximum cell local deformation cells consider maximum deformation pixels e.g. cells cell size pixels. regularizing global local deformations straightforward formulation. however notice signiﬁcant improvements displacements considered probably related small size latent suitable small images cifar-. figure shows performance be-svm using different similarity measures trained ﬁrst fold. seen invariant similarity measures improve recognition performance. particularly absence information modelling rigid deformations seems much beneﬁcial modelling local deformations. interesting observation aligning data higher resolutions much crucial models suffer performace losses resolution increased cell size pixels pixels. however be-svm achieves signiﬁcant performance gains aligning data higher resolutions compare tried training linear kernelized models jittering feature vectors manner invariant similarity measures jitter cells zeropadding cells outside images. resulted signiﬁcant performance losses linear kernelized also siginiﬁcantly increasing memory requirement computation times. believe reason boundary effects; also mentioned previous work e.g. also believe jittering input images combination boundary figure shows accuracy be-svm using different similarity measures different basis selection strategies; basis size exemplars. ﬁgure ‘rand’ refers random selection bases ‘indx’ refers selection samples according indices kmed’ refers kernel k-medoids approach based similarity measure ‘nystrom’ refers selection bases similar ‘indx’ approach nystr¨om normalization using spectrum clip indeﬁnite similarity measures. reported results ‘rand’ method averaged trials; variance signiﬁcant. observed methods except ‘nystrom’ result similar performances. also tried sophisticated sample selection criteria observed similar behaviour. attribute little variation quality exemplars cifar- dataset. observed this rest sub-sampling strategies average multiple random basis selection trials rather deterministic ‘indx’ approach. difference normalization factors be-svm nystr¨om method evident ﬁgure. be-svm normalization tends consistently superior case indeﬁnite similarity measures. kernels nystr¨om normalization tends better lower resolutions worse higher resolutions believe main reason lack signiﬁcant similarity bases higher resolutions absence alignment. cases rank assumption violated normalization diagonally dominant capture useful information. order analyze performance be-svm depends eigenvalues similarity measures provide following eigenvalue analysis. compute similarity bases corresponding perform eigen-decomposition resulting matrix. table shows ratio negative eigenvalues ‘ngrat’= rela|λi| |λi| function various similarity measures tive energy eigenvalues ‘ngeng’= cell size last columns namely ‘cornyst’ ‘corbe’ reﬂect correlation measured entities ‘ngrat’ ‘ngeng’ observed performance be-svm using nystr¨om normalization be-svm normalization. used pearson’s measure extent linear dependence test performances different normalization schemes. observed that normalization schemes positive correlation ratio negative eigenvalues relative energy be-svm normalization correlates strongly observed entities. this conclude negative eigenvectors contain discriminative information be-svm’s normalization suitable indeﬁnite similarity measures. also experimented spectrum spectrum square methods nystr¨om normalization generally provided slightly worse results comparison spectrum clip technique. different similarity measures contain complementary information. fortunately be-svm make multiple similarity measures construction. demonstrate this using fold training data greedily incremental augmented similarity measures contributing ones. using approach found sets similarity measures complementary information low-resolution two-resolution surprisingly resolution sequence resembles part based models multi resolution rigid models figure performance be-svm using multiple similarity measures various sizes basis set. results dotted dashed solid lines represent folds worth training data. text analysis. trained be-svm models using similarity measures various sizes basis various sizes training data. figures show results be-svm models trained folds. shown number supporting exemplars be-svm based size basis set. seen using basis size performance be-svm using tworesolution similarity measures surpass kernelized trained data based approximately support vectors. using low-resolution similarity measures outperforms kernelized svms trained folds training data. furthermore observed model complexity measured either number supporting exemplars model parametrs be-svm performs better kernelized svm. figure shows performance be-svm using different similarity measures various basis sizes different training sizes. observed using indeﬁnite similarity measures signiﬁcantly increase performance model compare curve curve line style. example using training data resolution deformable approach results improvements accuracy comparison best performing kernel furthermore two-resolution approach outperforms single resolution approach approximately accuracy measured model parameters be-svm roughly times sparser kernelized accuracy. measured supporting exemplars sparsity increases roughly need point different similarity measures different complexities e.g. expensive evaluate however bases shared different similarity measures cache utilized much efﬁciently less memory access computations. tried multiple kernel learning kernelized kernels. compared sophisticated methods found following procedure give competitive performances much less training costs. deﬁning approach consists performing line search optimal alpha results best figure performance be-svm model parameters various sizes basis using multiple similarity measures. curve linear kernelized represents result training folds training data. curve be-svm shows result training model basis size trained folds training data. fold cross validating performance. using procedure linear kernels found contribute anything gaussian kernels. optimal combination high resolution resolution gaussian kernels resulted performance gain less accuracy comparison founds insigniﬁcant report performance considering fact number parameters increases approximately times using approach. combined equal weights value depends similarities correlate respect bases. case normalized i.e. features centered values similarity measure weighted global weight similarity measures smaller variances similarity values. be-svm’s normalization empirical kernel maps optimal discrimination seen reasonable prior combining different similarity measures. utilizing prior combination linear classiﬁers regularizers important consequences centering helps reduce correlation dimensions scaling helps balance effect regularization different similarity measures irrespective overall norms scaling directly affects parameter tuning learning linear classiﬁers similarity measures various basis sizes parameter used train classiﬁers. cross-validation still better option cross-validating different parameters settings specially combining multiple similarity measures expensive prohibitive. using be-svm’s normalization essentially avoid searching optimal combining weights different similarity measures also tuning parameter linear training. section quantitatively evaluate normalization suggested be-svm compare combinations. particularly consider various normalizations feature vectors similarly various normalization schemes empirical kernel consider following normalizations figure performance be-svm different normalization schemes feature vector empirical kernel different similarity measures. legend reﬂects using normalization schemes feature vectors empirical kernel maps respectively results average test performance figure performance be-svm different normalization schemes feature vector empirical kernel different combinations similarity measures. legend reﬂects using normalization schemes feature vectors empirical kernel maps respectively results average test performance report test performances combinations normalizations feature vectors empirical kernel maps cases parameter crossvalidated cases bases uniformly subsampled ﬁrst fold training figure shows performance be-svm combination different normalizations feature vectors empirical kernel maps different similarity measures. reported numbers bottom cross validated. observed be-svm’s normalization works best feature empirical kernel normalizations. although z-scoring suitable linear similarity measures overall be-svm’s normalization feature space works better alternatives. particularly single similarity measure cases seems normalizing feature according be-svm’s normalization important normalizing empirical kernel map. cross-validation parameter marginally affects performance change conclusions drawn case. figure shows performance be-svm combination different normalizations feature vectors empirical kernel maps different combinations similarity measures observed be-svm’s normalization kernel much important effective combining multiple similarity measure analyzed scalable approaches using indeﬁnite similarity measures large margin scenarios. showed model based explicit basis expansion data according arbitrary similarity measures result competitive recognition performances scaling better respect size data. model named basis expanding thoroughly analyzed extensively tested cifar- dataset. study explore basis selection strategies mainly small intra-class variation dataset. expect basis selection strategies play crucial role performance resulting model challenging datasets e.g. pascal imagenet. therefore immediate future work apply be-svm larger scale challenging problems e.g. object detection combination data driven basis selection strategies.", "year": 2014}