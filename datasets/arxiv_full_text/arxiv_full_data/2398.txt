{"title": "Algorithmic Connections Between Active Learning and Stochastic Convex  Optimization", "tag": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "abstract": "Interesting theoretical associations have been established by recent papers between the fields of active learning and stochastic convex optimization due to the common role of feedback in sequential querying mechanisms. In this paper, we continue this thread in two parts by exploiting these relations for the first time to yield novel algorithms in both fields, further motivating the study of their intersection. First, inspired by a recent optimization algorithm that was adaptive to unknown uniform convexity parameters, we present a new active learning algorithm for one-dimensional thresholds that can yield minimax rates by adapting to unknown noise parameters. Next, we show that one can perform $d$-dimensional stochastic minimization of smooth uniformly convex functions when only granted oracle access to noisy gradient signs along any coordinate instead of real-valued gradients, by using a simple randomized coordinate descent procedure where each line search can be solved by $1$-dimensional active learning, provably achieving the same error convergence rate as having the entire real-valued gradient. Combining these two parts yields an algorithm that solves stochastic convex optimization of uniformly convex and smooth functions using only noisy gradient signs by repeatedly performing active learning, achieves optimal rates and is adaptive to all unknown convexity and smoothness parameters.", "text": "interesting theoretical associations established recent papers ﬁelds active learning stochastic convex optimization common role feedback sequential querying mechanisms. paper continue thread parts exploiting relations ﬁrst time yield novel algorithms ﬁelds motivating study intersection. first inspired recent optimization algorithm adaptive unknown uniform convexity parameters present active learning algorithm one-dimensional thresholds yield minimax rates adapting unknown noise parameters. next show perform d-dimensional stochastic minimization smooth uniformly convex functions granted oracle access noisy gradient signs along coordinate instead real-valued gradients using simple randomized coordinate descent procedure line search solved -dimensional active learning provably achieving error convergence rate entire real-valued gradient. combining parts yields algorithm solves stochastic convex optimization uniformly convex smooth functions using noisy gradient signs repeatedly performing active learning achieves optimal rates adaptive unknown convexity smoothness parameters. ﬁelds convex optimization active learning seem evolved quite independently other. recently pointed relatedness inherent sequential nature ﬁelds complex role feedback taking future actions. following that made connections explicit tying together exponent used noise conditions active learning exponent used uniform convexity optimization. used establish lower bounds stochastic optimization functions based proof techniques active learning. however unclear concrete algorithmic ideas common ﬁelds. dimensional stochastic optimization precisely complexity -dimensional active learning. inspired optimization algorithm adaptive unknown uniform convexity parameters design interesting one-dimensional active learner also adaptive unknown noise parameters. algorithm simpler adaptive active learning algorithm proposed recently handles pool based active learning setting. given access active learner subroutine line search show simple randomized coordinate descent procedure minimize uniformly convex functions much simpler stochastic oracle returns bernoulli random variable representing noisy sign gradient single coordinate direction rather full-dimensional real-valued gradient vector. resulting algorithm adaptive unknown smoothness parameters achieve minimax optimal convergence rates. first-order stochastic convex optimization task approximately minimizing convex function convex given oracle access unbiased estimates function gradient point using queries possible paper shall always assume k.k∗ deal strongly smooth uniformly convex functions parameters stochastic ﬁrst order oracle function accepts returns expectation internal randomness oracle. optimization algorithm method sequentially queries oracle points returns estimate optimum queries performance measured either function error paper consider coordinate descent algorithms motivated applications computing overall gradient even function value expensive high dimensionality huge amounts data computing gradient coordinate cheap. sign oracle weaker ﬁrst order oracle actually obtained returning sign ﬁrst order oracle’s noisy gradient mass noise distribution grows linearly around zero mean optimum along coorsign probability proportional value directional derivative point unreasonable circumstances even calculating gradient i-th direction could expensive estimating sign could much easier task requires estimating whether function values expected increase decrease along coordinate slightly power). also rates optimization crucially depend whether gradient noise sign-preserving not. instance rounding errors storing ﬂoats small precision deterministic rates exact gradient since rounding lower precision doesn’t signs. distribution query goal returning guess close possible. formal study classiﬁcation common study minimax rates regression function satisﬁes tsybakov’s noise margin condition exponent threshold diﬀerent versions boundary noise condition used regression density level-set estimation lead improvement minimax optimal rates here present version used bounding proof technique one-dimensional active threshold learning provide lower bounding proof technique d-dimensional stochastic convex optimization functions. particular showed minimax rate -dimensional active learning excess risk d-dimensional optimization function error scaled like either exponent exponent depending setting. importance connection cannot emphasized enough useful throughout paper. mentioned earlier require two-sided condition order prove risk upper bounds. similar note uniformly convex functions assume local k-strong smoothness condition around directional minima basic argument relating optimization active learning made context stochastic ﬁrst order oracles noise distribution unbiased grows linearly around zero mean i.e. clear convex; gradient ∇fxj increasing function switches signs min{α|x+αej∈s} equivalently directional minimum think sign]j true label sign]j observed label ﬁnding hence satisﬁes exponent active learning algorithm used obtain point small point-error excess risk. note function error convex optimization bounded excess risk corresponding active learner using grid-based probabilistic variant binary search called algorithm approximately learn threshold eﬃciently active setting setting satisﬁes known analysis proof following lemma discussed detail theorem theorem appendix described connection exponents approximately optimize dimensional uniformly convex function known uniform convexity parameters hence algorithm used point function error searching point risk. this combined lemma yields following important result. describe algorithm active learning one-dimensional thresholds adaptive meaning achieve minimax optimal rate even parameters unknown. quite diﬀerent non-adaptive algorithm ﬂavour though regarded robust binary search procedure design proof inspired optimization procedure adaptive unknown parameters even though considers speciﬁc optimization algorithm observe algorithm adapts unknown parameters optimal convex optimization algorithm subroutine within epoch. similarly adaptive active learning algorithm epoch-based optimal passive learning subroutine epoch. note also developed adaptive algorithm based disagreement coeﬃcient vc-dimension arguments pool-based setting access large pool unlabeled data much complicated. consider passive learning procedure uniform distribution samples ball around arbitrary point radius known contain true threshold without knowledge steps point close true threshold probability least assume lies within region since interval least constant width take constant number iterations point within formal argue would overall risk goes zero like point cannot stay outside constant sized region width since would accumulate large constant risk issues hindering completion proof. ﬁrst even though start with might case x∗e∗ away since chopping radius half every epoch. interestingly lemma prove round last round would imply secondly might concerned round move away later epochs. however show since radii decreasing geometrically half every epoch cannot really wander away xe∗. give bound like proof. hold epoch distance ﬁrst point epoch ball radius around actually contains mathematically |xe− trivially satiﬁed assuming true epoch show show induction holds true epoch w.p. notice within search radius) completion round probability describe algorithm stochastic optimization k-uc lkss functions dimensions given access stochastic sign oracle black-box active learning algorithm adaptive scheme previous section subroutine. procedure well-known literature idea needs noisy gradient signs perform minimization optimally active learning line-search procedure novel best knowledge. idea simply perform random coordinate-wise descent approximate line search subroutine line search optimal active threshold learning algorithm used approach minimum function along chosen direction. number epochs number time steps epoch line search approximates well function error steps using active learning subroutine resulting function-error desired result. notice section didn’t need know simply randomized coordinate descent epochs steps subroutine active learning subroutine also adaptive appropriately calculated parameters. summary theorem given access noisy gradient sign information stochastic sign oracle randomized stochastic-sign coordinate descent minimize lkss functions minimax optimal convergence rate expected function error adaptive unknown convexity smoothness parameters. special case strongly convex strongly smooth functions minimized steps. practical concern implementing optimization algorithms machine precision number decimals real numbers stored. finite space limit accuracy every gradient stored much inaccuracies aﬀect ﬁnal convergence rate query complexity optimization aﬀected true gradients rounded decimal points? gradients randomly rounded might guess could easily achieve stochastic ﬁrst-order optimization rates. however results give surprising answer question similar argument reveals lkss functions algorithm achieves exponential rates. since rounding errors sign gradient even gradient rounded decimal points dropped much possible return single coordinate true signs still achieve exponentially fast convergence rate observed non-stochastic settings algorithm needs logarithmic number epochs epoch active learning approach directional minimum exponentially fast noiseless gradient signs using perfect binary search. fact algorithm natural generalization higher-dimensional binary search deterministic stochastic settings. assumption smoothness natural strongly convex functions assumption lkss might appear strong general. possible relax assumption require lkss exponent diﬀer exponent assume strong smoothness still yields consistency algorithm rate achieved worse. epoch based algorithms achieve minimax rates lipschitz assumptions access full-gradient stochastic ﬁrst order oracle hard prove rates coordinate descent procedure without smoothness assumptions. given target function accuracy instead query budget similar randomized coordinate descent procedure achieves minimax rate similar proof non-adaptive since presently don’t adaptive active learning procedure given know adaptive optimization procedure given recently analysed stochastic gradient descent averaging show smooth functions possible algorithm automatically adapt convexity strong convexity comparision show adapt unknown uniform convexity possible combine ideas paper universally adaptive algorithm convex degrees uniform convexity. would also interesting ideas extend connections convex optimization learning linear threshold functions. paper exploit recently discovered theoretical connections providing explicit algorithms take advantage them. show could lead cross-fertilization ﬁelds directions hope beginning ﬂourishing interaction insights lead many algorithms leverage theoretical relations innovative ways.", "year": 2015}