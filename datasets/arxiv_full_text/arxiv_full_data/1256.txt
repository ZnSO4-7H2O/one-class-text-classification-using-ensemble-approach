{"title": "Learning Local Receptive Fields and their Weight Sharing Scheme on  Graphs", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "We propose a simple and generic layer formulation that extends the properties of convolutional layers to any domain that can be described by a graph. Namely, we use the support of its adjacency matrix to design learnable weight sharing filters able to exploit the underlying structure of signals in the same fashion as for images. The proposed formulation makes it possible to learn the weights of the filter as well as a scheme that controls how they are shared across the graph. We perform validation experiments with image datasets and show that these filters offer performances comparable with convolutional ones.", "text": "abstract—we propose simple generic layer formulation extends properties convolutional layers domain described graph. namely support adjacency matrix design learnable weight sharing ﬁlters able exploit underlying structure signals fashion images. proposed formulation makes possible learn weights ﬁlter well scheme controls shared across graph. perform validation experiments image datasets show ﬁlters offer performances comparable convolutional ones. convolutional neural networks achieved state-of-the-art accuracy many supervised learning challenges ability absorb huge amounts data lesser overﬁtting deep learning models golden standard data available. cnns beneﬁt ability create stationary multiresolution low-level features data independently location training images. authors draw parallel features scattering transforms obviously cnns rely ability deﬁne convolution operator signals. images amounts learn local receptive ﬁelds convolved training images. considering images deﬁned grid graph point receptive ﬁelds vertices included neighbors generally neighborhood. reciprocally convolution requires neighborhoods vertices underlying graph operator able match speciﬁc neighbors distinct vertices together. instance performing convolution images requires knowledge coordinates pixels directly accessible considering grid graph paper interested demonstrating underlying graph nevertheless enough achieve comparable results. convolution signal formalized multiplication convolution matrix. case images small convolution kernels interesting note convolution matrix support lattice graph. using idea propose introduce type layer based graph connects neurons neighbors. moreover convolution matrices entirely determined single since weights appear one. imitate process introduce weight sharing learning procedure consists using limited pool weights obtained operator make section presents related work. section describes methodology links existing architectures. section contains experimental results. section conclusion. effectiveness cnns image datasets models proposed adapt kind data e.g. shapes manifolds molecular datasets graphs review done particular cnns also adapted graph signals convolution formalized spectral domain graph deﬁned laplacian approach improved localized fast approximated formulation used back vision breed isometry invariant representations non-spectral approaches feature correspondences input domain allow deﬁne weights tied across layer images manifolds. graphs graph signals correspondences doesn’t necessarily exist. example weights tied according power attached ordering nodes used embedding learned degrees nodes. choices arbitrary unsimilar done regular convolutions. contrary propose generic layer formulation allows also learn weights linearly distributed local receptive ﬁeld. model ﬁrst designed task graph signal classiﬁcation another common task problem node classiﬁcation models learning part structures also proposed moreover model strongly ressembles regular convolutions also ressemble variants group equivariant convolutions intuitively values weight kernel linearly distributed pairs neighbours respect values reason call scheme receptive graph. sense scheme tensor receptive graph adjacency matrix graph. example depicted figure figure depiction graph corresponding receptive graph propagation associated weight sharing scheme note vector slices along ﬁrst ranks determines much weight allocated edge linking vertex vertex alike convolution images extended thirdrank tensor include multiple input output channels worth mentioning implementation must memory efﬁcient take care possibly large sparse proposed formulation allows learn perform jointly. learning amounts learning weights regular cnns whereas learning amounts learning weights tied receptive ﬁelds. also experiment ﬁne-tuning step consists freezing last epochs. indeed weight sharing scheme decided directly underlying structure necessary train inspiration cnns propose constraints parameters namely impose along third dimension. therefore vectors third rank interpreted performing weighted average parameters test types initialization ﬁrst consists distributing one-hot-bit vectors along third rank. impose receptive ﬁeld particular one-hotbit vector distributed other. refer one-hot-bit initialization. second consists using uniform random distribution limits described weight kernels learned using optimization routine usually based gradient descent able approximate objective function. containing type layer called multi-layer perceptron case cnns layers particular form convolution ﬁlters. case convolutional operation also written product input signal matrix toeplitz matrix. previous works shown obtain best accuracy vision challenges usually better small kernels resulting sparse figure depicts convolutional layer. propose introduce another type layer call receptive graph layer. based adjacency matrix aims extending principle convolutional layers domain described using graph. well ﬁtted signals learned sense describes underlying graph structure input features. deﬁne receptive graph layer associated using product third rank tensor weight kernel tensor would one-rank containing weights layer shape shape adjacency matrix shape ﬁrst ranks support must exceed obtain convolutional layer choose size kernel. would one-hot-bit encoded along third rank circulant along ﬁrst ranks. stride obtained removing corresponding rows. case similar obtained considering convolutional layers noticeable differences force weight allocate neighbor along third rank necessarily circulant along ﬁrst ranks. layer propagation ultimately handled tensor product. output determined weight sharing make sense must over-parameterize call number non-zeros shape former assumption requires equivalently implies number weights ﬁlter must lower total number ﬁlters number edges note without constraint support must exceed proposed formulation could also applied structure learning input features space operations along third rank might exploitable e.g. dropping connections training discovering sort structural correlations. however even done image datasets wouldn’t sparse would lead memory issues higher dimensions. didn’t include avenues scope paper. ﬁrst present experiments mnist contains classes gray levels images examples training testing. also experiments scrambled version hide underlying structure done previous work present experiments cifar contains classes images examples training testing. receptive graph layers wider convolutional counterparts experiments done shallow networks introductory paper. also note require times multiply models composed single receptive graph layer made feature maps without pooling followed fully connected layer neurons terminated softmax layer neurons. rectiﬁed linear units used activations dropout applied fully-connected layer. input layers regularized factor weight optimize adam epochs ﬁne-tune additional epochs. consider grid graph connects pixel nearest neighbors also square graph cube graph powers one-hot-bit initialization. test model setups either ordering node unknown one-hot-bit vectors distributed randomly modiﬁed upon training either ordering node known one-hot-bit vectors distributed circulant fashion third rank freezed state. number nearest neighbors dimension third rank also compare convolutional layer size thus containing many weights cube grid graph. table summarizes obtained results. ordering unknown ﬁrst result given known second result parenthesis. observe even without knowledge underlying euclidean structure receptive grid graph layers obtain comparable performances convolutional ones ordering known match convolutions. also noticed training even though one-hot-bit vectors used initialization changed ﬂoating point values signiﬁcant dimension always same. suggests room improve initialization optimization. figure plot test error rate various normalizations using square grid graph function number epochs training. observe little inﬂuence performance sometimes improve bit. thus optional hyperparameters. neurons. compare different graph supports obtained using underlying graph regular convolution support square grid graph. optimization done stochastic gradient descent epochs freezed last ones. circulant one-hot-bit intialization used. weak classiﬁers cifar enough analyse usefulness proposed layer. exploring deeper architectures left work. experiments times each. means standard deviations accuracies reported table iii. means parameters forced positive norm means norm vector third dimension forced both means constraints applied none means none used. introduced class layers deep neural networks consists using support graph operator linearly distributing pool weights deﬁned edges. linear distribution learned jointly pool weights. thanks structural dependencies showed possible share weights fashion similar convolutional neural networks performed experiments vision datasets receptive graph layer obtains similar performance convolutional ones even underlying image structure hidden. believe work proposed layer could fully extend performance cnns many domains described graph. future works also include exploration advanced graph inference techniques. example using gradient descent supervised task hand also notice case amounts select receptive ﬁelds breeding another avenue figure evolution test error rate learning mnist using square grid graph various normalizations function epoch training. legend reads means normalization weights used means parameters forced positive norm means norm vector third dimension forced number remaining edges corresponds certain density also infer graph based nearest neighbors inverse values covariance matrix latter using prior signal underlying structure. pixels input images shufﬂed re-ordering pixels used every image. dimension third rank chosen equal weights initialized random uniformly receptive graph layers also compared models obtained replacing ﬁrst layer fully connected convolutional one. architecture used previous section. results reported table cifar made experiments shallow architectures replaced convolutions receptive graphs. report results variant alexnet using little distortion input borrowed tutorial tensorﬂow composed convolutional layers feature maps pooling local response normalization followed fully connected layers sainath a.-r. mohamed kingsbury ramabhadran deep convolutional neural networks lvcsr acoustics speech signal processing ieee international conference duvenaud maclaurin iparraguirre bombarell hirzel aspuru-guzik adams convolutional networks graphs learning molecular ﬁngerprints advances neural information processing systems niepert ahmed kutzkov learning convolutional neural networks graphs proceedings international conference international conference machine learning shuman narang frossard ortega vandergheynst emerging ﬁeld signal processing graphs extending high-dimensional data analysis networks irregular domains ieee signal processing magazine vol. defferrard bresson vandergheynst convolutional neural networks graphs fast localized spectral ﬁltering advances neural information processing systems richardson discovery algorithm directed cyclic graphs proceedings twelfth international conference uncertainty artiﬁcial intelligence. morgan kaufmann publishers inc. t.-y. kwok d.-y. yeung constructive algorithms structure learning feedforward neural networks regression problems ieee transactions neural networks vol. abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp isard jozefowicz kaiser kudlur levenberg mané monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan viégas vinyals warden wattenberg wicke zheng tensorflow large-scale machine learning heterogeneous systems available http//tensorﬂow.org/", "year": 2017}