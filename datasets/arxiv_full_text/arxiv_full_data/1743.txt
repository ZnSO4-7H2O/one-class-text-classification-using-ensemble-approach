{"title": "Necessary and Sufficient Conditions and a Provably Efficient Algorithm  for Separable Topic Discovery", "tag": ["cs.LG", "cs.CL", "cs.IR", "stat.ML"], "abstract": "We develop necessary and sufficient conditions and a novel provably consistent and efficient algorithm for discovering topics (latent factors) from observations (documents) that are realized from a probabilistic mixture of shared latent factors that have certain properties. Our focus is on the class of topic models in which each shared latent factor contains a novel word that is unique to that factor, a property that has come to be known as separability. Our algorithm is based on the key insight that the novel words correspond to the extreme points of the convex hull formed by the row-vectors of a suitably normalized word co-occurrence matrix. We leverage this geometric insight to establish polynomial computation and sample complexity bounds based on a few isotropic random projections of the rows of the normalized word co-occurrence matrix. Our proposed random-projections-based algorithm is naturally amenable to an efficient distributed implementation and is attractive for modern web-scale distributed data mining applications.", "text": "abstract—we develop necessary sufﬁcient conditions novel provably consistent efﬁcient algorithm discovering topics observations realized probabilistic mixture shared latent factors certain properties. focus class topic models shared latent factor contains novel word unique factor property come known separability. algorithm based insight novel words correspond extreme points convex hull formed row-vectors suitably normalized word co-occurrence matrix. leverage geometric insight establish polynomial computation sample complexity bounds based isotropic random projections rows normalized word co-occurrence matrix. proposed randomprojections-based algorithm naturally amenable efﬁcient distributed implementation attractive modern webscale distributed data mining applications. associated algorithms discovering topical structure shared large corpus documents. important organizing searching making sense large text corpus paper describe novel geometric approach provable statistical computational efﬁciency guarantees learning latent topics document collection. work culmination series recent publications certain structure-leveraging methods topic modeling provable theoretical guarantees consider corpus documents indexed composed words ﬁxed vocabulary size distinct words vocabulary indexed document viewed unordered words represented empirical word-counts vector number times word appears document entire document corpus represented matrix vocabulary. topic model posits existence latent topics shared among documents corpus. topics collectively represented columns column-stochastic topic matrix document conceptually modeled generated independently documents two-step process ﬁrst draw document-speciﬁc distribution topics prior distribution probability simplex hyper-parameters draw words according document-speciﬁc word distribution βkθkm convex combination latent topics. goal estimate matrix empirical observations appreciate difﬁculty problem consider typical benchmark dataset news article collection york times experiments. dataset suitable pre-processing average thus sparse large. typically min. estimation problem topic modeling extensively studied. prevailing approach compute map/ml estimate true posterior given however intractable compute associated estimation problems fact np-hard general case necessitates sub-optimal methods based approximations heuristics variationalbayes mcmc produce impressive empirical results many real-world datasets guarantees asymptotic consistency efﬁciency approaches either weak non-existent. makes difﬁcult evaluate model ﬁdelity failure produce satisfactory results datasets could approximations heuristics model mis-speciﬁcation fundamental. furthermore sub-optimal approaches computationally intensive large text corpora overcome hardness topic estimation problem full generality approach emerged learn topic model imposing additional structure model parameters paper focuses structural property topic matrix called topic separability wherein every latent topic contains least word novel i.e. word unique topic absent topics. essence property support latent topic matrix topic separability property motivated fact many real-world datasets empirical topic estimates produced popular variational-bayes gibbs sampling approaches approximately separable moreover recently shown separability property approximately satisﬁed high probability dimension vocabulary scales sufﬁciently faster number topics realization approach exploits following geometric implication separability structure. associate word vocabulary row-vector suitably normalized empirical word co-occurrence matrix novel words correspond extreme points convex hull formed row-vectors words. leverage geometric insight develop provably consistent efﬁcient algorithm. informally speaking establish following result theorem topic matrix separable mixing weights satisfy minimum information-theoretically necessary technical condition proposed algorithm runs polynomial time estimates topic matrix consistently held ﬁxed. element-wise error probability least poly /ǫ). asymptotic setting held ﬁxed motivated text corpora number words single document small number documents large. note algorithm applied family topic models whose topic mixing weights prior satisﬁes minimum information-theoretically necessary technical condition. contrast standard bayesian approaches variational-bayes mcmc need hand-designed separately speciﬁc topic mixing weights prior. highlight approach identify novel words extreme points appropriately deﬁned random projections. speciﬁcally project row-vector word appropriately normalized word co-occurrence matrix along independent isotropically distributed random directions. fraction times word attains maximum value along random direction measure degree robustness extreme point. process random projections followed counting number times word maximizer efﬁciently computed robust perturbations induced sampling noise associated small number words document addition computationally efﬁcient turns random projections based approach requires minimum information-theoretically necessary technical conditions topic prior asymptotic consistency naturally parallelized distributed. consequence provably achieve efﬁciency guarantees centralized method requiring insigniﬁcant communication distributed document collections attractive webscale topic modeling large distributed text corpora. another advance paper identiﬁcation necessary sufﬁcient conditions mixing weights consistent separable topic estimation. previous work showed simplicial condition mixing weights necessary sufﬁcient consistently detecting novel words paper complete characterization showing afﬁne independence condition mixing weights necessary sufﬁcient consistently estimating separable topic matrix. conditions satisﬁed practical choices topic priors dirichlet distribution necessary conditions information-theoretic algorithm-independent i.e. irrespective speciﬁc statistics observations algorithms used. provable statistical computational efﬁciency guarantees proposed algorithm hold true necessary sufﬁcient conditions. rest paper organized follows. review related work topic modeling well separability property various domains sec. introduce separability property simplicial afﬁne independence conditions mixing weights extreme point geometry motivates approach sec. iii. discuss solid angle used identify robust extreme points deal ﬁnite number samples sec. describe overall algorithm sketch analysis sec. demonstrate performance approach sec. various synthetic real-world examples. proofs results appear appendices. idea modeling text documents mixtures semantic topics ﬁrst proposed mixing weights assumed deterministic. latent dirichlet allocation seminal work extended probabilistic setting modeling topic mixing weights using dirichlet priors. setting extended include topic priors log-normal prior correlated topic model models derivatives successful wide range problems terms achieving good empirical performance prevailing approaches estimation inference problems topic modeling based estimation however computation posterior distributions conditioned observations intractable moreover estimation objective non-convex shown np-hard therefore various approximation heuristic strategies employed. approaches fall major categories sampling approaches optimization approaches. sampling approaches based markov chain monte carlo algorithms seek generate independent samples markov chain carefully designed ensure sample distribution converges true posterior optimization approaches typically based socalled variational-bayes methods. methods optimize parameters simpler parametric distribution close true posterior terms divergence expectation-maximization-type algorithms typically used methods. practice variational-bayes mcmc algorithms similar performance variationalbayes typically faster mcmc nonnegative matrix factorization alternative approach topic estimation. nmf-based methods exploit fact topic matrix mixing weights decompose empirical nonnegative attempt measure closeness regularization term enforces desirable properties e.g. sparsity mixing weights. problem however also known non-convex np-hard general. suboptimal strategies alternating minimization greedy gradient descent heuristics used practice contrast approaches approach recently emerged based imposing additional structure model parameters approaches show topic discovery problem lends provably consistent polynomial-time solutions making assumptions structure topic matrix distribution mixing weights. category approaches methods based tensor decomposition moments algorithm uses second order empirical moments shown asymptotically consistent topic matrix special sparsity structure. algorithm uses third order tensor observations. however strongly tied speciﬁc structure dirichlet prior mixing weights requires knowledge concentration parameters dirichlet distribution furthermore practice approaches computationally intensive require initial coarse dimensionality reduction gradient descent speedups acceleration process large-scale text corpora like dataset work falls family approaches exploit separability property geometric implications asymptotically consistent polynomial-time topic estimation algorithm ﬁrst proposed however method requires solving linear programs variables computationally impractical. subsequent work improved computational efﬁciency theoretical guarantees asymptotic consistency unclear. algorithms practical provably consistent. requires stronger slightly different technical condition topic mixing weights speciﬁcally imposes full-rank condition second-order correlation matrix mixing weights proposes gram-schmidt procedure identify extreme points. similarly imposes diagonal-dominance condition second-order correlation matrix proposes random projections based approach. approaches tied speciﬁc conditions imposed fail detect novel words estimate topics imposed conditions fail hold examples random projections based algorithm proposed practical provably consistent. note separability property exploited recent work well singular value decomposition based approach proposed topic estimation. shown standard variationalbayes approximation asymptotically consistent separable. however additional constraints proposed essentially boil requirement document contain predominantly topic. addition assuming existence pure documents also requires strict initialization. thus unclear achieved using observations separability property re-discovered exploited literature across number different ﬁelds found application several problems. best knowledge concept ﬁrst introduced pure pixel index assumption hyperspectral image unmixing problem work assumes existence pixels hyper-spectral image containing predominantly species. separability also studied literature context ensuring uniqueness subsequent work development algorithms exploit separability uniqueness correctness results line work primarily focused noiseless case. ﬁnally note separability also recently exploited problem learning multiple ranking preferences pairwise comparisons personal recommendation systems information retrieval provably consistent efﬁcient estimation algorithms. section unravel ideas motivate algorithmic approach focusing ideal case sampling-noise i.e. document inﬁnitely long next section turn ﬁnite case. recall denote topic matrix empirical word counts/frequency matrix respectively. also denote respectively number documents vocabulary size number topics. convenience group document-speciﬁc mixing weights document-speciﬁc distributions am’s ative procedure describes topic model implies ideal case considered section empirical word frequency matrix notation vector without speciﬁcation denote column-vector all-ones column vector suitable size i-th column vector j-th vector matrix suitably row-normalized version nonnegative matrix also complementing work identiﬁes necessary sufﬁcient conditions consistent detection novel words paper identify necessary sufﬁcient conditions consistent estimation separable topic matrix. necessity results information-theoretic algorithm-independent nature meaning independent speciﬁc statistics observations algorithms used. novel words topics identiﬁed permutation accounted results. expectation vector correlation matrix weight prior without loss generality assume elements strictly positive since otherwise topic appear corpus. quantity diag−r diag− viewed normalized second-moment matrix weight vector. following conditions central results. condition matrix γs-simplicial row-vector euclidean distance least convex hull remaining row-vectors. topic model γs-simplicial normalized second-moment γs-simplicial. here called simplicial afﬁneindependence constants respectively. condition numbers measure degree conditions respectively associated hold. larger condition numbers easier estimate topic matrix. going forward matrix simplicial γs-simplicial simplicial condition ﬁrst proposed investigated paper ﬁrst identify afﬁneindependence necessary sufﬁcient consistent separable topic estimation. discuss geometric implications point afﬁne-independence stronger simplicial condition simplicial condition necessary sufﬁcient novel word detection ﬁrst focus detecting novel words distinct topics. task simplicial condition algorithm-independent informationtheoretic necessary condition. formally fig. example separable topic matrix underlying geometric structure space normalized document distribution matrix note word ordering visualization bearing separability. solid circles represent rows empty circles represent rows ﬁnite projections ¯aw’s along random isotropic direction used identify novel words. separable topics. words novel topic words topic word topic words appear multiple topics called nonnovel words identifying novel words distinct topics step proposed approach. note separability empirically observed approximately satisﬁed topic estimates produced variational-bayes mcmc based algorithms fundamentally recent work shown topic separability inevitable consequence relatively small number topics large vocabulary particular columns independently sampled dirichlet distribution -dimensional probability simplex) resulting topic matrix separable probability tending scales inﬁnity sufﬁciently faster dirichlet prior widelyused smoothed settings topic modeling discuss next sec. iii-c topic separability property combined additional conditions secondorder statistics mixing weights leads intuitively appealing geometric property exploited develop provably consistent efﬁcient topic estimation algorithm. topic separability alone guarantee unique consistent observations illustrated fig. therefore effort develop provably consistent topic estimation algorithms number different conditions imposed topic mixing weights literature fig. example showing topic separability alone guarantee unique solution problem estimating here document distribution matrix consistent different topic matrices separable. simplicial construct distinct separable topic matrices different sets novel words induce distribution empirical observations geometrically simplicial condition guarantees rows extreme points convex hull form. therefore simplicial exist least redundant topic convex combination topics. turns simplicial also sufﬁcient consistent novel word detection. direct consequence consistency guarantees approach outlined theorem afﬁne-independence necessary sufﬁcient separable topic estimation focus estimating separable topic matrix stronger requirement detecting novel words. naturally requires conditions stronger simplicial condition. afﬁneindependence turns algorithm-independent information-theoretic necessary condition. formally similar lemma afﬁne-independent construct distinct separable topic matrices induce distribution observation makes consistent topic estimation impossible. geometrically every point convex decomposed uniquely convex combination extreme points extreme points afﬁne-independent. hence afﬁne-independent non-novel word assigned different subsets topics. sufﬁciency afﬁne-independence condition separable topic estimation direct consequence consistency guarantees approach theorems note since afﬁne-independence implies simplicial condition afﬁne-independence sufﬁcient novel word detection well. connection conditions mixing weights brieﬂy discuss conditions mixing weights exploited literature. assumed full-rank assumed diagonaldominant i.e. ¯rii− ¯rij sufﬁcient conditions detecting novel words distinct topics. constants condition numbers measure degree full-rank diagonaldominance conditions hold respectively. counterparts like them larger easier consistently detect novel words estimate relationships conditions summarized proposition illustrated fig. fig. relationships simplicial afﬁne-independence full rank diagonal dominance conditions normalized second-moment proposition normalized second-moment topic prior. then note earlier work provable guarantees estimating separable topic matrix require full rank. analysis paper provably extends guarantees afﬁne-independence condition. demonstrate geometric implications topic separability combined simplicial/ afﬁne-independence condition topic mixing weights. highlight ideas focus ideal case then empirical document word-frequency matrix novel words extreme points expose underlying geometry normalize rows obtain row-stochastic matrices diag−a diag−θ. since diag−β diag row-normalized topic matrix row-stochastic separable sets novel words others next separability property ensures novel word topic ¯βwk ¯θk. revisiting example fig. rows correspond novel words e.g. words row-vectors together form convex hull extreme points. example however non-novel word lives inside convex hull rows fig. corresponds non-novel word inside convex hull summary novel words detected extreme points row-vectors also multiple novel words topic correspond extreme point formally lemma simplicial separable. then probability least exp−exp i-th extreme point convex hull spanned rows word novel. constant max. model parameters deﬁned follows. amin minimum element λmax maximum singular-value identifying novel words help estimate recall row-vectors corresponding novel words coincide rows thus known novel word topic known. also words ¯θk. thus uniquely decompose convex combination extreme points coefﬁcients decomposition give w-th unique decomposition exists high probability afﬁne-independent found solving constrained linear regression problem. gives finally noting diag diag recovered suitably renormalizing rows columns lemma novel word distinct topic given. afﬁne-independent then probability least recovered uniquely constrained linear regression. constant max. model parameters deﬁned follows. amin minimum element λmax maximum singular-value lemmas together provide geometric approach learning find extreme points rows cluster rows correspond extreme point group. express remaining rows convex combinations distinct extreme points. renormalize obtain detecting extreme points using random projections contribution approach efﬁcient random projections based algorithm detect novel words extreme points. idea illustrated fig. project every point convex body onto isotropically distributed random direction maximum projection value must correspond extreme points probability hand non-novel words maximum projection value along random direction. therefore repeatedly projecting points onto isotropically distributed random directions detect extreme points high probability number random directions increase. explicit bound number projections needed appears theorem finite practice geometric intuition discussed based row-vectors matrix row-normalized empirical word-frequencies documents. ﬁnite large wellapproximated thanks large numbers. however real-word text corpora therefore rowvectors signiﬁcantly perturbed away ideal rows illustrated fig. discuss effect small address accompanying issues next. extreme point geometry sketched sec. iii-c perturbed small highlighted fig. speciﬁcally rows empirical word-frequency matrix deviate rows creates several problems points convex hull corresponding non-novel words also become outlier extreme points extreme points correspond novel words longer extreme multiple novel words corresponding extreme point become multiple distinct extreme points unfortunately issues vanish increases ﬁxed regime captures characteristics typical benchmark datasets dimensionality rows also increases. averaging effect smoothen-out sampling noise. solution seek representation statistic smoothen sampling noise individual documents also preserve extreme point geometry induced separability afﬁne independence conditions. addition also develop extreme point robustness measure naturally arises within random projections based framework. robustness measure used detect exclude outlier extreme points. construct suitably normalized word co-occurrence matrix representation. co-occurrence matrix converges almost surely ideal statistic ﬁxed simultaneously asymptotic limit original novel words continue correspond extreme points representation overall extreme point geometry preserved. representation constructed follows. first randomly divide words document equal-sized independent halves obtain empirical word-frequency matrices containing words. normalize rows like sec. iii-c note random projection based approach explicitly constructed multiplying instead keep exploit sparsity properties reduce computational complexity subsequent processing. asymptotic consistency ﬁrst nice property word co-occurrence representation asymptotic consistency ﬁxed. number documents co-occurrence matrix size formally normalized second-moment topic priors deﬁned sec. row-normalized version make note abuse notion deﬁned sec. iii-c. shown deﬁned lemma limit deﬁned sec. iii-c convergence result lemma shows word co-occurrence representation consistently estimated large typical benchmark datasets. novel words extreme points another reason using word co-occurrence representation preserves extreme point geometry. consider ideal word cooccurrence matrix straightforward show separable simplicial also simplicial. using facts possible establish following counterpart lemma still vanishes however possibility observing outlier extreme points non-novel word lies facet convex hull rows next introduce extreme point robustness measure based certain solid angle naturally arises random projections based approach discuss used detect distinguish true novel words outlier extreme points. necessary applying approach realworld data also establish ﬁnite sample complexity bounds. intuitively robustness measure able distinguish true extreme points outlier extreme points towards goal leverage geometric quantity namely normalized solid angle subtended convex hull rows extreme point. visualize quantity revisit running example fig. indicate solid angles attached extreme point shaded regions. turns geometric quantity naturally arises context random projections discussed earlier. connection fig. observe shaded region attached extreme point another words novel words correspond extreme points row-vectors ideal word co-occurrence matrix consider example fig. based topic matrix fig. here distinct extreme points row-vectors corresponds non-novel word inside convex hull. novel words detected extreme points follow procedure lemma express unique convex combination extreme rows equivalently rows weights convex combination ¯βwk’s. fig. example separable topic matrix underlying geometric structure word co-occurrence representation. note word ordering visualization bearing separability. example topic matrix fig. solid circles represent rows shaded regions depict solid angles subtended extreme point. isotropic random directions along extreme point maximum projection value. used estimate solid angles. directions drawn isotropic distribution algorithmically approximate solid angle i-th word ﬁrst projecting row-vectors onto isotropic random directions calculating fraction times row-vector achieves maximum projection value. procedure taking random projections followed calculating number times word maximizer provides consistent estimate solid angle number projections increases. high-level idea simple increases empirical average converges corresponding expectation. random projections based approach also computationally efﬁcient following reasons. first enables avoid explicit construction dimensional nonzero entries. hence calculated using sparse matrix-vector multiplications. second turns number projections needed guarantee consistency small. fact theorem provide sufﬁcient upper bound polynomial function model parameters probability algorithm fails detect distinct novel words. parallelization distributed online settings another advantage proposed random projections based approach parallelized naturally amenable online distributed settings. based following observation projection additive structure coincides precisely directions along projection larger point example fig. projection along larger point. thus solid angle attached point formally deﬁned directions ehei di}. nonempty extreme points. solid angle deﬁned set. derive scalar robustness measure idea random projections adopt statistical perspective deﬁne normalized solid angle point probability point maximum projection value along isotropically distributed random direction. concretely i-th word normalized solid angle deﬁned drawn isotropic distribution spherical gaussian. condition introduced exclude multiple novel words topic correspond extreme point. instance fig. hence excluded. make practical handle ﬁnite sample estimation noise replace condition condition suitably deﬁned illustrated fig. solid angle extreme points strictly positive given γs-simplicial. hand non-novel corresponding solid angle zero deﬁnition. hence extreme point geometry lemma re-expressed term solid angles follows enough then solid angles outlier extreme points close bounded away zero true extreme points. hope correctly identify extreme points rank-ordering empirical solid angle estimates selecting distinct row-vectors largest solid angles. forms basis proposed algorithm. problem boils efﬁciently estimating solid angles establishing asymptotic convergence estimates next discuss random projections used achieve goals. directions across servers aggregate projection values. communication cost partial projection values therefore insigniﬁcant scale number observations increases. additive independent structure guarantees statistical efﬁciency variations centralized batch implementation. rest paper focus centralized version. outline overall approach overall approach summarized follows. estimate empirical solid angles using isotropic random directions select words distinct word co-occurrence patterns largest empirical solid angles. estimate topic matrix using constrained linear regression lemma discuss details overall approach next section establish guarantees computational statistical efﬁciency. algorithm describes main steps overall random projectons based algorithm call main steps novel word detection topic matrix estimation outlined algorithms respectively. algorithm outlines random projection rank-ordering steps. algorithm describes constrained linear regression renormalization steps combined way. efﬁciency achieved exploiting sparsity property small number novel words typical vocabulary. detailed analysis computational complexity presented appendix. point order upper bound computation time linear regression algorithm used matrix inversions words vocabulary. practice gradient descent implementation used constrained linear regression much efﬁcient. also note optimization problems decoupled given detected novel words. therefore parallelized straightforward manner asymptotic consistency statistical efﬁciency summarize asymptotic consistency sample complexity bounds algorithm analysis combination consistency novel word detection step topic estimation step state results steps. first detecting novel words distinct topics following result literature. ground truth available reconstruction error ground truth topics estimates proper topic alignment. real-world text corpus sec. vi-b report heldprobability standard measure used topic modeling literature. also qualitatively compare topics extracted different approaches using probable words topic. order validate proposed algorithm generate semi-synthetic text corpora sampling synthetic realistic ground truth topic model. ensure semi-synthetic data similar real-world data terms dimensionality sparsity characteristics following generative procedure adapted ﬁrst train model realworld dataset using standard gibbs sampling method default parameters obtain topic matrix size real-world dataset generate synthetic data derived york times articles dataset original vocabulary ﬁrst pruned based document frequencies. speciﬁcally standard practice words appear documents retained. thereafter standard practice words so-called stop-word list deleted recommended steps average document length generate semi-synthetic datasets various values ﬁxing using dirichlet topic prior. suggested used symmetric hyper-parameters dirichlet topic prior. topic matrix separable. enforce separability create separable dimensional topic matrix βsep inserting synthetic novel words suitable probabilities topic. speciﬁcally βsep constructed transforming follows. first synthetic novel word βsep value sole nonzero entry probability probable word topic novel word. resulting dimensional nonnegative matrix renormalized columnwise make column-stochastic. finally generate semisynthetic datasets various values ﬁxing using βsep symmetric dirichlet topic prior used proposed random projections based algorithm call compare provably efﬁcient algorithm recoverl standard gibbs sampling based algorithm order measure performance different algorithms experiments based semi-synthetic data compute algorithm fails probability model parameters deﬁned follows. min{ γ/λmax λmax maximum eigenvalue maxj∈ck ¯βjk non-novel words. finally minimum solid angle extreme points convex hull rows detailed proof presented appendix. results provide sufﬁcient ﬁnite sample complexity bound novel word detection. bound polynomial respect model parameters. number projections impacts computational complexity scales log/q sufﬁcient bound upper bounded practice found setting good choice note result theorem requires simplicial condition minimum condition required consistent novel word detection theorem holds true topic prior satisﬁes stronger conditions afﬁne-independence. also point proof paper holds isotropic distribution random projection directions previous result however applies speciﬁc isotopic distributions spherical gaussian uniform distribution unit ball. practice spherical gaussian since sampling prior simple requires time generating random direction. next given successful detection novel words topics following result accurate estimation separable topic matrix theorem topic matrix separable γaafﬁne-independent. given successful detection novel words distinct topics output algorithm element-wise speciﬁβ cally note sufﬁcient sample complexity bound polynomial terms model parameters. require afﬁne-independent. combining theorem theorem gives consistency sample complexity bounds overall approach algorithm novel words topic unlikely identical sufﬁciently large likely close other. thus threshold algorithm determines size neighborhood clustering novel words belonging topic made sufﬁciently small neighborhood novel words belonging topic. modiﬁcations discussed above number distinct neighborhoods suitably nonzero size among words whose solid angle estimates larger threshold provide estimate values should principle decrease zero increases inﬁnity. leaving task unraveling dependence future work provide brief empirical validation semi-syn+novel semi-syn datasets. reconstruction error essentially converged consider different choices threshold algorithm line code inserted lines input hyperparameter actual number estimated topics. interpreted specifying upper bound number topics. value algorithm terminates provides estimate number topics. figure illustrates solid angles words sorted descending order decay different choices used detect novel words estimate value note semi-synthetic datasets wide range values modiﬁed algorithm correctly estimates value large many interior points would declared novel words multiple ideal novel words would grouped cluster resulting. causes underestimated describe results actual real-world dataset used sec. vi-a construct semisynthetic datasets. since ground truth topics unavailable measure performance using so-called predictive heldlog-probability. standard measure typically used evaluate well learned topic model real-world data. calculate three topic estimation methods recoverl ﬁrst randomly select documents test goodness remaining documents results semi-syn+novel semi-syn summarized fig. three algorithms various choices number documents note ﬁgures norm error normalized number topics fig. norm error estimating topic matrix various semi-syn+novel nyt; semi-syn nyt. proposed algorithm recoverl provably efﬁcient algorithm gibbs gibbs sampling approximation algorithm fig. shows separability condition strictly satisﬁed reconstruction error converges becomes large outperforms approximation-based gibbs. separability condition strictly satisﬁed reconstruction error comparable gibbs solid angle model selection proposed algorithm number topics needs speciﬁed. unavailable needs estimated data. although focus work algorithm identiﬁes novel words sorting clustering estimated solid angles words suitably modiﬁed estimate novel words topic identical i.e. distance rows zero equivalently within neighborhood size zero other. thus number distinct neighborhoods size zero among nonzero solid angle words equals fig. solid-angles words semi-syn+sep dataset words semi-syn dataset estimated algorithm line code inserted lines values algorithm terminates indicated respectively position vertical dashed line rectangular next different since exact evaluation predictive log-likelihood intractable general calculate using mcmc based approximation proposed standard approximation tool sec. vi-a. report heldlog probability normalized total number words test documents averaged across training/testing splits. results summarized table shown table gibbs best descriptive power documents. recoverl similar somewhat lower values gibbs. attributed missing novel words appear test crucial success recoverl speciﬁcally real-world examples model-mismatch result data likelihoods recoverl suffer. finally qualitatively access topics produced algorithm. show example topics extracted trained entire dataset documents table topic frequent words listed. seen estimated topics form recognizable themes assigned meaningful labels. full list topics estimated dataset found paper proposed provably consistent efﬁcient algorithm topic discovery. considered natural structural property topic separability topic matrix exploited geometric implications. resolved necessary sufﬁcient conditions guarantee consistent novel words detection well separable topic estimation. proposed random projections based algorithm provably polynomial statistical computational complexity also state-of-the-art performance semisynthetic real-world datasets. focused standard centralized batch implementation paper turns random projections based scheme naturally amenable efﬁcient distributed implementation interest documents stored network distributed servers. isotropic projection directions precomputed shared across document servers counts projections co-occurrence matrix computations additive structure allows partial computations performed document server locally aggregated fusion center small communication cost. turns distributed implementation provably match polynomial computational statistical efﬁciency guarantees centralized counterpart. consequence provides provably efﬁcient alternative distributed topic estimation problem tackled using variations mcmc variational-bayes literature appealing modern web-scale databases e.g. generated twitter streaming. comprehensive theoretical empirical investigation distributed variation algorithm found separability general measures deﬁned studied notion separability topic matrix ﬁnite collection probability distributions ﬁnite turns extend notion separability ﬁnite collection measures measurable space. necessitates making small technical modiﬁcation deﬁnition separability accommodate possibility novel subsets zero measure. also show generalized deﬁnition separability equivalent so-called irreducibility property ﬁnite collection measures recently studied context mixture models establish conditions identiﬁability mixing components separability requires measure exists sequence measurable sets nonzero measure respect ratios vanish asymptotically. intuitively means measure exists sequence nonzero-measure measurable subsets asymptotically novel measure. ﬁnite topic modeling reduces existence novel words deﬁnition simply sets novel words topic separability property deﬁned equivalent socalled irreducibility property. informally collection measures irreducible nonnegative linear combinations produce measure. formally proof appears appendix topic models like discussed paper belong larger family mixed membership latent variable models successfully employed variety problems include text analysis genetic analysis network community detection ranking preference discovery. structure-leveraging approach proposed paper potentially extended larger family models. initial steps direction rank preference data explored finally entire paper topic matrix assumed separable. exact separability idealization shown approximate separability theoretically inevitable practically encountered extending results work approximately separable topic matrices interesting direction future work. steps direction explored context learning mixed membership mallows models rankings. article based upon work supported u.s. afosr award number fa--- u.s. award numbers views conclusions contained article authors interpreted necessarily representing ofﬁcial policies either expressed implied agencies. proof lemma proof. proof contradiction. show non-simplicial construct topic matrices whose sets novel words identical distribution models. difference constructed result column permutation. imply impossibility consistent novel word detection. suppose non-simplicial. assume withloss generality ﬁrst within conj= hull remaining rows denotes j-th convex combination weights. compactly recalling construct separable topic matrices follows. ﬁrst second ﬁrst second valid separable topic matrix. remaining rows choose small enough ensure element strictly less ensure column-stochastic construct topic matrices follows. ﬁrst second ﬁrst second valid topic matrix assume separable. remaining rows choose small enough ensure element strictly less ensure column-stochastic therefore valid topic matrices. note supports disjoint non-empty. appear distinct topics. proposition proposition summarizes relationships full-rank afﬁne-independence simplicial diagonal-dominance conditions. consider pairwise implication separately. proof. rayleigh-quotient characterization minimum eigenvalue symmetric positive-deﬁnite matrix gives minλ= kλ⊤rk/kλk therefore minλ=⊤λ= kλ⊤rk/kλk construct examples contradict reverse implication convex combination weights lower ¯rj| bounded therefore least γd-simplicial. straightforward construct examples contradict reverse implication next prove γs-simplicial constant then random matrix also simplicial high probability i.e. all-zero high probability. another words need show maximum absolute value entries strictly positive. noting m-th entry summarize probability simplicial exp. converges exponentially fast therefore high probability row-vectors extreme points convex hull form concludes proof. proof lemma proof. ﬁrst show afﬁne-independent also afﬁne-independent high probability i.e. high probability. proof similar lemma ﬁrst re-write m-th entry turn prove lemma lemma detecting distinct novel words topics equivalent knowing ¯θk. follows ¯βwk optimal solution following constrained optimization problem finally check renormalization steps. recall since diag diag diag directly obtained observations. ﬁrst renormalize rows removing diag simply column renormalization operation necessary know exact value diag. solving constrained linear regression followed suitable renormalization obtain unique solution ground truth topic matrix. concludes proof lemma lemma establishes second order co-occurrence estimator ﬁrst provide generic method establish explicit convergence bound function random variables apply establish lemma obtained ﬁrst splitting user’s comparisons independent halves rescaling rows make row-stochastic hence diag−x. also recall diag−β diag diag−r diag− stochastic. therefore ﬁrst vector least distant away convex hull remaining rows. similarly least distant away convex hull remaining rows hence least simplicial. rest proof exactly lemma note novel words topics detection corresponding columns linear regression. formally matrix formed columns correspond distinct novel words. then rest proof lemma show novel word need show topic sampled isotropic distribution exist directions nonzero probability first check deﬁnition least γs-simplicial projection along direction ﬁrst calculate projection values ¯x⊤dr maximizer index corresponding ˆji∗ evaluate words ˆji∗. |ck| elements asymptotically topic associated word considered results proposition provide statistics identifying novel words topic eii− ejj. ﬁrst straightforward latter efﬁcient calculate practice better computational complexity. speciﬁcally empirical version algorithm novel word non-novel word start show algorithm detect novel words distinct rankings consistently. illustrated lemma detect novel words ranking ordering solid angles denote minimum solid angle extreme points proof show estimated solid angle note diagonal entries calculated using time. steps takes running time. consider detecting clustering steps note conditions step calculated previous steps recall number novel words small constant topic then step require running time last consider topic estimation steps algorithm corresponding inputs linear regression already computed projection step. linear regression variables upper bound running time calculating row-normalization factors requires time. column re-normalization requires running time. overall need running time. steps also efﬁcient. splitting document independent halves takes linear time document since achieve using random permutation items. generate random direction requires complexity spherical gaussian prior. directly sort empirical estimated solid angles time) search words largest solid angles whose number constant w.r.t therefore would take time. focus case random projection directions sampled isotropic distribution. proof tied special form distribution; isotropic nature. ﬁrst provide useful propositions. denote novel word topic denote non-novel words. ﬁrst show proposition i-th suppose separable γs-simplicial following true without loss generality novel words topic ﬁrst consider solution constrained linear regression. simplify notation denote ﬁrst entries vector without super-scripts algorithm proof lemma proof. ﬁrst show irreducibility implies separability equivalently collection separable irreducible. suppose separable. exists that negative coefﬁcient nonnegative measurable implies collection measures irreducible. next show separability implies irreducibility. collection measures separable deﬁnition separability ciνi nonnegative measurable sets i.e. ciνi have ding rohban ishwar saligrama geometric approach latent topic modeling discovery ieee international conference acoustics speech signal processing ding ishwar rohban saligrama necessary sufﬁcient conditions novel word detection separable topic models advances neural information processing systems workshop topic models computation application lake tahoe dec. ding rohban ishwar saligrama efﬁcient distributed topic modeling provable guarantees proc. international conference artiﬁcial intelligence statistics reykjavik iceland apr. arora halpern mimno moitra sontag practical algorithm topic modeling provable guarantees proc. international conference machine learning atlanta jun. kumar sindhwani kambadur fast conical hull algorithms near-separable non-negative matrix factorization int. conf. machine learning atlanta jun. cichocki zdunek phan amari nonnegative matrix tensor factorizations applications exploratory multi-way data analysis blind source separation. wiley awasthi risteski provably correct cases variational inference topic models arxiv. bansal bhattacharyya kannan provable svd-based algorithm learning topics dominant admixture corpus advances neural information processing systems donoho stodden when non-negative matrix factorization give correct decomposition parts? advances neural information processing systems cambridge press gillis vavasis fast robust recursive algorithms separable nonnegative matrix factorization ieee trans. pattern analysis machine intelligence vol. farias jagabathula shah data-driven approach modeling choice advances neural information processing systems vancouver canada dec. mimno mccallum efﬁcient methods topic model inference streaming document collections proceedings sigkdd international conference knowledge discovery data mining. asuncion smyth welling asynchronous distributed learning topic models advances neural information processing systems koller schuurmans bengio bottou eds. scott rate convergence mixture proportion estimation application learning noisy labels proceedings international conference artiﬁcial intelligence statistics", "year": 2015}