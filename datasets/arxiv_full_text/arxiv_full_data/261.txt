{"title": "Learning to Learn from Weak Supervision by Full Supervision", "tag": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "abstract": "In this paper, we propose a method for training neural networks when we have a large set of data with weak labels and a small amount of data with true labels. In our proposed model, we train two neural networks: a target network, the learner and a confidence network, the meta-learner. The target network is optimized to perform a given task and is trained using a large set of unlabeled data that are weakly annotated. We propose to control the magnitude of the gradient updates to the target network using the scores provided by the second confidence network, which is trained on a small amount of supervised data. Thus we avoid that the weight updates computed from noisy labels harm the quality of the target network model.", "text": "paper propose method training neural networks large data weak labels small amount data true labels. proposed model train neural networks target network learner conﬁdence network meta-learner. target network optimized perform given task trained using large unlabeled data weakly annotated. propose control magnitude gradient updates target network using scores provided second conﬁdence network trained small amount supervised data. thus avoid weight updates computed noisy labels harm quality target network model. using weak noisy supervision straightforward approach increase size training data output heuristic methods used weak noisy signals along small amount labeled data train neural networks. usually done pre-training network weak data tuning true labels however independent stages leverage full capacity information true labels using noisy labels lower quality often brings little improvement. issue tackled noise-aware models denoising weak signal part learning process paper propose method leverages small amount data true labels along large amount data weak labels. proposed method train networks multi-task fashion target network uses large weakly annotated instances learn main task conﬁdence network trained small human-labeled estimate conﬁdence scores. scores deﬁne magnitude weight updates target network back-propagation phase. meta-learning perspective goal conﬁdence network meta-learner trained jointly target network learner calibrate learning rate target network instance batch. i.e. weights target network step updated follows global learning rate loss predicting input label scoring function learned conﬁdence network taking input instance noisy label ˜yi. thus eﬀectively control contribution parameter updates target network weakly labeled instances based reliable labels according conﬁdence network figure proposed multi-task network learning target task using large amount weakly labeled data small amount data true labels. faded parts network disabled training corresponding mode. red-dotted arrows show gradient propagation. parameters parts network frames updated backward pass parameters network blue frames ﬁxed training. approach similar separate recurrent neural network called optimizer learns predict optimal update rule updating parameters target network. optimizerreceivesagradientfromthetargetnetwork andoutputstheadjustedgradientmatrix. asthenumber ofparametersinmodernneuralnetworksistypicallyontheorderofmillionsthegradientmatrixbecomes large feed optimizer approach presented applied small models. contrast approach leverages additional weakly labeled data conﬁdence network predict per-instance scores calibrate gradient updates target network. setup requires running weak annotator label large amount unlabeled data done pre-processing time. many tasks possible simple heuristic generate weak labels. used train target network. contrast small human-labeled used train conﬁdence network estimates good weak annotations i.e. controls eﬀect weak labels updating parameters target network. helps alleviate updates instances unreliable labels corrupt target network. paper study approach sentiment classiﬁcation task.our experimental results suggest proposed method eﬀective leveraging large amounts weakly labeled data compared traditional ﬁne-tuning. also show explicitly controlling target network weight updates conﬁdence network leads faster convergence. proposed method following describe recipe training neural networks scenario along smallhuman-labeledtrainingsetalargesetofweaklylabeledinstancesisleveraged. formally givenaset ofunlabeledtraininginstanceswerunaweakannotatortogeneratenoisylabels. thisgivesusthetraining setu. itconsistsoftuplesoftraininginstances andtheirweaklabels ˜yii.e. ={...}. forasmall training instances true labels also apply weak annotator generate weak labels. creates training setv consisting triplets training instances weak labels true labels yji.e. ={...}. wecangeneratealargeamountoftrainingdatau atalmostnocostusing weak annotator. contrast limited amount data true labels i.e. <<|u|. proposed framework train multi-task neural network jointly learns conﬁdence score weak training instances main task using controlled supervised signals. high-level representation model shown figure comprises neural networks namely conﬁdence network target network. goal conﬁdence network estimate conﬁdence score training instances. learned triplets training input weak label true label score used control eﬀect weakly annotated training instances updating parameters target network. target network charge handling main task want learn. given data instance weak label training target network aims predict label ˆyi. target network parameter updates based noisy labels assigned weak annotator magnitude gradient update based output conﬁdence network. networks trained multi-task fashion alternating full supervision weak supervision mode. full supervision mode parameters conﬁdence network updated using batches instances training depicted figure training instance passed representation layer mapping inputs vectors. vectors concatenated corresponding weak labels ˜yj. conﬁdence network estimates probability taking data instance account training target network. weak supervision mode parameters target network updated using training shown figure training instance passed representation learning layer processed supervision layer part target network predicting label main task. also pass learned representation training instance along corresponding label generated weak annotator conﬁdence network estimate conﬁdence score training instance i.e. ˜ci. conﬁdence score computed instance conﬁdence scores used weight gradient updating target network parameters backpropagation. noteworthy representation layer shared networks conﬁdencenetworkcanbeneﬁtfromthelargenessofsetu andthetargetnetworkcanutilizethequalityofsetv. model training optimization objective composed terms conﬁdence network loss captures quality output conﬁdence network target network loss expresses quality main task. networks trained alternating weak supervision full supervision mode. full supervision mode parameters conﬁdence network updated using training instance drawn training cross-entropy loss function conﬁdence network capture diﬀerence predicted conﬁdence score instance i.e. target score j∈v−cjlog−log target score calculated based diﬀerence true weak labels respect main task. weak supervision mode parameters target network updated using training instances weighted loss function capture diﬀerence predicted label target network target label ˜cili task-speciﬁc loss training instance conﬁdence score weakly annotated instance estimated conﬁdence network. note treated constant weak supervision mode gradient propagation conﬁdence network backward pass minimize loss functions jointly randomly alternating full weak supervision modes training based chosen supervision mode sample batch training instances replacement without replacement experiments section apply method sentiment classiﬁcation task. task aims identify sentiment underlying individual sentence. target network convolutional model similar model representation learning layer learns input sentence dense vector representation. inputs ﬁrst passed embedding layer mapping sentence matrix rm×|s| followed series convolutional layers max-pooling. supervision layer feed-forward neural network softmax instead output layer returns probability distribution three classes. weak annotator sentiment classiﬁcation task simple unsupervised lexicon-based method averages predeﬁned sentiment score words sentence. details sentiment classiﬁcation model experimental setups provided appendix appendix respectively. following brieﬂy introduce baselines dataset used present results experiments. baselines. evaluate performance method compared following baselines weak annotator i.e. unsupervised method used annotating unlabeled data. target network trained weakly labeled data. full weak supervision only i.e. supervision only i.e. target network trained true labeled data. weak supervision fine tuning i.e. target network trained weakly labeled data tuned true labeled data. label inference similar proposed neural architecture inspired teacher-student paradigm instead conﬁdence table performance baseline models well proposed method diﬀerent datasets terms macro-f. ĳorźindicates improvements degradations respect weak supervision statistically signiﬁcant level using paired two-tailed t-test. figure lossofthetargetnetwork andtheconﬁdencenetwork compared loss training/validation performance llws test sets respect diﬀerent amount training data sentiment classiﬁcation. network predict conﬁdence score training instance label generator network trained weak labels instances labels. labels used target training target network. model diﬀerent training setup separate training i.e. consider conﬁdence network separate network without sharing representation learning layer train train target network controlled weak supervision signals. learning learn weak supervision joint training proposed neural architecture jointly train target network conﬁdence network alternating batches drawn sets data. train/test model semeval- semeval- semeval- twitter sentiment classiﬁcation task. large corpus containing tweets collected months unblabled set. results discussion. report oﬃcial semeval metric macro-f table based results llws provides signiﬁcant boost performance datasets. typical tuning i.e. ws+ft leads improvement weak supervision only. performance worse llws learning mapping imperfect labels accurate labels training target network labels essentially harder learning ﬁlter noisy labels hence needs supervised data. llwsst performs worse llws since training data enough train high-quality conﬁdence network without taking advantage shared representation learned vast amount weakly annotated data also noticed strategy leads slow convergence compared wso. besides general baselines also report best performing systems also convolution-based models semeval-; semeval-). proposed model outperforms best systems. controlling eﬀect supervision train neural networks improves performance also providesthenetworkwithmoresolidsignalswhichspeedsupthetrainingprocess. figureillustratesthe training/validation loss networks compared loss training target network weak supervisionalongwiththeirperformanceontestsetswithrespecttodiﬀerentamountsoftrainingdatafor thesentimentclassiﬁcationtask. asshown traininglt ishigherthanlwso butthetargetlabelswithrespectofwhichthelossiscalculatedareweaksoregardlessoverﬁttingproblemandlackofgeneralization loss means ﬁtting imperfection weak data. however validation decreases faster lwso compared performance llws test sets increases quickly llws passes performance weak annotator seeing fewer instances annotated conclusion paper propose neural network architecture uniﬁes learning estimate conﬁdence score weak annotations training neural networks controlled weak supervision. apply model sentiment classiﬁcation task empirically verify proposed model speeds training process obtains accurate results. marcin andrychowicz misha denil sergio gomez matthew hoﬀman david pfau schaul nando freitas. learning learn gradient descent gradient descent. advances neural information processing systems pages stefano baccianella andrea esuli fabrizio sebastiani. sentiwordnet enhanced lexical resource sentiment analysis opinion mining. lrec volume pages eyal beigman beata beigman klebanov. learning annotation noise. proceedings joint conference annual meeting international joint conference natural language processing afnlp volume -volume pages association computational linguistics mostafa dehghani aliaksei severyn sascha rothe jaap kamps. avoiding teacher’s mistakes training neural networks controlled weak supervision. arxiv preprint arxiv. deriu maurice gonzenbach fatih uzdilli aurelien lucchi valeria luca martin jaggi. swisscheese semeval- task sentiment classiﬁcation using ensemble convolutional neural networks distant supervision. proceedings semeval pages deriu aurelien lucchi valeria luca aliaksei severyn simon müller mark cieliebak thomas hofmann martin jaggi. leveraging large amounts weakly supervised data multi-language sentiment classiﬁcation. proceedings international international world wide conference pages thomas desautels andreas krause joel burdick. parallelizing exploration-exploitation tradeoﬀs gaussian process bandit optimization. journal machine learning research hussam hamdan frederic béchet patrice bellot. experiments dbpedia wordnet sentiwordnet resources sentiment analysis micro-blogging. second joint conference lexical computational semantics volume pages giorgio patrini alessandro rozza aditya menon richard nock lizhen making neural networks robust label noise loss correction approach. arxiv preprint arxiv. aliaksei severyn alessandro moschitti. twitter sentiment analysis deep convolutional neural networks. proceedings international sigir conference research development information retrieval pages aliaksei severyn alessandro moschitti. unitn training deep convolutional neural network twitter sentiment classiﬁcation. proceedings international workshop semantic evaluation association computational linguistics denver colorado pages nitish srivastava geoﬀrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. dropout simple prevent neural networks overﬁtting. mach. learn. res. andreas veit neil alldrin chechik ivan krasin abhinav gupta serge belongie. learning noisy large-scale datasets minimal supervision. conference computer vision pattern recognition sentiment classiﬁcation task identify sentiment underlying individual sentence. model used target network convolutional model similar training instance consists sentence sentiment label architecture target network illustrated figure describe setup target network i.e. description representation learning layer supervision layer. representation learning layer learns representation input sentence shared target network conﬁdence network. consists embedding function denotes vocabulary number embedding dimensions. function maps sentence matrix rm×|s| column represents embedding word corresponding position sentence. matrix passed convolution layer. layer ﬁlters applied sliding window length generate feature matrix feature denotes concatenation word vectors position concatenation produces feature vector r|s|−h+. vectors aggregated ﬁlters feature matrix also bias vector result convolution. convolutional layer followed non-linear activation function applied element-wise. afterward output passed pooling layer operates columns feature matrix returning largest value pool architecture similar state-of-the-art model twitter sentiment classiﬁcation semeval initialize embedding matrix wordvec embeddings pretrained collection tweets. supervision layer receives vector representation inputs processed representation learninglayerandoutputsaprediction weoptforasimplefullyconnectedfeed-forwardnetworkwith hidden layers followed softmax. hidden layer network computes denote weight matrix bias term corresponding hidden layer non-linearity. layers follow softmax layer returns probability distribution three classes. employ weighted cross entropy loss batch instances conﬁdence score weakly annotated instance classes. weak annotator sentiment classiﬁcation task simple unsupervised lexicon-based method sentiwordnet assign probabilities token sentence-level distribution derived simply averaging distributions terms yielding noisy label number classes i.e. empirically found using soft labels weak annotator works better assigning single hard label. target label conﬁdence network calculated using mean absolute diﬀerence true label weak one-hot encoding sentence label classes. proposed architectures implemented tensorflow adam optimizer back-propagation algorithm. furthermore prevent feature co-adaptation dropout regularization technique models. setup conﬁdence network predict fully connected feed forward network. given conﬁdence network learned small true labels speed training initialize representation learning layer pre-trained parameters i.e. pre-trained word embeddings. relu non-linear activation function target network conﬁdence network. collections. test model twitter message-level sentiment classiﬁcation semeval- task datasets semeval- subsume test sets previous editions semeval i.e. semeval- semeval-. tweet preprocessed urls usernames masked. data true labels. train development data semeval- training semeval--test validation. make results comparable oﬃcial runs semeval semeval- semeval- test sets data weak labels. large corpus containing tweets collected months both training word embeddings creating weakly annotated using lexicon based method explained section parameters settings. tuned hyper-parameters model including baselines separately withrespecttothetruelabelsofthevalidationsetusingbatchedgpbanditswithanexpectedimprovement acquisition function size number hidden layers classiﬁer conﬁdence network separately selected respectively. tested model both convolutional layers. number convolutional feature maps ﬁlter width selected respectively. initial learning rate dropout parameter selected {e−e−} {...} respectively. considered embedding sizes batch size experiments", "year": 2017}