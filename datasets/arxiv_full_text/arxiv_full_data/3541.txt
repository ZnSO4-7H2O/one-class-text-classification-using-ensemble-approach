{"title": "Convergent Block Coordinate Descent for Training Tikhonov Regularized  Deep Neural Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.", "text": "lifting relu function higher dimensional space develop smooth multi-convex formulation training feed-forward deep neural networks allows develop block coordinate descent training algorithm consisting sequence numerically well-behaved convex optimizations. using ideas proximal point methods convex analysis prove algorithm converge globally stationary point r-linear convergence rate order one. experiments mnist database dnns trained algorithm consistently yielded better test-set error rates identical architectures trained stochastic gradient descent variants caffe toolbox. feed-forward deep neural networks function approximators wherein weighted combinations inputs ﬁltered nonlinear activation functions organized cascade fully connected hidden layers. recent years dnns become tool choice many research areas machine translation computer vision. objective function training highly non-convex leading numerous obstacles global optimization notably proliferation saddle points prevalence local extrema offer poor generalization training sample observations motivated regularization schemes smooth simplify energy surface either explicitly weight decay implicitly dropout batch normalization solutions robust i.e. better generalized test data. training algorithms face many numerically difﬁculties make difﬁcult even local optimum. well-known issues so-called vanishing gradient back propagation i.e. long dependency chains hidden layers tend drive gradients zero optimum. issue leads slow improvements model parameters issue becomes serious deeper networks vanishing gradient problem partially ameliorated using non-saturating activation functions rectiﬁed linear unit network architectures shorter input-to-output paths resnet saddle-point problem addressed switching deterministic gradient descent stochastic gradient descent achieve weak convergence probability classic proximal-point optimization methods alternating direction method multipliers also shown promise training setting convergence properties remain unknown. contributions paper propose novel block coordinate descent based learning algorithm accordingly guarantee globally converge stationary points r-linear convergence rate order one; tikhonov regularization motivated fact relu activation function equivalent solving smoothly penalized projection problem higher-dimensional euclidean space. build tikhonov regularization matrix encodes information networks i.e. architectures well associated weights. training objective divided three sub-problems namely tikhonov regularized inverse problem least-square regression learning classiﬁers. since sub-problem convex coupled overall objective multi-convex. block coordinate descent often used problems ﬁnding exact solution sub-problem respect subset variables much simpler ﬁnding solution variables simultaneously case sub-problem isolates block variables solved easily advantages decomposition sub-problems long-range dependency hidden layers captured within subproblem whose solution helps propagate information inputs outputs stabilize networks therefore suffer vanishing gradient all. experiments demonstrate effectiveness efﬁciency algorithm comparing based solvers. stochastic regularization local regularization tikhonov regularization widely-used technique deep learning prevent training overﬁtting. basic idea multiple network weights random variables learned network robust generalized test data. dropout variants like classic examples ghahramani showed deep learning considered approximate variational inference bayesian neural networks. recently baldassi proposed smoothing non-convex functions local entropy latter chaudhari proposed entropy-sgd training dnns. idea behind methods locate solutions locally within large regions energy landscape favors good generalization. chaudhari provided mathematical justiﬁcation methods perspective partial differential equations contrast tikhonov regularization tends smooth non-convex loss explicitly globally data-dependently. deterministically learn tikhonov matrix well auxiliary variables ill-posed inverse problems. tikhonov matrix encodes information network auxiliary variables represent ideal outputs data hidden layer minimize objective. conceptually variables work similarly target propagation bottou proved weak convergence non-convex optimization. non-convex loss functions stochastic gradient unbiased bounded variance denotes number iterations. non-convex optimization based algorithm proven converge globally stationary points. parallel computing another based algorithm namely parallel successive convex approximation proposed proven convergent. admm alternating direction method multipliers proximal-point optimization framework recently championed boyd breaks nearlyseparable problem loosely-coupled smaller problems solved independently thus parallel. admm offers linear convergence strictly convex problems certain special non-convex optimization problems admm also converge unfortunately thus evidence mathematical argument training special cases. therefore even though empirically successfully applied training still lacks convergence guarantee. bcd-based training algorithm also amenable admm-like parallelization. importantly prove sec. converge globally stationary points r-linear convergence. problem setup notations denote i-th training data corresponding class label label output feature n-th hidden layer network rdn×dm weight matrix n-th m-th hidden layers input layer index n-th hidden layer +×dn weight matrix last hidden layer output layer nonempty closed convex sets convex loss function. network architectures networks consider relu activation functions. provide short paths allow multi-input relu units take outputs multiple previous layers inputs. fig. illustrates network architecture consider third hidden layers instance takes input data outputs ﬁrst second hidden layers inputs. mathematically deﬁne multi-input relu function layer data denotes entry-wise operator denotes dn-dim zero vector. note multi-input relus thought conventional relu skip layers identity matrices accordingly. conventional objective training dnns relu write general objective recursive used follows clarity {wnm}. note separate last layer rest hidden layers intentionally learning classiﬁers learning useful features. network architectures paper mainly extracting features arbitrary classiﬁer learned further. goal optimize propose novel based algorithm solve relaxation using tikhonov regularization convergence guarantee. {uin} denote predeﬁned regularization constants. larger values force uin∀i closely approximate output relu n-th hidden layer. arranging terms matrix rewrite familiar form tikhonov regularized objective ui∀i denotes concatenating vector hidden outputs well input data i.e. n=∀i predeﬁned constant matrix denotes another matrix constructed weight matrix proposition positive semideﬁnite leading following tikhonov regularization three sub-problems solved efﬁciently convexity. fact inverse subproblem alleviates vanishing gradient issue traditional deep learning tries obtain estimated solution output feature hidden layer dependent tikhonov matrix. functionality similar target propagation namely propagating information input data output labels. unfortunately simple alternating optimization scheme cannot guarantee convergence stationary points solving therefore propose novel based algorithm training dnns based listed alg. basically sequentially solve sub-problem extra quadratic term. extra terms well convex combination rule guarantee global convergence algorithm algorithm involves solving sequence quadratic programs whose computational complexity cubic general input dimension paper focus theoretical development algorithm consider fast implementations future work. pair test data potential label solve optimization problem leading unaffordably high computational complexity prevents using recall goal train feed-forward dnns using algorithm alg. considering this utilize network weights construct network extracting deep features. since features approximation learned classiﬁer never reused test time. therefore retain architecture weights trained network replace classiﬁcation layer linear support vector machine demonstrate effectiveness efﬁciency based algorithm alg. conduct comprehensive experiments mnist dataset using pixels input features. refer algorithm learning dense networks learning sparse networks bcd-s respectively. sparse learning deﬁne convex denotes k-th matrix denotes norm vector. comparisons performed implement algorithms using matlab implementation without optimizing code. compare algorithms based solvers caffe i.e. adadelta adagrad adam nesterov rmsprop coded python. network architecture implemented illustrated fig. network three hidden layers nodes layer four layers three skip layers inside. therefore mapping function input output deﬁned network max{ wui} max{ wui} max{ wxi}. simplicity without loss generality utilize loss function learn network parameters using different solvers inputs random initial weights layer. without ﬁne-tuning regularization parameters simply bcd-s algorithms. caffe solvers modify demo code caffe mnist comparison carefully tuning parameters achieve best performance can. report results within epochs averaging three trials point training methods seems convergent already. competing algorithms epoch entire training data passed update parameters. therefore algorithms epoch equivalent iteration iterations total. convergence fig. shows change training objective increase epochs bcd-s respectively. curves decrease monotonically become ﬂatter ﬂatter eventually indicating algorithms converge. bcd-s converges much faster objective higher bcd. bcd-s learns sparse models data well dense models learned bcd. testing error mentioned sec. utilize linear svms last-layer hidden features extracted training data retrain classiﬁer. based network fig. feature extraction function max{ max{ wxi} max{ max{ wxi}}}. conduct fair comparison retrain classiﬁers algorithms summarize test-time results fig. epochs. algorithm learns dense architectures based solvers performs best bcd-s algorithm works still better competitors although learns much sparser networks. results consistent training objectives fig. well. computational time compare training time fig. seems implementation signiﬁcantly faster caffe solvers. instance achieves times speed-up competitors achieving best classiﬁcation performance test time. sparseness order compare difference terms weights dense sparse networks learned bcd-s respectively compare percentage nonzero weights layer show results fig. expect last layer bcd-s ability learning much sparser networks deep feature extraction. case bcd-s learns network nonzero weights average classiﬁcation accuracy lower learns network nonzero weights. potentially ability could useful scenarios embedding systems sparse networks desired. optimization algorithm train dnns relaxed objective related ours. train mnist used i.e. hidden layers nodes layer sequentially fully connected details network). using image features consistently observe marginal improvement results reported however ﬁnish training within hour based implementation using matlab code training needs hours. similar observations made cifar- used using network hidden layers nodes layer. deﬁnition given point algorithm initial point point-to-set generates sequence {xk}∞ rule said global convergent chosen initial point sequence {xk}∞ generated converges point necessary condition optimality holds. deﬁnition {xk} sequence converges convergence r-linear sequence nonnegative scalars {vk} vk∀k {vk} converges q-linearly zero. lemma function convex minw∈rd theoretical results deﬁnition objectives three sub-problems respectively. assume lower-bounded lipschitz continuous constants respectively. proposition θˆx. lemma nonempty closed convex function convex lipschitz continuous constant scalar suppose minz∈x paper ﬁrst propose novel tikhonov regularization training dnns relu activation functions. tikhonov matrix encodes network architecture well parameterization. help reformulate network training block multi-convex minimization problem. accordingly propose novel block coordinate descent based algorithm proven converge globally stationary points r-linear converge rate order one. empirical results suggest algorithm converge suitable learning dense sparse networks work better traditional based deep learning solvers.", "year": 2017}