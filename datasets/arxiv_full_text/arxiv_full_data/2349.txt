{"title": "Improved learning of Bayesian networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The search space of Bayesian Network structures is usually defined as Acyclic Directed Graphs (DAGs) and the search is done by local transformations of DAGs. But the space of Bayesian Networks is ordered by DAG Markov model inclusion and it is natural to consider that a good search policy should take this into account. First attempt to do this (Chickering 1996) was using equivalence classes of DAGs instead of DAGs itself. This approach produces better results but it is significantly slower. We present a compromise between these two approaches. It uses DAGs to search the space in such a way that the ordering by inclusion is taken into account. This is achieved by repetitive usage of local moves within the equivalence class of DAGs. We show that this new approach produces better results than the original DAGs approach without substantial change in time complexity. We present empirical results, within the framework of heuristic search and Markov Chain Monte Carlo, provided through the Alarm dataset.", "text": "search space bayesian network struc­ tures usually defined acyclic directed graphs search done transformations bayesian networks ordered respect inclusion good search policy take count. first attempt using equivalence dags instead dags itself. produces better results significantly slower. present compromise approaches. uses dags search space ordering inclusion achieved repetitive within equivalence class dags. show approach produces bet­ results without substantial ity. present empirical framework heuristic search markov chain monte carlo alarm dataset. bayesian network variables variables. ture encodes assertions conditional dence distribution tional probability structure. graph node corresponds variable basic problem choice search space exists different dags assert independence among vari­ ables domain call networks problem whether search space dags space equivalence learning equivalence classes produces better results dags approach needs significantly paper argue better results presented caused usage equivalence space used. moreover recent results suggest space equiv­ alence classes times smaller major contributions introduce dags space. concept many works within next section dags equivalence brief comment sizes essential graph spaces. ferent implementation mcmc. neighbourhoods within par­ ticular section dataset. equivalent dags forms given equivalence class bayesian net­ works. lemmas follows model induced every dependence tained adding removing contained algorithm outputs sequence equivalent dags local maximum arcs ori­ ented according equivalent thanks lemma question equally probable mains open. another algorithm given nonzero probability repeated covered reversal algorithm reds algorithm. free parameter stands upper bound propose number repetitions. small constant like enough dags thanks asymptotic number dags given robinson follows search space dags grows exponentially think number equivalence rest paper random nature. used together believe combination order avoid local maxima much possible ideally search strategy independence model reach independence boundary inclusion clear satisfy property. however rcarr rcarnr neighbourhoods long number repetitions within rcar large enough meek's conjecture case last conditions hold rcarr rcarnr neigh­ bourhoods clearly allow reach much members inclusion boundary neighbourhoods. dealing general learning problem causal ordering assumed. simplest probably used heuristic bayesian networks hill-climber search) traversal neighbourhood. mcmc method samples target distribu­ tion case posterior networks given data p{gid). adapted metropolis-hastings bayesian decomposable networks so-called tion commonly known aperiodic search space graphs stationary move consists local transformation leads randomly chosen candidate proposed move accepted markov since previous ratio corre­ sponds bayes factor models involves efficient local computations. markov chain irreducible probability search space process converge posterior present experimental traversal much better results reasonable time. periments used metric known invariant used equivalent joint space uniform prior distribution throughout alarm dataset dard benchmark learning dataset first used cooper herskovits cases sampled ferent experiments single synthetically introduced arcs. arcs network supported herskovits bourhood running hill-climber lead different results different number trials escaping results runs confidence means score structural cluded. results appreciate best per­ using rcar within formance also used-the striking lies case cases rcarr. there average structural steps. eight runs ence corresponding data makes confidence tight. eight times result reached steps result rcar even steps tremely goal experimentation twofold. higher second vergenceheuristic learning behind success viously empty graph. dataset also chain starting network data. mobility cause higher mobility approximated reflect averages samples across across dif­ graphs visited ferent process well average kullback-leibler distance sential mobility visited third convergence terior distribution cardinality chain gives highest diagnostic almost true alarm network asterisk empty network. rcarr able reproduce quite similar cases dataset responding markov chain started almost true alarm net­ work. x-axis find essential graphs dered moment time first member visited chain. since plots neighbourhoods similar happens rcarnr rcarr compare rcarr. follows plots rcar approaches better shape different cardinalities classes. better estimation rcar empiri­ evidence follows rcar able reach neighbouring classes dags ultimate reason learning faster better bayesian networks. table extra computational cost rcar. first contains accumulated aver­ number iterations second markov chain. second contains accepts/rejects ratio lower ratio speeds clarity report comparison rcarr rcarrlo differences combinations. done cases dataset starting empty true alarm network using rcar using cost agrees observed heuristic learning consider good trade-off. standard neighbourhood invariant lence class. order solve problem introduced concepts traversal operators dags based upon idea make moves within equivalence reversal operation. shown usage rcar within heuristic reasonable edges alarm network transformations). defined constants rcar number repetitions maximum clever another moreover versal heuristic learning terintuitive longer true alarm network verges true alarm network suggests rcarnr would probably vergence. practical importance bayesian inclusion meek's conjecture thank chris meek making familiar conjecture keeping dici help mcmc method arno siebes anonymous reviewers useful marks. research frvs ticipation seminar structures\"", "year": 2013}