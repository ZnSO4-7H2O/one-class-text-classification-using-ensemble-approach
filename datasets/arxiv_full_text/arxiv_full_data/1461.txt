{"title": "Any-gram Kernels for Sentence Classification: A Sentiment Analysis Case  Study", "tag": ["cs.CL", "cs.AI", "stat.ML"], "abstract": "Any-gram kernels are a flexible and efficient way to employ bag-of-n-gram features when learning from textual data. They are also compatible with the use of word embeddings so that word similarities can be accounted for. While the original any-gram kernels are implemented on top of tree kernels, we propose a new approach which is independent of tree kernels and is more efficient. We also propose a more effective way to make use of word embeddings than the original any-gram formulation. When applied to the task of sentiment classification, our new formulation achieves significantly better performance.", "text": "any-gram kernels ﬂexible efﬁcient employ bag-of-n-gram features learning textual data. also compatible word embeddings word similarities accounted for. original any-gram kernels implemented tree kernels propose approach independent tree kernels efﬁcient. also propose effective make word embeddings original any-gram formulation. applied task sentiment classiﬁcation formulation achieves signiﬁcantly better performance. basic readily available features machine learning natural language words themselves. despite simplicity words proven essential part statistical text analysis words often presented learning algorithms form bag-of-n-grams speciﬁes number consecutive words grouped together. however main disadvantages using bag-of-n-gram features. firstly optimum value seems dependent problem data hand. example task sentiment polarity classiﬁcation work report favourable results bag-ofunigrams others higher orders useful secondly bagof-n-grams suffer high-dimensionality sparsity exacerbated increases. modelling exponential number features implicitly without need extract explicit features. principle kernel functions measure similarity every pair instances data set. measures become feature space learning algorithm operates. tree kernels example kernel functions used along kernel-based machine learning algorithms support vector machines perceptron applied tree structures usually generated syntactic semantic analyzers tree kernels measure similarity structures based number common tree fragments them. exploit feature tree kernels tackling problems associated learning bag-of-n-gram features previous work proposed any-gram kernels formats input sentences binary trees uses tree kernels compute kernel. approach models bag-of-n-gram features possible orders n-grams automatically included feature space accounting problem ﬁnding optimum problem high-dimensional sparse feature space. order represent sentence binary tree tree kernel extract possible n-gram sentence tree fragment every node duplicated additional nodes required. example shown figure sentence good fast service. nodes make extraction unigrams possible smallest tree fragment extracted tree kernels production rule rather node. overhead caused constraint imposed rather artiﬁcial tree kernels. work propose algorithm independent tree kernels eliminates requirement input sentence represented binary tree inprevious work also modiﬁed tree function anygram kernels exploit similarity words obtained comparing pre-trained embedding vectors. however found similarities any-gram kernels lead consistently better results simple string match. propose method better leverage word embedding similarities context any-gram kernels yielding higher performance. compare performance any-gram approach deep recurrent neural networks form unibidirectional lstms widely used sentiment polarity classiﬁcation data sets semeval aspect-based sentiment analysis data stanford sentiment treebank any-gram performance competitive lstm performance. advantage anygram approach neural approach parameter tuned. rest paper ﬁrst review related work kernel methods section explain implementation any-gram kernels section section section experiments presented results discussed. finally paper concluded section summary ﬁndings suggestions work area. introduced any-gram kernels previous work attempt address sparsity bag-of-n-gram features well reduce efforts engineering features terms ﬁnding useful order n-gram i.e. experiments show any-grams effectively replace bag-of-n-gram features without compromising performance. well widely used general-purpose kernel functions polynomial kernels radial basis function kernels tree kernels popular kernel functions applied various natural language processing tasks parsing tagging semantic role labelling quality estimation sentiment analysis kernels mainly used learn problems knowledge sentencial syntactic and/or semantic structure useful. input kernels often constituency dependency trees produced syntactic and/or semantic parsers. srivastava extend tree kernels graph kernels based work g¨artner random-walk-based graph kernels. treating dependency trees graphs kernel value computed summation similarities pairs equal-length random walks tree pairs. similarities walks computed product pairwise similarities nodes within walks. string kernels also known sequence kernels also used text classiﬁcation. originally string kernels applied character level measuring similarity document pairs terms number matching subsequences them. although kernels allow non-contiguous subsequences work ﬁxed-length subsequences. cancedda extend character-level string kernels word level present work arguing level granularity efﬁcient also linguistically motivated. similar string kernels word-sequence kernels also require sequence length speciﬁed. however cancedda suggest using dynamic programming techniques allows subsequences length considered idea replacing rigid string match soft similarities computation various kernel functions also investigated scholars. siolas d’alch´e propose document similarity metric document categorization based semantic proximity word pairs deﬁned inverse distance hypernym hierarchy wordnet metric incorporated kernel support vector machines used k-nearest neighbour algorithm. bloehdorn extend semantic similarity metric capitalizing tree structure wordnet including depth concepts tree complement distance measure. polynomial kernel cosine similarity word embedding vectors compute word kernels short text categorization tasks including sentiment classiﬁcation kernel function applied pair words. extend model phrase sentence kernels aggregating word kernels words forming phrases sentences. models also require phrase length ﬁxed. graph kernel method srivastava allows node similarity based word embedding vectors corresponding words computed. senna word embeddings apply kernel sentiment classiﬁcation paraphrase detection metaphor identiﬁcation. cancedda also propose word-sequence kernel extended rectify shortcoming exact symbol matching using document-term matrix estimates termterm similarity information retrieval techniques. represented tree avoiding overhead using principle compute kernel values. algorithm treats input sentence array tokens uses dynamic programming compute kernel value every pair sentence instances. kernel function deﬁned decay factor penalizes contribution longer n-grams bigger value tokens follow current tokens sentence. algorithm shows deﬁnitions implemented. outer loop algorithm scans tokens ﬁrst sentence reverse order inner loop scans tokens second sentence original order. iteration cell delta table corresponds equation ﬁlled value calculated based value tokens following current tokens respective sentences. latter value already calculated present table thanks reverse iteration outer loop. statement inner loop controls execution statement computing current tokens equal computation ignored. similar efﬁcient implementation tree kernel algorithm another efﬁciency algorithm since practice majority token pairs sentence pair equal. original implementation any-gram kernels built tree kernels requires input text represented binary tree special format contains nodes sentence length section present algorithm require input text even though computational complexity any-gram kernels remains original ones running time algorithm reduced elimination repetition addition nodes. foster incorporate word embeddings kernel computation. model based similarity threshold value considers nodes similar similarity corresponding embedding vectors passes certain threshold. words nodes considered embedding vectors least similar threshold speciﬁes. original tree kernel function modiﬁed follows deﬁned equation nodes replaced tokens. algorithm shows implementation formulation. change compared algorithm string match statement inner loop replaced function theta equivalent equation stated earlier similarity measure inputs. threshold-based approach however disadvantages. firstly imposes additional hyper-parameter tuned. secondly word pairs similarity reach threshold actually perfectly similar context. example cosine similarity superb brilliant experiments however optimum threshold found similarity ruled out. another perspective word pairs similarity score slightly lower threshold effect kernel value signiﬁcantly less similar all. production rules rewriting respectively |pr| |pr| number nodes production rules peer nodes production rules number children threshold function deﬁned follows similar string-match-based any-gram kernel described section ﬁrst design method eliminates dependence tree kernels works directly textual input. kernel function deﬁned fact similarity measure words vector similarity embeddings vectors. however range threshold values must consistent output range similarity function. modify any-gram kernel function alleviates problems. instead value similar different vectors based threshold value plug similarity value kernel computation. eliminates need threshold tuning takes account below-threshold similarities factors extent similarity. formal definition kernel function follows seen value equation equation accounted fact tokens similar replaced actual similarity score tokens accounting extent similar. implementation presented algorithm section carry sentiment polarity classiﬁcation experiments using methods described previous section. first data sets used experiments described. followed experimental settings results. slightly different problem sets aspectbased sentiment classiﬁcation sentence-level sentiment classiﬁcation. former data sets provided task semeval latter stanford sentiment treebank aspect-based sentiment analysis task problem sentiment analysis ﬁner level granularity sentence level. aspect terms fact targets towards sentiment expressed text. instance example figure service aspect term towards reviewer expressed positive sentiment. subtask task semeval dealt predicting polarity sentiment expressed towards aspect terms consumer reviews. polarity categorized four classes positive negative neutral conﬂict. organizers released data sets laptop restaurant reviews. data sets also used previous work data comes subsets training test. development purposes subset training also used original any-gram kernel paper comparable results. table summarizes statistics data sets. sentence-level sentiment analysis task sentence-level polarity classiﬁcation stanford sentiment treebank data based movie review data introduced pang originally developed analyze compositional effect language labelling constituents parse trees sentiment polarity label categories positive positive library uses one-versus-one method multi-class classiﬁcation default. svm’s error/margin trade-off parameter tuned development sets data sets. cosine similarity word embedding similarities glove common crawl word embeddings dimensionality also experimented pre-built word embeddings google news dimensionality also sentiment-speciﬁc word embeddings released tang glove common crawl vectors however performed better. noted aspect-based sentiment classiﬁcation requires aspect terms input learning algorithm. special tree representation used original any-gram kernel approach allows sort information incorporated input tree. previous work achieved inserting node labelled representing aspect term tokens. instance case example figure replaces node service indicate service aspect term. implementation however different approach needed accomplish this. case string-match-based any-grams achieved simply attaching sufﬁx aspect term tokens. instance sentence figure service aspect term represented good fast service case similarity-based any-grams sufﬁx technique however create unknown tokens pre-trained word embeddings. address issue aspect term identiﬁer word embedding vectors token aspect term sentence added word vector otherwise. therefore dimension word vectors increase approach used incorporate kind auxiliary input kernel function. compare any-grams state-of-the-art neural learning approaches build lstm models apply data sets. model unidirectional information ﬂows forward time bidirectional ﬂows directions model dependence previous words next words. input models pre-trained word embeddings used any-gram models. dropout layer added output layer regularization. hyper-parameters summarized table tuned development sets any-gram models. used adam optimization algorithm softmax function output layer cross-entropy loss function. models built using keras tensorflow backend. table illustrates performance models built experiments well reported tree-kernel-based any-grams systems ﬁrst second rows table names starting ngtk ones based tree-kernelbased any-grams middle section table names starting ones built using any-gram kernels. last rows table lstm models. sufﬁx stands string match west word embedding similarity threshold wess word embedding similarity score. first compare effect using implementation any-gram kernels independent tree kernels. i.e. ngtksm agksm ngtkwest agkwest. method outperforms original majority data sets improvements percent. however accuracy laptop development degraded percent. although primary goal behind designing implementation any-gram kernel efﬁciency turns leads better performance well. also noted that similar original any-grams thresholdbased word embedding similarity effective implementation either. next verify effectiveness proposed method using word embedding similarities any-gram kernels comparing agksm agkwest agkwess. seen similarity-score-based system outperforms systems data sets improvements ranging percent accuracy. systems also recovers afore-mentioned loss laptop development compared original any-grams. best any-gram kernel agkwess achieves competitive results lstm models. however noted lstm models large hyper-parameters require extensive tuning obtain optimum introduced formulation any-gram kernels independent tree kernels require input represented special binary tree form makes better word-vector-based comparison replace mere string match. method outperformed original implementation any-gram kernels achieved comparable results lstm models across three widely used sentiment analysis data sets. presented efﬁcient implementations variations any-gram kernel. applied any-gram kernels sentiment polarity classiﬁcation could applied text analysis problems including classiﬁcation regression. also worth noting experiments used n-grams. combining any-gram kernels features could beneﬁcial. achieve combination auxiliary input any-gram kernels described context marking aspect term sentence. another would combine any-gram kernel traditional kernels applied hand-crafted features.", "year": 2017}