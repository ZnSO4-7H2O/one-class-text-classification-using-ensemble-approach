{"title": "Deep Reinforcement Learning for Solving the Vehicle Routing Problem", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "We present an end-to-end framework for solving Vehicle Routing Problem (VRP) using deep reinforcement learning. In this approach, we train a single model that finds near-optimal solutions for problem instances sampled from a given distribution, only by observing the reward signals and following feasibility rules. Our model represents a parameterized stochastic policy, and by applying a policy gradient algorithm to optimize its parameters, the trained model produces the solution as a sequence of consecutive actions in real time, without the need to re-train for every new problem instance. Our method is faster in both training and inference than a recent method that solves the Traveling Salesman Problem (TSP), with nearly identical solution quality. On the more general VRP, our approach outperforms classical heuristics on medium-sized instances in both solution quality and computation time (after training). Our proposed framework can be applied to variants of the VRP such as the stochastic VRP, and has the potential to be applied more generally to combinatorial optimization problems.", "text": "present end-to-end framework solving vehicle routing problem using deep reinforcement learning. approach train single model ﬁnds near-optimal solutions problem instances sampled given distribution observing reward signals following feasibility rules. model represents parameterized stochastic policy applying policy gradient algorithm optimize parameters trained model produces solution sequence consecutive actions real time withneed re-train every problem instance. method faster training inference recent method solves traveling salesman problem nearly identical solution quality. general approach outperforms classical heuristics medium-sized instances solution quality computation time proposed framework applied variants stochastic potential applied generally combinatorial optimization problems. vehicle routing problem combinatorial optimization problems studied applied mathematics computer science decades many exact heuristic algorithms proposed. known considerably computationally difﬁcult traveling salesman problem special case. simplest form single capacitated vehicle responsible delivering items multiple customer nodes; vehicle must return depot pick additional items runs out. objective optimize routes beginning ending given node called depot order attain maximum possible reward often negative total vehicle traversed distance average service time. overview laporte laporte golden toth vigo prospect algorithm discovery without handengineered reasoning makes neural networks reinforcement learning compelling choice potential important milestone path approaching problems. work develop framework solving wide variety combinatorial optimization problems using deep reinforcement learning show applied solve vrp. purpose consider markov decision process formulation problem optimal solution viewed sequence decisions. allows produce near-optimal solutions increasing probability decoding desirable sequences. naive approach problem-speciﬁc solution considering every instance separately. obviously approach competitive algorithms terms either quality solutions runtime since many trajectories sampled produce near-optimal solution. moreover learned policy apply instances used training; small perturbation problem setting need rebuild policy scratch. rather focusing training separate model every problem instance propose structure performs well problem sampled given distribution. view trained model black-box heuristic generates high-quality solution reasonable amount time. trained model available used many times without needing re-train problems long generated training distribution. work originally motivated recent work bello generalized framework include wider range combinatorial optimization problems vrp. bello propose pointer network decode solution. major issue prohibits direct approach assumes system static time. contrast demands change time sense node visited demand becomes effectively zero. overcome this propose alternate approach—which actually simpler pointer network approach—that efﬁciently handle static dynamic elements system. model consists recurrent neural network decoder coupled attention mechanism. time step embeddings static elements input decoder output dynamic element embeddings attention mechanism forms distribution feasible inputs chosen next decision point. applied special case single route optimize method produces solutions similar quality reported bello signiﬁcantly faster training inference time. proposed framework appealing since utilize self-driven learning procedure requires reward calculation based generated outputs; long observe reward verify feasibility generated sequence learn desired meta-algorithm. instance know solve compute cost given solution provide signal required solving problem using method. unlike classical heuristic methods robust problem changes meaning inputs change automatically adapt solution. using classical heuristics entire distance matrix must recalculated system must re-optimized scratch often impractical especially problem size large. contrast proposed framework require explicit distance matrix feed-forward pass network update routes based data. sequence-to-sequence models useful tasks mapping sequence another required. extensively studied ﬁeld neural machine translation past several years numerous variants models. general architecture almost among different versions consists networks called encoder decoder. encoder network reads input sequence stores knowledge ﬁxed-size vector repvanilla sequence-to-sequence architecture source sequence appears encoder entire output sequence generated based vector extensions example bahdanau illustrate source information used wisely increase amount information decoding steps. addition encoder decoder network employ another neural network namely attention mechanism attends entire encoder states. mechanism allows decoder focus important locations source sequence relevant information decoding steps producing better output sequences. recently concept attention popular research idea capability align different objects e.g. computer vision neural machine translation study also employ special attention structure policy representation. section detailed discussion attention mechanism. last several years multiple methods developed tackle combinatorial optimization problems using recent advances artiﬁcial intelligence. ﬁrst attempt proposed vinyals introduce concept pointer network model originally inspired sequence-to-sequence models. invariant length encoder sequence pointer network enables model apply combinatorial optimization problems output sequence length determined source sequence. pointer network architecture supervised fashion nearoptimal tours ground truth optimal solutions. dependence supervision prohibits pointer network ﬁnding better solutions ones provided training. closest approach bello address issue developing neural combinatorial optimization framework uses optimize policy modeled pointer network. using several classical combinatorial optimization problems knapsack problem show effectiveness generality architecture. related topic solve optimization problems graphs using graph embedding structure algorithm even though represented graph weighted nodes edges proposed model directly apply since particular node might afﬁne function outputs input-sized vector state decoder summarizes information previously decoded steps y··· describe details proposed attention mechanism section remark model handle combinatorial optimization problems classical static setting well dynamically changing ones. static combinatorial optimization fully deﬁnes problem trying solve. example includes customer locations well demands depot location; then remaining demands updated respect vehicle destination load. consideration often exists well-deﬁned markovian transition function deﬁned sufﬁcient update dynamics decision points. however model also applied problems state transition function unknown and/or subject external noise since training explicitly make transition function. transition function helps simulate environment training algorithm interacts with. section example apply model stochastic vrp. although framework proposed bello works well problems knapsack problem applicable complicated combinatorial optimization problems system representation varies time vrp. bello feed random sequence inputs encoder. figure illustrates example using encoder restrictive. suppose ﬁrst decision step policy sends vehicle customer result demand satisﬁed i.e. second decision step need re-calculate whole network information order choose next customer. dynamic elements complicate forward pass network since encoder/decoder updates input changes. situation even worse back-propagation accumulate gradients since need remember dynamic elements changed. order resolve complication require model invariant input sequence changing order inputs affect network. section present simple network satisﬁes property. section formally deﬁne problem proposed framework. consider generic combinatorial optimization problem given inputs allow elements input change decoding steps fact case many problems vrp. dynamic elements might artifact decoding procedure itself imposed environment. example remaining customer demands change time vehicle visits customer nodes; might consider variant customers arrive adjust demand values time independent vehicle decisions. formally represent input sequence tuples static dynamic elements input respectively tuples. instance corresponds -dimensional coordinate customer location demand time denote input states ﬁxed time start arbitrary input every decoding time choose available inputs continue termination condition satisﬁed. termination condition problem-speciﬁc showing generated sequence satisﬁes feasibility constraints. instance termination criterion cities visited; consider work terminating condition demand satisfy. process generate sequence length possibly different sequence length compared inputs different dynamic elements. also notation denote decoded sequence time i.e. {y··· yt}. interested ﬁnding stochastic policy generates sequence minimizes loss objective satisfying problem constraints. optimal policy generate optimal solution probability goal make close possible. similar sutskever probability chain rule decompose probability generating sequence i.e. follows figure proposed model. embedding layer left maps inputs high-dimensional vector space. right decoder stores information decoded sequence. then hidden state embedded input produce probability distribution next input using attention mechanism. information inputs using variable-length alignment vector words speciﬁes much every input data point might relevant next decoding step recall simplicity calculations dynamic elements used attention layer. argue encoder adds extra complication encoder actually necessary approach made much general omitting rnns necessary inputs convey sequential information; e.g. text translation combination words relative position must captured order translation accurate. question need encoder combinatorial optimization problems meaningful order input set? example inputs unordered city locations order meaningful; random permutation contains information original inputs. therefore model simply leave encoder directly embedded inputs instead hidden states. modiﬁcation many computational complications disappear without decreasing model’s efﬁciency. illustrated figure model composed main components embeddings maps inputs d-dimensional vector space. might multiple embeddings corresponding different elements input shared among inputs. dimensional convolution layers embedding in-width input length number ﬁlters number in-channels number elements decoder points input every decoding step. common literature model decoder network. similar reasons discussion section notice static elements inputs decoder network; dynamic elements attention layer described next. attention mechanism differentiable structure addressing different parts input. figure illustrates attention mechanism employed method. decoder step utilize context-based attention mechanism similar vinyals extracts relevant trainable variables. remark model symmetry vinyals discuss extension sequence-to-sequence models empirically demonstrate tasks obvious input sequence sorting order inputs network matter. similar concern arises using pointer networks combinatorial optimization problems. however model proposed paper suffer complication since embeddings attention mechanism invariant input order. train network well-known policy gradient approaches. methods parameterize stochastic policy parameters policy gradient methods estimate gradient expected return respect policy parameters iteratively improve policy. utilize reinforce method similar bello solving stochastic vrp. algorithms networks actor network predicts probability distribution next action given decision step critic network estimates reward problem instance given state. paper critic network ﬁrst uses output probabilities actor network compute weighted embedded inputs then hidden layers dense layer relu activation another linear single output. section elaborate details reinforce algorithm; details appendix consider family problems denoted probability distribution them denoted training problem instances generated according distribution also distribution inference produce test examples. algorithm summarizes training algorithm. neural networks weight vectors associated actor critic respectively. draw sample problems monte carlo simulation produce feasible sequences respect current policy adopt superscript refer variables instance. termination decoding problems compute corresponding rewards well policy gradient step update actor network. reward approximation instance step second experiment study capacitated compare results existing heuristic algorithms. finally address extensions variants demonstrating experiment stochastic vrp. evaluate effectiveness framework compare route lengths solutions obtained framework given model bello random instances nodes. training phase generate instances problem size training epochs. city locations chosen uniformly unit square data distribution produce instances testing phase. table summarizes result different sizes using greedy decoder every decoding step city highest probability chosen destination. results averaged instances. ﬁrst column average tour length using proposed architecture second column result implementation bello optimal tour lengths reported last column. comparison ﬁrst columns suggests almost difference performance approaches. fact encoder pointer network learns convey information next steps i.e. hand approach around faster training inference since fewer rnns—one encoder actor network another encoder critic network. table summarizes training times epoch training time-savings gain eliminating encoder rnn. many variants extensively studied operations research laporte book toth vigo different variants problem). section consider speciﬁc capacitated version problem vehicle limited capacity responsible delivering items many geographically distributed customers ﬁnite demands. vehicle’s load runs returns depot reﬁll. denote vehicle’s remaining load time objective minimize total route length satisfying customer demands. assume node locations demands randomly generated ﬁxed distribution. speciﬁcally customers depot locations randomly generated unit square simplicity assume demand node discrete number chosen uniformly random. note however demand values generated distribution including continuous ones. assume vehicle located depot time ﬁrst input decoder embedding depot location. decoding step vehicle chooses among customer nodes depot visit next step. visiting customer node demands experiment employed different decoders greedy every decoding step node highest probability selected next destination beam search keeps track probable paths chooses highest reward. results indicate applying beam search algorithm quality solutions improved slight increase computation time. faster training generating feasible solutions used masking scheme sets log-probabilities infeasible solutions forces solution particular condition satisﬁed. following masking procedures nodes zero demand allowed visited; customer nodes masked vehicle’s remaining load exactly layer lstm decoder state size customer location also embedded vector size shared among inputs. employ similar embeddings dynamic elements; demand remaining vehicle load visiting node mapped vector -dimensional vector space used attention layer. variables actor critic network initialized xavier initialization training networks adam optimizer learning rate batch size clip gradients norm greater dropout probability decoder lstm. moreover tried entropy regularizer shown useful preventing algorithm getting stuck local optima show improvement experiments; therefore results reported below. single every training steps customer nodes takes approximately seconds. training epochs requires hours. tensorflow implementation code publicly available. savings sweep heuristic algorithms test multiple problem sizes different vehicle capacities; example consists customer nodes depot. results based instances sampled problem size. table shows average total lengths routes generated framework using greedy decoders number inside parentheses column header indicating beam-width parameter. addition also implemented randomized version heuristic algorithms improve solution quality; clarke-wright numbers inside parentheses randomization depth randomization iterations parameters sweep number random initial angles grouping nodes. observe average total length solutions found method using various decoders outperforms heuristic algorithms. also using beam search decoder signiﬁcantly improves solution adding small computational cost run-time. table also provides standard deviation parenthesis numbers close framework clarke-wright savings algorithms suggesting difference average tour length pathological examples algorithms fails. table presents solution time comparison. even though greedy clark-wright basic sweep heuristics fastest small instances provide competitive solutions. moreover larger problems framework faster randomized heuristics. training without embedding layer always yields inferior solution. possible explanation policy able extract useful features highdimensional input representations much efﬁciently. recall embedding afﬁne transformation necessarily keep embedded input distances proportional original -dimensional euclidean distances. figure shows ratio solution times number customer nodes. observe ratio stays almost framework different decoders. contrast time clarke-wright sweep heuristics increases faster linearly number nodes. observation motivation applying framework general combinatorial problems since suggests method scales well. proposed framework extended easily problems multiple depots; needs construct corresponding state transition function masking procedure. also possible incorporate various side constraints soft constraints applied penalizing rewards hard constraints time windows enforced masking scheme. however designing scheme might challenging task possibly harder solving optimization problem itself. framework also extended real-time services including on-demand deliveries taxis. major difﬁculty planning systems schedules deﬁned beforehand needs deal available nodes; largest-demand customer maximum demand chosen next destination; max-reachable vehicle chooses node highest demand making sure demand remain valid vehicle reaches node. strategies force vehicle route depot reﬁll load zero. table summarizes average demand satisﬁed percentage total demand represents various strategies averaged test instances. observe outperforms strategies. even though know information problem structure able perform better maxreachable strategy uses customer abandonment information. expect proposed architecture large potential used real-world problems improvements. possible approach studied replace reinforce algorithm policy-based value-based algorithms. also noting proposed algorithm limited important future research apply combinatorial optimization problems bin-packing job-shop ﬂow-shop. method quite appealing generate instance number nodes vehicle capacity location demand distributions ones used training trained policy work well solve problem right away without retraining every instance. long approximate generating distribution problem framework applied. requirement veriﬁer feasible solutions also reward signal demonstrate well policy working. unlike many classical heuristics proposed method scales well increasing problem size superior performance competitive solution-time. doesn’t require distance matrix calculation might computationally cumbersome especially dynamically changing vrps. also illustrate performance algorithm much complicated stochastic version vrp. various demand realization next design simulated experiment illustrate performance framework stochastic vrp. consider instance stochastic customers random demands arrive system according poisson process; without loss generality assume process rate similar previous experiments choose customer’s location uniformly unit square demand discrete number {··· depot position vehicle required satisfy much demand possible time horizon length time units. make system stable assume customer cancels demand gone unanswered time units. vehicle moves speed time unit. obviously continuous-time system view discrete-time vehicle make decisions either times customer arrivals time vehicle reaches node. network hyper-parameters experiment previous experiments. major difference training method asynchronous advantage actor-critic one-step reward accumulation. details training method described appendix difference instead using masking every time step input network available locations consists customers positive demand depot vehicle’s current location; latter decision allows vehicle stop current position necessary. also time-in-system customers dynamic element attention mechanism; allow training process learn customer abandonment behavior. references bahdanau dzmitry kyunghyun bengio yoshua. neural machine translation jointly learning align translate. international conference learning representations chen wang jiang chen liang-chieh haoyuan nevatia ram. abc-cnn attention based convolutional neural network visual question answering. arxiv preprint arxiv. kyunghyun merri¨enboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. conference empirical methods natural language processing hanjun khalil elias zhang yuyu dilkina bistra song learning combinatorial optimization algorithms graphs. advances neural information processing systems glorot xavier bengio yoshua. understanding difﬁculty training deep feedforward neural networks. proceedings thirteenth international conference artiﬁcial intelligence statistics laporte gilbert gendreau michel potvin jean-yves semet fr´ed´eric. classical modern heuristics vehicle routing problem. international transactions operational research luong minh-thang pham hieu manning christopher effective approaches attention-based neural machine translation. conference empirical methods natural language processing mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous internamethods deep reinforcement learning. tional conference machine learning xiao tianjun yichong yang kuiyuan zhang jiaxing peng yuxin zhang zheng. application two-level attention models deep convolutional neural network ﬁne-grained image classiﬁcation. proceedings ieee conference computer vision pattern recognition kelvin jimmy kiros ryan kyunghyun courville aaron salakhudinov ruslan zemel rich bengio yoshua. show attend tell neural image caption generation visual attention. international conference machine learning figure illustrates sample instances decoded trained model. greedy beam-search decoders used produce ﬁgures bottom rows respectively. evident solutions optimal. example part routes crosses itself never optimal euclidean instances. another similar suboptimality evident part make total distance shorter. however ﬁgures illustrate well policy model understood problem structure. tries satisfy demands nearby customer nodes vehicle load small. then automatically comprehends visiting nodes best decision returns depot starts tour. interesting behavior algorithm learned seen part solution reduces cost making partial delivery; example observe blue tours share customer node demand satisfying portion demand; able meet demands without needing initiate tour. also observe using beamsearch decoder produces improvements; example seen parts reduces number times tour crosses itself; reduces number tours required satisfy demands illustrated order illustrate attention mechanism working relocated customer node different locations observed affects selected action. figure illustrates attention initial decoding step instance drawn part speciﬁcally experiment coordinates node equal {··· parts small bottom left square corresponds case node located others similar interpretation. small square associated color ranging black white representing probability selecting corresponding node initial decoding step. part observe relocate node bottom-left plane positive probability directly going node; otherwise seen parts either node chosen high probability. display probabilities points since near- probability choosing them irrespective location node video demonstration model attention mechanism available online https//streamable.com/gadhf. clarke-wright savings heuristic best-known heuristics vrp. {··· customer nodes depot. distance nodes denoted distance customer depot. algorithm describes randomized version heuristic. basic idea behind algorithm initially considers separate route customer node reduces total cost iteratively merging routes. merging routes adding edge reduces total distance algorithm prefers mergers highest savings. algorithm equivalent original clarke-wright savings heuristic case feasible merger highest savings selected. allowing introduce randomization improve performance algorithm further. particular algorithm chooses randomly {··· best feasible mergers. then solves problem {··· times returns solution shortest total distance. paper refer hyper-parameters randomization depth randomization iteration respectively. figure illustration attention mechanism decoding step problem instance illustrated part nodes labeled sequential number; labels customer nodes depot. place node different locations observe affects probability distribution choosing ﬁrst action illustrated parts sweep heuristic solves breaking multiple tsps. rotating emanating depot groups nodes several clusters ensuring total demand cluster violating vehicle capacity. cluster corresponds solved using exact approximate algorithm. experiments dynamic programming optimal tour. solving tsps solution obtained combining tours. algorithm shows pseudo-code algorithm. asynchronous advantage actor-critic method proposed policy gradient approach shown achieve super-human performance playing atari games. paper utilize algorithm training policy stochastic vrp. architecture central network weights associated actor critic respectively. addition agents running parallel threads local network parameters; denote actor critic weights thread agent interacts copy time agents interacting theirs; time-step vehicle chooses next point visit receives reward goes next time-step. stochastic consider paper number demands satisﬁed time note system basically continuous-time algorithm consider discretetime running times system state changes ···}; reason normalize reward duration previous time step e.g. reward rt/. goal agent gather independent experiences agents send gradient updates central network. approach periodically update central network weights accumulated gradients send updated weight threads. asynchronous update procedure leads smooth training since gradients calculated independent instances.", "year": 2018}