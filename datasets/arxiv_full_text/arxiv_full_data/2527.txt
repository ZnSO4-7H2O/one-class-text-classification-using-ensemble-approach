{"title": "Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound $\\tilde O(\\frac{|\\mathcal S|^2 |\\mathcal A| H^2}{\\epsilon^2} \\ln\\frac 1 \\delta)$ and a lower PAC bound $\\tilde \\Omega(\\frac{|\\mathcal S| |\\mathcal A| H^2}{\\epsilon^2} \\ln \\frac 1 {\\delta + c})$ that match up to log-terms and an additional linear dependency on the number of states $|\\mathcal S|$. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finite-horizon MDPs which have a time-horizon dependency of at least $H^3$.", "text": "recently signiﬁcant progress understanding reinforcement learning discounted inﬁnite-horizon markov decision processes deriving tight sample complexity bounds. however many real-world applications interactive learning agent operates ﬁxed bounded period time example tutoring students exams handling customer service requests. scenarios often better treated episodic ﬁxed-horizon mdps looser bounds sample complexity exist. natural notion sample complexity setting number episodes required guarantee certain performance high probability paper derive upper bound match log-terms additional linear dependency number states |s|. lower bound ﬁrst kind setting. upper bound leverages bernstein’s inequality improve previous bounds episodic ﬁnitehorizon mdps time-horizon dependency least consider test preparation software tutors students national advanced placement exam taken year maximizing business revenue quarter. individual task instance requires making sequence decisions ﬁxed number steps therefore viewed ﬁnite-horizon sequential decision making uncertainty problem contrast inﬁnite horizon setting number time steps inﬁnite. domain parameters known advance opportunity repeat task many times treated episodic ﬁxed-horizon reinforcement learning important question understand much experience required well setting. formalize sample complexity reinforcement learning number time steps algorithm select action whose value near-optimal. algorithms sample complexity polynomial function domain parameters referred probably approximately correct though signiﬁcant work algorithms inﬁnite horizon setting relatively little work ﬁnite horizon scenario. paper present ﬁrst knowledge lower bound upper bound sample complexity episodic ﬁnite horizon reinforcement learning discrete state-action spaces. bounds tight log-factors time horizon accuracy number actions additive constant failure probability bounds improve upon existing results factor least results also apply reward model function within-episode time step addition state action space. assume stationary transition model results extended readily time-dependent statetransitions. proposed ucfh algorithm achieves upper guarantee applied directly wide range ﬁxed-horizon episodic mdps known rewards. require additional structure assuming access generative model state transitions sparse acyclic limited prior research upper bound results ﬁnite horizon mdps focused different settings partitioning longer trajectory ﬁxed length segments considering sliding time window tightest dependence horizon terms number episodes presented approaches least whereas dependence importantly alternative settings require optimal policy stationary whereas general ﬁnite horizon settings optimal policy nonstationary fiechter reveliotis bountourelis tackle closely related setting dependence least work builds recent work inﬁnite horizon discounted offers much tighter upper lower sample complexity bounds previously known. inﬁnite horizon algorithm ﬁnite horizon setting simple change augment state space time step enables learned policy non-stationary original state space unfortunately since recent bounds general quadratic function state space size proposed state space expansion would introduce least additional factor sample complexity term yielding least dependence number episodes sample complexity. somewhat surprisingly prove upper bound sample complexity ﬁnite horizon case scales quadratically horizon. part proof variance value function ﬁnite horizon setting satisﬁes bellman equation. also leverage recent insights state–action pairs estimated different precisions depending frequency visited policy extending ideas also handle policy followed nonstationary. lower bound analysis quite different prior inﬁnite-horizon results involves construction parallel multi-armed bandits required best certain portion bandits identiﬁed high probability achieve near-optimality. consider episodic ﬁxed-horizon mdps formalized tuple both statespace actionspace ﬁnite sets. learning agent interacts episodes time steps. time agent observes state choses action based policy potentially depends within-episode time step i.e. next state sampled stationary transition kernel initial state addition agent receives reward drawn distribution mean determined reward function. reward function possibly time-dependent takes values quality policy evaluated total expected reward episode simplicity assume reward function known agent transition kernel unknown. question study many episodes learning agent follow policy \u0001-optimal i.e. probability least chosen accuracy failure probability notation. following sections reason true empirical optimistic identical except transition probabilities ˜pt. provide details mdps later. introduce notation explicitly quantities carry additional tildes hats replacing previous works shown complexity learning state transitions usually dominates learning reward functions. therefore follow existing sample complexity analyses assume known rewards simplicity. algorithm bound extended readily case unknown reward functions. best action generally depend state number remaining time steps. tutoring example even student state knowledge optimal tutor decision space practice many days till test provide intensive short-term practice test tomorrow. straightforward reward depend state state/action state/action/next state. πi)f takes function operator returns expected value respect next time step. convenience value function time deﬁne multi-step version time deﬁned denote possible successor states state action maximum number denoted maxsa∈s×a |s|. general without making assumptions though many practical domains state transition subset full states notation similar usual o-notation ignores log-terms. precisely constants analogously natural logarithm base- logarithm. introduce model-based algorithm ucfh ﬁnite horizon episodic domains. later prove ucfh upper bound sample complexity smaller prior approaches. like many algorithms ucfh uses optimism uncertainty approach balance exploration exploitation. algorithm generally works phases comprised optimistic planning policy execution model updating take several episodes each. phases indexed agent acts environment observes tuples ucfh maintains conﬁdence possible transition parameters state-action pair consistent observed transitions. deﬁning conﬁdence holds high probability achieved using concentration inequalities like hoeffding inequality. innovation work particular conditions deﬁne conﬁdence enables obtain tighter bounds. discuss conﬁdence sets below. collection conﬁdence sets together form class mdps consistent observed data. deﬁne maximum likelihood estimate given previous observations. given ucfh computes policy performing optimistic planning. speciﬁcally ﬁnite horizon variant extended value iteration performs modiﬁed bellman backups optimistic respect given parameters. given conﬁdence possible transition model parameters selects time step model within maximizes expected future rewards. appendix provides details ﬁxed horizon evi. ucfh executes state-action pair visited often enough since last update updating model statistics -pair policy obtained optimistic planning again. refer iteration planning-execution-update phase index ambiguity omit phase indices avoid cluttered notation. ucfh inspired inﬁnite-horizon ucrl-γ algorithm lattimore hutter several important differences. first policy updated episode need explicit delay phases ucrl-γ. second policies ucfh time-dependent. finally ucfh directly deal non-sparse transition probabilities whereas ucrl-γ directly allows possible successor states -pair conﬁdence sets. class mdps consists ﬁxed-horizon mdps known true reward function transition probability time conﬁdence induced ˆp|s empirical solely purpose computationally efﬁcient optimistic planning allow time-dependent transitions affect theoretical guarantees true stationary still high algorithm ucfh upper-conﬁdence fixed-horizon episodic reinforcement learning algorithm input desired accuracy failure tolerance ﬁxed-horizon result probability least \u0001-optimal policy probability. unlike conﬁdence intervals used lattimore hutter include conditions based hoeffding’s inequality bernstein’s inequality also require analysis simplicity assume episode starts ﬁxed start state assumption crucial easily removed additional notational effort. theorem following holds. probability least ucfh produces sequence policies yield similarities analyses. proof theorem quite long involved builds similar techniques sample-complexity bounds reinforcement learning strehl littman general proof strategy closest ucrl-γ obtained bounds similar replace time horizon equivalent discounted case however important differences highlight brieﬂy. central quantity analysis lattimore hutter local variance value function. exact deﬁnition ﬁxed-horizon case given below. insight almost tight bounds lattimore hutter azar leverage fact local variances satisfy bellman equation discounted local variances bounded instead o−). prove lemma local value function variances also satisfy bellman equation ﬁxed-horizon mdps even transition probabilities rewards time-dependent. allows bound total local variances obtain similarly strong results setting. lattimore hutter assumed possible successor states allows easily relate local variances difference expected value successor states true optimistic ˜vi+j. relation less clear address proving bound tight dependencies avoid super-linear dependency ﬁnal bound additional condition equation conﬁdence set. show allows upper-bound total reward difference policy terms either depend decrease linearly number samples. gives desired linear dependency ﬁnal bound. therefore avoid assuming makes ucfh directly applicable generic mdps without impractical transformation argument used lattimore hutter introduce notion knownness importance state-action pairs essential analysis ucfh subsequently present several lemmas necessary proof theorem sketch proofs detailed proofs results available appendix. fine-grained categorization -pairs. many sample complexity proofs binary notion knownness distinguishing known unknown -pairs. however recently shown lattimore hutter inﬁnite horizon setting possible obtain much tighter sample complexity results using grained categorization. particular idea order obtain accurate estimates value function policy starting state sufﬁcient loose estimate parameters -pairs unlikely visited policy. weight -pair given policy expected frequency episode active state-action pairs unlikely current policy. intuitively model ucfh accurate categories knownness important current policy observed often far. recall time observations generated many policies condition always hold. therefore distinguish phases |xkκι| phases condition violated. condition essentially allows categories less known categories well known. fact show policy \u0001-optimal high probability phases satisfy condition. ﬁrst show validity conﬁdence sets lemma probability least proof sketch. combining hoeffding’s inequality bernstein’s inequality concentration result empirical standard deviations maurer pontil union bound probability least single phase ﬁxed ﬁxed show number model updates bounded umax apply union bound. following lemma bounds number episodes |xkκι| violated high probability. lemma number episodes |xkκι| emax proof sketch. ﬁrst bound total number times ﬁxed pair observed particular category xkκι phases |s|. show particular number episodes |xkκι| bounded high probability value implies minimum probability observing pair xkκι episode. since observations independent martingale concentration results show statement ﬁxed desired result follows union bound relevant next lemma states episodes condition |xkκι| satisﬁed true conﬁdence expected optimistic policy value close true value. lemma technically involved part proof. lemma assume |xkκι| transformations conﬁdence deﬁned since assume know ˜p|s satisfy bound bound difference expected value function successor state proving ˜vi+j| ij). ˜vt+h basic idea split bound parts partitioning space knownness ¯xκι using fact e.g. tightly coupled bound expression eventually ﬁnal pt−σth instead proof sketch theorem proof theorem consists following major parts true mdps phases probability least fixedhorizonevi algorithm computes value function whose optimistic value higher lower bound theorem exist positive constants every every algorithm satisﬁes guarantee outputs deterministic policy ﬁxed-horizon episodic mhard ranges possible similar order state-of-the-art lower bounds multi-armed bandits discounted mdps mostly determined bandit result mannor tsitsiklis build increasing parameter limits bandits would immediately result larger ranges lower bound focus analysis. proof sketch. basic idea show class mdps shown figure require least number observed episodes order equation start state agent ends states equal probability independent action. state agent transitions either good state reward state reward stays rest episode. therefore state essentially multi-armed bandit binary rewards either bandit probability ending equal except ﬁrst action possibly unknown optimal action episodic ﬁxed-horizon setting considering taking suboptimal action bandits necessarily yield suboptimal episode. consider average bandits instead. \u0001-optimal episode agent therefore needs follow policy would solve least certain portion multi-armed bandits probability least show best strategy agent achieve solve bandits equal probability. number samples required results lower bound equation similar mdps essentially solve multiple multi-armed bandits used prove lower sample-complexity bounds discounted mdps however analysis inﬁnite horizon case well sliding-window ﬁxed-horizon optimality criterion considered kakade signiﬁcantly simpler. criteria every time step agent follows policy \u0001-optimal counts mistake. therefore every time agent pick optimal multi-armed bandits counts mistake. contrasts ﬁxed-horizon setting must instead consider taking average bandits. related work fixed-horizon sample complexity bounds aware lower sample complexity bounds beyond multi-armed bandit results directly apply setting. upper bound theorem improves upon existing results least factor brieﬂy review existing results following. timestep bounds. kakade proves upper lower bounds similar setting agent interacts indeﬁnitely environment interactions divided segments equal length agent evaluated expected rewards segment. bound states agents acts \u0001-suboptimal. strehl improves state-dependency bounds delayed q-learning algorithm however episodic natural consider performance entire episode since suboptimality near episode issue long total reward entire episode sufﬁciently high. kolter interesting sliding-window criterion prove bounds bayesian setting instead pac. timestep-based bounds applied episodic case augmenting original statespace time-index episode allow resets steps. adds dependencies original bound results horizon-dependency least existing bounds. translating regret bounds ucrl corollary jaksch yields pac-bound number episodes least even ignores reset time steps. timestep-based lower pac-bounds cannot applied directly episodic reward criterion. episode bounds. similar fiechter uses value initial states optimality-criterion deﬁnes value w.r.t. γ-discounted inﬁnite horizon. results order episodes length therefore directly applicable setting. auer ortner investigate setting propose ucb-type algorithm noregret translates basic bound order episodes. improve bound substantially terms dependency reveliotis bountourelis also consider episodic undiscounted ﬁxed-horizon setting present efﬁcient algorithm cases transition graph acyclic agent knows state policy visits state known minimum probability assumptions quite limiting rarely hold practice bound order conclusion shown upper lower bounds sample complexity episodic ﬁxed-horizon tight log-factors time horizon accuracy number actions additive constant failure probability bounds improve upon existing results factor least might hope reduce dependency upper bound linear analysis similar mormax discounted mdps sample complexity linear penalty additional dependencies proposed ucfh algorithm achieves bound applied directly wide range ﬁxed-horizon episodic mdps known rewards require additional structure sparse acyclic state transitions assumed previous work. empirical evaluation ucfh interesting direction future work. acknowledgments thank lattimore helpful suggestions comments. also grateful shiau hong osband discovering small bugs previous versions paper. work supported career award young investigator program. references alexander strehl lihong eric wiewiora john langford michael littman. model-free reinforcement learning. international conference machine learning ronen brafman moshe tennenholtz. r-max general polynomail time algorithm near-optimal reinforcement learning. journal machine learning research mohammad gheshlaghi azar r´emi munos hilbert kappen. sample complexity reinforcement learning generative model. international conference machine learning spyros reveliotis theologos bountourelis. efﬁcient learning episodic tasks acyclic state spaces. discrete event dynamic systems theory applications alexander strehl lihong michael littman. incremental model-based learners formal learning-time guarantees. conference uncertainty artiﬁcial intelligence fixed-horizon extended value iteration want policy optimistic highest total reward maxπm∈mk planning relax problem instead compute policy optimistic maxπm∈m require transition probabilities convex hull conﬁdence sets instead conﬁdence sets. since relaxation policy dynamic programming similar extended value iteration optimal q-function computed feasible deﬁned conv|s n))}. optimal policy time simply maximizer inner operator transition probability maximizer outer maximum. inner solved efﬁciently enumeration outer maximum similar extended value iteration basic idea much probability mass possible successor states highest value. following algorithm implementation details. function fixedhorizonevi maxa∈a return transition probabilities optimal policy note nonlinear constraint equation confidenceset|s union disjoint intervals instead interval. still minmax-operations conﬁdence sets computed readily constant time. therefore transition probabilities single time step state-action pair computed given sorted states. sorting states takes results runtime complexity fixedhorizonevi algorithm requires additional space besides storage requirements input transition probabilities returned algorithm. required optimal policy interest additional space reduced lemma fixedhorizonevi returns maxm∈m proof sketch. result proved straight-forwardly showing optimal last time step highest possible reward subsequently previous time steps inductively. follows directly deﬁnition algorithm function fixedhorizonevi returned ciently high probability. results total runtime sampling update policy involves updating variables takes runtime call fixedhorizonevi runtime cost lemma below know policy updated umax times gives total runtime policy updates bound number policy changes ucfh lemma total number updates bounded umax proof. first note never never decreasing updates happen |s|mh exactly pair increases max{mwmin single pair updates happen times. hence proof lemma capturing true proof. single pair treat event successor state chosing action bernoulli random variable probability using hoeffding’s inequality realize while considered random variables strictly speaking necessarily independent treated concentration inequalities applied here. appendix strehl littman details. lemma umax updates umax different consider. since update single pair successor states updated umaxc different ˆp|s consider. applying union bound probability umaxcδ. setting desired result. presenting proof lemma bounds total number episodes |xkκι| establish bound individual following additional lemmas. lemma total number observations xkκι phases ×a|mwικ. variable smallest possible weight -pair importance prove lemma sufﬁcient consider ﬁxed phase avoid notational clutter therefore omit phase indices section. proof reason sequence mdps transition probabilities different reward functions reward function original reward function i.e. following reward functions deﬁned recursively local variance value function w.r.t. rewards note every complete analogy deﬁne ˆmd. ﬁrst prove sequence lemmas necessary lemma lemma inequality wrote deﬁnition applied triangle inequality. applied assumed bound bounded ˜vi+j)− vi+j∞ value functions nonnegative. inequality applied cauchy-schwarz inequality subsequently used fact term nonnegative ﬁnal equality follows deﬁnition ˜σij. ﬁrst equality follows lemma second step fact vt+h non-expansive. third introduce indicator function change value pairs. fourth step relies linearity operators. ﬁfth step realize πt}| function takes nonzero values input therefore replace argument second term without changing value. term becomes constant linearity write second inequality split pairs used fact non-expansive i.e. t+h∞ apply lemma subsequently terms nonnegative deﬁnition eventually using assumption ﬁrst split applied cauchy-schwarz inequality. used inequality |xκι| fourth step applied cauchy-schwarz ﬁnal inequality follows fact non-expansive. alternatively rewrite bound equation ﬁrst inequality used bound second inequality simpliﬁed expression noting terms nonnegative. next step re-parameterized sum. ﬁnal inequality used assumption therefore \u0001−−i proof theorem proof theorem lemma know number episodes |xκι| bounded emax|s ×a|m probability least episodes lemma ˜rπk rπk| since probability least lemma lemma gives ˜rπk conclude probabilty least episodes |xκι| applying union bound desired result satisﬁes proof theorem consider class mdps shown figure mdps essentially consist parallel multi-armed bandits. bandit exist possible instantiations denote instantiation hypothesis corresponds a}\u0001/ action small bias. hypotheses correspond a}\u0001/ aj}\u0001. indicate instance entire mdp. deﬁne aii} event policy generated chooses optimally bandit given instance difference optimal expected cumulative reward chose value specify exact value parameter later. condition basically states least fraction bandits need solved optimally resulting policy \u0001-accurate. -correct therefore need i{gi} poissonbinomial distributed i{gi} independent bernoulli random variables potentially different mean. therefore upper bounds must exist hypotheses since independent shown lemma supplementary material optimal solution optimization problem equation since left-hand side condition decreasing plug lower bound sufﬁcient condition proof. without indicator part objective show optimal solution checking conditions noting problem convex. denote number indicator function without loss generality assume value remaining take value problem transforms", "year": 2015}