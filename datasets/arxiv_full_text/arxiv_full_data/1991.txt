{"title": "Reinforcement Learning Using Quantum Boltzmann Machines", "tag": ["quant-ph", "cs.AI", "cs.LG", "cs.NE", "math.OC"], "abstract": "We investigate whether quantum annealers with select chip layouts can outperform classical computers in reinforcement learning tasks. We associate a transverse field Ising spin Hamiltonian with a layout of qubits similar to that of a deep Boltzmann machine (DBM) and use simulated quantum annealing (SQA) to numerically simulate quantum sampling from this system. We design a reinforcement learning algorithm in which the set of visible nodes representing the states and actions of an optimal policy are the first and last layers of the deep network. In absence of a transverse field, our simulations show that DBMs train more effectively than restricted Boltzmann machines (RBM) with the same number of weights. Since sampling from Boltzmann distributions of a DBM is not classically feasible, this is evidence of advantage of a non-Turing sampling oracle. We then develop a framework for training the network as a quantum Boltzmann machine (QBM) in the presence of a significant transverse field for reinforcement learning. This further improves the reinforcement learning method using DBMs.", "text": "abstract. investigate whether quantum annealers select chip layouts outperform classical computers reinforcement learning tasks. associate transverse ﬁeld ising spin hamiltonian layout qubits similar deep boltzmann machine simulated quantum annealing numerically simulate quantum sampling system. design reinforcement learning algorithm visible nodes representing states actions optimal policy ﬁrst last layers deep network. absence transverse ﬁeld simulations show dbms train eﬀectively restricted boltzmann machines number weights. since sampling boltzmann distributions classically feasible evidence advantage non-turing sampling oracle. develop framework training network quantum boltzmann machine presence signiﬁcant transverse ﬁeld reinforcement learning. improves reinforcement learning method using dbms. keywords. reinforcement learning machine learning neuro-dynamic programming markov decision process quantum monte carlo simulation simulated quantum annealing restricted boltzmann machine deep boltzmann machine general boltzmann machine quantum boltzmann machine quantum supremacy view recent advancements manufacturing superconducting qubits systems qubits second-order interactions important question quantum computation existence quantum supremacy whether near-term quantum devices outperform classical computers some—in fact any—computational task. motivation consider reinforcement learning computational task interest design method reinforcement learning using layout quantum bits similar deep boltzmann machine simulated quantum annealing demonstrate advantage information technologies vancouver british columbia canada department mathematics university british columbia vancouver british columbia canada school engineering science simon fraser university burnaby british columbia canada reinforcement learning known also neurodynamic programming area optimal control theory intersection approximate dynamic programming machine learning. used successfully many applications ﬁelds engineering sociology economics important diﬀerentiate reinforcement learning common streams research machine learning. instance supervised learning learning facilitated training samples provided source external agent computer. reinforcement learning training samples provided interaction agent environment. example motion planning problem uncharted territory desired agent learns fastest possible correctly navigate least number blind decisions required taken. known dilemma exploration versus exploitation; neither exploration exploitation pursued exclusively without facing penalty failing task. goal hence design algorithm eventually converges optimal policy figure general layout used rbm-based reinforcement learning visible layer left consists state action nodes connected hidden layer forming complete bipartite graph. general layout used dbm-based reinforcement learning visible nodes left represent states visible nodes right represent actions. training procedure captures correlations states actions weights edges nodes. able generate good policies early learning process. refer reader thorough introduction cases problem scenarios addressed reinforcement learning. core idea reinforcement learning deﬁning operator banach space real-valued functions states system ﬁxed point operator carries information optimal policy actions ﬁnite inﬁnite number decision epochs. numerical method computing ﬁxed point explore function space travelling direction minimizes distance consecutive applications contraction mapping operator optimization task called learning context reinforcement learning performed locally parametrizing function space using auxiliary variables applying gradient method variables. approach parametrization weights restricted boltzmann machine parameters free energy approximator follows universal approximation theorem rbms approximate joint distribution binary variables however context reinforcement learning rbms necessarily best choice approximating q-functions relating markov decision processes show rbms require exponential number hidden variables respect number visible variables order approximate desired joint distribution. hand dbms potential model higherorder dependencies rbms robust deep belief networks therefore consider replacing graphical models investigating performance models learning process. except case rbms calculating statistical data nodes graphical model amounts sampling boltzmann distribution creating bottleneck learning procedure performed classically real number representing penalty. previous maze additional stochastic rewards. optimal actions cell maze optimal traversal policy choice combination actions. sample conditional state transition probability windy problem obstacles wall present explain follows dbms good candidates reinforcement learning tasks. moreover important advantage layout quantum annealing system proximity couplings qubits layout similar sequence bipartite blocks d-wave devices therefore feasible layouts could manufactured near future. instead attempting embed boltzmann machine structure existing quantum annealing system work assumption network native connectivity graph near-future quantum annealer using numerical simulations attempt understand applicability reinforcement learning. quantum monte carlo numerical simulations found useful simulating timedependant quantum systems. simulated quantum annealing many ﬂavours methods based suzuki–trotter expansion path integral representation hamiltonian ising spin models presence transverse ﬁeld driver hamiltonian. even though eﬃciency ﬁnding ground state ising model topologically obstructed consider samples generated good approximations boltzmann distribution quantum hamiltonian experimental studies shown similarities behaviour quantum annealing physical realization d-wave systems. expect ﬁnal strength transverse ﬁeld negligible distribution samples approaches classical limit expects observe absence transverse ﬁeld. classical counterpart conventional simulated annealing based thermal annealing. algorithm used create boltzmann distributions ising spin model absence transverse ﬁeld. should therefore possible approximate boltzmann distribution classical boltzmann machine. however unlike possible approximate boltzmann distribution classical boltzmann machine also graphical model energy operator quantum hamiltonian presence simulations demonstrate evidence quantum annealing device approximates distribution improve learning process compared reinforcement learning method uses classical techniques. studies shown eﬃcient thermal therefore method conjunction also viewed quantum-inspired approach reinforcement learning. distinguishes work current trends quantum machine learning consider quantum annealing reinforcement learning applications rather frequently studied classiﬁcation recognition problems; using sqa-based numerical simulations assume connectivity graph directly maps native layout feasible quantum annealer; results experiments using simulate sampling entangled system spins suggest using quantum annealers reinforcement learning tasks advantage thermal sampling. maze traversal problem typically used develop benchmark reinforcement learning algorithms maze structured two-dimensional grid rows columns decision-making agent free move down left right stand still. maze traversal agent encounters obstacles rewards penalties cell maze contain either deterministic stochastic reward wall neutral value. fig. show examples mazes. fig. shows corresponding solutions maze fig. goal reinforcement learning algorithm maze traversal problem agent learn optimal action take cell maze maximizing total reward ﬁnding route across taking action state value contained destination state. moving cell containing reward returns favourable value moving cell containing penalty returns unfavourable value moving cell reward returns neutral value interval since interested dependencies states actions consider architecture layer states connected ﬁrst layer hidden nodes followed multiple hidden layers layer actions connected ﬁnal layer hidden nodes demonstrate advantages deep architecture trained using derivation sec. temporal-diﬀerence gradient method reinforcement learning using general boltzmann machines independent training runs reinforcement learning algorithm training samples used reinforcement learning. ﬁdelity measure i-th training sample deﬁned includes reward wall fig. maze includes stochastic rewards addition. experiments training samples generated sweeping maze. sweep iterates maze elements order. explains periodic behaviour ﬁdelity curves curves labelled ‘qbm-rl’ represent ﬁdelity reinforcement learning using qbms. sampling performed using sqa. experiments classical boltzmann machines graphical model. experiment labelled ‘rbm-rl’ graphical model trained classically using formula remaining curve labelled ‘dbm-rl’ classical reinforcement learning using dbm. experiments sampling conﬁgurations performed ﬁdelity results dbm-rl coincide closely sampling conﬁgurations using simulated annealing; constant experiments discount factor discount factor feature problem rather free parameter implementation. example ﬁnancial application scenario discount factor might function risk-free interest rate. study performance temporal-diﬀerence reinforcement learning algorithms using boltzmann machines. generalize method introduced compare policies obtained algorithms optimal policy using ﬁdelity measure deﬁne tivations hidden nodes machine learning terminology). explained sec. main advantage explicit formulas hiddennode activations given values visible nodes. moreover rbms entropy portion free energy written terms activations hidden nodes. more-complicated network architectures possess property need boltzmann distribution sampler. figure comparison rbm-rl dbm-rl qbm-rl training results. every underlying hidden nodes every layers hidden nodes. shaded areas indicate standard deviation training algorithm. ﬁdelity curves three algorithms maze ﬁdelity curves maze ﬁdelity curves mentioned three algorithms corresponding experiment except training performed uniformly generated training samples rather sweeping maze. ﬁdelity curves corresponding windy maze similar figure comparison performance rbm-rl dbm-rl size maze grows. boltzmann machines hidden nodes. schematics maze deterministic reward stochastic rewards walls. scaling average ﬁdelity algorithm instance maze. dotted line average ﬁdelity uniformly randomly generated policies. ﬁdelity curves fig. show dbm-rl algorithm outperforms rbm-rl algorithm respect number training samples. therefore expect conjunction high-performance sampler demonstrate performance rbm-rl dbmrl respect scaling deﬁne another measure called average ﬁdelity take average ﬁdelity last training samples ﬁdelity measure. given total training samples deﬁned above write boltzmann distributions dbm-rl algorithm improves performance reinforcement learning. qbm-rl algorithm improves upon dbm-rl results even taking advantage sampling presence signiﬁcant transverse ﬁeld. fig. shows that whereas maze solved fewer training samples using ordered sweeps maze periodic behaviour ﬁdelity curves periodic choice training samples. eﬀect disappears training samples chosen uniformly randomly. fig. shows improvement learning dbm-rl qbm-rl algorithms persists case more-complicated transition kernels. ordering ﬁdelity curves discussed earlier observed qbmrl outperforms dbm-rl dbm-rl outperforms rbm-rl. worth mentioning that even though seem connectivity hidden nodes allow boltzmann machine capture more-complicated correlations visible nodes training process boltzmann machine becomes computationally involved. reinforcement learning application hidden nodes visible nodes weights train. hidden layers equal size weights train. therefore training domain lower dimension. further hidden nodes forming complete graph reone observe fig. that maze size increases complexity reinforcement learning task increases decreases algorithm. algorithm always outperformed dbmrl shows much faster decay average ﬁdelity function maze size compared dbm-rl. larger mazes algorithm fails capture maze traversal knowledge approaches random action allocation whereas dbm-rl algorithm continues trained reasonably well. dbm-rl capable training agent traverse larger mazes whereas algorithm utilizing number hidden nodes larger number weights fails converge output better random policy. given ordering ﬁdelity curves discussed above expect qbm-rl perform even better dbm-rl problem sizes increase especially presence non-zero ﬁnal transverse ﬁeld. runtime computational resources needed dbm-rl qbm-rl comparison rbm-rl investigated here. expect view size rbms needed solve larger maze problems grow exponentially. therefore interesting research path pursue extrapolation asymptotic complexity size dbm-rl qbmrl algorithms quest quantum supremacy. section present details classical reinforcement learning using semi-classical approach base quantum reinforcement learning pseudo-code methods provided algorithms below. methods class quantum-inspired algorithms perform discrete optimization classically simulating quantum tunnelling phenomena introduction). algorithm used paper single spin-ﬂip version quantum monte carlo numerical simulation based suzuki–trotter formula uses metropolis acceptance probabilities. algorithm simulates quantum annealing phenomena ising spin model transverse ﬁeld hamiltonian system dimension higher representing quantum hamiltonian non-negligible alternatively classical monte carlo simulation used sample boltzmann distribution classical ising hamiltonian algorithm based thermal ﬂuctuations classical spin systems. algorithm recall steps classical reinforcement learning algorithm using graphical model similar shown fig. initial boltzmann machine weights using gaussian zero-mean values standard deviation common practice implementing boltzmann machines consequently initializes approximation q-function policy given represent pauli zx-matrices respectively time ranges quantum evolution strength transverse ﬁeld slowly reduced zero ﬁnite temperature. implementations used linear transverse ﬁeld schedule algorithm based suzuki–trotter formula idea algorithm approximate partition function ising model transverse ﬁeld partition function experiments strength transverse ﬁeld scheduled linearly decrease inverse temperature constant spin replicated times represent trotter slices extra dimension. simulation iterate replications spins time sweep number sweeps instance input algorithm times. termination conﬁguration replica well conﬁguration entire classical ising model dimension higher returned. although algorithm follow dynamics physical quantum annealer explicitly used simulate process since captures major quantum phenomena tunnelling entanglement example shown quantum monte carlo simulations used understand tunnelling behaviour quantum annealers. mentioned previously readily follows results limiting distribution boltzmann distribution used averages taken replica run; hence samples conﬁgurations hidden nodes state–action pair. strength transverse ﬁeld scheduled linearly decrease algorithm used linear inverse temperature schedule increases sweeps times. used sample points used approximation. results dbm-rl using match signiﬁcant diﬀerence. algorithm summarize dbm-rl method. here graphical model boltzmann machine similar shown fig. initialization weights performed similar fashion last algorithm qbm-rl presented algorithm initialization performed algorithms however according lines samples algorithm used approximate free energy points computing free energy corresponding eﬀective classical ising spin model dimension higher representing quantum ising spin model action removed graph conﬁgurations spins corresponding hidden nodes sampled using ising spin model constructed follows state contributes bias adjacent action contributes bias adjacent bias spin hidden node adjacent state action zero. subsequent state obtained state–action pair using transition kernel outlined sec. corresponding action chosen policy another sampling performed similar fashion pair. section derive q-learning method markov decision processes function approximator represented general boltzmann machine best knowledge derivation previously given although readily derived ideas presented choice action every state independent point time controlled process reaches stationary policy reduces time-homogeneous markov chain transition q-functions. stationary policy qfunction deﬁned mapping pair expected value reward markov chain begins taking action initial state continuing according denoting weights visible hidden visible visible hidden hidden nodes boltzmann machine respectively deﬁned function binary vectors corresponding visible hidden variables respectively. clamped neural network whose underlying graph subgraph obtained removing visible nodes eﬀect ﬁxed assignment visible binary variables contributes constant coefﬁcients associated energy clamped quantum boltzmann machine underlying graph clamped instead binary random variable qubit associated node network. energy function substituted quantum hamiltonian diagonal matrix σz-basis spectrum identical range remainder section formulated clamped qbms acknowledging easily specialized clamped classical boltzmann machines. clamped boltzmann machines. classical boltzmann machine type stochastic neural network sets visible hidden nodes respectively. visible hidden nodes represent binary random variables. notation node binary random variable represents. interactions variables represented respective nodes speciﬁed real-valued weighted edges underlying undirected graph. opposed models rbms dbms allows weights nodes. sical ising model dimension higher constructed sqa. approximate q-function take advantage applied classical ising model. precisely represent hamiltonian classical ising model dimension higher associated energy function free energy model written reinforcement learning clamped boltzmann machines. following ideas goal negative free energy boltzmann machine approximate q-function relationship admissible state–action pair here binary vectors encoding state action state nodes action nodes respectively boltzmann machine. recall reinforcement learning visible nodes partitioned subsets state nodes action nodes would like thank hamed karimi helmut katzgraber murray thom matthias troyer ehsan zahedinejad reviewing work providing many helpful suggestions. idea using experiments involving measurements non-zero transverse ﬁeld communicated person mohammad amin. would also like thank marko bucyk editing manuscript.", "year": 2016}