{"title": "Variational Inference for Uncertainty on the Inputs of Gaussian Process  Models", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG", "60G15 (Primary), 58E30", "G.3; G.1.2; I.2.6; I.5.4"], "abstract": "The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximized over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximizing an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from iid observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the nonlinear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain inputs and semi-supervised Gaussian processes. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data.", "text": "andreas damianou∗ dept. computer science shefﬁeld institute translational neuroscience university shefﬁeld michalis titsias∗ department informatics athens university economics business greece neil lawrence dept. computer science shefﬁeld institute translational neuroscience university shefﬁeld gaussian process latent variable model provides ﬂexible approach non-linear dimensionality reduction widely applied. however current approach training gp-lvms based maximum likelihood latent projection variables maximized rather integrated out. paper present bayesian method training gp-lvms introducing non-standard variational inference framework allows approximately integrate latent variables subsequently train gp-lvm maximizing analytic lower bound exact marginal likelihood. apply method learning gp-lvm observations learning non-linear dynamical systems observations temporally correlated. show beneﬁt variational bayesian procedure robustness overﬁtting ability automatically select dimensionality nonlinear latent space. resulting framework generic ﬂexible easy extend purposes gaussian process regression uncertain inputs semi-supervised gaussian processes. demonstrate method synthetic data standard machine learning benchmarks well challenging real world datasets including high resolution video data. keywords gaussian process variational inference dynamical systems latent variable models dimensionality reduction consider linear function general class probability densities recovered mapping simpler density linear function. example might decide drawn gaussian density models form appear several domains. used autoregressive prediction time series prediction regression model output input uncertain mackay considered form dimensionality reduction several latent variables {xj}q used represent high dimensional vector {yj}p adding dynamical component nonlinear dimensionality reduction approaches leads nonlinear state space models states often physical interpretation propagated time autoregressive manner intractabilities mapping distribution nonlinear function resulted range different approaches. density networks sampling proposed; particular importance sampling used. extending importance samplers dynamically degeneracy weights needs avoided thus leading resampling approach suggested bootstrap particle ﬁlter gordon approaches nonlinear state space models include laplace approximation used extended kalman ﬁlters unscented ensemble transforms dimensionality reduction generative topographic mapping obtain posterior process represents functions consistent data prior. initial focus application gaussian process models context dimensionality reduction. dimensionality reduction assume high dimensional data really result dimensional control signals perhaps nonlinearly related observed functions. words assume data approximated lower dimensional matrix vector valued function represents observed data point approximated data lower dimensional subspace immersed original high dimensional space. mapping linear e.g. methods like principal component analysis factor analysis independent component analysis follow. gaussian marginalization latent variable tractable placing gaussian density afﬁne transformation retains gaussianity data density however linear assumption restrictive natural look beyond linear mapping. context dimensionality reduction range approaches suggested consider neighborhood structures preservation local distances dimensional representation. machine learning community spectral methods isomap locally linear embeddings laplacian eigenmaps attracted attention. spectral approaches closely related kernel classical multi-dimensional scaling methods probabilistic interpretation described lawrence explicitly include assumption underlying reduced data dimensionality. iterative methods metric non-metric approaches sammon mappings t-sne also lack underlying generative model. probabilistic approaches generative topographic mapping density networks view dimensionality reduction problem different perspective since seek mapping low-dimensional latent space observed data space come certain advantages. precisely generative nature forward mapping deﬁne allows extended easily various ways incorporated bayesian framework parameter learning handle missing data. approach dimensionality reduction provides useful archetype algorithmic solutions providing paper require approximations allow latent variables propagated nonlinear function. framework takes generative approach prescribed density networks nonlinear variants kalman ﬁlters step further. because rather considering speciﬁc function latent variables data space consider entire family functions. subsumes restricted class either gauss markov processes bayesian basis function models models cast within framework gaussian processes gaussian processes probabilistic kernel methods kernel interpretation covariance associated prior density. covariance speciﬁes distribution functions subsumes special cases mentioned above. gaussian process latent variable model recent probabilistic dimensionality reduction method proven robust high dimensional problems gp-lvm seen non-linear generalisation probabilistic also bayesian interpretation contrast ppca non-linear mapping gp-lvm makes bayesian treatment much challenging. therefore gp-lvm extensions rely maximum posteriori training procedure. however principled bayesian formulation highly desirable since would allow robust training model automatic selection latent space’s dimensionality well intuitive exploration latent space’s structure. paper formulate variational inference framework allows propagate uncertainty gaussian process obtain rigorous lower bound marginal likelihood resulting model. procedure followed non-standard computation closed-form jensen’s lower bound true marginal likelihood data infeasible classical approaches variational inference. instead build signiﬁcantly extend variational method titsias prior augmented include auxiliary inducing variables approximation applied expanded probability model. resulting framework deﬁnes approximate bound evidence gp-lvm which optimised gives by-product approximation true posterior distribution latent variables given data. considering posterior distribution rather point estimates latent points means framework generic easily extended multiple practical scenarios. example treat latent points noisy measurements given inputs obtain method gaussian process regression uncertain inputs limit partially observed inputs. hand considering latent space prior depends time vector allows obtain bayesian model dynamical systems signiﬁcantly extends classical kalman ﬁlter models nonlinear relationship state space observed data along non-markov assumptions latent space based continuous time observations. achieved placing gaussian process prior latent space function time approach trivially extended replacing time time dependency prior latent space spatial dependency dependency arbitrary number high dimensional inputs. long valid covariance function derived leads bayesian approach warped gaussian process regression next section review main prior work dealing latent variables context gaussian processes describe model extended dynamical component. introduce variational framework bayesian training procedure section section describe variational approach applied range predictive tasks demonstrated experiments conducted simulated real world datasets section section discuss experimentally demonstrate natural important extensions model motivated situations inputs fully unobserved. extensions give rise auto-regressive variant performing iterative future predictions semi-supervised variant. finally based theoretical experimental results work present ﬁnal conclusions section section provides background material current approaches learning using gaussian process latent variables models speciﬁcally section speciﬁes general structure models section reviews standard gp-lvm i.i.d. data well dynamic extensions suitable sequence data. finally section discusses drawbacks estimation latent variables currently standard train gp-lvms. here individual components taken independent draws gaussian process kernel covariance function determines properties latent mapping. shown linear covariance function makes gp-lvm equivalent traditional ppca. hand nonlinear covariance functions considered model able perfom non-linear dimensionality reduction. non-linear covariance function considered exponentiated quadratic inﬁnitely many times differentiable uses common lengthscale parameter latent dimensions. covariance function results non-linear smooth mapping latent data space. parameters appear covariance function often referred kernel hyperparameters denoted throughout paper. covariance matrix deﬁned kernel function inputs kernel matrix latent random variables following prior distribution hyperparameters structure prior depend application hand whether observed data i.i.d. sequential dependence. remaining section shall leave unspeciﬁed keep discussion general speciﬁc forms given next section. comes directly assumed noise model equation come latent space. discussed detail section interplay latent variables makes inference challenging. however ﬁxing treat analytically marginalise follows omit refererence parameters order simplify notation. partial tractability model gives rise straightforward training procedure latent inputs selected according approach suggested lawrence subsequently followed authors finally notice point estimates hyperparameters also found maximising objective function. different gp-lvm algorithms result varying structure prior distribution latent inputs. simplest case suitable i.i.d. observations obtained selecting fully factorized latent space prior structured latent space priors also used could incorporate available information problem hand. example urtasun darrell discriminative properties gplvm considering priors encapsulate class-label information. existing approaches literature seek constrain latent space smooth dynamical prior obtain model dynamical systems. example wang extend gp-lvm temporal prior encapsulates markov property resulting auto-regressive model. extend models bayesian ﬁltering robotics setting whereas urtasun consider idea tracking. similar direction lawrence moore consider additional temporal model employs prior able generate smooth paths latent space. paper shall focus dynamical variants dynamics regressive setting data assumed multivariate timeseries ti}n time datapoint observed. gp-lvm dynamical model obtained deﬁning temporal latent function individual components taken independent draws gaussian process covariance function. datapoint assumed produced latent vector shown figure latent vectors stored matrix follows correlated prior distribution covariance matrix obtained evaluating covariance function observed times contrast fully factorized prior prior couples elements covariance function parameters determines properties temporal function instance ornstein-uhlbeck covariance function yields gauss-markov process exponentiated quadratic covariance function gives rise smooth non-markovian process. speciﬁc choices forms covariance functions used experiments discussed section current gp-lvm based models found literature rely training procedures discussed section optimizing latent inputs hyperparameters. however approach several drawbacks. firstly fact marginalise latent inputs implies could sensitive overﬁtting. further objective function cannot provide insight selecting optimal number latent dimensions since typically increases dimensions added. gp-lvm algorithms found literature require latent dimensionality either hand selected cross-validation. latter case renders whole training computationally slow practice limited subset models explored reasonable time. another consequence above current gp-lvms employ simple covariance functions complex covariance functions could help automatically select latent dimensionality popular. latter covariance function exponentiated quadratic different lengthscale input dimension covariance function could allow automatic relevance determination procedure take place unnecessary dimensions latent space assigned weight value almost zero. however standard training approach beneﬁts using covariance function cannot realised typically overﬁtting occur. therefore clear development fully bayesian approaches training gplvms could make models reliable provide rigorous solutions limitations training. variational method presented next section approach that demonstrated experiments shows great ability avoiding overﬁtting permits automatic selection latent dimensionality. section describe detail proposed method based non-standard variational approximation utilises auxiliary variables. resulting class training algorithms referred variational gaussian process latent variable models simply variational gp-lvms. start section explain obstacles need overcome applying variational methods gp-lvm speciﬁcally standard mean ﬁeld approach immediately tractable. section show auxiliary variables together certain variational distribution results tractable approximation. section give speciﬁc details apply framework different gp-lvm variants paper concerned with standard gp-lvm dynamical/warped one. finally outline extensions variational method enable application speciﬁc modelling scenarios. section explain multiple independent time-series accommodated within dynamical model section describe simple trick makes model applicable vast dimensionalities. bayesian treatment gp-lvm requires computation marginal likelihood associated joint distribution equation sets unknown random variables marginalised mapping values latent space thus difﬁculty bayesian approach propagating prior density nonj= term given proportional clearly term contains inputs kernel matrix rather complex nonlinear manner therefore analytical integration infeasible. make progress invoke standard variational bayesian methodology approximate marginal likelihood equation variational lower bound. speciﬁcally introduce factorised variational distribution unknown random variables ﬁrst term equation contains expectation distribution requires integration appears nonlinearly cannot done analytically. therefore standard mean ﬁeld variational methodologies lead analytically tractable variational lower bound. contrast framework allows compute closed-form jensen’s lower bound applying variational inference expanding prior include auxiliary inducing variables. originally inducing variables introduced computational speed regression models approach extra variables used within variational sparse framework titsias speciﬁcally expand joint probability model including extra samples latent mapping sample. inducing points collected matrix rm×p constitute latent function evaluations marginal prior inducing variables. expressions denotes covariance matrix constructed evaluating covariance function inducing points cross-covariance inducing latent points figure graphically illustrates augmented probability model. figure graphical model gp-lvm augmented auxiliary variables obtain variational gp-lvm model dynamical version shaded nodes represent observed variables. general level input arbitrary depending application. notice likelihood equivalently computed augmented model marginalizing crucially true value inducing inputs means that unlike inducing inputs random variables neither model hyperparameters; variational parameters. interpretation inducing inputs developing approximation arises variational approach titsias taking advantage observation simplify notation dropping expressions. arbitrary variational distribution. choose gaussian factorise across latent dimensions datapoints discussed section choice depend form prior distribution time being however shall proceed assuming general form gaussian. particular choice variational distribution allows analytically compute lower bound. reason behind conditional prior term appears joint density also part variational distribution. indeed making equations derivation lower bound follows notice optimally eliminating obtain tighter bound longer depends distribution i.e. also notice expectation appearing equation standard gaussian integral calculated closed form turns computation requires compute matrix inverses determinants involve instead something tractable since depend therefore expression straightforward compute long covariance function selected quantities equation computed analytically. worth noticing statistics computed decomposable since covariance matrices appearing evaluated pairs inputs taken respectively. particular statistics written sums independent terms term associated data point similarly column matrix associated data point. decomposition useful data vector inserted model also help speed computations test time discussed section also allow parallelization computations suggested therefore averages covariance matrices equation statistics computed separately marginal taken full equation notice statistics constitute convolutions covariance function gaussian densities tractable many standard covariance functions exponentiated quadratic linear one. analytic forms statistics aforementioned covariance functions given appendix lower bound jointly maximized model parameters variational parameters applying gradient-based optimization algorithm. approach similar optimization objective function employed standard gp-lvm main difference instead optimizing random variables optimize variational parameters govern approximate posterior mean variance furthermore inducing inputs variational parameters optimisation simply improves approximation similarly variational sparse regression investigating carefully resulting expression bound allows observe term depends single column data closely resembles corresponding variational lower bound obtained applying method titsias standard sparse regression. difference variational gp-lvm marginalized terms containing i.e. kernel quantities ukuf transformed averages respect variational distribution finally notice application variational method developed paper restricted latent points. fully bayesian approach obtained additionally placing priors kernel parameters subsequently integrating variationally methodology described section. different variational gp-lvm algorithms obtained varying form latent space prior left unspeciﬁed. useful property variational lower bound appears separate divergence term seen equation tractably computed gaussian. allows framework easily accommodate different gaussian forms latent space prior give rise different gp-lvm variants. particular incorporating speciﬁc prior mainly requires specify suitable factorisation compute corresponding term. contrast general structure complicated term remains unaffected. next demonstrate ideas giving details apply variational method gp-lvm variants discussed section cases follow recipe factorisation variational distribution resembles factorisation prior simplest case latent space prior standard normal density fully factorised across datapoints latent dimensions shown typical assumption latent variable models factor analysis ppca choose variational distribution follows factorisation prior denotes diagonal matrix resulting taking logarithm diagonal elements. train model simply need substitute term ﬁnal form variational lower follow gradient-based optimisation procedure. resulting variational gp-lvm seen non-linear version bayesian probabilistic experiments consider model non-linear dimensionality reduction demonstrate ability automatically select latent dimensionality. term substituted ﬁnal form variational lower bound allow training using gradient-based optimisation procedure. implemented naively procedure require many parameters tune since variational distribution depends free parameters. however applying reparametrisation trick suggested opper archambeau reduce number parameters variational distribution speciﬁcally here diagonal positive deﬁnite matrix n−dimensional vector. stationary conditions tell that since depends diagonal matrix reparametrise using diagonal elements matrix denoted n−dimensional vector then optimise parameters obtain original parameters using transformation optimisation strategies depending choose treat newly introduced parameters ¯µj. firstly inspired opper archambeau construct iterative optimisation scheme. precisely variational bound equation depends actual variational parameters equation depend newly introduced quantities which turn associated equation observations lead em-style algorithm alternates estimating parameter sets {ms} keeping ﬁxed. alternative approach implementation treat parameters completely free ones equation never used. case variational parameters optimised directly gradient based optimiser jointly model hyperparameters inducing inputs. overall reparameterisation appealing improved complexity also optimisation robustness. indeed equation conﬁrms original variational parameters coupled full-rank covariance matrix. reparametrising according equation treating parameters free ones manage approximately break coupling apply optimisation algorithm less correlated parameters. furthermore methodology described readily applied model dependencies different nature kind high dimensional input variable replace temporal inputs graphical model therefore simply replacing input kind observed input trivially obtain bayesian framework warped regression predict latent function values inputs non-linear latent warping layer using exactly architecture equations described section section similarly observed inputs layer taken outputs themselves obtain probabilistic auto-encoder non-parametric based gaussian processes. finally dynamical variational gp-lvm algorithm easily extended deal datasets consisting multiple independent sequences arising human motion capture applications. example dataset group independent commonality data. handle allowing different temporal latent function independent sequences latent variables corresponding sequence sets priori assumed independent since correspond separate sequences i.e. factorisation leads block-diagonal structure time covariance matrix block corresponds sequence. setting block variational framework makes inducing point representations provide low-rank approximations covariance standard variational gp-lvm allows avoid typical cubic complexity gaussian processes reducing computational cost since typically select small inducing points variational gp-lvm handle relatively large training sets dynamical variational gp-lvm however still requires inversion covariance matrix size seen equation thereby inducing computational cost further models scale linearly number dimensions speciﬁcally number dimensions matters performing calculations involving data matrix ﬁnal form lower bound matrix appears form precomputed. means that calculate substitute work instead matrix. practically speaking allows work data sets involving millions features. experiments model directly pixels quality video exploiting trick. section explain proposed bayesian models accomplish various kinds prediction tasks. star denote test quantities e.g. test data matrix denoted n∗×p test column vectors matrix denoted y∗j. ﬁrst type inference interested calculation probability density computation quantity allow model density estimator which instance represent class conditional distribution generative based classiﬁcation system. exploit section secondly discuss test data matrix probabilistically reconstruct unobserved part based observed part denote non-overlapping sets indices union second problem missing dimensions reconstructed approximating mean covariance bayesian predictive density section discusses solve tasks standard variational gp-lvm case section discusses dynamical case. furthermore dynamical case test points accompanied corresponding timestamps based perform additional forecasting prediction task given test time vector wish predict corresponding outputs. predictions standard variational gp-lvm ﬁrst discuss approximate density introducing latent variables test latent variables n∗×q write density interest ratio marginal likelihoods denominator marginal likelihood gp-lvm already computed variational lower bound. numerator another marginal likelihood obtained augmenting training data test points integrating newly inserted latent variable following explain detail approximate density equation constructing ratio lower bounds. lower bound variational lower bound computed section given equation maximization lower bound speciﬁes variational distribution latent variables training data. then distribution remains ﬁxed test approximated lower bound exactly analogous form optimisation fast factorisation imposed variational distribution equation means also fully factorised distribution write then held ﬁxed test time need optimise respect pai= further since statistics decompose across data test time re-use already estimated statistics corresponding averages need compute extra average terms associated note optimization parameters subject local minima. however sensible initializations employed based mean variational distributions associated nearest neighbours test point training data given above approximation given rewriting equation discuss second prediction problem partially observed test points given wish reconstruct missing part predictive density thus notice totally unobserved therefore cannot apply methodology described previously. instead objective approximate moments predictive density. achieve this ﬁrst need introduce underlying latent function values latent variables decompose exact predictive density follows based wish predict estimating mean covariance cov. problem takes form prediction uncertain inputs similar distribution expresses uncertainty inputs. ﬁrst term integral comes gaussian likelihood noisy version shown equation remaining terms together associated total observations following exactly section construct optimise lower bound quantity along allows compute gaussian variational distribution marginal. details form variational lower bound computed given appendix fact explicit form takes form projected process predictive distribution sparse substituting gaussian equation using fact also gaussian analytically compute mean covariance predictive density which based results girard take form calculated analytically several kernel functions explained section appendix using expressions gaussian noise model equation predicted mean equal predicted covariance equal σin∗. prediction tasks described previous section standard variational gp-lvm also solved dynamical variant similar fashion. speciﬁcally predictive approximate densities take exactly form equations whole approximation relies maximisation variational lower bound however dynamical case inputs priori correlated variational distribution factorise across makes optimisation distribution computationally challenging optimised respect parameters. issue explained appendix finally shall discuss solve forecasting problem dynamical model. problem similar second predictive task described section observed empty. therefore write predictive density similarly equation follows section investigate performance variational gp-lvm dynamical extension. variational gp-lvm allows handle high dimensional data using determine undelying dimensional subspace size automatically. generative construction allows impute missing values presented partial observation. evaluate models’ performance variety tasks namely visualisation prediction reconstruction generation data timeseries class-conditional density estimation. matlab source code repeating following experiments available on-line from https//github.com/sheffieldml/vargplvm supplementary videos from http//htmlpreview.github.io/?https//github.com/sheffieldml/vargplvm/b lob/master/vargplvm/html/index.htmlvgpds. experiments section structured follows; section outline covariance functions used experiments. section demonstrate method standard visualisation benchmark. section test both standard dynamical variant method realworld motion capture dataset. section illustrate proposed model able handle large number dimensions working directly pixel values high resolution videos. additionally show dynamical model interpolate also extrapolate certain scenarios. section consider classiﬁcation task standard benchmark exploiting fact framework gives access model evidence thus enabling bayesian classiﬁcation. proceeding actual evaluation method ﬁrst review give forms covariance functions used experiments. mapping input output spaces nonlinear thus covariance function equation also allows simultaneous model selection within framework. experiments method also model dynamics apart inﬁnitely differentiable exponantiated quadratic covariance function deﬁned equation also consider dynamical component mat´ern covariance function differentiable periodic suitable dynamical systems known approximately periodic. ﬁrst term captures periodicity dynamics whereas second corrects divergence periodic pattern enforcing datapoints form smooth trajectories time. ﬁxing variances particular ratios able control relative effect kernel. example sample paths drawn compound covariance function shown figure figure typical sample paths drawn covariance function. variances ﬁxed variances terms controlling relative effect. figures ratio large intermediate small respectively causing periodic pattern shifted proportionally period. kronecker delta function. deﬁne compound kernel kwhite noise level θwhite jointly optimised along rest kernel hyperparameters. similarly also include bias term θbias. given dataset known structure apply algorithm evaluate performance simple intuitive checking form discovered dimensional manifold agrees prior knowledge. illustrate method multi-phase data consists dimensional observations belonging three known classes corresponding different phases ﬂow. figure shows results data obtained applying variational gp-lvm latent dimensions using exponentiated quadratic kernel. means variational distribution initialized based variances variational distribution initialized neutral values around shown figure algorithm switches latent dimensions making inverse lengthscales almost zero. therefore two-dimensional nature dataset automatically revealed. figure shows visualization obtained keeping dominant latent directions dimensions remarkably high quality dimensional visualization data. comparison figure shows visualization provided standard sparse gp-lvm runs priori assuming latent dimensions. models inducing variables latent variables optimized standard gp-lvm initialized based pca. note standard gp-lvm latent dimensions model would overﬁt data would reduce dimensionality manner achieved variational gp-lvm. quality class separation twodimensional space also quantiﬁed terms nearest neighbour error; total error equals number training points whose closest neighbour latent space corresponds data point different class number nearest neighbour errors made ﬁnding latent embedding standard sparse gp-lvm points whereas variational gp-lvm resulted error. section consider data associated temporal information primary focus experiment evaluating dynamical version variational gp-lvm. followed taylor lawrence considering motion capture data walks runs taken subject motion capture database. used dynamical version model treated motion independent sequence. data constructed preprocessed described results separate -dimensional frames split figure panel shows latent space variational gp-lvm. dominant latent dimensions dimension plotted y-axis x-axis. plot shows visualization found standard sparse gp-lvm initialized dimensional latent space. nearest neighbor error count variational gp-lvm one. standard sparse gp-lvm training sequences average length frames each. model require explicit timestamp information since know priori constant time delay poses model construct equivalent covariance matrices given vector equidistant time points. model jointly trained explained last paragraph section walks runs i.e. algorithm learns common latent space motions. test time investigate ability model reconstruct test data previously unseen sequence given partial information test targets. tested providing dimensions correspond body subject providing correspond legs. compare results used approximations dynamical models nearest neighbour. also indirectly compare binary latent variable model taylor used slightly different data preprocessing. furthermore additionally tested non-dynamical version model order explore structure distribution found latent space. case notion sequences sub-motions modelled explicitly non-dynamical approach model correlations datapoints. however shown below model manages discover dynamical nature data reﬂected both structure latent space results obtained test data. performance method assessed using cumulative error joint scaled space deﬁned root mean square error angle space suggested lawrence models initialized nine latent dimensions. dynamical version performed runs using mat´ern covariance function dynamical prior using exponentiated quadratic. appropriate latent space dimensionality data automatically inferred models. non-dynamical model selected -dimensional latent space. model employed mat´ern covariance govern dynamics retained four dimensions whereas model used exponentiated quadratic kept three. latent dimensions completely switched parameters. table dynamical variational gp-lvm considerably outperforms approaches. best performance legs body reconstruction achieved dynamical model used mat´ern exponentiated quadratic covariance function respectively. intuitive result since smoother body movements expected better modelled using inﬁnitely differentiable exponentiated quadratic covariance function whereas mat´ern easier rougher motion. however although important take account available information nature data fact models outperform signiﬁcantly approaches shows bayesian training manages successfully covariance function parameters data case. furthermore non-dynamical variational gp-lvm manages discover latent space dynamical structure seen figure also proven robust making predictions. indeed table shows non-dynamical variational gp-lvm typically outperforms nearest neighbor performance comparable gp-lvm explicitly models dynamics using approximations. finally worth highlighting intuition gained investigating figure seen models split encoding walk regimes subspaces. further notice smoother latent space constrained less circular shape regime latent space encoding. explained noticing outliers left bottom positions plot latent points correspond training positions dissimilar rest training nevertheless temporally constrained model forced accommodate smooth path. intuitions conﬁrmed interacting model real time graphically presented supplementary video. figure latent space discovered models projected three principle dimensions. latent space found non-dynamical variational gp-lvm shown dynamical model uses mat´ern dynamical model uses exponentiated quadratic experiments considered video sequences sequences typically preprocessed modeling extract informative features reduce dimensionality problem. work directly pixel values demonstrate ability dynamical variational gp-lvm model data vast number features. also allows directly sample video learned model. table errors obtained motion capture dataset considering nearest neighbour angle space scaled space gp-lvm variational gp-lvm dynamical variational gp-lvm body data sets preprocessed corresponding datasets lawrence corresponds error scaled space taylor error angle space. best error column bold. firstly used model reconstruct partially observed frames test video sequences ﬁrst video discussed gave partial information approximately pixels gave approximately pixels frame. mean squared error pixel measured compare k−nearest neighbour method datasets considered following ﬁrstly ‘missa’ dataset standard benchmark used image processing. -dimensional video showing woman talking frames. data challenging translations pixel space. also considered video dimensionality shows artiﬁcially created scene ocean waves well −dimensional video showing running frames. later approximately periodic nature containing several paces dog. ﬁrst videos used mat´ern exponentiated quadratic covariance functions respectively model dynamics interpolated reconstruct blocks frames chosen whole sequence. ‘dog’ dataset constructed compound kernel presented section exponentiated quadratic term employed capture divergence approximately periodic pattern. used model reconstruct last frames extrapolating beyond original video. seen table method outperformed cases. results also demonstrated visually figures reconstructed videos available supplementary material. seen figures dynamical variational gp-lvm predicts pixels smoothly connected observed part image whereas method cannot predicted pixels overall context. figure focuses speciﬁc problem seen evidently corresponding video ﬁles. second task used generative model create samples generate video sequence. effective ‘dog’ video training examples approximately periodic nature. model trained frames generated frames correspond next time points future. input given generation future frames time-stamp vector results show smooth transition training test amongst test video frames. resulting video continuing sharp high quality. experiment demonstrates ability model reconstruct massively high dimensional images without blurring. frames result shown figure full video available supplementary material. experiment variational gp-lvm build generative classiﬁer handwritten digit recognition. consider well known usps digits dataset. dataset consists images digits divided training examples test examples. variational gp-lvms digit usps data base. used latent dimensions inducing variables model. allowed build probabilistic generative model digit compute bayesian class conditional densities test data form class conditional densities approximated ratio lower bounds described section whole approach allows classify digits determining class labels test data based highest class conditional density value using uniform prior class labels. comparison used -vs-all logistic regression classiﬁcation approach. shown table variational gp-lvm outperforms baseline. considered typical dimensionality reduction scenario where given high-dimensional output data seek low-dimensional latent representation completely unsupervised figure demonstrate reconstruction achieved dynamical variational gp-lvm respectively challenging frames ‘missa’ video i.e. translation occurs. contrast method works whole high dimensional pixel space method reconstructed images using -dimensional compression ‘missa’ video. manner. dynamical variational gp-lvm additional temporal information input space wish propagate uncertainty still treated fully unobserved. however framework propagating input uncertainty mapping applicable full spectrum cases ranging fully unobserved fully observed inputs known unknown amount uncertainty input. section discuss cases further show figure depict reconstruction achieved frame ‘ocean’ dataset. notice aforementioned datasets method recovers smooth image contrast simple dynamical var. gp-lvm reconstructed ocean images using -dimensional compression video. gaussian processes used extensively great success variety regression tasks. common setting given dataset observed input-output pairs denoted respectively wish infer unknown outputs n∗×p corresponding novel given inputs n∗×q. however many real-world applications inputs uncertain example measurements come noisy sensors. case methodology cannot trivially extended account variance associated input space aforementioned problem also closely related ﬁeld heteroscedastic gaussian process regression uncertainty noise figure here also demonstrate ability model automatically select latent dimensionality showing initial lengthscales covariance function values obtained training ‘dog’ data set. section show variational framework used explicitly model input uncertainty regression setting. assumption made observed inputs obtained noise-free latent inputs adding gaussian noise typically unknown parameter. given really inputs eventually passed latent function whole probabilistic model becomes gp-lvm special form prior distribution latent inputs making thus variational framework easily applicable. precisely using prior deﬁne variational bound well associated approximation true posterior variational distribution used probability estimate noisy input locations optimisation lower bound also learn parameter furthermore wish reduce number parameters variational distribution sensible choice would although choice optimal. method implicitly models uncertainty inputs also allows predictions autoregressive manner propagating uncertainty predictive sequence demonstrate context framework take simple case process interest multivariate time-series given pairs time points {t}n here take time locations deterministic equally spaced simply denoted subscript output points thus simply denote output point corresponds size dynamics’ memory. words deﬁne window size shifts time output time becomes input time therefore uncertain inputs method described earlier section applied dataset particular although training inputs necessarily uncertain case aforementioned performing inference particularly advantageous task extrapolation. detail consider simplest case described section posterior centered given noisy inputs allow variable noise around centers. perform extrapolation ﬁrstly needs train model dataset then perform iterative k−step ahead prediction order future sequence where similarly approach taken girard predictive variance step accounted propagated subsequent predictions. example algorithm make iterative -step predictions future; beginning output predicted given training set. next step training augmented include previously predicted part input predictive variance encoded uncertainty point. advantage method resembles state-space model future predictions almost immediately revert mean standard stationary regression neither underestimate uncertainty would happen predictive variance propagated inputs principled way. demonstration iterative k−step ahead forecasting demonstrate framework simulation state space model described previously. speciﬁcally consider mackey-glass chaotic time series standard benchmark also considered data one-dimensional timeseries represented pairs values simulates model trained dataset described previously modiﬁed dataset created used ﬁrst points train model predicted subsequent points future. comparison made ﬁrstly standard model input output pairs given standard form respectively predictions made standard given further compared standard model input output pairs given modiﬁed dataset mentioned previously; model referred ˆzˆy. latter model predictions made k−step ahead manner according predicted values iteration added training set. however standard model straight forward propagating uncertainty therefore input uncertainty zero every step iterative predictions. predictions obtained seen figure seen variational gp-lvm robust handling uncertainty throughout predictions something results lower predictive error. particular notice ﬁrst predictions methods give answer. however standard regression model quickly reverts mean expected test inputs training ones uncertainty large. iterative −step ahead prediction chaotic timeseries. comparing standard approach figure autoregressive approach propagate uncertainties variational gp-lvm autoregressive setting. hand ˆzˆy opposite problem; every predictive step results prediction underestimates uncertainty; therefore although initial predictions reasonable diverge little true values error carried ampliﬁed. section describe proposed model used data imputation semi-supervised regression problem part training inputs missing. scenario obviously special case uncertain input modeling discussed above. although general setting deﬁned consider case fully partially observed inputs i.e. denote rows contain fully partially observed inputs respectively. realistic scenario; often case certain input features difﬁcult obtain others would nevertheless wish model available information within model. features missing different number location individual point standard regression model cannot straightforwardly model jointly contrast framework inputs replaced distributions taken account naturally simply initialising uncertainty missing locations mean empirical mean then optionally optimising experiments slightly sophisticated approach resulted better results. speciﬁcally fully observed data subset train initial model given model estimate predictive posterior missing locations initializing proceed training model full training contains fully partially observed inputs. training phase variational distribution held ﬁxed locations corresponding observed values optimised locations missing inputs. given formulation deﬁne semi-supervised model naturally incorporates fully partially observed examples communicating uncertainty throughout relevant parts model principled way. speciﬁc predictive uncertainty obtained initial model trained fully observed data incorporated input uncertainty model trained extended dataset similarly extrapolation achieved autoregressive approach section extreme cases resulting non-conﬁdent predictions example presence outliers corresponding locations simply ignored automatically large uncertainty. mechanism together subsequent optimisation guards reinforcing predictions imputing missing values based smaller training set. particular limit observed values semi-supervised equivalent gplvm missing values equivalent regression. details algorithm approach given appendix algorithm deﬁned seen particular instance semi-supervised learning uses self-training initialisation. traditionally semi-supervised settings encountered classiﬁcation problems part training data associated known class labels. simple approach exploiting unlabelled examples self-training according initial model trained labelled examples used incorporate unlabelled examples manner dictated speciﬁc self-training methodology followed. bootstrap-based self-training approach incoroporation achieved predicting missing labels using initial model subsequently augmenting training using conﬁdent predictions subset. recently kingma demonstrated applicability generative models semi-supervised learning. method deﬁnes latent space estimates approximate factorised respect data points posterior using labelled unlabelled examples. subsequently algorithm builds classiﬁer latent label space sampling areas approximate posterior correspond labelled instances. framework adapted tackle aforementioned classiﬁcation scenario redirected future work. instead focused regression problem missing values appear inputs. however exist similarities work referenced previous paragraph. speciﬁc generative method treats semi-supervised task data imputation problem similarly differences work latent space representation inputs instead directly associate input space uncertainty. concerning relations methods self-training algorithm also trains initial model fully observed portion data predicts missing values. however predictions constitute initialisations later optimised along model parameters hence refer step partial self-training. further framework predictive uncertainty used hard measure discarding unconﬁdent predictions instead allow values contribute according optimised uncertainty measure. therefore uncertainty handled makes self-training part algorithm principled compared many bootstrap-based approaches. unknown competing models gave input another obtain corresponding outputs real-world data demonstration considered subset motion capture dataset discussed section corresponds walking motion human body represented joint locations. formulated regression problem ﬁrst dimensions original data used targets rest inputs. words given partial joint representation human body task infer rest representation. datasets simulated motion capture selected portion training inputs denoted randomly missing features. extended dataset used train method well multiple linear regression using observed data trained standard nearest neighbour cannot handle missing inputs straightforwardly. goal reconstruct test outputs given fully observed test inputs simulated data used following sizes |zo| |zu| |z∗| dimensionality inputs outputs motion capture data used |zo| |zu| |z∗| figure plot obtained competing methods varying percentage missing features simulated data experiment points plot average runs considered different random seeds. seen semi-supervised able handle extra data make better predictions even large portion missing. indeed performance starts converge standard missing values performs identically standard values missing. figure mean squared error predictions obtained different methods simulated motion capture data flat line errors correspond methods cannot take account partially observed inputs. results simulated data obtained trials hence errorbars also plotted. errorbars change x-axis clarity plotted separately right dashed vertical line methods resulted high compared rest shown clearer plots; speciﬁcally predicting data mean motion capture data performed badly average introduced approximation marginal likelihood gaussian process latent variable model form variational lower bound. provides bayesian training procedure robust overﬁtting allows appropriate dimensionality latent space automatically determined. framework extended case observed data constitute multivariate timeseries therefore obtain generic method dynamical systems modelling able capture complex non-linear correlations. demonstrated advantages rigorous lower bound deﬁned framework range disparate real world data sets. also emphasised ability model handle vast dimensionalities. approach easily extended applied training gaussian processes uncertain inputs inputs gaussian prior densities. gave rise auto-regressive semi-supervised variant model. future research envisage several extensions become computationally feasible using methodologies espouse. particular propagation uncertain inputs gaussian process allows bayes ﬁltering applications carried variational bounds. bayes ﬁlters non-linear dynamical systems time discrete observed data time point non-linearly related unobserved latent state assumed gaussian processes. propagation uncertainty processes achieved variational lower bound allowing fast efﬁcient approximations gaussian process dynamical models. bound also allows promising direction research deep gaussian processes. deep gaussian process idea placing temporal prior inputs extended hierarchical application. formalism leads powerful class models gaussian process priors placed function compositions example layer model draw gaussian process. combining models structure learning develop potential learn complex linear interactions data. contrast deep models uncertainty parameters latent variables marginalised out. research partially funded european research project fp-ict greek state scholarships foundation university shefﬁeld moody endowment fund. also thank colin litster life allowing video ﬁles datasets. appendix contains supplementary details deriving mathematical formulae related calculation ﬁnal expression variational lower bound training phase. detailed derivation ˆfj) quantity ˆfj) appears equation based derivations previous section rewrite equation function optimal found equation completing computations involve convolutions covariance function gaussian density. standard kernels exponentiated quadratic covariance linear covariance function statistics obtained analytically. particular exponentiated quadratic kernel equation have function diag transforms vector square diagonal matrix vice versa. given above parameters optimised gradient w.r.t inducing points however exactly form therefore presented here. derivatives w.r.t implementation prefer parametrise software data precision rather data variance therefore give directly derivatives precision. obviously chain rule relationship obtain derivatives variance. further comes model parameters write gradients respect single element contrast above term involve parameters involves variational parameters reparametrised turn depends demonstrate that forget moment reparametrisation express bound ¯µj) show explicitly dependency variational mean function calculations must take account term section provides details related task predictions based partially observed test data speciﬁcally section explains detail form variational lower bound aforementioned prediction scenario illustrates gives rise certain computational differences standard dynamical gp-lvm. section gives details mathematical formulae associated prediction task. discussed section predictions based partially observed outputs variational gp-lvm needs construct variational lower bound training phase. however needs associated full observations speciﬁcally need lower bound marginal likelihood given equation variational bound takes form involved terms given equation already computed training phase therefore held ﬁxed test time. similarly third term equation also held ﬁxed test time. second fourth term optimised exactly bound computed training phase difference data augmented test observations observed dimensions accounted for. contrast dynamical version model requires full latent variables fully coupled variational distribution together form timeseries. consequently expansion equation cannot applied here meaning case precomputations used training phase. however could apply approximation speed test phase. case latent variables still correlated sets not. however approximation used implementation expected speed predictions phase training case experiments. calculation posterior optimisation based variational bound constructed test phase partially observed outputs explained section gives rise posterior exactly happens training phase. therefore according equation write integrals inside product easy compute since types densities appearing gaussian according equations fact factor takes form projected process predictive distribution sparse consider fully partially observed inputs i.e. denote rows contain fully partially observed inputs respectively. features missing different number location individual point train model observations jointly replacing inputs distributions respectively using algorithm since posterior distribution factorised algorithm constrains close delta function regions observations i.e. areas corresponding areas corresponding non-missing locations rest posterior area’s parameters initialised according prediction model subsequently optimised augmented model mou. notice initial model obtained training variational gp-lvm model posterior whose mean fully constrained match observations small uncertainty thus model behaves almost standard regression model. section present additional ﬁgures obtained experiment described section using motion capture data. figure depicts optimised weights dynamical models employed experiment. figure illustrates examples predictive performance models plotting true predicted curves angle space. explained section employed models encode walk regime separate subspaces latent space. illustrate clearly sampled points learned latent space trained dynamical variational gp-lvm model generated corresponding outputs investigate kind information encoded subspace speciﬁcally considered model employed mat´ern covariance function constrain latent space based weights figure projected latent space dimensions interacting model revealed dimension separates walk regime. particular ﬁrst ﬁxed dimension value belonging region encoding walk seen figure sampled multiple latent points varying dominant dimensions namely seen figure corresponding outputs shown second figure dimension ﬁxed value belonging region encoding outputs obtained varying dimenfigure values scales kernel training motion capture dataset using exponentiated quadratic mat´ern kernel model dynamics dynamical variational gp-lvm. scales zero value switch corresponding dimension latent space. latent space therefore note scales initialized similar values figure prediction test angles body legs part continuous line original test data dotted line nearest neighbour scaled space dashed line dynamical variational gp-lvm sions produced smooth running motion seen third figure finally figure illustrates motion clearly different training obtained sampling latent position training data seen figure indicative generative model’s ability producing novel data. figure plots depict projection latent space dimensions blue corresponding value dimensions ﬁxed sampled latent points crosses represent latent points corresponding training outputs. intensity grayscale background represents posterior uncertainty region plot depicts latent space projection dimensions ﬁxed latent positions corresponding generated output depicted plot figure ﬁrst depicts projection latent space dimensions blue showing value dimensions ﬁxed sampled latent points. corresponding outputs depicted second third", "year": 2014}