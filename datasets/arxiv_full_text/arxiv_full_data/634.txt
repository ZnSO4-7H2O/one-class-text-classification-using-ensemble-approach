{"title": "Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games?", "tag": ["cs.AI", "cs.NE", "stat.ML"], "abstract": "Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyse multiagent play, and even develop a self play algorithm.", "text": "deep reinforcement learning achieved many recent successes understanding strengths limitations hampered lack rich environments fully characterize optimal behavior correspondingly diagnose individual actions characterization. consider family combinatorial games arising work erdos selfridge spencer propose environments evaluating comparing different approaches reinforcement learning. games number appealing features challenging current learning approaches form low-dimensional simply parametrized environment linear closed form solution optimal behavior state difﬁculty game tuned changing environment parameters interpretable way. erdos-selfridge-spencer games compare different algorithms test generalization make comparisons supervised learning analyse multiagent play even develop self play algorithm. deep reinforcement learning seen many remarkable successes past years mnih silver developing learning algorithms robust across tasks policy representations remains challenge. standard benchmarks like mujoco atari provide rich settings experimentation speciﬁcs underlying environments differ multiple ways hence determining principles underlying particular form sub-optimal behavior difﬁcult. optimal behavior environments generally complex fully characterized algorithmic success generally associated high scores typically copy training environment making hard analyze errors occurring evaluate generalization. ideal setting studying strengths limitations reinforcement learning algorithms would simply parametrized family environments optimal behavior completely characterized environment rich enough support interaction multiagent play. produce family environments look novel direction two-player combinatorial games roots work erdos selfridge placed general footing spencer roughly speaking erdos-selfridge-spencer games games players take turns selecting objects combinatorial structure feature optimal strategies deﬁned potential functions derived conditional expectations random future play. games thus provide opportunity capture general desiderata noted above clean characterization optimal behavior instances range easy hard sweep simple tunable parameters. focus particular best-known games genre spencer’s attacker-defender game roughly speaking attacker advances pieces levels board defender destroys subsets pieces prevent reaching ﬁnal level instance game parametrized quantities. ﬁrst number levels determines size state space approximate length game; latter directly related sparsity win/loss signals rewards. second quantity potential function whose magnitude characterizes whether instance favors defender attacker much margin error optimal play. environment therefore allows study learning defender attacker separately concurrently multiagent self-play. process able develop insights robustness solutions changes environment. types analyses longstanding goals generally approached much abstractly given difﬁculty characterizing step-by-step optimality non-trivial environments one. move-by-move characterization optimal play beyond simple measures reward based purely win/loss outcomes supervised learning techniques pinpoint exact location errors trajectory play. develop combinatorial games environments studying behavior reinforcement learning algorithms setting possible characterize optimal play tune underlying difﬁculty using natural parameters. show reinforcement learning algorithms domain able learn generalizable policies addition simply achieving high performance combinatorial results domain able develop strong methods multiagent play enhance generalization. characterize optimal play move-by-move level thus compare performance deep agent trained using supervised learning move-by-move decisions. discover intriguing phenomenon supervised learning agent accurate individual move decisions agent agent better playing game interpret result deﬁning notion fatal mistakes showing deep agent makes mistakes overall makes fewer fatal mistakes. summary present learning generalization experiments variety commonly used model architectures learning algorithms. show despite superﬁcially simple structure game provides signiﬁcant challenges standard reinforcement learning approaches number tools precisely understanding challenges. ﬁrst introduce family attacker-defender games games properties yield particularly attractive testbed deep reinforcement learning ability continuously vary difﬁculty environment parameters existence closed form solution expressible linear model. attacker-defender game involves players attacker moves pieces defender destroys pieces. instance game levels numbered pieces initialized across levels. attacker’s goal least pieces level defender’s goal destroy pieces happen. turn attacker proposes partition pieces still play. defender chooses sets destroy remove play. pieces moved level. game ends either pieces reach level pieces destroyed. figure shows turn play. setup varying number levels number pieces changes difﬁculty attacker defender. striking aspects attacker-defender game possible make trade-off precise route also identify linear optimal policy. figure turn attacker-defender game. attacker proposes partition current game state defender chooses destroy pieces remaining move level form next game state. start simple special case rather initializing board pieces placed arbitrarily require pieces start level special case directly think game’s difﬁculty terms number levels number pieces theorem consider instance attacker-defender game levels pieces pieces starting level defender always win. simple proof fact defender simply always destroys larger sets number pieces reduced least factor step; since piece must travel steps order reach level piece reach level move general case board initialized start game pieces placed arbitrary levels less immediately clear deﬁne larger sets therefore describe second proof theorem useful general settings. second proof spencer uses erdos’s probabilistic method proceeds follows. attacker strategy assume defender plays randomly. random variable defender playing randomly piece probability advancing level destroyed. pieces start level must advance levels reach happens probability choice since integer random variable implies distribution nonzero mass words choices defender guarantees destroying pieces. means attacker strategy wins probability random play defender; since game property player must able force follows defender force win. completes proof. consider general form game initial conﬁguration pieces arbitrary levels. thus point time state game described k-dimensional vector number pieces level extending argument used second proof above note piece level chance survival random play. motivates following potential function states deﬁnition potential function given game state deﬁne potential note linear function input state expressible vector state following generalization theorem spencer theorem consider instance attacker-defender game levels pieces pieces placed anywhere board initial state prove part theorem directly extending proof theorem deﬁnition potential function choice ﬁnish theorem deﬁnition potential function gives natural concrete strategy defender defender simply destroys whichever higher potential. claim strategy guarantees subsequent state also indeed suppose potential least high destroyed defender. since next state potential also less order attacker would need place piece level would produce potential least since sets defender’s strategy potential strictly less follows piece ever reaches level spencer proves part theorem deﬁning optimal strategy attacker using greedy algorithm pick sets potential purposes proof spencer results intractably large action space attacker; therefore deﬁne kind attacker preﬁx-attacker prove optimality. combinatorial insights game enable later perform multiagent play subsequently self-play. atari benchmark well known tasks ranging easy solve difﬁcult duan proposed continuous environments implemented mujoco simulator todorov advantage physics based environments varied continuously changing physics parameters randomizing rendering deepmind navigation based environments. openai contains atari mujoco benchmarks well classic control environments like cartpole algorithmic tasks like copying input sequence. difﬁculty algorithmic tasks easily increased increasing length input. proposed benchmark merges properties algorithmic tasks physics-based tasks letting increase difﬁculty discrete changes length continuous changes potential. section attacker-defender games family environments difﬁculty knob continuously adjusted start state potential number levels section describe baseline results attacker-defender games motivate exploration remainder paper. attacker-defender environment follows game state represented dimensional vector levels coordinate representing number pieces level defender agent input concatenation partition giving dimensional vector. start state initialized randomly distribution start states certain potential. ﬁrst look training defender agent attacker randomly chooses playing optimally playing suboptimally exploration recall speciﬁcation potential function deﬁnition theorem defender linear optimal policy given input partition aiwi number biwi number pieces level weighting deﬁning potential function. training defender agent choices difﬁculty parameters. potential start state changes close optimality defender play figure training linear network play defender agent dqn. linear model theoretically expressive enough learn optimal policy defender agent. practice many difﬁculty settings algorithms struggles learn optimal policy performs poorly using deeper models exception performs relatively well difﬁculty settings. values close giving much less leeway mistakes valuing sets. changing number levels directly affects sparsity reward higher resulting longer games less feedback. additionally also greatly increases number possible states game trajectories optimal policy expressed linear network ﬁrst training linear model defender agent. evaluate proximal policy optimization advantage actor critic deep q-networks using openai baselines implementations challenging learn harder difﬁculty settings game perform better deeper networks performs surprisingly well improvement performance variance deeper model. summary policy theoretically expressed linear model empirically gains performance reduction variance using deeper networks evaluated performance linear models turn using deeper neural networks policy net. identically above evaluate varying start state potentials algorithm random seeds plots show minimum mean maximum performance. results shown figures note algorithms show variation performance across different settings potentials show noticeable drops performance harder difﬁculty settings. varying potential figure show larger variation though mostly matches beats performance. varying shows less variation dqn. shows greatest variation worst performance three methods. figure training defender agent varying values potentials different choices deep network. overall signiﬁcant improvements using linear model. smaller performs relatively consistently across different potential values though quite matching left right panes tends fare worse dqn. figure training defender agent varying values different choices potential deep network. three algorithms show noticeable variation performance different difﬁculty settings though note seems robust larger tends fare worse dqn. figure plot showing overﬁtting opponent strategies. defender agent trained optimal attacker tested another optimal attacker environment disjoint support attacker environment. left pane shows resulting performance drop switching testing opponent strategy training different opponent strategy. right pane shows result testing optimal attacker disjoint support attacker training. performance disjoint support attacker converges signiﬁcantly lower level optimal attacker. previous section trained defender agents using popular algorithms evaluated performance trained agents environment. however noting attacker-defender game known optimal policy works perfectly game setting evaluate algorithms generalization performance. take setting parameters start state potential seen agent perform well change procedural attacker policy train test. detail train agents defender attacker playing optimally test agents defender disjoint support attacker. results shown figure performance high agents fail generalize opponent theoretically easier. leads natural question might achieve stronger generalization environment. mitigate overﬁtting issue method also training attacker train defender learned attacker multiagent setting. however determining correct setup train attacker agent ﬁrst requires devising tractable parametrization action space. naive implementation attacker would policy output many pieces allocated levels grow rapidly clearly impractical. address this ﬁrst prove theorem enables parametrize optimal attacker much smaller action space. theorem attacker-defender game levels start state exists partition contains pieces level contains pieces level proof. pieces levels excluding level thus exists complement done. assume since contains pieces levels potentials integer multiples value piece level letting guaranteed level pieces move pieces potential equals figure performance training attacker agent different difﬁculty settings. performance poor much greater variation performance changing affects sparseness reward well size action space. less variation potential high performance variance lower potentials. theorem gives different constructing parametrizing optimal attacker. attacker outputs level environment assigns pieces level pieces level splits level among keep potentials close possible. theorem guarantees optimal policy representable action space linear instead exponential setup train attacker agent optimal defender dqn. results poor show results algorithms found large variation performance changing affects reward sparsity action space size. observe less outright performance variability changes potential small increase variance attacker training look learning multiagent setting. ﬁrst explore effects varying potential shown figure overall attacker fares worse multiagent play single agent setting. particular note left pane figure attacker loses defender even compare figure potential single agent attacker succeeds winning optimal defender. finally return defender agent test generalization single multiagent settings. train defender agent single agent setting optimal attacker test attacker uses disjoint support strategy. also test defender trained multiagent setting disjoint support attacker. results shown figure defender trained part figure performance attacker defender agents learning multiagent setting. panes solid lines denote attacker performance. bottom panes solid lines defender performance. sharp changes performance correspond times switch agent training. note defender performs much better multiagent setting comparing bottom left panes variance lower performance attacker compared defender performance below. furthermore attacker loses defender potential despite winning optimal defender figure also attacker higher variance sharper changes performance even conditions guaranteed win. figure results generalizing different attacker strategies single agent defender multiagent defender. ﬁgure single agent defender trained optimal attacker tested disjoint support attacker multiagent defender also tested disjoint support attacker different values multiagent defender generalizes better unseen strategy single agent defender. previous section showed theoretical insight efﬁcient attacker action space paramterization possible train attacker agent. attacker agent parametrized neural network different implementing defender trained multiagent fashion. section present additional theoretical insights enables training self-play using single neural network parametrize attacker defender. insight following defender’s optimal strategy construction optimal attacker theorem depend primitive operation takes partition pieces sets determines larger defender leads directly strategy destroys higher potential. attacker primitive used binary search procedure desired partition theorem given initial partition preﬁx sufﬁx pieces sorted level determine higher potential recursively balanced split point inside larger sets. process summarized algorithm thus training single neural network designed implement primitive operation determining sets higher potential simultaneously train attacker defender invoke neural network. purpose found empirically quickest among alternatives converging consistent estimates relative potentials sets. train agents test defender agent attacker used figures results figure show defender trained self play signiﬁcantly outperforms defenders trained procedural attacker. aside testing generalization attacker-defender game also enables make comparison supervised learning. closed-form optimal policy enables evaluation ground truth move basis. thus compare supervised learning setup classify correct action large sampled states. carry test practice ﬁrst train defender policy reinforcement learning saving observations seen dataset. train supervised network dataset classify optimal action. test supervised network determine well play. interesting dichotomy proportion correct moves average reward. unsurprisingly supervised learning boasts higher proportion correct moves keep count ground truth correct move turn game lower proportion correct moves compared supervised learning however right pane figure better playing game achieving higher average reward difﬁculty settings signiﬁcantly beating supervised learning grows. figure train attacker defender self play using dqn. defender implemented figures neural network used train attacker agent performing binary search according values partitions input space test defender agent procedural attacker used figures self play shows markedly improved performance. figure plots comparing performance supervised learning attacker defender game different choices left pane shows proportion moves correct supervised learning unsurprisingly supervised learning better average getting ground truth correct move. however better playing game policy trained signiﬁcantly outperforms policy trained supervised learning difference growing larger contrast forms interesting counterpart recent ﬁndings silver context also compared reinforcement learning supervised approaches. distinction supervised work relative heuristic objective whereas domain able compare provably optimal play. makes possible rigorously deﬁne notion mistake also perform ﬁne-grained analysis remainder section. speciﬁcally agent achieving higher reward game making mistakes per-move level? gain insight this categorize per-move mistakes different types study separately. particular suppose defender presented partition pieces sets assume without loss generality defender makes terminal mistake defender chooses destroy note means defender faces forced loss optimal play whereas could forced optimal play destroyed also deﬁne subset family terminal mistakes follows defender makes fatal mistake defender chooses destroy note fatal mistake converts position defender forced forced loss. figure especially gets larger reinforcement learning makes terminal fatal mistakes much lower rate supervised learning does. suggests basis different performance even supervised learning making fewer mistakes overall making mistakes certain well-deﬁned consequential situations. figure figure showing frequencies different kinds mistakes made supervised learning would cost game. left pane shows frequencies fatal mistakes agent goes winning state losing state superset kind mistake terminal mistakes look agent makes wrong choice destroying instead cases makes signiﬁcantly fewer mistakes supervised learning particularly difﬁculty increases. paper proposed erdos-selfridge-spencer games rich environments investigating reinforcement learning exhibiting continuously tunable difﬁculty exact combinatorial characterization optimal behavior. demonstrated algorithms exhibit wide variation performance tune game’s difﬁculty characterization optimal behavior evaluate generalization performance. provide theoretical insights enable multiagent play binary search self play. finally compare supervised learning highlighting interesting tradeoffs move optimality average reward fatal mistakes. also develop results appendix including analysis catastrophic forgetting generalization across different values game’s parameters method investigating measures model’s conﬁdence. believe family combinatorial games used rich environment gaining insights deep reinforcement learning. references charles beattie joel leibo denis teplyashin ward marcus wainwright heinrich küttler andrew lefrancq simon green víctor valdés amir sadik deepmind lab. arxiv preprint arxiv. duan chen rein houthooft john schulman pieter abbeel. benchmarking deep reinforcement learning continuous control. international conference machine learning volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature issn http //dx.doi.org/./nature. volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. aravind rajeswaran sarvjeet ghotra sergey levine balaraman ravindran. epopt learning robust neural network policies using model ensembles. arxiv preprint arxiv. david silver julian schrittwieser karen simonyan ioannis antonoglou huang arthur guez thomas hubert lucas baker matthew adrian bolton yutian chen timothy lillicrap laurent sifre george driessche thore graepel demis hassabis. mastering game without human knowledge. nature issn http//dx.doi.org/./nature. josh tobin rachel fong alex jonas schneider wojciech zaremba pieter abbeel. domain randomization transferring deep neural networks simulation real world. arxiv preprint arxiv. emanuel todorov erez yuval tassa. mujoco physics engine model-based control. intelligent robots systems ieee/rsj international conference ieee note evaluating defender agent initially slightly suboptimal attacker randomizes playing optimally disjoint support strategy. disjoint support strategy suboptimal verison preﬁx attacker described theorem instead ﬁnding partition results sets equal possible disjoint support attacker greedily picks potential difference sets fraction total potential smaller uniformly sampled turn. exposes defender agent sets uneven potential helps develop generalizable strategy. train defender agent fully connected deep neural network hidden layers width represent policy. decided hyperparameters experimentation different depths widths found network width signiﬁcant effect performance seen section slightly deeper models performed noticeably better shallow networks. main text examined defender agent performance varies change difﬁculty settings game either potential returning fact attackerdefender game expressible optimal generalizes across difﬁculty settings might wonder training difﬁculty setting testing different setting perform. testing different potentials straightforwards testing different requires slight reformulation. input size defender neural network policy naively changing different number levels work. furthermore training smaller testing larger fair test model cannot expected learn weight lower levels. however testing converse easily implementable offers legitimate test generalization. varying potential training harder games results better generalization. testing smaller used training performance inverse difference train test recently several papers identiﬁed issue catastrophic forgetting deep reinforcement learning switching different tasks results destructive interference lower performance instead positive transfer. witness effects form attacker-defender games. section environments differ ﬁrst training small train larger lower difﬁculty settings curriculum learning improves play higher potential settings learning interferes catastrophically figure signiﬁcant performance drop figure motivates investigating whether simple rules thumb model successfully learned. perhaps simplest rule thumb learning value null consists zeros pieces defender agent reliably choose destroy surprisingly even simple rule thumb violated even frequently larger figure figure left train different potentials test potential training harder games leads better performance agent trained easiest potential generalizing worst agent trained harder potential generalizing best. result consistent across different choices test potentials. right pane shows effect training larger testing smaller performance appears inversely proportional difference train test figure conﬁdence function potential difference states. ﬁgure shows true potential differences model conﬁdences; green dots moves model prefers make right prediction moves moves prefers make wrong prediction. right shows data plotting absolute potential difference absolute model conﬁdence preferred move. remarkably increase potential difference associated increase model conﬁdence wide range even model wrong. also test model outputs well calibrated potential values model conﬁdent cases large discrepancy potential values ﬁfty-ﬁfty potential evenly split? results shown figure main paper mixed different start state distributions ensure wide variety states seen. begets natural question well generalize across start state distribution train purely distribution. results figure show training naively ‘easy’ start state distribution results signiﬁcant performance drop switching distribution. fact amount possible starting states given potential grows super exponentially number levels state following theorem theorem number states potential game levels grows like proof. state denoted trivial upper bound computed noting take value producting together gives roughly lower bound assume convenience power look non-negative integer solutions system simultaneous equations", "year": 2017}