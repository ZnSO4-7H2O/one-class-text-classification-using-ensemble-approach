{"title": "GraphVAE: Towards Generation of Small Graphs Using Variational  Autoencoders", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.", "text": "deep learning graphs become popular research topic many applications. however past work concentrated learning graph embedding tasks contrast advances generative models images text. possible transfer progress domain graphs? propose sidestep hurdles associated linearization discrete structures decoder output probabilistic fullyconnected graph predeﬁned maximum size directly once. method formulated variational autoencoder. evaluate challenging task molecule generation. deep learning graphs recently become popular research topic useful applications across ﬁelds chemistry medicine computer vision past work concentrated learning graph embedding tasks i.e. encoding input graph vector representation. stark contrast fast-paced advances generative models images text seen massive rise quality generated samples. hence intriguing question transfer progress domain graphs i.e. decoding vector representation. moreover desire method mentioned past g´omez-bombarelli however learning generate graphs difﬁcult problem methods based gradient optimization graphs discrete structures. unlike sequence generation graphs arbitrary connectivity clear best linearize construction sequence steps. hand learning order incremenwork propose sidestep hurdles decoder output probabilistic fully-connected graph predeﬁned maximum size directly once. probabilistic graph existence nodes edges well attributes modeled independent random variables. method formulated framework variational autoencoders kingma welling demonstrate method coined graphvae cheminformatics task molecule generation. molecular datasets challenging convenient testbed generative model easily allow qualitative quantitative tests decoded samples. method applicable generating smaller graphs performance leaves space improvement believe work important initial step towards powerful efﬁcient graph decoders. related work graph decoders. graph generation largely unexplored deep learning. closest work johnson incrementally constructs probabilistic graph world representation according sequence input sentences answer query. model also outputs probabilistic graph assume prescribed order construction transformations available formulate learning problem autoencoder. learns produce scene graph input image. construct graph object proposals provide initial embeddings node edge message passing obtain consistent prediction. contrast method generative model produces probabilistic graph single opaque vector without specifying number nodes structure explicitly. figure illustration proposed variational graph autoencoder. starting discrete attributed graph nodes stochastic graph encoder embeds graph continuous representation given point latent space novel graph decoder outputs probabilistic fully-connected graph predeﬁned reconstruction ability autoencoder facilitated approximate graph matching aligning discrete data decoders. text common discrete representation. generative models usually trained maximum likelihood fashion teacher forcing avoids need backpropagate output discretization feeding ground truth instead past sample step. bengio argued lead expose bias i.e. possibly reduced ability recover mistakes. recently efforts made overcome problem. notably computing differentiable approximation using gumbel distribution bypassing problem learning stochastic policy reinforcement learning work also circumvents non-differentiability problem namely formulating loss probabilistic graph. approach task graph generation devising neural network able translate vectors continuous code space graphs. main idea output probabilistic fully-connected graph standard graph matching algorithm align ground truth. proposed method formulated framework variational autoencoders kingma welling although forms regularized autoencoders would equally suitable brieﬂy recapitulate continue introducing novel graph decoder together appropriate objective. molecule decoders. generative models become promising novo design molecules fulﬁlling certain criteria able search continuous embedding space mind propose conditional version model. molecules intuitive representation graphs ﬁeld resort textual representations ﬁxed syntax e.g. so-called smiles strings exploit recent progress made text generation rnns syntax brittle many invalid strings tend generated recently addressed kusner incorporating grammar rules decoding. encouraging approach guarantee semantic validity similarly method. graph speciﬁed adjacency matrix edge attribute tensor node attribute matrix wish learn encoder decoder space graphs continuous embedding figure probabilistic setting encoder deﬁned variational posterior decoder generative distribution learned parameters. furthermore prior distribution imposed latent code representation regularization; simplistic isotropic gaussian prior whole model trained minimizing upper bound negative log-likelihood decoder deterministic. architecture simple multi-layer perceptron three outputs last layer. sigmoid activation function used compute whereas edgenode-wise softmax applied obtain since particular ordering nodes imposed either matrix representation graphs invariant permutations nodes comparison graphs hard. however approximate graph matching described subsection obtain binary assignment matrix assigned otherwise. knowledge allows information graphs. speciﬁcally input adjacency matrix mapped predicted graph whereas predicted node attribute matrix slices edge attribute matrix assumed encoded one-hot notation. formulation considers existence matched unmatched nodes edges attributes matched ones. furthermore averaging nodes edges separately shown beneﬁcial training otherwise edges dominate likelihood. overall reconstruction loss weighed previous terms ﬁrst term reconstruction loss enforces high similarity sampled generated graphs input graph second term kl-divergence regularizes code space allow sampling directly instead later. dimensionality usually fairly small autoencoder encouraged learn high-level compression input instead learning simply copy given input. regularization independent input space reconstruction loss must speciﬁcally designed input modality. following introduce graph decoder together appropriate reconstruction loss. graphs discrete objects ultimately. pose challenge encoding demonstrated recent developments graph convolution networks graph generation open problem far. related task text sequence generation currently dominant approach character-wise word-wise prediction however graphs arbitrary connectivity clear linearize construction sequence steps. hand iterative construction discrete structures training without step-wise supervision involves discrete decisions differentiable therefore problematic back-propagation. fortunately task become much simpler restrict domain graphs maximum nodes fairly small assumption handling dense graph representations still computationally tractable. propose make decoder output probabilistic fully-connected probabilistic graphs existence nodes edges modeled bernoulli variables whereas node edge attributes multinomial variables. discussed work continuous attributes could easily modeled gaussian variables represented mean variance. assume variables independent. tensor representation thus probamatrix contains node probabilities edge probabilities nodes edge attribute tensor rk×k×de indicates class probabilities edges similarly node attribute matrix rk×dn expressed integer quadratic programming problem similarity maximization typically approximated relaxation continuous domain case similarity function deﬁned follows empirically stable assignments training. summarize motivation behind equations method aims best graph matching improve gradient descent loss. given stochastic training deep network argue solving matching step approximately sufﬁcient. conceptually similar approach learning output unordered sets closest ordering training data sought. practice looking graph matching algorithm robust noisy correspondences easily implemented batch mode. max-pooling matching simple effective algorithm following iterative scheme power methods appendix details. used batch mode similarity tensors zero-padded i.e. amount iterations ﬁxed. max-pooling matching outputs continuous assignment matrix unfortunately attempts directly instead equation performed badly experiments direct maximization soft discretization softmax straight-through gumbel softmax therefore discretize using hungarian algorithm obtain strict one-on-one mapping. operation non-differentiable gradient still decoder directly loss function training convergence proceeds without problems. note approach often taken works object detection e.g. detections need matched ground truth bounding boxes treated ﬁxed encoder. feed forward network edge-conditioned graph convolutions used encoder although graph embedding method applicable. edge attributes categorical single linear layer ﬁlter generating network sufﬁcient. smaller graph sizes pooling used encoder except global employ gated pooling usual formulate encoder probabilistic enforce gaussian distribution last encoder layer outputs features interpreted mean variance allowing sample using re-parameterization trick disentangled embedding. practice rather random drawing graphs often desires control properties generated graphs. case follow sohn condition encoder decoder label vector associated input graph decoder concatenation encoder concatenated every node’s features graph pooling layer. size latent space small decoder encouraged exploit information label. limitations. proposed model expected useful generating small graphs. growth memory requirements number parameters well matching complexity small decrease quality high values section demonstrate results nevertheless many applications even generation small graphs still useful. quantitative evaluation generative models images texts troublesome difﬁcult measure realness generated samples automated objective way. thus researchers frequently resort qualitative evaluation embedding plots. however qualitative evaluation graphs unintuitive humans judge unless graphs planar fairly simple. fortunately found graph representation molecules undirected graphs atoms nodes bonds edges convenient testbed generative models. hand generated graphs easily visualized standardized structural diagrams. hand chemical validity graphs well many properties molecule fulﬁll checked using software packages simulations. makes qualitative quantitative tests possible. chemical constraints compatible types bonds atom valences make space valid graphs complicated molecule generation challenging. fact single addition removal edge change atom bond type make molecule chemically invalid. comparably ﬂipping single pixel mnist-like number generation problem issue. compare unconditional model characterbased generator g´omez-bombarelli grammar-based generator kusner used code architecture kusner baselines adapting maximum input length smallest possible. addition demonstrate conditional generative model artiﬁcial task generating molecules given histogram heavy atoms -dimensional label success easily validated. setup. encoder graph convolutional layers identity connection batchnorm relu; followed graph-level output formulation equation auxiliary networks single fully connected layer output channels; ﬁnalized outputting decoder fcls batchnorm embedding visualization. visually judge quality smoothness learned embedding model traverse ways along slice along line. former randomly choose c-dimensional orthonormal vectors sample regular grid pattern induced plane. latter randomly choose molecules label test interpolate embeddings also evaluates encoder therefore beneﬁts reconstruction error. plot planes figure frequent label less frequent label images show varied fairly smooth molecules. left image many valid samples broadly distributed across plane presumably autoencoder large portion database space. right exhibits stronger effect regularization valid molecules tend around center. example several interpolations shown figure meaningful less meaningful transitions though many samples lines form chemically valid compounds. decoder quality metrics. quality conditional decoder evaluated validity variety generated graphs. given label draw samples compute discrete point estimate decodings list chemically valid molecules list chemically valid molecules atom histograms equal interested ratios valid |/ns accurate |/ns. furthermore unique |set)|/|c fraction unique correct graphs novel |set) qm|/|set)| fraction novel out-ofdataset graphs; deﬁne unique novel finally introduced metrics aggregated frequencies labels e.g. valid validfreq). unconditional decoders evaluated assuming single label therefore valid accurate. table average generated molecules chemically valid case conditional models correct label decoder conditioned larger embedding sizes less regularized demonstrated higher number unique samples lower accuracy conditional figure decodings latent space points conditional model sampled random plane z-space left samples conditioned carbon nitrogen oxygen right samples conditioned carbon nitrogen oxygen color legend figure model decoder forced less rely actual labels. ratio valid samples shows less clear behavior likely discrete performance directly optimized for. models remarkable generated molecules dataset i.e. network never seen training. looking baselines cvae output valid samples expected gvae generates highest number valid samples variance additionally investigate importance graph matching using identity assignment instead thus learning reproduce particular node permutations training correspond canonical ordering smiles strings rdkit. ablated model produces many valid samples lower variety surprisingly outperforms gvae regard. comparison model achieve good performance metrics time. likelihood. besides application-speciﬁc metric introduced above also report evidence lower bound commonly used literature corresponds notation. table state mean bounds test using single sample graph. observe reconstruction loss kl-divergence decrease larger providing freedom. however seems strong correlation elbo implicit node probabilities. decoder assumes independence node edge probabilities allows isolated nodes edges. making fact molecules connected graphs investigate effect making node probabilities function edge probabilities. speciﬁcally consider probability node evaluation table shows clear improvement valid accurate novel metrics conditional unconditional setting. however paid lower variability higher reconstruction loss. indicates constraint useful model cannot fully cope zinc dataset contains druglike organic molecules heavy atoms distinct atomic numbers bond types split strategy investigate degree scalability unconditional generative model. figure linear interpolation row-wise pairs randomly chosen molecules z-space conditional model. color legend encoder inputs chemically invalid graphs valid graphs wrong label valid correct decoder quality metrics. best model archived valid clearly worse using implicit node probabilities brought improvement. comparison cvae failed generated valid sample gvae achieved valid attribute performance generally much higher chance producing chemically-relevant inconsistency conﬁrm relationship performance graph size kept graphs larger nodes corresponding zinc obtained valid verify problem likely caused proposed graph matching loss synthetically evaluate following. matching robustness. robust behavior graph matching using similarity function important good performance graphvae. study graph matching isolation investigate scalability. gaussian noise tensor input graph truncating renormalizing keep probabilistic interpretation create noisy version interested quality matching self using noisy assignment matrix advantage naive checking identity invariance permutation equivalent nodes. table vary tensor separately report mean accuracies random samples zinc size nodes. observe expected fall accuracy stronger noise behavior fairly robust respect increasing ﬁxed noise level sensitive adjacency matrix. note accuracies comparable across tables different dimensionalities random variables. conclude quality matching process major hurdle scalability. work addressed problem generating graphs continuous embedding context variational autoencoders. evaluated method molecular datasets different maximum graph size. achieved learn embedding reasonable quality small molecules decoder hard time capturing complex chemical interactions larger molecules. nevertheless believe method important initial step towards powerful decoders spark interesting community. table performance conditional unconditional models evaluated mean test-time reconstruction log-likelihood mean test-time evidence lower bound decoding quality metrics baselines cvae gvae listed embedding size highest valid. would like extend beyond proof concept applying real problems chemistry optimization certain properties predicting chemical reactions. advantage graph-based decoder compared smilesbased decoder possibility predict detailed attributes atoms bonds addition base structure might useful tasks. autoencoder might also used pre-train graph encoders ﬁne-tuning small datasets garrett siegel charles vishnu abhinav hodas nathan chemnet transferable generalizable deep neural network small-molecule property prediction. arxiv preprint arxiv. g´omez-bombarelli rafael duvenaud david hern´andez-lobato jos´e miguel aguilera-iparraguirre jorge hirzel timothy adams ryan aspuru-guzik al´an. automatic chemical design using data-driven continuous representation molecules. corr abs/. irwin john sterling teague mysinger michael bolstad erin coleman ryan zinc free tool discover chemistry biology. journal chemical information modeling ktena soﬁa parisot sarah ferrante enzo rajchl martin matthew glocker rueckert daniel. distance metric learning using graph convolutional networks application functional brain networks. miccai segler marwin kogej thierry tyrchan christian waller mark generating focussed molecule libraries drug discovery recurrent neural networks. corr abs/. unconditional models achieve mean test loglikelihood roughly log-likelihoods signiﬁcantly higher tables architecture achieve perfect reconstruction inputs. successful increase training log-likelihood zero ﬁxed small training sets hundreds examples network could overﬁt. indicates network problems ﬁnding generally valid rules assembly output tensors. sia;jb denote column-wise replica relaxed graph matching problem expressed quadratic programi= optimization strategy choice derived equivalent power method iterative update rule sx/||sx||. starting correspondences initialized uniform rule iterated convergence; case ﬁxed amount iterations. context graph matching matrix-vector product interpreted sum-pooling match candixjbsia;jb denote neighbors node authors argue formulation strongly inﬂuenced uninformative irrelevant elements propose robust max-pooling version considers best pairwise similarity neighbor", "year": 2018}