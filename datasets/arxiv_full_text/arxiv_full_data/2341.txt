{"title": "Interpolating Conditional Density Trees", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Joint distributions over many variables are frequently modeled by decomposing them into products of simpler, lower-dimensional conditional distributions, such as in sparsely connected Bayesian networks. However, automatically learning such models can be very computationally expensive when there are many datapoints and many continuous variables with complex nonlinear relationships, particularly when no good ways of decomposing the joint distribution are known a priori. In such situations, previous research has generally focused on the use of discretization techniques in which each continuous variable has a single discretization that is used throughout the entire network. \\ In this paper, we present and compare a wide variety of tree-based algorithms for learning and evaluating conditional density estimates over continuous variables. These trees can be thought of as discretizations that vary according to the particular interactions being modeled; however, the density within a given leaf of the tree need not be assumed constant, and we show that such nonuniform leaf densities lead to more accurate density estimation. We have developed Bayesian network structure-learning algorithms that employ these tree-based conditional density representations, and we show that they can be used to practically learn complex joint probability models over dozens of continuous variables from thousands of datapoints. We focus on finding models that are simultaneously accurate, fast to learn, and fast to evaluate once they are learned.", "text": "joint distributions frequently modeled decomposing products simpler lower-dimensional conditional connected tomatically computationally many datapoints ables complex particularly posing joint distribution ori. situations generally techniques vari­ able single discretization throughout entire network. bayesian networks popular method represent­ joint probability bayesian network contains directed vertex graph variable domain. directed independence ables. define variables whose nodes graph \"parents\" pendence relationships lows given values information conditionally variables corre­ sponding nodes descendants graph. independence decompose joint probability distribution ii=l number variables domain. thus addition also specify every variable specified valid probability entire domain. paper present compare wide variety tree-based algorithms evaluating conditional continuous variables. thought discretizations vary cording particular modeled; however leaf tree need assumed con­ stant show nonuniform leaf densities lead accurate density esti­ mation. developed bayesian net­ work structure-learning ploy tree-based conditional density rep­ resentations used practically ability ables thousands data points. focus finding models simultaneously curate fast learn learned. bayesian networks commonly used sit­ uations variables discrete; tinuous variables ically assumed follow simple parametric butions gaussians researchers recently vestigated within bayesian networks; gaussians gaussian kernel-based gaussian processes used approximate conditional probability tions continuous putationally expensive problematic structure dependencies priori structure cases search network usually using discretized data range variable number variable lent. discretization network structure-learning structure distributions search network tion policies szmidt previous variable tion particular interactions figure shows example distribution function several discrete continuous tion algorithm tree's variables continuous trinary discrete variable algorithm taking first possible second. decision distributions \"classification mately involves likely value) decision regression trees typically greedy top-down divide-and-conquer employ algorithm experiments paper. decision regression \"stump\" depth grown possible branching vari­ able. algorithm chooses branch vari­ able whose corresponding stump total conditional log-likelihoods subset training stump-training cursively appropriate ting discrete child every possible variable; splitting branch children variable midpoint current sible subrange variable. branching terminated training current subtree. training sion tree. many variations considered regression trees tinuous conditional fact near-gaussian involves nalized squared distance ever situations reasonably accurate models distributions complicated many possible criteria curacy models; common kullback-leibler model true distribution. scientific evaluations test sets cross-validation analogues divergence. density tree path root tree leaf first passes sequence branch nodes test parent variables another sequence branch nodes test child estimating conditional uous variable given another might look like figure clarity listed conditional leaves conditional ties trivially computed masses viding volumes leaves. note represent masses subtree containing parent variables forces learn trees stratified structure ables allowed alternate straint becomes nonlocal approaches learning tree difficult. details.) recursive learn stratified algorithm trees except wherever decision tree learner would call routine returns simple leaf distribution fitting provided data strat­ ified conditional learning procedure. subtree-learning also identical algorithm sion regression branch output variable algorithm mass child allocated proportion fall child's sub­ tree. entire subtree learned cart-like algorithm merely needs learn leaf dis­ tribution manner significantly expensive learn cart-like shall provide much accurate density estimation. leaf distributed stratified conditional variable-resolution parametric distributions cart-like trees use. however choices lead accurate den­ sity estimation. stratified previous section model conditional trees much level conditional ally expensive learn. many heuristics could tried alleviate learn­ cart -like tree first using tree's struc­ ture starting conditional sity tree. however heuristics increase accuracy resulting likely decrease achieve accurate density learning using alternative section discuss density trees obtain condi­ modeling joint distributions tional density estimates tree specifies joint probability take specific values probability within leaf's range given datapoint lies somewhere within bounds assuming density tree representing estimate particular joint density trees trivially bayesian classifiers manner. particular trees employed paper models discrete able independently nomial distribution) crete variables density tree one-level root node branching variable naive bayes classifiers model conditional distributions networks commonly used bayesian classifier continuous variables gaussian; model class distribution classifier obtained simply density stump branching class variable leaves employ­ gaussian distributions ables. generally discrete variables branch structure sity tree output variable tested branch node tests performed input variables subsequent levels joint density tree used estimate condi­ tional distributions ilar form function hybrid decision naive bayesian classifier also developed search equal zero. either equation gives simple calculating ditional joint distributions tion within leaf marginalized compute precompute marginalized marginalization constant-density work kozlov koller message-passing rithms inference models tree computed compute conditional tion simply numerator computing denominator require locating bution appropriate useful leaf density estimators dition including distributions trees leaves results marginalized whose leaves contain mixture distributions components nificant amount operations density trees sampling compression able compute functions ations much naturally leaf probabilities ditional pose algorithm capable sample gaussian distribution. routine generate random sample mixture gaussians mixture component sample corresponding hand straightforward generate random sample represented however situations evaluation conditional auxiliary tree single leaf original tree structure density tree branching ables performed child variable. create auxiliary structure first using marginalization similar employed kozlov koller. produces tree marginalization branches removed con­ tains leaf every distinct possible leaves original tree single 'lfj. recursively figure example conditionalized tree. leaf auxiliary pointer back single leaf original tree geometrical representation shown left tree-based representation right. auxiliary conditional relatively original given value it;. speedup roughly similar would achieved using marginalized density tree mixture models leaves compute roughly factor however experiments speed conditional evaluation joint den­ sity trees introducing tion. within context subtree branching ditional tree leaf mean datapoints ta's constraints. conditional imately tinuous leaf's bution expressed algorithm maximize log-likelihood cause distribution mixture fixed prior probabilities components tively quickly suboptimal trapped fast higher levels apoints algorithm d-dimensional datapoints pendent iterations. described pling limitation negligible density actually sity estimation tional density trees optimized ditional distributions lationship helps function completely experimental density timates conditionalized approximately coefficients tree-learning oriented data-either stratified case joint density log-likelihood quality form arbitrarily plex fully bayesian commonly around namely adjust overall bution slightly ad-hoc fashion. simplicity known priori variables. details discussion handle scenarios. section types described large scientific datasets. sampling synthetic datapoints mensions. data high-throughput records tinuous; able take either values. experiments tribution tribution experimented represent butions tinuous variables stant matrices covariance figure shows sample experimental results cart -like stratified different cart-like gaussians parent variables sion. stratified results previous performed; test sets well empirically best algorithm confidence interval. dataset shown bold italics worse least confi­ dence according synthetic model conditional distribution able given other; datasets using bayesian variables fixed structure. density space restrictions network details. algorithm allows eling joint probability uous discrete variables accuracy time. compare works global mixture variables stutz results higher-dimensional bayesian rate density learned well even gaussian would favor autoclass's els. difference dramatic approaches depends better fares better explored resentations shown used feasibly networks many thousands models simultaneously resulting accu­ rate faster learn faster evaluate global mixture mentally oped global", "year": 2012}