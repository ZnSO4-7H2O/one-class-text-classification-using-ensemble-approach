{"title": "Learning to learn with backpropagation of Hebbian plasticity", "tag": ["cs.NE", "cs.AI", "cs.LG", "q-bio.NC", "I.2.6; I.5"], "abstract": "Hebbian plasticity is a powerful principle that allows biological brains to learn from their lifetime experience. By contrast, artificial neural networks trained with backpropagation generally have fixed connection weights that do not change once training is complete. While recent methods can endow neural networks with long-term memories, Hebbian plasticity is currently not amenable to gradient descent. Here we derive analytical expressions for activity gradients in neural networks with Hebbian plastic connections. Using these expressions, we can use backpropagation to train not just the baseline weights of the connections, but also their plasticity. As a result, the networks \"learn how to learn\" in order to solve the problem at hand: the trained networks automatically perform fast learning of unpredictable environmental features during their lifetime, expanding the range of solvable problems. We test the algorithm on various on-line learning tasks, including pattern completion, one-shot learning, and reversal learning. The algorithm successfully learns how to learn the relevant associations from one-shot instruction, and fine-tunes the temporal dynamics of plasticity to allow for continual learning in response to changing environmental parameters. We conclude that backpropagation of Hebbian plasticity offers a powerful model for lifelong learning.", "text": "hebbian plasticity powerful principle allows biological brains learn lifetime experience. contrast artiﬁcial neural networks trained backpropagation generally ﬁxed connection weights change training complete. recent methods endow neural networks long-term memories hebbian plasticity currently amenable gradient descent. derive analytical expressions activity gradients neural networks hebbian plastic connections. using expressions backpropagation train baseline weights connections also plasticity. result networks learn learn order solve problem hand trained networks automatically perform fast learning unpredictable environmental features lifetime expanding range solvable problems. test algorithm various on-line learning tasks including pattern completion one-shot learning reversal learning. algorithm successfully learns learn relevant associations oneshot instruction ﬁne-tunes temporal dynamics plasticity allow continual learning response changing environmental parameters. conclude backpropagation hebbian plasticity offers powerful model lifelong learning. living organisms endowed neural systems exhibit remarkably complex behaviors. much complexity results evolutionary learning millions years also results ability neural systems learn experience lifetime. indeed ability lifelong learning product evolution fashioned overall connectivity brain also plasticity connections. lifetime learning beneﬁcial several reasons. thing many environmental features simply predicted birth and/or change time requiring learning experience contact environment. furthermore even predictable environmental features much information necessary produce adaptive behavior obtained free learning environment thus removing potentially huge chunk search space evolution must explore. example connectivity primary visual cortex fashioned hebbian plasticity rather single connection genetically speciﬁed allowing huge number cells organize powerful reliable information-processing system minimal genetic speciﬁcation. lifetime long-term plasticity living brains generally follows hebbian principle cell consistently contributes making another cell build stronger connection cell. note generic principle implemented many different ways including covariance learning instar outstar rules learning etc. references therein). backpropagation train neural networks perform remarkably complex tasks. however generally used train ﬁxed-weights networks changes connectivity training. several methods proposed make lifelong learning amenable backpropagation including recently neural turing machines memory networks however would useful incorporate powerful well-studied principle hebbian plasticity backpropagation training. derive analytical expressions activity gradients neural networks hebbian plastic connections. using expressions backpropagation train baseline weights connections also plasticity. allows backpropagation learn learn order solve general types problems unpredictable features rather speciﬁc instances. software used present paper available http//github.com/thomasmiconi. consider networks strength connection vary according hebbian plasticity course network’s lifetime. arrange things network fully speciﬁed ﬁxed parameters determine baseline weight degree plasticity connection. training parameters ﬁxed unchanging network’s lifetime govern connection changes time result experience according hebbian plasticity. model hebbian plasticity maintain time-dependent quantity connection network call hebbian trace connection. noted above many possible expressions hebbian plasticity paper simplest stable form hebbian trace namely running average product prepost-synaptic activities. thus given target cell hebbian trace associated k-th incoming connection deﬁned follows activity post-synaptic cell activity pre-synaptic cell time constant. expressions hebbian plasticity possible simple form turns adequate present purposes simpliﬁes mathematics. hebbian trace maintained automatically independently network parameters connection. given hebbian trace actual strength connection time determined ﬁxed parameters ﬁxed weight determines baseline unchanging component connection; plasticity parameter speciﬁes much hebbian trace inﬂuences actual connection. formally response given cell written function inputs follows order backpropagation must gradient parameters. importantly gradients necessarily involve activities previous times since plasticity activity time inﬂuences activity future times effects hebbian traces. fortunately gradients turn simple recursive form. equations express gradient function gradients recursively. equations summand previous times essentially partial derivative hebbian traces time respect either since hebbian trace exponential average previous products partial derivatives turn sums previous gradients corresponding parameter multiplied concomitant activity input cell thus gradient time function gradients times note hebbian traces inputs associated connection computing gradient. values affecting also inﬂuence hebbian traces connections turn affect later times. effect must accounted gradients. expression omits tanh nonlinearity really provides gradient expression within curly braces output provided incoming excitation biases. obtain full gradient simply rewrite tanh apply chain rule provided tasks described below lifetime experience divided episodes lasts certain number timesteps. beginning episode hebbian traces initialized then timestep network processes input pattern produces output according current parameters hebbian traces updated according equation furthermore errors gradients also computed. episode errors gradients timestep used update network parameters according error backpropagation gradient descent. whole process iterates ﬁxed number episodes. last episodes training stops networks parameters frozen. source code experiments available http//github.com/thomasmiconi. test bohp method ﬁrst apply task hebbian learning known efﬁcient namely pattern completion. network composed input output layer neurons. every episode network ﬁrst exposed random binary vector length least nonzero element. binary vector represents pattern learned. then next timestep partial pattern containing non-zero bits pattern presented. task network produce full pattern output layer. error episode manhattan distance network’s output second time step full pattern algorithm quickly reliably learns solve task ﬁnal networks training exhibit expected pattern input node sends strong ﬁxed connection figure results pattern completion experiment. mean absolute error timestep episode mutually exclusive stimuli. dark line indicates median error runs shaded areas indicate interquartile range. last episodes training halted parameters frozen. schema typical network training. elements shown clarity corresponding output node well plastic connection every output node. result pattern presentation non-zero input develops strong connection every non-zero output hebbian learning ensuring successful pattern completion second step nonzero inputs stimulated. task episode network must learn associate random binary vectors label. labels simply two-element vectors vectors other. importantly learning one-shot ﬁrst timestep input consists ﬁrst pattern sufﬁxed label second timestep input vector second pattern sufﬁxed label times labels presented inputs times input patterns sufﬁxed neutral sufﬁx network’s output must label associated current pattern. patterns random vectors elements value least position differing patterns learned networks input layer hidden layer output layer simplicity ﬁrst layer weights plasticity. ﬁnal layer implements softmax competition nodes. episode lasts timestep ﬁrst contain expected label pattern. cross-entropy loss output values figure results one-shot learning experiment. median absolute error timestep episode. conventions figure schema typical network training. addition label nodes pattern elements shown clarity text details. figure results reversal learning experiment. conventions figure mean absolute error timestep episode. schema typical network training. notice negative plasticity connections pattern nodes hidden layer. text details. expected label time step except -step learning period network output ignored. again algorithm reliably learns solve task trained networks organized hidden node learns pattern label learns pattern associated label receive strong ﬁxed connections label bits receive strong plastic connections pattern bit. allows node imprinted corresponding pattern. weights hidden layer ensures nodes produce adequate label. note networks displayed somewhat different pattern connections pattern nodes hidden nodes negative plasticity coefﬁcients. discuss conﬁguration next subsection. previous experiments show algorithm train networks learn fast associations environmental inputs. also teach networks adapt changing environment perform continual learning lifetime? test this adapt previous one-shot learning task continual learning task halfway episode invert patterns pattern previously associated label associated label vice-versa. show pattern updated label once. resume showing input patterns neutral sufﬁxes expect network’s output adequate label input pattern. algorithm also successfully learns solve problem networks somewhat similar ones obtained one-shot learning important difference connections pattern input hidden nodes consistently negative plasticity coefﬁcients. networks trained one-shot learning task also feature networks trained reversal learning consistently show seems crucial feature reversal learning clipping plasticity coefﬁcients positive values prevents learning task still allowing successful learning one-shot task reversal learning seem require negative plasticity? negative plasticity implies hidden layer ﬁring anti-correlated presence imprinted stimulus. unlikely effect since effect negated switching signs output layer label node inputs. however negative plasticity also important consequence making hebbian traces decrease time rather increase would plasticity coefﬁcient positive. well-known hebbian plasticity creates positive feedback correlation input output increases connection weight turn increases correlation ﬁring etc. would pose problem one-shot reversal learning time patterns shown existing associations would strong erased single timestep. however negative plasticity coefﬁcients opposite true large hebbian trace created initial imprinting becomes self-decreasing negative feedback loop. result hebbian traces created ﬁrst association decrease time. little effect ongoing responses since output nonlinearities magnify even small differences produce correct responses; reason one-shot learning mostly indifferent sign plasticity coefﬁcients since episodes short enough ﬁnal hebbian traces always large enough support correct choice. however case reversal learning decreasing hebbian traces vital second association shown existing hebbian traces small enough completely erased single presentation would case positive plasticity increasing hebbian traces. short bohp method determined connections must plastic learn association; also develop precise ﬁne-tuning temporal dynamics plasticity modulating sign plasticity coefﬁcients. remarkable result conﬁrms potential bohp deal temporal dynamics environmental learning. paper introduced method designing networks endowed hebbian plasticity gradient descent backpropagation. method allows network learn learn order solve problem unpredictable parameters. method successfully solved simple learning tasks including one-shot reversal learning. expository simple form hebbian plasticity namely running average product prepost-synaptic activities. however possible formulations hebbian plasticity covariance learning instar outstar rules learning. implemented bohp rewriting gradient equations appropriately might expand capacities bohp. however shown above simple hebbian plasticity used already produce fast reliable lifelong learning. furthermore networks shown ﬁxed plasticity constants determine strength plasticity. however biological brains plasticity modulated time various neuromodulators neural control. thus biological brains decide where apply plasticity crucial learning complex behaviors. neuromodulation used neural networks built evolutionary methods methods described paper could extended allow backpropagation design modulable-plasticity networks. conclusion suggest backpropagation hebbian plasticity efﬁcient endow neural networks long-term memories lifelong learning abilities. provide derivation gradients output cell response given timestep regard coefﬁcient incoming connection first simply write full expression equation ﬁrst summand right-hand side denotes inputs incoming connections ﬁxed weights; since term depend remove gradient computation. second summand denotes inputs plastic connections. cases must handled differently since differentiating regard regard derivative ﬁrst right-hand-side term form d)/dαk since hebbl depends neither does. contrast second right-hand-side term form d)/dαk must develop using identity therefore last equation above runs incoming connections including recursive gradient equation identical derived much manner identity). future applications many-layers networks equations gradient", "year": 2016}