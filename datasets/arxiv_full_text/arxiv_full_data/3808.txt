{"title": "Convolutional Neural Networks for Sentence Classification", "tag": ["cs.CL", "cs.NE"], "abstract": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.", "text": "report series experiments convolutional neural networks trained pre-trained word vectors sentence-level classiﬁcation tasks. show simple little hyperparameter tuning static vectors achieves excellent results multiple benchmarks. learning task-speciﬁc vectors ﬁne-tuning offers gains performance. additionally propose simple modiﬁcation architecture allow task-speciﬁc static vectors. models discussed herein improve upon state tasks include sentiment analysis question classiﬁcation. introduction deep learning models achieved remarkable results computer vision speech recognition recent years. within natural language processing much work deep learning methods involved learning word vector representations neural language models performing composition learned word vectors classiﬁcation word vectors wherein words projected sparse -of-v encoding onto lower dimensional vector space hidden layer essentially feature extractors encode semantic features words dimensions. dense representations semantically close words likewise close—in euclidean cosine distance—in lower dimensional vector space. local features originally invented computer vision models subsequently shown effective achieved excellent results semantic parsing search query retrieval sentence modeling traditional tasks present work train simple layer convolution word vectors obtained unsupervised neural language model. vectors trained mikolov billion words google news publicly available. initially keep word vectors static learn parameters model. despite little tuning hyperparameters simple model achieves excellent results multiple benchmarks suggesting pre-trained vectors ‘universal’ feature extractors utilized various classiﬁcation tasks. learning task-speciﬁc vectors ﬁne-tuning results improvements. ﬁnally describe simple modiﬁcation architecture allow pre-trained task-speciﬁc vectors multiple channels. work philosophically similar razavian showed image classiﬁcation feature extractors obtained pretrained deep learning model perform well variety tasks—including tasks different original task feature extractors trained. model architecture shown ﬁgure slight variant architecture collobert k-dimensional word vector corresponding i-th word sentence. sentence length rn−h+. apply max-overtime pooling operation feature take maximum value max{c} feature corresponding particular ﬁlter. idea capture important feature—one highest value—for feature map. pooling scheme naturally deals variable sentence lengths. described process feature extracted ﬁlter. model uses multiple ﬁlters obtain multiple features. features form penultimate layer passed fully connected softmax layer whose output probability distribution labels. kept static throughout training ﬁne-tuned backpropagation multichannel architecture illustrated ﬁgure ﬁlter applied channels results added calculate equation model otherwise equivalent single channel architecture. regularization employ dropout penultimate layer constraint l-norms weight vectors dropout prevents co-adaptation hidden units randomly dropping out—i.e. setting zero—a proportion hidden units fowardbackpropagation. given penultimate layer instead using element-wise multiplication operator ‘masking’ vector bernoulli random variables probability gradients backpropagated unmasked units. test time learned weight vectors scaled used score unseen senˆ tences. additionally constrain l-norms weight vectors rescaling ||w|| whenever ||w|| gradient descent step. table summary statistics datasets tokenization. number target classes. average sentence length. dataset size. vocabulary size. |vpre| number words present pre-trained word vectors. test test size movie reviews sentence review. classiﬁcation involves detecting positive/negative reviews sst- stanford sentiment treebank—an extension train/dev/test splits provided ﬁne-grained labels re-labeled socher https//www.cs.cornell.edu/people/pabo/movie-review-data/ http//nlp.stanford.edu/sentiment/ data actually provided phrase-level hence train model phrases sentences score sentences test time socher kalchbrenner mikolov thus training order magnitude larger listed table http//cogcomp.cs.illinois.edu/data/qa/qc/ http//www.cs.uic.edu/∼liub/fbs/sentiment-analysis.html otherwise perform datasetspeciﬁc tuning early stopping sets. datasets without standard randomly select training data set. training done stochastic gradient descent shufﬂed mini-batches adadelta update rule initializing word vectors obtained unsupervised neural language model popular method improve performance absence large supervised training publicly available wordvec vectors trained billion words google news. vectors dimensionality trained using continuous bag-of-words architecture words present pre-trained words initialized randomly. cnn-static model pre-trained vectors wordvec. words— including unknown ones randomly initialized—are kept static parameters model learned. cnn-non-static premodel cnn-rand cnn-static cnn-non-static cnn-multichannel mv-rnn rntn dcnn paragraph-vec ccae sent-parser nbsvm g-dropout f-dropout tree-crf crf-pr svms table results models methods. recursive autoencoders pre-trained word vectors wikipedia mv-rnn matrix-vector recursive neural network parse trees rntn recursive neural tensor network tensor-based feature function parse trees dcnn dynamic convolutional neural network k-max pooling paragraph-vec logistic regression paragraph vectors ccae combinatorial category autoencoders combinatorial category grammar operators sent-parser sentiment analysis-speciﬁc parser nbsvm naive bayes multinomial naive bayes uni-bigrams wang manning g-dropout f-dropout gaussian dropout fast dropout wang manning tree-crf dependency tree conditional random fields crf-pr conditional random fields posterior regularization svms uni-bi-trigrams word head word parser hypernyms hand-coded rules features silva order disentangle effect variations versus random factors eliminate sources randomness—cv-fold assignment initialization unknown word vectors initialization parameters—by keeping uniform within dataset. results models methods listed table baseline model randomly initialized words perform well own. expected performance gains pre-trained vectors surprised magnitude gains. even simple model static vectors performs remarkably well giving competitive results sophisticated deep learning models utilize complex pooling schemes require parse trees computed beforehand results suggest pretrained vectors good ‘universal’ feature extractors utilized across datasets. finetuning pre-trained vectors task gives still improvements multichannel single channel models initially hoped multichannel architecture would prevent overﬁtting thus work better single channel model especially smaller datasets. results however mixed further work regularizing ﬁne-tuning process warranted. instance instead using additional channel non-static portion could maintain single channel employ extra dimensions allowed modiﬁed training. table neighboring words—based cosine similarity—for vectors static channel ﬁnetuned vectors non-static channel multichannel model sst- dataset training. static non-static representations case single channel non-static model multichannel model able ﬁne-tune non-static channel make speciﬁc task-at-hand. example good similar wordvec presumably syntactically equivalent. vectors non-static channel ﬁnetuned sst- dataset longer case similarly good arguably closer nice great expressing sentiment indeed reﬂected learned vectors. tokens pre-trained vectors ﬁne-tuning allows learn meaningful representations network learns exclamation marks associated effusive expressions commas conjunctive architecture single channel model. example max-tdnn randomly initialized words obtains sst- dataset compared model. attribute discrepancy much capacity improvewordvec obtained slight ments sampling dimension chosen randomly initialized vectors variance pre-trained ones. would interesting employing sophisticated methods mirror distribution pre-trained vectors initialization process gives improvements. brieﬂy experimented another publicly available word vectors trained wikipedia collobert found wordvec gave superior performance. clear whether mikolov architecture billion word google news dataset. present work described series experiments convolutional neural networks built wordvec. despite little tuning hyperparameters simple layer convolution performs remarkably well. results well-established evidence unsupervised pre-training word vectors important ingredient deep learning nlp. toutanova platt meek. learning discriminative projections text similarity measures. proceedings fifteenth conference computational natural language learning", "year": 2014}