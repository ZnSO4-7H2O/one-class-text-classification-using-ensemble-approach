{"title": "A Hybrid Loss for Multiclass and Structured Prediction", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We propose a novel hybrid loss for multiclass and structured prediction problems that is a convex combination of a log loss for Conditional Random Fields (CRFs) and a multiclass hinge loss for Support Vector Machines (SVMs). We provide a sufficient condition for when the hybrid loss is Fisher consistent for classification. This condition depends on a measure of dominance between labels--specifically, the gap between the probabilities of the best label and the second best label. We also prove Fisher consistency is necessary for parametric consistency when learning models such as CRFs. We demonstrate empirically that the hybrid loss typically performs least as well as--and often better than--both of its constituent losses on a variety of tasks, such as human action recognition. In doing so we also provide an empirical comparison of the efficacy of probabilistic and margin based approaches to multiclass and structured prediction.", "text": "abstract—we propose novel hybrid loss multiclass structured prediction problems convex combination loss conditional random fields multiclass hinge loss support vector machines provide sufﬁcient condition hybrid loss fisher consistent classiﬁcation. condition depends measure dominance labels speciﬁcally probabilities best label second best label. also prove fisher consistency necessary parametric consistency learning models crfs. demonstrate empirically hybrid loss typically performs least well often better constituent losses variety tasks human action recognition. also provide empirical comparison efﬁcacy probabilistic margin based approaches multiclass structured prediction. machines seen representative different approaches classiﬁcation problems. former purely probabilistic conditional probability classes given observation explicitly modelled latter classiﬁcation performed without attempt model probabilities. approaches strengths weaknesses. crfs known yield bayes optimal solution asymptomatically known tight generalisation bounds. contrast svms tighter generalisation bounds typically shrink margin grows easily incorporate interesting labelcost score hamming distance structured cases. svms could inconsistent classes despite differences crfs svms appear similar viewed optimisation problems. salient difference loss used each crfs trained using loss svms typically hinge loss. attempt capitalise relative strengths avoid weaknesses propose novel hybrid loss blends losses. background provide following analysis argue fisher consistency classiﬁcation a.k.a. classiﬁcation calibration coarse notion introduce distribution-dependent reﬁnement hengel wang australian centre visual technologies computer vision group university adelaide australia. e-mail {javen.shi anton.vandenhengelzhenhua.wang}adelaide.edu.au reid caetano australian national university called conditional fisher consistency classiﬁcation prove hybrid loss conditionally give noise condition relates hybrid loss’s mixture parameter margin-like property data distribution show that although effectively non-parametric condition also necessary condition consistent risk minimisation using parametric models finally empirically test hybrid loss various domains including multiclass classiﬁcation text chunking human action recognition show consistently performs least well often better constituent losses losses classiﬁcation problems observations paired labels joint distribution write joint probability conditional probability given since labels ﬁnite discrete also notation conditional probability emphasise distributions thought vectors also denote distributions reserve distributions generated models. drawn i.i.d. learner produce predictor minimises misclassiﬁcation error since true distribution unknown approximate solution problem typically found minimising regularised empirical classes theory although many multiclass algorithms intractable structured case. structured labels formed assume structured input-output pairs i.i.d. joint distribution. predictors models learned fashion similar models usually speciﬁed terms parameter vector feature deﬁning case choice regulariser framework used implement svms crfs used experiments described section although much analysis assume particular parametric model explicitly discuss implications mixture losses controlled parameter setting recovers loss hinge loss respectively. intention choosing close emphasise maximum largest second largest label probabilities close force models prefer accurate probability assessments strong classiﬁcation. family hybrid losses similar recent proposal zhang also deﬁne single parameter family loss functions called coherence functions interpolate hinge loss loss closely related loss based log-likelihood. like loss presented here losses surrogates loss families hinge loss limit point. difference proposals consistency losses family coherence losses fisher consistent probability estimation whereas hybrid losses satisfy weaker model assigns vector scores observation regulariser penalises overly complex functions. model found transformed predictor deﬁning argmaxy∈y ties broken arbitrary deterministic overload deﬁnition misclassiﬁcation error sometimes write shorthand otherwise maxy=y margin vector intuitively hinge loss minimised models classify observations correctly also maximise difference highest second highest scores assigned labels. other consistent losses svms cannot scale large example j=y]+ shown consistent however requires evaluating possible labels except true intractable labels possible assignments grow exponentially. known consistent multiclass hinge losses similar intractability. structured prediction multiclass case assumed i.i.d. however many cases i.i.d. structured prediction deal cases grouping correlated labels form structured label structured label object associated example automated paragraph breaking problem input document output sequence whose entries denote beginning positions paragraphs. image segmentation input image structured label lattice {yij}i=··· n;=··· framework probabilistic graphical models provides principled modelling dependencies components components i.e. graph consists node {··· edge reﬂects dependencies. assuming component {··· many possible assignments words structured label seen multiclass problem many despite properties coherence functions using structured cases intractable. require evaluation function classes i.e. algorithm step note grows exponentially structured cases. encounter problem consistent multiclass svms. hybrid loss problem. fisher consistency classification desirable property loss that given enough data models obtained minimising loss observation make predictions consistent true label probabilities observation. mainly concerned distributions ﬁxed therefore overload denote distribution whether represents distribution labels distribution labels observations clear context. vector aligned distribution whenever maximisers also maximisers argmaxy∈y argmaxy∈y since probabilistic models described pass components vector rescale clear prediction aligned aligned correspondence following deﬁnitions consistency equivalent regardless whether general models losses probabilistic counterparts used. label distributions minimising conditional risk ey∼d] loss yields vector aligned fisher consistent classiﬁcation classiﬁcation calibrated important property losses since related asymptotic consistency empirical risk minimiser loss standard multiclass hinge loss known inconsistent classiﬁcation classes analysis shows hinge loss inconsistent whenever instance non-dominant distribution conversely distribution dominant instance contrast loss used train non-parametric crfs fisher consistent probability estimation associated risk minimised true conditional distribution thus since minimising distribution equal thus aligned note fisher consistency classiﬁcation weaker fisher consistency density estimation. former requires prediction only latter requires estimated density true data distribution. paper focus former only. analysis fisher consistency density estimation refer reader conditional consistency hybrid loss order analyse consistency hybrid loss require following reﬁned notion fisher consistency. distribution labels loss conditionally respect whenever minimising conditional risk w.r.t. ey∼d yields predictor aligned course loss conditionally w.r.t. deﬁnition fcc. theorem distribution dmax maxy largest probability assigned also ymax dmax} labels maximal probability dnext maxy /∈ymax second largest probability assigned label dnext ymax hybrid loss conditionally whenever dmax proof contradiction proceeds high level showing distribution satisﬁes dmax minimiser risk aligned derive falsehood. argument broken cases risk minimising distribution unique maximum probability not. cases show construct alternative distribution yielding required contradiction. ﬁrst case obtained swapping probable label second case obtained perturbing slightly towards proof since free permute labels within assume without loss generality ties probable label ymax deﬁning ey∼d proof proceeds contradiction assuming minimiser argminq aligned occur must label least large simplicity without loss generality assume label largest probability according also free permuted labels within ymax ensure argmaxy∈ymax ﬁrst case consider strictly larger construct distribution swaps values leaves values unchanged. y−{t y∗}. intuitively show point closer therefore component loss reduced theorem inverted interpreted constraint conditional distributions data distribution hybrid loss parameter yield consistent predictions. speciﬁcally hybrid loss consistent dominant label probabilities larger case classiﬁcation problem instance sense difﬁcult disambiguate. sense bound seen property distributions akin tsybakov’s noise condition conditions non-constructive depend unknown distribution provide guidance effect parameter choices exploring relationship conditional tsybakov noise condition focus ongoing work. parametric consistency since fisher consistency deﬁned point-wise observations directly applicable parametric models enforce inter-observational constraints abstractly assuming parametric hypotheses seen restriction space allowable scoring functions. learning parametric models risks minimised subset functions instead possible functions. show that given weak assumptions hypothesis class loss necessary condition loss also f-consistent. suppose maximum probability label least maximising labels coincides maximising labels case show slight perturbation yields distribution strictly smaller loss. deﬁne also thus since substituting inequality deﬁnition loss f-consistent distribution minimising associated risk yields classiﬁer minimal loss recall section risk hypothesis associated loss distribution risk misclassiﬁcation error classiﬁer deterministically derived tie-breaking procedure precisely tie-breaker function power guarantees non-empty finally deﬁne classiﬁer derived using intuitively ﬁrst condition says distribution labels must function class models perfectly point input space. second condition requires mode modelled input function ties maximum value. importantly properties fairly weak anything constraints function class might relationships distributions modelled different inputs. proof proof contradiction. assume regular function class loss fconsistent fcc. holds exists distribution minimises conditional risk aligned argmaxy∈y py). assumption regularity property means deﬁne distribution puts mass {x}×y since distribution concentrated single full risk conditional risk same. thus choice tie-breaker used deﬁne construction implies py∼p label predicted however since aligned assumption holds free choose tie-breaker deﬁning argmaxy thus since argmaxy second regularity property must also unique maximiser since unique maximiser choice tie-breaker result classiﬁer satisfying must guarantee therefore arrive contradiction since minimiser thus shown exists distribution minimiser risk minimiser misclassiﬁcation rate contradicting assumption f-consistency therefore must fcc. analysis hybrid loss suggests outperform hinge loss improved consistency distributions non-dominant labels. furthermore also make efﬁcient data loss distributions dominant labels. hypotheses conﬁrmed next section applying hybrid hinge losses number synthetic multiclass data sets data size proportion examples non-dominant labels carefully controlled. also compare hybrid loss hinge losses several real structured estimation problems observe hybrid loss regularly outperforms losses consistently performs least well either losses problem. multiclass classification types multiclass simulations performed. ﬁrst examined performances hybrid hinge losses observation dominant label. observations drawn labels second experiment considered distributions controlled mixture observations dominant non-dominant labels. non-dominant distributions make experiment simple possible considered observation space size focused varying number labels probabilities. losses observing entire data population. fig. performance hybrid hinge losses non-dominant/dominant mixtures. points denote pairs test accuracies models trained data sets using losses named axes. score denotes vertical loss wins losses case. thus plot resulting training errors hinge hybrid losses figure function number labels. clearly hinge loss error increases number classes increases whereas errors hybrid losses remain constant concordance consistency analysis. models found using lbfgs inexact line search thus landing hinge point almost never happens. theory problem converge non-smooth optimisation problem. practice works well. non-dominant dominant distributions second synthetic experiment examined three losses performed given various training sizes various proportions instances non-dominant distributions generated different data sets following manner instances came either non-dominant class distribution dominant class distribution. non-dominant class case predeﬁned constant non-zero vector label distribution dominant case dimension drawn normal distribution depending class proportion ranged values test validation sets size generated. training sizes used value total training sets. optimal regularisation parameter hybrid loss parameter selected using validation loss training set. models parameters found using lbfgs three losses training sets assessed using test set. fig. training error various number classes. hybrid loss. fisher consistency analyses behaviour loss observing entire data population. thus training data entire data testing data. consequently training error testing error. mimic seeing entire data population dominant/non-dominant class case constant vector features learn parameter vectors labels take different values proportionally follows label took sizes label assigned probability remainder given equal portion note means label sizes least always greater hybrid consistency condition always met. size training data affect training error long proportions values altered. vein proportions values test data training data thus test errors training errors structured estimation unlike general multiclass case structured estimation problems higher chance non-dominant distributions large number labels well ties ambiguity regarding labels. example text chunking changing phrase leaving rest unchanged drastically change probability predictions especially ambiguities. prevalence nondominant distributions expect models trained using hinge loss perform poorly problems relative trained hybrid losses. emphasise main motivation investigating structured prediction problems that multiclass problems tend non-dominant distributions. conll text chunking ﬁrst structured estimation experiment carried conll text chunking task data training sentences test sentences phrases respectively. task divide text syntactically correlated parts words noun phrases verb phrases sentence chunks label consists tagging sequence chunks i.e. chunking chunk common task label modelled chain-structured graphical model account dependency adjacent chunking tags given observation clearly model exponentially many possible labels suggests absence dominant class. since true underlying distribution unknown train training apply trained model testing training datasets obtain estimate conditional distributions instance. sort sentences highest lowest estimated probability true chunking label given result plotted figure observe existence many non-dominant distributions testing sentences training sentences. feature template crf++ toolkit code leon bottou stochastic gradient descent used training. training dynamic programming inference used. split data parts training testing validation regularisation parameter weight determined parameter selection using validation set. performance different training sizes took part training data learn model gathered statistics test set. accuracy precision pgms model dependency actions image. consider graph node representing action variable edge reﬂecting dependency action variables. edge constructed according annotated interaction status. interaction persons annotation edge corresponding nodes added edge cast estimation problem ﬁnding energy function observation image assign actions receive smallest energy respect feature representation combination several visual cues including multiclass action classiﬁcation scores human body poses relative position individuals exploited distinguish different actions combine visual cues similar speciﬁc represent unit vector yi-th dimension equals similarly denote another unit vector rijth dimension equals denotes relative position person compute employ simple method requires bounding boxes person value represents relative position {overlap adjacent− adjacent−right near−lef near−right ar}. used training. training dynamic programming inference used. split data parts training testing validation again determined model selection validation set. report test accuracy precision recall score table training increasing proportions training set. hybrid marginally outperforms losses measures. consider recognising human actions episodes contains persons interact other. evaluate method tvhi dataset contains short videos collected episodes includes action classes handshake high-ﬁve kiss others person labelled action others means interaction person persons image. video contains number people performing action classes. groundtruth provided dataset includes upper body bounding boxes discrete body poses action labels interaction status pair persons manually choose images dataset divide examples three sets without intersection training validating testing determined model selection validation set. note task predict interactions actions whereas task predict actions given interaction status. speciﬁcally goal solve estimation problem ﬁnding actions sub-gradient hybrid loss simply convex combination sub-gradient hinge loss gradient loss. known subgradient hinge loss computed standard inference techniques gradient loss computed standard marginal inference techniques. max-product algorithm hinge loss sum-product algorithm loss. accelerate training apply stochastic subgradient method hybrid loss. maximum number iterations min-batch size parameters learned standard max-product algorithm make prediction testing data. order evaluate recognition performance different losses show confusion matrices figure seen hybrid loss achieves best true positive rates classes action classes loss hinge loss perform best class class respectively. note losses perform much worse class rest classes. training highly biased number persons performing high-ﬁve action training much less classes. also give recognition examples shown figure ﬁrst column shows four input images containing multiple persons occlusions making recognition task difﬁcult. hinge loss performs worst persons mislabelled. loss outperforms hinge loss general persons misclassiﬁed. hybrid loss persons images perfectly classiﬁed except third image persons misclassiﬁed. conclusion discussion provided theoretical empirical motivation novel hybrid loss multiclass structured prediction problems used place common loss multiclass hinge loss. loss attempts blend strength purely discriminative approaches classiﬁcation support vector machines probabilistic approaches conditional random fields. theoretically hybrid loss enjoys better consistency guarantees hinge loss experimentally seen addition purely discriminative component improve accuracy data less prevalent. general consistency condition hold selected cross-validation. example selected small. however observe selected values experiments always close score vector contains action classiﬁcation scores obtained applying multiclass classiﬁer histograms gradients descriptor extracted bounding area person similarly represents another score vector pose classiﬁcation scores consider body pose classes {prof prof right rontal rontal right backwards}. extract features superimpose grid bounding area accumulate grid cell using orientation bins. ﬁnal descriptor concatenation subdescriptors cells. implementing kronecker product naively compute energies could memory time consuming. fortunately feature vectors highly sparse thus need multiply non-zero components feature vectors corresponding components without using kronecker product. fig. confusion matrices tvhi dataset. class best highlighted green rectangles. hybrid loss achieves best classiﬁcation accuracy three action classes i.e. other kiss. fig. visualisation action predictions using different losses. first column input images; second column hinge loss results; third column loss results; fourth column hybrid loss results. pgms superimposed images green node node indicate correct incorrect predictions respectively. note denote action classes others handshake highﬁve kiss. edges come interaction annotation dataset used model dependency action variables subjects. future work theoretically expect stronger sufﬁcient conditions possible since bounds used establish theorem tight. conjecture necessary sufﬁcient condition would include dependency number classes. also investigating connections multiclass tsybakov noise condition knowledge notion regular function class purposes consistency analysis novel. characterisations property existing parametric models would make testing regularity easier. structured prediction still between analysis practice. example structured prediction know parametric hinge loss consistent binary label cost function don’t know whether parametric hybrid loss moreover don’t theoretical results general label cost functions. better connect theory actual practice structured prediction problems plan investigate consistency general cost functions commonly used problems. acknowledgments bulk research performed nicta. nicta funded australian government represented department broadband communications digital economy australian research council centre excellence program. research partly supported australian research council discovery projects funding scheme australian research council discovery early career researcher award funding scheme supported australian centre visual technologies computer vision group university adelaide. crammer singer. learnability design output codes multiclass problems. cesa-bianchi goldman editors proc. annual conf. computational learning theory pages francisco morgan kaufmann publishers. lafferty mccallum pereira. conditional random ﬁelds probabilistic modeling proc. segmenting labeling sequence data. intl. conf. machine learning volume pages francisco morgan kaufmann. zhang jordan yeung. coherence functions multicategory margin-based classiﬁcation methods. proceedings twelfth conference artiﬁcial intelligence statistics qinfeng decra research fellow australian centre visual technologies school computer science university adelaide. received computer science australian national university completing bachelor master study computer science technology northwestern polytechnical university mark reid research fellow australian national university canberra. received machine learning university south wales completing bachelor science honours pure mathematics computer science institution. between worked research scientist various companies including canon. tiberio caetano received degree electrical engineering degree computer science universidade federal grande brazil. research part program undertaken computing science department university alberta canada. principal researcher statistical machine learning group nicta adjunct senior fellow research school computer science australian national university honorary researcher school information technologies university sydney. anton hengel prof hengel founding director australian centre visual technologies prof hengel received computer vision masters degree computer science bachelor laws bachelor mathematical science university adelaide. zhenhua wang ph.d candidate australian centre visual technologies school computer science university adelaide. supervised prof. anton hengel qinfeng anthony dick. received bachelor’s degree master’s degree northwest university.", "year": 2014}