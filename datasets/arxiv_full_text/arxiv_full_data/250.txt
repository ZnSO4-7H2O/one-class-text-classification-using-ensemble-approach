{"title": "Progressive Learning for Systematic Design of Large Neural Networks", "tag": ["cs.NE", "cs.CV", "cs.LG", "stat.ML"], "abstract": "We develop an algorithm for systematic design of a large artificial neural network using a progression property. We find that some non-linear functions, such as the rectifier linear unit and its derivatives, hold the property. The systematic design addresses the choice of network size and regularization of parameters. The number of nodes and layers in network increases in progression with the objective of consistently reducing an appropriate cost. Each layer is optimized at a time, where appropriate parameters are learned using convex optimization. Regularization parameters for convex optimization do not need a significant manual effort for tuning. We also use random instances for some weight matrices, and that helps to reduce the number of parameters we learn. The developed network is expected to show good generalization power due to appropriate regularization and use of random weights in the layers. This expectation is verified by extensive experiments for classification and regression problems, using standard databases.", "text": "saikat chatterjee∗ alireza javid∗ mostafa sadeghi† partha mitra‡ mikael skoglund∗ school electrical engineering royal institute technology stockholm sweden abstract—we develop algorithm systematic design large artiﬁcial neural network using progression property. non-linear functions rectiﬁer linear unit derivatives hold property. systematic design addresses choice network size regularization parameters. number nodes layers network increases progression objective consistently reducing appropriate cost. layer optimized time appropriate parameters learned using convex optimization. regularization parameters convex optimization need signiﬁcant manual effort tuning. also random instances weight matrices helps reduce number parameters learn. developed network expected show good generalization power appropriate regularization random weights layers. expectation veriﬁed extensive experiments classiﬁcation regression problems using standard databases. standard architecture artiﬁcial neural network comprised several layers signal transformation ﬂows input output direction. literature often known feed-forward neural network layer comprised linear transform input vector followed non-linear transform generate output vector. output vector layer used input vector next layer. linear transform represented weight matrix. non-linear transform input vector typically realized scalarwise non-linear transform known activation function. note standard architecture figure exists vast literature design functional approximation capabilities contemporary research community highly active area resurgence based deep learning structures extreme learning machines conﬂuence deep learning extreme learning recurrent neural networks residual networks etc. many aspects designing structure practical application. example many layers used deep learning structures deep neural networks often argued many layers provides varying abstraction input data helps generate informative feature vectors. many techniques parameter optimization deep neural networks particular weight matrices several heuristic design principles often well understood theory point time. deep neural networks also structured forms convolutional neural networks residual neural networks hand extreme learning machine typically uses layers wide layers many nodes. necessary learn majority weight matrices instead chosen instances random matrices. elms theoretical arguments supporting random weight matrices resulting universal approximation practice implementation simple shows good performance several applications image classiﬁcation standard databases. related methods based random weights neural networks then extension kernel methods described structural architecture particular size network matters practice researchers continue investigate many layers needed wide network achieve reasonable performance. background lack systematic design regarding choose size ann. practice choice size ad-hoc many cases choice experimentally driven high manual intervention pain-staking tuning parameters. given learning problem examples pertinent questions follows. design appropriate regularization network parameters avoid over-ﬁtting training data? means expect good generalization sense high quality test data performance? deep network show approximation error modeling appropriate target function decreases addition layer? deep network comprised many layers layer ﬁnite number nodes using cross validation. engineer non-linear activation functions hold progression property deﬁne input vector passes nonlinear transformation output vector exactly equal input vector. property formally stated later. show rectiﬁer linear unit function derivatives hold using design progression nodes layers added sequentially regularization. leads systematic design large neural network. start single-layer network build multi-layer network. output optimal linear system partial input network beginning. layer corresponding weight matrix comprised parts part optimized part random matrix instance. relevant optimization problem layer convex. convexity starting single-layer network show addition layer lead reduced cost certain technical conditions. means two-layer network better single-layer network multiple layers. training phase saturation trend cost reduction helps choose number nodes layer number layers network. non-linear function takes scalar argument provides scalar output. neural network literature function often called activation function. input vector non-linear function stack functions scalar component input vector treated independently scalarwise function. commonly used non-linear functions step function sigmoid logistic regression tanhyperbolic rectiﬁer linear unit function etc. non-linear function follow expect article design training algorithm systematic manner address questions. structure decided systematic training progressively adding nodes layers appropriate regularization constraints. name progressive learning network well known joint optimization parameters non-convex problem. progressive learning approach layer-wise design principle. layer added existing optimized network layer learned optimized time appropriate norm-based regularization. learning layer convex optimization problem solve. training whole greedy nature general sub-optimal. examples existing greedy and/or layer-wise learning approaches found then examples norm-based regularization approaches softweights dropout found supervised learning problem pair-wise form data vector observe target vector wish infer. function acts inference function parameters function. training phase typically learn optimal parameters minimizing cost example using k.kp denotes p’th norm denotes expectation operator. then testing phase optimal parameters learned training phase de-facto. expectation operation cost function realized practice sample average using training data. denoting j’th pair-wise data-and-target considering total training instances cost learning constraint kθkq acts regularization avoid over-ﬁtting training data. usually chosen alternative choice and/or enforces sparsity. often choice experimentally driven example s.t. abbreviation ‘such that’. also k.kq norm matrix argument means ℓq-norm vectorized form matrix. denote output optimal linear system lsx. regularization constraint kwlskq rewrite optimization problem regularized leastsquares problem unconstrained form follows chosen constant kuqkq notation denote corresponding input optimization problem convex. denote output optimized single-layer architecture single-layer shown figure layer nodes random nodes. term ‘random nodes’ means input nodes generated linear transform linear transform random matrix instance. note optimize jointly hence overall single-layer sub-optimal. remark assume ﬁnite norm. matrix ﬁnite norm construction inherently regularized. matrix also regularized constraint kokq relation optimal linear system single layer note equality condition invoking denotes zero matrix size otherwise inequality relation follow. next focus test data performance. parameters regularized expect single layer provide better test data performance vis-a-vis optimal linear system. remark suppose single-layer plns. ﬁrst nodes. second nodes weight matrix created taking weight matrix ﬁrst then concatenating -dimensional random matrix instance matrix bottom. lrelu holds show existing relu lrelu functions hold possible invent functions hold example propose generalized relu function following deﬁnition. following standard architecture shown figure note layer comprised linear transform input vector non-linear transform. linear transform represented weight matrix non-linear transform function. i’th layer denote input vector output vector denotes corresponding weight matrix. pursuit answering questions raised section design grows size appropriate constraints. inclusion node layer inclusion layer always results non-increasing cost training data. provide appropriate regularization parameters good generalization quality. weight parameters large network engineered random weights deterministic weights. clarity develop describe single-layer ﬁrst two-layer ﬁnally multi-layer pln. assume single layer nodes layer non-linear transformation takes place. single layer comprises weight matrix rn×p holding non-linear transformation output matrix rq×n signal transformation relations parameters single layer need learn construct matrix combination deterministic random matrices follows rq×p deterministic matrix vqw⋆ rq×p optimal linear transform matrix asw⋆ sociated corresponding linear system. matrix instance random matrix. optimal linear system linear transform inference wlsx. using training data optimal linear transform optimal cost follows remark note vqo⋆ ﬁnite norm choose ﬁnite norm matrix inherently regularized. matrix regularized constraint kokq second layer two-layer built corresponding single layer optimal cost equality condition solution invoking denotes zero matrix size otherwise inequality relation follow. parameters regularized expect two-layer provide better test data performance single-layer pln. denote optimized cost nodes then construction optimization problem inequality relation helps choose number nodes single-layer pln. nodes time step-wise fashion optimized cost shows saturation trend stop adding node tangible decrease cost. two-layer built optimized single-layer adding layer progression. single-layer access signals assume second layer two-layer nodes. second layer comprises weight matrix rn×n holding function output matrix rq×n second layer signal transformation relations parameters learn combination deterministic random matrices remark suppose two-layer plns built single-layer pln. second layer ﬁrst nodes. second nodes weight matrix created taking weight matrix second layer ﬁrst then concatenating n-dimensional random matrix instance matrix bottom. denote optimized cost nodes second layer. then construction optimization problem property helps choose number nodes second layer. described two-layer built singlelayer adding layer. build multi-layer layers. start optimized layer then l’th layer. architecture layer shown figure assume notation clear context. l’th layer optimal output matrix optimal cost follows rq×n deterministic matrix vqo⋆ optimal output matrix corresponding singlelayer pln. matrix instance random matrix nodes second layer random nodes. two-layer relation denotes optimized cost layers. note denote optimized cost layer nodes l’th layer. then construction optimization problems increase nodes l’th layer number parameters increases approximately linear scale addition layer. hand typical weight matrix l’th layer dimension nl−. hence total number parameters learn approximately design using training data start singlelayer important building block regularized least-squares. single layer increase number nodes layer. number nodes step-wise manner cost improvement shows saturation trend. then design two-layer adding layer second layer increase nodes number long tangible cost improvement. next third layer design three-layer continue layers. layers added optimized time. design deep network. access validation data test cost improvement validation data instead training data. choose performance cost normalized-mean-error deﬁned follows subsection discuss approximation error deep many layers assume layer ﬁnite number nodes. using note optimized cost monotonically non-increasing increase number layers denote output optimized l-layer proposition using technical condition denotes zero matrix size optimized cost monotonically decreasing increase number layers large number layers means arbitrarily small non-negative real scalar. proposition shows possible achieve small approximation error deep layer ﬁnite number nodes. next show limitation multi-layer following corollary. corollary assume layers nodes vqo⋆ random matrix instances. strategy progressively adding layers l’th layer optimized output matrix then addition layer help reduce cost. locally optimum point denotes predicted output. denote current nmec addition node/layer network denoted nmeo. training data satisﬁes nmec nmeo. decide progressive increase nodes layer number layers increase depth network long change certain threshold nmeo−nmec threshold. choose separate thresholds adding nodes adding layers. threshold corresponding addition nodes denoted threshold corresponding addition layers denoted upper limit number nodes layer denoted nmax avoid much wide layers. also upper limit number layers network denoted lmax. need manually following parameters regularized least-squares optimization threshold parameters maximum allowable number nodes nmax layer maximum allowable number layers lmax. among parameters tune carefully achieve good regularized least-squares. recall uses regularized least-squares ﬁrst layer. much effort tune parameters nmax lmax. section report experimental discuss relations single-layer standard learning methods regularized least-squares regularized elm. discussed section iii-a single-layer provides output regularized least-squares optimal output matrix denotes zero matrix size hand output matrix comprised zero matrix regularized part bottom zero matrix size output single-layer equivalent regularized elm. overall single-layer interpreted linear combination regularized least-squares regularized elm. nontrivial extend interpretation multi-layer pln. l’th layer need solve optimization problem optimization problem convex practical problem computational complexity large amount data. therefore computationally simple convex optimization method called alternating-direction-method-ofmultipliers admm iterative algorithm familiar distributed convex optimization problems deﬁne matrices remark matrix inversion independent iterations such precomputed save computations. furthermore tall matrix invoke woodbury matrix identity take inverse instead denotes iteration index controls convergence rate admm stands lagrange multiplier matrix. noting subproblems closed-form solutions ﬁnal algorithm written regression. experiments performed using laptop following speciﬁcations intel-i .ghz windows matlab compare vis-a-vis regularized least-squares regularized elm. recall uses regularized least-squares ﬁrst layer discussed section iii-a. therefore carefully implement regularized least-squares choose regularization parameter ﬁnding optimal linear matrix multi-fold cross-validation. good performance regularized least-squares also ensure good performance based regularized least-squares. single-layer regularization performed carefully optimizing output matrix. regularization parameter lagrangian form output matrix optimization problem denoted λelm. also experimentally tuned number nodes achieving good performance. number nodes denoted nelm. relu activation function. perform experiments using holding non-linear functions. mention elements random matrix instances drawn uniform distribution range further used practical trick output corresponding random nodes layer scaled unit norm. ensure transformed signal grow arbitrarily large ﬂows layers. conduct experiments using databases mentioned table classiﬁcation regression tasks respectively. databases extensively used relevant signal processing machine learning applications classiﬁcation tasks databases mentioned table ‘vowel’ database vowel recognition task databases image classiﬁcation give examples caltech database contains images distinct object classes background class. number images class vary objects class considerable variations shape. caltech database achieving higher classiﬁcation accuracy known challenging. table mention instances training data instances testing data dimension input data vector number classes database. q-dimensional target vector classiﬁcation task discrete variable indexed representation -outof-q-classes. target variable instance scalar component scalar components table informs databases regression tasks shows instances training data instances testing data dimension input data vector target dimension several databases tables table clear demarcation training data testing data. cases randomly partition total dataset parts repeat experiments reduce effect randomness. denoted term ‘random partition’ tables. ‘yes’ random partition; ‘no’ databases already clear partition. point discuss source randomness experiments. mentioned type randomness partition total database training testing datasets another type randomness arises random weight matrices architecture. considering types randomness reduce effects repeat experiments several times report average performance results standard deviations. classiﬁcation performance results shown table show training testing training time testing accuracy important performance measure testing accuracy. seen provides better and/or competitive performance several databases vis-a-vis regularized least-squares regularized elm. caltech database provides signiﬁcant performance improvement compared elm. next show corresponding parameters associated algorithms table parameters regularized least-squares regularized carefully chosen using extensive cross validation manual tuning. recall regularized least-squares used building block pln. except part parameters associated regularized least-squares modest effort choosing parameters pln. fact deliberately chose parameters databases show careful tuning parameters necessary. helps reduce manual effort. next show regression performance results table note provides better and/or competitive performance. table displays parameters associated algorithms shows indeed require high manual effort tuning. argued wish avoid effort natural question happens manually tune parameters pln. possible improve performance? investigate show performance tuned parameters table classiﬁcation tasks randomly picked datasets. exercise extensive tuning perform limited tuning. tuned parameters among parameters. tuning performed using intuition driven optimization gathered experience previous experiments. experimental results shown table conﬁrm performance improvement indeed possible parameter tuning. finally explore following issue architecture build buildup affect performance? architecture builds according strategy section iii-d. i’th layer nodes includes random nodes. total number random nodes comprised layers show behavior buildup instance ‘letter’ database time training. behavior shown figure instance eight layers. number random nodes layer shown ﬁrst sub-ﬁgure. interesting observe proposed learning algorithm helps build self organizing architecture layers nodes fig. behavior buildup instance ‘letter’ database time training. instance comprised layers. number random nodes across layers. shows layers nodes layers less nodes. normalized-mean-error versus number random nodes. classiﬁcation accuracy versus number random nodes. note stair-case type behavior sudden change occurs layer added. layers less nodes. natural questions include reason behind architecture? layer could nodes layer less nodes? questions non-trivial answer. sub-ﬁgures note stair-case type behavior sudden change occurs layer added. addition layer brings relatively high performance improvement sudden jump. performance shows saturation trend number nodes layer increases number layers increases. training strategy ensures non-increasing training dataset interesting testing dataset also shows similar trend. further classiﬁcation accuracy training testing datasets show consistent improvement size grows. natural question sudden change performance improvement occurs addition layer. point time mathematically justiﬁed argument question non-trivial. qualitative argument would addition layer brings richer feature representation previous layer. further observe training dataset performance test dataset performance increases network grows size. ﬁxed amount training dataset size observation consistent knowledge network generalization power diminishes number parameters grows increase network size. conclude large multi-layer designed systematically number layers number nodes layer learned training data. learning process require high manual intervention. appropriate activation functions regularization convex optimization random weights allows learned network show promising generalization properties. progressive strategy growing network size addition layer typically brings performance improvement sudden jump. attributed common belief nonlinear transformations provide information rich feature vectors. development investigate scope using back propagation based learning approach training design optimized network investigated future. yoshua bengio aaron courville pascal vincent representation learning review perspectives ieee transactions pattern analysis machine intelligence vol. chris hettinger tanner christensen ehlert jeffrey humpherys tyler jarvis sean wade forward thinking building training neural networks layer time arxiv preprint https//arxiv.org/abs/.. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov dropout simple prevent neural networks overﬁtting journal machine learning research vol. boyd parikh peleato eckstein distributed optimization statistical learning alternating direction method multipliers foundations trends machine learning vol. learning border active learning imbalanced data classiﬁcation proceedings sixteenth conference conference information knowledge management york cikm acm. jiang davis label consistent k-svd learning discriminative dictionary recognition ieee transactions pattern analysis machine intelligence vol. bolei zhou agata lapedriza jianxiong xiao antonio torralba aude oliva learning deep features scene recognition using places database advances neural information processing systems ghahramani welling cortes lawrence weinberger eds. curran associates inc. aharon elad bruckstein k-svd algorithm designing overcomplete dictionaries sparse representation ieee transactions signal processing vol. nov. robert schapire brief introduction boosting proceedings international joint conference artiﬁcial intelligence volume francisco ijcai’ morgan kaufmann publishers inc. guang-bin huang hongming zhou xiaojian ding zhang extreme learning machine regression multiclass classiﬁcation ieee transactions systems cybernetics part vol. tomas mikolov martin karaﬁt lukas burget cernock sanjeev khudanpur recurrent neural network based language model zhang deep residual learning image recognition ieee conference computer vision pattern recognition june alex krizhevsky ilya sutskever geoffrey hinton imagenet classiﬁcation deep convolutional neural networks advances neural information processing systems pereira burges bottou weinberger eds. curran associates inc. w.f. schmidt m.a. kraaijveld r.p.w. duin feed forward neural networks random weights proc. iapr conf vol. pattern recognition methodology systems yoh-han gwang-hoon park dejan sobajic learning generalization characteristics random vector functional-link neurocomputing vol. backpropagation part jianjun zhenghua zhou zhao guoqiang chen leukocyte image segmentation using feed forward neural networks random weights international conference natural computation rahimi benjamin recht weighted sums random kitchen sinks replacing minimization randomization learning advances neural information processing systems koller schuurmans bengio bottou eds. curran associates inc. ellis morgan size matters empirical study neural network training large vocabulary continuous speech recognition ieee international conference acoustics speech signal processing. proceedings. icassp vol. vol.. yoshua bengio pascal lamblin popovici hugo larochelle greedy layer-wise training deep networks advances neural information processing systems vol.", "year": 2017}