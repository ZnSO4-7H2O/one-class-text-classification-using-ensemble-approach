{"title": "Advancing Bayesian Optimization: The Mixed-Global-Local (MGL) Kernel and  Length-Scale Cool Down", "tag": ["cs.LG", "cs.AI", "stat.ML", "68T99, 78M50, 68T05"], "abstract": "Bayesian Optimization (BO) has become a core method for solving expensive black-box optimization problems. While much research focussed on the choice of the acquisition function, we focus on online length-scale adaption and the choice of kernel function. Instead of choosing hyperparameters in view of maximum likelihood on past data, we propose to use the acquisition function to decide on hyperparameter adaptation more robustly and in view of the future optimization progress. Further, we propose a particular kernel function that includes non-stationarity and local anisotropy and thereby implicitly integrates the efficiency of local convex optimization with global Bayesian optimization. Comparisons to state-of-the art BO methods underline the efficiency of these mechanisms on global optimization benchmarks.", "text": "bayesian optimization become core method solving expensive black-box optimization problems. much research focussed choice acquisition function focus online length-scale adaption choice kernel function. instead choosing hyperparameters view maximum likelihood past data propose acquisition function decide hyperparameter adaptation robustly view future optimization progress. further propose particular kernel function includes non-stationarity local anisotropy thereby implicitly integrates eﬃciency local convex optimization global bayesian optimization. comparisons state-of-the methods underline eﬃciency mechanisms global optimization benchmarks. bayesian optimzation became almost ubiquitous tool general black-box optimization high function evaluation cost. idea make gathered data computing bayesian posterior objective function. algorithm principle characterized choices prior objective function? given posterior decision theoretic criterion so-called acquisition function choose next query point? previous research extensively focussed second question. instance recent work goes beyond expected improvement proposing entropy optimum location inﬁnite metric based criterion acquisition function. paper rather focus ﬁrst question choice model prior objective function. clearly purely bayesian stance prior must given subject discussion. however number reasons reconsider this choice hyperparameters large number methods notably exception hern´andez-lobato rely online point estimate hyperparameters. many convergence proofs fact rely apriori chosen hyperprior several experimental results reported hyperpriors carefully chosen hand optimized samples objective function apriori given practise choosing hyperprior online so-far seen data) prone local optima lead signiﬁcant ineﬃciency w.r.t. optimization process. instance fig. maximum likelihood estimate length-scale unuseful view objective function. taken together believe online selection hyperparameters online learning remains challenge. paper take stance chooses point estimate hyperprior online maximum likelihood seen data appropriate model selection criterion. instead choose hyperprior accelerate optimization process. propose cool length-scale parameter based hyperprior selection w.r.t. acquisition function hyperprior promises acquisition next query point. combine lower-bound heuristic robustness. heteroscedastic various local optima diﬀerent non-isotropic conditioning hessian local optimum. preliminary experiments heteroscedastic non-isotropic models reported paper propose novel type kernel function following mind. classical model-based optimization convex black-box functions extremely eﬃcient know function convex. therefore purpose optimization presume objective function local convex polynomial regions regions objective function convex reasonably approximated nd-order polynomial within regions quasi-newton type methods converge eﬃciently. like would case eﬀect propose mixed-global-local kernel expresses prior assumption local convex polynomial regions well automatically implying local search strategy analogous local model-based optimization. eﬀectively choice kernel integrates eﬃciency local model-based optimization within bayesian optimization framework. polynomial regions implies local quadratic interpolation optimization steps analogous classical model-based optimization combined global bayesian optimization. paper structured follows giving essential background explain length-scale cool scheme isotropic kernels. next introduce kernel make adaption scheme ﬁrst step. show signiﬁcant performance improvements compared classical ’optimal’ hyperparameters. illustration mis-leading online lengthfigure scale selection kernel based maxlikelihood poor prediction minimum using kernel gp-mean gp-variance true underlaying function misleading estimates hyperparameters randomized initial observations function contains high frequency part directly jump high frequency modelling small number samples. table overview common algorithms properties performance guarantees online model adaption case model adaption explicitly treated performance analysis. hyperparameters model assumption builds basis many algorithms. general prototype algorithm given alg. represents algorithm speciﬁc acquisition function. experiments well known theoretical extensively studied expected improvement acquisition function deﬁned classical online model adaption maximum likelihood loo-cv compared bachoc loo-cv turns robust model misspeciﬁcation whereas maximum likelihood gains better results long model chosen ’well’. jones already discusses problem highly misleading initial objective function samples. forrester jones idea earlier mentioned jones studied hyperparameter adaption combined calculation step. authors show better performance certain cases resulting sub-optimization quite tedious described quttineh holmstr¨om another approach improving length-scale hyperparameter adaption presented wang limit local exploration setting upper bound length-scale. bound propose independent dimensionality experimentally chosen hand applications. papers address problem modiﬁcations basic approach adjust hyperparameters maximize data likelihood. contrast adjust hyperparameters based aquisition function aiming optimization performance rather data likelihood. mohammadi introduce idea local length-scale adaption based maximizing acquisition function value eﬃcient nevertheless endorse underlying idea since related motivation. model side several ideas yield non-isotropic models building ensemble local isotropic kernels e.g. based trees however introduce speciﬁc kernel rather concept combining kernels gaussian processes taylored improving also concepts regarding locally deﬁned kernels e.g. krause guestrin idea martinez-cantin somehow closely related ours local global kernel function great approach believe. parametrize location local kernel well respective parameters. consequently large number hyperparameters makes model selection diﬃcult. constrast work able gain comparable better performance well-known benchmarks. time overcome problem many hyperparameters separated eﬃcient algorithm determining location local minimum regions. furthermore non-isotropic kernel better ﬁtting local minimum regions. last aspect want mention also incorporates local minimum region criterion encapsulated acquisition function itself interesting conceptually connected ideas. compare performance along another state-of-the-art algorithm advances combination classical experimental section. length-scale used previous iteration. approach want decide whether reuse length-scale decrease speciﬁc smaller length-scale iteration experiments choose hard lower bound present next section. neglecting bound want decide whether half length-scale. optimal aquisition value using length-scale typical situations expect because reduced length-scale leads larger posterior variance typically leads larger aquisition values i.e. chances progress optimization process. turn argument around substantially larger choosing smaller length-scale yield substantially chances progress optimization process. case smaller length-scale higher risk overﬁtting decide stick length-scale ln−. remark straight-forward transfer existing convergence guarantees bayesian optimization given bull hold also case cool constant absolute lower bound increase robustness length-scale adaptation especially early phase optimization propose length-scale lower bound explicitly takes search space dimensionality account. lower bound assume ¯yn} chosen minimal correlation point data highest. based diﬀerent number samples ¯xn| calculate corresponding minimum length-scale satisfy minimum required correlation since likely samples acquired alg. violate best case assumption calculated length-scale serves lower bound. formally consider kernel heteroscedastic sense quadratic kernels convex neighborhood implies fully diﬀerent variances global stationaryisotropic kernel around neighborhoods. strict seperation corresponding regions posterior calculation decoupled. result following idea order transfer bound higher dimensions max. distance samples implies minimal empty space. demand higher dimensions ratio volume whole search space minimal empty space remains same. thereby seek equal coverage every sample case following idea transform minimum distance samples satisfy def. ball shaped local minimum region. line calculate potential local minimum region. line quadratic form using samples inside region. lines used selection criterion local regions. besides trivial criterions line local convergence criteria. line force close points local region contribute local model turned improve overall performance. line removes regions overlap better regions order disjoint convex neighborhoods. original source code used generate results found supplementary material published. tests choose following conﬁgurations alg. mgl-kernel take kernel estimated observation variance constant mean prior maximum likelihood scaled observation variance factor consistency quadratic part local region detected. computing alg. line ﬁrst solved minimization using kernel compared results minimization problems using kernel local minimum remark following often refer optimal choice hyperparameters. mean random samples respective objective function taken. data exhaustive loo-cv used select length-scale maxlikelihood select prior variance meanprior illustrate cool within bayesian optimization consider ideal setting objective sampled stationary kernel. fig. display regret using optimal parameters online exhaustive loo-cv length-scale parameter estimated data using maximum likelihood cool method. true objective indeed sample stationary isotropic online loo-cv optimal hyperparameters work well cool less variance performance. belﬁgexamplesminkernel immediate regret optimizing sample function zero mean unit process noise kernel function shown online loo-cv lengthscale adaption optimal length-scale. sec. details. plot counter example objective function ordinary model adaption zero mean unit observation variance kernel function local quadratic regions inserted. large length-scale local optima. regret curves fig. compare three methods described above. both optimal online loo-cv methods stationary hyperparameters heteroscedastic objective function. lead rather poor optimization behavior. cool behaviors clearly superior. treats hyperparameters bayesian acquisition function) inﬁnite metric optimization classical optimal hyperparameters using alpharatio model adaption mgl-kernel fig. displays another objective function corresponds kernel assumption. shown fig. using kernel together cool able gain performance improvement compared plain cool even optimal hyperparameters. mgl-kernel outperforms signiﬁcantly case quadratic quadratic like rosenbrock objective. also branin-hoo hartmann exponential method signiﬁcantly outperforms existing state-of-the-art bayesian optimization methods. case hartmann turns work better. nevertheless want emphasize outstanding improvement compared plain optimal hyperparameters every test case. core contributions length-scale cool based acquisition function kernel function concern model selection. higherlevel perspective proposed context bayesian optimization select models diﬀerently standard machine learning instead selecting hyperparameters based maximum likelihood previous data judge implications choice hyperparameter future data e.g. acquisition function. instead choosing standard uninformed squared exponential kernel want choose kernel function indirectly expresses prior modelbased optimization eﬃcient local convex regions global length-scale characterizes local regions hidden. found novel concept length-scale adpation outperforms leave-one-out cross validation even a-posteriori optimal hyperparameters robust setting similar performance nominal case. further combining length-scale cool novel kernel function expected improvement shows benchmark problems better performance assumptions model hyperparameters made beforehand. furthermore limited enables community combine length-scale adaption model acquisition functions potentially lead overall performance improvement regarding bayesian optimization believe. ziyu wang frank hutter masrour zoghi david matheson nando feitas. bayesian optimization billion dimensions random embeddings. journal artiﬁcial intelligence research", "year": 2016}