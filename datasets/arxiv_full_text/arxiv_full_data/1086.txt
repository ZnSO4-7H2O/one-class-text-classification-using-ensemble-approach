{"title": "Gated Orthogonal Recurrent Units: On Learning to Forget", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We present a novel recurrent neural network (RNN) based model that combines the remembering ability of unitary RNNs with the ability of gated RNNs to effectively forget redundant/irrelevant information in its memory. We achieve this by extending unitary RNNs with a gating mechanism. Our model is able to outperform LSTMs, GRUs and Unitary RNNs on several long-term dependency benchmark tasks. We empirically both show the orthogonal/unitary RNNs lack the ability to forget and also the ability of GORU to simultaneously remember long term dependencies while forgetting irrelevant information. This plays an important role in recurrent neural networks. We provide competitive results along with an analysis of our model on many natural sequential tasks including the bAbI Question Answering, TIMIT speech spectrum prediction, Penn TreeBank, and synthetic tasks that involve long-term dependencies such as algorithmic, parenthesis, denoising and copying tasks.", "text": "jing henaff szlam lecun attracted increasing amount attention machine learning community. trend following demonstration matrices effective solving tasks involving long-term dependencies gradients vanishing/exploding problem. thus unitary/orthogonal capture long term dependencies effectively sequential data conventional lstm. result type model shown perform well tasks would require rote memorization simple reasoning copy task sequential mnist. models viewed extension vanilla rnns replaces transition matrices either unitary orthogonal matrices. paper refer ability model omit parts input sequence contain redundant information ﬁlter noise input general means forgetting mechanism. previously shown importance forgetting mechanism lstm networks similar motivations discuss utilization forgetting mechanism rnns orthogonal transitions. importance forgetting networks mainly unitary/orthogonal rnns backpropagate gradients without vanishing time easy output depends equal amounts elements whole input sequence. perspective learning forget difﬁcult unitary/orthogonal rnns clog memory useless information. however real-world applications natural tasks require model ﬁlter irrelevant redundant information input sequence. argue difﬁculties forgetting cause unitary orthogonal rnns perform badly many realistic tasks demonstrate empirically task. propose architecture gated orthogonal recurrent unit combines advantages frameworks namely ability capture long term dependencies using orthogonal matrices ability forget using structure. demonstrate goru able learn long term dependencies present novel recurrent neural network based model combines remembering ability unitary rnns ability gated rnns effectively forget redundant/irrelevant information memory. achieve extending unitary rnns gating mechanism. model able outperform lstms grus unitary rnns several long-term dependency benchmark tasks. empirically show orthogonal/unitary rnns lack ability forget also ability goru simultaneously remember long term dependencies forgetting irrelevant information. plays important role recurrent neural networks. provide competitive results along analysis model many natural sequential tasks including babi question answering timit speech spectrum prediction penn treebank synthetic tasks involve long-term dependencies algorithmic parenthesis denoising copying tasks. recurrent neural networks gating units long short term memory gated recurrent units rapid progress different areas machine learning language modeling neural machine translation speech recognition works proven importance gating units recurrent neural networks. main advantage using gated units rnns primarily ease optimization models using reduce learning degeneracies vanishing gradients cripple conventional rnns importantly designing special gates easier impose particular behavior model creating shortcut connections time using input forget gates lstms resetting memory reset gate gru. feature also brings modularity neural network design seems make training models easier. gated rnns also empirically shown achieve better results wide variety real-world tasks. effectively even complicated datasets require forgetting ability. work focus implementation orthogonal transition matrices subset unitary matrices. goru outperforms recent variation unitary called eurnn language modeling denoising parenthesis question answering tasks. show unitary fails catastrophically denoising task requires model forget. question answering speech spectrum prediction algorithmic parenthesis denoising tasks goru achieves better accuracy test models compare against. attempted gates unitary matrices complex numbers encountered training challenges training gating mechanisms thus decided focus orthogonal matrices paper. rdh×dh rdx×dh model parameters nonlinear activation function. rnns proven effective solving sequential tasks ﬂexibility however well-known problem called gradient vanishing gradient explosion prevented rnns efﬁciently learning long-term dependencies several approaches developed solve problem lstms grus successful widely used. gated recurrent unit step forward lstm gated recurrent unit proposed removed extra memory state lstm. speciﬁcally hidden state updated follows although lstms grus proposed solve exploding vanishing gradient problem practice still suffer issue long-term tasks. result gradient clipping usually required training process although addresses gradient explosion. thanks property unitary/orthogonal matrix able preserve norm vector ﬂows thus allow gradient propagate longer time steps. recent papers arjovsky pointed unitary/orthogonal matrices effectively prevent gradient vanishing/explosion problem conventional rnns. work several unitary/orthogonal models proposed showing promising abilities capturing long term dependencies data. unitary matrices nonlinear activation function needs handle complex-valued inputs. paper generalizations popular real-valued activation function relu known bias vector. variant found perform effectively suite benchmarks even though modrelu developed complex value models turns activation function unitary/orthogonal matrices best. gated orthogonal recurrent unit problem forgetting orthogonal rnns first argue advantage forget past inputs. desirable seek state representation capture important elements past sequence throw away irrelevant details noise. ability becomes particularly critical dimensionality state smaller product sequence length input dimension; i.e. form compression necessary. compression useful processing consider whose state obtained sequence orthogonal transformations transformation function input given time step. focus class orthogonal transformations basically rotation simplicity analysis analogous addition space angles. compose several orthogonal operators angles together. forget mild sense state combination several rotations lose track exactly individual rotations applied. advantage that space angles derivative ﬁnal angle individually added angle vanishing gradient. however cannot complete forgetting e.g. making state independent past inputs rotation cancel rotation would need rotation know rotation cancel i.e. would need function rotation. happens rotation chosen looking current state. instead regular state update depends non-linear past state example particular value state reached reset would possible composition orthogonal transformations. considerations motivate architecture combine orthogonal unitary transformations non-linearities trained forget appropriate. goru architecture section introduces gated orthogonal recurrent unit architecture change hidden state loop matrix orthogonal matrix change respective activation function modrelu activation function rdh×dh rdx×dh. reset update gates respectively. rdh×dh kept orthogonal. fact modiﬁed main loop absorbs information orthogonal leaving gates unchanged compared gru. figure demonstrated architecture goru model. enforce matrix orthogonal using parametrization method purposed decomposed sequence -by- rotation matrices shown figure -by- rotation contains trainable rotation parameter. figure orthogonal matrix parametrization -by- rotation matrices. represents neuron hidden state. junction represents -by- rotation matrix corresponding neurons. thought acting like low-pass ﬁlter. orthogonal transition matrices help model prevent gradients vanish time. however ways orthogonal transformation interact hidden state limited reﬂections rotations. reset gate enables model rescale magnitude hidden state activations compare goru unitary rnns well-known gatedrnns previous research unitary rnns mainly focused memorization tasks; contrast focus realistic noisy tasks require model discard parts input sequence able capacity efﬁciently. copying memory task ﬁrst task consider well known copying memory task. copying task synthetic task commonly used test network’s ability remember information seen time steps earlier. speciﬁcally task deﬁned follows. alphabet consists symbols {ai} ﬁrst represent data remaining representing blank marker respectively. choose input sequence contains data steps followed blank. model supposed output blank experiment rmsprop optimization learning rate decay rate models. batch size models roughly number hidden hidden parameters despite similar neuron layer sizes. figure copying memory task delay time goru lstm eurnn. hidden state sizes respectively match total number hidden hidden parameters. goru gatedsystem successfully solve task lstm stuck baseline. eurnn seen converges within hundreds iterations. task requires model efﬁciently overcome gradient vanishing/explosion problem require forgetting ability. eurnn performs perfectly goes baseline time previously seen. goru gated-system successfully solve task lstm stuck baseline shown figure denoise task evaluate forgetting ability architecture synthetic denoise task. list data points located randomly long noisy sequence. model supposed ﬁlter useless part output remaining sequential labels. similarly labels copying memory task above alphabet consists symbols {ai} ﬁrst represent data remaining represent noise marker respectively. input sequence contains randomly located data steps rest ﬁlled noise. model supposed output data sequence sees marker. previous experiment rmsprop optimization algorithm learning rate decay rate models. batch size figure denoise task sequence length goru lstm eurnn. hidden state sizes respectively match total number hidden hidden parameters. eurnn stuck baseline lacking forgetting mechanism goru successfully solve task. task requires ability learning long dependencies also ability forget noisy input. goru able successfully outperform lstm terms learning speed ﬁnal performances shown figure eurnn however gets stuck baseline intuitively expected. parenthesis task parenthesis task requires model count number type unmatched parentheses time step given types parentheses. input data contains pairs different parenthesis types e.g. ...’ mixed random noise/characters them. neural network outputs many unmatched parentheses are. instance given examines rnn’s ability understand language perform basic logical reasoning. training example statements logically related fashion. instance training example consists three statements question mary went bathroom. john moved hallway. mary traveled ofﬁce. mary? answer ofﬁce. twenty different types questions asked requiring deduction lines requiring association. babi dataset useful contains small sized vocabulary short sentences requires word answers story. thus good benchmarking test word mapping layers dominant sources parameters. test task uni-directional without attention mechanism. detail word-embed feed sequence statements. another word-embedded question. then concatenate outputs rnn’s single input third outputs correct word. language modeling character-level prediction test character-level language modeling. character step real context supposed output prediction next character. used penn treebank corpus task single supporting fact supporting facts three supporting facts arg. relations three arg. relations yes/no questions counting lists/sets simple negation indeﬁnite knowledge basic coreference conjunction compound coref. time reasoning basic deduction basic induction positional reasoning size reasoning path finding agent’s motivations mean performance table question answering task babi dataset. test accuracy goru lstm eurnn. models performed one-way without extra memory attention mechanism. goru achieves highest average accuracy. show ﬁnal test performance table comparing performance terms bits-per-character. goru performing comparable lstm experiments performs signiﬁcantly better eurnn. also done ablation study disabling reset update gates. since relevant information character-level prediction obtained using recent rather distant past core character-prediction challenge involve main strength eurnn. table penn treebank character-level modeling test goru lstm eurnn. single layer models. choose size models match number parameters. goru able outperform eurnn. also tested performance restricted goru shows necessity reset update gates. tested ability models real-world speech spectrum prediction task short-time fourier transform used timit dataset sampled khz. audio .wav initially divided different time frames fourier transformed frequency domain ﬁnally normalized training/testing. stft operation uses hann analysis window samples window samples task rnns required predict log-magnitude stft frame time given log-magnitudes stft frames time built novel brings beneﬁts orthogonal matrices gated architectures gated orthogonal recurrent units replacing hidden hidden matrix reseting path orthogonal matrix replacing non-linear activation modrelu goru gains advantage unitary/orthogonal rnns since gradient pass long time steps without exploding. empirical results showed goru model found could solve synthetic copying task denoise task. moreover goru able outperform lstm several benchmark tasks. results suggest goru ﬁrst step bringing explicit forgetting mechanism class unitary/orthogonal rnns. method demonstrates incorporate orthogonal matrices variety neural network architectures excited open gate next level neural networks. work partially supported army research ofﬁce institute soldier nanotechnologies contract wnf--d national science foundation grant ccf- semiconductor research corporation grant -ep--b.", "year": 2017}