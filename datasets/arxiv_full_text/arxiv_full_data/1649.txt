{"title": "Learned in Translation: Contextualized Word Vectors", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.", "text": "computer vision beneﬁted initializing multiple deep layers weights pretrained large supervised training sets like imagenet. natural language processing typically sees initialization lowest layer deep models pretrained word vectors. paper deep lstm encoder attentional sequence-to-sequence model trained machine translation contextualize word vectors. show adding context vectors improves performance using unsupervised word character vectors wide variety common tasks sentiment analysis question classiﬁcation entailment question answering ﬁne-grained sentiment analysis entailment cove improves performance baseline models state art. signiﬁcant gains made transfer multi-task learning synergistic tasks. many cases synergies exploited architectures rely similar components. computer vision convolutional neural networks pretrained imagenet become facto initialization complex deeper models. initialization improves accuracy related tasks visual question answering image captioning distributed representations pretrained models like wordvec glove become common initializations word vectors deep learning models. transferring information large amounts unlabeled training data form word vectors shown improve performance random word vector initialization variety downstream tasks e.g. part-of-speech tagging named entity recognition question answering however words rarely appear isolation. ability share common representation words context sentences include could improve transfer learning nlp. inspired successful transfer cnns trained imagenet tasks computer vision focus training encoder large task transferring encoder tasks nlp. machine translation requires model encode words context decode another language attentional sequence-to-sequence models often contain lstm-based encoder common component models. hypothesize data general holds potential comparable imagenet cornerstone reusable models. makes mt-lstm pairing natural candidate mirroring imagenet-cnn pairing computer vision. depicted figure begin training lstm encoders several machine translation datasets show encoders used improve performance models trained tasks nlp. order test transferability encoders develop common architecture variety classiﬁcation tasks modify dynamic coattention network question answering append outputs mt-lstms call context vectors word vectors typically used inputs models. approach improved performance models downstream tasks baseline models using pretrained word vectors alone. stanford sentiment treebank stanford natural language inference corpus cove pushes performance baseline model state art. experiments reveal quantity training data used train mt-lstm positively correlated performance downstream tasks. another advantage relying data abundant supervised tasks suggests higher quality mt-lstms carry useful information. even though machine translation might seem unrelated text classiﬁcation question answering reinforces idea machine translation good candidate task research models stronger sense natural language understanding. transfer learning. transfer learning domain adaptation applied variety areas researchers identify synergistic relationships independently collected datasets. saenko adapt object recognition models developed visual domain imaging conditions learning transformation minimizes domain-induced changes feature distribution. matrix factorization incorporate textual information tagged images enhance image classiﬁcation. natural language processing collobert leverage representations learned unsupervised learning improve performance supervised tasks like named entity recognition part-of-speech tagging chunking. recent work continued direction using pretrained word representations improve models entailment sentiment analysis summarization question answering ramachandran propose initializing sequence-to-sequence models pretrained language models ﬁne-tuning speciﬁc task. also propose method transferring higher-level representations word vectors transfer pretrained encoders sequence-to-sequence models classiﬁcation question answering models without additional ﬁne-tuning. neural machine translation. source domain transfer learning machine translation task seen marked improvements recent years advance neural machine translation models. sutskever investigate sequence-to-sequence models consist neural network encoder decoder machine translation. bahdanau propose augmenting sequence sequence models attention mechanism gives decoder access encoder representations input sequence step sequence generation. luong study effectiveness various attention mechanisms respect machine translation. attention mechanisms also successfully applied tasks like entailment summarization question answering semantic parsing show attentional encoders trained transfer well tasks. transfer learning machine translation. machine translation suitable source domain transfer learning task nature requires model faithfully reproduce sentence target language without losing information source language sentence. moreover abundance machine translation data used transfer learning. hill study effect transferring variety source domains semantic similarity tasks agirre hill demonstrate ﬁxed-length representations obtained encoders outperform obtained monolingual encoders semantic similarity tasks. unlike previous work transfer ﬁxed length representations produced encoders. instead transfer representations token input sequence. approach makes transfer trained encoder directly compatible subsequent lstms attention mechanisms general layers expect input sequences. additionally facilitates transfer sequential dependencies encoder states. transfer learning computer vision. since success cnns imagenet challenge number approaches computer vision tasks relied pretrained cnns off-the-shelf feature extractors. girshick show using pretrained extract features region proposals improves object detection semantic segmentation models. propose cnn-based object tracking framework uses hierarchical features pretrained image captioning train visual sentinel pretrained ﬁne-tune model smaller learning rate. fukui propose combine text representations visual representations extracted pretrained residual network although model transfer seen widespread success computer vision transfer learning beyond pretrained word vectors less pervasive nlp. begin training attentional sequence-to-sequence model english-to-german translation based klein goal transferring encoder tasks. training given sequence words source language glove sequence glove vectors corresponding words sequence randomly initialized word vectors corresponding words feed glove standard two-layer bidirectional long short-term memory network refer mt-lstm indicate two-layer bilstm later transfer pretrained encoder. mt-lstm used compute sequence hidden states time-step decoder ﬁrst uses two-layer unidirectional lstm produce hidden state hdec based previous target embedding context-adjusted hidden state ˜ht− since several bilstm variants deﬁne follows. bilstm represent output sequence bilstm operating input sequence forward lstm computes lstm ﬁnal outputs bilstm time step corresponding sequence word vectors produced glove model sequence context vectors produced mt-lstm. classiﬁcation question answering input sequence concatenate vector glove corresponding vector cove describe general biattentive classiﬁcation network test well cove transfer tasks. model shown figure designed handle single-sentence two-sentence classiﬁcation tasks. case single-sentence tasks input sequence duplicated form sequences assume input sequences rest section. input sequences converted sequences vectors described task-speciﬁc portion model function applies feedforward network relu activation element bidirectional lstm processes resulting sequences obtain task speciﬁc representations sequences stacked along time axis matrices order compute representations interdependent biattention mechanism biattention ﬁrst computes afﬁnity matrix extracts attention weights column-wise normalization figure uses feedforward network relu activation bilstm encoder create task-speciﬁc representations input sequence. biattention conditions representation other bilstm integrates conditional information maxout network uses pooled features compute distribution possible classes. integrate conditioning information representations sequence separate one-layer bidirectional lstms operate concatenation original representations differences context summaries element-wise products originals context summaries outputs bidirectional lstms aggregated pooling along time dimension. mean pooling used models extract features found adding pooling parameter-less form self-attentive pooling tasks. captures different perspective conditioned sequences. self-attentive pooling computes weights time step sequence question answering obtain sequences classiﬁcation except function replaced function uses tanh activation instead relu activation. case sequences document question question-document pair. sequences coattention dynamic decoder implemented original dynamic coattention network machine translation. three different english-german machine translation datasets train three separate mt-lstms. tokenized using moses toolkit smallest dataset comes multi-modal translation shared task training consists sentence pairs brieﬂy describe flickr captions often referred multik. nature image captions dataset contains sentences average shorter simpler larger counterparts. medium-sized dataset version machine translation task prepared international workshop spoken language translation training consists sentence pairs transcribed presentations cover wide variety topics conversational language machine translation datasets. largest dataset comes news translation shared task training consists roughly million sentence pairs comes crawl data news commentary corpus european parliament proceedings european union press releases. refer three datasets mt-small mt-medium mt-large respectively refer context vectors encoders trained turn cove-s cove-m cove-l. task dataset sentiment classiﬁcation sst- sentiment classiﬁcation sst- sentiment classiﬁcation imdb question classiﬁcation trec- trec- question classiﬁcation snli squad sentiment analysis. train model separately sentiment analysis datasets stanford sentiment treebank imdb dataset datasets comprise movie reviews sentiment. binary version dataset well ﬁve-class version sst. imdb contains multi-sentence reviews truncate ﬁrst words. sst- contains examples neutral class removed sub-trees included sst- contains reviews classes sub-trees. question classiﬁcation. question classiﬁcation small trec dataset dataset open-domain fact-based questions divided broad semantic categories. experiment ﬁfty-class six-class versions trec refer trec- trec- respectively. training examples trec- ﬁner-grained labels. entailment. entailment stanford natural language inference corpus training validation testing examples. example consists premise hypothesis label specifying whether premise entails contradicts neutral respect hypothesis. question answering. stanford question answering dataset large-scale question answering dataset training examples development examples test released public. examples consist paragraph english wikipedia associated question-answer pairs paragraph. squad examples assume question answerable answer contained verbatim somewhere paragraph. mt-lstm trained mt-small obtains uncased tokenized bleu score multik test model trained mt-medium obtains uncased tokenized bleu score iwslt test mt-lstm trained mt-large obtains uncased tokenized bleu score test set. results represent strong baseline machine translation models respective datasets. note that smallest dataset highest bleu score also much simpler dataset restricted domain. training details. training mt-lstm used ﬁxed -dimensional word vectors. used commoncrawl-b glove model english word vectors completely ﬁxed training mt-lstm learn pretrained vectors translation. hidden size lstms mt-lstms mt-lstms bidirectional output -dimensional vectors. model trained stochastic gradient descent learning rate began decayed half epoch validation perplexity increased ﬁrst time. dropout ratio applied inputs outputs layers encoder decoder. classiﬁcation question answering explore varying input representations affects ﬁnal performance. table contains validation performances experiments. table cove improves validation performance. cove advantage character n-gram embeddings using improves performance further. models beneﬁt using mtlstm trained mt-large accuracy reported classiﬁcation tasks; squad. training details. unsupervised vectors mt-lstms remain ﬁxed experiments. lstms hidden size models trained using adam dropout applied feedforward layers dropout ratio maxout network reduces dimensionality inputs projecting output dimension. entry table experiments searching dropout ratios reduction factors. beneﬁts cove. figure shows models used cove alongside glove achieved higher validation performance models used glove. figure shows using cove brings larger improvements using character n-gram embeddings also shows altering additionally appending character n-gram embeddings boost performance even tasks. suggests information provided cove complementary word-level information provided glove well character-level information provided character n-gram embeddings. effects training data. experimented different training datasets mt-lstms varying training data affects beneﬁts using cove downstream tasks. figure shows important trend extract table appears positive correlation larger datasets contain complex varied language improvement using cove brings downstream tasks. evidence hypothesis data potential large resource transfer learning nlp. test performance. table shows ﬁnal test accuracies best classiﬁcation models achieved highest validation accuracy task using glove cove character n-gram embeddings. final test performances sst- snli reached state art. table shows validation exact match scores best squad model compare scores recent models literature. submit squad model testing addition cove enough push validation performance original already used character n-gram embeddings validation performance published version r-net. test performances tracked squad leaderboard introduce approach transferring knowledge encoder pretrained machine translation variety downstream tasks. cases models used cove best pretrained mt-lstm performed better baselines used random word vector initialization baselines used pretrained word vectors glove model baselines used word vectors glove model together character n-gram embeddings. hope step towards goal building uniﬁed models rely increasingly general reusable weights. pytorch code https//github.com/salesforce/cove includes example generate cove mt-lstm used best models. hope making best mt-lstm available encourage research shared representations models. bowman angeli potts manning. large annotated corpus learning natural language inference. proceedings conference empirical methods natural language processing association computational linguistics girshick donahue darrell malik. rich feature hierarchies accurate object detection semantic segmentation. proceedings ieee conference computer vision pattern recognition pages koehn hoang birch callison-burch federico bertoldi cowan shen moran zens dyer bojar constantin herbst. moses open source toolkit statistical machine translation. maas daly pham huang potts. learning word vectors sentiment analysis. proceedings annual meeting association computational linguistics human language technologies pages portland oregon june association computational linguistics. http//www.aclweb.org/anthology/p-.", "year": 2017}