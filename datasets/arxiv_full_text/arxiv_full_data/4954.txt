{"title": "Learning to Factor Policies and Action-Value Functions: Factored Action  Space Representations for Deep Reinforcement learning", "tag": ["cs.LG", "cs.AI"], "abstract": "Deep Reinforcement Learning (DRL) methods have performed well in an increasing numbering of high-dimensional visual decision making domains. Among all such visual decision making problems, those with discrete action spaces often tend to have underlying compositional structure in the said action space. Such action spaces often contain actions such as go left, go up as well as go diagonally up and left (which is a composition of the former two actions). The representations of control policies in such domains have traditionally been modeled without exploiting this inherent compositional structure in the action spaces. We propose a new learning paradigm, Factored Action space Representations (FAR) wherein we decompose a control policy learned using a Deep Reinforcement Learning Algorithm into independent components, analogous to decomposing a vector in terms of some orthogonal basis vectors. This architectural modification of the control policy representation allows the agent to learn about multiple actions simultaneously, while executing only one of them. We demonstrate that FAR yields considerable improvements on top of two DRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage Actor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous n-step Q-Learning) in 9 out of 13 tasks.", "text": "deep reinforcement learning methods performed well increasing numbering high-dimensional visual decision making domains. among visual decision making problems discrete action spaces often tend underlying compositional structure said action space. action spaces often contain actions left well diagonally left representations control policies domains traditionally modeled without exploiting inherent compositional structure action spaces. propose learning paradigm factored action space representations wherein decompose control policy learned using deep reinforcement learning algorithm independent components analogous decomposing vector terms orthogonal basis vectors. architectural modiﬁcation control policy representation allows agent learn multiple actions simultaneously executing them. demonstrate yields considerable improvements algorithms atari farac outperforms tasks faraql outperforms tasks. traditional reinforcement learning algorithms worked relatively simple environments wherein policy estimates constructed using tabular methods simple linear parameterizations. state representation problems often consists hand-crafted features. recent advances deep learning enabled methods scale problems exponentially larger state spaces even continuous action domains. combination based cost functions based compositional hierarchical representations state policies value functions resulted ﬁeld deep reinforcement learning methods perform impressively many challenging high-dimensional sequential decision making problem atari mujoco torcs wymann board game many algorithms operate discrete action space domains atari total number actions domain eighteen. eighteen actions visualized figure observe eighteen actions although presented algorithms smallest indivisible units action fact indivisible all. exists inherent underlying compositional structure action space algorithm potentially take advantage training. consider agent executes action diagonally left gets reward corresponding action. insight work feedback used learn diagonally left action also actions left. hence every time diagonal step executed possible learn individual action factors well. atari domain action space decomposed along three independent dimensions vertical motion horizontal motion ﬁring work explicitly factors policies action-value functions along dimensions building factoring agent’s network architecture. notion factoring setup. sharma demonstrated remarkable improvements variety domains using framework factors policy choosing actions another choosing repetitions. motivations work principles underlying biological systems shown learn representations independent orthogonal manner abstract level work also similar intra-option learning frameworks revolves around idea learning option executing another. arguably humans also make decisions similar manner fundamental unit decisions orthogonal directions along decision space decomposed. factoring scheme used extend algorithm operates compositional discrete action spaces paper extend asynchronous advantage actor critic asynchronous n-step q-learning algorithms. demonstrate empirically proposed paradigm factored action space representations provides considerable improvements algorithms. also provide analysis factored policy learned using farac demonstrates robustness compared policies. q-learning control algorithms estimate optimal action-value function guides policy executed agent. off-policy td-learning algorithm q-learning q-learning results convergence estimated q-function optimal q-function. q-learning updates give equation actor-critic algorithms policy-gradient methods actor critic algorithms parametric representations actor critic biased sample-estimate policy gradient given lower variance updates baseline term introduced extending advantage actor-critic algorithms naively setup fails work stochastic gradient descent algorithms commonly used setups assume input samples independent identically distributed. on-policy actor-critic algorithm parametrized neural networks highly correlated inputs therefore performs poorly gradient based methods used. asynchronous advantage actor-critic methods overcome problem using asynchronous parallel actor-learners simultaneously explore different parts state space. learner maintains parameters routinely synchronized learners. using parallel actor learners ensures exploration different parts state space translates updates neural network relatively uncorrelated. modiﬁed version q-learning uses n-step return based td-target order achieve faster reward propagation trade-off bias variance estimation action-value function. modiﬁed update equation given factored action representations framework extend algorithm operates problems compositional discrete action spaces. includes algorithms model q-functions asynchronous n-step q-learning actor-critic algorithms actor critic methods extends policy modeled actor; critic modiﬁed. deep q-learning methods modiﬁes representation action-value function algorithm models. represent either policy actor-critic algorithm q-function deep q-learning algorithm. represents using factored representation. factors represents different dimension composite action space. explain atari domain action space example. figure visualizes factoring complete atari action space. action complete atari action space represented tuple down don’t move vertically} left right don’t move horizontally} {ﬁre don’t ﬁre}. choice factors decomposed depends possible actions given task. decomposed representation allows agent learn corresponding multiple actions simultaneously executing single action. action up-right-ﬁre executed parameters corresponding individual factors right updated. hence xup-ﬁre xright-ﬁre xup-left also adjusted denote states denote discrete action agent. claim often action spaces compositional thus allow decomposition action independent action-factors values factor take. claim instead modeling agent would better modeling individual components factor-spaces individual components realized using independent output layers neural network f... corresponds size |ai|. written terms factor-layer outputs equation combination function suitable parameterized non-parametrized must enforced. detailed training algorithms full action space atari basis actions. basis actions decomposed three independent action factors. ﬁrst factor encodes horizontal movement second factor encodes vertical movement third factor encodes whether visualization action space decomposition atari domain shown figure composite action space visualized -dimensional cuboid dimensions cell represents composite action. axes cuboid represent action-factors. describe instantiation algorithm subsection. policy actor part denote action-factors corresponding horizontal vertical ﬁring dimensions. action-factors combined using combination function softmax action policy given consider action sampling composite action equivalent independently sampling softmax) sub-section demonstrates choice additive combination function gives action policy nice alternate interpretation terms product constituent factors’ policies describe instantiation framework algorithm subsection. q-function modeled aql. denote action-factors corresponding horizontal vertical ﬁring dimension. action-factors combined using combination function case kind combination functions work well action space factoring? action factoring improve performance algorithms learn fundamentally robust policy representations compared baselines? subsection answer ﬁrst question. chosing good many competing farac networks trained implementing different non-parametric choice alien task atari domain. networks trained million steps starting random initialization. combination functions experimented form softmax fn)) {summation multiplication arithmetic mean harmonic mean geometric mean minimum}. note arithmetic mean summation functions identical barring scaling factor subtle difference could however lead different policies learned. observed choosing summation resulted highest performance farac indicated figure decided stick similar functional form case well demonstrate robustness choice combination function. hence q-values computed using summation factor-layer outputs trained farac networks depicted figure tasks atari domain. tasks trained network three different random seeds averaged results estimate performance algorithm. baseline agent also trained random seeds. comparison farac agents’ performance figure results experiments tabulated table appendix evolution game-play performance tasks plotted versus training time. training curves averaged three random seeds. figure contains training curves games. training curves games found appendix detailed explanations training procedure evaluation procedure hyper-parameters found appendix similar figure modiﬁed network constructed faraql using equation trained thirteen tasks atari domain. performance estimated averaging performances across three random seeds. baseline agent trained using random seeds. visual comparison faraql agent presented figure training curves games presented figure training curves games found appendix claim policies learned farac robust compared learned using factored policy representation enables farac agents learn multiple actions executing one. result would expect farac better ordering actions policy order validate hypothesis empirically conducted experiments comparing robustness farac policies. uniformly random best-k analysis experiment follows trained agent taken. probability agent samples actions learned policy. probability samples actions uniformly random best actions. introduces noise learned policy agents robust policies would demonstrate lower drop performance increases. value varied steps corresponding normalized performances plotted. individual curves normalized using maximum score ensure fair comparison farac agents comparing relative changes. experiments sub-section conducted figure observe farac agent comparatively robust policy corruption based analysis trained agent particular task taken. policy agent corrupted injecting noise based using temperature policy agent written softmax) corrupted policy πcor agent represented πcor softmax/z). increasing results injection noise policy. hence robust policy expected smaller drop performance increasing hyper-parameter varied steps performance plotted normalizing maximum score obtained agent. figure clear tasks farac agents robust noise injection. propose novel framework decomposition policies action-value functions discrete action space. factors policy/value function independent components models using independent output layers neural network. framework allows agents exploit underlying compositional structure discrete action spaces found common domains learn better policies action-value functions. empirically demonstrate superiority baseline methods considered. possible extension framework would combine action-space factoring concept action repetition discussed sharma action repetition could another factor action space extension could allow capture large macro actions. baumann simon grifﬁths timothy petkov christopher thiele alexander rees adrian. orthogonal representation sound dimensions primate midbrain. nature neuroscience bellemare marc naddaf yavar veness joel bowling michael. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research june xiaoxiao singh satinder honglak lewis richard wang xiaoshi. deep learning real-time atari game play using ofﬂine monte-carlo tree search planning. advances neural information processing systems jaderberg mnih volodymyr czarnecki wojciech marian schaul leibo joel silver david kavukcuoglu koray. reinforcement learning unsupervised auxiliary tasks. appear international conference learning representations lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. arxiv preprint arxiv. mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. arxiv preprint arxiv. mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg petersen stig beattie charles sadik amir antonoglou ioannis king helen kumaran dharshan wierstra daan legg shane hassabis demis. human-level control deep reinforcement learning. nature february mnih volodymyr agapiou john osindero simon graves alex vinyals oriol kavukcuoglu koray strategic attentive writer learning macro-actions. arxiv preprint arxiv. mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. international conference machine learning sharma sahil lakshminarayanan aravind ravindran balaraman. learning repeat fine grained action repetition deep reinforcement learning. appear international conference learning representations silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam vedavyas lanctot marc dieleman sander grewe dominik nham john kalchbrenner sutskever ilya lillicrap timothy leach madeleine kavukcuoglu koray graepel thore hassabis demis. mastering game deep neural networks tree search. nature ./nature. http//dx.doi.org/./nature. todorov emanuel erez tassa yuval. mujoco physics engine model-based control. ieee/rsj international conference intelligent robots systems iros vilamoura algarve portugal october iros... http//dx.doi.org/./iros... wymann bernhard espié eric guionneau christophe dimitrakakis christos coulom rémi sumner andrew. torcs open racing simulator. software available http//torcs. sourceforge. appendix document experimental details experiments. note reported game-play performances graphs averages across random seeds ensure comparisons robust random starting points parameter vectors. used lstm-variant algorithm farac experiments. async-rmsprop algorithm used updating parameters hyper-parameters mnih initial learning rate used linearly annealed million steps. used n-step returns entropy regularization used encourage exploration similar mnih entropy regularization found hyper-parameter tuning farac tuned optimal learning rate found farac separately. learning rate tuned discounting factor rewards retained since seems work well large number methods models trained million time steps. evaluation done every million steps training followed strategy described sharma evaluation done episodes episode’s length capped steps arrive average score. evolution average game-play performance training progress demonstrated games figure expanded version ﬁgure games found appendix table appendix contains scores obtained farac agents atari games. evaluation done using best agent obtained training million steps used level architecture similar mnih sharma turn uses level architecture mnih figure contains visual depiction network used farac. architecture used farac agents described below ﬁrst three layers convolutional layers ﬁlter sizes strides padding number ﬁlters mnih sharma convolutional layers followed fully connected layers lstm layer. policy value function derived lstm outputs using different output heads. number neurons layers lstm layers similar mnih actor critic networks share ﬁnal output layer. functions policy value function realized different ﬁnal output layer value function outputs non-linearity policy softmax-non linearity output non-linearity model multinomial distribution. learning rate value n-step returns keeping \u0001-greedy strategy highlighted training asynchronous n-step q-learning mnih randomly sampled thread decayed various different values training period lasted million time steps. starting value epsilon linearly decayed million steps ﬁnal value. network architecture mnih used experiments. better understand generalization capabilities framework general farql algorithm particular decided exact hyper-parameter choices faraql. none hyper-parameters faraql tuned speciﬁcally algorithm. represents hyper-parameter setting favorable faraql. even setting observe faraql signiﬁcantly outperforms aql. tuning hyper-parameters speciﬁcally faraql could result improvement performance. assume global shared parameter vectors assume global step counter number independent factors tmax total number training steps farac agent policy agent list factor heads independent factors initialize local thread’s step counter repeat tmax assume global shared parameter vector assume global shared parameter vector target network assume global step counter number independent factors tmax total number training steps faraql agent action-value function agent list factor heads independent factors initialize local thread’s step counter initialize target network parameters initialize local thread’s parameter repeat tmax appendix document performances agents corresponding baseline performances. performances obtained averaging across random seeds ensure robust comparison.", "year": 2017}