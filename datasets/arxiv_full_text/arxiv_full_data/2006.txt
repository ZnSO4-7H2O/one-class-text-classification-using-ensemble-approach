{"title": "Implementation of a Practical Distributed Calculation System with  Browsers and JavaScript, and Application to Distributed Deep Learning", "tag": ["cs.DC", "cs.LG", "cs.MS", "cs.NE", "stat.ML"], "abstract": "Deep learning can achieve outstanding results in various fields. However, it requires so significant computational power that graphics processing units (GPUs) and/or numerous computers are often required for the practical application. We have developed a new distributed calculation framework called \"Sashimi\" that allows any computer to be used as a distribution node only by accessing a website. We have also developed a new JavaScript neural network framework called \"Sukiyaki\" that uses general purpose GPUs with web browsers. Sukiyaki performs 30 times faster than a conventional JavaScript library for deep convolutional neural networks (deep CNNs) learning. The combination of Sashimi and Sukiyaki, as well as new distribution algorithms, demonstrates the distributed deep learning of deep CNNs only with web browsers on various devices. The libraries that comprise the proposed methods are available under MIT license at http://mil-tokyo.github.io/.", "text": "deep learning achieve outstanding results various ﬁelds. however requires signiﬁcant computational power graphics processing units and/or numerous computers often required practical application. developed distributed calculation framework called sashimi allows computer used distribution node accessing website. also developed javascript neural network framework called sukiyaki uses general purpose gpus browsers. sukiyaki performs times faster conventional javascript library deep convolutional neural networks learning. combination sashimi sukiyaki well distribution algorithms demonstrates distributed deep learning deep cnns browsers various devices. libraries comprise proposed methods available license http//mil-tokyo.github.io/. utilization data recently come play increasingly important role various business ﬁelds. data often handled deep learning algorithms achieve outstanding results various ﬁelds. example almost teams participated ilsvrc image recognition competition used deep learning algorithms. algorithms also used speech recognition molecular activity prediction tional complexity often require graphics processing units numerous computers practical distributed computation. difﬁcult construct distributed computation environment frequently requires preparation certain operating systems installation speciﬁc software e.g. hadoop practical application deep learning algorithms instant distributed calculation environment eagerly anticipated. developed distributed calculation framework called sashimi. general distributed calculation framework difﬁcult increase number node computers users must install client software node computer. sashimi computer become node computer accessing certain website browser without installing client software. proposed system execute code written javascript distributed manner. sashimi consists servers calculationframework distributor servers user runs project includes distributed processing using calculationframework accesses distributor browsers processes distributed executed multiple browsers. results processed distributed machines used processed local machine without conscious differences using calculationframework. calculationframework describes calculations include distributed processing. user writes code according certain interface processes distributed computed multiple browsers distributor. results computed browsers automatically colproject programming unit calculationframework endpoint process starts. project user execute distributed processing creating task instance. note processes require distributed processing also supported. task process distributed executed browsers. user writes task according certain interface arguments automatically divided distributed browsers. processed results automatically collected. primelistmakerproject task determines whether input integer prime number called isprimetask. task distributed among browsers. task given arguments generated project must return calculated results using callback function. note user external libraries datasets. example task calls prime function external library determines whether input integer prime number. project generates task instance arguments framework generates tickets divided argument. framework sends codes arguments tickets external libraries datasets distributor mysql. tickets distributed distributor collected used calculationframework mysql. since project implemented server-side javascript node.js task implemented javascript used without considering whether code executed server browsers. httpserver server implemented node.js provides static ﬁles include basic program discloses apis offer datasets used distributed calculation. user wants make browser function node user needs access basic program provided httpserver browser. basic program consists static html javascript ﬁle. basic program works follows. task external data cached browser. program runs long time memory usage increases cache. therefore implemented garbage collection basis least recently used algorithm. error occurs task running error report includes stack trace sent ticketdistributor. then browser reloads itself. thus task described tickets generated calculationframework continuously executed without special maintenance user accesses program. users check progress task tickets httpserver control console. console users project name number tasks number tickets waiting processed number executed tickets number error reports client information project. note console used execute code browsers also provided. console user make browsers reload redirect another distributed system. responsive design techniques user interface basic program control console. techniques adapt screen size tablet smartphone makes easy devices distributed calculation check progress. tasks tickets generated calculationframework distributed browsers websocket ticketdistributor. processed results also collected ticketdistributor. unlike httpserver ticketdistributor runs single process communicates browser unitarily efﬁciently. ticketdistributor receives ticket request browser obtains tickets ascending order virtual created time mysql server. virtual created time determined follows. thus results returned within minutes tickets treated re-created. note tickets redistributed ascending order distribution time tickets distributed. thus browser terminated receives ticket and/or clients computational capability anclient execute task. therefore throughput enhanced. tickets redistributed intervals least seconds prevents last ticket distributed many clients prevents next calculation delayed. implemented algorithm using quickly select tickets distributed. using sashimi demonstrate task high computational cost computed parallel efﬁciently. here compare time required classify mnist dataset nearest neighbor method changing number clients. experiment images mnist test images classiﬁed comparing training images. used four clients desktop computer tablet described table accessed distributor using google chrome browser desktop tablet environments. remarkable proposed system used tablet tablet lower computational power desktop computer overhead time required distribution becomes relatively shorter. believe proposed distributed computing method become effective feature extraction methods high computational costs sift deep learning. implemented learning algorithm deep neural networks browsers based sashimi. here explain proposed framework implementation dnns. also discuss advantages proposed method existing library stand-alone environment. distributed computation dnns explained next section. primarily implemented deep convolutional neural networks obtain high classiﬁcation accuracy image recognition tasks. convnetjs implemented library using javascript. however computational speed limited runs single thread. therefore developed deep neural network framework called sukiyaki utilizes fast matrix library called sushi sushi matrix library fast implemented webcl utilize general purpose gpus efﬁciently. sukiyaki framework consists sukiyaki object handles procedures learning testing neural network layer objects. version deep cnns implemented convolutional layer pooling layer fully-connected layer activation layer. note layers implement certain methods forward backward update. forward backward update methods layer implemented using sushi matrix library; thus executed parallel gpgpus. where scalar learning rate i-th element parameter time step i-th element gradient time step however update rule learning usually becomes unstable squared gradients minuscule early learning process. therefore modiﬁed update rule using constant follows. designed sukiyaki framework used node.js browsers dnns trained distributed manner using sashimi distributed calculation framework. example model wherein parameters encoded base formatted json. note although model platform independent string format exchanged among machines without rounding errors. experiment used deep model shown figure fifty images mini-batch learned training images cifar- note cifar- consists -bit color images classes. model convolves input images kernels convolutional layer generates three feature maps size convolutional layer followed activation layer pooling layer size output halved. fourth layer fully-connected layer results shown table figure observed sukiyaki learned network faster convnetjs node.js firefox. convergence speed sukiyaki also faster convnetjs. note sukiyaki node.js learned network times faster convnetjs. allelize training deep cnns using model parallelism data parallelism efﬁciently. generally deep cnns consist many convolutional layers fullyconnected layers. weight sharing convolutional layers incur signiﬁcant computational cost relative small number parameters. however fully-connected layers many parameters convolutional layers less computational complexity. krizhevsky developed efﬁcient method parallelize training deep cnns applying data parallelism convolutional layers model parallelism fully-connected layers. however focus distributed computation internet; thus must reduce communication costs proposed framework. implemented another effective method distributed deep learning. parallelize training convolutional layers using data parallelism multi-gpus. gpus synchronized fully-connected layers trained single gpu. computational complexity training fully-connected layers relatively small model parallelism fullyconnected layers necessarily contribute fast learning. method combining parallelized standalone learning works efﬁciently easy implement. however method computational resources stay idle fully-connected layers learned single still room improvement. singa distributed deep learning platform. supports model partition data partition manage automatically distributed array data structure without much awareness array partition. singa designed accelerate deep learning using unclear whether approach appropriate distributed calculation internet. study developed method parallelizes training convolutional layers using data parallelism apply model parallelism fullyconnected layers. proposed method trains fullyconnected layers server clients train convolutional layers. unlike method convolutional layers fully-connected layefﬁcient distributed computing method dnns. distbelief network partitioned subnetworks different machines responsible computation different subnetworks. nodes edges cross partition boundaries must share state information between machines. however since consider machines connected internet slow throughput difﬁcult share nodes’ state different machines proposed framework. distbelief focuses fully-connected network; thus also difﬁcult directly apply approach convolutional networks share weights among different nodes. meeds developed mlitb wherein different training data batches assigned different clients. clients compute gradients send master computes weighted average gradients clients updates network. network weights sent clients clients restart compute gradients basis weights. approach simple easy implement; however must communicate network weights gradients master clients. thus communication overhead becomes excessively large large network. tion number clients. furthermore developed sukiyaki framework utilizes gpgpus train deep cnns times faster conventional javascript library. building sukiyaki sashimi also developed parallel computing method deep cnns suitable distributed computing internet. shown deep cnns trained parallel using browsers. libraries available license http//mil-tokyo.github.io/. future plan improve efﬁciency distribution algorithm considering clients’ computational capabilities supporting annetwork sukiyaki. note welcome suggestions improvements code documentation. hope many programmers develop sukiyaki sashimi become high-performance distributed computing platform anyone easily.", "year": 2015}