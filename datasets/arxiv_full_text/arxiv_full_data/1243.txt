{"title": "Memory-augmented Attention Modelling for Videos", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We present a method to improve video description generation by modeling higher-order interactions between video frames and described concepts. By storing past visual attention in the video associated to previously generated words, the system is able to decide what to look at and describe in light of what it has already looked at and described. This enables not only more effective local attention, but tractable consideration of the video sequence while generating each word. Evaluation on the challenging and popular MSVD and Charades datasets demonstrates that the proposed architecture outperforms previous video description approaches without requiring external temporal video features.", "text": "refer actions events taking place multiple frames video. example video depicting someone waving hand waving action start frame continue variable number following frames. time videos contain many frames provide additional information smaller frames necessary generate summarizing description. given challenges surprising even recent advancements image captioning video captioning remained challenging. motivated observations introduce memorybased attention mechanism video captioning description. model utilizes memories past attention video reasoning attend current time step. allows model effectively leverage local attention also consider entire video generates word. mechanism effectively binds information vision language sources coherent structure. work shares goals recent work attention mechanisms sequence-to-sequence architectures rockt¨aschel yang rockt¨aschel consider domain entailment relations goal determine entailment given input sentences. propose soft attention model focused current state previous well. model previous attentions explicitly stored memory system learns memorize encoded version input videos conditioned previously seen words. yang work solve problem locality attention visionto-language yang introduce memory architecture optimized single image caption generation introduce memory architecture operates streaming video’s temporal sequence. present method improve video description generation modeling higher-order interactions video frames described concepts. storing past visual attention video associated previously generated words system able decide look describe light already looked described. enables effective local attention tractable consideration video sequence generating word. evaluation challenging popular msvd charades datasets demonstrates proposed architecture outperforms previous video description approaches without requiring external temporal video features. source code paper available https// github.com/rasoolfa/videocap. deep neural architectures remarkable progress computer vision natural language processing problems. image captioning problem combination convolutional structures sequential recurrent structures leads remarkable improvements previous work emerging modelling paradigms shared models image captioning well related visionlanguage problems notion attention mechanism guides model attend certain parts image generating attention models used problems image captioning typically depend single image consideration partial output generated jointly capturing region image words generated. however models cannot directly capture temporal reasoning necessary effectively produce words primary challenges learning mapping visual space language space learning representation effectively represents modalities also able translate representation space other. rohrbach developed model generates semantic representation visual content used source language language generation module. venugopalan proposed deep method translate video sentence entire video represented single vector based mean pool frame features. however recognized representing video average frames loses temporal structure video. address problem recent work proposed methods model temporal structure videos well language. majority methods inspired sequence-tosequence attention models. sequence learning proposed input sequence source language target language applying method additional attention mechanism problem translating video description showed promising initial results however revealed additional challenges. first modelling video content ﬁxed-length vector order language space complex problem mapping language language given complexity visual content difference modalities. since frames video equally salient short description event happen multiple frames important model identify frames salient. further models need additional work able focus points interest within video frames select talk about. even variable-length vector represent video using attention problems. speciﬁcally current attention methods local since attention mechanism works sequential structure lack ability capture global structure. moreover combining video description sequence-to-sequence problem motivates using variant recurrent neural network given limited capacity recurrent network model long sequences memory networks introduced help memorize sequences. however problem memory networks suffer difﬁculty training model. model proposed weston requires supervision layer makes training backpropagation challenging task. sukhbaatar proposed memory network trained end-to-end current work follows research line tackle challenging problem modeling vision language memories video description generation. main challenge video description mapping capture connection video frames video description. sequence-to-sequence models work well connecting input output sequences machine translation perform well task direct alignment full video sequence summarizing description. goal video description problem create architecture learns moments focus video sequence order generate summarizing natural language description. modelling challenges forth video description problem processing temporal structure video; learning attend important parts video; generating description word relevant video. high-level understood three primary parts moments video particularly salient; concepts focus talk them. directly address issues end-to-end network three primary corresponding components temporal model iterative attention/memory model decoder. summary temporal model place capture temporal structure video functions component. iterative attention/memory main contribution work functioning component remember relationships words video frames storing longer term memories. decoder generates language functions component create ﬁnal description. notational note numbered equations bold face denote multi-dimensional learnable parameters e.g. distinguish different sets time steps video frames words description notation video language. throughout terms description caption used interchangeably. recurrent neural network shown effectual modelling temporal structure sequential data video speech order apply video sequences generate description seek capture fact frame-to-frame temporal variation tends local critical modeling motion visual features extracted last fully connected layers convolutional neural networks shown produce state-of-the-art results image classiﬁcation recognition thus seem good choice modeling visual frames. however features tend discard level information useful modeling motion video address challenges implement call temporal model time step video frame encoding serves input. rather extracting video frame features fully connected layer pretrained extract intermediate convolutional maps. detail given video frames convolutional maps size rl×d extracted number locations input frame number dimensions enable network store important locations frame soft location attention mechanism flatt ﬁrst softmax compute probabilities specify importance different parts frame creates input flatt. formally given video frame time rl×d component addresses several issues generating coherent video description. video description single word phrase often describes action spanning multiple frames within input video. employing model effectively capture relationship relatively short language action occurs multiple frames. also functions directly address problem identifying parts video relevant description. proposed iterative attention/memory mechanism formalized attention update memory update detailed figure figure illustrates sits within full model attention module shown memory module shown formalized figure attention update computes probabilities given time step attention within input video states memory state decoder state. memory update stores attended described. serves memorization component combining previous memory current iterative attention lstm equations described enable network learn multi-layer attention input video corresponding language. output function hidden state time dimensions rl×k. video frame time step learns vector representation applying location attention frame convolution conditioned previously seen frames rnn/lstm/gru cell parameters fact vanilla rnns gradient vanishing exploding problems gradient clipping lstm following handle potential vanishing gradients main contribution work global view video description task memory-based attention mechanism learns iterative attention relationships efﬁcient sequence-to-sequence memory structure. refer iterative attention/memory mechanism aggregates information previously generated words input frames. component iterative memorized attention between input video description. speciﬁcally learns iterative attention structure attend video given previously generated words previous states functions memory structure remembering encoded versions video corresponding language turn enabling decoder access full encoded video previously generated words generates words. vocabulary size. fully train network end-to-end fashion using ﬁrst-order stochastic gradient-based optimization method adaptive learning rate. speciﬁcally order optimize network parameters adam learning rate respectively. training batch size source code paper available https//github.com/rasoolfa/videocap. experiments dataset evaluate model charades dataset microsoft video description corpus charades contains videos provides video descriptions. follow train/test splits sigurdsson train test validation. main difference dataset others uses hollywood homes approach data collection actors crowdsourced different actions. yields diverse videos containing speciﬁc action. msvd youtube videos annotated workers mechanical turk asked pick video clips representing activity. dataset clip annotated multiple workers single sentence. dataset contains videos descriptions videos training data test rest validation. order results comparable approaches follow exact training/validation/test splits provided venugopalan evaluation metrics report results video description generation task. order evaluate descriptions generated model model-free automatic evaluation metrics. adopt meteor bleu-n cider metrics available microsoft coco caption evaluation code score system. video caption preprocessing preprocess captions datasets using natural language toolkit clip description words since majority less. extract sample frames video pass frame vggnet without ﬁne-tuning. experiments paper feature maps conv layer applying relu. feature layer component operates ﬂattened feature cubes. ablation studies features fully connected layer dimensions used well. hyper-parameter random search validation select hyper-parameters datasets. wordembedding size hidden layer size memory size best model charades respectively. values model msvd dataset. stack lstms used decoder tem. number frame samples hyperparameter selected among validation set. best results validation frames number frames models ablation study. ﬁrst present ablation analysis elucidate contribution different components proposed model. then compare overall performance model recent models. ablation results shown table evaluating msvd test set. ﬁrst corresponds simpler version model remove component instead pass frame video extracting features last fullyconnected hidden layer. addition replace simpler version model memorizes current step instead previous steps. next variation ﬁrst except instead fully connected features. next ablation remove component model keep rest model as-is. next variation remove calculate features frame similar tem. finally last table proposed model components. plays signiﬁcant role proposed model removing causes large drop performance measured bleu meteor. hand removing drop performance much dropping iam. putting together complement another result overall better performance meteor. however development component future work warranted. condition entire video must represented ﬁxed-length vector contribute lower performance contrast models apply single layer attention search relevant parts video aligned description. extensively evaluate proposed model compare state-of-the-art models baselines video caption generation task msvd dataset. experiment frames video inputs module. shown table proposed model achieves state-of-the-art scores bleu- outperforms almost systems meteor. closest-scoring comparison system shows trade-off meteor bleu bleu prefers descriptions short-distance ﬂuency high lexical overlap observed descriptions meteor permits less direct overlap longer descriptions. detailed study generated descriptions systems would needed better understand differences. improvement previous work particularly noteworthy external features video optical flow -dimensional convolutional network features ﬁne-tuned features enhances aspects action recognition leveraging external dataset ucf-. system using external features outperforms model proposed uses slightly different version dataset along features large improvement results future work explore utility external visual features work. here demonstrate proposed architecture maps visual space language space improved performance previous work addition resources. additionally report results charades dataset challenging train because captions video. experiment frames video input module. shown table method achieves relative improvement venugopalan model reported sigurdsson worth noting humans reach meteor score bleu- score illustrating upper bound task. show example descriptions generated system figure model generates mostly correct descriptions naturalistic variation ground truth. errors illustrate preference describe items higher likelihood mentioned even appear less frames. example trampoline video model focuses appears frames generates incorrect description washing bath. errors alongside ablation study shown table suggest module particular improved focusing frames video sequence captured passed module. natural language description. model utilizes deep learning architecture represents video explicit model video’s temporal structure jointly models video description temporal video sequence. effectively connects visual video space language description space. memory-based attention mechanism helps guide attend reason description generated. allows model reason efﬁciently local attention also consider full sequence video frames generation word. experiments conﬁrm memory components architecture notably module play signiﬁcant role improving performance entire network. devlin jacob cheng fang gupta saurabh deng xiaodong zweig geoffrey mitchell margaret. language models image captioning quirks works. acl-ijcnlp july iandola forrest srivastava rupesh deng dollar piotr jianfeng xiaodong mitchell margaret platt john lawrence zitnick zweig geoffrey. captions visual concepts back. cvpr june guadarrama sergio krishnamoorthy niveda malkarnenkar girish venugopalan subhashini mooney raymond darrell trevor saenko kate. youtubetext recognizing describing arbitrary activities using semantic hierarchies zero-shot recognition. iccv krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. pereira burges bottou weinberger nips sigurdsson gunnar varol g¨ul wang xiaolong farhadi laptev ivan gupta abhinav. hollywood homes crowdsourcing data collection activity understanding. eccv venugopalan subhashini huijuan donahue jeff rohrbach marcus mooney raymond saenko kate. translating videos natural language using deep recurrent neural networks. naacl kelvin jimmy kiros ryan kyunghyun courville aaron salakhudinov ruslan zemel rich bengio yoshua. show attend tell neural image caption generation visual attention. icml", "year": 2016}