{"title": "Piecewise convexity of artificial neural networks", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Although artificial neural networks have shown great promise in applications including computer vision and speech recognition, there remains considerable practical and theoretical difficulty in optimizing their parameters. The seemingly unreasonable success of gradient descent methods in minimizing these non-convex functions remains poorly understood. In this work we offer some theoretical guarantees for networks with piecewise affine activation functions, which have in recent years become the norm. We prove three main results. Firstly, that the network is piecewise convex as a function of the input data. Secondly, that the network, considered as a function of the parameters in a single layer, all others held constant, is again piecewise convex. Finally, that the network as a function of all its parameters is piecewise multi-convex, a generalization of biconvexity. From here we characterize the local minima and stationary points of the training objective, showing that they minimize certain subsets of the parameter space. We then analyze the performance of two optimization algorithms on multi-convex problems: gradient descent, and a method which repeatedly solves a number of convex sub-problems. We prove necessary convergence conditions for the first algorithm and both necessary and sufficient conditions for the second, after introducing regularization to the objective. Finally, we remark on the remaining difficulty of the global optimization problem. Under the squared error objective, we show that by varying the training data, a single rectifier neuron admits local minima arbitrarily far apart, both in objective value and parameter space.", "text": "although artiﬁcial neural networks shown great promise applications including computer vision speech recognition remains considerable practical theoretical diﬃculty optimizing parameters. seemingly unreasonable success gradient descent methods minimizing non-convex functions remains poorly understood. work oﬀer theoretical guarantees networks piecewise aﬃne activation functions recent years become norm. prove three main results. firstly network piecewise convex function input data. secondly network considered function parameters single layer others held constant piecewise convex. finally network function parameters piecewise multi-convex generalization biconvexity. characterize local minima stationary points training objective showing minimize certain subsets parameter space. analyze performance optimization algorithms multi-convex problems gradient descent method repeatedly solves number convex sub-problems. prove necessary convergence conditions ﬁrst algorithm necessary suﬃcient conditions second introducing regularization objective. finally remark remaining diﬃculty global optimization problem. squared error objective show varying training data single rectiﬁer neuron admits local minima arbitrarily apart objective value parameter space. machine translation. however little understood process trained supervised learning tasks. problem optimizing parameters active area practical theoretical research. despite considerable sensitivity initialization choice hyperparameters neural networks often achieve compelling results optimization gradient descent methods. nonconvexity massive parameter space functions poorly understood sub-optimal methods proven successful. indeed training certain kind neural network known npcomplete making diﬃcult provide worst-case training guarantees much recent work attempted reconcile diﬀerences theory practice article attempts modest step towards understanding dynamics training procedure. establish three main convexity results certain class neural network current state art. first objective piecewise convex function input data parameters ﬁxed corresponds behavior test time. second objective piecewise convex function parameters single layer input data parameters held constant. third training objective function parameters variable input data ﬁxed piecewise multi-convex. continuous function represented ﬁnite number multi-convex functions active multi-convex parameter set. generalizes notion biconvexity found optimization literature piecewise functions arbitrary index sets prove results require main restrictions deﬁnition neural network layers piecewise aﬃne functions objective function convex continuously diﬀerentiable. deﬁnition includes many contemporary cases least squares logistic regression convolutional neural network rectiﬁed linear unit activation functions either maxmean-pooling. recent years networks mostly supplanted classic sigmoid type except case recurrent networks make assumptions training data results apply current state many practical scenarios. piecewise multi-convexity allows characterize extrema training objective. case biconvex functions stationary points local minima guaranteed optimality larger sets would general smooth functions. speciﬁcally points partial minima restricted relevant piece. points decrease made training objective without simultaneously varying parameters across multiple layers crossing boundary diﬀerent piece function. unlike global minima show partial minima reliably found optimization algorithms used current practice. finally provide guarantees solving general multi-convex optimization problems various algorithms. first analyze gradient descent proving necessary convergence conditions. show every point gradient descent converges piecewise partial minimum excepting boundary conditions. prove stronger results deﬁne diﬀerent optimization procedure breaking parameter update number convex sub-problems. procedure show necessary suﬃcient conditions convergence piecewise partial minimum. interestingly adding regularization training objective needed prove necessary conditions. similar results independently established many kinds optimization problems including bilinear biconvex optimization machine learning special case linear autoencoders analysis extends existing results alternating convex optimization case arbitrary index sets general multi-convex point sets needed neural networks. admit biconvex problems therefore linear autoencoders special case. despite results diﬃcult pass partial global optimality results. unlike encouraging case linear autoencoders show single rectiﬁer neuron squared error objective admits arbitrarily poor local minima. suggests much work remains done understanding sub-optimal methods succeed neural networks. still piecewise multi-convex functions senses easier minimize general class smooth functions none previous guarantees made. hope characterization neural networks could contribute better understanding important machine learning systems. note deﬁnition piecewise convexity diﬀers found convex optimization literature focuses convex piecewise convex functions i.e. maxima convex functions note also claim unique representation terms active functions pieces exists least representation. bj). function pieces divided hyperplane x+bi said max. thus pieces intersections half-spaces convex polytopes. since pieces intersections pieces convex polytopes well. proof. piece piece note convex functions convex thus convex furthermore convex intersection convex sets since holds pieces continuous piecewise convex work deﬁne neural network composition functions kinds convex continuously diﬀerentiable objective function continuous piecewise aﬃne functions constituting layers. furthermore outermost function must denotes entire network. deﬁnition restrictive seem upon ﬁrst glance. example easily veriﬁed rectiﬁed linear unit neuron continuous piecewise aﬃne maximum taken pointwise. shown maxima minima aﬃne functions piecewise aﬃne includes convolutional variant toeplitz matrix. similarly pooling continuous piecewise linear mean pooling simply linear. furthermore many objective functions commonly seen machine learning convex continuously diﬀerentiable least squares logistic regression. thus seemingly restrictive class neural networks actually encompasses current state art. theorem composition layers continuous piecewise aﬃne. therefore neural network ultimately composition continuous convex function single continuous piecewise aﬃne function. thus theorem network continuous piecewise convex. figure provides visualization result example network neural networks piecewise continuously diﬀerentiable note objective continuously diﬀerentiable aﬃne active functions layers. thus composition continuously diﬀerentiable. follows non-diﬀerentiable points found boundaries pieces. previous section deﬁned neural networks functions labeled data. functions relevant testing parameters constant data variable. section extend results parameters data testing hold constant consider function data training hold constant consider function parameters mean network considered function parameters. leads additional stipulation deﬁnition neural network. layer must piecewise aﬃne function parameters well. easily veriﬁed layer types previously mentioned. example relu neuron section said neural network considered function input data convex continuously diﬀerentiable piece. neural network need piecewise convex function entirety parameters. however regain piecewise convexity considering function parameters single layer others held constant. proof. time being assume input data consists single point deﬁnition composition convex objective layers function denote network considered function parameters layer others held constant. layers constant respect parameters write thus piece deﬁnition continuous piecewise aﬃne function parameters. since constant continuous piecewise aﬃne function parameters theorem continuous piecewise aﬃne function parameters thus theorem continuous piecewise convex. established theorem case single data point conk=. continuous piecewise convex. furthermore arithmetic mean preserves piecewise continuous differentiability. thus results hold mean value network dataset. previous section analyzed convexity properties neural networks optimizing parameters single layer others held constant. ready extend results ultimate goal simultaneously optimizing network parameters. although convex problem special convex substructure exploit proving future results. begin deﬁning substructure point sets functions. words subset every point equal components indexed note diﬀers typical deﬁnition intersection hyperplane. example xy-plane. note also crosssections unique example case ﬁrst components cross section irrelevant maintain notational convenience. apply concept functions formalizes notion restricting non-convex function variable subset convex section neural network restricted parameters single layer. example convex function ﬁxed similarly convex function ﬁxed thus multi-convex respect fully deﬁne multi-convex optimization problem introduce similar concept point sets. generalizes notion biconvexity found optimization literature here extend deﬁnition multi-convex functions. however drop topological restrictions pieces function since multi-convex sets need connected. extend results section multiple layers must ﬁnal constraint deﬁnition neural network. layers must continuous piecewise multi-convex considered functions parameters input. again easily veriﬁed layer types previously mentioned. already shown piecewise convex cross-section taking index sets separate parameters input data. remains show number pieces ﬁnite. layer merits consideration relu equation consists pieces component dead constant region compliment. components pieces corresponding binary assignments dead alive component. lemma continuous piecewise multi-convex respect collection index sets respect indexes variables variables continuous piecewise multi-convex respect proof. piece piece chosen clearly multiconvex respect remains show multi-convex set. shall show cross-sections convex. first {y}. similarly y)}. sets convex cartesian products convex sets finally proof theorem cover ﬁnite collection pieces taken proof. moment assume single data point. denote layers parameters since continuous piecewise multi-convex functions parameters input write two-layer sub-network repeatedly applying lemma whole network multi-convex ﬁnite number sets covering input parameter space. coming sections shall multi-convexity allows give certain guarantees convergence various optimization algorithms. ﬁrst shall prove basic results independent optimization procedure. results summarized gorksi case biconvex diﬀerentiable functions extend piecewise functions arbitrary index sets. first deﬁne special type minimum relevant multi-convex functions. clear multi-convexity provides wealth results concerning partial minima piecewise multi-convexity restricts results subset domain. less obvious partial minima smooth multi-convex functions need local minima. example pointed reviewer work biconvex function partial minimum origin local minimum. however converse easily veriﬁed even absence diﬀerentiability. seen multi-convex functions close relationship between stationary points local minima partial minima. functions inﬁnitesimal results concerning derivatives local minima extended larger sets. however make guarantees global minima. good news that unlike global minima shall easily solve partial minima. realm non-convex optimization also called global optimization methods divided groups certiﬁably global minimum cannot. former group sacriﬁce speed latter correctness. work focuses algorithms latter kind called local sub-optimal methods type used practice deep neural networks. particular common methods variants gradient descent gradient network respect parameters denotes jacobian operator. note parameters considered ﬁxed whereas parameters variable input data ﬁxed. thus gradient respect parameters exists. special observation proceed layer neural network bottom ◦...◦gm+ time computing gradient respect parameters need store vector matrix forgotten step. known backward pass allows eﬃcient computation gradient neural network respect parameters. similar algorithm computes value function input data often needed evaluate first compute store function input data known forward pass. forward backward pass computed respect network parameters. variants basic procedure preferred practice computational cost scales well number network parameters. covers usually done deep neural networks. note deﬁned happens since ultimately interested neural networks ignore case sequence diverges. gradient descent guaranteed converge global minimum diﬀerentiable functions. however natural points converge. brings basic important result. convex optimization literature simple result sometimes stated connection zangwill’s much general convergence theorem note however unlike zangwill state necessary rather suﬃcient conditions convergence. many similar results known diﬃcult strictly weaken conditions theorem example relax condition summable take always converge non-stationary point. similarly relax constraint continuously diﬀerentiable taking decreasing monotonically zero always converge origin diﬀerentiable. furthermore constant converge almost possible prove much stronger necessary suﬃcient conditions gradient descent results require additional assumptions step size policy well function minimized possibly even initialization worth discussing greater detail since piecewise aﬃne function thus interest investigation neural networks. said convergence point diﬀerentiable remains subdifferentiable convergence results known subgradient descent work shall make subgradients instead considering descent piecewise continuously diﬀerentiable function pieces although theorem apply function relevant results hold anyways. minimal piece result extends continuous piecewise convex function saddle point guaranteed minimize piece. note analysis fails practice. assumed gradient precisely known. practice often prohibitively expensive compute average gradient large datasets. instead take random subsamples procedure known stochastic gradient descent. analyze properties here current results topic impose additional restrictions objective function step size require diﬀerent deﬁnitions convergence restricting true gradient allows provide simple proofs applying extensive class neural networks. ready generalize results neural networks. slight ambiguity boundary points pieces need diﬀerentiable even sub-diﬀerentiable. since interested necessary conditions gradient descent diverges exist. however next theorem least handle non-diﬀerentiable limit points. theorem collection sets covering continuous piecewise multi-convex respect piecewise continuously diﬀerentiable. then {xk}∞ result gradient descent limk→∞ either proof. ﬁrst condition holds result follows directly theorems second condition holds {xk}∞ convergent gradient descent sequence active function since continuously diﬀerentiable ﬁrst condition holds since partial minimum well. ﬁrst condition theorem holds every point interior piece boundary points. second condition extends results non-diﬀerentiable boundary points long gradient descent eventually conﬁned single piece function. example consider continuous piecewise convex function shown ﬁgure converge piece converging smooth function example also illustrates important caveat regarding boundary points although extremum extremum although previous section contained powerful results theorem suﬀers main weaknesses necessary condition requires extra care non-diﬀerentiable points. diﬃcult overcome limitations gradient descent. instead shall deﬁne diﬀerent optimization technique necessary suﬃcient convergence results follow regardless diﬀerentiability. iterated convex optimization splits non-convex optimization problem number convex sub-problems solving sub-problems iteration. neural network shown problem optimizing parameters single layer others held constant piecewise convex. thus restricting given piece yields convex optimization problem. section show convex sub-problems solved repeatedly converging piecewise partial optimum. work assume convex sub-problems solvable without delving speciﬁc solution techniques. methods alternating solvable sub-problems studied many authors many diﬀerent types sub-problems context machine learning results developed special case linear autoencoders still extra care must taken extending results arbitrary index sets. updated sub-problems solved iteration consists solving convex sub-problems. equivalent usual alternating convex optimization biconvex functions consists sets general multi-convex functions. basic convergence results follow immediately solvability problem first note feasible point implies limk→∞ exists long bounded below. however imply existence limk→∞ gorski example biconvex function diverges prove stronger convergence results introduce regularization objective. theorem collection sets covering multi-convex respect next λkxk convex norm. finally {xk}∞ result iterated convex optimization least convergent subsequence topology induced metric kx−yk. proof. lemma multi-convex allowed iterated convex optimization. λkxk thus whenever since nonincreasing sequence kxkk equivalently lies /λ}. since continuous closed bounded thus compact. then bolzano-weierstrauss theorem least convergent subsequence theorem function called regularized version practice regularization often makes non-convex optimization problem easier solve reduce over-ﬁtting. theorem shows iterated convex optimization regularized function always least convergent subsequence. next shall establish rather strong properties limits subsequences. theorem collection sets covering multi-convex respect next {xk}∞ result iterated convex optimization limit every convergent subsequence partial minimum respect topology induced metric kx−yk norm kxk. furthermore {xmk {xnk convergent subsequences limk→∞ limk→∞ proof. denote subsequence limn→∞ assume sake contradiction partial minimum ints respect continuous must kx−x′k implies furthermore since interior point must open ball radius centered shown ﬁgure must kxnk min. then since previous theorem extension results reviewed gorski arbitrary index sets gorski explicitly constrain domain compact biconvex show regularization guarantees cannot escape certain compact establishing necessary condition convergence. furthermore results hold general multi-convex sets earlier result restricted cartesian products compact sets. results iterated convex optimization considerably stronger shown gradient descent. bounded sequence convergent subsequence guarantee boundedness variants gradient descent cannot normally much limits subsequences. iterated convex optimization shown limit subsequence partial minimum limits subsequences equal objective value. practical purposes good saying original sequence converges partial minimum. although provided necessary suﬃcient conditions convergence various optimization algorithms neural networks points convergence need minimize cross-sections pieces domain. course would prefer results relating points convergence global minima training objective. section illustrate diﬃculty establishing results even simplest neural networks. recent years much work devoted providing theoretical explanations empirical success deep neural networks full accounting beyond scope article. order simplify problem many authors studied linear neural networks layers form parameter matrix. multiple layers clearly linear function output parameters. special case piecewise aﬃne functions previous results suﬃce show networks multi-convex functions parameters. proven special case linear autoencoders baldi many authors claimed linear neural networks contain local minima i.e. every local minimum global minimum especially evident study linear autoencoders shown admit many points inﬂection single strict minimum powerful claim apply networks seen practice. this consider dataset consisting three pairs parameterized note dataset zero mean unit variance variable common practice machine learning. however take zero mean variable model shall adopt non-negative. squared error single relu neuron parameterized chosen simplest networks solve local minima closed form show indeed bad. first note continuous piecewise convex function pieces realized dividing plane along line shown ﬁgure pieces relu dead least three data points i.e. pieces least three terms equation constant. remaining terms minimized represented three dashed lines ﬁgure exactly three points lines intersect easily show strict local minima. speciﬁcally point minimizes ﬁrst terms equation minimizes ﬁrst last term. case remaining term constant piece containing point intersection. thus points strict global minima respective pieces strict local minima furthermore compute gives might objected permitted take require variable unit variance. however limits achieved variance tending unity adding instances point dataset. thus even fairly stringent requirements construct dataset yielding arbitrarily local minima parameter space objective value. provides weak justiﬁcation empirical observation success deep learning depends greatly data hand. shown results concerning local minima linear networks extend nonlinear case. ultimately surprise linear networks problem relaxed linear regression convex objective. composition linear layers aa...anx equivalent function matrix previous assumptions problem ﬁnding optimal convex. furthermore easily shown number parameters relaxed problem polynomial number original parameters. since relaxed problem data least well original surprising original problem computationally tractable. simple example merely meant illustrate diﬃculty establishing results every local minimum every neural network. since training certain kind network known np-complete diﬃcult give guarantees worst-case global behavior made claims however probabilistic behavior average practical dataset ruled eﬀects specialized networks deep ones. showed common class neural networks piecewise convex layer parameters ﬁxed. extended theory piecewise multi-convex functions showing training objective function represented ﬁnite number multi-convex functions active multi-convex parameter set. derived various results concerning extrema stationary points piecewise multi-convex functions. established convergence conditions gradient descent iterated convex optimization class functions showing converge piecewise partial minima. similar results likely hold variety optimization witnessed utility multi-convexity proving convergence results various optimization algorithms. however property practical well. better understanding training objective could lead development faster reliable optimization methods heuristic otherwise. results provide insight practical success sub-optimal algorithms neural networks. however also seen local optimality results extend global optimality linear autoencoders. clearly much left discover even optimize deep nonlinear neural networks.", "year": 2016}