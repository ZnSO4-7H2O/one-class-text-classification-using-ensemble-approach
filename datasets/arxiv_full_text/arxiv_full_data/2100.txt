{"title": "GPatt: Fast Multidimensional Pattern Extrapolation with Gaussian  Processes", "tag": ["stat.ML", "cs.AI", "cs.LG", "stat.ME"], "abstract": "Gaussian processes are typically used for smoothing and interpolation on small datasets. We introduce a new Bayesian nonparametric framework -- GPatt -- enabling automatic pattern extrapolation with Gaussian processes on large multidimensional datasets. GPatt unifies and extends highly expressive kernels and fast exact inference techniques. Without human intervention -- no hand crafting of kernel features, and no sophisticated initialisation procedures -- we show that GPatt can solve large scale pattern extrapolation, inpainting, and kernel discovery problems, including a problem with 383400 training points. We find that GPatt significantly outperforms popular alternative scalable Gaussian process methods in speed and accuracy. Moreover, we discover profound differences between each of these methods, suggesting expressive kernels, nonparametric representations, and exact inference are useful for modelling large scale multidimensional patterns.", "text": "gaussian processes typically used smoothing interpolation small datasets. introduce bayesian nonparametric framework gpatt enabling automatic pattern extrapolation gaussian processes large multidimensional datasets. gpatt uniﬁes extends highly expressive kernels fast exact inference techniques. without human intervention hand crafting kernel features sophisticated initialisation procedures show gpatt solve large scale pattern extrapolation inpainting kernel discovery problems including problem training points. gpatt signiﬁcantly outperforms popular alternative scalable gaussian process methods speed accuracy. moreover discover profound differences methods suggesting expressive kernels nonparametric representations exact inference useful modelling large scale multidimensional patterns. future human enterprise well depend data exclaimed west writing scientiﬁc american. indeed quickly entered data focussing recent machine learning efforts developing scalable models large datasets notable results deep neural architectures neural networks ﬁrst became popular allowed adaptive basis functions opposed ﬁxed basis functions well known linear models. adaptive basis functions neural networks could automatically discover interesting structure data retaining scalable learning procedures newfound expressive power came cost interpretability lack principled framework deciding upon network architecture activation functions following neural networks came kernel inﬁnitely many ﬁxed basis functions used ﬁnite computational resources kernel trick implicitly representing inner products basis functions using kernel. kernel methods ﬂexible often interpretable manageable neural network models. example gaussian processes used rich prior distributions functions properties smoothness periodicity etc. controlled interpretable covariance kernel. indeed gaussian processes success challenging non-linear regression classiﬁcation problems within machine learning community gaussian process research developed neural networks research. neal argued since typically improve performance model accounting additional structure data ought pursue limits large models. accordingly neal showed bayesian neural networks become bayesian nonparametric gaussian processes neural network kernel number hidden units approach inﬁnity. thus gaussian processes nonparametric kernel machines part natural progression ﬂexibility dataset automatically calibrated complexity easy interpretable model speciﬁcation covariance kernels principled probabilistic framework learning kernel hyperparameters. however kernel machines like gaussian processes typically unable scale large modern datasets. methods improve scalability usually involve simplifying assumptions ﬁnite basis function expansions sparse approximations using pseudo inputs methods promising simplify standard gaussian process models sometimes already simple particularly large number training instances available learn sophisticated structure data. gpatt signiﬁcantly outperforms popular alternative gaussian process methods speed accuracy stress tests. furthermore discover profound behavioural differences methods suggesting expressive kernels nonparametric representations exact inference used together useful large scale multidimensional pattern extrapolation. indeed popular covariance kernels used gaussian processes often expressive enough capture rich structure data perform extrapolation prompting mackay whether thrown baby bathwater. general choice kernel profoundly affects performance gaussian process much choice architecture affects performance neural network. typically gaussian processes used either ﬂexible statistical tools human manually discovers structure data hard codes structure kernel popular gaussian mat´ern kernels. either case gaussian processes used smoothing interpolators able discover limited covariance structure. likewise multiple kernel learning typically involves hand crafting combinations gaussian kernels specialized applications modelling dimensional structure high dimensional data intended automatic pattern discovery extrapolation. paper propose scalable expressive gaussian process framework gpatt automatic pattern discovery extrapolation large multidimensional datasets. begin section brief introduction gaussian processes. section introduce expressive interpretable kernels build recent kernels wilson adams especially structured multidimensional inputs fast exact inference learning techniques later introduce section inference techniques work exploiting existing structure kernels section also work popular alternative kernels. techniques relate recent inference methods saatchi relax full grid assumption made methods. exact inference learning costs computations storage datapoints input dimensions compared standard computations storage associated cholesky decomposition. experiments section combine fast inference techniques expressive kernels form gpatt. experiments emphasize that although gaussian processes typically used smoothing interpolation small datasets gaussian process models fact developed automatically solve variety practically important large scale pattern extrapolation problems. gpatt able discover underlying structure image extrapolate structure across large distances without human intervention hand crafting kernel features sophisticated initialisation exposure similar images. gpatt reconstruct large missing regions pattern images restore stained image reconstruct natural scene removing obstructions discover sophisticated ground truth kernel movie data. gpatt leverages large number training instances scale mixtures gaussians centred origin generally model would density estimation. scale-location mixtures gaussians however approximate distribution arbitrary precision enough components even small number components highly ﬂexible models. noting spectral densities real data must symmetric assuming therefore also take inverse fourier transform spectral density equation analytically obtain corresponding spectral mixture kernel function derived wilson adams applied solely simple time series examples small number datapoints. extend formulation tractability large datasets multidimensional inputs. many popular stationary kernels multidimensional inputs decompose product across input dimensions. decomposition helps computational tractability limiting number hyperparameters model like stationarity provides restriction bias help learning. higher dimensional inputs propose leverage useful product assumption inherent many popular kernels spectral mixture product kernel component hyperparameters spectral mixture kernel product {θp}p hyperparameters kernel. enough components kernel model product kernel arbitrary precision ﬂexible even small number components. smp-a shorthand kernel components dimension marginal likelihood pleasingly compartmentalises automatically calibrated model complexity terms optimized learn kernel hyperparameters used integrate using mcmc problem model selection learning gaussian processes exactly problem ﬁnding suitable properties covariance function. note gives model data characteristics interpret. heart gaussian process model kernel encodes inductive biases sorts functions likely model. popular kernels often expressive enough automatic pattern discovery extrapolation. learn rich structure data present highly expressive kernels combine scalable exact inference procedures introduce section general difﬁcult learn covariance structure single gaussian process realisation assumptions. popular kernels including gaussian mat´ern γ-exponential rational quadratic kernels assume stationarity meaning invariant translations input space words stationary kernel function pair inputs bochner’s theorem shows stationary kernel spectral density fourier duals therefore approximate arbitrary accuracy also approximate stationary kernel arbitrary accuracy intuition spectral densities stationary kernels. example fourier transform popular kernel gaussian centred origin. likewise fourier transform mat´ern kernel distribution centred origin. results provide intuition arbitrary additive compositions popular kernels limited expressive power equivalent density estimation with e.g. imaginary observations corrupting effect inference moments resulting predictive distribution exactly standard predictive distribution e.g. preconditioned conjugate gradients preconditioning matrix solve preconditioning matrix speeds convergence ignoring imaginary observations exploiting fast multiplication kronecker matrices takes total operations compute learning must evaluate marginal likelihood cannot efﬁciently decompose compute complexity penalty marginal likelihood kronecker matrix since incomplete grid. approximate complexity penalty gaussian process inference learning requires evaluating covariance matrix vector datapoints noise variance equations respectively. purpose standard practice take cholesky decomposition requires computations storage dataset size however nearly kernel imposes signiﬁcant structure ignored taking cholesky decomposition. show exploit structure perform exact inference hyperparameter learning storage operations compared standard storage operations. ﬁrst assume inputs multidimensional grid meaning relax grid assumption section using approximate eigenvalues eigenvalues particularly good approximation large emphasize determinant term marginal likelihood undergoes small approximation inference remains exact. remaining terms marginal likelihood computed exactly efﬁciently using pcg. total runtime cost hyperparameter learning exact inference incomplete grid thus fast exact inference learning procedures section method henceforth call gpatt perform extrapolation variety sophisticated patterns embedded large datasets. contrast gpatt many alternative gaussian process kernel methods. particular compare recent sparse spectrum gaussian process regression method provides fast ﬂexible kernel learning. ssgp models kernel spectrum point masses ssgp ﬁnite basis function model many basis functions spectral point masses. ssgp similar recent models rahimi recht except learns locations point masses marginal likelihood optimization. ssgp implementation provided authors http//www.tsc.ucm.es/ miguel/downloads.php. test importance fast inference used gpatt compare uses kernel section popular fast fitc inference implemented gpml. also compare gaussian processes popular squared exponential rational quadratic mat´ern kernels catalogued rasmussen williams respectively smooth multi-scale ﬁnitely differentiable functions. since gaussian processes kernels cannot scale large datasets consider combine kernels fast inference techniques gpatt enable comparison. write gpatt-a gpatt uses smp-a kernel. http/www.gaussianprocess.org/gpml also considered model duvenaud model intractable datasets considered structured fast inference section experiments assume gaussian noise express likelihood data solely function kernel hyperparameters learn optimize marginal likelihood using bfgs. simple initialisation scheme frequencies {µa} drawn uniform distribution nyquist frequency length-scales {/σa} truncated gaussian distribution mean proportional range data weights {wa} initialised empirical standard deviation data divided number components used model. general gpatt robust initialisation. extrapolate missing region shown figure real metal tread plate texture. training instances test instances inputs pixel locations outputs pixel intensities. full pattern shown figure texture contains shadows subtle irregularities identical diagonal markings patterns correlations across input dimensions. reconstruct missing region well training region gpatt components kernel dimension gpatt reconstruction shown figure plausible true full pattern shown figure largely automatic. without human intervention hand crafting kernel features suit image sophisticated initialisation exposure similar images gpatt discovered underlying structure image extrapolated structure across large missing region even though structure pattern independent across spatial input dimensions. indeed separability kernel represents soft prior assumption rule posterior correlations input dimensions. reconstruction figure produced ssgp using basis functions. principle ssgp model spectral density inﬁnitely many components however since components point masses component highly limited expressive power. moreover many components ssgp experiences practical difﬁculties regarding initialisation overfigure automatic model selection gpatt. initial learned weight frequency parameters gpatt- input dimension metal tread plate pattern figure gpatt- overspeciﬁed pattern. training weights extraneous components automatically shrink zero helps indicate whether model overspeciﬁed helps mitigate effects model overspeciﬁcation. initial components dimension near zero training. note smp- kernel used gpatt components needed problem. however shown figure model overspeciﬁed complexity penalty marginal likelihood shrinks weights extraneous components proxy model selection effect similar automatic relevance determination complexity penalty written eigenvalues covariance matrix components signiﬁcantly contribute model therefore automatically pruned shrinking weights decreases eigenvalues thus minimizes complexity penalty. weight shrinking helps mitigate effects model overspeciﬁcation helps indicate whether model overspeciﬁed. following stress tests gpatt scales efﬁciently number components kernel. stress test gpatt alternative methods terms speed accuracy varying datasizes extrapolation ranges basis functions pseudo inputs components. assess accuracy using standardised mean square error mean standardized loss deﬁned rasmussen williams page using empirical mean variance data would give smse msll respectively. smaller smse negative msll values correspond better data. runtime stress test figure shows number components used gpatt signiﬁcantly affect runtime gpatt much faster fitc ssgp even components slope curve roughly indicates figure extrapolation metal tread plate pattern. missing data shown black. training region testing region full tread plate pattern gpatt ssgp basis functions fitc pseudo inputs smp- kernel fast exact inference section squared exponential mat´ern rational quadratic kernels. ﬁtting computation time although ssgp discover interesting structure equal training test performance unable capture enough information convincing reconstruction basis functions improved performance. likewise fitc smp- kernel pseudo-inputs cannot capture necessary information interpolate extrapolate. note fitc ssgp- respectively took days hour example compared gpatt took minutes. kernels truly bayesian nonparametric models kernels derived inﬁnite basis function expansions. therefore seen figure methods completely able capture information training region; however kernels proper structure reasonably extrapolate across missing region simply smoothing ﬁlters. note comparison possible using fast exact inference techniques section table compare test performance gpatt- ssgp using squared exponential mat´ern rational quadratic kernels combined inference section patterns train test split metal treadplate pattern figure figure stress tests. runtime stress test. show runtimes seconds function training instances evaluating marginal likelihood relevant derivatives standard kernel fitc pseudo-inputs smp- smp- kernels ssgp basis functions gpatt- gpatt- gpatt-. runtimes intel processor cone pattern shown appendix. ratio training inputs imaginary training inputs gpatt smallest training sizes training sets. accuracy stress test. msll function holesize metal pattern figure values horizontal axis represent fraction missing data full pattern compare gpatt- gpatt- kernels ssgp basis functions. msll gpatt- holesize asymptotic scaling method. experiment standard slope close cubic scaling expect. curves slope indicating linear scaling number training instances. however fitc ssgp used ﬁxed number pseudo inputs basis functions. pseudo inputs basis functions used training instances methods scale quadratically pseudo inputs basis functions ﬁxed number training instances. gpatt hand scale linearly runtime function training size without deterioration performance. furthermore gaps curve ﬁxed orders magnitude gpatt outperforms alternatives practically important asymptotic scaling. accuracy stress test figure shows extrapolation performance metal tread plate pattern figure varying holesizes running missing data testing kernels steadily increase error function holesize. conversely ssgp increase error function holesize ﬁnite basis functions ssgp cannot extract much information larger datasets alternatives. gpatt performs well relative methods even small number components. gpatt particularly able exploit extra information additional training instances holesize large data missing gpatt’s performance degrade level alternative methods. table compare test performance gpatt ssgp using kernels extrapolating different patterns train test split tread plate pattern figure patterns shown appendix. gpatt consistently lowest standardized mean squared error mean standardized loss note many datasets sophisticated patterns containing intricate details subtleties strictly periodic lighting irregularities metal impurities etc. indeed ssgp periodic kernel capable modelling multiple periodic components perform well gpatt examples. section particularly large example gpatt- perform learning exact inference pores pattern training points extrapolate large missing region test points. smse total runtime secfigure recovering sophisticated product kernels. product three kernels used generate movie training points. data gpatt- reconstructs component kernels kernels function clarity presentation kernel scaled relatively small number components gpatt able accurately recover wide range product kernels. test gpatt’s ability recover ground truth kernels simulate movie data using kernel kper kper kper kper kse. exp/] consecutive slices testing leaving large number training points. case datasize helpful training instances information learn true generating kernels. moreover gpatt- able reconstruct complex class kernels minutes. compare learned kernel true generating kernels figure appendix show true predicted frames movie. ﬁrst consider wallpaper image stained black apple mark shown ﬁrst figure remove stain apply mask separate image three channels results pixels channel training. channel gpatt using smp-. combined results channel restore image without stain particularly impressive given subtleties pattern lighting. figure image inpainting gpatt. left right mask applied original image gpatt extrapolates mask region three image channels results joined produce restored image. removing stain bottom removing rooftop restore natural scene coast masked training attempt extrapolate testing. obscured prominent rooftop shown second figure applying mask following procedure stain time pixels channel training gpatt reconstructs scene without rooftop. reconstruction captures subtle details waves ocean even though image used training. fact example used inpainting algorithms given access repository thousands similar images results emphasized conventional inpainting algorithms gpatt profoundly different objectives sometimes even cross purposes inpainting attempts make image look good human placed boats water) gpatt general purpose regression algorithm simply aims make accurate predictions test input locations training data alone. gaussian processes often used smoothing interpolation small datasets. however believe bayesian nonparametric models naturally suited pattern extrapolation large multidimensional datasets extra training instances provide extra opportunities learn additional structure data. support inductive biases gaussian process naturally encoded covariance kernel. covariance kernel must always structure reﬂect inductive biases; structure principle exploited scalable exact inference without need simplifying approximations. models could play role machine learning models expressive scalable also interpretable manageable simple exact learning inference procedures. hope make small step direction gpatt gaussian process based bayesian nonparametric framework automatic pattern discovery large multidimensional datasets scalable exact inference procedures. without human intervention sophisticated initialisation hand crafting kernel features gpatt used accurately quickly extrapolate large missing regions variety patterns. acknowledgements thank richard turner ryan prescott adams zoubin ghahramani carl edward rasmussen helpful discussions. duvenaud lloyd j.r. grosse tenenbaum j.b. ghahramani structure discovery nonparametric regresinternational sion compositional kernel search. conference machine learning kostantinos gaussian mixtures applications signal processing. advanced signal processing handbook theory implementation radar sonar medical imaging real time systems l´azaro-gredilla qui˜nonero-candela rasmussen c.e. figueiras-vidal a.r. sparse spectrum gaussian process regression. journal machine learning research provide detail eigendecomposition kronecker matrices runtime complexity kronecker matrix vector products. also provide spectral images learned kernels metal tread plate experiment section larger versions images table images extrapolation results large pore example images gpatt reconstruction several consecutive movie frames. inputs multidimensional grid covariance matrix decomposes kronecker product matrices input dimension eigendecomposition similarly decomposes covariance matrix kronecker product entries decomposed operator returns r-by-c matrix whose elements taken column-wise operator stacks matrix columns onto single vector reshape vec− operator deﬁned vec−) finally using standard kronecker property vec) note argument vector exact result. words performing inference given observations give result directly using observations proof predictive covariances remain unchanged proceeds similarly. gain insight behavior gpatt looking spectral density learned spectral mixture kernel. figure shows spectrum representations learned kernels section smoothers popular kernels concentrate spectral energy around origin differing tail support higher frequencies. methods used kernel gpatt fitc able learn meaningful features spectrum space. rest supplementary material provide images results referenced main text. figure illustrates images used stress tests section figure provide results large pore example. finally figure shows true predicted movie frames discussed section figure spectral representation learned kernels section methods used kernel gpatt fitc) plot analytical spectrum using learned hyperparameters. c)squared exponential rational quadratic mat´ern- plot instead empirical spectrum using fast fourier transform kernel. figure images used stress tests section figures show textures used accuracy comparison table figure cone image used runtime analysis shown figure figure using gpatt recover consecutive slices movie. slices missing training data true slices take middle movie. bottom inferred slices using gpatt-.", "year": 2013}