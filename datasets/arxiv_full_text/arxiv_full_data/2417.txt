{"title": "A compact, hierarchical Q-function decomposition", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Previous work in hierarchical reinforcement learning has faced a dilemma: either ignore the values of different possible exit states from a subroutine, thereby risking suboptimal behavior, or represent those values explicitly thereby incurring a possibly large representation cost because exit values refer to nonlocal aspects of the world (i.e., all subsequent rewards). This paper shows that, in many cases, one can avoid both of these problems. The solution is based on recursively decomposing the exit value function in terms of Q-functions at higher levels of the hierarchy. This leads to an intuitively appealing runtime architecture in which a parent subroutine passes to its child a value function on the exit states and the child reasons about how its choices affect the exit value. We also identify structural conditions on the value function and transition distributions that allow much more concise representations of exit state distributions, leading to further state abstraction. In essence, the only variables whose exit values need be considered are those that the parent cares about and the child affects. We demonstrate the utility of our algorithms on a series of increasingly complex environments.", "text": "previous work hierarchical reinforcement learning faced dilemma either ignore values different possible exit states subroutine thereby risking suboptimal behavior represent values explicitly thereby incurring possibly large representation cost because exit values refer nonlocal aspects world paper shows that many cases avoid problems. solution based recursively decomposing exit value function terms q-functions higher levels hierarchy. leads intuitively appealing runtime architecture parent subroutine passes child value function exit states child reasons choices affect exit value. also identify structural conditions value function transition distributions allow much concise representations exit state distributions leading state abstraction. essence variables whose exit values need considered parent cares child affects. demonstrate utility algorithms series increasingly complex environments. hierarchical reinforcement learning aims speed providing prior knowledge form hierarchical constraints policies paper consider policies constrained follow partial program whose choice points designate places policy unspeciﬁed. learning agent equipped partial program aims best possible completion given experiences running program mdp. fundamental result combination partial program yields semimarkov decision process whose states joint states—pairs environment states program states. algorithms solve smdp; here consider algorithms learn function i.e. expected rewards taking action joint state acting optimally thereafter. deﬁned small state variables. standard forms q-decomposition known. maxq approximates expected rewards obtained executing expected rewards current subroutine completes. alisp decomposition includes third term cover rewards obtained subroutine exits. decompositions lead different notions optimal completion. maxq yields recursive optimality—i.e. policy within subroutine optimized ignoring calling context. alisp yields hierarchical optimality—i.e. optimality among policies consistent partial program. recursively optimal policies worse hierarchically optimal ones context relevant ignore exit values hand alisp need samples learn accurately. also argued that whereas represent sums rewards local given subroutine hence likely well approximated low-dimensional function represents non-local rewards therefore difﬁcult learn. thus seem dilemma ignore risk seriously suboptimal behavior include incur potentially high learning cost. function terms q-functions higher levels hierarchy. algorithms learning execution based decomposition presented. section show idea additive irrelevance used dramatically simplify representation based conditional independencies hold domain. finally section describe general simpliﬁcation depends size interface subroutine context. utility successive decomposition demonstrated experimentally. section describe main illustrative example. begin simple version environment made complex course paper introduce various components approach. markov decision processes modelling framework assume state represented instantiation state variables. example based example taxi models taxi moving around serving passengers grid. states consist taxi’s position passengers source destination generosity level status actions available taxi moves four cardinal directions fail probability; pickup picks passenger taxi’s current position assuming taxi doesn’t already passenger; dropoff drops passenger currently taxi assuming taxi destination. upon successful dropoff action probability episode terminates otherwise passengers reset using distribution passenger’s source destination generosity ﬁxed cost charged time step reward depends passenger’s generosity given successful dropoff. even simple domain optimal policy completely clear. example deciding passenger requires trading time expected reward serving passenger centrally placed taxi serving passenger. alisp language specifying constraints policies using partial programs. provide brief overview alisp here. complete description found literature figure alisp program basic taxi domain. alispspeciﬁc statements bold. second argument call with-choice action statements label. functions waiting-passengers pass-being-served pass-src pass-dest taxi-pos shown here; simply extract required components environment state tion taxi-main says taxi must repeatedly serve passenger episode terminates. serving passenger consists choosing passenger get-pass followed put-pass. get-pass subroutine consists navigating passenger’s source pickup action environment. similarly put-pass consists navigating passenger’s destination dropoff. subroutine involves repeatedly choosing direction move goal reached. choices passenger serve directions move navigating speciﬁed program left open using with-choice command. means learning algorithm learn completion speciﬁes make choices optimally function state. consider partial program executed environment. point denote state environment denote machine state i.e. program counter memory runtime stack. deﬁne joint state joint states program counter choice statement. formally completion function ranges choices available basic theorem hierarchical states given partial program case deﬁne value function expected total reward gained environment start continue partial program making choices using interested ﬁnding hierarchically optimal must satisfy deﬁne expected total reward gained begin making choice continue partial program making choices using paper always work undiscounted value q-functions restrict attention completions eventually terminate probability virtues hierarchical structure partial program yields additive decomposition q-function. consider navigation choice trajectory shown figure expected reward future trajectories. written expected reward expected reward exiting current subroutine expected reward gained exiting subroutine. motivation components might amenable state abstraction. reward moving step constant reward reaching current goal depends taxi’s position destination. neither components depends passenger’s destination exist algorithms learning learning three components separately note behaviour runtime agent given learnt q-function conceptually simple—at state pick maximize problem -part decomposition component often depend many variables. components local i.e. refer total rewards gained within subroutine whereas measures future rewards received current subroutine. navigation choice previous example includes rewards gained dropping current passenger serving future passengers. scale domains huge numbers variables typically depend them. thus number parameters learn scale exponentially number state variables. goal avoid represent learn preserve possibility hierarchically optimal behaviour. begin expressing terms functions higher levels hierarchy. given subroutine state choice occurring call stack write probability exiting state given choose follow also write special case occurs itself leave superscript refer conditional distribution exit distribution write similarly then derivation mirrors structure program figure note ﬁgure depicts possible trajectory program; trajectories different call structures. nevertheless trajectories passing contain exit states states referred following theorem generalizes derivation. theorem joint state alisp program stack given choice completion deﬁne random variables exit state reached choosing following thereafter. then might seem like exchanged problem another since instead learn exit probability distributions fact individual exit distributions often structure determinism conditional context-sensitive independencies might known beforehand. example figure execution trajectory partial program example circles represent successive joint states. vertical position circle depends depth stack choices/calls state. exception leftmost state call statement taxi-main circles labelled label corresponding choice action statement. example joint state program counter points choice statement label navigation choice figure deterministically leads state taxi passenger location deterministically leads state passenger taxi. also either results termination keeps taxi current location reinitializes passenger independently quantities need estimated termination probability distribution individual passenger’s source destination generosity. algorithm functions computing runtime. initialize called overall alisp interpreter starting program. callsub called subroutine called. qstack global stack maintained interpreter. stack popped leaving subroutine. decisions made maximizing q-function stack. operations expectation getvaluefn assumed provided blackboxes. algorithm maintains stack q-functions parallel call stack. course really q-function point q-function stack particular subroutine compact representation valid states encountered subroutine. moreover operations within algorithm expectation operate directly representations. example might decision trees qfunctions dbns exit distributions methods operate efﬁciently. calling subroutine exit value function subroutine computed using parent’s q-function. combined child’s exit distribution representation q-function child added stack. decisions within subroutine made maximizing q-function. intuitive description overall procedure subroutine passes exit value function children compute q-function distribution next choice state level encountered choosing choice states subroutine therefore modify original hordq algorithm hierarchically optimal cascaded q-learning unique i.e. depend choice made maxu maxu condition applies navigation choice since given joint state arising subroutine possible exit state namely taxi navigation goal variables unchanged. thus need even compute state. hand condition hold passenger choice since choice passenger affects taxi position dropoff. practice unique exit condition rarely hold. even navigation choice adding fuel variable would destroy since navigation choices would affect amount fuel exiting. thus look general apply lemma motivating example. example consider modifying example adding high-dimensional variable represents state trafﬁc roads. evolves according transition distribution independently rest state time passenger served. further whenever statistic exceeds threshold constant toll charged passenger since unique exit condition hold would seem must learn includes exit distribution note however actually affect exit distribution also thought shows exit value function decomposed pass source destination generosity passengers consists expected future tolls includes rewards. since depends contribution constant respect therefore ignored. need compute expected exit value requires knowing variables decoupled conditionally inchoice respect dependent given distribution call variable variables coupled member decoupled set. typically take exit distribution. example decoupled choice respect incorporate additive irrelevance recursive decomposition need deﬁnition applies entire subroutines. deﬁnition variables decoupled subroutine occurring descendants completion within decoupled choice respect deﬁnition triple satisﬁes factored exit condition respect subroutine function figure compares performance hocq hordq recursively optimal counterpart rordq example somewhat surprisingly even without state abstraction hocq slight edge hordq. note also rordq converges slightly suboptimal policy previous section showed replace local quantity still making full locality. describes exit distribution state variables even connection going subroutine. seems unnecessary. example stock-owning taxi driver shouldn’t condition low-level navigation decisions unrelated variables current state nasdaq exchange even variable affect exit value. section formalize intuitions. begin obvious useful fact. lemma maxu maxu consequence fact exit distribution current subroutine state r\\d|ω straightp factored exit triple blackbox function often easy write structured representation information subroutine empty reduce identity function. thus using factored exit condition speed things runtime since compute expectations reduced exit value function. also improvement learning; need learn exit distributions relevant variables subroutine also make factorization figure picture separator dashed surrounds part distribution used runtime subroutine containing compute expected value part outside used parent compute suppose passenger choice example choosing passengers take longer serve generous. since trafﬁc evolves timestep exit value coupled choice passenger useful application theorem example consider environment example modiﬁed squares designated cliffs. moving onto squares results large negative reward immediate termination episode. correctness partial program also modiﬁed terminate cliff. suppose passenger choice example boolean variable true taxi moved cliff square. then assuming moves noisy coupled choice passenger; example picking passenger region many cliff squares would carry high risk accidentally entering them. longer possible nontrivial additive decomposition exit value function components depend intuitively deciding stingy passenger plains generous mountains optimal decision depends much future expected reward lost second case possibility falling cliff. cannot ignore portion exit value function. commonality last examples existence small variables makes exit values variables relevant either making coupled current choice ﬁrst example jointly participating value function component second case. formalize idea. imagine subroutines separate entities subroutine knows direct effects actions local rewards exit values subset variables. thus serve subroutine knows choice passenger affects rewards within subroutine exit distribution xy}. given knowledge additional information needed make optimal decisions answer need know projection exit value function onto variables separator using known distribution intuitively parent computes passes child uses together separator variables compute shown figure general deﬁnition suppose separator function exit states depends light lemma modify algorithm take separators account shown algorithm algorithm takes additional input sets pair subroutines call separator within exit value function depends learning algorithm must also modiﬁed separately learn distribution trafﬁc evolves time exit distribution separator variables always easy abstract last examples. example consider combination examples trafﬁc evolves every timestep cliffs. suppose navigation choice example trafﬁc threshold beyond tolls charged. steps reach fork road must choose paths. ﬁrst path safe slow fast goes mountainous regions. suppose rewards trafﬁc doesn’t change much reach fork make sense choose slow route trafﬁc level rises sufﬁciently period make sense risk fast route increase chance avoiding toll. trafﬁc making future choice turn depends current trafﬁc. somewhat surprisingly exit distribution optimal completion depends current trafﬁc even though trafﬁc direct effect events within subroutine problem separated variables nevertheless affect future decisions made subroutine quantity relevant predicting future values variables becomes relevant exit distribution separator variables. unfortunately seem possible handle cases exactly keeping exit distribution compact. example could probably ignore trafﬁc without much harm general approximation would systematically underestimate exit value would based assumption future decisions made using unsafe abstraction. alternative show level structure exit distribution allows principled approximations. dependence exit distribution separator variables current value separated variables arbitrary; happens separated variables’ effect exit value function therefore local policy future choices subroutine. deﬁnition completion instantiation variables local policy deﬁned suppose state subroutine decoupled root subroutine program. trajectory states corresponding random sequence local poli. note cies independent given rewrite second component decomposition exit distribution separator variables directly related task performed subroutine. passenger choice example component exit distribution time position. assuming deterministic moves completion follows shortest path towards goal distribution depends current location passenger’s source destination. passenger choice example second component exit distribution i.e. distribution taxi position whether taxi falls cliff subroutine. again depends taxi’s location passenger’s source destination. figure measure effect using separator decomposition example trafﬁc variable integer made follow random walk slight upward drift time. toll proportional square trafﬁc. without separator conditions algorithm must directly learn exit distribution trafﬁc function passenger choice learns quite slowly. separator conditions used algorithm take advantage abstractions described above learns much faster. quest simplicity replaced large ﬁnite observable object unbounded unobservable might seem like progress. fact never represent explicitly since serves hidden mixture component. also claim exposes numerical structure function approximators provides useful hook specifying prior knowledge. inﬁnite mixture often well approximated small ﬁnite components reduced model corresponds event hurry reach upcoming fork. addition work mentioned introduction many bodies research explored ideas decomposition value functions exit distributions hierarchical reinforcement learning. based idea dividing state space regions communicate small sets interface states solving subproblems separately combining solutions somehow. papers mainly utilize structure state-transition graph deal planning given model. contrast focused viewing states factored variables structure within factored representation lead representational statistical beneﬁts learning. interesting question whether factored structure also algorithmic beneﬁts i.e. used conjunction methods aforementioned papers yield efﬁcient planning algorithms? note also notion local policies reminiscent policy-cache idea might possible methods policy-cache construction therein approximate mixture representation exit distribution section recent paper studies oversubscription planning problem given tasks consume resources must complete many possible given global resource constraints. framework viewed alisp program top-level loop chooses next task task subroutine chooses actions task aborting. state factored global variables variables local task task indicator whether task completed. terminology engaged particular task exit values resource amounts completion indicator current task separators remaining variables. paper shows reset assumption states task aborted completion local variables revert initial state dynamic programming algorithm alternates between planning top-level planning subtasks options framework models expected behavior options created somewhat similar notion work. difference work options thus usually assumed low-level options speciﬁed beforehand whereas interested problem simultaneously planning high levels. primary contributions paper present method conditions hierarchically optimal solutions obtained without large representational parameter learning costs. unlike previous work reasons explicitly exit states subroutines method generalizes easily arbitrarily deep hierarchies captures notion parent passes child subroutine exit value function concise possible. keep presentation focused discussed exact decomposition q-function paper. practice approximate versions factored exit separator conditions required components decompositions need approximated. applying algorithms larger domains requiring facing issues. work step toward full understanding structure value functions. link structure behavioural hierarchy complexity decision making needs investigated further. dean s.-h. lin. decomposition techniques planning stochastic domains. ijcai hierarchical reinforcement learning maxq value function decomposition. jair", "year": 2012}