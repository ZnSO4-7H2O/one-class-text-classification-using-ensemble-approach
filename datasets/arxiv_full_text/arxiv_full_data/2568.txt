{"title": "Toward a general, scaleable framework for Bayesian teaching with  applications to topic models", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Machines, not humans, are the world's dominant knowledge accumulators but humans remain the dominant decision makers. Interpreting and disseminating the knowledge accumulated by machines requires expertise, time, and is prone to failure. The problem of how best to convey accumulated knowledge from computers to humans is a critical bottleneck in the broader application of machine learning. We propose an approach based on human teaching where the problem is formalized as selecting a small subset of the data that will, with high probability, lead the human user to the correct inference. This approach, though successful for modeling human learning in simple laboratory experiments, has failed to achieve broader relevance due to challenges in formulating general and scalable algorithms. We propose general-purpose teaching via pseudo-marginal sampling and demonstrate the algorithm by teaching topic models. Simulation results show our sampling-based approach: effectively approximates the probability where ground-truth is possible via enumeration, results in data that are markedly different from those expected by random sampling, and speeds learning especially for small amounts of data. Application to movie synopsis data illustrates differences between teaching and random sampling for teaching distributions and specific topics, and demonstrates gains in scalability and applicability to real-world problems.", "text": "data—they teach. purposeful selection data plays featured role theories cognition cognitive development culture cases teaching conceived purposeful rather random selection small examples goal facilitating accurate inferences body knowledge. question model teaching also appeared across literatures cognitive scientists proposed investigated probabilistic models optimal example selection algorithmic teaching researchers proposed deterministic methods select examples rule confusable concepts machine learning researchers become interested machine teaching paths research vary details share goal facilitate learning optimizing input learners. previous approaches either tailored speciﬁc circumscribed domains propose general mathematical formalization scalable. offers formalization exponential family distributions cast teaching bilevel optimization np-hard offer solution limited case data differentiable learner’s objective function convex regular. shafto goodman zilles offer general formalizations probabilistic deterministic inference respectively suffer computational complexity renders inapplicable real-world application. propose simple general framework selecting examples teach probabilistic learners. framework leverages advances sampling-based approximations specifically pseudo-marginal sampling coupled importance sampling offer general purpose approximation bayesian normalizing constant required teach probabilistic learners. demonstrate efﬁcacy approach probabilistic topic models. results show distribution teaching data differ signiﬁcantly data likelihood teaching data improve learning especially learning small numbers examples teaching optimized variety tasks scaling problems markedly complex possible previous general-purpose teaching algorithms. machines humans world’s dominant knowledge accumulators humans remain dominant decision makers. interpreting disseminating knowledge accumulated machines requires expertise time prone failure. problem best convey accumulated knowledge computers humans critical bottleneck broader application machine learning. propose approach based human teaching problem formalized selecting small subset data will high probability lead human user correct inference. approach though successful modeling human learning simple laboratory experiments failed achieve broader relevance challenges formulating general scalable algorithms. propose generalpurpose teaching pseudo-marginal sampling demonstrate algorithm teaching topic models. simulation results show samplingbased approach effectively approximates probability ground-truth possible enumeration results data markedly different expected random sampling speeds learning especially small amounts data. application movie synopsis data illustrates differences teaching random sampling teaching distributions speciﬁc topics demonstrates gains scalability applicability real-world problems. introduction machines increasingly integral society. expert human intuition algorithms increasingly present. learning algorithms used learn complex hypotheses complex data used augment human intuition. paradigm creates bottleneck knowledge accumulated machines depends highly trained human experts interpret conveying humans. remains barrier broader usefulness machine learning. machines communicate perfectly among exchanging bits humans communicate allows deﬁne efﬁcient gibbs sampler requires maintaining counts probability word assigned speciﬁc topic given words documents assignment words teaching topics documents goal produce choose documents teach topic model learner. learner assumed know prior parameters number topics; must marginalize possible assignments words documents topics possible probability documents teaching model importance sampling common approach estimating intractable integrals re-framing expectations respect easy-to-sample-from importance distribution integral estimated simulating taking arithmetic mean importance weights p/q. example estimate marginal likelihood estimating marginal likelihood importance distribution close posterior areas high posterior density contribute more. case naive approach drawing uniform categorical distribution inefﬁcient afford scaling real-world problems; many importance samples come probability regions importance distribution ﬂatter target. better approach collapsed gibbs sampler equation implement sequential importance sampler proposals drawn incrementally generating starting random value draw background bayesian teaching teacher generates data lead learner speciﬁc hypothesis teacher must consider learner’s posterior inference given every possible choice data thus learning sub-problem teaching. teacher simulates learner considers probabilities hypotheses given data topic models latent dirichlet allocation latent dirichlet allocation popular formalization topic models. documents bags words generated mixtures ﬁxed number topics. generate documents topics distinct words number words document topic word document generated word document variables interest topics topic mixture weights θd}. vector entry words vocabulary; entry probability word occurring topic. document associated entry probability word document generated topic figure comparison teaching probability approximations. panels show true teaching probabilities plotted approximations uniform importance sampling sequential importance sampling approximation calculated using total samples. point plot represents exact approximate probability pair sampled documents words generated three topics ﬁve-word vocabulary panel shows bias estimators. panel shows effective sample sizes estimators. posterior within acceptable error increases. simulations evaluate number samples required achieve relative sample error calculating marginal likelihood different values relative error importance samples importance weight. figure displays results averaged runs. teaching probability single sixty-word document twenty-topic model calculated within relative sample error thousand samples. number samples required increases linearly number words. values effect scaling. model becomes sparser smaller proportion assignments hold probability mass makes high-mass areas difﬁcult sampler thus samples required sparser models. performs signiﬁcantly better naive importance sampling especially sparser models evaluated accuracy efﬁciency uniform sequential importance sampling comparing estimates true teaching probabilities. generated pairs documents threetopic models document contained words ﬁve-word vocabulary. given parameters number terms equation figure shows accuracy samplers indicates much lower variability. figure compares effective sample size samplers. represents number unweighted samples weighted samples equivalent—higher better. calculated varq sample variance importance weights. simulations average uniform importance sampling pseudo-marginal sampling generate documents teaching distribution employ pseudo-marginal sampling allows exact metropolis-hasting performed using approximated functions. standard metropolis-hasting algorithm generates samples probability distribution known constant drawing values proposal distribution accepting probability where difﬁcult calculate pseudo-marginal approach allows replace equation approximation pseudo-marginal sampling works provided bias weight approximation expected value weights constant. weights implicitly treated random variable joint distribution marginalized away leaving target. pseudomarginal sampling coupled generate documents teaching model; need generate documents approximation. scalability posterior approximation main factor scaling bayesian teaching. topic models chosen approximation method requires arithmetic operations multinomial random numbers shown yield high ess. sample generated independently importance sampling embarrassingly parallel. however number words topics increases number samples needed approximate used pseudo-marginal metropolis-hastings algorithm generate teaching documents. document sets drawn served starting state markov chain proposals generated randomly ﬂipping small number words. likelihood teaching probabilities estimated using sis. iterations document pair. topics derived counts compared true topics squared error. label switching gibbs sampler calculated error true topics label permutation inferred topics report minimum error across permutations. figure shows documents teaching model produce lower error effect greater fewer documents. number documents increases teaching model random generation produce similar learning outcomes. figure distribution error inferred true topics function number -word documents drawn teaching distributions. three ten-word topics comprise true model bulk beneﬁt teaching expected learning fewer documents number documents random teaching-based sampling equivalent varies depending problem. number topics model increase base distribution becomes sparser equivalence point increase. example three teaching documents beneﬁcial teach three-topic model four teaching documents produced results similar documents drawn randomly lda. consistent idea number documents surpasses number topics increasing probability random sampling will chance represent topics. model sparse random documents contain topics likely unique words individual documents become similar. case relative gains teaching reduced. figure number samples achieve relative sample error calculating marginal likelihood teaching probability single documents. point represents average runs. error bars represent conﬁdence interval. examples characterizing teaching distribution calculate equation exactly using small number small documents simple topic models. figure plot teaching distribution single -word documents several two-topic models three-word vocabularies documents plotted using normalized counts barycenter coordinates. documents word proportions consistent topic closer topic simplex. figure shows teaching model assigns highest density documents closer topics likelihood assigns higher density documents corners. documents generated mostly contain words generated topic; teaching model suggests choosing documents topics better teaching topics. density topics expected small proportion words; prevents teaching model favoring documents centered topics topics well-separated determine whether data beneﬁt learners provided teaching documents documents drawn randomly computed error inferred topics topics used generate documents. generated sets three four word documents teaching model lda. figure comparison teaching likelihood distribution representing probability selecting single -word document two-topic three-word-vocabulary model -dimensional density represented barycenter coordinates corner triangle represents word. crosses plot represent position topics. documents plotted normalizing word counts. normalized teaching distribution. darker areas indicated higher density. normalized likelihood. difference distributions. indicates areas teaching distribution higher density likelihood; blue indicates areas likelihood higher density teaching distribution. demonstrate feature chose teach single topic roughly corresponded war. mostoccurring words topic american team army us/us battle mission british. repeated procedure above used equation place equation apocalypse sits atop imdb’s greatest movies time best synopsis teaching topic. results show teaching likelihood distributions differ signiﬁcantly. fact likelihood synopses celebration nobody dramas nothing war. explore spatial qualities teaching distribution computed minus cosine distance synopsis normalized target topics negative correlation cosine distance teaching probabilities documents full topic model topic result explore scalability real-world implications work applied teaching model select movie synopses internet movie database movies synopses processed standard stop words words occur fewer three times removed leaving vocabulary words total words across documents. target topic model comprised topics derived running synopses. start ﬁnding single synopsis best captures entire topic model. teaching probability synopsis calculated independent times using samples best documents represent topics. best teaching synopsis brokeback mountain longer represents many topics—most contain drama keywords—though many words come topics working friends/family going trips. drawing analogy figure ideal single documents topics keeping lda’s sparsity closer others; brokeback mountain exhibits qualities. figure shows likelihood teaching probabilities correlate extent substantial differences. much beneﬁt teaching probability comes considering possible inferences directing learners’ inferences toward target away confusable alternatives. circumstances interested teaching acsubsets topics complished marginalizing remaining topics. figure comparison normalized teaching probability likelihood adjusted number words document gray) minus cosine synopses topic models entire topic model topic internet movie database movies. leftmost ﬁlms highest probability teaching model. point represents mean sixteen estimates. standard errors estimates represented often smaller points. title every shown. suggests selecting documents based cosine distance effective heuristic approximating teaching topic teaching model selects documents capture richer structure simpler approach. searching teaching probability documents employ cosine distance reduce number teaching probability calculations computing teaching probability cosine distance documents. discussion problem optimally selecting examples teaching important across variety domains. general scalable method elusive teaching requires simulating learner. whereas probabilistic learning proceed drawing samples posterior distribution teaching requires approximating distribution itself. building advances approximate inference including pseudomarginal sampling sequential importance sampling introduced general-purpose approach provides accurate simulation-based approximation optimal teaching examples demonstrate selecting documents teach distribution topics internet movie database movies problem simulations suggests bayesian teaching scales linearly number words teaching document. thought applied approach problem much larger could managed existing general approaches teaching scaling problems order teaching selecting books realized. optimistic continued progress toward truly scalable practically realizable applications. grifﬁths steyvers finding scientiﬁc topics. proceedings national academy sciences united states america suppl maceachern clyde sequential importance sampling nonparametric bayes models next generation. canadian journal statistics", "year": 2016}