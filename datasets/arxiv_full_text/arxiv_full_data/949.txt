{"title": "Tunable Efficient Unitary Neural Networks (EUNN) and their application  to RNNs", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Using unitary (instead of general) matrices in artificial neural networks (ANNs) is a promising way to solve the gradient explosion/vanishing problem, as well as to enable ANNs to learn long-term correlations in the data. This approach appears particularly promising for Recurrent Neural Networks (RNNs). In this work, we present a new architecture for implementing an Efficient Unitary Neural Network (EUNNs); its main advantages can be summarized as follows. Firstly, the representation capacity of the unitary space in an EUNN is fully tunable, ranging from a subspace of SU(N) to the entire unitary space. Secondly, the computational complexity for training an EUNN is merely $\\mathcal{O}(1)$ per parameter. Finally, we test the performance of EUNNs on the standard copying task, the pixel-permuted MNIST digit recognition benchmark as well as the Speech Prediction Test (TIMIT). We find that our architecture significantly outperforms both other state-of-the-art unitary RNNs and the LSTM architecture, in terms of the final performance and/or the wall-clock training speed. EUNNs are thus promising alternatives to RNNs and LSTMs for a wide variety of applications.", "text": "ploding gradient problems known caused matrix eigenvalues unity raised large powers. severity problems grows depth neural network particularly grave recurrent neural networks whose recurrence equivalent thousands millions equivalent hidden layers. several solutions proposed solve problems rnns. long short term memory networks help rnns contain information inside hidden layers gates remains popular implementations. recently proposed methods grus bidirectional rnns also perform well numerous applications. however none approaches fundamentally solved vanishing exploding gradient problems gradient clipping often required keep gradients reasonable range. recently proposed solution strategy using orthogonal hidden weight matrices complex generalization eigenvalues absolute values unity safely raised large powers. shown help weight matrices initialized unitary kept unitary training either restricting tractable matrix subspace alternating gradient-descent steps projections onto unitary subspace paper ﬁrst present efﬁcient unitary neural network architecture parametrizes entire space unitary matrices complete computationally efﬁcient thereby eliminating need time-consuming unitary subspace-projections. architecture wide range capacity-tunability represent subspace unitary models ﬁxing parameters; above-mentioned unitary subspace models correspond special cases architecture. also implemented eunn earlier introduced fft-like architecture efﬁciently approximates unitary space minusing unitary matrices artiﬁcial neural networks promising solve gradient explosion/vanishing problem well enable anns learn long-term correlations data. approach appears particularly promising recurrent neural networks work present architecture implementing efﬁcient unitary neural network main advantages summarized follows. firstly representation capacity unitary space eunn fully tunable ranging subspace entire unitary space. secondly computational complexity training eunn merely parameter. finally test performance eunns standard copying task pixelpermuted mnist digit recognition benchmark well speech prediction test architecture signiﬁcantly outperforms state-of-the-art unitary rnns lstm architecture terms ﬁnal performance and/or wall-clock training speed. eunns thus promising alternatives rnns lstms wide variety applications. deep neural networks successful numerous difﬁcult machine learning tasks including image recognition speech recognition natural language processing. however deep neural networks suffer vanishing ex*equal contribution massachusetts institute technology york university facebook research. correspondence jing <ljingmit.edu> yichen shen <ycshenmit.edu>. benchmark eunn’s performance simulated real tasks standard copying task pixelpermuted mnist task speech prediction timit dataset show eunn algorithm hidden layer size compute entire gradient matrix using computational steps memory access parameter. superior computational complexity existing training method full-space unitary network efﬁcient subspace unitary rnn. recurrent neural network takes input sequence uses current hidden state generate hidden state step memorizing past information hidden layer. ﬁrst review basic architecture. training neural network minimize cost function depends parameter vector gradient descent method updates vector ﬁxed learning rate vanishing exploding gradient problem signiﬁcant back propagation hidden hidden layers focus gradient hidden layers. training input-to-hidden hidden-to-output matrices relatively trivial hidden-to-hidden matrix successfully optimized. eigenvalues order unity eigenvalues cause gradient explosion eigenvalues cause gradient vanishing either situation prevents working efﬁciently. breakthrough paper arjovsky shah bengio showed unitary rnns overcome exploding vanishing gradient problems perform well long term memory tasks hiddento-hidden matrix parametrized following unitary form diagonal matrices element eiωj reﬂection matrices tries parameter trained. ﬁxed permutation matrix. fourier inverse fourier transform matrices respectively. since factor matrix unitary product also unitary matrix. model uses parameters spans merely part whole o-dimensional space unitary matrices enable computational efﬁciency. several subsequent papers tried expand space order achieve better performance summarized below. order maximize power unitary rnns preferable option optimize weight matrix full space unitary matrices rather subspace above. straightforward method implementing simply updating standard backpropagation projecting resulting matrix back onto space elements last except diagonal zero affected later transformations. since transformations unitary last column also contain zeros except diagonal method shows full space unitary networks superior many tasks limitation back-propation method canavoid n-dimensional matrix multiplication incurring computational cost. following ﬁrst describe general parametrization method able represent arbitrary unitary matrices degrees freedom. present efﬁcient algorithm parametrization scheme requiring computational memory access steps obtain gradient parameter. finally show scheme performs signiﬁcantly better mentioned methods well-known benchmarks. unitary matrix parametrization unitary matrix represented product rotation matrices {rij} diagonal matrix deﬁned n-dimensional identity matrix elements replaced follows unique parameters corresponding rij. matrices performs unitary transformation two-dimensional subspace ndimensional hilbert space leaving -dimensional subspace unchanged. words series rotations used successively make off-diagonal elements given unitary matrix zero. generalizes familiar factorization rotation matrix rotations parametrized three euler angles. provide intuition works brieﬂy describe simple similar gaussian elimination ﬁnishing column time. inﬁnitely many alternative decomposition schemes well; fig. shows particularly convenient implement software unitary matrix multiplied right succession unitary matrices figure unitary matrix decomposition arbitrary unitary matrix decomposed square decomposition method clements discussed section approximated fast fourier transformation style decomposition method discussed section junction graphs represent matrix shown effective dimensionality matrix thus reduced procedure repeated times effective dimension reduced leaving diagonal matrix rnn−rnn− rijrij− diagonal matrix whose diagonal elements eiwj write direct representation note gaussian elimination would make merely upper triangle matrix vanish requiring subsequent series rotations zero lower triangle. need subsequent series since unitary easy show unitary matrix triangular must diagonal. parametrization thus involves different θij-values different φij-values different wi-values combining parameters total spans entire unitary space. note always portion parameters span subset unitary space indeed benchmark test show certain tasks full unitary space parametrization necessary. representation made compact reordering grouping speciﬁc rotational matrices shown optical community context universal multiport interferometers. example unitary matrix decomposed parameters schematically shown fig. choosing different values span different subspace unitary space. speciﬁcallywhen span entire unitary space. inspired alternative organize rotation matrices implementing fftstyle architecture. instead using adjacent rotation matrices performs certain distance pairwise rotations shown fig. requires matrices total log/ rotational pairs. also minimal number rotations input coordinates interacting other providing approximation arbitrary unitary matrices. represents element-wise multiplication refers general rotational matrices fa/b case tunable-space implementation want implement deﬁne permutation follows computational complexity viewpoint since operations permute take computational steps evaluating requires steps. product trivial consisting element-wise vector multiplication. therefore product total unitary sign relu |zi| absolute value real number empirically nonlinearity function performs best. believe function possibly also serves forgetting ﬁlter removes noise using bias threshold. compare networks applying well deﬁned copying memory task copying task synthetic task commonly used test network’s ability remember information seen time steps earlier. speciﬁcally task deﬁned follows alphabet consists symbols {ai} ﬁrst represent data remaining representing blank start recall respectively; illustrated following example example {ai} input consists random data symbols followed blanks start recall symbol blanks. desired output consists blanks followed data sequence. cost function deﬁned cross entropy input output sequences vanishes perfect performance. input length symbol input represented n-dimensional one-hot vector. trained rnns batch size using rmsprop optimization learning rate decay rate eurnn models respectively. results show eurnn architectures introduced sec.. sec.. outperform lstm model unitary models in-terms learnability in-terms convergence rate. note unitary model able beat baseline signiﬁcantly slower method. moreover either choosing smaller using fft-style method eurnn converges toward optimal performance signiﬁcantly efﬁciently partial projective unitary methods. eurnn also performed robustly. means fullcapacity unitary matrix necessary particular task. mnist handwriting recognition problem classic benchmarks quantifying learning ability neural networks. mnist images formed grayscale image target label test different models feed pixels mnist images models time steps pixel time ﬂoating-point number. ﬁxed random permutation applied order input pixels. output probability distribution quantifying digit prediction. used rmsprop learning rate decay rate batch size table performance comparison four recurrent neural network algorithms urnn purnn eurnn denotes length denotes hidden state size. tunable-style eurnn integer parametrizing unitary matrix capacity. figure copying task eurnn corresponds algorithm projective urnn corresponds algorithm presented urnn corresponds algorithm presented useful baseline performance memoryless strategy outputs blanks followed random data symbols produces cross entropy also apply eurnn real-world speech prediction task compare performance lstm. main task consider predicting log-magnitude future frames short-time fourier transform timit dataset sampled khz. audio .wav initially diced different time frames audio amplitude frame table speech prediction task result. eurnn corresponds algorithm projective urnn corresponds algorithm presented urnn corresponds algorithm presented fourier transformed frequency domain. logmagnitude fourier amplitude normalized used data training/testing model. stft operation uses hann analysis window samples window samples frame prediction task follows given log-magnitudes stft frames time predict log-magnitude stft frame time minimum mean square error training utterances validation utterances evaluation utterances. training validation evaluation sets distinct speakers. trained rnns batch size using rmsprop optimization learning rate momentum decay rate presented method implementing efﬁcient unitary neural network whose computational cost merely parameter efﬁcient methods discussed above. signiﬁcantly outperforms existing architectures standard copying task pixel-permuted mnist task using comparable parameter count hence demonstrating highest recorded ability memorize sequential information long time periods. want emphasize generality tunability method. ordering rotation matrices presented fig. merely many possibilities; used simply concrete example. ordering options result spanning full unitary matrix space used algorithm well identical speed memory performance. tunability span unitary space correspondingly total number parameters makes possible different capacities different tasks thus opening optimal performance eunn. example shown small subspace full unitary space preferable copying task whereas mnist task timit task better performed eunn covering considerably larger unitary space. finally note method remains applicable even unitary matrix decomposed different product matrices powerful robust unitary architecture also might promising natural language processing because ability efﬁciently handle tasks long-term correlation high dimensionality. work partially supported army research ofﬁce institute soldier nanotechnologies contract wnf--d national science foundation grant ccf- rothberg family fund cognitive science. berglund mathias raiko tapani honkala mikko k¨arkk¨ainen vetek akos karhunen juha bidirectional recurrent neural networks generative models. advances neural information processing systems kyunghyun merri¨enboer bart bahdanau dzmitry bengio yoshua. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. collobert ronan weston jason bottou l´eon karlen michael kavukcuoglu koray kuksa pavel. natural language processing scratch. journal machine learning research donahue jeffrey anne hendricks lisa guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate darrell trevor. long-term recurrent convolutional networks visual recognition proceedings ieee conference description. computer vision pattern recognition garofolo john lamel lori fisher william fiscus jonathon pallett david darpa timit acousticphonetic continous speech corpus cd-rom. nist speech disc nasa sti/recon technical report wisdom scott powers thomas hershey john roux jonathan atlas les. full-capacity unitary recurrent neural networks. advances neural information processing systems krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems reck michael zeilinger anton bernstein herbert realization bertani philip. phys. rev. lett. discrete unitary operator. ./physrevlett. http//link.aps.org/doi/. /physrevlett... timefrequency feature representation using energy concentration overview recent advances. digital signal processing issn http//dx.doi.org/./j.dsp... http//www.sciencedirect.com/ science/article/pii/sx. shen yichen harris nicholas skirlo scott prabhu mihika baehr-jones hochberg michael zhao shijie larochelle hugo englund dirk deep learning coherent nanophotonic circuits. arxiv preprint arxiv.", "year": 2016}