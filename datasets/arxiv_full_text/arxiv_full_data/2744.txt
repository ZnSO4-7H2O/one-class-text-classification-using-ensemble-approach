{"title": "Deep Learning for Case-Based Reasoning through Prototypes: A Neural  Network that Explains Its Predictions", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as \"black box\" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.", "text": "work create architecture deep learning explains reasoning process. learned models naturally come explanations prediction explanations loyal network actually computes. discuss shortly creating architecture encode explanations contrast creating explanations previously trained black models aligns closely work prototype classiﬁcation case-based reasoning. past neural networks often designed purely accuracy posthoc interpretability analysis. case network architecture chosen ﬁrst afterwards aims interpret trained model learned high-level features. interpretability analysis requires separate modeling effort. problem generating explanations posthoc explanations change based model explanation. instance easy create multiple conﬂicting convincing explanations network would classify single object none correct reason object classiﬁed way. related issue posthoc methods often create explanations make sense humans. means extra modeling needed ensure explanations interpretable. happens instance activation maximization approach aims input pattern produces maximum model response quantity interest user since images generally interpretable regularized optimization used interpretable high activation image regularization however result combination network actually computes extrinsic regularization. given explanations come separate modeling process strong priors part training wonder trust explanations posthoc analysis. fact growing literature discussing issues mentioned images posthoc analysis often involves visualization layers neural network. instance alternative provided zeiler fergus deconvolution technique visualize deep neural networks widely used classiﬁcation. deep models often suffer lack interpretability particularly difﬁcult understand non-linear nature. result neural networks often treated black models past trained purely optimize accuracy predictions. work create novel network architecture deep learning naturally explains reasoning prediction. architecture contains autoencoder special prototype layer unit layer stores weight vector resembles encoded training input. encoder autoencoder allows comparisons within latent space decoder allows visualize learned prototypes. training objective four terms accuracy term term encourages every prototype similar least encoded input term encourages every encoded input close least prototype term encourages faithful reconstruction autoencoder. distances computed prototype layer used part classiﬁcation process. since prototypes learned training learned network naturally comes explanations prediction explanations loyal network actually computes. machine learning algorithms gained importance important societal questions interpretability become issue whether trust predictions coming models. cases incorrect data black models gone unnoticed leading unfairly long prison sentences radiology lack transparency causes challenges approval deep learning products. issues opening black neural networks become debated issue media artiﬁcial neural networks particularly difﬁcult understand highly nonlinear functions naturally lend explanation humans able process. convolutional neural network learned. deconvolution method decoding; method type decoder visualize prototypes including deconvolution. addition zeiler fergus visualize parts images strongly activate given feature provide explanation network reaches decision. contrast build reasoning process network consider posthoc analysis work. works also build interpretability deep neural networks without using posthoc analysis. pinheiro collobert design network weakly supervised image segmentation training classiﬁcation network extracts important pixels could potentially belong object class. barzilay jaakkola propose network architecture extracts parts input rationale uses rationale predictions. works build interpretability neural networks extracting parts input focusing parts respective tasks. method differs case-based reasoning instead extractive reasoning model explains predictions based similarity prototypical cases rather highlighting relevant parts input; possible ideas combined ours. gales improve interpretability activation patterns feature maps deep neural networks used speech recognition. contrast model enforce particular structure feature maps allows ﬂexibility feature learning introduces special prototype layer decision interpretation. network form prototype classiﬁer observations classiﬁed based proximity prototype observation within dataset. instance handwritten digit example determine observation classiﬁed network thinks looks like particular prototypical within training set. prediction uncertain would identify prototypes similar observation different classes e.g. often hard distinguish would expect prototypes classes identiﬁed network asked classify image work closely aligned prototype classiﬁcation techniques machine learning prototype classiﬁcation classical form case-based reasoning however because work uses neural networks distance measure prototypes observations measured ﬂexible latent space. fact latent space adaptive driving force behind high quality performance. word prototype overloaded various meanings. prototype close identical observation training prototypes representative whole data set. contexts prototype required close training examples could convex combination several observations. few-shot zero-shot learning prototypes points feature space used represent single class distance protoype determines observation classiﬁed. example protonets utilize mean several embedded support examples prototype class fewshot learning. wang generative probabilistic model generate prototypes zero shot learning points feature space. cases prototypes optimized resemble actual observations required interpretable class prototype. deep architecture uses autoencoder create latent low-dimensional space distances prototypes computed latent space. using latent space distance computation enables better dissimilarity measure pixel space. works also latent spaces e.g. salakhutdinov hinton conduct soft k-nearest neighbors classiﬁcation latent space restricted boltzman machine autoencoder although interpretability. network architecture training dataset model architecture consists components autoencoder prototype classiﬁcation network illustrated figure network uses autoencoder reduce dimensionality input learn useful features prediction; uses encoded input produce probability distribution classes prototype classiﬁcation network network made three layers prototype layer fully-connected layer softmax layer network learns prototype vectors latent space. prototype layer computes squared distance encoded input prototype vectors figure prototype unit corresponding executes computation fully-connected layer computes weighted sums distances weight matrix. weighted sums normalized softmax layer output probability distribution classes. k-th component output softmax layer deﬁned k-th component squared distance original reconstructed input penalizing autoencoder’s reconstruction error. reconstruction loss denoted training data given terms averages minimum squared distances. minimization would require prototype vector close possible least training examples latent space. long choose decoder network continuous function expect close vectors latent space decoded similar-looking images. thus push prototype vectors meaningful decodings pixel space. minimization would require every encoded training example close possible prototype feature space. special case prototype every class weight matrix fully-connected layer negative identity matrix −ik×k data predicted class nearest prototype latent space. realistically typically know many prototypes assigned class want different number prototypes number classes i.e. case allow learned network result distances prototype vectors contribute probability prediction class. network architecture least three advantages. first unlike traditional case-based learning methods method automatically learns useful features. image datasets dimensions equal number pixels perform classiﬁcation using original input space hand-crafted feature spaces methods tend perform poorly second prototype vectors live space encoded inputs feed vectors decoder visualize learned prototypes throughout training process. property coupled case-based reasoning nature prototype classiﬁcation network gives users ability interpret network reaches predictions visualize prototype learning process without posthoc analysis. third allow weight matrix learnable able tell strengths learned weight connections prototypes representative class. cost function network’s cost function reﬂects needs accuracy interpretability. addition classiﬁcation error term penalizes reconstruction error autoencoder. error terms encourage learned prototype vectors correspond vectors. means cluster training examples around prototypes latent space. notice although involve minimization function differentiable everywhere terms differentiable almost everywhere many modern deep learning libraries support type differentiation. ideally would take minimum distance entire training every prototype; therefore gradient computation would grow linearly size training set. however would impractical optimization large dataset. address problem relax minimization random minibatch used stochastic gradient descent algorithm. three terms since summation entire training natural apply randomly selected batches gradient computation. begin detailed walkthrough applying model well-known mnist dataset. modiﬁed nist benchmark dataset gray-scale images segmented centered handwritten digits used training examples validation examples testing examples every image size pixels. preprocess images every pixel value section organized follows ﬁrst introduce architecture training details compare performance network model noninterpretible network models ﬁnally visualize learned prototypes weight matrix speciﬁc image classﬁed. architecture details hinton salakhutdinov show multilayer fully connected autoencoder network achieve good reconstruction mnist even using dimensional latent space. choose multilayer convolutional autoencoder symmetric architecture encoder decoder model’s autoencoder; types networks tend reduce spatial feature extraction redundancy image data sets learn useful hierarchical features producing state-of-the-art classiﬁcation results. convolutional layer consists convolution operation followed pointwise nonlinearity. achieve downsampling encoder strided convolution strided deconvolution corresponding layer decoder. passing original image encoder network ﬂattens resulted feature maps code vector feeds prototype layer. resulting unﬂattened feature maps decoder reconstruct original image. visualize prototype vector pixel space ﬁrst reshape vector shape encoder output feed shaped vector decoder. autoencoder network four convolutional layers encoder decoder. four convolutional layers encoder kernels size zero padding stride size convolution stage. ﬁlters corresponding layers encoder decoder constrained transposes other. outputs ﬁrst three layers feature maps last layer given input image dimension shape encoder layers thus therefore network compresses every -dimensional image input -dimensional code vector every layer uses sigmoid function +e−x nonlinear transformation. speciﬁcally sigmoid function last encoder layer output encoder restricted unit hypercube allows initialize prototype vectors uniformly random hypercube. rectiﬁed linear unit last encoder layer because using would make difﬁcult initialize prototype vectors initial states throughout would need explored network would take longer stabilize. also speciﬁcally choose sigmoid function last decoder layer make range pixel values reconstructed output roughly preprocessed image’s pixel range. training details hyperparameters learning rate minimize whole employ greedy layer-wise optimization different layers autoencoder ﬁrst train autoencoder prototype classiﬁcation network. goal work obtain reasonable accuracy also interpretability. general techniques improving performance neural networks possible using techniques would improve accuracy. particular data augmentation technique elastic deformation improve prediction accuracy reduce potential overﬁtting. elastic deformations superset afﬁne transformations. every mini-batch size randomly sampled training apply random elastic distortion gaussian ﬁlter standard deviation equal scaling factor used displacement ﬁeld. randomness data augmentation process network sees slightly different images every epoch signiﬁcantly reduces overﬁtting. examine elements interpretable network affect predictive power performed type ablation study. particular trained classiﬁcation networks similar ours removed pieces networks. ﬁrst network substitutes prototype layer fullyconnected layer whose output -dimensional vector dimension output prototype layer; second network also removes decoder changes nonlinearity relu. second network regular convolutional neural network similar architectural complexity lenet training networks using elastic deformation epochs obtained test accuracies respectively. test accuracies along test accuracy reported lecun comparable test accuracy obtained using interpretable network. result demonstrates changing traditional convolutional neural network interpretable network architecture hinder predictive ability network general always true accuracy needs sacriﬁced obtain interpretability; could many models almost equally accurate. extra terms cost function encourage model interpretable among approximately equally accurate models. visualization ﬁrst discuss quality autoencoder good performance autoencoder allow interpret prototypes. training network’s autoencoder achieved average squared reconstruction error undeformed training examples shown figure reconstruction result assures decoder faithfully prototype vectors pixel space. sending decoder. decoded prototype images sharp-looking mostly resemble real-life handwritten digits owing interpretability terms cost function. note one-toone correspondence classes prototypes. since multiply output prototype layer learnable weight matrix prior feeding softmax layer distances encoded image prototype differing effects predicted class probabilities. look transposed weight matrix connecting prototype layer softmax layer shown table inﬂuence distance prototype every class. observe decoded prototype visually similar image class corresponding entry weight matrix signiﬁcantly negative value. call class decoded prototype visually similar visual class prototype. reason signiﬁcantly negative value understood follows. prototype layer computing dissimilarity input image prototype squared distance representations latent space. given image prototype belong visual class distance large multiplied highly negative weight connection prototype visual class product also highly negative therefore signiﬁcantly reduce activation visual class result image likely classiﬁed visual class conversely belongs visual class small squared distance multiplied highly negative weight connection between visual class product decrease activation pj’s visual class much. activations every class belong signiﬁcantly reduced non-similar prototype leaving activation xi’s actual class comparatively large. therefore correctly classiﬁed general. interesting prototype learned network last prototype table visually similar image class however strong negative weight connections class class well. therefore think prototype shared three classes means encoded input image away prototype latent space would unlikely image surprising look decoded prototype image carefully hide tail digit would look like image connect upper-left endpoint lower-right endpoint would look like image look learned prototypes figure three prototypes class seem represent different writing habits terms loop angle looks like. ﬁrst third loops bottom second loop ends side. show similar variation. prototypes correspond different curvatures. table transposed weight matrix prototype layer softmax layer. represents prototype node whose decoded image shown ﬁrst column. column represents digit class. negative weight shaded prototype. general prototype negative weight towards visual class except prototype last row. distances computed prototype layer encoded input image prototypes shown below decoded prototypes table three smallest distances correspond three prototypes resemble decoding. observe three distances quite different encoded input image signiﬁcantly closer third prototype two. indicates model indeed capturing subtle differences within class. prototype layer computes -dimensional vector distances shown table multiplied weight matrix table output unnormalized probability vector used logit softmax layer. predicted probability class speciﬁc image convolutional layers encoder decoder. ﬁrst second layer encoder uses respectively convolutional ﬁlters size stride zero padding. architecture decoder symmetric encoder. sigmoid activation function last layer encoder decoder leaky relu autoencoder layers. number prototypes eleven number classes. figure shows eleven decoded prototypes model. compare figure figure color observe network determined color important determining angle decoded prototypes average color. learned weight matrix shown table supplementary material. compared model network without interpretable parts removed decoder replaced prototype layer fully connected layer size. accuracies models shown table result illustrates sacriﬁce much acuse case study illustrate importance interpretability terms cost function. remove decoded prototypes look like real images shown figure leave decoded prototypes look like real observations shown figure remove network chooses prototypes fully represent input space prototypes tend similar other shown figure intuitively pushes every prototype close training example latent space decoded prototypes realistic forces every training example close prototype latent space thereby encouraging prototypes spread entire latent space distinct other. words helps make prototypes meaningful keeps explanations faithful forcing network nearby prototypes classiﬁcation. fashion mnist dataset zalando’s article images consisting training examples test examples. example grayscale image associated label classes type clothes item. dataset shares image size structure training testing splits mnist. model case study fashion dataset achieved testing accuracy result comparable obtained using standard convolutional neural networks pooling reported dataset website learned prototypes shown figure class least prototype representing class. learned prototypes fewer details original images. shows model recognized information important classiﬁcation task contour shape input useful ﬁne-grained details. learned weight matrix shown table supplementary material. combine strength deep learning interpretability case-based reasoning make interpretable deep neural network. prototypes provide useful insight inner workings network relationship classes important aspects latent space demonstrated here. although model provide full solution problems accountability transparency black decisions allow partially trace path classiﬁcation observation. noticed experiments addition interpretability terms tend regularizers help make network robust overﬁtting. extent interpretability reduces overﬁtting topic could explored future work. supplementary material code supplementary material code available https// github.com/oscarcarli/prototypedl. acknowledgments work sponsored part lincoln laboratory.", "year": 2017}