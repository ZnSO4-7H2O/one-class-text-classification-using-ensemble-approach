{"title": "An efficient framework for learning sentence representations", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.", "text": "work propose simple efﬁcient framework learning sentence representations unlabelled data. drawing inspiration distributional hypothesis recent work learning sentence representations reformulate problem predicting context sentence appears classiﬁcation problem. given sentence context classiﬁer distinguishes context sentences contrastive sentences based vector representations. allows efﬁciently learn different types encoding functions show model learns high-quality sentence representations. demonstrate sentence representations outperform state-of-the-art unsupervised supervised representation learning methods several downstream tasks involve understanding sentence semantics achieving order magnitude speedup training time. methods learning meaningful representations data received widespread attention recent years. become common practice exploit representations trained large corpora downstream tasks since capture prior knowlege domain interest lead improved performance. especially attractive transfer learning setting small amount labelled data available supervision. unsupervised learning allows learn useful representations large unlabelled corpora. idea self-supervision recently become popular representations learned designing learning objectives exploit labels freely available data. tasks predicting relative spatial location nearby image patches inpainting solving image jigsaw puzzles successfully used learning visual feature representations. language domain distributional hypothesis integral development learning methods obtaining semantic vector representations words assumption meaning word characterized word-contexts appears. neural approaches based assumption successful learning high quality representations large text corpora. recent methods applied similar ideas learning sentence representations encoder-decoder models learn predict/reconstruct context sentences given sentence. despite success several modelling issues exist methods. numerous ways expressing idea form sentence. ideal semantic representation insensitive form meaning expressed. existing models trained reconstruct surface form sentence forces model predict semantics aspects irrelevant meaning sentence well. problem associated models computational cost. methods word level reconstruction objective involves sequentially decoding words target sentences. training output softmax layer entire vocabulary signiﬁcant source slowdown training process. limits size vocabulary model sampling based softmax sub-word representations help alleviate issue). circumvent problems proposing objective operates directly space sentence embeddings. generation objective replaced discriminative approximation model attempts identify embedding correct target sentence given sentence candidates. context interpret ‘meaning’ sentence information sentence allows predict predictable information context sentences. name approach quick thoughts mean efﬁcient learning thought vectors. propose simple general framework learning sentence representations efﬁciently. train widely used encoder architectures order magnitude faster previous methods achieving better performance time. learning unlabelled corpora. mikolov proposed paragraph vector model embed variable-length text. models trained predict word given context words appearing small window based vector representation source document. unlike methods work sentences considered atomic units instead compositional function words. encoder-decoder models successful learning semantic representations. kiros proposed skip-thought vectors model consists encoder produces vector representation source sentence decoder sequentially predicts words adjacent sentences. drawing inspiration model explore convolutional neural network encoders. base model uses encoder reconstructs input sentence well neighboring sentences using rnn. also consider hierarchical version model sequentially reconstructs sentences within larger context. autoencoder models explored representation learning wide variety data domains. advantage autoencoders context prediction models require ordered sentences learning. socher proposed recursive autoencoders encode input sentence using recursive encoder decoder reconstructs hidden states encoder. hill considered de-noising autoencoder model noise introduced sentence deleting words swapping bigrams decoder required reconstruct original sentence. bowman proposed generative model sentences based variational autoencoder. kenter learn bag-of-words representations sentences considering conceptually similar task identifying context sentences candidates evaluate representations sentence similarity tasks. hill introduced fastsent model uses representation input sentence predicts words appearing context sentences. model trained predict whether word appears target sentences. arora consider weighted model followed simple post-processing show performs better models trained paraphrase data. jernite paragraph level coherence learning signal learn representations. following related task considered work. given ﬁrst three sentences paragraph choose next sentence sentences later paragraph. related objective local coherence model hovy binary classiﬁer trained identify coherent/incoherent sentence windows. contrast encourage observed contexts plausible contrastive ones formulate multi-class classiﬁcation problem. experimentally found relaxed constraint helps learn better representations. encoder-decoder based sequence models known work well slow train large amounts data. hand bag-of-words models train efﬁciently ignoring word order. figure overview. approach adopted prior work given input sentence model attempts generate context sentence. approach replaces decoder classiﬁer chooses target sentence candidate sentences. structured resources. attempts labeled/structured data learn sentence representations. hill learn words dictionary deﬁnitions using margin loss encourages encoded representation deﬁnition similar corresponding word. wieting wieting gimpel paraphrase data learn encoder maps synonymous phrases similar embeddings using margin loss. hermann blunsom consider similar objective minimizing inner product paired sentences different languages. wieting explore machine translation obtain paraphrase data back-translation learning paraphrastic embeddings. conneau consider supervised task natural language inference means learning generic sentence representations. task involves identifying three relationships given sentences entailment neutral contradiction. training strategy consists learning classiﬁer embeddings input pair sentences. authors show sentence encoders trained task perform strongly downstream transfer tasks. distributional hypothesis operationalized prior work different ways. common approach illustrated figure encoding function computes vector representation input sentence decoding function attempts generate words target sentence conditioned representation. skip-thought model target sentences appear neighborhood input sentence. variations decoder autoencoder models predict input sentence instead neighboring sentences predicting properties window words input sentence instead training model reconstruct surface form input sentence neighbors take following approach. meaning current sentence predict meanings adjacent sentences meaning represented embedding sentence computed encoding function. despite simplicity modeling approach show facilitates learning rich representations. approach illustrated ﬁgure given input sentence encoded using function. instead generating target sentence model chooses correct target sentence candidate sentences. viewing generation choosing sentence possible sentences seen discriminative approximation generation problem. formally described parametrized functions take sentence input encode ﬁxed length vector. given sentence. sctxt sentences appearing context training data. scand candidate sentences considered given context sentence sctxt sctxt. words scand contains valid context sentence sctxt many non-context sentences used classiﬁcation objective described below. modeling approach encapsulates skip-gram approach mikolov words play role sentences. case encoding functions simple lookup tables considering words atomic units training objective maximizes similarity source word target word context given negative samples. alternatively considered objective function similar negative sampling approach mikolov takes form binary classiﬁer takes sentence window input classiﬁes plausible implausible context windows. found objective work better presumably relaxed constraint imposes. instead requiring context windows classiﬁed positive/negative requires ground-truth contexts plausible contrastive contexts. objective also performed empirically better maxmargin loss. experiments simply deﬁned inner product motivated considering pathological solutions model learns poor sentence encoders rich classiﬁer compensate undesirable since classiﬁer discarded sentence encoders used extract features downstream tasks. minimizing number parameters classiﬁer encourages encoders learn disentangled useful representations. consider different parameters although motivated perspective modeling sentence meaning. another motivation comes word representation learning methods different sets input output parameters. parameter sharing signiﬁcant concern since models trained large corpora. test time given sentence consider representation concatenation outputs encoders framework allows ﬂexible encoding functions used. rnns widely used recent sentence representation learning methods. words sentence sequentially input ﬁnal hidden state interpreted representation sentence. gated recurrent units cell similar kiros evaluate sentence representations using feature representations downstream tasks. alternative ﬁne-grained evaluation tasks identifying word appearance word order proposed although provides useful insight representations tasks focus syntactic aspects sentence. interested assessing well representations capture sentence semantics. although limitations evaluations pointed stick traditional approach evaluating using downstream tasks. models trained novels bookcorpus dataset dataset consists ordered sentences. also consider larger corpus training umbc corpus dataset pages crawled internet preprocessed tokenized paragraphs. dataset sentences three times larger bookcorpus. models trained scratch used case-sensitive vocabularies sizes datasets respectively. minibatch constructed using contiguous sets sentences corpus. sentence sentences minibatch considered candidate pool scand sentences classiﬁcation. simple scheme picking contrastive sentences performed well schemes random sampling picking nearest neighbors input sentence. hyperparameters including batch size learning rate prediction context size obtained using prediction accuracies validation set. context size used i.e. predicting previous next sentences given current sentence. used batch size learning rate adam optimizer experiments. rnn-based models single-layered cells. weights initialized using uniform xavier initialization gate biases initialized word embeddings initialized tasks evaluate sentence representations tasks require understanding sentence semantics. following classiﬁcation benchmarks commonly used movie review sentiment product reviews subjectivity classiﬁcation opinion polarity question type classiﬁcation paraphrase identiﬁcation semantic relatedness task sick dataset involves predicting relatedness scores given pair sentences correlate well human judgements. subj mpqa tasks binary classiﬁcation tasks. -fold cross validation used reporting test performance tasks. tasks come train/dev/test splits used choosing regularization parameter. follow evaluation scheme kiros feature representations sentences obtained trained encoders logistic/softmax classiﬁer trained embeddings task keeping sentence embeddings ﬁxed. kiros al.’s scripts used evaluation. table compares work representations prior methods learn unlabelled data. dimensionality sentence representations training time also indicated. based encoder consider variations analogous skip-thought model. uniqt model uses uni-directional rnns sentence encoders bi-qt model concatenation ﬁnal hidden states rnns represent processing sentence different direction. combine-qt model concatenates representations learned uni-qt bi-qt models. models trained scratch bookcorpus. fastsent model efﬁcient train efﬁciency stems using bag-of-words encoder. words provides strong baseline ability preserves word identity information. however model performs poorly compared methods. bag-of-words also conceptually less attractive representation scheme since ignores word order aspect meaning. table comparison sentence representations downstream tasks. baseline methods glove bag-of-words representation de-noising auto-encoders fastsent hill paragraph vector distributed memory model skip-thought vectors model training times indicated using refers trained models assumes concatenated representations trained independently. performance ﬁgures sdae fastsent paragraphvec obtained hill higher numbers better columns except last table divided different sections. bold-face numbers indicate best performance values among models current previous sections. best overall values column underlined. order information encoded representation. however fails perform well tasks require higher level sentence understanding also inefﬁcient train. uni/bi/combine-qt variations perform comparably skipthought model cnn-based variation tasks despite requiring much less training time. since models trained scratch also shows model learns good word representations well. multichannel-qt. next consider using pre-trained word vectors train model. multichannel-qt model deﬁned concatenation bi-directional rnns. uses ﬁxed pre-trained word embeddings coming large vocabulary input. uses tunable word embeddings trained scratch model inspired multi-channel model considered sets embeddings. different input representations models discover less redundant features opposed variations suggested kiros glove vectors pre-trained word embeddings. mc-qt model outperforms previous methods including variation uses pre-trained word embeddings. umbc data. framework efﬁcient train also experimented larger dataset documents. results models trained bookcorpus umbc corpus pooled together shown bottom table. observe strict improvements majority tasks compared bookcorpus models. shows exploit huge corpora obtain better models keeping training time practically feasible. computational efﬁciency. models implemented tensorﬂow. experiments performed using cuda cudnn libraries titan gpu. best bookcorpus model trains training time skip-thoughts model mentioned weeks kiros recent tensorﬂow implementation reports training time days augmented dataset models take train observe monotonic improvements tasks except trec task. framework allows training much larger vocabulary sizes previous models. approach also memory efﬁcient. paragraph vector model memory footprint since store vectors documents used training. softmax computations vocabulary skip-thought models word-level reconstruction objectives incur heavy memory consumption. based implementation within memory majority consumed word embeddings. table comparison task-speciﬁc supervised models. models adasent tf-kld dependency-tree lstm note performance values correspond linear classiﬁer trained ﬁxed pre-trained embeddings task-speciﬁc methods tuned end-to-end. table compares approach methods learn labelled/structured data. captionrep dictrep models hill trained respectively tasks matching images captions mapping words dictionary deﬁnitions machine translation. infersent model conneau trained task. addition benchmarks considered before additionally also include sentiment analysis binary classiﬁcation task stanford sentiment treebank infersent model strong performance tasks. multichannel model trained data outperforms infersent tasks signiﬁcant margins trec tasks. infersent strong sick task presumably following reasons. model gets observes near paraphrases sentences not-paraphrases training time. furthermore considers difference features multiplicative features input pair sentences training. identical feature transformations used sick evaluation well. ensemble consider ensembling exploit strengths different types encoders. since models efﬁcient train able feasibly train many models. consider subset following model variations ensemble. model pre-trained unsupervised sentence representations combine-skip combine-cnn mc-qt direct supervision sentence representations dvsa gmm+hglmm m-rnn order models combined using weighted average predicted log-probabilities individual models weights normalized validation performance scores. results presented table performance best purely supervised task-speciﬁc methods shown bottom reference. note numbers directly comparable unsupervised methods since sentence embeddings ﬁne-tuned. observe ensemble model closely approaches performance best supervised task-speciﬁc methods outperforming tasks. image-to-caption caption-to-image retrieval tasks commonly used evaluate sentence representations multi-modal setting. task requires retrieving image matching given text description vice versa. evaluation setting identical kiros images captions represented vectors. given matching image-caption pair scoring function determines compatibility corresponding vector representations scoring function trained using margin loss encourages matching pairs higher compatibility mismatching pairs. prior work vgg-net features image representation. sentences represented vectors using representation learning method evaluated. representations held ﬁxed training. scoring function used prior work projection matrices project image sentence vectors dimensionality. mscoco dataset traditionally used task. train/val/test split proposed karpathy fei-fei training validation test sets respectively consist images annotated captions. performance reported average splits image-caption pairs test set. results presented table outperform previous unsupervised pre-training methods signiﬁcant margin strictly improving median retrieval rank annotation search tasks. also outperform purely supervised task speciﬁc methods metrics. model skip-thought model conceptually similar objective functions. suggests examining properties embedding spaces better understand encode semantics. consider nearest neighbor retrieval experiment compare embedding spaces. pool sentences wikipedia dump experiment. given query sentence best neighbor determined cosine distance embedding space retrieved. table shows random sample query sentences dataset corresponding retrieved sentences. examples show retrievals often related query sentence compared skip-thought model. interesting ﬁrst example model identiﬁes sentence similar meaning even though main clause conditional clause different order. line goal learning representations less sensitive form meaning expressed. bihar uttar pradesh insisted urdu pakistan ofﬁcial language georges charachidz historian linguist georgian origin dumzil tutelage became noted specialist caucasian cultures aided dumzil reconstruction ubykh language wali mohammed wali visit thus stimulated growth development urdu ghazal delhi query exception known douglas treaties negotiated james douglas native people victoria area treaties signed british columbia assets natal railway company including locomotive ﬂeet three purchased natal colonial government exceptions treaties signed proposed framework learn generic sentence representations efﬁciently large unlabelled text corpora. simple approach learns richer representations prior unsupervised supervised methods consuming order magnitude less training time. establish state-ofthe-art unsupervised sentence representation learning methods several downstream tasks. believe exploring scalable approaches learn data representations exploit unlabelled data available abundance. material based part upon work supported career sloan research fellowship. thank jongwook choi junhyuk kibok ruben villegas seunghoon hong xinchen yijie yuting zhang helpful comments discussions. eneko agirre carmen banea claire cardie daniel mona diab aitor gonzalez-agirre weiwei rada mihalcea german rigau janyce wiebe. semeval- task multilingual semantic textual similarity. semeval coling alexis conneau douwe kiela holger schwenk loic barrault antoine bordes. supervised learning universal sentence representations natural language inference data. arxiv preprint arxiv. carl doersch abhinav gupta alexei efros. unsupervised visual representation learning context prediction. proceedings ieee international conference computer vision bill dolan chris quirk chris brockett. unsupervised construction large paraphrase corpora exploiting massively parallel news sources. proceedings international conference computational linguistics association computational linguistics yunchen ricardo henao chunyuan xiaodong lawrence carin. unsupervised learning sentence representations using convolutional neural networks. arxiv preprint arxiv. james mayﬁeld johnathan weese. umbc ebiquity-core semantic textual similarity systems. proceedings second joint conference lexical computational semantics. association computational linguistics june andrej karpathy fei-fei. deep visual-semantic alignments generating image descriptions. proceedings ieee conference computer vision pattern recognition ryan kiros yukun ruslan salakhutdinov richard zemel raquel urtasun antonio torralba sanja fidler. skip-thought vectors. advances neural information processing systems benjamin klein sadeh lior wolf. associating neural word embeddings deep image representations using ﬁsher vectors. proceedings ieee conference computer vision pattern recognition marco marelli stefano menini marco baroni luisa bentivogli raffaella bernardi roberto zamparelli. sick cure evaluation compositional distributional semantic models. lrec tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems pang lillian lee. sentimental education sentiment analysis using subjectivity summarization based minimum cuts. proceedings annual meeting association computational linguistics association computational linguistics pang lillian lee. seeing stars exploiting class relationships sentiment categorization respect rating scales. proceedings annual meeting association computational linguistics association computational linguistics deepak pathak philipp krahenbuhl jeff donahue trevor darrell alexei efros. context encoders feature learning inpainting. proceedings ieee conference computer vision pattern recognition richard socher eric huang jeffrey pennington andrew christopher manning. dynamic pooling unfolding recursive autoencoders paraphrase detection. nips volume richard socher alex perelygin jean jason chuang christopher manning andrew christopher potts recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing volume citeseer sheng richard socher christopher manning. improved semantic representations tree-structured long short-term memory networks. arxiv preprint arxiv. experiment compare ability model skip-thought vectors reason analogies sentence embedding space. analogy task widely used evaluating word representations. task involves answering questions type answer word shares relationship word identical relationship words consider analogous task sentence level formulate retrieval task query vector used identify closest sentence vector pool candidates. evaluation favors models produce meaningful dimensions. exploit word analogy datasets construct sentence tuples analogical relationships. mine sentence pairs yelp dataset approximately differ single word pairs construct sentence analogy tuples based known word analogy tuples. dataset tuples sentences collected fashion. sentence tuple derive questions considering three sentences form query vector. candidate pool sentence retrieval consists sentences dataset sentences yelp dataset. table compares retrieval performance representations skip-thought vectors task. results classiﬁed word-pair categories google microsoft word analogy datasets model outperforms skip-thoughts across several categories good performance family verb transformation categories table shows qualitative retrieval results. table shows three sentences form query answer identiﬁed model. last shows example model fails. common failure case methods model assumes identical question retrieves sentence answer. experiments show representations possess better linearity properties. transformations evaluated mostly syntactic transformations involving words. would interesting explore high-level transformations switching sentiment polarity analogical relationships involve several words future work. section assess representations learned encoders semantic similarity tasks. datasets consist pairs sentences annotated humans similarity scores. representations evaluated measuring correlation human judgments cosine similarity vector representations given pair sentences. consider types encoders trained using objective encoders encoders. models trained scratch bookcorpus data. version combine-qt model table describe encoder training below. train encoder using training objective. hyperparameter choices embedding size number contrastive sentences context size made based validation training model bookcorpus dataset takes hours titan gpu. similar encoders representation sentence obtained concatenating outputs input output sentence encoders. table compares different unsupervised representation learning methods trained bookcorpus data scratch. methods categorized sequence models bag-of-words models. rnn-based encoder performs strongly compared sequence encoders. bag-of-words models known perform strongly task better able encode word identity information. variation performs comparably prior based models. table comparison sentence representations semantic textual similarity tasks. sdae cbow skipgram fastsent hill baselines skip-thoughts siamese cbow models trained encoders respectively. better assess training efﬁciency models perform following experiment. train encoder architecture using objective skip-thought objective compare performance certain number hours training. since training objective large embedding sizes takes many days consider lower dimensional sentence encoder experiment. chose encoder architecture single-layer recurrent neural hidden state size word embedding size vocabulary size words used. models initialized randomly distribution. models trained data epoch using adam optimizer learning rate batch size dimensional model considered model trained objective objective take respectively. -dimensional sentence embeddings used evaluation. evaluation follows protocol section figure compares performance models downstream tasks number training hours. speed beneﬁts training objective apparent comparisons. overall training speedup observed objective note output encoder discarded model unlike experiments main text representations input output encoders concatenated. speedups achieved training encoders half size concatenating explore trade-off training efﬁciency quality representations varying representation size. trained models different representation sizes evaluate downstream tasks. multi-channel model used experiments. models trained bookcorpus dataset. table shows training time performance corresponding different embedding sizes. training times listed assume component models mc-qt trained parallel. reported performance average classiﬁcation benchmarks note classiﬁers trained embeddings downstream tasks differ size embedding size. difﬁcult make strong conclusions quality embeddings different sizes. however able reduce embedding size train models efﬁciently expense marginal loss performance cases. -dimensional skip-thought model combine-cnn model achieve mean accuracies respectively. note dimensional model -dimensional model respectively better models terms mean performance across benchmarks suggests high-quality models obtained even efﬁciently training lower-dimensional models large amounts data using objective.", "year": 2018}