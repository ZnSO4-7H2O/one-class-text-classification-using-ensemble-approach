{"title": "Wider and Deeper, Cheaper and Faster: Tensorized LSTMs for Sequence  Learning", "tag": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Long Short-Term Memory (LSTM) is a popular approach to boosting the ability of Recurrent Neural Networks to store longer term temporal information. The capacity of an LSTM network can be increased by widening and adding layers. However, usually the former introduces additional parameters, while the latter increases the runtime. As an alternative we propose the Tensorized LSTM in which the hidden states are represented by tensors and updated via a cross-layer convolution. By increasing the tensor size, the network can be widened efficiently without additional parameters since the parameters are shared across different locations in the tensor; by delaying the output, the network can be deepened implicitly with little additional runtime since deep computations for each timestep are merged into temporal computations of the sequence. Experiments conducted on five challenging sequence learning tasks show the potential of the proposed model.", "text": "long short-term memory popular approach boosting ability recurrent neural networks store longer term temporal information. capacity lstm network increased widening adding layers. however usually former introduces additional parameters latter increases runtime. alternative propose tensorized lstm hidden states represented tensors updated cross-layer convolution. increasing tensor size network widened efﬁciently without additional parameters since parameters shared across different locations tensor; delaying output network deepened implicitly little additional runtime since deep computations timestep merged temporal computations sequence. experiments conducted challenging sequence learning tasks show potential proposed model. consider time-series prediction task producing desired output timestep given observed input sequence x··· yt∈rs vectors. recurrent neural network powerful model learns hidden state vector encapsulate relevant features entire input t−∈rr+m concatenation current input history timestep hcat previous hidden state ∈rm×s differentiable function depending task. however vanilla difﬁculties modeling long-range dependencies vanishing/exploding gradient problem long short-term memories alleviate ∗corresponding authors shaobing <gaoshaobingscu.edu.cn> zhen <hezhen.csgmail.com>. vectors assumed form throughout paper. problems employing memory cells preserve information longer adopting gating mechanisms modulate information ﬂow. given success lstm sequence modeling natural consider increase complexity model thereby increase tasks lstm proﬁtably applied. consider capacity network consist components width depth naive widen lstm increase number units hidden layer; however parameter number scales quadratically number units. deepen lstm popular stacked lstm stacks multiple lstm layers however runtime proportional number layers information input potentially lost propagates vertically layers. paper introduce widen deepen lstm whilst keeping parameter number runtime largely unchanged. summary make following contributions tensorize hidden state vectors higher-dimensional tensors allow ﬂexible based merge deep computations temporal computations network deepened little additional runtime resulting tensorized extend trnn lstm namely tensorized lstm integrates seen parameter number scales quadratically size hidden state. popular limit parameter number widening network organize parameters higher-dimensional tensors factorized lower-rank sub-tensors contain signiﬁcantly fewer elements known tensor factorization. implicitly widens network since hidden state vectors fact broadcast interact tensorized parameters. another common reduce parameter number share small parameters across different locations hidden state similar convolutional neural networks adopt parameter sharing cutdown parameter number rnns since compared factorization following advantages scalability i.e. number shared parameters independent hidden state size separability i.e. information carefully managed controlling receptive ﬁeld allowing shift deep computations temporal domain also explicitly tensorize hidden state vectors since compared vectors tensors better ﬂexibility i.e. specify dimensions share parameters increase size dimensions without introducing additional parameters efﬁciency i.e. higher-dimensional tensors network widened faster w.r.t. depth ﬁxing parameter number ease exposition ﬁrst consider tensors tensorize hidden state ht∈rm become ht∈rp×m tensor size channel size. locally-connect ﬁrst dimension order share parameters fully-connect second dimension allow global interactions. analogous fully-connects dimension globally fuse different feature planes. also compares hidden state stacked akin number stacked hidden layers size hidden layer. start describe model based tensors ﬁnally show strengthen model higher-dimensional tensors. since already deep temporal direction deepen input-to-output computation associating input future output. this need ensure output separable i.e. inﬂuenced future input thus concatenate projection previous hidden state gradually shift input figure examples srnn trnns tlstms. -layer srnn. trnn without feedback connections thought skewed version trnn. tlstm without memory cell convolutions. tlstm. model blank circles column denote hidden state timestep respectively blue region denotes receptive ﬁeld current output outputs delayed timesteps depth. information temporal computation proceeds ﬁnally generate bottom ht+l− number delayed timesteps computations depth example shown fig. fact skewed srnn used however method need change network structure also allows different kinds interactions long output separable increase local connections feedback beneﬁcial srnns order share parameters update using convolution learnable kernel. manner increase complexity input-to-output mapping limit parameter growth rr×m then update tensor implemented convolution h∈rk×m kernel weight size input channels output channels kernel bias rp×m hidden activation convolution operator since kernel convolves across different hidden layers call cross-layer convolution. kernel enables interaction bottom-up top-down across layers. finally generate channel vector ht+l−p located bottom ht+l− ceil operation. derivation please appendix call model deﬁned tensorized model widened increasing tensor size whilst parameter number remains ﬁxed also unlike srnn runtime complexity trnn breaks runtime complexity means either increasing sequence length network depth would signiﬁcantly increase runtime. kernel size input channels output channels rp×m activations content input gate forget gate output gate respectively element-wise sigmoid function rp×m memory cell. however since previous memory cell gated along temporal direction long-range dependencies input output might lost tensor size becomes large. memory cell convolution. capture long-range dependencies multiple directions additionally introduce novel memory cell convolution memory cells larger receptive ﬁeld also dynamically generate convolution kernel timelocation-dependent allowing ﬂexible control long-range dependencies different directions. results tlstm tensor update equations where contrast kernel additional output channels generate activation rp×k dynamic kernel bank qt∈rp×k qtp∈rk vectorized rk×× adaptive kernel location dynamic kernel size single input/output channel reshaped illustration). channel previous memory cell convolved whose values vary forming memory cell convolution rp×m note produces convolved memory cell cconv employ softmax function normalize channel dimension which similar stabilize value memory cells help prevent vanishing/exploding gradients idea dynamically generating network weights used many works locationdependent convolutional kernels also dynamically generated improve cnns. contrast works focus broadening receptive ﬁeld tlstm memory cells. whilst ﬂexibility retained fewer parameters required generate kernel since kernel shared different memory cell channels. channel normalization. improve training adapt layer normalization tlstm. similar observation work well cnns channel vectors different locations different statistics also unsuitable tlstm lower level information near input higher level information near output. cn/ln neglected small compared number parameters model. using higher-dimensional tensors. observe ﬁxing kernel size tensor size tlstm grows linearly w.r.t. depth expand tensor volume rapidly network widened efﬁciently? achieve goal leveraging higher-dimensional tensors. based previous deﬁnitions tlstms replace tensors d-dimensional tensors obtaining ct∈rp×p×...×pd−×m tensor size since hidden states longer matrices concatenate projection corner thus extended hcat concatenated hidden state also increase dimensionality kernel size note reshaped vector illustrated fig. correspondingly generate output opposite corner ht+l− therefore modiﬁed evaluate tlstm challenging sequence learning tasks different conﬁgurations slstm implementation slstm parameters shared across layers. tlstm standard tlstm deﬁned tlstm–m removing memory cell convolutions deﬁned tlstm–f removing feedback connections tlstm tensorizing tlstm. tlstm+ln applying tlstm+cn applying deﬁned compare different conﬁgurations also denote number layers slstm denote hidden size slstm layer. kernel size tlstm–f tlstms case according conﬁguration parameter number increase tensor size performance tlstm boosted without increasing parameter number. also investigate runtime affected depth runtime measured average milliseconds spent forward backward pass timestep single example. next compare tlstm state-of-the-art methods evaluate ability. finally visualize internal working mechanism tlstm. please appendix training details. hutter prize wikipedia dataset consists million characters taken different characters including alphabets markups special symbols. model dataset character-level predict next character input sequence. parameter number corresponding channel sizes slstm tlstm–f tlstms tlstms. conﬁgurations evaluated depths bits-per-character measure model performance. results shown fig. slstm tlstm–f outperform models larger increasing performances slstm tlstm–m improve become saturated tlstms memory cell convolutions improve increasing ﬁnally outperform slstm tlstm–m. tlstm–f surpassed tlstm turn surpassed tlstm. performance tlstm+ln beneﬁts however tlstm+cn consistently improves tlstm different whilst runtime slstm alproportional nearly constant tlstm conﬁguration largely independent compare larger model i.e. tlstm+cn state-of-the-art methods test reported table model achieves parameters competitive best performing methods similar parameter numbers. addition task -digit integers. network ﬁrst reads integers digit timestep predicts summation. follow processing symbol used delimit integers well input/target sequence. -digit integer addition task form evaluate conﬁgurations tasks addition memorization. performance measured symbol prediction accuracy. fig. show results. tasks large degrades performances slstm tlstm– contrast performance tlstm–f steadily improves increasing enhanced using feedback connections higher-dimensional tensors helps note tasks correct solution found repetitive nature task. experiment also observe addition task tlstm+cn outperforms conﬁgurations ﬁnds solution training samples memorization task tlstm+cn beats others conﬁgurations achieves perfect memorization seeing training samples. also unlike slstm runtime tlstms largely unaffected compare best performing conﬁgurations state-of-the-art methods tasks models solve tasks signiﬁcantly faster models achieving state-of-the-art results. mnist dataset consists handwritten digit images size training/validation/test. tasks dataset sequential mnist goal classify digit sequentially reading pixels scanline order therefore timestep sequence learning task single output produced last timestep; task requires long range dependencies sequence. sequential permuted mnist permute original image pixels ﬁxed random order resulting permuted mnist problem even longer range dependencies across pixels harder. tasks conﬁgurations evaluated model performance measured classiﬁcation accuracy. results shown fig. slstm tlstm–m longer beneﬁt increased depth increasing depth tensorization boost performance tlstm. however removing feedback connections tlstm seems affect performance. hand enhances tlstm outperforms tlstm+cn achieves highest performances tasks validation accuracy mnist pmnist. runtime tlstms negligibly affected tlstms become faster slstm figure visualization diagonal channel means tlstm memory cells task. horizontal rows bottom correspond diagonal locations pout columns left right correspond different timesteps values normalized range better visualization. full sequences zoomed horizontally. also compare conﬁgurations highest test accuracies state-of-the-art methods sequential mnist tlstm+cn performs well state-of-the-art dilated model test accuracy sequential pmnist tlstm+cn test accuracy close state-of-the-art produced dilated analysis experimental results different model conﬁgurations different tasks suggest performance tlstms improved increasing tensor size network depth requiring additional parameters little additional runtime. network gets wider deeper found memory cell convolution mechanism crucial maintain improvement performance. also found feedback connections useful tasks sequential output moreover tlstm strengthened tensorization also intriguing examine internal working mechanism tlstm. thus visualize memory cell gives insight information routed. task best performing tlstm random example. record channel mean memory cell timestep visualize diagonal values channel mean location pout visualization results fig. reveal distinct behaviors tlstm dealing different tasks wikipedia input carried output location less modiﬁcation sufﬁcient determine next character vice versa; addition ﬁrst integer gradually encoded memories interacts second integer producing sum; memorization network behaves like shift register continues move input symbol output location correct timestep; sequential mnist network sensitive pixel value change gradually accumulate evidence ﬁnal prediction; sequential pmnist network sensitive high value pixels conjecture permutation destroys topology digit making high value pixel potentially important. fig. also observe common phenomena tasks timestep values different tensor locations markedly different implying wider tensors encode information less effort compress input output values become increasingly distinct shifted time revealing deep computations indeed performed together temporal computations long-range dependencies carried memory cells. figure examples models related tlstms. single layer clstm vector array input. -layer slstm -layer grid lstm -layer -layer qrnn kernel size costly computations done temporal convolution. convolutional lstms. convolutional lstms proposed parallelize computation lstms input timestep structured e.g. vector array vector matrix vector tensor unlike clstms tlstm aims increase capacity lstms input timestep non-structured i.e. single vector advantageous clstms that performs convolution across different hidden layers whose structure independent input structure integrates information bottom-up top-down; clstm performs convolution within hidden layer whose structure coupled input structure thus fall back vanilla lstm input timestep single vector; widened efﬁciently without additional parameters increasing tensor size; clstm widened increasing kernel size kernel channel signiﬁcantly increases number parameters; deepened little additional runtime delaying output; clstm deepened using hidden layers signiﬁcantly increases runtime; captures long-range dependencies multiple directions memory cell convolution; clstm struggles capture long-range dependencies multiple directions since memory cells gated along direction. deep lstms. deep lstms extend slstms making deeper keep parameter number small ease training graves kalchbrenner mujika zilly apply another rnn/lstm along depth direction dlstms which however multiplies runtime. though implementations accelerate deep computation generally simple architectures slstms. compared dlstms tlstm performs deep computation little additional runtime employs cross-layer convolution enable feedback mechanism. moreover capacity tlstm increased efﬁciently using higher-dimensional tensors whereas dlstm hidden layers whole equal tensor dimensionality ﬁxed. parallelization methods. methods parallelize temporal computation sequence training case full input/target sequences accessible. however online inference input presents sequentially temporal computations longer parallelized blocked deep computations timestep making methods potentially unsuitable real-time applications demand high sampling/output frequency. unlike methods tlstm speed training also online inference many tasks since performs deep computation temporal computation also human-like convert signal action meanwhile receive signals non-blocking way. note online inference tasks previous output current input tlstm cannot parallel deep computation since requires delay timesteps yt−. introduced tensorized lstm employs tensors share parameters utilizes temporal computation perform deep computation sequential tasks. validated model variety tasks showing potential popular approaches. yoshua bengio. learning deep architectures foundations trends machine learning luca bertinetto joão henriques jack valmadre philip torr andrea vedaldi. learning feed-forward greg diamos shubho sengupta bryan catanzaro mike chrzanowski adam coates erich elsen jesse engel awni hannun sanjeev satheesh. persistent rnns stashing recurrent weights on-chip. icml jeffrey elman. finding structure time. cognitive science timur garipov dmitry podoprikhin alexander novikov dmitry vetrov. ultimate tensorization marcus hutter. human knowledge compression contest. http//prize.hutter.net ozan irsoy claire cardie. modeling compositionality multiplicative recurrent neural networks. łukasz kaiser samy bengio. active memory replace attention? nips łukasz kaiser ilya sutskever. neural gpus learn algorithms. iclr kalchbrenner danihelka alex graves. grid long short-term memory. iclr diederik kingma jimmy adam method stochastic optimization. iclr krause liang iain murray steve renals. multiplicative lstm sequence modelling. yann lecun bernhard boser john denker donnie henderson richard howard wayne hubbard lawrence jackel. backpropagation applied handwritten code recognition. neural computation zhang. training rnns fast cnns. arxiv preprint arxiv. gundram leifert tobias strauß tobias grüning welf wustlich roger labahn. cells multidimen asier mujika florian meier angelika steger. fast-slow recurrent neural networks. nips alexander novikov dmitrii podoprikhin anton osokin dmitry vetrov. tensorizing neural networks. aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. arxiv preprint arxiv. figure illustration calculating constraint column concatenated hidden state tensor tensor size channel size volume output receptive ﬁeld determined kernel radius output current timestep delayed timesteps. leifert proved lambda gate similar memory cell convolution kernel help prevent vanishing/exploding gradients differences approach lambda gate normalize kernel values though softmax function normalize gate values dividing share kernel channels not. however neither modiﬁcations affects conditions validity theorem memory cell convolution also help prevent vanishing/exploding gradients. number training sequences length n-th training sequence deﬁned softmax function used generate nts=. class distribution ynt. then likelihood calculated used training objective minimized adam learning rate forget gate biases image classiﬁcation tasks others. models implemented torch accelerated cudnn tesla gpus. apply output tlstm hidden state tried different combinations found robust always improve performance tasks. output hidden state becomes split dataset m/m/m training/validation/test. iteration feed model mini-batch subsequences length forward pass hidden values last timestep preserved initialize next iteration. terminate training epochs.", "year": 2017}