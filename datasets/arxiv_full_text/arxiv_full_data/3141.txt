{"title": "Sparse Projections of Medical Images onto Manifolds", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Manifold learning has been successfully applied to a variety of medical imaging problems. Its use in real-time applications requires fast projection onto the low-dimensional space. To this end, out-of-sample extensions are applied by constructing an interpolation function that maps from the input space to the low-dimensional manifold. Commonly used approaches such as the Nystr\\\"{o}m extension and kernel ridge regression require using all training points. We propose an interpolation function that only depends on a small subset of the input training data. Consequently, in the testing phase each new point only needs to be compared against a small number of input training data in order to project the point onto the low-dimensional space. We interpret our method as an out-of-sample extension that approximates kernel ridge regression. Our method involves solving a simple convex optimization problem and has the attractive property of guaranteeing an upper bound on the approximation error, which is crucial for medical applications. Tuning this error bound controls the sparsity of the resulting interpolation function. We illustrate our method in two clinical applications that require fast mapping of input images onto a low-dimensional space.", "text": "abstract. manifold learning successfully applied variety medical imaging problems. real-time applications requires fast projection onto low-dimensional space. out-ofsample extensions applied constructing interpolation function maps input space low-dimensional manifold. commonly used approaches nystr¨om extension kernel ridge regression require using training points. propose interpolation function depends small subset input training data. consequently testing phase point needs compared small number input training data order project point onto low-dimensional space. interpret method out-of-sample extension approximates kernel ridge regression. method involves solving simple convex optimization problem attractive property guaranteeing upper bound approximation error crucial medical applications. tuning error bound controls sparsity resulting interpolation function. illustrate method clinical applications require fast mapping input images onto low-dimensional space. manifold learning maps high-dimensional data low-dimensional manifold recently successfully applied variety applications. speciﬁcally medical imaging manifold learning used segmentation registration computational anatomy classiﬁcation detection respiratory gating best knowledge little work done using manifold learning medical imaging applications require fast projections onto low-dimensional space. paper demonstrate method achieves fast projection input data onto low-dimensional manifold constructing projection function depends small subset training data. method sparse variant kernel ridge regression interpreted interpolation function optimized training data. furthermore construction interpolation function guarantees upper bound interpolation error training data. error measured terms average squared euclidean distance predicted points interpolator versus kernel ridge regression using points. interpolator parametric model data points complexity driven complexity training data bound approximation error. related work out-of-sample extensions. manifold learning speciﬁc case nonlinear dimensionality reduction refers host diﬀerent algorithms medical image analysis manifold learning used construct low-dimensional space images subsequent statistical analysis performed. many manifold learning techniques construct mapping entire input space training points. methods estimating point’s location low-dimensional space performed out-of-sample extension nystr¨om extensions commonly used. certain manifold learning methods nystr¨om extension special case kernel ridge regression nystr¨om extension kernel ridge regression resulting interpolation function mapping input point low-dimensional space depends training data. thus need compare point training data points computationally expensive volumetric images especially number input data used learn manifold large. work similar reduced rank kernel ridge regression also approximates kernel ridge regression using small number input training points. reduced rank kernel ridge regression greedily selects training points minimize particular cost function. speciﬁcally algorithm incrementally adds training point causes largest decrease overall cost. diﬀerent criteria could used greedy procedure terminated pre-speciﬁed desired number training points reached overall cost drops pre-speciﬁed desired error tolerance. importantly medical applications latter criterion directly connected error analysis whole processing pipeline. approach also requires user specify desired error tolerance uses diﬀerent cost function. rather using greedy approach select training points solve convex optimization problem implied cost function. remark proposed cost function also diﬀers support vector regression essentially achieves sparsity excluding training points sufﬁciently close estimated function. cost lenient asking average error small rather asking error small individual training point. finding smallest subset np-hard. instead consider convex relaxation sparsity induced mixed norm. proposed sparse approximation kernel ridge regression used generally multivariate regression tasks restrict focus paper out-of-sample extensions manifold learning. apply method medical imaging applications require fast projection onto low-dimensional space. ﬁrst application respiratory gating ultrasound assign breathing state ultrasound frame acquisition real-time. second application estimation patient’s position magnetic resonance imaging scanner patient moved target location. method builds heavily kernel ridge regression reviewed below. also brieﬂy discuss result nystr¨om extension special case kernel ridge regression certain conditions consequence sparse approximation kernel ridge regression also contains sparse approximation widely used nystr¨om extension. kernel ridge regression. family functions mapping reproducing kernel hilbert space kernel function given points assume exists function noise term kernel ridge regression seeks nystr¨om extension. nystr¨om method approximates certain type eigenfunction problem used out-of-sample extensions manifold learning manifold learning algorithms assign low-dimensional coordinates directly eigenvectors e.g. isomap locally linear embeddings laplacian eigenmaps derive nystr¨om extension special case kernel ridge regression speciﬁcally eigendecomformula low-dimensional embedding using nystr¨om extension importantly kernel function depends choice manifold learning algorithm relationship shows certain manifold learning algorithms kernel ridge regression richer model out-of-sample extensions nystr¨om extension. present method. seek interpolation function within family functions decision variable; solving problem yields implies sparse approximation kernel ridge regression solution noting thatn frobenius norm given fact given i-th rows respectively. thus bound rewritten minimizing mixed norm encourage vector either consist zeros non-zero entries note instead sparsest solution possible objective function becomes norm optimization problem becomes np-hard solve optimization problem reduce solving many instances unconstrained lagrangian form already fast solver. specifically lagrangian duality convexity solving optimization problem equivalent solving dual problem ﬁxed compute eﬃciently using fast iterative shrinkagethresholding algorithm moreover standard result lagrangian duality dual problem maximizes concave function case scalar variable thus eﬃciently solve right hand side making many calls fista needed achieve desired number nonzero vectors depends error tolerance regulardata points corresponding nonzero support vectors. observe potentially candidate solutions meanwhile coeﬃcient matrixα kernel ridge regression deﬁned approachesα goes large result also gets pushed empirically next section decreasing parameters generally produce support vectors used projection. surprising increasing increases size feasible optimization problem allowing choose similarity kernel match speciﬁc choice manifold learning algorithm used embed training data. allows provide sparse approximation nystr¨om extension discussed section alternatively method applicable kernel regardless manifold learning algorithm used training. whose computational cost directly proportional number support vectors. interpolator always least fast compute kernel ridge regression uses training points support vectors corresponds special case interpolator apply sparse interpolator synthetic data respiratory gating ultrasound classiﬁcation. report number support vectors proxy computational speed since wall-clock time directly proportional number support vectors. furthermore datasets still relatively small scenarios method intends address making wall-clock time experiments reﬂective real use. however empirical results suggest method work larger datasets since number support vectors scales size training dataset instead complexity training data’s low-dimensional embedding. synthetic data hessian eigenmaps manifold learning which best knowledge known nystr¨om extension. experiments real data laplacian eigenmaps manifold learning construct sparse interpolator using kernel function used laplacian eigenmap’s nystr¨om extension neighbor threshold parameters chosen based application interest. also nearest neighbors rather deﬁning nearest neighbors within ball radius kernel function constructing sparse interpolator yields laplacian eigenmap’s nystr¨om extension uses training points. manifold learning algorithm datasets; choice manifold learning algorithm depends dataset application interest. sian eigenmaps -nearest-neighbor graph. construct sparse /σ). probe behavior interpolator vary kernel ridge regression parameter kernel width error tolerance fig. reports resulting number support vectors illustrates results setting parameters. fig. results swiss roll points original data points; embedding; number support vectors function error tolerance various remaining panels support vectors found; approximated embedding support vectors; comparison embeddings method kernel ridge regression observe support vectors uniformly sampled input space learned manifold. instead appear along boundaries form skeleton within learned manifold. also observe fig. largest discrepancies predicted point locations sparse interpolator kernel ridge regression occur along boundaries. unsurprisingly increasing kernel width reduces number support vectors needed achieve error tolerance support vector broader spatial inﬂuence input space. furthermore increasing kernel ridge regression regularization parameter also reduces number support vectors discussed section repeating experiment using swiss roll points empirically variety parameter settings number support vectors remains roughly constant grows large. example obtain support vectors points respectively. suggests number support vectors depend low-dimensional embedding’s complexity dataset size fig. ultrasound gating. ultrasound images liver time bottom left correlation coeﬃcient error tolerance bottom right number support vectors error tolerance ﬁgures bottom report results diﬀerent values kernel ridge regression regularization parameter respiratory gating tracks patient’s breathing cycle numerous applications imaging radiation therapy image mosaicing manifold learning used highly accurate respiratory gating ultrasound images data reconstruction achieved retrospective gating i.e. gating calculated data acquisition ﬁnished. extend work attain real-time gating. small number breathing cycles acquired used input manifold learning construct respiratory signal done retrospective gating. incoming stream ultrasound images gated performing out-of-sample extension. frames captured given image sequence image sequence input data point learning manifold laplacian eigenmaps -nearest-neighbor graph associated heat kernel temperature embedding learned using entire sequence images serves reference signal evaluating sparse out-of-sample extension versus kernel ridge regression baseline. follows compare embedding sparse out-of-sample extension reference signal computing correlation coeﬃcient them. kernel ridge table results respiratory gating ultrasound images. image sequence show number frames contains correlation coeﬃcient kernel ridge regression sparse interpolator number support vectors parameter values regression baseline method. train ﬁrst frames test remaining frames. compare results obtained training frames would done retrospective gating. ﬁrst examine inﬂuence parameters resulting interpolator. training ﬁrst images ultrasound image sequences compute correlation coeﬃcient reference signal number support vectors versus error tolerance expected smaller error tolerance requires support vectors also leads higher correlation coeﬃcient respect reference signal. also higher kernel ridge regression regularization parameter leads fewer support vectors. however stronger regularization also leads lower correlation coeﬃcients. results suggest natural tradeoﬀ accuracy computational cost projection operation. next experiment training ﬁrst frames testing rest frames report correlation coeﬃcients number support vectors table number support vectors kernel ridge regression case. repeat experiment training frames. case number support vectors kernel ridge regression length sequence. achieve high correlation sequences comparable performance sparse interpolator kernel ridge regression. comparing number support vectors training ﬁrst frames training frames note number support vectors stays roughly given image sequence. suggests number support vectors depends low-dimensional embedding’s complexity training size. radio frequency power magnetic resonance imaging leads tissue heating monitored measuring speciﬁc absorption rate depends position patient scanner. current high-resolution scanners imposes restrictions either fewer slices acquired fig. leave-one-out classiﬁcation results data classiﬁcation rate sparse out-of-sample extension kernel ridge regression number support vectors classiﬁcation rate; number support vectors error tolerance ﬁgures report results diﬀerent values kernel ridge regression regularization parameter first low-resolution images acquired patient lies moves inside scanner. images embedded low-dimensional space axial image associated body part using nearest-neighbor classiﬁer. knowing slices correspond body parts estimate position patient scanner. important estimation done real-time provide position information high-resolution scan starts. application apply manifold learning oﬄine large database scans. actual scan out-of-sample extension project acquired slices low-dimensional space. large training datasets diﬃcult meet time requirements kernel ridge regression. consequently reduction small support vectors oﬀers substantial advantage. dimensional space; -nearest-neighbor graph heat kernel temperature predict anatomical label axial image perform nearest-neighbor classiﬁcation learned low-dimensional space. repeat classiﬁcation procedure diﬀerent values error tolerance ranging compare classiﬁcation performance embeddings obtained sparse interpolator kernel ridge regression. fig. reports leave-one-out classiﬁcation performance diﬀerent values error tolerance classiﬁcation rates kernel ridge regression provided comparison; change diﬀerent values figs. characterize sparsity interpolation function constructed reporting number support vectors function classiﬁcation rate error tolerance total number frames used experiment corresponds number support vectors kernel ridge regression. observe clear correlation error tolerance classiﬁcation performance. smaller values lead better classiﬁcation performance require support vectors. thus trade computational speed classiﬁcation performance tuning parameters large possible maintaining classiﬁcation rate minimum tolerated threshold. derived novel method multivariate regression approximates kernel ridge regression ﬁnal estimated interpolation function depends subset original input points acting support vectors. approach provides guarantee approximation error training data. applied method out-of-sample extension manifold learning illustrating applications respiratory gating classiﬁcation. turning toward nonlinear dimensionality reduction generally many widely used algorithms computationally expensive massive datasets. thus ideally would like support vectors ﬁrst applying dimensionality reduction. results suggest support vectors interpolation uniformly sampled input space. invites question non-uniformly sample training data input space adjust dimensionality reduction algorithm accordingly account geometry samples.", "year": 2013}