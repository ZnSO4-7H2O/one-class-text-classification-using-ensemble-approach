{"title": "Ternary Neural Networks for Resource-Efficient AI Applications", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limits their deployability on ubiquitous computing devices such as smart phones, wearables and autonomous drones. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach based on a novel, layer-wise greedy methodology. Thanks to our two-stage training procedure, the teacher network is still able to use state-of-the-art methods such as dropout and batch normalization to increase accuracy and reduce training time. Using only ternary weights and activations, the student ternary network learns to mimic the behavior of its teacher network without using any multiplication. Unlike its -1,1 binary counterparts, a ternary neural network inherently prunes the smaller weights by setting them to zero during training. This makes them sparser and thus more energy-efficient. We design a purpose-built hardware architecture for TNNs and implement it on FPGA and ASIC. We evaluate TNNs on several benchmark datasets and demonstrate up to 3.1x better energy efficiency with respect to the state of the art while also improving accuracy.", "text": "abstract—the computation storage requirements deep neural networks usually high. issue limits deployability ubiquitous computing devices smart phones wearables autonomous drones. paper propose ternary neural networks order make deep learning resource-efﬁcient. train tnns using teacher-student approach based novel layer-wise greedy methodology. thanks two-stage training procedure teacher network still able state-of-the-art methods dropout batch normalization increase accuracy reduce training time. using ternary weights activations student ternary network learns mimic behavior teacher network without using multiplication. unlike binary counterparts ternary neural network inherently prunes smaller weights setting zero training. makes sparser thus energy-efﬁcient. design purpose-built hardware architecture tnns implement fpga asic. evaluate tnns several benchmark datasets demonstrate better energy efﬁciency respect state also improving accuracy. deep neural networks achieved state-of-theart results wide range tasks including computer vision speech recognition natural language processing dnns become complex number layers number weights computational cost increase. dnns generally trained powerful servers support gpus used classiﬁcation tasks variety hardware. however networks bigger deployability autonomous mobile devices drones self-driving cars mobile phones diminishes extreme hardware resource requirements imposed high number synaptic weights ﬂoating point multiplications. goal paper obtain dnns able classify high throughput low-power devices without compromising much accuracy. recent years main directions research explored reduce cost dnns classiﬁcations. ﬁrst preserves ﬂoating point precision dnns drastically increases sparsity weights sharing compression advantage signiﬁcantly diminishing memory power consumption preserving accuracy. however power savings limited need ﬂoating-point operation. second direction reduces need ﬂoatingpoint operations using weight discretization extreme cases binary neural networks completely eliminating need multiplications main drawback approaches signiﬁcant degradation propose teacher-student approach obtaining ternary weights activations constrained teacher network trained stochastic ﬁring using back-propagation beneﬁt techniques exist literature dropout batch normalization convolutions student network architecture neuron mimics behavior equivalent neuron teacher network without using multiplications design specialized hardware able process tnns better throughput better energy efﬁciency better area efﬁciency state-ofthe-art competitive accuracy make training code publicly available provide demonstration hardware design tnns using fpga. rest paper organized follows. following section introduce procedure training ternary detailing teacher-student paradigm eliminate need multiplications altogether test time still beneﬁting state-of-the-art techniques batch normalization dropout training. section provide survey related works compare performance with. present experimental evaluation ternarization classiﬁcation performance different benchmark datasets section section describe purpose-built hardware able handle fully connected multi-layer perceptrons convolutional high throughput low-energy budget. finally conclude discussion future studies section two-stage teacher-student approach obtaining tnns. first train teacher network stochastically ﬁring ternary neurons. then student network learn imitate teacher’s behavior using layer-wise greedy algorithm. teacher student networks architecture. student network’s weights ternarized version teacher network’s weights. student network uses step function thresholds activation function. table provide notations descriptions. order emphasize difference denote discrete valued parameters inputs bold font. real-valued parameters denoted normal font. denote matrix vector. subscripts enumeration purposes superscripts differentiation. deﬁned output neuron teacher network output neuron student network. detail stages following subsections. teacher network architecture number neurons trained using standard training algorithms. train teacher network single constraint only stochastically ﬁring ternary neurons output values beneﬁt approach technique already exists efﬁcient training batch normalization dropout etc. order ternary output teacher neuron denoted stochastic ﬁring step activation step. achieving stochastically tanh hard tanh soft-sign activation function teacher network neuron output range ternarization. range determine ternary output neuron described table although require restrictions weights teacher network several studies showed regularization effect reduces over-ﬁtting approach compatible regularization technique well. teacher network trained begin training student network. goal student network predict output teacher real-valued network. since architecture networks one-to-one correspondence neurons both. learns mimic behavior student neuron denoted corresponding teacher neuron individually independently neurons. order achieve this student neuron uses corresponding teacher neuron’s weights guide determine ternary weights using thresholds teacher neuron’s weights. step called weight ternarization. order ternary neuron output step activation function thresholds output ternarization step determines these. figure depicts ternarization procedure sample neuron. plot distributions weights activations ternary output sample neuron teacher network respectively. student neuron’s weight distribution determined plotted teacher’s weight distribution. transfer function output student neuron grouped according teacher neuron’s output input determine thresholds step activation function. resulting output distribution teacher student neurons similar. following subsections detail step. output ternarization student network uses twothresholded step activation function ternary output described table output ternarization ﬁnds step ternary activation function’s thresholds neuron given ternary weights order achieve this compute three different transfer function output distributions student neuron using teacher neuron’s ternary output value input. denote transfer function outputs student neuron teacher neuron’s output value deﬁned teacher neuron output values respectively. simple classiﬁer boundaries three clusters student neuron transfer function outputs boundaries thresholds step activation function. classiﬁcation done using linear discriminant kernel density estimates three distributions. discriminant selected discriminant gives weight ternarization weight ternarization order sign teacher network’s weights preserved. ternarize weights neuron teacher network using thresholds max. weights student neuron obtained weight ternarization follows student neuron. using threshold output score student neuron selectively exhaustive search subset neurons. empirically cases rare. provide detailed analysis section iv-a. ternarization output layer slightly different since soft-max classiﬁer. ternarization process instead using teacher network’s output actual labels training set. again treat neurons independently make several iterations output neuron round-robin fashion. iteration check convergence. experiments observed method converges passes neurons. layer-wise approach allows update weights teacher network ternarization layer. optional weight update staggered retraining approach non-ternarized layers modiﬁed. teacher network’s weights updated input layer teacher student networks become equal early stopping optional retraining dozen iterations sufﬁce. section give brief survey several related works energy-efﬁcient nns. table provide comparison approach related works binary ternary weights deployment phase summarizing constraints inputs weights activations training testing. courbariaux propose binaryconnect method binarizing weights leaving inputs activations real-values. idea also used ternaryconnect ternary weight networks ternary weights instead binary back-propagation algorithm additional weight binarization step. forward pass weights binarized either deterministically using sign stochastically. stochastic binarization converts realvalued weights probabilities hard-sigmoid function decides ﬁnal value weights this. back-propagation phase quantization mechanism used multiplication operations converted bitshift operations. binarization scheme helps reducing number multiplications training testing fully hardware-friendly since reduces number ﬂoating point multiplication operations. recently idea extended activations neurons also binarized sign activation function used obtaining binary neuron activations. also shift-based operations used training test time order gain energyefﬁciency. although method helps improving efﬁciency multiplications eliminate completely. xnor-nets provide solution convert convolution operations cnns bitwise operations. method ﬁrst learns discrete convolution together real-valued scaling factors convolution calculations handled using bit-wise operations scaling factors optimal threshold values weights evaluating ternarization quality score function. given neuron positive weights negative weights total number possible ternarization schemes since respect original sign order weights. given conﬁguration positive negative threshold values given neuron calculate following score assessing performance ternary network mimicking original network. input sample teacher neuron input input sample student neuron. note ﬁrst layer. since ternarize network feed-forward manner order prevent ternarization errors propagating upper layers always teacher’s original input determine output probability distribution. output probability distribution teacher neuron input calculated using stochastic ﬁring described table output probability distribution student neuron input calculated using ternary weights current conﬁguration step activation function thresholds. thresholds selected according current ternary weight conﬁguration output probability values accumulated scores input samples output student neuron matches output teacher neuron. optimal ternarization weights determined selecting conﬁguration maximum score. algorithm ow). propose using greedy dichotomic search strategy instead fully exhaustive one. make search grid candidate values values thi. select equally-spaced pivot points along dimensions using pivot points calculate maximum score along axis. reduce search space selecting region maximum point lies. since points reduce search space twothirds step. then repeat search procedure reduced search space. faster strategy runs local maxima guaranteed optimal solution. multiple local extremum stuck. fortunately detect possible suboptimal solutions using score values obtained following goal dorefa-net quantized neural networks propose using n-bit quantization weights activations well gradients. possible gain speed energy efﬁciency extent training also inference time. combine several techniques achieve quantization compression weights setting relatively unimportant ones also develop dedicated hardware called efﬁcient inference engine exploits quantization sparsity gain large speed-ups energy savings fully connected layers currently soudry propose expectation backpropagation algorithm learning weights binary network using variational bayes technique. algorithm used train network that weight restricted binary ternary values. strength approach training algorithm require tuning hyper-parameters learning rate standard back-propagation algorithm. also neurons middle layers binary making hardware-friendly. however approach assumes bias real currently applicable cnns. methods described partially discretized leading reduction number ﬂoating point multiplication operations. order completely eliminate need multiplications result maximum resource efﬁciency discretize network completely rather partially. extreme limitations studies exist literature. smaragdis propose bitwise completely binary approach inputs weights outputs binary. straightforward extension back-propagation learn bitwise network’s weights. first real-valued network trained constraining weights network using tanh. also tanh non-linearity used activations constrain neuron output then second training step binary network trained using real-valued network together global sparsity parameter. epoch forward propagation weights activations binarized using sign function original announced energy efﬁcient truenorth chip designed spiking neural network architectures esser propose algorithm training networks compatible truenorth chip. algorithm based backpropagation modiﬁcations. first gaussian approximation used summation several bernoulli neurons second values clipped satisfy boundary requirements truenorth chip. ternary weights obtained introducing synaptic connection parameter determines whether connection exits. connection exists sign weight used. recently work extended architectures well main goals experiments demonstrate performance ternarization procedure respect real-valued teacher network classiﬁcation performance fully discretized ternary networks. perform experiments several benchmarking datasets using multi-layer perceptrons permutation-invariant manner convolutional neural networks varying sizes. mlps experiment different architectures terms depth neuron count. neurons layer layer networks. cnns following vgg-like architecture proposed lsvm convolutional layer maxpooling layer fully connected layer. lsvm output layer. different network sizes architecture call networks cnn-small cnn-big respectively. perform experiments following datasets mnist database handwritten digits well-studied database benchmarking methods real-world data. mnist training examples test examples gray-scale images. last samples training validation early stopping model selection. cifar- cifar- color-image classiﬁcation datasets contain images. dataset consists images training images test sets. cifar- images come classes contain airplanes automobiles birds cats deer dogs frogs horses ships trucks. cifar- number image classes svhn consists color images digits cropped street view images. total training size examples test contains images. gtsrb composed images german road signs classes. images great variability terms size illumination conditions. also dataset unbalanced class frequencies. images dataset extracted -second video sequences recorded fps. order representative validation extract track random trafﬁc sign validation. number images train validation test respectively. order allow fair comparison related works perform experiments similar conﬁgurations. mnist mlps. minimize cross entropy loss using stochastic gradient descent mini-batch size training random rotations degrees. report test error rate associated best validation error rate epochs. perform preprocessing mnist threshold-based binarization input. datasets architectures cnnsmall cnn-big. before train teacher network obtaining student tnn. teacher network modiﬁed version binarized nn’s algorithm ternarize weights training. obtain better accuracy teacher network gain considerable speed-up obtaining student network. since already discretized weights teacher network training mimic output neurons using step activation function thresholds student network. teacher network training minimize squared hinge loss adam mini-batches size train epochs report test error rate associated best validation epoch. input binarization approach described either transduction ﬁlters. augmentation datasets. ternarization performance ability student network imitate behavior teacher. measure using accuracy difference teacher network student network. table shows difference teacher student networks training test sets three different exhaustive search threshold values. corresponds fully exhaustive search case whereas represents fully dichotomic search. results show ternarization performance better deeper networks. since always teacher network’s original output reference errors ampliﬁed network. contrary deeper networks allow student network correct mistakes upper layers dampening errors. also perform retraining step early stopping ternarizing layer since slightly improves performance. ternarization performance generally decreases lower threshold values decrease marginal. occasion performance even increases. teacher network’s weight update allows network escape local minima. order demonstrate effect terms run-time classiﬁcation performance conduct detailed analysis without optional staggered retraining. figure shows distribution ratio neurons ternarized exhaustively different together performance gaps training test datasets. optimal trade-off achieved exhaustive search used neurons expected value accuracy gaps practically largest layer neurons ternarization operations take dichotomic exhaustive search respectively -core intel xeon .ghz server ram. output layer ternarization time reduced exhaustive search. classiﬁcation performance terms error rate several benchmark datasets provided table compare results several related methods described previous section. make distinction fully discretized methods partially discretized ones latter resulting network completely discrete requires ﬂoating points multiplications providing maximum energy efﬁciency. since benchmark datasets studied ones several known techniques tricks give performance boosts. order eliminate unfair comparison among methods follow majority’s lead data augmentation experiments. moreover using ensemble classiﬁers common technique performance boosting almost classiﬁers unique neural networks reason ensemble networks cite compatible results related works. mnist studied dataset deep learning literature. state-of-the-art already erroneous classiﬁcations extremely difﬁcult obtain without extensive data augmentation. tnn’s error rate mnist single -layer neurons layer. bitwise neurons layers achieves slightly better performance. architecture similar size bitwise worse over-ﬁtting. since selects different sparsity level neuron perform better smaller networks larger networks cause over-ﬁtting mnist. bitwise nn’s global sparsity parameter better regularization effect mnist relatively bigger networks. performance smaller networks datasets unknown. truenorth single network achieves error rate. alleviate limitations single network performance committee networks used reducing error rate networks. error rate cifar compared partially discretized alternatives fully discretized obtained cost points accuracy exceeds performance truenorth svhn similar achievement lower margins. cifar hand perform better truenorth. given relatively lower number related works report results cifar opposed cifar conclude challenging dataset resourceefﬁcient deep learning room improvement. remarkable performance gtsrb dataset. error rate cnn-big model exceeds human performance partially discretized approaches real-valued input contains information. therefore expected able higher classiﬁcation accuracy. compared partially discretized studies tnns lose small percentage accuracy return provide better energy efﬁciency. next describe unique hardware design tnns investigate extent tnns area energy efﬁcient. designed hardware architecture tnns optimized ternary neuron weights activation values section ﬁrst describe purpose-built hardware designed evaluate performance terms latency throughput energy area efﬁciency. figure outlines hardware architecture fullyconnected layer multi-layer design forms pipeline corresponds sequence processing steps. efﬁciency reasons number layers maximum layer dimensions decided synthesis time. given architecture design still user-programmable layer contains memory programmed run-time neuron weights output ternarization thresholds bhi. seen previous experiments section given architecture reused different datasets success. ternary values represented bits using usual two’s complement encoding. compute part neuron reduced integer adder/subtractor register width bits input size neuron. neuron tens asic gates small. inside layer neurons work parallel input item processed clock cycle. layers pipelined order simultaneously work different sets inputs i.e. layer processes image layer processes image ternarization block processes neuron outputs sequentially consists memory threshold values signed comparators multiplexer. generic register transfer level implementation synthesized field-programmable gate array application-speciﬁc integrated circuit technologies. fpgas reprogrammable off-theshelf circuits ideal general-purpose hardware acceleration. typically high-performance cloud solutions high-end fpga tightly coupled general-purpose multicore processors asic used throughput battery-powered embedded devices. preliminary measurements used dataset mnist fpga board sakura-x features precise power measurements capabilities. accommodate -layer fully connected neurons layer using kintex- fpga. performance fpga design terms latency throughput energy efﬁciency given table clock frequency throughput images/s power consumption classiﬁcation latency know truenorth operate extremes power consumption accuracy. consumes network accuracy consumes high committee networks achieves hardware cannot operate extremes middle operating zone outperform truenorth terms energy-efﬁciency accuracy trade-off speed. truenorth consumes image accuracy throughput images/s latency. hardware consuming image achieves accuracy rate images/s latency rest fpga experiments larger board equipped virtex- fpga used support much larger designs. also synthesized design asic using stmicroelectronics fdsoi manufacturing technology. results given table compare fpga asic solutions state truenorth asic version compares well truenorth throughput area efﬁciency energy efﬁciency even though uses -bit precision achieves high throughput takes advantage weight sparsity skips many useless computations. however achieve better energy area efﬁciencies since hardware elements signiﬁcantly reduced thanks ternarization. energy results would even better taking account weight sparsity zeroactivations like done works. finally implemented cnn-big cnn-small described section fpga asic. results given table vii. give worst-case fpga results important users general-purpose hardware accelerators. asic technology took account perdataset zero-activations reduce power consumption similar done works. compare truenorth paper gives ﬁgures merit related cnns asic. truenorth area calculated according number cores used. using different models truenorth’s achieve better accuracy three datasets four higher throughput better energy efﬁciency much better area efﬁciency. study propose tnns resource-efﬁcient applications deep learning. energy efﬁciency area efﬁciency brought using multiplication ﬂoating-point operation. develop student-teacher approach train tnns devise purpose-built hardware making available embedded applications resource constraints. experimental evaluation demonstrate performance tnns terms accuracy resource-efﬁciency cnns well mlps. related work features truenorth since bitwise support cnns terms accuracy tnns perform better truenorth relatively smaller networks benchmark datasets except one. unlike truenorth bitwise tnns ternary neuron activations using step function thresholds. allows neuron choose sparsity parameter gives opportunity remove weights little contribution. respect tnns inherently prune unnecessary connections. also develop purpose-built hardware tnns offers signiﬁcant throughput area efﬁciency highly competitive energy efﬁciency. compared truenorth asic hardware offers improvements area efﬁciency energy efﬁciency throughput. also often higher accuracy training approach. project funded part grenoble alpes métropole nano esprit project. authors would like thank olivier menut microelectronics valuable inputs continuous support. hertel barth käster martinetz. deep convolutional neural networks generic feature extractors. international joint conference neural networks pages july pedram horowitz dally. efﬁcient inference engine compressed deep neural network. acm/ieee annual international symposium computer architecture esser appuswamy merolla arthur modha. backpropagation energy-efﬁcient neuromorphic computing. advances neural information processing systems pages esser merolla arthur cassidy appuswamy andreopoulos berg mckinstry melano barch nolfo datta amir taba flickner modha. convolutional networks fast energy-efﬁcient neuromorphic computing. proceedings national academy sciences ioffe szegedy. batch normalization accelerating deep network proceedings training reducing internal covariate shift. international conference machine learning pages courbariaux bengio david. binaryconnect training deep neural networks binary weights propagations. advances neural information processing systems pages soudry hubara meir. expectation backpropagation parameter-free training multilayer neural networks continuous discrete weights. advances neural information processing systems pages merolla arthur alvarez-icaza cassidy sawada akopyan jackson imam nakamura brezzo esser appuswamy taba amir flickner risk manohar modha. million spiking-neuron integrated circuit scalable communication network interface. science netzer wang coates bissacco reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning stallkamp schlipsing salmen igel. computer benchmarking machine learning algorithms trafﬁc sign recognition. international joint conference neural networks cheng soudry lan. training binary multilayer neural networks image classiﬁcation using expectation backpropagation. choi cong fang reinman wei. quantitative analysis microarchitectures modern cpu-fpga platforms. proceedings design automation conference pages", "year": 2016}