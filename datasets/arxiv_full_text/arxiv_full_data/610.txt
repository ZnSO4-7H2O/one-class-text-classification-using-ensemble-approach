{"title": "Semi-supervised Multitask Learning for Sequence Labeling", "tag": ["cs.CL", "cs.LG", "cs.NE", "I.5.1; I.2.6; I.2.7"], "abstract": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the system to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on a range of datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.", "text": "propose sequence labeling framework secondary training objective learning predict surrounding words every word dataset. language modeling objective incentivises system learn general-purpose patterns semantic syntactic composition also useful improving accuracy different sequence labeling tasks. architecture evaluated range datasets covering tasks error detection learner texts named entity recognition chunking pos-tagging. novel language modeling objective provided consistent performance improvements every benchmark without requiring additional annotated unannotated data. accurate efﬁcient sequence labeling models wide range applications including named entity recognition part-of-speech tagging error detection shallow parsing. specialised approaches sequence labeling often include extensive feature engineering integrated gazetteers capitalisation features morphological information tags. however recent work shown neural network architectures able achieve comparable improved performance automatically discovering useful features speciﬁc task requiring sequence tokens input learn general language features available text. many sequence labeling tasks relevant labels dataset sparse words contribute little training process. example conll dataset tokens represent entity. ratio even lower error detection tokens annotated error dataset sequence labeling models able learn bias label distribution without obtaining much additional information words majority label therefore propose additional training objective allows models make extensive available data. task language modeling offers easily accessible objective learning predict next word sequence requires plain text input without relying particular annotation. neural language modeling architectures also many similarities common sequence labeling frameworks words ﬁrst mapped distributed embeddings followed recurrent neural network module composing word sequences informative context representation compared sequence labeling dataset task language modeling considerably larger varied possible options predict making better available word encouraging model learn general language features accurate composition. paper propose neural sequence labeling architecture also optimised language model predicting surrounding words dataset addition assigning labels token. speciﬁc sections network oporder predict label token either softmax output architecture. softmax model directly predicts normalised distribution possible labels every word conditioned vector possible labels k-th output weight matrix model optimised minimising categorical crossentropy equivalent minimising negative log-probability correct labels architecture returns predictions based words input labels still predicted independently. tasks named entity recognition scheme strong dependencies subsequent labels beneﬁcial explicitly model connections. output architecture modiﬁed include conditional random field allows network look optimal path possible label sequences model optimised maximising score correct label sequence minimising scores sequences also make character-level component described builds alternative representation word. individual characters word mapped character embeddings passed bidirectional lstm. last hidden states direction concatenated passed timised forwardbackward-moving language model label predictions performed using context directions. secondary unsupervised objective encourages framework learn richer features semantic composition without requiring additional training data. evaluate sequence labeling model datasets ﬁelds pos-tagging chunking error detection learner texts. experiments show including unsupervised objective training process sequence labeling model achieves consistent performance improvements benchmarks. multitask training framework gives largest improvements error detection datasets outperforming previous state-of-the-art architecture. neural network model baseline architecture sequence labeling experiments. model takes input sentence separated tokens assigns label every token using bidirectional lstm. tokens ﬁrst mapped sequence distributed word embeddings lstm components moving opposite directions sentence used constructing context-dependent representations every word. lstm takes input hidden state previous time step along word embedding current step outputs hidden state. hidden representations directions concatenated order obtain context-speciﬁc representation word conditioned whole sentence directions next concatenated representation passed feedforward layer mapping components joint space allowing model learn features based context directions figure unfolded network structure sequence labeling model additional language modeling objective performing sentence fischler proposes measures. input tokens shown bottom expected output labels top. arrows variables indicate directionality component straightforward modiﬁcation sequence labeling model would second parallel output layer token optimising predict next word. however model access full context side target token predicting information already explicit input would incentivise model learn composition semantics. therefore must design loss objective sections model observed next word optimised perform prediction. solve predicting next word sequence based hidden representation forward-moving lstm. similarly previous word sequence predicted based backward-moving lstm. architecture avoids problem giving correct answer input language modeling component full framework still optimised predict labels based whole sentence. weight matrices. separate transformation learns extract features speciﬁc language modeling lstm optimised objectives. also opportunity representation smaller size since language modeling nonlinear layer. resulting vector representation combined regular word embedding using dynamic weighting mechanism adaptively controls balance word-level character-level features. framework allows model learn character-based patterns handle previously unseen words still taking full advantage word embeddings. sequence labeling model section optimised based correct labels. token input desired label many contribute little training process. example conll dataset possible labels tokens label indicating named entity detected. ratio even higher error detection tokens containing errors dataset sequence labeling models able learn bias label distribution without obtaining much additional information majority labels. therefore propose supplementary objective would allow models make full training data. addition learning predict labels word propose optimising speciﬁc sections architecture language models. task predicting next word require model learn general patterns semantic syntactic composition reused order predict individual labels accurately. objective also generalisable requirements model sequence labeling equal baseline section since language modeling components ignored testing additional weight matrices used. implementation uses basic softmax output layer language modeling components efﬁciency training could improved integrating noise-contrastive estimation hierarchical softmax proposed architecture evaluated different sequence labeling datasets covering tasks error detection chunking postagging. word embeddings model initialised publicly available pretrained vectors created using wordvec general-domain datasets used -dimensional embeddings trained google news. biomedical datasets word embeddings initialised -dimensional vectors trained pubmed pmc. neural network framework implemented using theano make code publicly available online. hyperparameters follow settings order facilitate direct comparison previous work. lstm hidden layers size direction wordcharacter-level components. digits text replaced character words occurred training data replaced token. order reduce computational complexity experiments language modeling objective predicted frequent words extra token covering words. sentences grouped batches size parameters optimised using adadelta default learning rate training stopped performance development improved epochs. performance development also used select best model evaluated test set. order avoid outlier results randomness model initialisafigure shows diagram unfolded neural architecture performing short sentence words. token position network optimised predict previous word current label next word sequence. added language modeling objective encourages system learn richer feature representations reused sequence optimised predict labeling. example proposes next word indicating current word subject possibly named entity. optimised predict fischler addition previous word features used input predict table precision recall score alternative sequence labeling architectures error detection datasets. dropout lmcost modiﬁcations added incrementally baseline. tion conﬁguration trained different random seeds averaged results presented paper. previously established splits training development testing datasets. based development experiments found error detection task beneﬁt module output layer since labels sparse dataset contains possible labels explicitly modeling state transitions improve performance task. therefore softmax output error detection experiments datasets. publicly available sequence labeling system used baseline. development found applying dropout word embeddings improves performance nearly datasets compared baseline. therefore elementwise dropout applied input embeddings probability training weights multiplied testing. order separate effects modiﬁcation newly proposed optimisation method report results three different systems publicly available baseline applying dropout baseline system applying dropout novel multitask objective section based development experiments value controls importance language modeling objective experiments throughout training. since context prediction part main evaluation sequence labeling systems expected additional objective mostly beneﬁt early stages training whereas model would later need specialise towards assigning labels. therefore also performed experiments development data value gradually decreased found small static value performed comparably well even better. experiments indicate language modeling objective helps network learn general-purpose features useful sequence labeling even later stages training. ﬁrst evaluate sequence labeling architectures task error detection given sentence written language learner system needs detect tokens manually tagged annotators error. ﬁrst benchmark publicly released first certiﬁcate english dataset containing manually annotated sentences. texts written learners language examinations response prompts eliciting free-text answers assessing mastery upper-intermediate proﬁciency level. addition evaluate conll shared task dataset converted error detection task. contains sentences written higher-proﬁciency learners technical topics. manually corrected separate annotators report results annotations. datasets training model optimisation results conll- dataset indicate outof-domain performance. yannakoudakis present results datasets using setup along evaluating shared task submissions task error detection. main evaluation metric measure consistent previous work also adopted conll- shared task. table contains results three different sequence labeling architectures error detection datasets. found including dropout actually decreases performance setting error detection likely relatively small amount error examples available dataset better model memorise without introducing additional noise form dropout. however verify dropout indeed gives improvement combination novel language modeling objective. model receiving additional information every token dropout longer obscuring limited training data instead helps generalisation. bottom shows performance language modeling objective added baseline model along dropout word embeddings. architecture outperforms baseline benchmarks increasing precision recall giving absolute improvement test set. results also improve previous best results yannakoudakis systems trained dataset. added components also require computation time difference excessive training batch dataset processed seconds baseline system seconds using language modeling objective. task sparse labels datasets error tokens infrequent apart. without language modeling objective network little available words contain errors. possible labels correct incorrect likely makes difﬁcult model learn feature detectors many different error types. language modeling uses much larger number pos. finally task error detection directly related language modeling. learning better model overall text training corpus system easily detect irregularities. also analysed performance different architectures training. figure shows score development model every epoch training data. baseline model peaks quickly followed gradual drop performance likely overﬁtting available data. dropout provides effective regularisation method slowing initial performance preventing model overﬁtting. added language modeling objective provides substantial improvement system outperforms conﬁgurations already early stages training results also sustained later epochs. english section conll corpus containing news stories reuters corpus. also report results biomedical datasets biocreative chemical drug corpus abstracts annotated mentions chemical drug names jnlpba corpus abstracts annotated mentions different cells proteins. finally conll dataset created wall street journal sections penn treebank evaluating sequence labeling task chunking. standard conll entity-level score used main evaluation metric. compared error detection corpora labels balanced datasets. however majority labels still exist roughly tokens datasets tagged indicating word entity label covers tokens chunking data. table contains results evaluating different architectures chunking. tasks application dropout provides consistent improvement applying variance onto input embeddings results robust models chunking. addition language modeling objective consistently improves performance benchmarks. results comparable respective state-of-the-art results datasets ﬁne-tune hyperparameters speciﬁc task instead providing controlled analysis language modeling objective different settings. jnlpba system achieves compared zhou conll- lample achieve considerably higher result possibly specialised word embeddings custom version lstm. however sysfigure shows chemdner development every training epoch. withdropout performance peaks quickly trails system overﬁts training set. using dropout best performance sustained throughout training even slightly improved. finally adding language modeling objective dropout allows system consistently outperform architectures. also evaluated language modeling training objective four pos-tagging datasets. penn treebank pos-tag corpus contains texts wall street journal annotated different part-of-speech tags. addition pos-annotated subset genia corpus containing biomedical pubmed abstracts. following tsuruoka document test set. finally also evaluate finnish spanish sections universal dependencies dataset order investigate performance morphologically complex romance languages. datasets somewhat balanced terms label distributions compared error detection single label covers tokens. pos-tagging also offers large variance unique labels labels genia provide useful information models training. baseline performance datasets also close upper bound therefore expect language modeling objective provide much additional beneﬁt. results different sequence labeling architectures pos-tagging seen table accuracy development shown figure performance improvements small consistent across domains languages datasets. application dropout provides robust model language modeling cost improves performance further. even though labels already offer varied training objective learning predict surrounding words time provided model additional general-purpose features. work builds previous research exploring multi-task learning context different sequence labeling tasks. idea multi-task learning described caruana since extended many language processing tasks using neural networks. example collobert weston proposed multitask framework using weight-sharing networks optimised different supervised tasks. cheng described system detecting out-of-vocabulary names also predicting next word sequence. regular recurrent architecture propose language modeling objective integrated bidirectional network making applicable existing state-of-the-art sequence labeling frameworks. plank described related architecture pos-tagging predicting frequency word together part-of-speech showed improve tagging accuracy low-frequency words. predicting word frequency useful pos-tagging language modeling provides general training signal allowing apply model many different recently augenstein søgaard explored multi-task learning classifying keyphrase boundaries incorporating tasks semantic super-sense tagging identiﬁcation multi-word expressions. bingel søgaard also performed systematic comparison task relationships combining different datasets multi-task learning. approaches involve switching auxiliary datasets whereas proposed language modeling objective requires additional data. proposed novel sequence labeling framework secondary objective learning predict surrounding words word dataset. half bidirectional lstm trained forward-moving language model whereas half trained backward-moving language model. time also combined order predict probable label word. modiﬁcation applied several common sequence labeling architectures requires additional annotated unannotated data. objective learning predict surrounding words provides additional source information training. model incentivised discover useful features order learn language distribution composition patterns training data. language modeling main goal system additional training objective leads accurate sequence labeling models several different tasks. learner texts named entity recognition chunking pos-tagging. found additional language modeling objective provided consistent performance improvements every benchmark. largest beneﬁt architecture observed task error detection learner writing. label distribution original dataset sparse unbalanced making difﬁcult task model learn. added language modeling objective allowed system take better advantage available training data leading absolute improvement previous best architecture. language modeling objective also provided consistent improvements sequence labeling tasks named entity recognition chunking pos-tagging. future work could investigate extension architecture additional unannotated resources. learning generalisable language features large amounts unlabeled in-domain text could provide sequence labeling models additional beneﬁt. common pretrain word embeddings large-scale unannotated corpora limited research done towards useful methods pre-training cotraining advanced compositional modules. references rami al-rfou guillaume alain amjad almahairi christof angermueller dzmitry bahdanau nicolas ballas fr´ed´eric bastien justin bayer anatoly belikov alexander belopolsky yoshua bengio arnaud bergeron james bergstra valentin bisson josh bleecher snyder nicolas bouchard nicolas boulanger-lewandowski others. theano python framework fast computaarxiv e-prints tion mathematical expressions. abs/.. http//arxiv.org/abs/.. isabelle augenstein anders søgaard. multitask learning keyphrase boundary classiﬁcaproceedings annual meeting tion. association computational linguistics. http//arxiv.org/abs/.. ciprian chelba tom´aˇs mikolov mike schuster thorsten brants phillipp koehn tony robinson. billion word benchmark measuring progress statistical language modeling. arxiv preprint. http//arxiv.org/abs/.. cheng fang mari ostendorf. open-domain name error detection using multitask rnn. proceedings conference empirical methods natural language processing. uniﬁed architecture natural language processing deep neural networks multitask internalearning. tional conference machine learning https//doi.org/./.. ronan collobert jason weston l´eon bottou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research https//doi.org/./.. alex graves abdel-rahman mohamed geoffrey hinton. speech recognition deep recurrent neural networks. international conference acoustics speech signal processing https//doi.org/./icassp... ozan irsoy claire cardie. opinion mining deep recurrent neural networks. proceedings conference empirical methods natural language processing jin-dong tomoko ohta yoshimasa tsuruoka yuka tateisi nigel collier. introduction bio-entity recognition task international joint workshop natural language processing biomedicine applications https//doi.org/./.. martin krallinger florian leitner obdulia rabal miguel vazquez julen oyarzabal alfonso valencia. chemdner drugs chemical names extraction challenge. journal cheminformatics https//doi.org/./--s-s. john lafferty andrew mccallum fernando pereira. conditional random ﬁelds probabilistic models segmenting labeling seproceedings internaquence data. tional conference machine learning. guillaume lample miguel ballesteros sandeep subramanian kazuya kawakami chris dyer. neural architectures named entity recognition. proceedings naacl-hlt labeling models compositional error detection learner writing. proceedings annual meeting association computational linguistics. https//aclweb.org/anthology/p/p/p-.pdf. nitish srivastava geoffrey hinton alex krizhevsky ilya sutskever ruslan salakhutdinov. simple prevent neural networks overﬁtting. journal machine learning research https//doi.org/./-aos. erik tjong sang sabine buchholz. introduction conll- shared task chunking. proceedings workshop learning language logic conference computational natural language learning https//doi.org/./.. erik tjong sang fien meulder. introduction conll- shared task language-independent named entity recogproceedings seventh conference nition. natural language learning hlt-naacl http//arxiv.org/abs/cs/. yoshimasa tsuruoka yuka tateishi dong tomoko ohta john mcnaught sophia ananiadou jun’ichi tsujii. developing robust partproceedof-speech tagger biomedical text. ings panhellenic conference informatics. helen yannakoudakis briscoe medlock. dataset method automatiproceedings cally grading esol texts. annual meeting association computational linguistics human language technologies. http//www.aclweb.org/anthology/p-. guodong zhou jian exploring deep knowledge resources biomedical name recognition. workshop natural language processing biomedicine applications coling hierarchical probabilistic neural network language tenth international model. workshop artiﬁcial intelligence statistics https//doi.org/./jcdl... hwee siew briscoe christian hadiwinoto raymond hendy susanto christopher bryant. conll- shared task proceedings grammatical error correction. eighteenth conference computational natural language learning shared task. http//www.aclweb.org/anthology/w/w/w. zeljko agi´c maria jesus aranzabe masayuki asahara aitziber atutxa miguel ballesteros john bauer kepa bengoetxea riyaz ahmad bhat cristina bosco bowman lindat/clarin digital library institute formal applied linguistics charles university. http//hdl.handle.net//-. tomoko ohta yuka tateisi jin-dong kim. genia corpus annotated research abstract corpus molecular biology domain. proceedings second international conference human language technology research http//portal.acm.org/citation.cfm?id=. barbara plank anders søgaard yoav goldberg. multilingual part-of-speech tagging bidirectional long short-term memory modproceedings auxiliary loss. annual meeting association computational linguistics pages http//arxiv.org/abs/..", "year": 2017}