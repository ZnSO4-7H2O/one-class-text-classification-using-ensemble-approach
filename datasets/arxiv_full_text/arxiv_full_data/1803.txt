{"title": "Using Graphs of Classifiers to Impose Declarative Constraints on  Semi-supervised Learning", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "We propose a general approach to modeling semi-supervised learning (SSL) algorithms. Specifically, we present a declarative language for modeling both traditional supervised classification tasks and many SSL heuristics, including both well-known heuristics such as co-training and novel domain-specific heuristics. In addition to representing individual SSL heuristics, we show that multiple heuristics can be automatically combined using Bayesian optimization methods. We experiment with two classes of tasks, link-based text classification and relation extraction. We show modest improvements on well-studied link-based classification benchmarks, and state-of-the-art results on relation-extraction tasks for two realistic domains.", "text": "propose general approach modeling semisupervised learning algorithms. speciﬁcally present declarative language modeling traditional supervised classiﬁcation tasks many heuristics including well-known heuristics co-training novel domainspeciﬁc heuristics. addition representing individual heuristics show multiple heuristics automatically combined using bayesian optimization methods. experiment classes tasks link-based text classiﬁcation relation extraction. show modest improvements well-studied link-based classiﬁcation benchmarks state-of-the-art results relation-extraction tasks realistic domains. introduction semi-supervised learning methods operate introducing soft constraints learned classiﬁer behave unlabeled instances different constraints leading different methods. example logistic regression entropy regularization transductive svms constrain classiﬁer make conﬁdent predictions unlabeled points similarly many graph-based approaches require instances associated endpoints edge similar labels embeddings weakly-supervised methods also viewed imposing constraints predictions made classiﬁer instance distantly-supervised information extraction useful constraint requires classiﬁer applied mentions entity pair member relation classiﬁes least mention positive instance propose general approach modeling constraints. deﬁne succinct declarative language specifying semi-supervised learners. call declarative framework d-learner. easily. combined tuning strategy bayesian optimization collectively evaluate effectiveness constraints obtain tailor-made settings individual problems. examine efﬁcacy dlearner apply tasks link-based text classiﬁcation relation extraction. link-based classiﬁcation task show ﬂexibility declarative language deﬁning several constraints exploiting network structures. relation extraction task show declarative language express several intuitive problem-speciﬁc constraints. comparison existing methods shows d-learner achieves significant improvements state-of-the-art domains. example supervised classiﬁcation begin simple example. left-hand side figure illustrates traditional supervised classiﬁcation expressed declaratively logic program. following conventions used logic programming capital letters universally quantiﬁed variables. extend traditional logic programs allowing rules annotated features weighted deﬁne strength rule symbol true goal always succeeds. d-learner programs associated backwardchaining proof process read either logical constraints non-deterministic program invoked query submitted. queries processed program form predict constant theory left-hand side figure interpreted saying prove goal form predict—i.e. predict label instance non-deterministically pick possible class label prove goal classify; proofs every goal classify immediately succeed strength based weighted combination features hasf eature}. encoded usual sparse vector φxiyj dimension every object form class label vocabulary word. example vocabulary contains word hope sports possible label feature φxiyj might active exactly document contains word hope sports. proofs associated theory described plate diagram left-hand side figure upward-pointing blue arrows denote logical implication repetition suggested plates meaning graphical models. black arrows explained below. semantics d-learner constraints d-learner constraints implemented logic programming language called proppr whose semantics deﬁned slightly different graph. proppr graph contains nodes proof graph weighted different edge set—namely downward-pointing black arrows opposite implication edges. example edges describe forest tree labeled example tree forest begins branching distance root nodes labeled true number possible class labels. forest augmented additional edges particular self-loop true node reset edge returns root non-true node. light upward-pointing edges implicit weight ﬁxed unannotated edges implicit weight weight feature-annotated edges discussed below. finally feature vector associated rule edges produced using rule annotated feature vector. weight edge learned parameter vector nonlinear squashing function +e−x example edges ones exit node labeled classify. labeled function recall vector encodes features associated document label deﬁne markov process based repeatedly transitioning node neighbors using normalized outgoing edges transition function. process well-deﬁned positive edge weights graph architecture assign probability score every node conceptually viewed probabilistic proof process reset corresponding abandoning proof imagine trained example true label φij∗ largest score possible labels note every true node corresponds label hard ordering true nodes graph maintains close correspondence classiﬁcation performance. particular even though graph locally normalized φij-labeled edges compete upward-pointing reset edges lead root larger weight direct probability mass walk toward true node associated label speciﬁcally true node associated label highest true node graph. thus example close connection problem setting minimize empirical loss classiﬁer setting satisfy constraints markov-walk scores nodes graph. proppr possible train weights maximize minimize score particular query response i.e. query predict responses positive responses negative. speciﬁcally positive example incurs loss true nodes log− support answer negative example incurs loss training data needed supervised learning case indicated bottom left-hand plate diagram. learning performed stochastic gradient descent example semi-supervised learning ﬁnally turn right-hand parts figures d-learner speciﬁcation method. rules consistency test applied unlabeled example ordinary classiﬁcation tasks distinct classes mutually exclusive. theory right-hand side figure asserts mutual exclusion failure occurs classiﬁed distinct classes. corresponding plate diagram shown figure simpliﬁed omitting reset edges self-loops true nodes. enforce constraint need introduce negative examples unlabeled example specifying proofs goal mutexfailure scores. conceptually constraint encodes common bias decision boundaries drawn systems low-probability regions space. instance transductive svms maximize unlabeled data margin based low-density separation assumption good decision hyperplane lies sparse area feature space case decision boundary close unlabeled example classify goals succeed high score. link-based classiﬁcation d-learner example explains detail specify particular type constraint widely used past works. course type constraint could speciﬁed d-learner would great interest value d-learner allows succinctly specify many plausible constraints potentially improve learning. show application task link-based text classiﬁcation. task constraints many real-world datasets contain interlinked entities exhibit correlations among labels linked entities. link-based classiﬁcation aims improving classiﬁcation accuracy exploiting link structures besides attribute values entity. task classify publication pre-deﬁned class e.g. neural networks. publication features views text content view citation view. thus publication represented features terms features citations. similar classiﬁer deﬁned above text view classiﬁer predictt ←picklabel ∧classifyt. classifyt true hasfeature citation view classiﬁer predictc ←picklabel ∧classifyc. classifyc true cites mutual-exclusivity constraints mutexfailuret mutexfailurec text citation classiﬁers deﬁned done section cotraining constraints. d-learner coordinates classiﬁers views make consistent predictions testing data imposing penalty disagree cofailure pickmutex ∧classifyt cofailure pickmutex ∧classifyc propagation constraints. initial narrative label propagation algorithms neighbors good labeled example classiﬁed consistently express text view d-learner penalizes violators lpfailure sim∧pickmutex∧predictt. pickmutex replace pickmutex another constraint deﬁned near∧sim. true. cites cited near true. thus lpfailure encourages publications citation path take label extending one-step walk based clause deﬁne two-step walk based near∧near∧sim. true. accordingly constraint using referred lpfailure. encourages publications cite cited publication label. regularization constraints. d-learner implement well-studied regularization technique smoothing behavior classiﬁer unlabeled data. speciﬁcally unlabeled example smooth label neighbors’ labels smoothfailure pickmutex∧ settings three datasets citeseer cora pubmed statistical information given table dataset publications testing publications training. among training publications randomly pick labeled examples class remaining ones used unlabeled. examples prepared employ proppr learn multi-class classiﬁers maximum epoch number training usually converges less epochs. note constraints combined equal weights control effect different constraints using different numbers examples results compare supervised learning baselines slsvm sl-proppr employ text view features train classiﬁers svms proppr respectively. sl-svm linear kernel used train multi-class classiﬁers. another compared baseline ssl-naive simply uses examples constraints semi-supervised learning regardless actual effects. d-learner invokes tuning strategy determine constraints. employ accuracy evaluation metric deﬁned ratio number correct predictions number total predictions results given table d-learner consistently outperforms sl-proppr three datasets relative improvements compared sl-svm d-learner achieves better results cora pubmed datasets relative improvements respectively. citeseer dataset d-learner’s performance comparable sl-svm. although constraint intuitive interpretation ssl-naive perform well particularly poor citeseer cora suggests appropriate often domainspeciﬁc needs careful assessment heuristics. tuning bayesian optimization exploit domain-speciﬁc constraints dlearner incorporates bayesian optimization based tuning method learning algorithm’s generalization performance modeled sample gaussian process. released package spearmint allows program wrapper communication tuned algorithm searching optimal parameters. speciﬁcally wrapper passes parameters suggested spearmint learning algorithm collects results algorithm spearmint generate suggestion. instead adding weight control constraint’s effect adopt straightforward tunes example number constraint use. thus parameters example numbers cofailure lpfailure lpfailure mutexfailuret mutexfailurec smoothfailure listed table lpfailure found helpful three datasets. mutual-exclusivity constraint text view helpful cora pubmed datasets. cotraining classiﬁers citation view text content view improve results because three datesets average citation number relation extraction d-learner task note many tasks plausible number task-speciﬁc constraints easily formulated domain expert. section describe number constraints relation extraction entity-centric corpora. document entity-centric corpus describes aspects particular entity e.g. wikipedia article document. relation extraction entity-centric document reduced predicting relation subject entity entity mention document. example drug article aspirin target relation sideeffects need predict candidate whether side effect aspirin. relation holds predict special label other. task medical domain initially proposed reader could details. co-training relation type classiﬁers second argument relation usually particular unary type. example second argument interactswith relation always drug. therefore plausible idea impose agreement constraint relation type classiﬁers analogous enforcing agreements between classiﬁers different views. inranget∧pickmutext∧predictt pickreallabel consists trivial rules nonother relation label inranget consists trivial rules relation label value range type pickmutext consists trivial rules distinct pair type labels predictr predictt relation type classiﬁers respectively. words cofailure says penalize cases mention predicted real relation label whose range type predicted type mutually exclusive thus specifying proofs goal cofailure scores classiﬁers make sort error downweighted. before. constraints relation extraction document constraint. entity string appears multiple mentions document relation label other label docfailure pickmutex∧pickreallabel∧ pickreallabel∧classify∧classify. example heartburn appears twice aspirin document make consistent predictions i.e. sideeffects them predict other. note mention also refer coordinate-term list vomiting headache nausea multiple nps. fact compute intersection sets mentions empty constraint applies vomiting headache vomiting heartburn. pair mentions extracted single sentence. constraint penalizes extracting multiple relation objects single sentence. example sentence some products interact drug include aliskiren inhibitors some products aliskiren inhibitors labeled interactswith relation simultaneously. section title constraint. entity-centric corpora content document organized sections. constraint basically says mentions section documents relation label relative document subjects titlefailure pickmutex∧pickreallabel∧ pickreallabel∧classify∧classify. e.g. heartburn appears adverse reactions sections drugs aspirin singulair plausible infer mentions sideeffects other. experiments settings follow experimental settings drug corpus dailymed containing articles disease corpus wikidisease containing articles. relations extracted drugs diseases respectively. pipeline’s task extract correct values second argument given relation test document. range types second arguments relations also deﬁned paper. details preprocessing features distant labeling seeds annoated evaluation tuning datasets please refer training classiﬁers without manually labeled training relation examples employ seed triples freebase distantly label articles corpora done instance triple sideeffects label mention heartburn aspirin article example sideeffects relation. data distant labeling noisy employ distillation method cleaner relation examples conﬁdent scored examples used training data. also pick mentions testing corpus labeled relation general negative examples. training type examples collected diel extracts instances range types freebase seeds extends seed that pick training examples. similarly also randomly pick number general negative type examples. compared baselines compare three existing methods multir models relation mention separately aggregates labels using deterministic mintz++ improves original model training multiple classiﬁers allowing multiple labels entity pair; miml-re similar structure multir uses classiﬁer aggregate mention level predictions entity pair prediction. used publicly available code authors experiments. tuned parameters including negative example number epoch number training fold number directly evaluation data report best performance. supervised learning baselines distant supervision ds-svm ds-proppr ds-dist-svm dsdist-proppr. ﬁrst distantly-labeled examples testing corpora training data last distilled examples training data done d-learner. ds-proppr ds-dist-proppr proppr used learn multi-class classiﬁers done d-learner. ds-svm ds-dist-svm binary classiﬁers trained svms ds-dist-svm exactly diejob target results evaluate performance different systems perspective title entity relation together query extracted retrieval results. evaluation results given table systems directly tuned evaluation data. systems tuned tuning dataset. disease domain d-learner achieves best performance without using examples cofailure constraint drug domain achieves best performance without using examples docfailure secfailure titlefailure constraints. tuning details discussed later. drug domain. result shows d-learner framework overall superiority relation extraction task mainly capability specifying tailor-made constraints. precision values dlearner much higher compared systems domains. recall d-learner performs much better multir mintz++ miml-re drug domain good disease domain. svm-based baselines outperform proppr-based baselines shows good performance dlearner task using proppr implementation. comparisons ds-x ds-distx show distillation step useful better results. precision-recall curves given figures disease domain d-learner’s precision almost always better recall level methods. drug domain d-learner’s precision generally better recall level tuning bayesian optimization listed table parameters number training relation examples picked ranked distantly labeled examples; number general negative relation examples numbers type examples general negative type examples example numbers cofailure mutexfailurer mutexfailuret docfailure titlefailure sentfailure unlabeled constraint examples randomly picked entire testing corpus. columns give value ranges columns give step sizes searching optimal. pages annotated tuning domains d-learner achieves better results using portion top-ranked relation examples training data. shows using smaller cleaner training examples combined constraints cotraining improve performance. disease domain d-learner performs better withcotraining type classiﬁers thus optimal type related parameters i.e. disease drug domain cotraining bootstrap performance relation extraction. explanation domain e.g. drug second argument values relations less ambiguous cotrained type classiﬁers accurate help relation classiﬁer learner explore useful features captured relation training examples. disease domain second argument values relations ambiguity causes learnt type classiﬁers inaccurate unhelpful relation classiﬁers. respect tailor-made constraints relation extraction i.e. docfailure sentfailure titlefailure d-learner also behaves differently domains. drug domain optimal using examples them disease domain using examples preferred. extraction constraints cotraining type classiﬁers under integral setting d-learner i.e. using constraints cotraining simultaneously. tuning experiments show drug domain using relation extraction constraints also improves performance however improvement signiﬁcant using cotraining setting; disease domain using cotraining setting also improves results much using constraints. using constraints cotraining integral setting necessarily value-added effect. domains mutexfailure examples found always helpful. note mutexfailuret constraint becomes meaningless thus disease domain. another point mention that shown table optimal values parameters maximums ranges indicates d-learner likely achieve even better performance examples constraints used. conclusions future work modelproposed general approach d-learner constraints. approximate traditional supervised learning many natural heuristics declaratively specifying desired behaviors classiﬁers. bayesian optimization based tuning strategy collectively evaluate effectiveness constraints obtain tailor-made settings individual problems. efﬁcacy approach examined tasks link-based text classiﬁcation relation extraction encouraging improvements achieved. open questions explore adding hyperparameters adding control supervised loss versus constraint-based loss; testing approach tasks. references mikhail belkin partha niyogi vikas sindhwani. manifold regularization geometric framework learning labeled unlabeled examples. journal machine learning research lidong bing sneha chaudhari richard wang william cohen. improving distant supervision information extraction using label propagation lists. proceedings conference empirical methods natural language processing pages lisbon portugal september association computational linguistics. lidong bing mingyang ling richard wang william cohen. distant bootstrapping using lists document structure. proceedings thirtieth aaai conference artiﬁcial intelligence february phoenix arizona usa. pages strapping distantly supervised using joint learning small well-structured corpora. proceedings thirty first aaai conference artiﬁcial intelligence february francisco california usa. raphael hoffmann congle zhang xiao ling luke zettlemoyer daniel weld. knowledge-based weak supervision information exproceedings traction overlapping relations. annual meeting association computational linguistics human language technologies-volume pages association computational linguistics thorsten joachims. transductive inference text classiﬁcation using support vector machines. proceedings sixteenth international conference machine learning icml pages francisco morgan kaufmann publishers inc. mike mintz steven bills rion snow jurafsky. distant supervision relation extraction without labeled data. proceedings joint conference annual meeting international joint conference natural language processing afnlp volume -volume pages association computational linguistics sebastian riedel limin andrew mccallum. modeling relations mentions without labeled text. machine learning knowledge discovery databases pages springer prithviraj galileo mark namata mustafa bilgic lise getoor brian gallagher tina eliassi-rad. collective classiﬁcation network data. magazine jasper snoek hugo larochelle ryan adams. practical bayesian optimization machine learning algorithms. pereira burges bottou weinberger editors advances neural information processing systems pages curran associates inc. mihai surdeanu julie tibshirani ramesh nallapati christopher manning. multiinstance multi-label learning relation extraction. proceedings joint conference empirical methods natural language processing computational natural language learning emnlp-conll pages stroudsburg association computational linguistics. partha pratim talukdar koby crammer. regularized algorithms transductive learning. machine learning knowledge discovery databases pages springer pagerank locally groundable ﬁrst-order probabilistic logic. proceedings international conference conference information knowledge management pages jason weston fr´ed´eric ratle hossein mobahi ronan collobert. deep learning semisupervised embedding. neural networks tricks trade second edition pages dengyong zhou olivier bousquet thomas navin bernhard sch¨olkopf. learning local global consistency. advances neural information processing systems pages ghahramani lafferty. semi-supervised learning using gaussian ﬁelds harproceedings icml- monic functions. international conference machine learning", "year": 2017}