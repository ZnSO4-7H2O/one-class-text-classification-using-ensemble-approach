{"title": "Dynamic Sum Product Networks for Tractable Inference on Sequence Data  (Extended Version)", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Sum-Product Networks (SPN) have recently emerged as a new class of tractable probabilistic graphical models. Unlike Bayesian networks and Markov networks where inference may be exponential in the size of the network, inference in SPNs is in time linear in the size of the network. Since SPNs represent distributions over a fixed set of variables only, we propose dynamic sum product networks (DSPNs) as a generalization of SPNs for sequence data of varying length. A DSPN consists of a template network that is repeated as many times as needed to model data sequences of any length. We present a local search technique to learn the structure of the template network. In contrast to dynamic Bayesian networks for which inference is generally exponential in the number of variables per time slice, DSPNs inherit the linear inference complexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other models on several datasets of sequence data.", "text": "mazen melibari pascal poupart david cheriton school computer science university waterloo waterloo ontario canada prashant doshi department computer science university georgia athens georgia george trimponias huawei noah’s hong kong sum-product networks recently emerged class tractable probabilistic models. unlike bayesian networks markov networks inference exponential size network inference spns time linear size network. since spns represent distributions ﬁxed variables only propose dynamic product networks generalization spns sequence data varying length. dspn consists template network repeated many times needed model data sequences length. present local search technique learn structure template network. contrast dynamic bayesian networks inference generally exponential number variables time slice dspns inherit linear inference complexity spns. demonstrate advantages dspns dbns models several datasets sequence data. keywords tractable probabilistic models; dynamic sum-product networks; sequence data. probabilistic graphical models bayesian networks markov netwoks provide general framework represent multivariate distributions exploiting conditional independence. years many approaches proposed learn structure networks however even resulting network small inference intractable practitioners must often resort approximate inference techniques. recent work focused development alternative probabilistic models arithmetic circuits sum-product networks inference guaranteed tractable means networks learned data directly used inference without approximation. work focused learning models ﬁxed number variables based ﬁxed-length data extension spns model sequence data varying length. similar dynamic bayesian networks dspns consist template network repeats many times length data sequence. describe invariance property template network sufﬁcient ensure resulting dspn valid complete decomposable. since existing structure learning algoritms spns assume ﬁxed variables ﬁxed-length data cannot used learn structure dspn. propose general anytime searchand-score framework speciﬁc local search technique learn structure template network deﬁnes dspn based data sequences varying length. demonstrate advantages dspns static spns dbns hidden markov models recurrent neural networks synthetic real sequence data. deﬁnition sum-product network binary variables rooted directed acyclic graph whose leaves indicators i¯xn whose internal nodes sums products. edge emanating node non-negative weight wij. value product node j∈ch wijvj value seen output network polynomial whose variables indicator variables coefﬁcients weights polynomial represents joint probability distribution variables valid. completeness decomposability sufﬁcient conditions validity impose conditions scope node variables appear sub-spn rooted node. several basic distributions encoded simple spns. instance univariate distribution encoded using whose root node linked indicator single variable factored distribution variables encoded root product node linked univariate distributions variable naive bayes model encoded root node linked factored distributions product naive bayes models encoded root product node linked naive bayes models inference queries answered taking ratio values obtained bottom passes spn. ﬁrst pass initialize remaining indicators order compute value proportional desired query. figure univariate distribution binary variable factored distribution three binary variables naive bayes model three binary variables product naive bayes models. second pass initialize remaining indicators order compute normalization constant. linear complexity inference spns appealing property given inference models exponential size network worst case. spns computational graphs based difﬁcult infer relationships variables zhao showed convert spns equivalent bipartite bayesian networks without exponential blow contrast compilation bayesian network equivalent yield exponential blow spns syntactically equivalent arithmetic circuits sense reduced linear time space. extension network polynomials dynamic bayesian networks given procedure based variable elimination proposed compile recursive network polynomial represented special call dynamic since risk compiled dynamic intractable authors boyen-koller method approximate output factored representation. thus compiling dynamic reduce complexity inference makes linear size compiled dynamic could intractable. contrast propose approach learn tractable models directly sequence data. propose dynamic spns generalization spns modeling sequence data varying length. dspns equivalent dynamic develop structure learning algorithm learns tractable dspn directly sequence data also show sufﬁcient conditions ensure estimated dspns valid therefore permit exact sequential inference linear time. figure example generic template network. notice interface nodes red. generic example complete dspn unrolled time slices. template networks stacked bottom network capped network. dspn models sequences varying length ﬁxed number parameters using template repeated slice. analogous dbns template corresponds network connects consecutive slices. deﬁnition template network slice binary variables time directed acyclic graph roots leaf nodes. leaf remaining leaves nodes indicator variables equal number roots interface nodes template previous next slices respectively. interface interior nodes either product nodes. edge emanating node non-negative weight spn. furthermore deﬁne bijective mapping input output interface nodes. deﬁnition bottom network ﬁrst slice binary variables directed acyclic graph roots leaf nodes. leaf nodes indicator variables roots interface nodes template network next slice. interface interior nodes either product nodes. edge emanating node non-negative weight spn. deﬁnition deﬁne network rooted directed acyclic graph composed product nodes leaves. leaves network interface nodes introduced previously. edge emanating node non-negative weight spn. networks stacked merging input interface nodes upper network output interface nodes lower network. figure shows example slices variables each. mentioned previously completeness decomposability sufﬁcient ensure validity spn. could check node dspn complete product node decomposable provide simpler ensure dspn complete decomposable. particular describe invariance property template network veriﬁed directly template without unrolling dspn. invariance property sufﬁcient ensure completeness decomposability satisﬁed dspn number slices. deﬁnition template network invariant scope pairs input interface nodes input interface node excludes variables following properties hold intuitively template network invariant assign scope input interface node pair input interface nodes scope disjoint scopes relation holds scopes corresponding output nodes. scopes pairs corresponding interface nodes must disjoint product node decomposable children disjoint scopes node complete children identical scope. hence verifying identity disjoint relation scopes every pair input interface nodes helps verifying completeness decomposability remaining nodes template. theorem shows invariance property def. used ensure corresponding dspn complete decomposable. theorem bottom network complete decomposable scopes pairs output interface nodes bottom network either identical disjoint scopes output interface nodes bottom network used assign scopes input interface nodes template networks template network invariant network complete decomposable corresponding dspn complete decomposable. proof sketch proof induction base case consider single-slice dspn capped networks. bottom network complete decomposable assumption. since interface output nodes bottom network merged input interface nodes network assigned scope ensures network also complete decomposable. induction step assume dspn slices complete decomposable. consider dspn slices shares bottom network ﬁrst copies template network dspn slices. hence bottom network ﬁrst copies template network dspn slices complete decomposable. since next copy template network invariant input interface nodes assigned scopes identity disjoint relations scopes output interface nodes bottom network also complete decomposable. similarly network complete decomposable. dspn could ignore repeated structure learn number variables corresponding longest sequence. shorter sequences could treated sequences missing data unobserved slices. unfortunately intractable long sequences inability model repeated structure implies large learning computationally intensive. approach feasible datasets contain short sequences nevertheless amount data needed prohibitively large absence repeating structure number parameters much higher. furthermore could asked perform inference sequence longer training sequences likely perform poorly. alternately tempting apply existing algorithms learn repeated structure dspn. unfortunately possible. existing algorithms assume ﬁxed variables could break data sequences ﬁxed-length segments corresponding slice. learned dataset segments. however clear resulting construct template network regular single root template network multiple roots equal number input leaves indicator variables. would treat segment independent data instances could answer queries probability variables slice given values variables slices. present anytime search-and-score framework learn structure template dspn. starts arbitrary structure generates several neighbouring structures. ranks neighbouring structures according scoring function selects best neighbour. steps repeated stopping criterion met. framework instantiated multiple ways based choice initial structure neighbour-generation process scoring function stopping criterion. proceed description speciﬁc instantiation below although instantiations possible. without loss generality propose product nodes interface nodes input output template network. also propose bottom network identical template network removing nodes indicator variable descendent. design single algorithm learn structure template network wlog assume dspn alternates layers product nodes. since dspn consists repeated structure ﬂexibility choosing interfaces template. chose interfaces layers product nodes interfaces could shifted level layers nodes even traverse several layers obtain mixture product nodes. boundaries equivalent subject suitable adjustments bottom networks. since bottom network automatically determined learned template. also propose network root node directly linked input product nodes. template network initialize rooted output product node factored model univariate distributions. figure shows example initial structure interface nodes three variables. output product node four children child node corresponding univariate distribution. three children univariate distributions linked indicators three variables fourth node distribution interface input nodes. merging interface nodes repeated instantiations template obtain hierarchical mixture model. begin single interface node iteratively increase number score stops improving. alg. summarizes steps compute initial structure. simple scoring function likelihood data since exact inference dspns done quickly. goal produce generative model data likelihood data natural criterion. goal produce discriminative model classiﬁcation conditional likelihood class variables given remaining variables suitable criterion. given structure parameters estimated using various parameter learning algorithms including gradient ascent expectation maximization neighbour generation process begins sampling product node uniformly replacing sub-spn rooted product node sub-spn. note satisfy decomposability property product node must partition scope disjoint scopes children. also note different partitions scope seen different conditional independencies variables hence search space product node generally corresponds partitions scope. ’restricted growth string encoding partitions deﬁne lexicographical order possible partitions select next partition according lexicographic ordering sampling distribution possible partitions. distribution uniform absence prior knowledge informed otherwise. since search space exponential number variables scope product node greedily split scope mutually independent subsets according pairwise independence tests applied recursively similar case independent subsets found sample partition random number variables greater threshold select next partition according lexicographic ordering otherwise. alg. describes process ﬁnding next partition based construct product naive bayes models naive bayes model children encode factored distributions. since constructing template learning parameters computing score done time linear size template network dataset iteration anytime search-and-score algorithm scales linearly size template network amount data. sample product node uniformly templn newp artition getpartition construct product naivebayes models based newp artition newt empl replace templn proof scope input interface nodes identical. initial structure template network collection factored distributions variables. hence output interface nodes scope hence alg. produces initial template network invariant. alg. replaces sub-spn product node sub-spn change scope product node. follows fact partition used construct sub-spn variables original partition. since scope product node change sub-spn change nodes product node including output interface nodes preserve scope. hence alg. produces neighbour template networks invariant. table mean negative log-likelihood standard error based -fold cross validation synthetic datasets. indicates number data instances length sequence number observed variables. lower likelihoods better. data comparing negative log-likelihoods static spns learned using learnspn dynamic models hidden markov models dbns recurrent neural networks threshold alg. experiments. synthetic datasets include three dynamic processes different structures sequences observations sampled hidden variable well-known water bayesian automated taxi also evaluate dspns real-world sequence datasets repository include applications online handwriting recognition speech recognition ﬁrst compare dspns true model synthetic datasets. learnspn cannot used data variable length include synthetic datasets experiment only sample sequences ﬁxed length. table shows negative log-likelihoods based -fold cross validation synthetic datasets. three synthetic datasets dspn learned generative models exhibited likelihoods close true models. also outperforms learnspn three cases. next compare dspns classic hmms parameters learned baum-welch hmm-spns observation distribution fully observable dbns whose structure learned reveal algorithm bayesnet toolbox partially observable dbns whose structure hidden variables learned search score rnns input node hidden layer consisting long short term memory units output sigmoid unit cross-entropy loss function. select lstm units popularity success sequence learning input node corresponds value current observation output node predicted value next observation sequence. train network backpropagation time truncated time steps learning rate implementation based theano library python. table shows results real datasets. dspns outperform approaches except dataset achieved better results. dspns expressive classic hmms hmm-spns since search score algorithm ﬂexibility learning suitable structure multiple interface nodes transition dynamics structure transition dynamics ﬁxed single hidden variable classic hmms hmm-spns. dspns also expressive fully observable dbns found reveal since nodes template networks implicitly denote hidden variables. dspns expressive partially table mean negative log-likelihood standard error based -fold cross validation real world datasets. indicates number data instances average length sequences number observed variables. observable dbns found search score better results achieved dspns linear inference complexity allows explore larger space structures quickly. dspns less expressive rnns since dspns restricted product nodes rnns product sigmoid operators. nevertheless rnns notoriously difﬁcult train non-convexity loss function vanishing/exploding gradient issues arise backpropagation time. explains rnns outperform dspns datasets. table shows time learn inference dspn models models trained till convergence days. report total time learning reveal time iteration learning algorithms since anytime algorithms. learning dspns generally faster training rnns search-and-score dbns. time inference sequences dataset variable observed variables hidden reported right hand side table. dspns rnns fast since allow exact inference linear time respect size network dbns obtained reveal search-and-score slow inference exponential number hidden variables become correlated. existing methods learning spns become inadequate task involves modeling sequence data time series data points. speciﬁc challenge sequence data could composed instances different lengths. motivated dynamic bayesian networks presented model called dynamic utilized template network building block. also deﬁned notion invariance showed invariant template networks composed safely ensure resulting dspn valid. provided anytime algorithm based framework search-and-score learning structure template network data. experiments demonstrated dspn sequential data better static spns also showed dspns found search-and-score algorithm achieve higher likelihood competing hmms dbns rnns several temporal datasets. approximate inference typically used dbns avoid exponential blow inference done exactly linear time dspns. proof give proof induction based level node. base case consider input leaf nodes since input node input satisﬁes induction step assume nodes level satisfy since scope node level union scopes children lower levels relations ensures identity disjoint relations also preserved pair nodes network. useful corollary show completeness decomposability also preserved. scopes input nodes network either identical disjoint corollary shows changing scopes input nodes preserves identity disjoint relations ensures completeness decomposability preserved throughout network. useful lemma show composing multiple template networks preserves invariance. proof according lemma pairs nodes scope still scope relabeling scopes hence complete nodes still complete relabeling scopes similary according lemma pairs nodes disjoint scopes still disjoint scopes relabeling scopes hence decomposable product nodes still decomposable relabeling scopes proof give proof induction based number copies template network. base case consider stack copy template network. since template network invariant stack copy template network invariant. induction step assume copies template network invariant. means bijective function maps input ﬁrst template output template pairs inputs ﬁrst template following properties hold scope scope scope) scope) scope scope scope) scope) scope) scope) scope) scope) since assigning scopes output nodes bottom network input nodes template ensures template complete decomposable viewed relabeling scopes lemma corollary template also invariant. result entire stack templates invariant. theorem bottom network complete decomposable scopes pairs output interface nodes bottom network either identical disjoint scopes output interface nodes bottom network used assign scopes input interface nodes template networks template network invariant network complete decomposable corresponding dspn complete decomposable. proof bottom network complete decomposable assumption. since also assume scopes pairs output interface nodes bottom network either identical disjoint output interface nodes bottom network used assign scopes interface nodes template network lemma stack number template networks invariant finally show network also complete decomposable. bijective function associates input ﬁrst template output last template pairs inputs ﬁrst template following properties hold", "year": 2015}