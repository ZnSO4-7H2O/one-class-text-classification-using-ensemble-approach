{"title": "Training Deep Networks with Structured Layers by Matrix Backpropagation", "tag": ["cs.CV", "cs.AI"], "abstract": "Deep neural network architectures have recently produced excellent results in a variety of areas in artificial intelligence and visual recognition, well surpassing traditional shallow architectures trained using hand-designed features. The power of deep networks stems both from their ability to perform local computations followed by pointwise non-linearities over increasingly larger receptive fields, and from the simplicity and scalability of the gradient-descent training procedure based on backpropagation. An open problem is the inclusion of layers that perform global, structured matrix computations like segmentation (e.g. normalized cuts) or higher-order pooling (e.g. log-tangent space metrics defined over the manifold of symmetric positive definite matrices) while preserving the validity and efficiency of an end-to-end deep training framework. In this paper we propose a sound mathematical apparatus to formally integrate global structured computation into deep computation architectures. At the heart of our methodology is the development of the theory and practice of backpropagation that generalizes to the calculus of adjoint matrix variations. The proposed matrix backpropagation methodology applies broadly to a variety of problems in machine learning or computational perception. Here we illustrate it by performing visual segmentation experiments using the BSDS and MSCOCO benchmarks, where we show that deep networks relying on second-order pooling and normalized cuts layers, trained end-to-end using matrix backpropagation, outperform counterparts that do not take advantage of such global layers.", "text": "deep neural network architectures recently produced excellent results variety areas artiﬁcial intelligence visual recognition well surpassing traditional shallow architectures trained using hand-designed features. power deep networks stems ability perform local computations followed pointwise non-linearities increasingly larger receptive ﬁelds simplicity scalability gradient-descent training procedure based backpropagation. open problem inclusion layers perform global structured matrix computations like segmentation higher-order pooling preserving validity efﬁciency end-to-end deep training framework. paper propose sound mathematical apparatus formally integrate global structured computation deep computation architectures. heart methodology development theory practice backpropagation generalizes calculus adjoint matrix variations. proposed matrix backpropagation methodology applies broadly variety problems machine learning computational perception. illustrate performing visual segmentation experiments using bsds mscoco benchmarks show deep networks relying second-order pooling normalized cuts layers trained end-to-end using matrix backpropagation outperform counterparts take advantage global layers. recently end-to-end learning deep architectures using stochastic gradient descent based large datasets produced impressive results realistic settings variety computer vision machine learning domains. renewed enthusiasm creating integrated automatic models handle diverse tasks associated able perceiving system. widely used architecture convolutional network deep processing model based composition convolution pooling pointwise nonlinearities efﬁcient classiﬁcation learning. convnets sufﬁciently expressive classiﬁcation tasks comprehensive deep architecture uniformly covers types structured non-linearities required calculations established. turn matrix factorization plays central role classical algorithms many different computer vision machine learning problems image segmentation feature extraction descriptor design structure motion camera calibration dimensionality reduction among others. singular value decomposition particular extremely popular ability efﬁciently produce global solutions various problems. paper propose enrich dictionary deep networks layer generalizations fundamental matrix function computational blocks proved successful ﬂexible years vision learning models global constraints. consider layers explicitly structure-aware sense preserve global invariants underlying problem. paper makes main mathematical contributions. ﬁrst shows operate structured layers learning deep network. purpose outline matrix generalization backpropagation offers rigorous formal treatment global properties. second contribution derive instantiate methodology learn convolutional networks different successful types structured layers second-order pooling normalized cuts illustration resulting figure overview deepop recognition architecture made possible methodology. levels represent standard convolutional layers. layer global matrix logarithm layer presented paper. followed fully connected layers logistic loss. methodology presented paper enables analytic computation local global layers system remains trainable end-to-end local global parameters using matrix variation generalizations entitled matrix backpropagation. deep architecture given challenging datasets like bsds mscoco experimentally demonstrate feasibility added value types networks counterparts using global computational layers. work relates extensive literature area neural networks review) architectures proven popular successful machine learning computer vision. deep neural networks models focused traditionally generality scalability shallow computer vision machine learning architectures often designed global computation structure modeling mind. objective work provide ﬁrst steps possible approach towards formally marrying lines work. neural networks modern realization traced back least perceptron ﬁrst layer network although limited expressiveness. derivation backpropagation advances decade later allowed development integration layers exploration complex expressive architectures. process lead successes practical applications e.g. digit recognition recently availability hardware large scale datasets development complex enough architectures lead models currently outperform existing representations challenging general recognition problems. recommends neural networks forefront methodologies building representations prediction problems computer vision beyond. showed even complex deeper models obtain even better results. lead computer vision researchers focus transferring success detection semantic segmentation problems ﬁelds handcrafted features statistically inspired deformable part models dominant time. r-cnn uses standard networks vgg- classify object proposals detection. uses input streams original image second image background region masked alexnet architectures take advantage shape information provided mask. propose global spatial pyramid pooling layer fully connected layers perform simple max-pooling pyramid-structured cells image. uses committees improve robustness pushed performance close beyond human performance tasks like trafﬁc sign recognition house number identiﬁcation. ﬁrst application illustrate deep architecture log-covariance pooling layer proved dominant free-form region description manually designed local features sift. methodology propose makes possible deal difﬁculties learning underlying features even presence complex intermediate representation. part also related kernel learning approaches manifold positive-deﬁnite matrices however introduce different mathematical techniques related matrix backpropagation advantages scalability ﬁtting together perfectly existing deep network layers. among ﬁrst methods integrating structured models cnns work showed hmms integrated deep networks showed results speech text analysis problems. recently demonstrated using crfs deep networks trained end-to-end showing strong results digit recognition protein secondary structure prediction. cast conditional random ﬁeld semantic segmentation almost immediately taken advantage deep network revolution providing useful smoothing highperforming pixel classiﬁer predictions. showed fully connected components usually discarded previous methods also made convolutional i.e. original resolution lost pooling operations recovered means trained deconvolution layer. obtained state-of-the-art semantic segmentation results using architecture similar enforcing structure using globally connected crfs unary potentials learnt. simultaneous work show that since mean ﬁeld based approximate updates differentiable ﬁxed number inference steps unrolled loss applied gradients backpropagated back ﬁrst inference convolutional layers potentials. efﬁcient learning method obtained blending inference training order obtain procedure updates parameters inference progresses. unlike previous methods learns based pairwise potentials separate unary potential. learning model requires piece-wise training minimizes upper-bound energy decouples potentials. matrix backpropagation methodology generally applies models expressed composed structured non-linear matrix functions. such applied deep models structure well e.g. belief propagation models gaussian potentials expressed solution linear system. crf-based methods designed deep nets traditionally focus iterative inference learning order construct derivatives ﬁnal layer must combine derivatives inference iterations methodology expressed terms invariants converged solution linear systems therefore require iterative derivative calculations inference. second model used illustrate matrix backpropagation methodology normalized cuts received less attention deep network community evidenced fact leading methods still handcrafted. spectral formulations like normalized cuts obtained state-of-the-art results used strong pixel-level classiﬁers hand-designed features. different approach taken show inference relaxed spectral problem. turaga ﬁrst demonstrate learning image segmentation model end-to-end using features optimizing standard segmentation criterion. learning inference ncuts placed ﬁrmer ground bach jordan introduced learning formulation build upon work several important differences. first uses matrix derivatives makes appeal directly eigen-decompostion derive instead projectors allows truncate spectrum consider eigenspace corresponding largest eigenvalues cost making criterion non-differentiable. instead consider entire eigenspace rely projectors learn dimensionality process. importantly however instead learning parameters ﬁxed features directly learn afﬁnity matrix adapting underlying feature representation modeled deep network. resulting method combining strong pixel-level classiﬁers global representation naturally adapt pixel-level semi-local predictions object detection semantic segmentation operations require structured global computations also consistency propagation information image. careful application methodology keeps entire architecture trainable end-to-end. another direction effort generalize convolutions general non-euclidean non-equally spaced grids work realizes necessity spectral layers learning graph structure since computational issues brought process main focus handle directly. aspects partially addressed authors focus learning parameters applied eigenvalues instead learning eigenvectors eigenvalues context focus underlying theory backpropagation handling structured objects like matrices allowing derive many similar also complex derivatives. symbolic matrix partial derivatives basis work ﬁrst systematically studied seminal paper although complex non-linear layers like eigen-decomposition. since received interest mainly context studying estimators statistics econometrics recently ﬁeld automatic differentiation also shown interest theory considering matrix functions. powerful machinery however appeared scarcely computer vision machine learning. instances although treating general case focusing subset elements interest appeared context camera calibration learning parameters normalized cuts model learning parameters gaussian crfs denoising learning deep canonical correlation models recent surge interest deep networks exposed limitations current compositional architectures based local operations turn pushes research direction structured models requiring matrix based representations. recently multiplied outputs networks matrices order obtain improved ﬁne-grained recognition models although matrix derivatives case straightforward. knowledge ﬁrst bring methodology full generality fore context composed global non-linear matrix functions deep networks show promising results different computer vision machine learning models. methodological contributions follows formulation matrix back-propagation generalized chain rule mechanism computing derivatives composed matrix functions respect matrix inputs relying theory adjoint matrix variations; introduction spectral non-linear layers like eigen-decomposition allow calculation derivatives respect quantities interest particular singular values singular vectors eigen-values eigen-vectors formulation non-linear matrix function layers take eigen-decompositions inputs instantiations second-order pooling models recipes computing derivatives matrix projector operations instantiated normalized-cuts models. novel methodology applies broadly illustrated end-to-end visual learning deep networks competitive results. next section brieﬂy present main elements current deep network models. paper organization outline challenges computational recipe handle matrix layers. presents matrix function layer using either decomposition instantiated build deep second-order pooling models. introduce in-depth treatment learn deep normalized cuts models. experiments presented deep processing networks y)}i=...n data points corresponding desired targets drawn distribution loss function i.e. penalty mismatch model prediction function parameters input i.e. desired output foundation many learning approaches including ones considered here principle empirical risk minimization states mild conditions concentration measure empirical risk sufﬁces minimize empirical risk learn function well general i.e. continuous gradient descent learning parameters. supports general effective framework learning provided gradient exists. deep networks model consider class functions written series successive function compositions parameter tuple called layers parameters layer number layers. denote loss function layer xl−. notation convenient conceptually separates network architecture layer design. since computation gradient requirement learning important step effective principle backpropagation backprop described literature algorithm efﬁciently compute gradient loss respect parameters case layers outputs expressed vectors scalars general form individually expressed non-linear functions input. algorithm recursively computes gradients respect inputs layers parameters making chain rule. data tuple layer computing chain rule given however formulation processes spatial coordinates independently immediately generalize complex mathematical objects. consider matrix view layer spatial coordinate index input feature. deﬁne non-linearity entire rml×dl matrix instead spatial coordinate separately. matrix derivative respect vector longer well-deﬁned matrix generalization backpropation necessary. clarity draw distinction data structures used represent layers mathematical computational operations performed. example convolutional neural network layer viewed current implementations tensor dimensions correspond spatial coordinates dimension corresponds features. however mathematical calculations level layers parameters backpropagation recursively expresses partial derivative loss w.r.t. current layer parameters based partial derivatives next layer cfeq. derivative calculations) expressed tensors. instead performed vectors scalar outputs used selectively index tensor data structures. contrast genuine matrix calculus would rely matrices data structures ﬁrst class objects. would require coherent formulation non-linear structured operations like forward propagation derivative calculations directly expressed using matrices. distinction stylistic complex matrix operations e.g. eigen-decomposition derivatives simply cannot implemented index-ﬁlling. second-order pooling competitive hand-designed feature descriptors regions used top-performing method pascal semantic segmentation comp. track represents global high-order statistics local descriptors inside region computing covariance matrix rm×d matrix image features present region spatial locations feature dimensions applying tangent space mapping using matrix logarithm computed using svd. instead pooling hand-designed local descriptors sift could learn deep feature extractor end-to-end upper second-order pooling structured layer form constructs matrix local interactions rm×d similar feature matrix spatial locations dimensions descriptor solves generalized eigenvalue problem determine global image partitioning. instead manually designed afﬁnities could given ground truth target segmentation learn end-to-end deep features produce good normalized cuts. strategy derivation outlined below involves important concepts. variation corresponds forward sensitivity allows easy manipulation ﬁrst higher order terms taylor expansion. e.g. matrix function write odx) matrix size depending derivative deﬁnition linear ‘coefﬁcient’ taylor expansion i.e. coefﬁcient ergo variation partial derivative different objects always deﬁned deﬁned take matrix inputs space matrices. contrast partial derivative also makes sense matrix inputs deﬁned scalar co-domain variation used convenience derivation needs implemented practice. ultimately after purpose matrix backpropagation partial derivative. derivation variation involves forward mapping layer also invariants associated variables. satisﬁes certain invariants need preserved ﬁrst order computing invariants diagonality symmetry orthogonality need explicitly enforced methodology means additional equations beyond given produced step above know holds. thus properties matrix inner product trb) obtain partial derivatives respect lower layer variables. since operator inner product space matrices equivalent constructively producing non-linear adjoint operator holds general variation e.g. non-symmetric even symmetric. remain within subspace like symmetric diagonal orthogonal matrices consider projection onto space admissible variations transfer projection onto derivative obtain projected gradient. technique repeatedly derivations. summarizing objective calculation obtain ∂l◦f process resulting expression using matrix identities order obtain analytic expression expression allows compute global matrix operations used deep networks compound processing layers performed along way. steps architecture speciﬁc although calculations like spectral decomposition widespread central many vision machine learning models. possesses powerful structure allows express complex transformations like matrix functions algorithms numerically stable form. sequel show widely useful singular value decomposition symmetric eigenvalue problem leveraged towards constructing layers perform global calculations deep networks. spectral layers layer receives matrix input produces tuple matrices notation above means matrices satisfy regular invariants decomposition i.e. diagonal taken account derivation. following proposition gives variations outputs i.e. partial derivative note respect layer correspond respectively ﬁrst second step methodology presented sequel denote asym in-depth treatment including questions existence uniqueness partial derivatives matrix functions. appendices contain basic identities used steps. arguments adjoint operator deﬁned property proof. rm×n rm×n diagonal rm×m rn×n orthogonal. given variation want calculate variations dudς variation diagonal like whereas satisfy constraints respectively. taking ﬁrst variation decomposition layer receives matrix input produces pair matrices given notation means matrices satisfy regular invariants eigen-decomposition i.e. diagonal matrix. following proposition identiﬁes variations outputs i.e. partial derivative respect layer function non-linear layers using layers presented ready produce layers like involve matrix functions e.g. learned end-to-end. consider deep feature matrix notice \u0001i)v last equality obtained deﬁnition matrix functions given schur decomposition eigendecomposition coincide real symmetric matrices. thus implement matrix function create layer receives outputs layer produces gσ+\u0001i)v applied element-wise diagonal elements thus much easier handle. matrix function layer receives input tuple matrices produces response \u0001i)v analytic function parameter consider ﬁxed simplicity. notation section \u0001i)v following proposition gives variations outputs i.e. partial derivatives respect note layer layer depend derivative proposition matrix function diagonalizable matrix written since diagonal equivalent applying element-wise diagonal elements. combining idea decomposition matrix function written \u0001i)v similarly matrix function layer receives input pair matrices produces response notation note inputs obey invariants decomposition real symmetric matrix i.e. eigenvectors eigenvalues layer produces result matrix function holds similar reasons since case schur decomposition coincides eigen-decomposition following proposition shows variations outputs case i.e. partial derivatives respect layer central computer vision machine problem grouping segmentation clustering i.e. discovering datapoints belong several partitions. successful approach clustering normalized cuts. number pixels image indices. want compute partition matrix }m×k otherwise. rm×d data feature matrix descriptor size data similarity matrix positive entries. simplicity consider parameter matrix. note also apply global non-linearities segmentation layer presented previous section. diagonal matrix main diagonal i.e. diagonal elements sums corresponding rows normalized cuts criterion finding matrix minimizes equivalent ﬁnding partition minimizes energy penalizes unbalanced solutions. easy show trd−/w d−/z) that piecewise constant respect ignoring second condition obtain relaxed problem solved theorem eigen-decomposition propose learn parameters piecewise constant then solving relaxed problem equivalent original problem. input features ﬁxed thus parameters permit alignment. case place global objective convolutional network inputs. therefore take leverage network parameters order change directly thus training bottom layers produce representation appropriate normalized cuts. obtain piecewise constant respect align span d/eed/. projectors corresponding space spanned orthogonal projector moore-penrose inverse alignment achieved minimizing frobenius norm projectors associated model prediction desired output respectively notice criterion superﬁcially similar important differences. truncate spectrum consider eigenspace corresponding largest eigenvalues cost making criterion non-differentiable. contrast consider entire eigenspace rely projectors aiming also learn dimensionality space process. obtain partial derivatives objective respect matrices depends relying matrix backpropagation. since projectors play important role number different places section treat separately. consider layer takes matrix input produces corresponding orthogonal projector aa+. notation section following proposition gives variations outputs i.e. partial derivative respect layer note since exists non-trivial spectral decomposition training although ‘notationally’ hidden nevertheless requires computation. perspective matrix backpropagation split computation following layers consider reverse order objective inputs. first focus derivative frobenius norm well known layer taking inputs producing corresponding projectors i.e. derivatives obtained applying lemma subsequently consider layer receiving inputs producing notation introduced deﬁned above. following proposition gives variations outputs i.e. partial derivative respect layer finally cases consider layer receives inputs outputs data similarity following procedure section ﬁrst compute ﬁrst order variations trace properties make partial derivatives identiﬁable important feature formulation restrict rank training. alignment optimization choose collapse certain directions thus reducing rank. prove topological lemma implying frobenius distance projectors drops certain value ranks projectors match. conversely reason ranks cannot converge objectives bounded away zero. following lemma shows projectors matrices close enough norm matrices rank. lemma rm×n respective orthogonal projectors. rank rank proof. spectral norm indeed deﬁned supx= assume ranks different i.e. w.l.o.g. rank rank fundamental theorem linear algebra exists vector range orthogonal range section validate proposed methodology constructing models standard datasets region-based object classiﬁcation like microsoft coco image segmentation bsds matconvnet implementation models methods publicly available. region classiﬁcation mscoco recognition mscoco dataset provides segmented training instances across classes divided training validation sets. main goal assess second-order pooling layer various training settings. secondary goal study behavior convnets learned scratch segmented training data. explored context deep learning relatively small size datasets associated object segmentations pascal experiments section convolutional architecture component alexnet global layers propose order obtain deepop models classiﬁcation fully connected layers topology alexnet. crop resize object bounding pixels largest side standard alexnet input size small translation jittering limit over-ﬁtting. also randomly images mini-batch horizontally standard practice. training performed stochastic gradient descent momentum. batch size methods learning rate optimized model independently. deepop models used parameter value table classiﬁcation error validation mscoco models sufﬁxes trained scratch mscoco dataset. deepop models classiﬁcation layer deepop layer whereas deepop-fc also fully connected layers topology alexnet. parameters proposed global models reﬁned jointly end-to-end using proposed matrix backpropagation. architecture implementation details. implementing spectral layers efﬁciently challenging since support still limited parallelization efforts even using latest cuda solver delivered slower implementation standard cpu-based. consequently implementations incur penalty moving data back forth cpu. numerical experiments revealed implementation single precision obtained signiﬁcantly less accurate gradient learning. therefore computations proposed layers forward backward passes made double precision. experiments still noticed signiﬁcant accuracy penalty inferior precision layers still computed single precision gpu. second formal derivation non-linear spectral layer based eigen-decomposition instead also possible numerical experiments favor formulation using svd. alternative implementation formally correct exhibits numerical instability derivative multiple eigenvalues close values thus producing blow numerical issues expected appear implementations complex layers like ones presented integrated deep network settings. results. results recognition experiment presented table show proposed deepopfc models containing global layers outperform standard convolutional pipelines based alexnet problem. bottom layers pre-trained imagenet using alexnet might provide ideal initial input table segmentation results give best average covering pool ground truth segmentations bsds dataset baselines original normalized cuts using intervening contour afﬁnities well normalized cuts afﬁnities derived non-ﬁnetuned deep features different layers alexnet deepncuts models trained end-to-end based proposed matrix backpropagation methodology using objective note fully connected layers deepop layer offer good performance beneﬁts. hand-crafted sift performs considerably less well deepop models suggesting large potential gains achieved deep features replace existing descriptors. full-image segmentation bsds bsds dataset validate deep normalized cuts approach. bsds contains training images testing images human annotations relevant regions image. although small standards neural network learning provides exactly supervision need reﬁne model using global information. note since supervision pixel-wise number effective datapoint constraints much larger. evaluate using average best covering metric optimal image scale criterion given full image segmentations computed image selecting maximizes average best covering respectively compared pool ground truth segmentations. architecture implementation details. alexnet vgg- architectures feed global layers. parameters deep global models reﬁned end-to-end. linear afﬁnity need entries positive. thus relu layers feed segmentation ones. initially cascaded segmentation layer different layers alexnet resulting models hard learn. best results obtained adding conv-relu pairs initialized randomly normalized cuts layer. results many ﬁlters lower layer high capacity layer limit maximal rank alexnet chose last convolutional layer used ﬁrst relu layer block layer block gives feeds layers different invariances receptive ﬁeld sizes coarseness used initial learning rate larger rates newly initialized layers. dropout layer last layers rate reduces overﬁtting. inference generate segmentations clustering connected components split separate segments. results. results table show cases obtain important performance improvements respect corresponding models perform inference directly original alexnet/vgg features. training using matlab implementation takes images/s considering image batch testing images/s standard titan core cpu. experiments monitor objective rank similarity matrix. rank reduction usually good indicator performance training testing. context rank analysis interpret ﬁndings mean rank similarity large compared target objective sufﬁcient lead rank reduction. however rank predicted similarity ground truth initially apart rank reduction occur improves results. motivated recent success deep network architectures work introduced mathematical theory computational blocks support development complex models layers perform structured global computations like segmentation higher-order pooling. central methodology development matrix backpropagation methodology relies calculus adjoint matrix variations. provide detailed derivations operating conditions spectral non-linear layers illustrate methodology normalized cuts second-order pooling layers. region visual recognition segmentation experiments figure segmentation results images test bsds. show ﬁrst column input image followed baseline deepncuts using alexnet relu-. pairs baselines deepncut models trained based objective follow. ﬁrst pair uses relu- second relu-. improvements obtained learning quantitatively signiﬁcant easily visible side-by-side comparison. based mscoco bsds show deep networks relying second-order pooling normalized cuts layers trained end-to-end using introduced practice matrix backpropagation outperform counterparts take advantage global layers. acknowledgements. work partly supported cncs-uefiscdi ct-erc-- pce-- jrp-ro-fr--. thank carreira helpful discussions nvidia generous graphics board donation. notation basic identities section present completeness notation basic linear algebra identities useful calculations associated matrix backpropagation instantiation log-covariance descriptors normalized cuts segmentation following notation used derivations symmetric part asym diagonal operator adiag arbitrary matrix rm×n matrix matches main diagonal elsewhere. using notations diag denote diagonal diagonal matrix vector diagonal resp. adiag", "year": 2015}