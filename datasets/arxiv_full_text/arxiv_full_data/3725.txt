{"title": "Doubly Robust Policy Evaluation and Learning", "tag": ["cs.LG", "cs.AI", "cs.RO", "stat.AP", "stat.ML"], "abstract": "We study decision making in environments where the reward is only partially observed, but can be modeled as a function of an action and an observed context. This setting, known as contextual bandits, encompasses a wide variety of applications including health-care policy and Internet advertising. A central task is evaluation of a new policy given historic data consisting of contexts, actions and received rewards. The key challenge is that the past data typically does not faithfully represent proportions of actions taken by a new policy. Previous approaches rely either on models of rewards or models of the past policy. The former are plagued by a large bias whereas the latter have a large variance.  In this work, we leverage the strength and overcome the weaknesses of the two approaches by applying the doubly robust technique to the problems of policy evaluation and optimization. We prove that this approach yields accurate value estimates when we have either a good (but not necessarily consistent) model of rewards or a good (but not necessarily consistent) model of past policy. Extensive empirical comparison demonstrates that the doubly robust approach uniformly improves over existing techniques, achieving both lower variance in value estimation and better policies. As such, we expect the doubly robust approach to become common practice.", "text": "presented receive information presented. health care success rates patients received treatments alternatives. problems instances contextual bandits context refers additional information user patient. here focus ofﬂine version assume access historic data ability gather data kinds approaches address ofﬂine learning contextual bandits. ﬁrst call direct method estimates reward function given data uses estimate place actual reward evaluate policy value contexts. second kind called inverse propensity score uses importance weighting correct incorrect proportions actions historic data. ﬁrst approach requires accurate model rewards whereas second approach requires accurate model past policy. general might difﬁcult accurately model rewards ﬁrst assumption restrictive. hand usually possible model past policy quite well. however second kind approach often suffers large variance especially past policy differs signiﬁcantly policy evaluated. paper propose technique doubly robust estimation overcome problems existing approaches. doubly robust estimation statistical approach estimation incomplete data important property either estimators correct estimation unbiased. method thus increases chances drawing reliable inference. study decision making environments reward partially observed modeled function action observed context. setting known contextual bandits encompasses wide variety applications including health-care policy internet advertising. central task evaluation policy given historic data consisting contexts actions received rewards. challenge past data typically faithfully represent proportions actions taken policy. previous approaches rely either models rewards models past policy. former plagued large bias whereas latter large variance. work leverage strength overcome weaknesses approaches applying doubly robust technique problems policy evaluation optimization. prove approach yields accurate value estimates either good model rewards good model past policy. extensive empirical comparison demonstrates doubly robust approach uniformly improves existing techniques achieving lower variance value estimation better policies. such expect doubly robust approach become common practice. values along census statistics used form estimator probability response conditioned family income. using importance weighting inverse estimated probabilities estimator overall opinions formed. alternative estimator formed directly regressing predict survey outcome given available sources information. doubly robust estimation uniﬁes techniques unbiasedness guaranteed either probability estimate accurate regressed predictor accurate. apply doubly robust technique policy value estimation contextual bandit setting. core technique analyzed terms bias section variance section unlike previous theoretical analyses assume either reward model past policy model correct. instead show deviations models truth impact bias variance doubly robust estimator. knowledge style analysis novel provide insights doubly robust estimation beyond speciﬁc setting studied here. section apply method policy evaluation optimization ﬁnding approach substantially sharpens existing techniques. prior work doubly robust estimation widely used statistical inference references therein). recently used internet advertising estimate effects features online advertisers previous work focuses parameter estimation rather policy evaluation/optimization addressed here. furthermore previous analysis doubly robust estimation studies asymptotic behavior relies various modeling assumptions lunceford davidian kang schafer analysis non-asymptotic makes assumptions. several papers machine learning used ideas related basic technique discussed here although language. benign bandits hazan kale construct algorithms reward estimators order achieve worst-case regret depends variance bandit rather time. similarly offset tree algorithm thought using crude reward estimate offset. cases algorithms estimators described substantially sophisticated. note neither distribution policy known. given data collected above interested tasks policy evaluation policy optimization. policy evaluation interested estimating value stationary policy deﬁned hand goal policy optimization optimal policy maximum value argmaxπ theoretical sections paper treat problem policy evaluation. expected better evaluation generally leads better optimization experimental section study policy evaluation approach used policy optimization classiﬁcation setting. challenge estimating policy value given data described previous section fact partial information reward hence candirectly simulate proposed policy data common solutions overcoming limitation. ﬁrst called direct method forms estimate expected reward conditioned context action. policy value estimated clearly good approximation true expected reward deﬁned estimate close also unbiased unbiased estimate problem method estimate formed without knowledge hence might focus approximating mainly areas irrelevant sufﬁciently areas important beygelzimer langford reﬁned analysis. indicator function evaluating argument true zero otherwise. estimate approximately unbiased estimate since typically good understanding datacollection policy often easier obtain good estimate thus estimator practice less susceptible problems bias compared direct method. however typically much larger variance range random variable increasing. issue becomes severe gets smaller. approach alleviates large variance problem taking advantage estimate used direct method. doubly robust estimators take advantage estimate expected reward estimate action probabilities here estimator form ﬁrst suggested cassel regression previously studied policy learning practice rare accurate estimation either thus basic question estimator perform estimates deviate truth? following sections dedicated bias variance analysis respectively estimator. general neither estimators dominates others. however either expected value doubly robust estimator close true value whereas requires requires also still outperform similarly roles reversed. thus effectively take advantage sources information better estimation. bounds primary dependence variance lower variance implies faster convergence rate. treat case stationary past policy hence drop dependence throughout. previous section sufﬁces analyze second moment single term similar decomposition simplify derivation notation i/ˆp. note that conditioned expectation zero. hence write second moment thus variance decomposed three terms. ﬁrst accounts randomness rewards. second term variance estimator randomness last term viewed importance weighting penalty. similar expression derived estimator ﬁrst term identical second term similar magnitude corresponding term estimator provided however third term much larger smaller contrast direct method obtain thus variance direct method terms depending either past policy randomness rewards. fact usually sufﬁces ensure signiﬁcantly lower variance ips. however mention previous section bias direct method typically much larger leading larger errors estimating policy value. section provides empirical evidence effectiveness estimator compared consider classes problems multiclass classiﬁcation bandit feedback public benchmark datasets estimation average user visits internet portal. begin description turn k-class classiﬁcation task k-armed contextual bandit problem. transformation allows compare using public datasets policy evaluation learning. alternatively turn data point costsensitive classiﬁcation example loss predicting then classiﬁer interpreted action-selection policy classiﬁcation error exactly policy’s expected loss. construct partially labeled dataset exactly loss component example observed following approach beygelzimer langford speciﬁcally given randomly select label unif reveal component ﬁnal data thus form when considering classiﬁcation problems natural talk minimizing classiﬁcation errors. loss minimization problem symmetric reward maximization problem deﬁned section cost-sensitive multiclass classiﬁcation algorithms used learn classiﬁer losses completed either ﬁrst filter tree reduction beygelzimer applied decision tree training fully revealed losses direct loss minimization algorithm mcallester obtain classiﬁer classiﬁer constitutes policy evaluate test data; require estimating expected conditional loss denoted given linear loss model parameterized weight vectors {wa}a∈{...k} least-squares ridge regression based training set. step repeated times resulting bias rmse reported fig. predicted analysis unbiased since probability estimate accurate. contrast linear loss model fails capture classiﬁcation error accurately result suffers much larger bias. average classiﬁcation errors runs plotted fig. clearly policy optimization advantage even greater policy evaluation. datasets provides substantially reliable loss estimates results significantly improved classiﬁers. fig. also includes classiﬁcation error offset tree reduction designed speciﬁcally policy optimization partially labeled data. versions filter tree rather weak versions competitive offset tree datasets cases signiﬁcantly outperform offset tree. finally note provided similar improvements different algorithms based gradient descent based tree induction. suggests generality combined different algorithmic choices. lion bcookies randomly selected bcookies march bcookie associated sparse binary feature vector size around features describe browsing behavior well information bcookie. chose ﬁxed time window march calculated number visits selected bcookie window. summarize dataset contains data {}i=...n i-th bcookie corresponding binary feature vector number visits. sample uniformly random sample mean unbiased estimate true average number user visits problem. however various situations difﬁcult impossible ensure uniform sampling scheme practical constraints thus sample mean reﬂect true quantity interest. known covariate shift special case problem formulated section arms. formally partially labeled data consists tuples indicates whether bcookie sampled aivi observed number visits probability goal evaluate value constant policy deﬁne sampling probabilities adopted similar approach gretton particular obtained ﬁrst principal component features {xi} projected data onto univariate normal distribution mean estimator consistently better especially dataset size smaller. estimator often reduces rmse fraction average comparing bias metrics clear dr’s gain accuracy came lower variance accelerated convergence estimator true value. results conﬁrm analysis tends reduce variance provided reasonable reward estimator available. doubly robust policy estimation effective technique virtually always improves widely used inverse propensity score method. analysis shows doubly robust methods tend give reliable accurate estimates. theory corroborated experiments benchmark data large-scale real-world problem. future expect technique become common practice improving contextual bandit algorithms. example interesting develop variant offset tree take advantage better reward models rather crude constant reward estimate figure.classiﬁcation error ﬁlter tree note representations used trees differ radically conﬂating comparison approaches. however offset filter tree approaches share similar representation differences performance purely matter superior optimization. table precise numbers. control data size randomly subsampled fraction entire dataset bcookie subsample probability otherwise. calculated estimates subsample. whole process repeated times. estimator required building reward model which given feature predicted average number visits. again least-squares ridge regression used linear model sampled data. given data perform approximate gradient descent policy loss experiments section policy speciﬁed weight vectors given policy predicts follows maxa∈{...k}{x θa}. optimize adapt towards-better version direct loss minimization method mcallester follows given data current weights weights adjusted maxa ǫla} maxa decaying learning rate input parameter. computational reasons actually performed batched updates rather incremental updatess. found learning rate t−./ batched iteration worked well across datasets. parameter ﬁxed datasets. updates continued weights converged. furthermore since policy loss convex weight vectors repeated algorithm times randomly perturbed starting weights returned best run’s weight according learned policy’s loss training data. also tried using holdout validation choosing best weights candidates observe beneﬁts filter tree reduction cost-sensitive classiﬁcation binary classiﬁcation. input form direct loss minimization output binary-tree based predictor node filter tree uses binary classiﬁer—in case decision tree implemented weka thus -class decision trees nodes nodes arranged filter tree. training filter tree proceeds bottom-up trained node ﬁltering examples observed parent entire tree trained. testing proceeds root-to-leaf implying test time computation logarithmic number classes. test all-pairs filter tree test time computation linear class count similar dlm. beygelzimer langford ravikumar multiclass classiﬁcation ﬁlter-trees. unpublished technical report http// www.stat.berkeley.edu/∼pradeepr/paperz/ﬁlter-tree.pdf gretton smola huang schmittfull borgwardt sch¨olkopf dataset shift machine learning. covariate shift local learning distribution matching press kang schafer demystifying double robustness comparison alternative strategies estimating population mean incomplete data. statist. sci. discussions.", "year": 2011}