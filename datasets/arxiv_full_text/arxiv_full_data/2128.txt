{"title": "Multi-fidelity Gaussian Process Bandit Optimisation", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "In many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function $f$. Traditional settings for this problem assume just the availability of this single function. However, in many cases, cheap approximations to $f$ may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of $f$ in a small but promising region and speedily identify the optimum. We formalise this task as a \\emph{multi-fidelity} bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a novel method based on upper confidence bound techniques. In our theoretical analysis we demonstrate that it exhibits precisely the above behaviour, and achieves better regret than strategies which ignore multi-fidelity information. Empirically, MF-GP-UCB outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments.", "text": "kirthevasan kandasamy gautam dasarathy junier oliva jeff schneider barnab´as p´oczos machine learning department carnegie mellon university pittsburgh electrical computer engineering rice university houston many scientiﬁc engineering applications tasked optimisation expensive evaluate black function traditional settings problem assume availability single function. however many cases cheap approximations obtainable. example expensive real world behaviour robot approximated cheap computer simulation. approximations eliminate function value regions cheaply expensive evaluations small promising region speedily identify optimum. formalise task multi-ﬁdelity bandit problem target function approximations sampled gaussian process. develop mf-gp-ucb novel method based upper conﬁdence bound techniques. theoretical analysis demonstrate exhibits precisely behaviour achieves better regret strategies ignore multi-ﬁdelity information. empirically mf-gp-ucb outperforms naive strategies multi-ﬁdelity methods several synthetic real experiments. keywords multi-ﬁdelity optimisation bandits bandit optimisation bayesian optimisation gaussian processes. introduction stochastic bandit optimisation wish optimise function sequentially querying obtaining bandit feedback i.e. query observe possibly noisy evaluation typically expensive goal identify maximum keeping number queries possible. applications hyper-parameter tuning expensive machine learning algorithms optimal policy search complex systems scientiﬁc experiments martinez-cantin parkinson snoek historically bandit problems studied settings goal maximise cumulative reward queries payoff instead ﬁnding maximum. applications setting include clinical trials online advertising. conventional methods settings assume access single expensive function interest collectively refer single ﬁdelity methods. many practical problems however cheap approximations might available. instance tuning hyper-parameters learning algorithms goal maximise cross validation score training expensive training large. however validation curves tend vary smoothly training size; therefore train cross validate small subsets approximate validation accuracies entire dataset. concrete example consider kernel density estimation need tune bandwidth kernel. figure shows average cross validation likelihood dataset size smaller subset size since cross validation performance hyper-parameter depends training size obtain biased estimate cross validation performance points using subset size consequently maximisers also different. said curve approximates curve quite well. since training cross validation small cheap eliminate values hyper-parameters reserve expensive experiments entire dataset promising hyper-parameter values conventional treatment online advertising query public display internet certain time period. however could also choose smaller experiments conﬁning display small geographic region and/or shorter periods. estimate biased since users different geographies likely different preferences nonetheless useful gauging round performance optimal policy search robotics autonomous driving vastly cheaper computer simulations used approximate expensive real world performance system scientiﬁc experiments approximated varying degrees using less expensive data collection analysis computational techniques present formalism multi-ﬁdelity bandit optimisation using gaussian process assumptions approximations. develop novel algorithm multi-fidelity gaussian process upper conﬁdence bound setting. theoretical analysis proves mf-gp-ucb explores space lower ﬁdelities uses high ﬁdelities successively smaller regions converge optimum. lower ﬁdelity queries cheaper mf-gp-ucb better regret single ﬁdelity strategies rely expensive function explore entire space. demonstrate mf-gp-ucb outperforms single ﬁdelity methods alternatives empirically series synthetic examples three hyper-parameter tuning tasks inference problem astrophysics. matlab implementation experiments available github.com/kirthevasank/mf-gp-ucb. since seminal work robbins multi-armed bandit problem studied extensively k-armed setting. recently surge interest optimism uncertainty principle k-armed bandits typiﬁed upper conﬁdence bound methods strategies also used bandit tasks linear payoffs. plethora work single ﬁdelity methods global optimisation noisy noiseless evaluations. examples branch bound techniques dividing rectangles jones simulated annealing genetic algorithms kawaguchi kirkpatrick munos suite single ﬁdelity methods framework closely related work bayesian optimisation several techniques particular interest gaussian process upper conﬁdence bound algorithm srinivas many applied domains research aerodynamics industrial design hyper-parameter tuning studied multi-ﬁdelity methods plurality techniques. however none treatments neither formalise analyse notion regret multi-ﬁdelity setting. contrast mf-gpucb intuitive idea good theoretical properties. agarwal derive oracle inequalities hyper-parameter tuning computational budgets. setting general applies bandit optimisation task. sabharwal present based idea tuning hyper-parameters incremental data allocation. however theoretical results idealised non-realisable algorithm. cutler study reinforcement learning multi-ﬁdelity simulators treating ﬁdelity markov decision process. finally zhang chaudhuri study active learning access cheap weak labeler expensive strong labeler. latter work study problems different optimisation. recently kandasamy studied classical k-armed bandit multi-ﬁdelity settings. here build work study multi-ﬁdelity bayesian optimisation; such share similarities assumptions algorithm analysis techniques. preliminary version paper appeared kandasamy provided theoretical results continuous domains ﬁdelities here also analyse discrete domains present results general number ﬁdelities. furthermore eliminate technical assumptions previous work present cleaner interpretable versions theorems. follow work extend multi-ﬁdelity optimisation settings continuous approximations. assumptions considerably different builds main intuitions work. best knowledge ﬁrst line work formalise notion regret provide theoretical analysis multi-ﬁdelity optimisation. remainder manuscript organised follows. section presents formalism including notion simple regret multi-ﬁdelity optimisation. section presents algorithm. present theoretical results section beginning informal discussion results ﬁdelities section elucidate main ideas. proofs given section section presents experiments details deferred appendix appendix collects ancillary material including table notations abbreviations appendix problem wish maximise function compact subset dimension interact querying obtaining noisy evaluation noise satisﬁes argmaxx∈x maximiser maximum value. primary distinction usual setting access successively accurate approximations function interest refer approximations ﬁdelities. multi-ﬁdelity framework attractive following conditions true problem. approximations approximate assume uniform bound ﬁdelities bounds known. therefore ﬁdelity increases approximations become better also costly. algorithm multi-ﬁdelity bandits sequence query-ﬁdelity pairs {}t≥ time algorithm chooses using information previous query-observation-ﬁdelity triples {}n− smoothness assumptions needed make problem tractable. standard bayesian nonparametric literature gaussian process prior covariance kernel popular kernels choice squared exponential respectively. parameters kernels gamma modiﬁed bessel functions. convenience framework offers posterior distributions analytically tractable. sample observations gaussian noise posterior distribution also gaussian remark appendix argue assumption continues hold nontrivial probability sample generative mechanism would keep sampling functions deliver conditions hold true. assumption relaxed hold different kernels noise variances ﬁdelity different minimal modiﬁcations analysis form simplify presentation results. fact practical implementation uses different kernels. simple regret multi-ﬁdelity optimisation goal achieve small simple regret spending capital resource. provide any-capital bounds meaning assume game played indeﬁnitely bound regret values similar spirit any-time analyses single ﬁdelity bandit methods opposed ﬁxed time horizon analyses. {mt}t≥ ﬁdelities queried multi-ﬁdelity i.e. number queries strategy makes across ﬁdelities capital optimum interest lower ﬁdelities useful extent help optimise less cost reward optimising cheaper approximation. accordingly instantaneous reward time denote instantaneous regret optimisation simple regret simply best instantaneous regret mint=...n equivalently note reduces access λ/λ. proceed note customary bandit literature analyse cumulative regret. deﬁnition cumulative regret depends application hand results extended many sensible notions cumulative regret. however simplify exposition since focus paper optimisation stick simple regret. challenges conclude section commentary challenges multi-ﬁdelity optimisation using figure illustration. simplicity focus ﬁdelities approximation expensive function assume suboptimal seemingly straightforward optimum solution might search appropriate subset neighborhood however neighborhood small might miss optimum crucial challenge multi-ﬁdelity methods stuck optimum lower ﬁdelity. exploiting information lower ﬁdelities also important explore sufﬁciently higher ﬁdelities. experiments demonstrate naive strategies would stuck optimum lower ﬁdelity. alternatively pick large subset might miss however defeats objectives multi-ﬁdelity goal approximation prudent query figure seems like sensible subset remains seen chosen. further subset might even neighborhood illustrated figure multi-modal optima different modes. cases appropriate algorithm explore modes. above algorithm actually know sensible algorithm explore simultaneously identify subset either implicitly explicitly exploration second ﬁdelity time query maximiser upper bound argmaxx∈x work builds gp-ucb takes σt−. posterior mean standard form deviation conditioned previous queries {}t− intuition mean encourages exploitative strategy want query know function high conﬁdence band encourages explorative strategy want query regions uncertain lest miss high valued regions. presented gp-ucb algorithm illustrated figure figure illustration gp-ucb. solid black line dashed blue line upper bounds high probability. observations shown black crosses. time query maximiser argmaxx∈x depends kernel important quantity analysis. given kernel typically scales volume e.g. od)). known kernel ψnd) o)d+) mat´ern kernel ψnd) log) next need following regularity conditions kernel. satisﬁed four times differentiable kernels kernel mat´ern kernel following theorem srinivas bounds simple regret gp-ucb. theorem denote log. pick failure probability following bounds simple regret hold probability multi-ﬁdelity gaussian process upper conﬁdence bound propose mf-gp-ucb extends gp-ucb multi-ﬁdelity setting. like gp-ucb mf-gp-ucb also maintain obtained previous queries ﬁdelities. denote posterior mean standard deviation conditioned previous queries ﬁdelity upper bound high probability. appropriately chosen upper bounds bounds minimum gives best upper bound following strategies gp-ucb next query maximiser argmaxx∈x next need decide ﬁdelity query consider conditions lengthened constrain value conﬁdence band obtain conﬁdence large means constrained sufﬁciently well query ﬁdelity. hand querying indeﬁnitely region reduce uncertainty ﬁdelity region help much elongation caps much learn i.e. even knew perfectly constrained within band. algorithm captures simple intuition. selected begin checking smaller threshold proceed second ﬁdelity. ﬁrst ﬁdelity. stage query ﬁdelity proceed ﬁdelity query discuss choices sections summarise resulting procedure algorithm proceed make essential observation. posterior conditioned gaussian constraints holds high probability since conditioning however queries ﬁdelity gaussianity figure panels illustrate execution mf-gp-ucb ﬁdelities times panel ﬁgure illustrates upper bounds lection bottom ﬁgure illustrates selection initialised mf-gp-ucb random points ﬁrst ﬁdelity. ﬁgures solid lines brown blue respectively dashed lines solid green line min. small crosses queries star maximiser i.e. next query optimum shown magenta. bottom ﬁgures solid orange line dashed black line play ﬁdelity otherwise cyan region last panel good described section illustration mf-gp-ucb figure illustrates mf-gp-ucb simulation –ﬁdelity problem. initial stages mf-gp-ucb mostly exploring ﬁrst ﬁdelity. large constrain well proceed constrained falls query sufﬁciently well region around optimum. notice dips change region. mf-gp-ucb identiﬁed maximum queries region shaded cyan last ﬁgure good alluded section deﬁne formally explain signiﬁcance multi-ﬁdelity shortly. analysis predicts second ﬁdelity queries mf-gp-ucb conﬁned simulation corroborates claim. example last ﬁgure algorithm decides explore point away optimum. however query occurs ﬁrst ﬁdelity since sufﬁciently constrained region large. idea necessary query regions second ﬁdelity ﬁrst ﬁdelity alone enough conclude suboptimal. addition observe large portion given next present main theoretical results. wish remind reader table ignore constants polylog terms dominated terms. denote inequality equality ignoring constants. denote complement fundamental -ﬁdelity problem good high-valued region away optimum. multi-ﬁdelity strategy second ﬁdelity queries regret dependence high ﬁdelity queries. contrast strategy operates highest ﬁdelity dependence. small i.e. good approximation much smaller then multi-ﬁdelity strategy signiﬁcantly better regret single ﬁdelity strategy. alas achieving somewhat ideal goal possible without perfect knowledge approximation. however mf-gp-ucb come quite close. show second ﬁdelity queries conﬁned slightly inﬂated good ργ}. parameter explained theorems. following lemma bounds number ﬁrst second ﬁdelity evaluations complement xgρ. denote number queries ﬁdelity within ﬁrst time steps lemma consider mf-gp-ucb total evaluations either ﬁdelity. then exists depending following statements hold high probability show second ﬁdelity queries inside xgρ. number second ﬁdelity queries outside small precisely appropriate strong result possible multi-ﬁdelity setting. gp-ucb best achievable bound number plays suboptimal kernel worse mat´ern kernel. example simulation figure queries fact conﬁned subset xgρ. allows obtain regret scales explained above. second lemma control number queries mf-gp-ucb within capital number queries single ﬁdelity method operating second ﬁdelity. could large arbitrary multi-ﬁdelity method. however using bounds show larger value below detail main ingredients proof lemma ﬁnite regret need show eventually query region switching condition step algorithm ensures query region indeﬁnitely. keep querying certain region ﬁrst ﬁdelity uncertainty reduce region. expected bound critical dependence threshold parameter discuss implications subsection section queries outside obtain tighter independent bound using simple argument. small outside unlikely contain maximiser selected step algorithm several times. certain region ﬁrst ﬁdelity conﬁdence band small. implies must several ﬁrst ﬁdelity queries region turn implies learn high conﬁdence. alone would tell point suboptimal maximiser unlikely region frequently. hence query outside often. threshold parameter appear bound dependence mild. follows number second ﬁdelity queries scales finally invoke techniques srinivas control regret using mig. however unlike them analyse separately number second ﬁdelity queries different subset. allows obtain tighter bound following form. instructive compare rates gp-ucb theorem dropping whereas mf-gp-ucb latter term mf-gp-ucb small decaying therefore ﬁrst term dominates whenever approximation good vol) rates mf-gp-ucb appealing. approximation worsens xgxgρ become larger bound decays gracefully. worst case mf-gp-ucb never worse gp-ucb constant terms term required since initial stages mf-gp-ucb exploring proceeding stage regret still costs factored result condition. large ﬁxed larger amount capital spent ﬁrst ﬁdelity large. analyse effect parameter result. ﬁrst sight large seems increase size would suggest keep small possible. however smaller also increases intuitively small wait long time step algorithm decrease without proceeding might expect optimal choice depends e.g. approximation extremely cheap makes sense small learn much possible however also depends problem dependent quantities kandasamy able exactly specify practical choice k-armed setting which unfortunately straightforward setting. section describe heuristic values worked well experiments. partition ﬁdelity problem. sets indicated next boundaries. sets hhhh shown green blue yellow respectively. capital invested points queries ﬁdelity function constant number queries algorithm ignores lower ﬁdelities. max/ depends depends several problem dependent quantities including sizes sets consequently first note log. comparing result theorem outperform gp-ucb factor ofψnλ)/ψnλ asymptotically. determined values approximations parameters better approximations smaller advantage single ﬁdelity strategies. approximations worsen advantage multi-ﬁdelity optimisation diminishes expected never worse gp-ucb constant factors. continuous compact discrete begin partitioning partitioning depend values before addition also depend parameter deﬁned discrete case. make dependence explicit deﬁne max)} subset whose value least max) optimum deﬁne implementation uses standard techniques bayesian optimisation literature given below. addition describe heuristics used parameters method. initialisation kernel following recommendations brochu methods initialised uniform random queries using initialisation capital single ﬁdelity methods used ﬁdelity whereas multi-ﬁdelity methods used ﬁrst ﬁdelity second ﬁdelity. next initialise kernel maximising marginal likelihood update kernel every iterations using marginal likelihood. choice speciﬁed theorems unknown constants tends conservative practice. following kandasamy captures dominant dependencies values assume satisﬁes assumption allows work value initialise small value range initial queries. whenever query ﬁdelity also check posterior mean ﬁdelity. query ﬁdelity. update twice violation. choice this following intuition algorithm stuck ﬁdelity long probably small. start small values algorithm query ﬁdelity iterations double values initialised range initial queries. experiments present experiments compact continuous since practically relevant setting. compare mf-gp-ucb following baselines. single ﬁdelity methods gp-ucb; expected improvement criterion direct dividing rectangles method multi-ﬁdelity methods mf-naive naive baseline gp-ucb query ﬁrst ﬁdelity large number times query last ﬁdelity points queried decreasing order -value; mf-sko multi-ﬁdelity sequential kriging method previous works multi-ﬁdelity methods made code available straightforward implement. hence could compare them. discuss appendix along single multi-ﬁdelity baselines tried excluded comparison avoid clutter ﬁgures. also detail design choices hyper-parameters baselines appendix currin exponential park borehole functions ﬁdelity experiments hartmann functions ﬁdelities respectively. ﬁrst three functions taken previous multi-ﬁdelity literature tweaked hartmann functions obtain lower ﬁdelities latter cases. appendix give formulae functions approximations used lower ﬁdelities. show simple regret capital figure mf-gp-ucb outperforms baselines problems. last panel figure shows histogram number queries ﬁdelity queries mf-gp-ucb different ranges hartmann-d function. many queries values ﬁdelity progress decrease second ﬁdelity queries increase. third ﬁdelity dominates close optimum used sparingly figure simple regret spent capital synthetic functions. title states function dimensionality number ﬁdelities costs used ﬁdelity experiment. curves barring direct produced averaging experiments. error bars indicate standard error. ﬁgures follow legend ﬁrst ﬁgure currin exponential function. last panel shows number queries different function values ﬁdelity hartmann-d example. elsewhere. corroborates prediction analysis mf-gp-ucb uses ﬁdelities explore successively higher ﬁdelities promising regions zero common occurrence mf-naive started querying ﬁdelity regret barely decreased. diagnosis cases same stuck around maximum suboptimal suggests cheap approximations problem means trivial. explained previously also important explore higher ﬁdelities achieve good regret. efﬁcacy mf-gp-ucb compared single ﬁdelity methods conﬁnes exploration small containing optimum. experiments found mf-sko consistently beat single ﬁdelity methods. despite best efforts reproduce method found quite brittle. fact also tried another multi-ﬁdelity method found perform desired appendix present additional experiments test implementation mf-gp-ucb. study performs approximations cost approximation varies. present results three hyper-parameter tuning tasks maximum likelihood inference task astrophysics. compare methods computation time since cost experiments. include processing time method comparison results given figure mf-gp-ucb outperforms baselines tasks. experimental optimisation problem described below. classiﬁcation using svms trained support vector classiﬁer magic gamma dataset using sequential minimal optimisation algorithm accuracy goal tune kernel bandwidth soft margin coefﬁcient ranges respectively dataset size ﬁdelity experiment entire training second ﬁdelity points ﬁrst. query required -fold cross validation respective training sets. regression using additive kernels used salsa method additive kernel ridge regression -dimensional coal power plant dataset. tuned hyper-parameters –the regularisation penalty kernel scale kernel bandwidth dimension– range using -fold cross validation. experiment used points ﬁdelity respectively. viola jones face detection viola jones cascade face classiﬁer uses cascade weak classiﬁers popular method face detection. classify image pass classiﬁer. point classiﬁer score falls threshold image classiﬁed negative. passes cascade classiﬁed positive. popular implementations comes opencv uses cascade weak classiﬁers. threshold values opencv implementation pre-set based heuristics reason think optimal given face detection problem. goal tune thresholds optimising training set. modiﬁed opencv implementation take thresholds parameters. domain chose neighbourhood around conﬁguration used opencv. ﬁdelity experiment second ﬁdelity used images viola jones face database ﬁrst used figure results real experiments. ﬁrst three ﬁgures hyper-parameter tuning tasks last astrophysical maximum likelihood problem. title states experiment dimensionality number ﬁdelities. three hyper-parameter tuning tasks plot best cross validation error astrophysics task plot highest likelihood hyper-parameter tuning tasks obtained lower ﬁdelities using smaller training sets indicated ﬁgures astrophysical problem used coarser grid numerical integration indicated grid. mf-naive visible last experiment performed poorly. curves produced averaging experiments. error bars indicate standard error. lengths curves different time method pre-speciﬁed number iterations concluded different times. type supernovae type supernovae data davis maximum likelihood inference cosmological parameters hubble constant dark matter fraction dark energy fraction unlike typical parametric maximum likelihood problems machine learning likelihood available black-box. computed using robertson–walker metric requires numerical integration sample dataset. ﬁdelity task. third ﬁdelity integration performed using trapezoidal rule grid size ﬁrst second ﬁdelities used grids size respectively. goal maximise likelihood third ﬁdelity. section present proofs main theorems. self contained reader beneﬁt ﬁrst reading intuitive discussion section goal section bound simple regret given recall random number plays within capital trivial upper bound loose purposes. fact show sufﬁciently large number queries ﬁdelity number queries ﬁdelities smaller sublinear hence number plays algorithm operates highest ﬁdelity. introduce notation keep track evaluations ﬁdelity mf-gp-ucb. steps queried multiple times ﬁdelities. denotes denotes subset number queries ﬁdelity steps. roadmap bound discrete continuous settings begin studying algorithm evaluations ﬁdelity analyse following quantity readers familiar bandit literature similar notion cumulative regret except consider queries ﬁdelity. identify contains high value payoff function determined approximations provided lower ﬁdelity evaluations. then decompose follows evaluations inside multi-ﬁdelity setting. hence regret mf-gp-ucb scale instead case gp-ucb. finally convert bound terms depends show total number evaluations number highest ﬁdelity evaluations order sufﬁciently large. this list ingredients lemma section bounded intuitions lucid discrete proofs continuous case. latter dilated slightly dilation shrinks i.e. dilation appears covering argument extending discrete proofs continuous analysis need control conditional standard deviations queries subset provide lemma below whose gist borrowed srinivas lemma time query observe assume queried points points denote posterior variance time i.e. queries. thenxt∈a proof queries inside order queried. assuming queried inside denote ˜σt− posterior standard deviation queries. then queries outside decrease variance upper bound ﬁrst posterior variances queries third step uses inequality log/ ˜σt−/η last step uses lemma appendix result follows fact maximises mutual information among subsets size discrete proof theorem consider mf-gp-ucb steps control number evaluations different ﬁdelities different regions place. allow bound among things total number plays within capital number ﬁdelity evaluations outside need following lemmas analysis. lemma used establish different proofs upper bounds lemma bounds given sections respectively. since sublinear queried number times bound less therefore expended budget rounds satisﬁes λn/. since bounds hold probability invert inequality bound number random plays capital i.e. need make sure guaranteed since λ/λ. proceed bound terms ˜rn. first note following bound instantaneous regret. straightforward argument using gaussian concentration union bound. would like emphasise subtle conditioning argument multiple ﬁdelities. consider given allows lemma posterior conditioned queries gaussian constraints. using conditioning argument repeatedly analysis. statement follows union bound proof lemma first consider assume already queried ηβn/γ times since gaussian variance observations queries elsewhere /βn. therefore decrease conditional variance have design algorithm play ﬁrst line enumerates conditions algorithm played time ﬁdelity second step relaxed conditions noting particular maximised must larger ϕt). last step uses fact assumption ϕt). consider event choose ηβn/∆ third step used ηβn/∆ fourth step uses lemma conditioning uses last step uses δ/|x|π union bound second inequality theorem follows noting terms summation. before used queried time least larger larger assumption theorem. second condition necessary ensure switching procedure proceeds beyond ﬁdelity. also necessary relaxed them. ﬁrst bound probability event prove theorem require fairly delicate continuous setting. denote covering number metric analysis time consider instance sufﬁcient discretisation would equally spaced grid nα/d points side. {ain}n points covering {ain}n cells covering i.e. points closest union sets next deﬁne another partitioning space similar spirit using covering. first {ain since typically sublinear natural recursively apply obtain tighter bound instance since polylog kernel theorem repeating argument however improves dependence worsens dependence fact using discretisation argument similar lemma variance bound lemma polylog/poly bound shown poly kernel mat´ern kernel. appears. kernel log+d//γd mat´ern kernel log+d/γd. ﬁrst observation scales volume also grows rate therefore dependence discrete case mat´ern kernel worse /γd+ dependence. secondly also indicates approximation good small therefore minimum capital also decreases. proof lemma since posterior variance decreases observations upper bound considering posterior variance observations maximum variance within occurs pick points distance apart observations highest posterior variance. therefore bound scenario. depends kernel. denote gram matrix scenario described using sherman-morrison formula posterior variance plugging bound retrieves ﬁrst result κ/h. mat´ern kernel lipschtiz constant atd. second result since kernel decays fast stronger result posterior variance translates better bound theorems. first invoke discretisation used proof lemma event already accounted lemma bint argmaxx∈ain maximiser upper conﬁdence bound time using relaxation ϕt]t) introduced studied multi-ﬁdelity bandit problem gaussian process assumptions builds work multi-ﬁdelity k-armed bandits theorems demonstrate mf-gp-ucb explores space using cheap lower ﬁdelities uses higher ﬁdelity queries successively smaller regions hence achieving better regret single ﬁdelity strategies. experiments synthetic functions three hyper-parameter tuning tasks astrophysical maximum likelihood estimation problem demonstrate efﬁcacy method generally utility multi-ﬁdelity framework. matlab implementation experiments downloaded github.com/kirthevasank/mf-gp-ucb. going forward wish study multi-ﬁdelity optimisation different model assumptions extend algorithm deal approximations structured ﬁdelity spaces. total budget used experiment. limit avoid unnecessary computation problems queries required maximum. methods multi-ﬁdelity optimisation none made code available methods straightforward implement includes mf-sko. straightforward incorporate lower ﬁdelity information gp-ucb share kernel parameters. kernel learned jointly maximising marginal likelihood. idea seems natural mixed results practice. problems improved performance methods others performed poorly. explanation lower ﬁdelities approximate function values always best described kernel. results presented lower ﬁdelities learn robust. mf-gp-ucb learned independently using queries ﬁdelity querying uniformly random highest ﬁdelity taking maximum. problems variant mf-naive instead gp-ucb queried ﬁrst ﬁdelity uniformly random. problems better querying gp-ucb probably since unlike gp-ucb stuck maximum however generally performed worse. figure performance implementation mf-gp-ucb different values ﬁdelity borehole experiment. implementation uses techniques heuristics described section experiments used also shown curve gp-ucb reference. cost approximation figure test implementation -ﬁdelity borehole experiment different costs approximation. ﬁxed varied increases performance worsens expected. indistinguishable gp-ucb overhead managing ﬁdelities becomes signiﬁcant compared improvements using approximation. approximations natural mf-gp-ucb performs approximations lower ﬁdelities. found implementation heuristics suggested section quite robust. demonstrate using currin exponential function using negative ﬁrst ﬁdelity approximation i.e. figure illustrates gives simple regret understandably loses single ﬁdelity methods since ﬁrst ﬁdelity queries wasted spends time second ﬁdelity recovering approximation. however eventually able achieve regret. remark argue functions sampled assumption i.e. occurs positive probability. then generative mechanism would repeatedly sample output constraints satisﬁed. since sufﬁcient show supremum bounded positive probability. argument straightforward discrete. continuous claim true well behaved kernels. instance using assumption establish high probability bound lipschitz constant sample since given positive need lipschitz constant /diam. description number ﬁdelities. payoff function ﬁdelity approximation. typically denotes capital resource expended upon evaluation ﬁdelity. cost i.e. amount capital expended querying ﬁdelity random number queries ﬁdelity within capital domain optimising optimum point value ﬁdelity function. complement x\\a. cardinality countable. logical respectively. inequalities equality ignoring constant terms. instantaneous reward regret respectively. simple regret spending capital mint=...n bound maximum difference mean ﬁdelity conditioned covariance ﬁdelity conditioned standard deviatiation ﬁdelity conditioned time queried point observation time queried ﬁdelity time queries ﬁdelity time {}tmt=m. coefﬁcient trading exploration exploitation ucb. theorems upper conﬁdence bound provided ﬁdelity combined provided ﬁdelities minm parameter mf-gp-ucb switching ﬁdelity ﬁdelity cumulative regret rounds. number queries ﬁdelity subset time number queries ﬁdelities greater subset time number plays strategy querying ﬁdelity within capital λ/λ. partitioning discrete case. equation partitioning continuous compact. equation analysis mf-gp-ucb hinges partitioning. additional n-dependent inﬂation paragraph equation arms above/below good ﬁdelity problems. inﬂated good mf-gp-ucb. ργ}. ε–covering number subset metric. minimum capital needs expended bound hold theorem expected improvement multi-ﬁdelity sequential kriging optimisation naive multi-ﬁdelity method described section dividing rectangles squared exponential", "year": 2016}