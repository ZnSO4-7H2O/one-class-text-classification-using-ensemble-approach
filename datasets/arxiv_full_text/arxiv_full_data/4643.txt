{"title": "Multi-view constrained clustering with an incomplete mapping between  views", "tag": ["cs.LG", "cs.AI"], "abstract": "Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated constraints to be transferred across views via the partial mapping. It uses co-EM to iteratively estimate the propagation within each view based on the current clustering model, transfer the constraints across views, and then update the clustering model. By alternating the learning process between views, this approach produces a unified clustering model that is consistent with all views. We show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views. Our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data, explaining their benefit to clustering performance in both single- and multi-view learning scenarios.", "text": "abstract. multi-view learning algorithms typically assume complete bipartite mapping diﬀerent views order exchange information learning process. however many applications provide partial mapping views creating challenge current methods. address problem propose multiview algorithm based constrained clustering operate incomplete mapping. given pairwise constraints view approach propagates constraints using local similarity measure instances mapped views allowing propagated constraints transferred across views partial mapping. uses co-em iteratively estimate propagation within view based current clustering model transfer constraints across views update clustering model. alternating learning process views approach produces uniﬁed clustering model consistent views. show approach signiﬁcantly improves clustering performance several methods transferring constraints allows multi-view clustering reliably applied given limited mapping views. evaluation reveals propagated constraints high precision respect true clusters data explaining beneﬁt clustering performance singlemulti-view learning scenarios. using multiple diﬀerent views often synergistic eﬀect learning improving performance resulting model beyond learning single view. multi-view learning especially relevant applications simultaneously collect data diﬀerent modalities unique modality providing views data. example textual ﬁeld report associated image video content internet page contain text audio. view contains unique complementary information object; combination views yield complete representation original object. concepts challenging learn view easier recognize another view providing avenue improve learning. multi-view learning share learning progress single view improve learning views direct correspondences views. current multi-view algorithms typically assume complete bipartite mapping instances diﬀerent views represent correspondences denoting object represented views. predictions model view transferred mapping instances views providing additional labeled data improve learning. however happens partial mapping views limited number objects multi-view representations? problem arises many industrial military applications data diﬀerent modalities often collected processed stored independently specialized analysts. consequently mapping instances different views incomplete. even situations connections views recorded sensor availability scheduling result many isolated instances diﬀerent views. although feasible identify partial mapping views lack complete bipartite mapping presents challenge current multi-view learning methods. without complete mapping methods unable transfer information involving isolated instance views. address problem propose method multi-view learning incomplete mapping context constrained clustering. constrained clustering class semi-supervised learning methods cluster data subject hard soft constraints specify relative cluster membership pairs instances. constraints serve background information clustering specifying instance pairs belong either cluster diﬀerent clusters given constraints view approach transfers constraints aﬀect learning views. complete mapping constraint direct correspondence views therefore directly transferred views using current methods. however partial mapping constraints instances equivalences views presenting challenge multi-view learning especially mapping limited. article proposes ﬁrst multi-view constrained clustering algorithm considers incomplete mapping views. given incomplete mapping approach propagates given constraints within view pairs instances equivalences views. since propagated constraints involve instances mapping views fig. illustration multi-view constrained clustering disjoint data views text images. given limited mapping views pairwise constraints images view must-link constraints cannot-link constraint based current clustering given constraint propagated pairs images close proximity given constraint mapped text view. propagated must-link cannot-link constraints directly transferred mapping form constraints text documents inﬂuence clustering next co-em iteration. directly transferred instances views aﬀect clustering. weight propagated constraint given similarity original constraint measured local radial basis weighting function based current estimate clustering. process depicted figure approach uses variant co-em iteratively estimate propagation within view transfer constraints across views update clustering model. experiments show using co-em constraint propagation provides eﬀective mechanism multiview learning incomplete mapping views yielding signiﬁcant improvement several mechanisms transferring constraints across views. also demonstrate constraint propagation improve clustering performance even single-view scenarios demonstrating precision inferred constraints utility constraint propagation. ﬁrst survey related work constrained clustering multi-view learning section present details section speciﬁc constrained clustering co-em algorithms base approach. section describes problem setting mathematical notation. develop multiview constrained clustering algorithm section describing constraint propagation clustering processes sections extension views section implementation eﬃciency section section evaluates performance approach several multi-view scenarios analyzes performance constraint propagation indeconstrained clustering algorithms incorporate side information inﬂuence resulting clustering model. constrained clustering research focused using side information given constraints specify relative cluster membership sets instances. typically algorithms must-link constraints specify sets instances belong cluster cannot-link constraints specify instances belong diﬀerent clusters. depending algorithm labeled knowledge treated either hard constraints cannot violated soft constraints violated penalty. types constraints successfully integrated wide variety clustering methods including k-means mixturemodels hierarchical clustering spectral clustering density-based techniques. although approach current constrained clustering algorithms focus using k-means variants concentrate survey methods. cop-kmeans ﬁrst constrained clustering algorithm based k-means performs k-means clustering ensuring constraints honored cluster assignments. pckmeans performs soft constrained clustering combining k-means objective function penalties constraint violations. mpck-means algorithm builds pck-means learn distance metrics cluster clustering process. pck-means mpck-means algorithms base clustering methods experiments describe algorithms detail section kulis explore connections several diﬀerent formulations constrained clustering showing semi-supervised weighted kernel k-means graph clustering spectral clustering closely related. based connections develop kernel-based constrained clustering approach operate either vectorgraph-based data unifying areas. domeniconi propose alternative formulation semi-supervised clustering using composite kernels suitable heterogeneous data fusion. models generated pck-means/mpck-means equivalent particular forms gaussian mixtures work also closely related research constrained mixture modeling. shental incorporate hard constraints gaussian mixture models using equivalence formulation learn soft constrained gaussian mixture model using constraints inﬂuence prior distribution instances clusters. recent work focused incorporating constraints nonparametric mixture models intuitive nature must-link cannot-link constraints user interaction constrained clustering applied problem interactive clustering system user collaborate generate model. cohn desjardins present interactive approaches user iteratively provides feedback improve quality proposed clustering. cases user feedback incorporated form constraints. interactive process useful extension could permit user knowledge brought multi-view clustering algorithm. additionally discuss section user could specify constraints view data interaction quick intuitive multi-view clustering algorithm could used automatically propagate transfer constraints aﬀect clustering views interaction diﬃcult interaction could improved active learning query users speciﬁc constraints multi-view learning originated blum mitchell cotraining algorithm semi-supervised classiﬁcation. co-training uses model view incrementally label unlabeled data. labels predicted high conﬁdence transferred corresponding unlabeled instances views improve learning process iterates instances labeled. co-training assumes independence views shows decreased performance assumption violated dasgupta provide generalization analysis co-training algorithm bounds error co-training based observed disagreement partial rules. analysis relies independence views often violated real-world domains. alternative co-training nigam ghani present co-em algorithm iterative multi-view form expectation maximization co-em algorithm probabilistically labels data transfers labels view iteration repeating process convergence. unlike co-training require independence views order perform well. co-em algorithm forms foundation approach described detail section clustering multiple views introduced bickel scheﬀer developed multi-view algorithm alternates views used learn model parameters estimate cluster assignments. typical goal multi-view clustering learn models exhibit agreement across multiple views data. chaudhuri develop multi-view approach support clustering high-dimensional spaces. canonical correlation analysis construct low-dimensional embeddings multiple views data apply co-training simultaneously cluster data multiple lower-dimensional spaces. extends spectral clustering multi-view scenario objective minimize disagreement views. kumar daum´e’s work co-training approach used perform spectral clustering within view constraint similarity graph another view subsequent work used regularization approach optimize shared clustering eﬀect approaches propagating entire clustering across views contrast method propagates explicit constraints nearby pairs instances. also unlike approach works assume complete bipartite mapping views. multi-view clustering variations based cross-modal clustering perceptual channels information-theoretic frameworks recently several researchers studied ways develop mappings alternative views. harel mannor describe method learning refer multiple outlooks. outlooks similar views diﬀerent feature spaces; however assumption instances would appear multiple outlooks. learn aﬃne mapping scales rotates given outlook another outlook matching moments empirical distributions within outlooks. work assumes outlooks mapped globally single aﬃne mapping whereas work assumes local potentially nonlinear mappings views based learned clustering structure. quadrianto lampert introduce technique projecting multiple views common shared feature space producing joint distance function across views. work assumes instance appears every view. neighborhood relationship deﬁnes similar instances optimize similarity function shared feature space respects neighborhood relationships. problem setting work rather diﬀerent ours since assume complete bipartite mapping views provided input consists neighborhood sets rather pairwise constraints. signiﬁcant amount recent research applying co-training perform clustering relational data. methods generally attributes objects relational network view relations another view greene cunningham example combine text similarity co-citation data identify communities citation data. banerjee introduce general framework co-clustering attributes relations using approach ﬁnds clustering minimal information loss. banerjee al.’s framework applied single combination bregman loss functions permitting applied variety diﬀerent contexts. bhattacharya getoor show problem entity resolution seen constrained clustering problem. information combined equating multiple objects entity resolution seen analogous multi-view clustering role entity plays relational network thought view object. applications multi-view clustering include image search biomedical data analysis audio-visual speech gesture analysis multilingual document clustering word sense disambiguation e-mail classiﬁcation constrained clustering algorithms take input constraints inform clustering process. pairwise constraint typei denotes relative clustering instances non-negative weight constraint given type {must -link cannot -link} speciﬁes whether belong either cluster diﬀerent clusters soft constrained clustering viewed penalty violating constraint. throughout article wherever weight type constraint obvious context omit indicate pairwise constraint mentioned earlier approach combined constrained clustering method. current implementation supports pck-means mpck-means algorithms; give results methods section remainder section provide brief overview methods; details available original papers. data point given previous assignments points clusters. m-step consists parts re-estimating cluster centroids given e-step cluster assignments updating metric matrices {mh}k decrease jmpck latter step enables mpck-means learn metrics cluster combination learning constrained clustering model. learning mahalanobis metric also considered xing bar-hillel pck-means algorithm simpliﬁed form approach minimizes objective function mpck-means eliminates metric learning aspect assumes identity distance metric setting co-em iterative algorithm based expectation maximization learns model multiple views data. iteration co-em estimates model view uses probabilistically label data; labels transferred train another view next iteration. co-em repeats process models views converge. co-em algorithm nigam ghani given algorithm note algorithm assumes complete bipartite mapping views order transfer labels. unlike co-training co-em require views independent order perform well. co-em algorithm also learns large data noisy labels iteration contrast co-training adds labeled instances training iteration. reason nigam ghani argue co-em algorithm closer spirit original framework multi-view learning described blum mitchell co-training algorithm. approach explore article uses varitakes input multiple views data view data given instances feature dimensionality diﬀer views. initially focus case views given extend approach handle arbitrary number views section within pairs instances correspond diﬀerent views objects. denote connection instances diﬀerent views relation relations ra×b deﬁnes bipartite graph work multi-view learning assumes ra×b deﬁnes complete bipartite mapping ra×b provides partial mapping views. also consider instances relations views view given v×r+ denotes space possible pairwise constraints view depending application constraints either manually speciﬁed user extracted automatically labeled data. note constraints describe diﬀerent views although focus primarily case views also generalize approach multiple views described section objective approach determine k-partitioning data view respects constraints within view mapping views. approach given algorithm iteratively clusters view infers constraints within view transfers inferred constraints across views mapping. process progress learning model view rapidly transmitted views making approach particularly suited problems diﬀerent aspects model easy learn view diﬃcult learn others. compute transitive closure cascbsra×b. augment ra×b additional constraints transitive closure involving instances from respectively instances involved ra×b; similarly deﬁne cross-view relations ra×b. either pck-means mpck-means algorithms ckmeans subfunction native support soft constraints mpck-means metric learning. however approach utilize constrained k-means clustering algorithms provided meet criteria ckmeans function listed above. clustering model across views using variant coem algorithm described section estep propagate given constraints based current clustering models instances direct mappings views propagated constraints described section inﬂuence clustering m-step note instead taking direct union constraints keep maximally weighted constraint type every pair instances; operation notated operator step following previous work co-em multi-view clustering iterate e-step view propagate constraints followed m-step view transfer constraints update clustering. iteration co-em loop contains iterations e-step m-step view. relationship steps illustrated figure co-em process continues view internally converged. assume convergence occurred pck-means/mpck-means objective function’s value diﬀers less successive iterations. like nigam ghani observed co-em variant converged iterations practice. iterative exchange constraints views ensures consistent clustering respects constraints within mapping views. next sections describe step co-em process detail. direct mapping endpoints likely small. consequently unable directly many constraints views; constraint cannot represents lost information improved clustering. instances view mapped another view. given initial constraints infer constraints pairs instances based local similarity constraints propagate constraint pair suﬃciently similar original constraint. process essentially considers spatial constraints aﬀect endpoints local neighborhoods instance space around endpoints. eﬀect pair points neighborhood realized weighted constraint instances. constraint propagation method infers constraints instances respect current clustering model. since constraints instances direct mapping views constraints directly transferred views propagation process occurs respect current clustering model view since k-means variants base learning algorithm learned model essentially equivalent gaussian mixture model particular assumptions uniform mixture priors conditional distributions based constraints therefore consider cluster generated gaussian covariance matrix base clustering algorithms support metric learning cluster covariance related inverse cluster metric learned part clustering process. bar-hillel note that practice metric learning typically constructs metric modulo scale factor although scale factor aﬀect clustering since relative distances required constraint propagation requires absolute distances. therefore must rescale learned covariance matrix match data. compute based empirical covariance data adding small amount regularization ensure non-singular compute scale small data samples. given variances ﬁrst principal component matrix identical. take eigendecomposition matrix yielding αhm− covariance matrix cluster base learning algorithm support metric learning pck-means instead cluster covariance matrix. model cluster given assume constraint propagated respect current clustering model shape propagation equivalent shape respective clusters additionally assume propagation distance proportional constraint’s location cluster. intuitively constraint located near center cluster propagated distance cluster’s edges since located near center cluster implies model high conﬁdence relationship depicted constraint. similarly constraint located near edges cluster propagated short distance since relative cluster membership points less certain cluster’s fringe. according gaussian radial basis function distance construction weight propagated constraint decreases according centered dimensional space original constraint’s endpoints form propagation covariance matrices endpoint scale covariance matrix associated endpoint weight assigned endpoint according clustering model ensures amount propagation falls increasing distance centroid direct relation model’s conﬁdence cluster membership covariance matrix constraint propagation function given denotes cluster denotes zero matrix. construction assumes independence assumption likely violated practice empirically show yields good results. convenience represent covariance matrices associated endv figure point illustrates results process example cluster. fig. constraint propagation applied single example cluster showing learned covariance matrix rescaled data constraints weighting functions centered endpoint decrease variance move farther centroid since ordering instances matters propagation compute possible pairings constraint endpoints target endpoints taking maximum value propagation equation determine best match. propagation scheme constraint propagated endpoints given weight weight propagated constraint decreases endpoints section describes mechanisms implementing constraint propagation eﬃciently taking advantage independence assumption endpoints constraint memoization repeated computations. e-step algorithm uses equation propagate given constraints within view instances cross-view mappings thereby inferring expected value constraints instances given constraints views update clustering model reﬂect constraints. steps together constitute m-step algorithm propagated constraint endpoints transferred another view bipartite mapping ru×v deﬁne mapping typei function fu→v takes given constraint maps constrain instances using construction deﬁne mapping functions fb→a fa→b algorithm functions fa→b fb→a propagated constraints views step transferring constraints inferred view related views. transferred constraints combined original constraints view inform clustering. instead taking direct union constraints keep maximally weighted constraint pair instances form independently maintains sets given constraints threshold data involved cross-view relations current partitioning current cluster metrics propagated constraints handle views maintain separate mappings ru×v pair views mapping deﬁne pairwise mapping functions fu→v views. views approach yield mapping functions. generalize approach views hold propagated constraints ﬁxed iteratively update clustering recompute propagated constraints view. uniﬁed sets constraints view becomes overall computational complexity algorithm determined maximum number iterations complexity ckmeans function depends chosen clustering algorithm. besides aspects constraint propagation step incurs greatest computational cost. make step computationally eﬃcient implementation relies independence assumption inherent equation endpoints constraint. eﬃciently compute weight propagated constraints memoize value endpoint’s propagation gaussian evaluations. memoization applies similarly views. constraint propagation inherently independent others making approach suitable parallel implementation using hadoop/mapreduce evaluated multi-view constrained clustering variety data sets synthetic real showing approach improves multi-view learning under incomplete mapping compared several methods. results reveal constraints inferred propagation high precision respect true clusters data. also examined performance constraint propagation individual views revealing constraint propagation also improve performance single-view clustering scenarios. order examine performance approach various data distributions combination synthetic real data experiments. follow methodology nigam ghani create multi-view data sets pairing classes together create super-instances consisting instance class pair. original instances represent diﬀerent views super-instance connection forms mapping views. methodology trivially extended arbitrary number views. data sets described summarized table four quadrants synthetic data composed instances drawn four gaussians space identity covariance. gaussians centered coordinates four quadrants. quadrants belong cluster quadrants belong cluster. challenge simple data identify clusters automatically requires constraints improve performance beyond random chance. form views drew instances four gaussians divided evenly views created mappings nearest neighbors quadrant diﬀerent views. data previously used xing containing respectively instances classes connect instances following pairs classes create views construction model learned clustering view used inform clustering view. since clusters instances newsgroups {rec.autos rec.motorcycles} view instances newsgroups {talk.politics.guns talk.politics.mideast} talk view. process view independently removing stop words representing data binary vector discriminatory words determined weka’s string-to-wordvector ﬁlter previous data sets form mapping views pairing clusters order. create low-dimensional embedding data using spectral features order improve clustering exception four quadrants original features dimensionality already low. view compute pairwise aﬃnity matrix instances using radial basis function distance aﬃnity falls increasing distance. form normalized lapla identity matrix. eigendecomposition normalized laplacian matrix qλqt yields spectral features data columns eigenvector matrix keep eigenvectors features clustering; discard ﬁrst eigenvector since constant therefore discriminate rec/talk data set. additionally standardize features zero mean unit variance. spectral features computed independently diﬀerent views emphasizing mapping connection views. within view true cluster labels instances sample pairwise constraints ensuring equal proportions must-link cannot-link constraints. weight sampled constraints also sample portion mapping transferring constraints views. sets constraints mapping views resampled trial. compare constraint propagation several potential methcluster membership used infer constraints instances approach simply considers relative cluster membership pair instances infers appropriate type constraint weight represents direct application co-em infer transfer constraints views partial mapping. base constrained clustering algorithms pck-means mpck-means implementations provided wekaut extension weka machine learning toolkit default values. pck-means constraint propagation uses full empirical covariance cluster; mpck-means uses diagonal weighted euclidean metrics learned per-cluster basis mpck-means. measure performance using pairwise f-measure version information-theoretic f-measure adapted measure number same-cluster pairs clustering pairwise f-measure harmonic mean precision recall given trial consider performance vary number constraints used learning percentage instances view mapped views. results shown figure averaged trials. shown figure constraint propagation clearly performs better baseline single view clustering better cluster membership inferring constraints cases except learning constraints letters/digits. constraint propagation also yields improvement direct mapping percentage instances mapped views shown figure unlike direct mapping constraint propagation able transfer constraints would otherwise discarded increasing performance multi-view clustering. performance constraint propagation direct mapping improve mapping becomes complete fig. comparison multi-view constrained clustering performance. percentage instances mapped views denoted type line constraint transfer method denoted color marker shape. several plots truncated space limitations; plots markers colors depict constraint transfer methods plots. fig. performance improvement constraint propagation direct mapping figure averaged learning curve. peak whiskers depict maximum percentage improvement. views constraint propagation still retaining advantage direct mapping even complete mapping shown data sets. hypothesize case complete mapping constraint propagation behaves similarly spatial constraints warping underlying space inference constraints improve performance. data sets number constraints inferred constraint propagation approximately linear number original constraints shown figure clearly mapping views becomes complete constraint propagation able infer larger number constraints instances improvement clustering performance high precision propagated constraints. figure shows average weighted precision propagated constraints mapping case measured complete pairwise constraints inferred true cluster labels. proportion propagated constraint contributed weighted precision given constraint’s inferred weight also measured precision propagated constraints various partial mappings results comparable complete mapping. constraints inferred propagation show high average precision data sets signifying propagation method infers incorrect constraints. omit four quadrants results figure relatively high performance gain constraint propagation averages improvement direct mapping peak gains much greater scale improvements obvious would make smaller gains domains less discernible ﬁgure. fig. number propagated constraints function number original constraints multi-view clustering averaged co-em iterations. line type depicts mapping percentage deﬁned corresponding plot figure explained counting argument many chances cannotlink constraint correctly propagated must-link constraint. example clusters cluster contains instances given mustfound high-dimensional data curse dimensionality causes instances separated constraint propagation able infer constraints weight. consequently works best low-dimensional embedding data motivating spectral feature reduction. approaches could also used creating low-dimensional embedding principal components analysis manifold learning. additionally like multi-view algorithms found constraint propagation somewhat sensitive cutoﬀ thresholds problem remedied using cross-validation choose high threshold yields performance identical direct mapping threshold yields decreased performance exhibited co-training algorithms. reason recommend setting optimize cross-validated performance constrained instances. several additional experiments multi-view data sets poor mappings distributions violated mixture-of-gaussians assumption k-means clustering. data sets constraint propagation decreased performance cases inferring constraints justiﬁed data. would occur example clusters nested half-moon shape concentric rings constraint propagation would incorrectly infer constraints instances opposing cluster. cases clustering using directly mapped constraints yielded best performance. dissect beneﬁts constraint propagation independently multiview learning evaluated constraint propagation traditional single-view clustering scenarios. found using constraints inferred propagation also improve performance single-view clustering explaining performance multi-view scenarios also demonstrating high quality inferred constraints. instead propagating constraints across views used constraint propagation infer additional constraints within single view employed constraints inform clustering view used resulting clusters guide propagation next iteration clustering. process repeated clustering model view converged. implement approach modiﬁed algorithm single view eliminating steps involving view altering step altering step compute uniﬁed constraints modiﬁcations algorithm alternates m-step clustering e-step constraint propagation learn model single view evaluated performance single-view clustering method individual data sets. figure depicts performance single-view clustering constraint propagation sample data sets averaged trials. parameters speciﬁed described table exceptions using protein digits since individual view. compare single-view clustering constraint propagation standard constrained clustering. analysis omits methods tested multi-view experiments since direct mapping analogous case singleview scenarios constraints inferred cluster membership single view alter resulting model. results show constraints inferred propagation within single view also used improve clustering performance view. mpck-means pck-means show improved performance using constraints inferred propagation augment original constraints. maximum improvement constraint propagation occurs moderate number constraints; would expect constraint propagation provides little beneﬁt number original constraints either small large. even single-view case number constraints inferred propagation roughly linear number original constraints also found inferred constraints high precision true cluster assignments single-view scenarios multi-view experiments inferred cannot-link constraints slightly higher precision inferred must-link constraints. fig. comparison single-view clustering performance without constraint propagation. solid lines depict clustering constraint propagation dotted lines depict standard constrained clustering. base algorithm either mpck-means pck-means denoted color marker shape. black error bars depict standard error mean performance. decrease performance applied data sets violate assumptions k-means clustering. conditions assumed k-means necessary successful constraint propagation that clusters globular cluster metrics clusters separable however diﬃcult determine priori whether particular data meets conditions therefore appropriate candidate constraint propagation. reason important determine whether data satisﬁes conditions clustering process; next discuss potential methods making determination. cases data violate conditions observed clustering primarily driven provided constraints rather k-means component. strengths mpck-means pckmeans algorithms situations k-means poorly data large numbers constraints overcome k-means component ensure proper clustering. terms pck-means/mpck-means objective function second third terms equation dominate ﬁrst term situations. data meet conditions cluster distributions speciﬁed learned centroids metrics little common resulting partitions. clustering process examine learned cluster distributions disagreement resulting partitions; presence large disagreement indication data inappropriate constraint propagation. determine whether data meets second condition also directly examine resulting clusters overlap. since constraints propagated based distance given constraint relative cluster membership endpoints constraint propagation infer constraints correctly points belonging overlapping clusters. detect overlap clusters measuring divergence learned distributions; presence signiﬁcant overlap another indication data violate conditions necessary successful constraint propagation. currently simply alert user data inappropriate constraint propagation based either indicator; leave robust determination whether constraint propagation guaranteed improve performance future work. constraint propagation ability improve multi-view constrained clustering mapping views incomplete. besides improving performance constraint propagation also enables information supplied view propagated transferred improve learning views. especially beneﬁcial applications natural users interact particular views data. example users able rapidly intuitively supply constraints images require lengthy examination views order infer constraints. cases users access particular views privacy restrictions. scenarios approach would able propagate user-supplied constraints within across views maximize clustering performance. beyond approach variety methods could adapted learning partial mapping views manifold alignment transfer learning. work problem improve ability isolated instances corresponding multi-view representation improve learning enable multi-view learning used wider variety applications. acknowledgements. would like thank kiri wagstaﬀ katherine oates finin anonymous reviewers feedback. work based ﬁrst author’s master’s thesis umbc partially conducted ﬁrst author lockheed martin advanced technology laboratories. research supported graduate fellowship goddard earth sciences technology center umbc grant grant n--c- lockheed martin.", "year": 2012}