{"title": "There and Back Again: A General Approach to Learning Sparse Models", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a simple and efficient approach to learning sparse models. Our approach consists of (1) projecting the data into a lower dimensional space, (2) learning a dense model in the lower dimensional space, and then (3) recovering the sparse model in the original space via compressive sensing. We apply this approach to Non-negative Matrix Factorization (NMF), tensor decomposition and linear classification---showing that it obtains $10\\times$ compression with negligible loss in accuracy on real data, and obtains up to $5\\times$ speedups. Our main theoretical contribution is to show the following result for NMF: if the original factors are sparse, then their projections are the sparsest solutions to the projected NMF problem. This explains why our method works for NMF and shows an interesting new property of random projections: they can preserve the solutions of non-convex optimization problems such as NMF.", "text": "propose simple efﬁcient approach learning sparse models. approach consists projecting data lower dimensional space learning dense model lower dimensional space recovering sparse model original space compressive sensing. apply approach non-negative matrix factorization tensor decomposition linear classiﬁcation—showing obtains compression negligible loss accuracy real data obtains speedups. main theoretical contribution show following result original factors sparse projections sparsest solutions projected problem. explains method works shows interesting property random projections preserve solutions non-convex optimization problems nmf. settings sparse models trained efﬁciently dense analogs? sparse models commonly arise feature selection problems highly indicative features resource-constrained environments tasks sparsity reduce data requirements desired predictive power improve interpretability inference speed; range popular techniques provide means learning sparse models dense and/or high-dimensional data work study additional dimension sparse statistical models extent model’s sparsity leverage computational advantage. present general efﬁcient approach learning sparse models explicitly leverages target model sparsity. adapt now-classic techniques compressive sensing simple framework learning sparse models high-dimensional data project data lowerdimensional subspace using random projection learn dense model lower-dimensional subspace recover sparse model original dimensionality using compressive sensing. approach several beneﬁts. first allows perform bulk model training often much lower-dimensional dense subspace offering signiﬁcant efﬁciency gains data high-dimensional target model sparse. second useful settings dataset large cannot memory; approach allows work much smaller dimensional projection still recover original model. third provides conceptually simple approach learning sparse models combining powerful ideas dimensionality reduction compressive sensing. although low-dimensional projections extremely popular tools speeding variety optimization learning problems framework conceptually different interested able recover sparse solution original space solution lower-dimensional space. note even information theoretically possible many tasks even original model solution sparse solution lowerdimensional space projection original solution performing compressive sensing based recovery hope recovering original solution contributions. develop simple framework learning sparse models investigate context non-negative matrix factorization tensor decomposition linear classiﬁcation. show approach recovers true solution negligible error across several synthetic real datasets. empirically sparse learning allows compression negligible loss accuracy real data obtains speedups terms wall clock time obtain accuracy. theoretically establishing speed-ups high ﬁdelity reconstructions possible justiﬁed requires care establish novel uniqueness guarantees demonstrate original factors sparse projection factors original high-dimensional problem sparsest possible factorization low-dimensional projected problem. algorithms implicitly explicitly learn sparse factorizations results explain factorization preserved projection explaining empirical success procedure. results assume separability factors commonly used—but strong— assumption showing uniqueness nmf. also derive simple conditions uniqueness provable recovery tensor decomposition projected space. theoretical insights backed empirical ﬁndings illustrate beneﬁts random projections learn sparse models— learning faster without compromising model quality. results uniqueness random projection—and empirical ﬁndings—open perspective properties random projections. well known random projections preserve distances geometric properties results show random projections also preserve solutions non-convex optimization problems nmf. approach successfully leverages property efﬁciently learn sparse models believe aproach several promising applications beyond target settings. brieﬂy outline three main classes sparse models consider paper non-negative matrix factorization goal approximate non-negative matrix rm×n product non-negative matrices rm×r rr×n. matrix regarded dictionary stores combination weights column terms dictionary applications include document clustering image analysis crucial property enables learn good representation data coefﬁcients typically sparse document image expressed combination basis vectors dictionary. settings document clustering image classiﬁcation columns correspond samples data many scenarios number samples large datasets. interested setting large factor sparse. tensor decomposition. goal tensor decomposition decompose tensor here denotes column factor rm×r denotes outer product a⊗b⊗c rn×n×n aibjck. consider settings third mode corresponding factor represents data points large—but sparse. ﬁrst modes could correspond features data point; example topic modeling setup ﬁrst modes could represent word co-occurrences third mode representing different documents. linear classiﬁcation. also consider problem ﬁnding linear classiﬁer classify data points dimensions. modern datasets number features often large containing numerous irrelevant features settings best classiﬁer sparse relies features predictive/indicative. related work sparse recovery. compressive sensing sparse recovery framework sparse signal given measurements rd×n goal recover using measurements property sparse. compressive sensing extensively studied several different communities many algorithms proposed high compression rate allow fast decoding particularly relevant work work sparse recovery using sparse projection matrices results efﬁciently recover back solution original space solution projected space. evident difference approach compressive sensing direct access measurements solution projected space instead hope recover approximation solving optimization problem projected space. non-negative matrix factorization. many algorithms proposed popular ones multiplicative updates alternating least squares mentioned introduction many modern datasets number samples large. approaches multiplicative weights scale online setting data large memory. online setting many algorithms proposed dictionary learning problem goal learn matrix approaches unable learn coefﬁcient matrix context using random projection wang obtained speedups compared projecting matrix random projection. however attempt directly recover factors original space using factors projected space exploiting properties original solution sparsity. tensor decomposition. popular algorithm tensor decomposition alternating least squares proceeds ﬁxing factor matrices solving third least squares problem continuing till convergence. recently burst exciting work speeding sampling sketching approaches approaches obtain signiﬁcant speedups naive algorithm leverage fact factors tensor sparse exploit paper. random projections classiﬁcation. data points labels separated linear classiﬁer therefore sign. perform random projection points dimensions still separable? number papers examined basic question line summary results original data points separable large margin random projection preserves margin. framework however goal classify accurately projected space recover optimal classiﬁer projected space. zhang also consider question obtaining optimal classiﬁer original space optimal classiﬁer projected space. show novel approach based dual optimization problem projected space recover original classiﬁer data matrix approximately rank. framework instead employ sparsity original classiﬁer order able recover approach instantiate framework context tensor decomposition linear classiﬁcation. case obtain lower-dimensional problem random projection denote random projection matrix rd×n original dimension solving projected problem obtain approximation solution original space solving sparse recovery problem following linear program solution projected problem. compressive sensing literature technique also known basis pursuit describe speciﬁcs application method. non-negative matrix factorization. consider problem factorizing non-negative matrix rm×n product non-negative matrices interested setting matrix sparse ﬁrst project corresponds projecting dimensions dimensions. factorize zhang also obtain guarantees true classiﬁer sparse require data points settings like desired solution non-negative additional constraint practice projecting solutions post-hoc onto non-negative orthant often effective heuristic. omitting additional constraint typically yields faster solution times. solving problem estimate recover solve compressive sensing problem therefore rank-r approximation solve instances outputs compressive sensing step estimates rows tensor decomposition. goal decompose tensor rm×m×n terms factors sparse. project tensor along understand projection denote mode matricization tensor ﬂattening tensor along mode obtained stacking matrix slices together columns project project mode matricization rmm×n obtain projection mode matricization rmm×d computing decompose projected space obtain factors estimates original factors obtained solving compressive sensing problem column above yields instances linear classiﬁcation. problem classifying data points ﬁrst perform random projection project points dimensions using projection matrix hence projected points given solve classiﬁcation problem projected space learn classiﬁer ˜w∗. finally compressive sensing recover estimate true classiﬁer ˜w∗. choosing projection matrix several requirements projection matrix must satisfy. ﬁrst obvious requirement able solve compressive sensing problem recover sparse solution original space solution projected space. slightly subtle requirement chosen optimal solution original space solution projected space compressive sensing based recovery carried out. requires uniqueness optimization problem projected space requirement could depend original optimization problem. section analyze requirements tensor decomposition. finally computational efﬁciency want projections efﬁcient compute property sparse recovery problem solved efﬁciently using them. consider sparse projection matrix }d×n every column exactly non-zero entries refer number buckets coordinate original dimensional space contributes locations projected dimensional space. support non-zero entries column chosen independently uniformly random. property enables compressive sensing adjacency matrix bipartite expander consider bipartite graph nodes left part nodes right part every node left part degree call expander every subset nodes least neighbors difﬁcult show randomly chosen matrix non-zero entries column adjacency matrix expander high probability formally prove lemma appendix note sparse projection step carried efﬁciently. basis pursuit problem described seminal results compressive sensing show using dense projection guarantees recovery original solution k-sparse. property subsequently extended sparse projection matrices leverage fact. theorem berinde shows k-sparse deﬁned recovered using adjacency matrix expander. randomly chosen expander high probability hence used sparse recovery k-sparse signals. efﬁcient recovery algorithms much faster solving also known sparse projection matrices. hence using deﬁned ensures sparse recovery step computationally efﬁcient provably works original solution k-sparse solution projected space however still need ensure solution analyze case tensor decomposition next section summarize conditions table tensor decomposition. note bounds suggested theoretical results tight logarithmic factors projecting matrix tensor dimension rank preserve uniqueness decomposition. guarantees approach section show uniqueness guarantees performing non-negative matrix factorization tensor decomposition projected matrix tensor. establish factors projected space correspond projection factors original space prerequisite able compressive sensing recover factors original space. donoho stodden showed non-negative matrix factorization unique separability condition. however separability strong assumption. instead using separability show uniqueness results using sparsity weight matrix uniqueness results original space theorem similar principle uniqueness results obtained sparse coding spielman show matrix true rank factorization sparse rank factorization strictly non-zero entries found recover sparse solutions many algorithms explicitly enforce sparsity reasonable hope recovery procedure successful true solution sparsest possible solution. main contribution theorem part shows uniqueness results projected space. signiﬁcantly challenging showing uniqueness original space original space could fact entries drawn independently random simplify arguments. however lose access independence random projection step entries projected matrix longer independent. instead proof projected space uses expansion properties projection matrix control dependencies enabling prove uniqueness results projected space. theorem assumes full column rank non-zero entries independent gaussian random variables. gaussian assumption necessary stated simplicity—any continuous probability distribution non-zero entries sufﬁces. theorem shows rows k-sparse projecting dimensions preserves uniqueness failure probability re−βk/n constant theorem consider rank matrix rm×n factorization rm×r full column rank rr×n exactly non-zero entries chosen uniformly random entry drawn independently denotes element wise product. assume ﬁxed constant. consider projection matrix }d×n column exactly non-zero entries expander assume note factorization then proof. prove part theorem appendix here provide outline proof challenging part moving proof details appendix argue space matrix ﬁrst claim space equals space verify note space subspace space full column rank rank space equals rank space therefore space equals space argument alternative factorization space must equal space equals space space equals space therefore must space therefore goal prove rows sparsest vectors space implies alternative factorization outline proof follows. first argue take subset rows number columns non-zero entries least rows large. implies taking linear combination rows result vector large number non-zero entries—unless non-zero entries cancel many columns. using properties projection matrix fact non-zero entries original matrix drawn continuous distribution show happens zero probability. lemma shows number columns least zero entry subset rows grows proportionately size proof proceeds showing choosing randomly chosen non-zero entries ensures expansion high probability fact given expander. lemma subset rows deﬁne subset columns non-zero entry least rows every subset rows min{|s|kp/ failure probability re−βk/n. prove second part argument–that linear combination rows cannot much fewer non-zero entries probability many non-zero entries canceled zero. lemma showing this. deﬁne vector fully dense entries non-zero. lemma subset rows submatrix corresponding rows columns. probability every subset columns size least |s|p fully dense vector left null space. proof. without loss generality assume corresponds ﬁrst rows corresponds ﬁrst columns partition columns groups g|s|}. group size select ﬁrst group choose entry appears ﬁrst column example ﬁrst ﬁrst column random variable appears ﬁrst column choose choose columns appears. remove columns select second group pick remaining columns choose entry appears column columns appears. repeat procedure obtain groups size every variable appears columns. left null space ﬁrst groups columns. deﬁne r|s|. show either rank contain fully dense vector. prove induction. consider step groups gj}. induction hypothesis either contain fully dense vector rank contain fully dense vector done implies also contain fully dense vector. assume contains fully dense vector choose column already assigned sets. following elementary proposition probability orthogonal zero. provide proof appendix. proposition vector independent random variables. subset r|s| refer subset corresponding indices consider subsets deﬁnes linear relation r|si|. assume variable appear probability distribution variables {vt+ conditioned linear relations deﬁned still continuous density function full support. particular linear combination variables {vt+ zero probability zero. contains fully dense vector probability rank rank− n−j−. proves induction argument. therefore probability either rank n|s| contain fully dense vector lemma follows. figure pearson correlation recovered factors enron dataset data non-negative tensor factorization ‘recovered coefﬁcients’ refers factors obtained along larger mode matrix/tensor projected down. ‘dictionary’ refers factors along modes projected down. complete proof second part theorem note rows non-zero entries. consider rows consider linear combination rows combination weights r|s| non-zero. lemma min{|s|kp/ failure probability re−βk/n. claim |s|p zero entries. prove contradiction. assume fewer zero entries. consider submatrix corresponding rows columns. |s|p fewer zero entries must subset columns |s|p columns least non-zero entry fully dense vector lies left null space lemma probability happening zero. hence |s|p zero entries. lemma obtains lower bound |s|p using simple algebra. lemma |s|p hence linear combination least non-zero entries failure probability re−βk/n. hence rows sparsest vectors space failure probability re−βk/n. guarantees tensor decomposition showing uniqueness tensor decomposition random projection difﬁcult tensor decompsition unique every general condition factor matrices example tensors unique decomposition factor matrices full rank. following elementary fact follows property whenever projection matrix full column rank true high probability randomness choosing wiai full column rank tensor unique decomposition projecting dimension hence tensors unique decomposition projecting dimension least rank tensor decomposition beyond uniqueness prove efﬁcient recovery factors also possible projected space. show using known guarantees tensor power method popular tensor decomposition algorithm. results provable recovery require projection dimension higher uniqueness requirement possibly gaps analysis tensor power method. state results appendix experiments figure visualization sample factor decomposition signal tensor. decomposition original space. bottom decomposition compressed version tensor temporal dimension reduced notice able recover major features factors including compressed figure sparse recovery linear classiﬁer censusincome dataset additional distractor features. dashed line shows score lregularized logistic regressor trained uncompressed space. plotted scores medians runs. observe obtain compression loss accuracy. evaluate practical feasibility sparse model recovery series experiments real synthetic data. experiments server equipped xeon .ghz ram. timing measurements taken single-threaded execution. non-negative matrix factorization. evaluate performance compressed number standard non-negative datasets. experiments used matlab’s built-in nnmf alternating least squares. unless otherwise stated optimization convergence tolerance rank table give normalized frobenius errors along wall clock runtimes original compressed space. lp-based reconstruction recovery step. table shows compressed runs achieve achieve speedup cost slightly increased error. usually interested learning interesting factors rather minimizing squared error linear correlation recovered factors ground-truth factors gives ﬁner-grained evaluation recovery quality frobenius error alone. factors obtained original space convergence proxy ground-truth absence cluster labels evaluation. since sets factors order compute correlation scores maximum bipartite matching. fig. plot correlation factors enron word-document count matrix various compression factors. ﬁxed quality level able achieve speedups operating compressed space able recover back solution original space small error using compressive sensing even compression. tensor decomposition. chb-mit scalp database collection scalp electroencephalogram recordings children susceptible epileptic seizures. recording consists real-valued time series; channel corresponds measurement electrical activity different region brain. preprocess trace dataset computing spectrogram channel. yields non-negative tensor size corresponding approximately hours data. expect data exhibit desired sparsity property since signals typically exhibit well-localized time-frequency structure. experiments rank compute non-negative decomposition tensor using projected cp-als tensor projected along ﬁrst mode using random projection described section treat factors obtained original space convergence ground truth evaluate mean correlation factors maximum bipartite matching. fig. plot correlation factor time three compression ratios. experiments observe uncompressed dictionary factors approach converges good solution correlation faster running decomposition along original dimension. compressed factors modest speedup shows able accurately recover original factors even compression. provide additional details regarding experiment qualitative analysis resulting decomposition appendix. linear classiﬁcation. present empirical results recovering sparse linear classiﬁers trained compressed feature space. -class censusincome dataset augmented random binary features sampled bernoulli. including distractor features total dimensionality standard train-test split range projection dimensions trained logistic regressors compressed data recovered linear classiﬁers original space using sparse recovery. sparsity random projections ﬁxed addition performing sparse recovery solving equality-constrained -minimization problem also experimented less-stringent -ball constraints ˜w∗∞. also experiments used heuristic setting compare test performance recovered classiﬁers test performance obtained naive approach projecting test data using random projection classifying directly weights learnt compressed space. results experiment summarized fig. -ball constraint slightly outperforms stricter equality constraint across values achieve compression feature space loss points versus classiﬁer trained uncompressed space recovery techniques signiﬁcantly outperform naive approach highlighting importance denoising effect realized sparse recovery. conclusion proposed simple framework learning sparse models combining ideas dimension reduction compressive sensing theoretical empirical results illustrate viability approach tensor decomposition well preliminary results linear classiﬁers. natural direction future work extend theoretical experimental results models applications require learning sparse models. references ling huang jinzhu byung-gon chun petros maniatis mayur naik. predicting execution time computer programs using sparse polynomial regression. advances neural information processing systems pages emmanuel candes terence tao. near-optimal signal recovery random projections universal encoding strategies? ieee transactions information theory radu berinde anna gilbert piotr indyk howard karloff martin strauss. combining geometry combinatorics uniﬁed approach sparse signal recovery. communication control computing annual allerton conference pages ieee david donoho victoria stodden. non-negative matrix factorization give correct decomposition parts? advances neural information processing systems pages yihong gong. document clustering based non-negative matrix factorization. proceedings annual international sigir conference research development informaion retrieval pages stan hong jiang zhang qian sheng cheng. learning spatially localized parts-based representation. computer vision pattern recognition cvpr proceedings ieee computer society conference volume pages i–i. ieee animashree anandkumar rong daniel sham kakade matus telgarsky. tensor decompositions learning latent variable models. journal machine learning research radu berinde piotr indyk milan ruzic. practical near-optimal sparse recovery norm. communication control computing annual allerton conference pages ieee julien mairal francis bach jean ponce guillermo sapiro. online learning matrix factorization sparse coding. journal machine learning research naiyang guan dacheng zhigang yuan. online nonnegative matrix factorization robust stochastic approximation. ieee transactions neural networks learning systems wang chenhao ping arnd christian könig. efﬁcient document clustering online nonnegative matrix factorizations. proceedings siam international conference data mining pages siam yining wang hsiao-yu tung alexander smola anima anandkumar. fast guaranteed tensor decomposition sketching. advances neural information processing systems pages evangelos papalexakis christos faloutsos nicholas sidiropoulos. parcube sparse parallelizable tensor decompositions. machine learning knowledge discovery databases pages rosa arriaga santosh vempala. algorithmic theory learning robust concepts random projection. foundations computer science annual symposium pages ieee lijun zhang mehrdad mahdavi rong tianbao yang shenghuo zhu. random projections classiﬁcation recovery approach. ieee transactions information theory joseph kruskal. three-way arrays rank uniqueness trilinear decompositions application arithmetic complexity statistics. linear algebra applications anima anandkumar prateek jain yang naresh niranjan. tensor matrix methods robust tensor decomposition block sparse perturbations. artiﬁcial intelligence statistics pages massih amini nicolas usunier cyril goutte. learning multiple partially observed views-an application multilingual text categorization. advances neural information processing systems pages figure visualization factor correlates onset seizures patient. plots shows temporal mode bottom shows channel mode. labels refer level compression. dotted lines indicate actual onset times seizures. compression level preserves peaks compression peaks lost. channel dictionary well-preserved across levels compression. emmanuel candes justin romberg terence tao. stable signal recovery incomplete inaccurate measurements. communications pure applied mathematics preprocessing. channel individually whitened mean standard deviation estimated segments data known contain periods seizure. spectrogram computed hann window size window overlap order capture characteristic sequences across time windows transform spectrogram concatenating groups sequential windows following shoeb guttag concatenate groups size three. detection seizures. original goal work train patient-speciﬁc classiﬁers detect onset epileptic seizures. tensor decomposition time series yields factor correlates onset seizures shown fig. example illustrates tradeoff compression factor ﬁdelity recovered modes. figure synthetic experiments nmf. total number zero entries ﬁxed rank project dimensions. obtain signiﬁcant asymptotic speedups negligible change accuracy case. matrix rn×r entries independent gaussian random variables. number non-zero entries project theoretical limit fig. signiﬁcant speedups setting negligible degradation accuracy. course setting highly artiﬁcial serves sanity check compressed works. additional proofs section uniqueness proof. part argue space matrix argument part alternative factorization space must equal space equals space space equals space therefore must space hence goal prove rows sparsest vectors space implies factorization lemma shows number columns least zero entry subset rows grows proportionately size proof. consider bipartite graph nodes left part corresponding rows nodes right part corresponding columns indices factor. node edge nodes corresponding non-zero indices note neighborhood nodes part lemma graph expander failure probability re−βk/n ﬁxed constant expander every nodes least |s|k/ neighbors. size nodes must include subset size neighbours hence every size least neighbors. therefore every subset rows min{|s|k/ failure probability re−βk/n. show second part proof—that subset rows every linear combination rows must |−|s| non-zero entries. following simple lemma restatement result spielman provide proof appendix completeness. deﬁne vector fully dense entries non-zero. lemma }n×n binary matrix least nonzero column. entry drawn independently probability random matrix left nullspace contain fully dense vector. proof. null space ﬁrst columns deﬁne show probability either contain fully dense vector rank proof proceeds induction. consider consider full dense vector every column least non-zero entry probability orthogonal ﬁrst column exactly zero. non-zero entries independent gaussian random variables hence probability ﬁxed linear combination zero exactly zero. hence rank rank probability consider step. induction hypothesis either contain fully dense vector rank contain fully dense vector done implies also contain fully dense vector. contain fully dense vector column least non-zero entry entries independent previous columns. probability orthogonal zero previous argument. hence probability either contain fully dense vector rank rank proves induction argument. therefore probability either rank contain fully dense vector. hence left null space contain fully dense vector. complete proof ﬁrst part theorem consider rows consider linear combination rows combination weights r|s| non-zero. lemma min{|s|k/ failure probability re−βk/n. claim zero entries. prove contradiction. assume zero entries. consider submatrix corresponding rows columns. fewer zero entries must subset columns least non-zero entry fully dense vector lies left null space lemma probability happening zero. hence |−|s| zero entries. note hence linear combination non-zero entries failure probability re−βk/n. hence rows sparsest vectors space failure probability re−βk/n. proof. recall proof lemma considered bipartite graph corresponding rows indices. projection step indices projected dimensions projection matrix expander consider tripartite graph adding third nodes. edge node node subset rows nodes reachable nodes lemma number neighbors nodes size least min{|s|k/ failure probability re−βk/n. projection matrix expander every subset size least min{tp/ neighbors combining argument lemma follows every subset rows min{|s|kp/ failure probability re−βk/n. proposition vector independent random variables. subset r|s| refer subset corresponding indices consider subsets deﬁnes linear relation r|si|. assume variable appear probability distribution variables {vt+ conditioned linear relations deﬁned still continuous density function full support. particular linear combination variables {vt+ zero probability zero. proof. prove induction. base case note without linear constraints random variables {v··· continuous full support random variables independent gaussian. consider step linear constraints deﬁned sets s··· imposed variables. claim distribution random variables {vj+··· continuous full support imposition constraints s··· induction hypothesis distribution random variables {vj··· continuous full support imposition constraints s··· sj−. note linear constraint satisﬁed assignment subset variables {vj+··· appear constraint chosen appropriately induction hypothesis full support conditioned previous constraints s··· sj−. hence probability distribution variables {vj+··· still continuous full support adding constraint lemma |s|p prove stronger result symmetric incoherent tensors guarantee accurate recovery compressed space using tensor power method. tensor power method tensor analog matrix power method ﬁnding eigenvectors. equivalent ﬁnding rank factorization using alternating least squares algorithm. incoherent tensors tensors factors small inner products other. deﬁne incoherence maxi=j{|at aj|}. guarantees tensor decomposition follow analysis tensor power method sharan valiant proposition shows guarantees recovering true factors multiple random initializations used tensor power method recover back factors wiai cmax maxi=j ratio largest smallest weight. assume constant consider projection matrix {±}n×d every exactly non-zero entries chosen uniformly independently random non-zero entries uniformly independently distributed signs. take dimensional projection original tensor decomposition problem initialization chosen uniformly random unit sphere high probability tensor power method converges true factors steps estimate projected tensor decomposition problem initialization chosen uniformly random unit sphere high probability tensor power method converges true factors steps estimate satisﬁes proof. proof relies theorem sharan valiant sparse johnson lindenstrauss transforms kane nelson claim proposition theorem sharan valiant show claim need ensure incoherence parameter projected space small. johnson lindenstrauss property projection matrix ensure this. matrix regarded johnson lindenstrauss matrix preserves norm randomly chosen unit vector factor failure probabilty results kane nelson show high probability matrix {±}n×d every non-zero entries chosen uniformly independently random non-zero entries uniformly independently distributed signs preserves pairwise distances within factor easy verify inner-products preserved within additive error pairwise distances preserved within factors choosing union bound pairs factors factors incoherent projected space ensures high probability incoherent original space. setting claim follows theorem sharan valiant section provide proof claim randomly chosen projection matrix expander desired expansion. part lemma part used proof uniqueness lemma randomly choose bipartite graph vertices left part vertices right part every vertex degree then proof. consider subset denote probability upper bound probability expander upper-bounding probability subset expanding. denote probability neighborhood entirely contained subset α|s|d. union bound", "year": 2017}