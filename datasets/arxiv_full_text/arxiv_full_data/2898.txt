{"title": "Action-Conditional Video Prediction using Deep Networks in Atari Games", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.", "text": "motivated vision-based reinforcement learning problems particular atari games recent benchmark aracade learning environment consider spatio-temporal prediction problems future image-frames depend control variables actions well previous frames. composed natural scenes frames atari games high-dimensional size involve tens objects objects controlled actions directly many objects inﬂuenced indirectly involve entry departure objects involve deep partial observability. propose evaluate deep neural network architectures consist encoding actionconditional transformation decoding layers based convolutional neural networks recurrent neural networks. experimental results show proposed architectures able generate visually-realistic frames also useful control approximately -step action-conditional futures games. best knowledge paper ﬁrst make evaluate long-term predictions high-dimensional video conditioned control inputs. years deep learning approaches survey) shown great success many visual perception problems however modeling videos still challenging problem often involves high-dimensional natural-scene data complex temporal dynamics. thus recent studies mostly focused modeling simple video data bouncing balls small patches next frame highly-predictable given previous frames many applications however future frames depend previous frames also control action variables. example ﬁrst-person-view vehicle affected wheel-steering acceleration. camera observation robot similarly dependent movement changes camera angle. generally vision-based reinforcement learning problems learning predict future images conditioned actions amounts learning model dynamics agent-environment interaction essential component model-based approaches paper focus atari games arcade learning environment source challenging action-conditional video modeling problems. composed natural scenes frames atari games high-dimensional involve tens objects objects controlled actions directly many objects inﬂuenced indirectly involve entry departure objects involve deep partial observability. best knowledge paper ﬁrst make evaluate long-term predictions high-dimensional images conditioned control inputs. paper proposes evaluates contrasts spatio-temporal prediction architectures based deep networks incorporate action variables experimental results show architectures able generate realistic frames -step action-conditional future frames without diverging atari games. show representations learned architectures approximately capture natural similarity among actions discover objects directly controlled agent’s actions indirectly inﬂuenced controlled. evaluated usefulness architectures control ways replacing emulator frames predicted frames previously-learned model-free controller proposed learn temporal correlations sequential data introducing recurrent connections rbm. structured rtrbm scaled rtrbm learning dependency structures observations hidden variables data. recently michalski proposed higher-order gated autoencoder deﬁnes multiplicative interactions consecutive frames mapping units showed temporal prediction problem viewed learning inferring higher-order interactions consecutive images. srivastava applied sequence-to-sequence learning framework video domain showed long short-term memory networks capable generating video bouncing handwritten digits. contrast previous studies paper tackles problems control variables affect temporal dynamics addition scales spatio-temporal prediction larger-size images. combining deep learning atari games provide challenging environments high-dimensional visual observations partial observability delayed rewards. approaches combine deep learning made signiﬁcant advances speciﬁcally combined q-learning convolutional neural network achieved state-of-the-art performance many atari games. used ale-emulator making action-conditional predictions slow monte-carlo tree search method generate training data fast-acting outperformed several domains. throughout paper refer architecture used used deeper data produce currently best-performing atari game players). action-conditional predictive model idea building predictive model vision-based problems introduced schmidhuber huber proposed neural network predicts attention region given previous frame attention-guiding action. recently lenz proposed recurrent neural network multiplicative interactions predicts physical coordinate robot. compared previous work work evaluated much higher-dimensional data complex dependencies among observations. attempts learn data transition-model makes predictions future frames. line work divides game images patches applies bayesian framework predict patch-based observations. however approach assumes neighboring patches enough predict center patch true atari games many complex interactions. evaluation prior work -step prediction loss; contrast make evaluate long-term predictions quality pixels generated usefulness control. proposed architectures training method goal architectures learn function frame action variables time frames time time figure shows architectures composed encoding layers extract spatio-temporal features input frames action-conditional transformation layers transform encoded features prediction next frame high-level feature space introducing action variables additional input ﬁnally decoding layers predicted high-level features pixels contributions novel action-conditional deep convolutional architectures high-dimensional long-term prediction well novel architectures visionbased domains. variants feedforward encoding recurrent encoding feedforward encoding takes ﬁxed history previous frames input concatenated channels stacked convolution layers extract spatio-temporal features directly concatenated frames. encoded feature vector henc xt−m+t r×h×w denotes frames pixel images color channels. mapping pixels high-level feature vector using multiple convolution layers fully-connected layer followed non-linearity. encoding viewed early-fusion also applied architecture). recurrent encoding takes frame input time-step extracts spatio-temporal features using temporal dynamics modeled recurrent layer high-level feature vector extracted convolution layers paper lstm without peephole connection used recurrent layer follows memory cell retains information deep history inputs. intuitively given input lstm lstm captures temporal correlations high-level spatial features. multiplicative action-conditional transformation multiplicative interactions encoded feature vector control variables action-transformed feature henc action-vector time rn×n×a -way tensor weight bias. action represented using one-hot vector using -way tensor equivalent using different weight matrices action. enables architecture model different transformations different actions. advantages multiplicative interactions explored image text processing practice -way tensor scalable large number parameters. thus approximate tensor factorizing three matrices follows wdec rn×f wenc rf×n rf×a number factors. unlike -way tensor factorization shares weights different actions mapping size-f factors. sharing desirable relative -way tensor common temporal dynamics data across different actions convolutional decoding recently shown capable generating image effectively using upsampling followed convolution stride similarly inverse operation convolution called deconvolution maps spatial region input using deconvolution kernels. effect upsampling achieved without explicitly upsampling feature using stride found operation efﬁcient upsampling followed convolution smaller number convolutions larger stride. reshape fully-connected layer hidden units form feature deconv consists multiple deconvolution layers followed non-linearity except last deconvolution layer. curriculum learning multi-step prediction almost inevitable predictive model make noisy predictions high-dimensional images. model trained -step prediction objective small prediction errors compound model trained multiple phases based increasing suggested michalski words model trained predict short-term future frames ﬁne-tuned predict longer-term future frames previous phase converges. found curriculum learning approach necessary stabilize training. stochastic gradient descent backpropagation time used optimize parameters network. experiments experiments follow following goals architectures. evaluate predicted frames ways qualitatively evaluating generated video quantitatively evaluating pixel-based squared error evaluate usefulness predicted frames control ways replacing emulator’s frames predicted frames using predictions improve exploration analyze representations learned architectures. begin describing details data model architecture baselines. data preprocessing. used replication generate game-play video datasets using \u0001-greedy policy i.e. forced choose random action probability. game dataset consists training frames test frames actions chosen dqn. following actions chosen every frames reduces video fps. number actions available games varies represented one-hot vectors. used full-resolution images preprocessed images subtracting mean pixel values dividing pixel value network architecture. across game domains network architecture follows. encoding layers consist convolution layers fully-connected layer hidden units. convolution layers ﬁlters stride every layer followed rectiﬁed linear function recurrent encoding network lstm layer hidden units added fully-connected layer. number factors transformation layer decoding layers consists fully-connected layer hidden units followed deconvolution layers. deconvolution layers ﬁlters stride feedforward encoding network last frames given input time-step. recurrent encoding network takes frame time-step unrolled last frames initialize lstm hidden units making prediction. implementation based caffe toolbox details training. curriculum learning scheme three phases increasing prediction step objectives steps learning rates respectively. rmsprop used momentum gradient momentum squared gradient batch size training phase feedforward encoding network recurrent encoding network respectively. recurrent encoding network trained -step prediction objective network unrolled steps predicts last frames taking ground-truth images input. gradients clipped non-linearity gate lstm suggested baselines comparison. ﬁrst baseline multi-layer perceptron takes last frame input hidden layers units. action input concatenated second hidden layer. baseline uses approximately number parameters recurrent encoding model. second baseline no-action feedforward feedforward encoding model except transformation layer consists fully-connected layer action input. figure example predictions steps freeway. ‘step’ ‘action’ columns show number prediction steps actions taken respectively. white boxes indicate object controlled agent. prediction step controlled object crosses boundary reappears bottom; non-linear shift predicted architectures predicted naff. horizontal movements uncontrolled objects predicted architectures naff mlp. evaluation predicted frames qualitative evaluation prediction video. prediction videos models baselines available supplementary material following website https//sites.google. com/a/umich.edu/junhyuk-oh/action-conditional-video-prediction. seen videos proposed models make qualitatively reasonable predictions steps depending game. games baseline quickly diverges naff baseline fails predict controlled object. example long-term predictions illustrated figure observed models predict complex local translations well movement vehicles controlled object. predict interactions objects collision objects. since architectures effectively extract hierarchical features using able make prediction requires global context. example figure model predicts sudden change location controlled object -step. however models difﬁculty accurately predicting small objects bullets space invaders. reason squared error signal small model fails predict small objects training. another difﬁculty handling stochasticity. seaquest e.g. objects appear left side right side randomly hard predict. although models generate objects reasonable shapes movements generated frames necessarily match ground-truth. quantitative evaluation squared prediction error. mean squared error -step predictions reported figure predictive models outperform baselines domains. however predictive models naff baseline large except seaquest. fact object controlled action occupies small part image. figure comparison encoding models controlled object moving along horizontal corridor. recurrent encoding model makes small translation error frame true position object crossroad predicted position still corridor. object moves upward possible predicted position predicted object keeps moving right. less likely happen feedforward encoding position prediction accurate. objects move staying location ﬁrst steps. feedforward encoding model fails predict movement gets last four frames input recurrent model predicts downwards movement correctly. figure game play performance using predictive model emulator. ‘emulator’ ‘rand’ correspond performance true frames random play respectively. x-axis number steps prediction re-initialization. y-axis average game score measured plays. qualitative analysis relative strengths weaknesses feedforward recurrent encoding. hypothesize feedforward encoding model precise spatial transformations convolutional ﬁlters learn temporal correlations directly pixels concatenated frames. contrast convolutional ﬁlters recurrent encoding learn spatial features one-frame input temporal context captured recurrent layer high-level features without localized information. hand recurrent encoding potentially better modeling arbitrarily long-term dependencies whereas feedforward encoding suitable long-term dependencies requires memory parameters frames concatenated input. evidence figure show case feedforward encoding better predicting precise movement controlled object recurrent encoding makes pixel translation error. small error leads entirely different predicted frames steps. since feedforward recurrent architectures identical except encoding part conjecture result failure precise spatio-temporal encoding recurrent encoding. hand recurrent encoding better predicting enemies move space invaders fact enemies move steps hard feedforward encoding predict takes last four frames input. observed similar results showing feedforward encoding cannot handle long-term dependencies games. evaluating usefulness predictions control replacing real frames predicted frames input dqn. evaluate useful predictions playing games implement evaluation method uses predictive model replace game emulator. speciﬁcally controller takes last four frames ﬁrst pre-trained using real frames used play games based .greedy policy input frames generated predictive model instead game emulator. evaluate depth predictions inﬂuence quality control re-initialize predictions using true last frames every n-steps prediction note controller never takes true frame outputs predictive models. steps yields score close using real frames. architectures produce much better scores baselines deep predictions would suggested based much smaller differences squared error. likely cause models better able predict movement controlled object relative baselines even though ability always lead better squared error. three games score remains much better score random play even using steps prediction. improving informed exploration. learn control domain exploration actions states necessary without agent stuck sub-optimal policy. cnn-based agent trained using \u0001-greedy policy agent chooses either greedy action random action ﬂipping coin probability random exploration basic strategy produces sufﬁcient exploration slower informed exploration strategies. thus propose informed exploration strategy follows \u0001-greedy policy chooses exploratory actions lead frame visited least often rather random actions. implementing strategy requires predictive model next frame possible action considered. method works follows. recent frames stored trajectory memory denoted predictive model used next frame every action estimate visit-frequency every predicted frame summing similarity predicted frame recent frames stored trajectory memory using gaussian kernel follows threshold kernel bandwidth. trajectory memory size qbert games freeway others games. computational efﬁciency trained feedforward encoding network gray-scaled images used input dqn. details network architecture provided supplementary material. table summarizes results. informed exploration improves dqn’s performance using predictive model three games signiﬁcant improvement qbert. figure shows informed exploration strategy improves initial experience dqn. analysis learned representations similarity among action representations. factored multiplicative interactions every action linearly transformed factors figure present cosine similarity every pair action-factors training seaquest. corresponds ‘no-operation’ ‘ﬁre’. arrows correspond movements without ‘ﬁre’. positive correlations actions movement directions negative correlations actions opposing directions. results reasonable discovered automatically learning good predictions. distinguishing controlled uncontrolled objects hard interesting problem. bellemare proposed framework learn contingent regions image affected agent action suggesting contingency awareness useful model-free agents. show architectures implicitly learn contingent regions learn predict entire image. i)a) higher architectures factor likely transform image differently depending actions assume factors responsible transforming parts image related actions. therefore collected high variance factors model trained seaquest collected remaining factors variance subset. given image action controlled forward propagations giving highvar factors vice versa. results visualized ‘action’ ‘non-action’ figure interestingly given highvar-factors model predicts sharply movement object controlled actions parts mean pixel values. contrast given lowvar-factors model predicts movement objects background controlled object stays previous location. result implies model learns distinguish controlled objects uncontrolled objects transform using disentangled representations related work disentangling factors variation). conclusion paper introduced different novel deep architectures predict future frames dependent actions showed qualitatively quantitatively able predict visuallyrealistic useful-for-control frames -step futures several atari game domains. knowledge ﬁrst paper show good deep predictions atari games. since architectures domain independent expect generalize many vision-based problems. future work learn models predict future reward addition predicting future frames evaluate performance architectures model-based acknowledgments. work supported grant iis- bosch research grant n---. opinions ﬁndings conclusions recommendations expressed authors necessarily reﬂect views sponsors. references bellemare naddaf veness bowling. arcade learning environment evaluafigure distinguishing controlled uncontrolled objects. action image shows prediction given learned actionfactors high variance; non-action image given low-variance factors. bengio louradour collobert weston. curriculum learning. icml ciresan meier schmidhuber. multi-column deep neural networks image classiﬁcation. graves. generating sequences recurrent neural networks. arxiv preprint arxiv. singh lewis wang. deep learning real-time atari game play using nair hinton. rectiﬁed linear units improve restricted boltzmann machines. icml reed sohn zhang lee. learning disentangle factors variation manifold schmidhuber. deep learning neural networks overview. neural networks schmidhuber huber. learning generate artiﬁcial fovea trajectories target detection. internetwork architectures training details network architectures proposed models baselines illustrated figure weight lstm initialized uniform distribution weight fullyconnected layer encoded feature factored layer action factored layer initialized uniform distribution respectively. total number iterations training phase learning rate multiplied every iterations. figure network architectures. indicates element-wise multiplication. text convolution layer describes number ﬁlters size kernel padding stride. choose action observe reward image xt−t+ preprocess images store image store transition sample mini-batch transitions φj+} update based mini-batch bellman equation steps steps predictive model informed exploration. feedforward encoding network trained down-sampled gray-scaled images used computational efﬁciency. trained model -step prediction objective learning rate batch size pixel values subtracted mean pixel values divided rmsprop used momentum gradient momentum squared gradient comparison random exploration. figure visualizes difference random exploration informed exploration games. freeway agent gets rewards reaching lane agent moves around bottom area random exploration results steps ﬁrst reward. hand agent moves around locations informed exploration receives ﬁrst reward steps. similar result found pacman. application deep q-learning. results informed exploration using game emulator predictive model reported figure table replication follows uses smaller figure learning curves dqns standard errors. blue curves informed exploration using predictive model emulator respectively. black curves dqns random exploration. average game score measured game plays \u0001-greedy policy table average game score standard error. ‘i.e’ indicates combined informed exploration method. ‘emulator’ ‘prediction’ correspond emulator predictive model computing seaquest models predict movement enemies yellow submarine controlled actions. ‘naff’ predicts movement objects correctly submarine disappears steps. ‘mlp’ predict objects generate mean pixel image. seaquest proposed models predict location controlled object accurately -step predictions. generate objects ﬁshes human divers. although generated objects match ground-truth images shapes colors realistic. space invaders enemies center move change shapes step step movement predicted proposed models ‘naff’ predictions ‘mlp’ almost last input frame. space invaders although models make errors long generated images still realistic objects reasonably arranged moving right directions. hand frames predicted ‘mlp’ ‘naff’ almost last input frame. freeway proposed models predict movement controlled object correctly depending different actions ‘naff’ fails handle different actions. ‘mlp’ generates blurry objects realistic. freeway feedforward network diverges -step agent starts stage bottom lane. fact actions ignored -steps stage begins successfully handled feedforward network. qbert controlled object jumps third fourth row. meantime actions chosen agent effects. models ‘naff’ predicts movement whereas ‘mlp’ predict objects. qbert recurrent model predicts controlled object color cubes correctly feedforward model diverges -step predicts blurry controlled object -step. baselines diverged -step. pacman proposed models predict different movements pacman depending different actions whereas ‘naff’ ignores actions. ‘mlp’ predicts mean pixel image.", "year": 2015}