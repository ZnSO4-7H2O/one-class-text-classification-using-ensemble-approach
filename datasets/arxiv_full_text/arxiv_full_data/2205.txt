{"title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement  Learning", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "This paper introduces Dex, a reinforcement learning environment toolkit specialized for training and evaluation of continual learning methods as well as general reinforcement learning problems. We also present the novel continual learning method of incremental learning, where a challenging environment is solved using optimal weight initialization learned from first solving a similar easier environment. We show that incremental learning can produce vastly superior results than standard methods by providing a strong baseline method across ten Dex environments. We finally develop a saliency method for qualitative analysis of reinforcement learning, which shows the impact incremental learning has on network attention.", "text": "paper introduces reinforcement learning environment toolkit specialized training evaluation continual learning methods well general reinforcement learning problems. also present novel continual learning method incremental learning challenging environment solved using optimal weight initialization learned ﬁrst solving similar easier environment. show incremental learning produce vastly superior results standard methods providing strong baseline method across environments. ﬁnally develop saliency method qualitative analysis reinforcement learning shows impact incremental learning network attention. complex environments starcraft many modern video-games present profound challenges deep reinforcement learning solved. often require long precise sequences actions domain knowledge order obtain reward learned random weight initialization. solutions problems would mark signiﬁcant breakthrough path artiﬁcial general intelligence. recent works reinforcement learning shown environments atari games learned pixel input superhuman expertise agents start randomly initialized weights learn largely trial error relying reward signal indicate performance. despite successes complex games including rewards sparse montezuma’s revenge notoriously difﬁcult learn. methods intrinsic motivation used partially overcome challenges suspect becomes intractable complexity increases. additionally environments become complex become expensive simulate. poses signiﬁcant problem since many atari games already require upwards million steps using state-of-the-art algorithms representing days training single machine. thus appears likely complex environments become costly learn randomly initialized weights increased simulation cost well inherent difﬁculty task. therefore form prior information must given agent. seen alphago agent never learned play game without ﬁrst using supervised learning human games. supervised learning certainly shown reinforcement learning costly obtain sufﬁcient samples requires environment task humans play reasonable skill therefore impractical wide variety important reinforcement learning problems. paper introduce ﬁrst continual reinforcement learning toolkit training evaluating continual learning methods. present demonstrate novel continual learning method call incremental learning solve complex environments. incremental learning environments framed task learned agent. task split series subtasks solved simultaneously. similar natural language processing object detection subtasks neural image caption generation reinforcement learning environments also subtasks relevant given environment. subtasks often include player detection player control obstacle detection enemy detection player-object interaction name few. subtasks common many environments often sufﬁciently different function representation reinforcement learning algorithms fail generalize across environments atari. critical subtasks expert humans utilize quickly learn environments share subtasks previously learned environments reason humans superior data efﬁciency learning complex tasks. case deliberately similar environments construct subtasks similar function representation agent trained ﬁrst environment accelerate learning second environment preconstructed subtask representations thus partially avoiding complex environment’s increased simulation cost inherent learning difﬁculty. transfer learning method utilizing data domain enhance learning another domain. sharing signiﬁcant similarities continual learning transfer learning applicable across machine learning domains rather conﬁned reinforcement learning. example signiﬁcant using networks trained imagenet accelerate enhance learning classiﬁcation accuracy ﬁnetuning variety vision tasks concept continual learning agent learns variety experiences enhance future learning deﬁned time remained largely untapped recent powerful algorithms best beneﬁt effects. recent work done progressive neural networks transfer learning used apply positive transfer variety reinforcement learning domains. however method differs additional parameters environment lateral connections features result increased memory space training time. recent work related subtask utilization comes kirkpatrick shows expertise maintained multiple environments experienced long time elastic weight consolidation viable weights found simultaneously achieve expertise variety atari games. incremental learning similarly trains multiple environments goal achieving enhanced expertise single environment rather expertise environments. leave future work overcome limitation. novel deep reinforcement learning toolkit training evaluating agents continual learning methods. acts wrapper game open hexagon sending screen pixel information reward information performing actions openai like contains hundreds levels acting environment. environments collected groups similar environments task continual learning. environments vary greatly difﬁculty ranging simple levels agents achieve superhuman performance less minutes levels consider complex previously learned environments. refer videos available github.com/innixma/dex environments shown figure screenshots capture environment complexity. open hexagon game involving navigating triangle around center ﬁgure avoid incoming randomly generated walls. screenshots various environments game shown figure game progresses regardless player action thus player must react environment real-time. player contacts wall game over. point game player three choices actions move left right stay put. game survival score thus total reward survival time seconds. open hexagon contains hundreds levels figure environments. small triangle player must rotated around center avoid incoming walls. many environments incorporate various distortion effects evident screenshots. reversal periodically ﬂips game controls environments even additional actions arithmetic requires agent correctly solve various math equations level numpad. novel continual learning method incremental learning deﬁned follows. formal case agent must learn series environments identical legal game actions note series environments made action space considering superset possible actions game actions performing identically action assuming action within action space. environment corresponding cost step step count indicating number steps taken environment. typically complex environments higher cost step. resource maximum given indicates total amount data gathered shown following inequality assumed correlation environments beyond data action dimensions. appear optimal solution examine gather data done virtually reinforcement learning algorithms past always case. example highly correlated training superior lesser cost. additionally environment contain important aspects avoiding state spaces rewards useful training potentially allowing training optimal even taking environments represent real environments respective costs solution problem corresponds globally optimal sequence training steps achieve maximum performance ﬁnite amount computational resources given algorithm. therefore necessarily contains solution achieving artiﬁcial general intelligence minimal resources. unfortunately several drawbacks. importantly selection optimal environments obvious order even less formal deﬁnition useful deﬁne incremental learning following simpliﬁed version paper focuses simpliﬁed case variables same except steps must taken environments sequentially without going back previous environments. thus step taken steps taken future environments furthermore assumed environments correlated environment contains subset subtasks environment environment typically harder environment ei−. intuition behind simpliﬁed case simple environments cheap simulate easy learn features strategies learned simple environment could transfer difﬁcult correlated environment. process done repeatedly producing compounding acceleration learning additional useful incremental environments added. thus general process incremental environment selection easier subsets goal environment. furthermore means every environment simpliﬁed incremental learning problem seen goal environment easier problem containing analyse effectiveness incremental learning agents learned environments dex. training agents asynchronous advantage actor-critic framework introduced coupled network architecture described below. convnet implementation details. experiments learned network following architecture. network takes input series consecutive grayscale images game state. processed series convolutional layers stride size ﬁlter counts respectively. pair convolutional layers dimensional maxpooling layer size intermediate output ﬂattened processed dense layer nodes. output layers identical speciﬁed convolutional layers maxpooling layers zero-padded. results network approximately parameters. intermediate layers followed rectiﬁed linear units adam used optimizer learning rate learning rate changed course training opposed mnih instead stays constant. gamma used along n-step reward used speed value propagation network cost minor instability learning. training done batches samples. architecture created using tensorflow keras since runs real time slightly altered method learning utilized avoid inconsistent time steps. implemented modiﬁed work ofﬂine manner experience replay done deep-q networks naive approximation complex acer algorithm destabilize algorithm bases computations data representative network current state agents still able learn environments signiﬁcantly outperform double deep-q networks thus serving reasonable baseline. performed experiments different environments incremental learning. environments split incremental learning sets three seven environments. ﬁrst referred deals increasingly complex patterns. second referred deals increasingly complex task representation. results show incremental learning signiﬁcant positive impact learning speed task performance also lead initializations trained overly simplistic environments. code reproduce experiments paper released future date. setup following experiments naively assume achieving near optimal performance minimal resources requires prerequisite learning minimal resources. proceeds downward base case considered trivially solvable environment random weight initialization. assumption used simplify training. furthermore exploratory nature baseline experiments costs step ignored seek show positive feature transfer occurring rather optimizing feature transfer itself leave future work. environments learned identical architecture algorithm hyperparameters. gather data environments times second. \u0001-greedy policy replay memory size replay memory initialized random agent. experiments agents trained batches episode. total reward equal number seconds survived environment episode. mean reward calculated hour testing weights without training. episode scales difﬁculty indeﬁnitely longer agent survives. evaluate four different types models experiment. ﬁrst random agent comparison. second baseline standard reinforcement learning training method. establish baseline environment trained random initialization hour equivalent roughly training steps. weights achieve maximum total reward single episode selected output weights training called third model shall call initial initial weights incremental learning method continued training. thus environment model uses weights wi−. used measure correlation environments. would expect uncorrelated environments result near random agent reward initial. establish incremental learning agents environment take weights outputted baseline using initial weights training additional hour. weights outputted method call results found table table seen table incremental learning provided superior roughly equivalent results every environment environments experiencing substantial increases maximum reward incrementally learned achieving nearly triple baseline maximum mean reward. however harder environments signiﬁcant improvement seen. likely earlier environments sufﬁciently learned along fact tasks generally difﬁcult learn hour training indicated near random performance baseline incremental methods. results shown table show incremental learning harmful case likely difference wall patterns single wall screen time requiring simple avoidance. four separate walls occupy screen once requiring complex method involving future planning understanding walls important given state. suspect learning leads agent table shows mean reward agent given environment given training method described experimental setup. observe incremental learning provided superior results baseline nearly environments particularly table shows mean reward agent given environment given training method described experimental setup. observe incremental learning provided inferior results baseline environment likely overﬁtting. overtrain variety weights hindering future learning. indicates certain environments simple include incremental learning learned reward less four minutes. additionally initial model shows environments correlated generally superior performance random despite never training environment explicitly. case nearly equal baseline maximum metric indicating signiﬁcant correlation. likely reason greatly enhanced performance incremental learning baseline note experimental results strictly simple baseline incremental learning suffer instability short timeline training lack repeated experiments. means agents necessarily consistently improve throughout training rather quickly improve maximum performance followed decreased performance remainder training. leave comprehensive experiments future work. qualitatively analyze effects incremental learning networks weights develop saliency visualization method based simonyan reinforcement learning. heatmaps generated given networks weights input image. method gathering heatmaps identical simonyan thus equations derivations shall repeated paper. likely action network take given frame used action minimize gradient. difference supervised learning case ground truth used. reinforcement learning ground truth known thus must inferred done wang results visualization trained weights environment sets seen figure figure expected saliency well performing trained agent focus player location sensitive changes small region surrounding player. intuition conﬁrmed ﬁrst figure agents trained easier environments focus attention player nearby threats. interestingly trained environment becomes complex agent appears less developed increasingly focuses irrelevant locations state prominent agent trained explicitly avoids attention player nearby threats walls. indicates agent learn environment sufﬁciently time given complexity training environment. conﬁrmed experimental results near random. additionally saliency mappings show correlation environments indicated similarity saliency figure figure despite never explicitly trained environment state visualization from. interestingly saliency mappings incrementally learned agents closely resemble mappings weights initialized weights environment learned without incremental learning. suggests signiﬁcant amount initialized weights retained incrementally learning. visualizations included video realtime saliency agent episode github.com/innixma/dex. hope expand analysis done paper investigating incremental learning chains involving environments. effectively explore effects incremental learning complex problems. also wish extend training time experiments allow complex environments shown figure learned well compare method progressive neural networks incremental learning could expanded function feature extractor reinforcement learning. would allow contained action space incremental learning tasks varying domains similar work done rusu additionally training could done separately multiple environments combined learn environment shares subproblems previous environments. would natural merging incremental learning elastic weight consolidation also provide synergistic effect algorithms unreal rely auxiliary tasks. tasks could maximize network utilization across multiple environments potentially leading network better generalizations incremental learning. finally environments montezuma’s revenge labyrinth require exploration sparse rewards natural expansion application incremental learning. developing incremental exploration environments could result superior performance exploration tasks. ability learn transfer knowledge across domains essential advancement agents solve complex tasks. paper introduced continual learning toolkit training evaluation continual learning methods. proposed method incremental learning deep reinforcement learning demonstrated ability accelerate learning produce drastically superior results standard training methods multiple environments supporting notion avoiding randomly initialized weights instead using continual learning techniques solve complex tasks. source code training methods used paper well toolkit source code found github.com/innixma/dex. references abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard józefowicz kaiser kudlur levenberg mané monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan viégas vinyals warden wattenberg wicke zheng. tensorﬂow large-scale machine learning heterogeneous distributed systems. corr abs/. kirkpatrick pascanu rabinowitz veness desjardins rusu milan quan ramalho grabska-barwinska hassabis clopath kumaran hadsell. overcoming catastrophic forgetting neural networks. proceedings national academy sciences ./pnas.. nair hinton. rectiﬁed linear units improve restricted boltzmann machines. fürnkranz joachims editors proceedings international conference machine learning pages omnipress oquab bottou laptev sivic. learning transferring mid-level image representations using convolutional neural networks. ieee conference computer vision pattern recognition june silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot dieleman grewe nham kalchbrenner sutskever lillicrap leach kavukcuoglu graepel hassabis. mastering game deep neural networks tree search. nature issn ./nature.", "year": 2017}