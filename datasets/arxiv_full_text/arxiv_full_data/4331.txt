{"title": "Capturing spatial interdependence in image features: the counting grid,  an epitomic representation for bags of features", "tag": ["cs.CV", "stat.ML"], "abstract": "In recent scene recognition research images or large image regions are often represented as disorganized \"bags\" of features which can then be analyzed using models originally developed to capture co-variation of word counts in text. However, image feature counts are likely to be constrained in different ways than word counts in text. For example, as a camera pans upwards from a building entrance over its first few floors and then further up into the sky Fig. 1, some feature counts in the image drop while others rise -- only to drop again giving way to features found more often at higher elevations. The space of all possible feature count combinations is constrained both by the properties of the larger scene and the size and the location of the window into it. To capture such variation, in this paper we propose the use of the counting grid model. This generative model is based on a grid of feature counts, considerably larger than any of the modeled images, and considerably smaller than the real estate needed to tile the images next to each other tightly. Each modeled image is assumed to have a representative window in the grid in which the feature counts mimic the feature distribution in the image. We provide a learning procedure that jointly maps all images in the training set to the counting grid and estimates the appropriate local counts in it. Experimentally, we demonstrate that the resulting representation captures the space of feature count combinations more accurately than the traditional models, not only when the input images come from a panning camera, but even when modeling images of different scenes from the same category.", "text": "recent scene recognition research images large image regions often represented disorganized bags features analyzed using models originally developed capture co-variation word counts text. however image feature counts likely constrained different ways word counts text. example camera pans upwards building entrance ﬁrst ﬂoors fig. feature counts image drop others rise drop giving features found often higher elevations. space possible feature count combinations constrained properties larger scene size location window capture variation paper propose counting grid model. generative model based grid feature counts considerably larger modeled images considerably smaller real estate needed tile images next tightly. modeled image assumed representative window grid feature counts mimic feature distribution image. provide learning procedure jointly maps images training counting grid estimates appropriate local counts experimentally demonstrate resulting representation captures space feature count combinations accurately traditional models input images come panning camera even modeling images different scenes category. illustration fig. provides synthetic example starting several images train station taken windows larger scene ii). illustrative purposes hand-labeled scene feature labels shown iii). realistic application want train model assigns high likelihood images train stations likely available images would taken narrower ﬁeld view simulated here. feature extractors would presumably generalize much less effectively ideal features still enough permit comparisons images different train stations too. question learning model captures feature count co-variation uses training data efﬁciently. assuming images taken random scene wonder feature counts images sufﬁcient predict possible feature counts images scene. particular consider images taken regions close question image would deﬁned train station class. literature uses sets approaches problem. kernel nearest-neighbor techniques start comparisons feature counts test image previously studied exemplars although comparison done many different ways note approaches would complicated fact none images combination features present approach consider bags features together generalize simplest approach would simply merge bags. case danger overgeneralization. particular example need interpolating feature count vectors abc. however interpolation best performed spatial reasoning. across various windows scene window bottom sometimes roof train tracks order times mountain grass roof train. infer grass roof train tracks combination likelier existence mountain roof train tracks combination features. furthermore proportions different features images carry information thickness layers features useful inferring previously unseen feature count combinations found elsewhere scene. show paper that surprisingly much spatial organization features training images needs retained order perform spatial reasoning feature combinations likely. fig. -iv) show counting grid inferred iterating eqs. label counts windows scene taken random avoiding windows contain features proportion. training image represented feature bags without using fig. feature counts change slightly ﬁeld view moves. example abundance features reduced counts features found building facades increased. counting grid model accounts changes naturally also account images different scenes. introduction popular deal diversity imaging conditions well geometric variation objects entire scenes simply represent images image regions disordered bags image features models particularly attractive computational efﬁciency simplicity achieved ignoring spatial relationships image patches object parts. features arise variety ways. example extracting local low-level features images often clustered discrete codeword assigned feature descriptor. image described histogram codebook entries. ideally features highly discriminative categories images interest uniquely identiﬁable presence handful features. practice however individual features sufﬁciently discriminative modeling joint variation feature counts becomes interesting machine learning problem. tempting existing discrete models histograms multinomial mixtures topic models already extensively validated text data document also simply represented count distribution entire vocabulary. however bags features extracted natural images imprint images’ spatial structure evident bags related images considered together. thus ignoring natural constraints feature counts negative consefig. counting grid illustration. images feature representation. images train station taken windows larger scene. iii) hand labeled features. scene reconstructed starting bags taken windows fig. illustration counting grids using different data different levels abstraction. show examples training images extracted bags. bottom panel illustrates features. counting grid learned using patches extracted train station example fig. case bags come windows image reconstructed grid. counting grid estimated images taken wearable camera case learned dictionary features clustering image patches illustrate ofﬁce scene overlap textons much patches overlapping feature extraction process average create clearer visual representation. iii) counting grid estimated starting labelme annotations original window location information counting grid computed training image window counting grid found appropriate sections matching histograms. resolution reconstructed feature layout large scene goes well beyond would expected crude tessellation input images although none training examples taken area close features seen single image part scene reconstructed well histogram matched well. simple example training images different views single scene. however feature level images train stations likely similar layout could used learn counting grid. practice rarely access highly discriminative reliable features instead fake features example experiments hundreds simpler automatically derived features infer counting grids related images different scenes. example fig. -iii) used features human-supplied labels labelme dataset outputs hundreds simple computational feature detectors applied images various scenes sensecam dataset opposed train station example input images subimages larger single scene rather images types scenes. window counting grid represents possible feature combination present dataset model able reconstruct feature layout exploiting spatial patterns coarsely depending feature count covariation reasoning power. example despite relying tessellation input image full resolution panoramas ofﬁce corridors visible fig. -ii). paper presents extends counting grid model basic model extended include priors help overﬁtting issues. also formally introduce tessellated counting grid model analyzed extreme tessellations captured single feature collapsing representation discrete epitome paper also provides full comparisons different algorithms various datasets including effect grid window size variation found experimental section. specialization extensions counting grid model already appeared tier conferences nevertheless paper want limit attention basic variants counting grid model properties relationships standard techniques modeling bags words computer vision found representation captured space possible feature count combinations various image categories signiﬁcantly better generalization techniques simple generative model used unsupervised learning clustering often rivals state based discriminative techniques require supervision. words models spatial relationships among features completely ignored order facilitate computational efﬁciency high level generalization. topic models example assign topic codeword based co-occurrence describe images admixtures topics. another word model described scene model mixture gaussians model trained gist descriptors paper basic counting grid model also reduces mixture highly tied parameters reﬂecting inherent spatial structure data. represented point large grid feature counts. latent point corner window grid points uniformly combined match feature counts image. capture spatial information possible separate bags originating different regions image. models sometimes referred spatial-bow models tessellated counting grids introduce also ﬂavor although approach tessellation helps guide quilting bags words reconstructing layout sub-region accuracy. thus tessellated counting grids capture layout-driven constraints counts within regions even though information directly provided learning layout within region image inferred based feature distributions found regions many images assuming misalignments images often smaller size tessellated regions. contrast typical spatial-bow models requires modeled images approximately aligned. recent approaches relax assumption former reconﬁgurable model represents scene collection parts arranged reconﬁgurable pattern. image divided predeﬁned regions latent variable speciﬁes region model assigned image region. hand represents scenes using deformable parts. lower-resolution root ﬁlter placed center image higher-resolution part ﬁlters arranged ﬂexible spatial conﬁguration. resentation input allowing model capture problems rigidity various levels uncertainty modeling. example epitome-like models quilts images image patches essentially building giant panoramas consisting probability distributions location. based pixel-to-pixel comparisons cannot generalize well case large geometric deformations mostly used model relatively small image patches typically synthesis modeling large scenes textures tolerate lack transformation invariance beyond translation epitomes employed scene analysis particular datasets panoramic stitching would work e.g. sequences taken wearable cameras various scene modeling techniques also take different approaches representing componential structure natural scenes. ad-mixtures rather simple mixtures topic models simple example multi -part -object models. examples componential models ﬂexible sprites model allow image mapped multiple sources sector mapped independently. hand counting grids epitomes histogram-based approaches essentially mixtures entire scene single point position mixture component. main topic paper modeling bags features computer vision. experimental section mainly consider generative approaches compare counting grids latent dirichlet allocation mixture models epitomes reconﬁgurable words model nature generative approach discussed summarized tab. noted however models presented used components hierarchical models basic idea modeling intersections laying inferred grid used within machine learning techniques including non-generative approaches. discussed above would like understand hidden constraints govern often-practiced simpliﬁcation images bags features. simpliﬁcation stages. first image features extracted grid inside image. features discrete point codebook features obtained clustering multidimensional real-valued features calculated local image processing e.g. sift next feature counts computed indicator function. counts retained spatial distribution typically forgotten justiﬁcation establishing correspondence individual image locations across different images thing would prohibitively expensive practice presence absence features informative spatial distribution. however consider bags words related images feature counts disordered bags features still indirectly follow rules spatial organization. example bags indexed extracted several overlapping windows larger image spatial structure image imprinted counting grid model basic counting grid normalized counts features indexed grid everywhere grid given image features represented counts {cz} assumed follow distribution found somewhere counting grid. words generated ﬁrstly averaging counts window size placed location generating features bag. carried locations window example counting grid geometry illustrated fig. -ii) words position window grid latent variable given probability features {cz}z bayesian network model illustrated fig. -i). given grid deﬁnes following joint distribution indexed correbags features sponding latent window positions counting grid summarize notation throughout paper hidden variable represents mapping location grid; mapped different location superscript refer particular t-th therefore represent mapping particular count combinations bags. furthermore spatial layout features large image even recoverable disordered bags bags created overlapping windows large image source location known easily minimal additional assumptions regarding boundaries image reconstruct feature indices location large image solving system linear equations arise count constraints. consider horizontally neighboring windows count differences completely determined feature identities columns share. separate effect columns consider another pair overlapping images whose count differences depend columns. break column apart consider vertically neighboring windows etc. long image thick enough border single feature present propagate constraints given location’s feature uniquely determined. reconstruct large grid features count combinations given bags found appropriate window reconstruction. implies bags features images scene considered jointly obey strong constraints thus taking constraints account likely improve image analysis tasks depend feature count representations. insight leads several interesting problems address next section. layout matching bags windows bags features many overlapping windows large scene provided windows withheld original still reconstruct least original spatial arrangement features? category modeling bags features coming windows single scene instead different related images would bags considered jointly layout features would layout help predict combinations feature counts likely bags features extracted images category question? fully parameterized optimizing bound equivalent optimizing likelihood data long distributions also optimized. keeping model parameters ﬁxed optimizing distribution leads optimize bound respect model parameters note ﬁrst term involves parameters requires another summation applying logarithm. summation grid positions within window bound using variational distribution jensen’s inequality distribution locations i.e. positive indexed normalization i∈wk done differently window indexed different different features indexed term inside summation different distribution could needed different bags distribution could thought information proportion features type contributed different sources window however performing constrained optimization assuming ﬁxed parameters distribution maximizes bound consider distributions feature mapping counting grid result intuitive. know containing features type mapped grid section additional information proportions features contributed different incremental counts best guess proportions follow proportions among inside window. since introducing position probabilistic model interesting estimate prior probability mapping t-th sample location case t-th sample’s hidden variable generic constant index represents possible location grid samples share. distribution table values shared across samples hand posterior distribution k|ct) variational counterpart function counts seen t-th sample capture quality different windows grid t-th sample particular. compute likelihood data need latent variable computing logarithm which mixture models epitomes makes difﬁcult perform assignment latent variables ex×ey mask ones upper left corner’s entries zeros elsewhere. experiments update proved valid overcome local minima prior location learned. update prior must course normalized across locations. starting non-informative initialization iterative process jointly estimate counting grid align bags avoid severe local minima important however consider counting grid torus consider windowing operations accordingly previously proposed learning epitomes prevents problems grid boundaries otherwise crossed space need grow layout features. reader note above simply optimized likelihood data single weights variational bound model ﬁxed expanded previous section. thus iteration equations would optimize parameters given observed data ignoring prior full network fig. -i). course dirichlet prior parameters appropriate conjugate prior making inclusion inﬂuence trivial parameters feature pseudocounts feature prior elegantly precludes zero counts feature anyπ preventing overtraining numerical problems. innermost equation carried across locations whose window contains generic location indexed lhs. simply reduces summing shifted windows represents lower right corner. finally taking derivatives respect prior probabilities different locations readily show update prior locations updated follows surprising mathematically model mixture distributions essentially update mixture prior. however consider data efﬁciency update differs dramatically efﬁciently data used learn distributions consider large model e.g. grid uses relatively large windows e.g. even though individual distributions learn fact used aggregates time distributions makes parameters mixture’s sources highly tied. fact tile non-overlapping windows grid rest overlapping windows special kind interpolation thus equivalent capacity model compared simple mixture allowing grids trained without overtraining order data capacity number. words able train model fractional sources around bags words. equivalently efﬁcient data estimating parts grid used others would require similar aggregation fractional probabilities individual cells like distributions aggregated distributions.the similar issue resolved epitome models literally aggregating updates important note bags contribute mapping grid. therefore tessellated counting grid model inherits componential nature counting grid making spatial information reported tab. step using section bags reduces three plates fig. show even considering representation consisting four bags features image sections provides enough symmetry breaking good counting grids estimated. discrete epitomes limit tessellation window size equal images size e.g. obtain discrete feature epitome model. case composed single feature sector index indexes pixel m-step re-arrangement features window simply copied according mappings qt). e-step thus becomes likewise epitome counting grid discrete eptiomes single-component models. however differently former characterized multinomial observation model differently latter consider original feature layout making model less efﬁcient harder generalize. finally local histograms used pixel descriptor. helped overcome rigidity epitome models reach great performances location recognition. technique loosely correspond hybrid counting grid epitome another alternative layout features image updating counting grid words representation compute mapping result hybrid counting grids epitomes used experimental section conference version paper dataset strategy proved successful. relationships models model collapses mixture unigrams point grid mixture component. tessellation also enforced model becomes similar spatial models introduced finally despite counting grid shares focus modeling image feature counts neither model generalization another. however using large windows collate many grid distributions large grid counting grid model thought fig. tessellation step illustration. features extracted quantized decide tessellation sx×sy image compute feature counts separately different section illustrated second third column. limit obtain discrete feature epitome model. grids algorithm works remarkably well given task essentially infer many image patches case image epitome models features representation patches. task formidable directionality provided representation. unfortunately iterative algorithm start features topologically correctly following inconsistent directions different parts counting grid leading local minima however modify model rules deal image representations consist several bags words corresponding section image. case feature re-arrangement tolerated within region regions cannot move relatively model becomes similar spirit speciﬁcally deﬁne tessellation feature compute feature counts separately different section {cts bi-dimensional index runs across sectors process illustrated fig. -ii). inferring mapping section bags window tessellated sections size indexed images tessellated. histogram comparisons done accordingly formulae fig. implementation details useful computational efﬁciency. panles shows efﬁciently computed using cumulative sums panel iii) show shifted versions large mixture sources without overtraining sources highly correlated small shifts grid change window distribution slightly. model beneﬁt thus deal smaller number topics avoid overtraining. topic mixing cannot quite appropriately represent feature correlations traslational camera motion. computational complexity implementation careful examination steps reveals efﬁcient cumulative sums versions steps complexity size counting grid except epitome version. last version counting grid update utilizes feature layout original images requires convolution operation still manageable complexity. previous formula explicated coordinates generic position indeces fig.-i) show slice want compute yellow window. sums done efﬁciently ﬁrst computing linear time cumulative efﬁciency computation multiple section bags eqs. increased sections break window uniformly along directions. case section keep kx+τsky +τsz’s shifted versions fig. -iii) scene/object classiﬁcation tasks image features typically clustered around hundreds centers image locations associated pointers discretized features. example classiﬁcation experiments below clustered sift features visual words. illustrations fig. fig. provide enough insight well counting grids inferred large sets features considered. visualizing feature identities grid difﬁcult order simply study properties counting grid estimation procedures discussed above ﬁrst tests ﬁfty color patches taken random drawing sub-sampled resolution drawing illustrated fig. -i). patches ﬁrst transformed feature maps pointing colors obtained approximating color map. then histograms computed appropriate sections images obtain section bags words algorithm deﬁned appropriate equations algorithm section representation separately obtain counting grids iii) iv). finally plate shows result combination counting grid step i.e. mapping windows based single words epitome step uses known layout features counting grid re-estimate assumption layout could help arrange features counting grid even coarse tessellation. fig. source image patches taken random locations counting grids estimated various versions algorithm. remarkably reconstruction obtained using histograms image features reconstruction iii) used sets histograms result using histograms result using cg-epitomes. cases colors treated unrelated discrete features. videos additional material visualize different counting grids counting grid location assigned color equal average colors color weighted normalized local feature counts πkz. image therefore attempt reconstructing image ﬁfty color histograms provide additional information source image provided algorithm locations images ﬁfty histograms extracted. note also algorithm aware similarities among colors treated discrete features. remarkably spatial structure feature distributions reconstructed histograms. algorithm discovers dark brown tones together bordered green. elongated dark structures blue background discovered coast/island boundary. sense counting grid provides good model interpolating among original histograms histograms original image also likely inferred counting grid. using bags representation images already sufﬁcient break symmetry problems reconstruct almost entire scene. improvement also remarkable case ostensibly little information image patches used source image locations patches available algorithm algorithm uses ﬁfty sets histograms colors found appropriate sections reconstruct island trees. accurate reconstruction obtained iterating eqs. interesting epitome modeling point view. counting grid considered feature epitome detailed feature maps generated rather simply bags features inference step considers patch histograms efﬁciently replaces convolutional step epitome model furthermore case also found combination less prone local minima epitome models pure counting grid inference learning eqs. finally note extreme case tessellating patches individual pixels counting grid becomes feature epitome model. results possible course high redundancy images makes example extracted count numbers represent image patches used reconstruction sufﬁcient partial recovery parameters necessary represent next show procedures used analyze images related fact belong category rather large image resulting generalization space possible feature count distributions surpasses standard count models including latent models latent dirichlet allocation experimental section experiments visual words used sift features clustered discretized features. sift processing based pixel patches spaced pixels apart. image transformed feature features created. fair comparison used implementations reconﬁgurable part-model latent dirichlet allocation features. task unless speciﬁed employed dataset author’s training/testing/validation protocol. classify test image learned model class assigned test samples class gives lowest free energy. considered counting grids various complexities window size limiting tests combinations overall capacity ex·ey wx·wy number training samples. considered updated capacity roughly equivalent number topics represents number independent windows grid; compared results using parallelism scene classiﬁcation scene classiﬁcation task useful shows counting grids generalize well even basic spatial interpolation assumption perfectly met. particular empirically demonstrate individual image thought window larger visual word represented counting grid. previously illustrated fig. sampling windows gave rise features combinations present dataset. fig. -scenes classiﬁcation results. using pink circles also reported results computed using hybrid cg-epitome presence many training images method generalizes well datasets considered -scenes indoor scenes classiﬁcation accuracy former reported fig. x-axis reported different model complexities term capacity whereas yaxis reported accuracy. obtained different choices speciﬁed counting grid size using gray levels lighter marker color bigger grid. fig. shows counting grids performed better latent dirichlet allocation. accuracy regularly increased independently grid size also worth noticing helped prevent overtraining capacities ﬁgure also reported results hybrid cgepitome approach comprises basic cg’s e-step epitome m-step. efﬁciency reasons considered version grid size unequivocally determined therefore used pink markers show results. hybrid generalized well probably abundance training data. tab. reported numerical comparison models discriminative baseline. counting grids well used -fold crossvalidation training pick model complexity. recently -classes dataset introduced. dataset subset whole visual input subject wore wearable camera weeks. images dataset exhibit dramatic viewing angle scale illumination variations foreground objects clutter. category presents images taken particular place house rooms ofﬁce environments outdoors locations. images class shown fig. task place classiﬁcation. validation protocol used -folds cross evaluation. results summarized fig. bag-of-word scenario e.g. latent dirichlet allocation performed better regular counting grids mixture models. explained local minima issues classes limited number training samples counting grid simply cannot well recover panoramic structure half classes. provide directionality information counting grids better exploit panorama outperformed signiﬁcantly naive tessellated extension learns model sector summing likelihoods. finally last panel compared tessellated counting grid tessellated latent dirichlet allocation reconﬁgurable part model uses spatial information. finer tessellations didn’t help recognition neither hurt limit nx×ny accuracy exceed ﬁnal experiment dataset repeated experiment using training images class previously done want test robustness models overtrain regimes. reported ﬁnal accuracy tab.. summarizing counting grids images onto bigger real estate features window stitch overlapping windows trying recover panoramic nature scene. qualities data acquired wearable camera indeed model largely outperform fig. results sensecam dataset. tessellated version iii) tessellated version comparison reconﬁgurable words model latent dirichlet allocation using tessellation. original paper based learning gaussian image mixture model descriptor. addition also compared epitomes among applications epitome successful. method uses resolution epitome image location represented histogram features. method combines several cues histograms disparity features gist. concern counting grids used quantized sift complexity model using cross-validation considering models capacity fig. results torralba dataset. reported results original papers. followed evaluation procedure error bars indicate variability accuracy across different image sequences. training model scene goal compute place posterior probabilities every frame test sequence given previous images easily achieved using forwardbackwards procedure ﬁxed observation likelihood negative free energy given model used estimate transition matrix place posteriors. using observation likelihood dominated transition prior. balance contribution re-scaled likelihood terms using constant chosen crossevaluation results presented fig. improvement signiﬁcant. tessellation marginally helped training data abundant metaphor upon based moving camera perfectly here. indeed spatial layout least piece-wise recovered also single bag. tessellation ﬁner hurt. latent dirichlet allocation rec-bow performances slightly inferior report graph sake clarity. also investigated happens equally scale considered counting grids size three scales experiment torralba’s sequences. results shown fig. represents different scale. results easily interpretable counting grids sensitive choice really matters ratio also evinced fig. fig. complexities characterized similar performed equally well. higher variances large indicate local minima issues. general window sufﬁciently spatial interpolation scaled models learn scaled versions scene quantitatively similar. real estate model learn multiple copies scene. ﬁnally considered worth images sensecam collection repeated test combining counting grids hidden markov models. camera bearer visited labeled locations full dataset nevertheless trained models classes a-priori cannot know locations visited day. torralba sequences goal compute place posterior probabilities instant given previous images used images class learn models. results reported tab. using mixture dirichlet model quantized sift histograms sake completeness also implemented method extracting original descriptors whole images within four sectors. cases performance image clustering sensecam ﬁnal test analyzed subset sensecam divided categories used images subset suitable epitomes actually stitched together using pixels therefore comparison fair. spirit data collection provide summary subject’s life trained counting grids unsupervised investigated images separated counting grid accordance human labeling. compared visual summarization approaches visual input larger grid epitomic approaches clusters pixel measurements within epitome. standard epitomes images mapped epitome means pixel wise comparisons placing bags dimensional space i.e. image mapped particular spot bag-of-word representation agrees images mapped neighborhood. make comparison fair ﬁxed complexity counting grids used epitomes upon learning test image labeled label closest mapped training image. results reported table conclusions introduce counting grid model images captures natural constraints image feature histograms assuming represented averaging feature distributions window grid. ﬂexibility words representation indirectly enriched spatial constraints epitome-like models. observing actual observation model counting grid model attempting capture spatial constraints explicitly often done past. fact view counting grid producing large mixture histograms whose parameters constrained natural consequence fact images features collected live ordered space. despite simplicity conceptual algorithmic ultimate parametrization used likelihood computation simply histograms generative model signiﬁcantly outperforms histogram-based representations variety tasks often approaching discriminative state computationally algorithm efﬁcient computational steps also lend improvement model scale/rotation reasoning. experiments show that despite apparent need setting algorithm sensitive ratio. concern performances counting grids especially tessellated version outperformed standard words approaches computer vision across datasets considered. finally observe variety perina jojic spring lattice counting grids scene recognition using deformable positional constraints. eccv ser. lecture notes computer science fitzgibbon lazebnik perona sato schmid eds. vol. springer coates analysis single-layer networks unsupervised feature learning proceedings fourteenth international conference artiﬁcial intelligence statistics aistats fort lauderdale april available http//www.jmlr.org/ proceedings/papers/v/coatesa/coatesa.pdf jojic perina multidimensional counting grids inferring word order disordered bags words proceedings twenty-seventh conference uncertainty artiﬁcial intelligence barcelona spain july csurka dance willamowski bray visual categorization bags keypoints workshop statistical learning computer vision eccv yang jiang hauptmann evaluating bag-of-visual-words representations scene classiﬁcation proceedings international workshop workshop multimedia information retrieval. york available http//dx.doi.org/. fei-fei perona bayesian hierarchical model learning natural scene categories. proceedings ieee computer society conference computer vision pattern recognition lazebnik schmid ponce beyond bags features spatial pyramid matching recognizing natural scene categories proceedings ieee computer society conference computer vision pattern recognition boiman shechtman irani defense nearestneighbor based image classiﬁcation proceedings ieee computer society conference computer vision pattern recognition componential models layered epitome componential counting grid proceedings ieee computer society conference computer vision pattern recognition amer todorovic sum-product networks modeling activities stochastic structure cvpr jiang yuan randomized spatial partition scene recognition proceedings european conference computer vision volume part ser. eccv’. berlin heidelberg springer-verlag perina jojic castellani cristani murino object recognition hierarchical stel models proceedings european conference computer vision pandey lazebnik scene recognition weakly supervised object localization deformable part-based models iccv deng o’shaughnessy speech processing dynamic optimization-oriented approach ser. signal processing communications series. marcel dekker incorporated available http//books.google.ca/books?id=wrmft perina jojic sight wearable camera classifying visual experience ieee workshop egocentric vision conjunction cvpr tech. rep. available http//arxiv.org/abs/. perina cristani castellani murino jojic free energy score spaces using generative information discriminative classiﬁers ieee trans. pattern anal. mach. intell. vol.", "year": 2014}