{"title": "A Minimalistic Approach to Sum-Product Network Learning for Real  Applications", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Sum-Product Networks (SPNs) are a class of expressive yet tractable hierarchical graphical models. LearnSPN is a structure learning algorithm for SPNs that uses hierarchical co-clustering to simultaneously identifying similar entities and similar features. The original LearnSPN algorithm assumes that all the variables are discrete and there is no missing data. We introduce a practical, simplified version of LearnSPN, MiniSPN, that runs faster and can handle missing data and heterogeneous features common in real applications. We demonstrate the performance of MiniSPN on standard benchmark datasets and on two datasets from Google's Knowledge Graph exhibiting high missingness rates and a mix of discrete and continuous features.", "text": "sum-product networks class expressive tractable hierarchical graphical models. learnspn structure learning algorithm spns uses hierarchical co-clustering simultaneously identifying similar entities similar features. original learnspn algorithm assumes variables discrete missing data. introduce practical simpliﬁed version learnspn minispn runs faster handle missing data heterogeneous features common real applications. demonstrate performance minispn standard benchmark datasets datasets google’s knowledge graph exhibiting high missingness rates discrete continuous features. sum-product network tractable interpretable deep model. advantage spns graphical models bayesian networks allow efﬁcient exact inference linear time network size. represents multivariate probability distribution directed acyclic graph consisting nodes product nodes leaf nodes shown figure standard algorithms learning structure assume discrete data missingness mostly test benchmark data sets satisfy criteria reasonable assumption dealing messy data sets real applications. google knowledge graph semantic network facts based freebase used generate knowledge panels google search. data quite heterogeneous missing since much known entities graph others. high missingness rates also worsen impact discretizing continuous variables structure learning results losing already scarce covariance information. applications like common need learning algorithm handle kind data. paper present minispn simpliﬁcation state-of-the-art learning algorithm improves applicability performance speed. demonstrate performance minispn data standard benchmark data sets. learnspn greedy algorithm performs co-clustering recursively partitioning variables approximately independent sets partitioning training data clusters similar instances shown figure variable instance partitioning steps applied data slices produced previous steps. variable partition step uses pairwise independence tests variables approximately independent sets connected components resulting dependency graph. instance clustering step uses naive bayes mixture model clusters variables cluster assumed independent. clusters learned using hard restarts avoiding overﬁtting using exponential prior number clusters. splitting process continues data standard learnspn algorithm assumes variables discrete missing data. hyperparameter values cluster penalty independence test critical value determined using grid search. clustering step seems unnecessarily complex involving penalty prior restarts hyperparameter tuning. complicated part algorithm seems difﬁcult justify likely time-consuming restarts hyperparameter tuning. propose variation learnspn called minispn handles missing data performs lazy discretization continuous data variable partition step simpliﬁes model instance clustering step require hyperparameter search. simplify naive bayes mixture model instance clustering step attempting split clusters given point eliminating cluster penalty prior results greedy approach learnspn require restarts hyperparameter tuning. seems like natural choice simpliﬁcation extension greedy approach used level learnspn algorithm. compare partition univariate leaves mixture partitions univariate leaves split succeeds two-cluster version higher validation likelihood. split succeeds apply resulting data slices move variable partition step clustering step fails. greedy approach similar used spn-b method however alternates variable instance splits default thus building even deeper spns. variable partition step perform independence test using subset rows variables missing conclude independence number rows threshold. apply binary binning continuous variable using median data slice cutoff. compare pareto algorithm previously used learning models inspired work grosse produces pareto-optimal models trading degrees freedom validation likelihood score. iteration production rules randomly applied partition mixture splits models current model models added model set. model model lower degrees freedom higher likelihood score another model inferior model removed set. algorithm returns model pareto model highest validation likelihood. also compare hybrid method pareto algorithm initialized minispn. data sets knowledge graph people collection. professions data variables boolean indicators whether person belongs particular profession. boolean variables continuous variables. dates data continuous variables representing dates life events person spouse data sets compare minispn pareto hybrid algorithms. able apply standard learnspn algorithm data sets since contain missing data. table shows likelihood performance test runtime performance. minispn better pareto terms likelihood runtime. hybrid performs comparably minispn usually slowest three. benchmark data sets literature compare performance minispn standard learnspn algorithm. particularly interested effect minispn’s simple two-cluster instance split relative complex instance split exponential prior restarts used standard learnspn. table shows likelihood performance test runtime performance. like data minispn uniformly outperforms pareto performs similarly hybrid learnspn runs much faster sum-product networks receiving increasing attention researchers expressiveness efﬁcient inference interpretability many learning algorithms developed past years. recent developments mostly focused improving performance benchmark data sets variation classical learning algorithm simple large impact usability improving speed making possible apply messy real data sets. references kurt bollacker colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database structuring human knowledge. proceedings sigmod international conference management data hoifung poon pedro domingos. sum-product networks deep architecture. proceedings twenty-seventh conference uncertainty artiﬁcial intelligence barcelona spain july amirmohammad rooshenas daniel lowd. learning sum-product networks direct indirect variable interactions. tony jebara eric xing proceedings international conference machine learning jmlr workshop conference proceedings antonio vergari nicola mauro floriana esposito. simplifying regularizing strengthening sum-product network structure learning. machine learning knowledge discovery databases european conference ecml pkdd porto portugal september proceedings part zhao mazen melibari pascal poupart. relationship sum-product networks bayesian networks. proceedings international conference machine learning icml lille france july", "year": 2016}