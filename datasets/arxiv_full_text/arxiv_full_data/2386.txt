{"title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based  Search", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Bayesian model-based reinforcement learning is a formally elegant approach to learning optimal behaviour under model uncertainty, trading off exploration and exploitation in an ideal way. Unfortunately, finding the resulting Bayes-optimal policies is notoriously taxing, since the search space becomes enormous. In this paper we introduce a tractable, sample-based method for approximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our approach outperformed prior Bayesian model-based RL algorithms by a significant margin on several well-known benchmark problems -- because it avoids expensive applications of Bayes rule within the search tree by lazily sampling models from the current beliefs. We illustrate the advantages of our approach by showing it working in an infinite state space domain which is qualitatively out of reach of almost all previous work in Bayesian exploration.", "text": "bayesian model-based reinforcement learning formally elegant approach learning optimal behaviour model uncertainty trading exploration exploitation ideal way. unfortunately ﬁnding resulting bayes-optimal policies notoriously taxing since search space becomes enormous. paper introduce tractable sample-based method approximate bayesoptimal planning exploits monte-carlo tree search. approach outperformed prior bayesian model-based algorithms signiﬁcant margin several well-known benchmark problems avoids expensive applications bayes rule within search tree lazily sampling models current beliefs. illustrate advantages approach showing working inﬁnite state space domain qualitatively reach almost previous work bayesian exploration. objective theory markov decision processes maximize expected discounted rewards dynamics unknown. discount factor pressures agent favor short-term rewards potentially costly exploration identify better rewards long-term. conﬂict leads well-known explorationexploitation trade-off. solve dilemma augment regular state agent information acquired dynamics. formulation idea augmented bayes-adaptive extra information posterior belief distribution dynamics given data observed. agent starts belief state corresponding prior executing greedy policy bamdp whilst updating posterior acts optimally original mdp. framework rich prior knowledge statistics environment naturally incorporated planning process potentially leading efﬁcient exploration exploitation uncertain world. unfortunately exact bayesian reinforcement learning computationally intractable. various algorithms devised approximate optimal learning often rather large cost. here present tractable approach exploits extends recent advances monte-carlo tree search avoiding problems associated applying mcts directly bamdp. iteration algorithm single sampled agent’s current beliefs. used simulate single episode whose outcome used update value node search tree traversed simulation. integrating many simulations therefore many sample mdps optimal value future sequence obtained respect agent’s beliefs. prove process converges bayes-optimal policy given inﬁnite samples. increase computational efﬁciency introduce innovation lazy sampling scheme considerably reduces cost sampling. applied algorithm representative sample benchmark problems competitive algorithms literature. consistently signiﬁcantly outperformed existing bayesian methods also recent non-bayesian approaches thus achieving state-of-the-art performance. algorithm efﬁcient previous sparse sampling methods bayes-adaptive planning partly update posterior belief state course simulation. thus avoids repeated applications bayes rule expensive simplest priors mdp. consequently algorithm particularly well suited support planning domains richly structured prior knowledge critical requirement applications bayesian reinforcement learning large problems. illustrate beneﬁt showing algorithm tackle domain inﬁnite number states structured prior dynamics challenging intractable task existing approaches. describe generic bayesian formulation optimal decision-making unknown following described -tuple states actions state transition probability kernel bounded reward function discount factor components tuple known standard planning algorithms used estimate optimal value function policy off-line. general dynamics unknown assume latent variable distributed according distribution observing history actions states sasa at−st posterior belief updated using bayes’ rule uncertainty dynamics model transformed uncertainty current state inside augmented state space state space original problem possible histories. dynamics associated augmented state space described optimal action state readily derived. optimal actions bamdp executed greedily real constitute best course action bayesian agent respect prior belief obvious expected performance bamdp policy bounded optimal policy obtained fullyobservable model equality occurring example degenerate case prior support true model. algorithm description goal bamdp planning method decision point encountered action maximizes equation algorithm bayes-adaptive monte-carlo planning performing forward-search space possible future histories bamdp using tailored monte-carlo tree search. employ algorithm allocate search effort promising branches state-action tree sample-based rollouts provide value estimates node. clarity denote bayes-adaptive algorithm applies vanilla bamdp sample-based search bamdp using ba-uct requires generation samples every single node. operation requires integration possible transition models least sample transition model expensive procedure simplest generative models avoid cost sampling single transition model posterior root search tree redundancy state-history tuple notation sufﬁx present ensure start simulation using generate necessary samples simulation. sample-based tree search acts ﬁlter ensuring correct distribution state successors obtained tree nodes sampled root sampling method originally introduced pomcp algorithm developed solve partially observable mdps. root node search tree decision point represents current state bamdp. tree composed state nodes representing belief states action nodes representing effect particular actions parent state node. visit counts state nodes action nodes initialized updated throughout search. value initialized also maintained action node. simulation traverses tree without backtracking following policy state nodes deﬁned appropriately. given action transition distribution corresponding current simulation used sample next state. action node sampled state node has. simulation reaches leaf tree expanded attaching state node connected action nodes rollout policy used control deﬁned current ﬁxed depth rollout provides estimate value leaf action node. estimate used update value action nodes traversed simulation sampled discounted return obtained traversed action node given simulation update value action node a)/n detailed description bamcp algorithm provided algorithm diagram example bamcp simulations presented figure tree policy treats forward search meta-exploration problem preferring exploit regions tree currently appear better others continuing explore unknown less known parts tree. leads good empirical results even small number simulations effort expended search seems fruitful. nevertheless parts tree eventually visited inﬁnitely often therefore algorithm eventually converge bayes-optimal policy finally note history transitions generally compact sufﬁcient statistic belief fully observable mdps. indeed replaced unordered transition counts considerably reducing number states bamdp potentially complexity planning. given addressing scheme suitable resulting expanding lattice bamcp search reduced space. found version bamcp offer marginal improvement. common ﬁnding stemming tendency concentrate search effort several equivalent paths implying limited effect performance reducing number paths. previous work sample-based tree search indeed including pomcp complete sample state drawn posterior root search tree. however computationally costly. instead sample lazily creating particular transition probabilities required simulation traverses tree also rollout. consider parametrized latent variable state action pair. depend other well additional latent variables posterior {θsa|s deﬁne {θsa··· θstat} parameters required course bamcp simulation starts time ends time using chain rule rewrite length simulation denotes parameters required simulation. simulation sample root lazily sample θstat parameters required conditioned parameters sampled current simulation. process stopped simulation potentially parameters sampled. example transition parameters different states actions independent completely forgo sampling complete instead draw necessary parameters individually state-action pair. leads substantial performance improvement especially large mdps single simulation requires small subset parameters choice rollout policy important simulations especially domain display substantial locality rewards require carefully selected sequence actions obtained. otherwise simple uniform random policy chosen provide noisy estimates. work learn optimal q-value real model-free manner samples obtained off-policy result interaction bayesian agent environment. acting greedily according translates pure exploitation gathered knowledge. rollout policy bamcp following could therefore over-exploit. instead similar select \u0001-greedy policy respect rollout policy πro. biases rollouts towards observed regions high rewards. method provides valuable direction rollout policy negligible computational cost. complex rollout policies considered example rollout policies depend sampled model however usually incur computational overhead. deﬁnition theorem implies bamcp converges bayes-optimal solution asymptotically. conﬁrmed result empirically using variety bandit problems bayes-optimal solution computed efﬁciently using gittins indices section compare bamcp existing bayesian algorithms. given limited space provide comprehensive list planning algorithms exploration rather concentrate related sample-based algorithms bayesian bayesian maintains posterior distribution transition models. step single model sampled action optimal model executed. best sampled algorithm generalizes idea boss samples number models posterior combines optimistically. drives sufﬁcient exploration guarantee ﬁnite-sample performance guarantees. boss quite sensitive parameter governs sampling criterion. unfortunately difﬁcult select. castro precup proposed sboss variant provides effective adaptive sampling criterion boss algorithms generally quite robust suffer over-exploration. sparse sampling sample-based tree search algorithm. idea sample successor nodes state apply bellman backup update value parent node values child nodes. wang applied sparse sampling search belief-state mdps. tree expanded non-uniformly according sampled trajectories. decision node promising action selected using thompson sampling i.e. sampling beliefstate solving taking optimal action. chance node successor belief-state sampled transition dynamics belief-state mdp. asmuth littman extended idea algorithm adaptation forward search sparse sampling belief-mdps. although described algorithm montecarlo tree search fact uses bellman backup rather monte-carlo evaluation. bellman backup updates lower upper bounds value node. like wang tree expanded non-uniformly according sampled trajectories albeit using different method action selection. decision node promising action selected maximising upper bound value. chance node observations selected maximising uncertainty bayesian exploration bonus solves posterior mean additional reward bonus depends visitation counts similarly sorg propose algorithm different form exploration bonus algorithms provide performance guarantees polynomial number steps environment. however behavior early steps exploration sensitive precise exploration bonuses; turns hard translate sophisticated prior knowledge form bonus. table experiment results summary. algorithm report mean rewards conﬁdence interval best performing parameter within reasonable planning time limit bamcp simply corresponds number simulations achieve planning time imposed limit. results reported without timing information. experiments ﬁrst present empirical results bamcp standard problems comparisons popular algorithms. showcase bamcp’s advantages large scale task inﬁnite grid complex correlations reward locations. standard domains algorithms following algorithms bamcp algorithm presented section implemented lazy sampling. algorithm different number simulations span different planning times. experiments \u0001-greedy policy exploration constant left unchanged experiments experimented values similar results. sboss domain varied number samples resampling threshold parameter domain varied bonus parameter domain varied branching factor number simulations depth search domains except larger grid maze domain also tuned vmax parameter domain vmin always addition report results several prior algorithms. domains domains double-loop domain -state deterministic actions steps executed domain. grid grid reward anywhere except reward state opposite reset state. actions cardinal directions executed small probability failure steps. grid grid designed like grid. collect steps domain. dearden’s maze -states maze ﬂags collect special reward state gives number ﬂags collected since last visit reward steps executed domain. quantify performance algorithm measured total undiscounted reward many steps. chose measure performance enable fair comparisons drawn prior work. fact optimising different criterion discounted reward start state might expect evaluation unfavourable algorithm. major advantage bayesian specify priors dynamics. double-loop domain bayesian algorithms simple dirichlet-multinomial model symmetric dirichlet parameter |s|. grids maze domain algorithms sparse dirichlet-multinomial model described models efﬁcient collapsed sampling schemes available; employed ba-uct algorithms experiments compress posterior parameter sampling transition sampling single transition sampling step. considerably reduces cost belief updates inside search tree using simple probabilistic models. general efﬁcient collapsed sampling schemes available results summary results presented table figure reports planning time/performance trade-off different algorithms grid maze domain. domains tested bamcp performed best. algorithms came close tasks parameters tuned speciﬁc domain. particularly evident required different value exploration bonus achieve maximum performance domain. bamcp’s performance stable respect choice exploration constant require tuning obtain results. bamcp’s performance scales well function planning time evident figure contrast sboss follows opposite trend. samples employed build merged model sboss actually becomes optimistic over-explores degrading performance. cannot take advantage prolonged planning time all. generally scales planning time appropriate choice parameters obvious trade-off branching factor depth number simulations domain. bamcp greatly beneﬁted lazy figure performance algorithm grid maze domain function planning time. point corresponds single algorithm associated setting parameters. increasing brightness inside points codes increasing value parameter second dimension variation coded size points range parameters speciﬁed section performance algorithm grid domain. performance algorithm maze domain. maze domain performance vanilla ba-uct without rollout policy learning maze domain performance bamcp without lazy sampling rollout policy learning presented sections root sampling included. sampling scheme experiments providing speed improvement naive approach maze domain example; illustrated figure dearden’s maze aptly illustrates major drawback forward search sparse sampling algorithms bfs. like many maze problems rewards zero least steps solution length. without prior knowledge optimal solution length upper bounds higher true optimal value tree fully expanded depth even simulation happens solve maze. contrast bamcp discovers successful simulation monte-carlo evaluation immediately bias search tree towards successful trajectory. also applied bamcp much larger problem. generative model inﬁnite-grid follows column associated latent parameter beta associated latent parameter beta. probability grid cell reward piqj otherwise reward agent knows grid always free move four cardinal directions. rewards consumed visited; returning location subsequently results reward opposed independent dirichlet priors employed standard domains here dynamics tightly correlated across states posterior inference performance ﬁrst steps environment averaged sampled environments reported terms undiscounted discounted rewards. bamcp either correct generative model prior incorrect prior clear bamcp take advantage correct prior information gain rewards. performance uniform random policy also reported. dynamics model requires approximation non-conjugate coupling variables inference done mcmc domain illustrated figure planning algorithms attempt solve based sample posterior cannot directly handle large state space. prior forward-search methods deal state space large belief space every node search tree must solve approximate inference problem estimate posterior beliefs. contrast bamcp limits posterior inference root search tree directly affected size state space belief space allows algorithm perform well even limited planning time. note lazy sampling required setup since full sample dynamics involves inﬁnitely many parameters. figure demonstrates planning performance bamcp complex domain. performance improves additional planning time quality prior clearly affects agent’s performance. supplementary videos contrast behavior agent different prior parameters. future work algorithm known several drawbacks. first ﬁnite-time regret bounds. possible construct malicious environments example optimal policy hidden generally reward region tree misled long periods second algorithm treats every action node multi-armed bandit problem. however actual beneﬁt accruing reward planning theory appropriate pure exploration bandits nevertheless algorithm produced excellent empirical performance many domains bamcp able exploit prior knowledge dynamics principled manner. principle possible encode many aspects domain knowledge prior distribution. important avenue future work explore rich structured priors dynamics mdp. prior knowledge matches class environments agent encounter exploration could signiﬁcantly accelerated. conclusion suggested sample-based algorithm bayesian called bamcp signiﬁcantly surpassed performance existing algorithms several standard tasks. showed bamcp tackle larger complex tasks generated structured prior existing approaches scale poorly. addition bamcp provably converges bayes-optimal solution. main idea employ monte-carlo tree search explore augmented bayes-adaptive search space efﬁciently. naive implementation idea proposed ba-uct algorithm cannot scale priors expensive belief updates inside search tree. introduced three modiﬁcations obtain computationally tractable sample-based algorithm root sampling requires beliefs sampled start simulation model-free algorithm learns rollout policy; lazy sampling scheme sample posterior beliefs cheaply.", "year": 2012}