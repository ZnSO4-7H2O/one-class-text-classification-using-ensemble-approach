{"title": "Are we Done with Object Recognition? The iCub robot's Perspective", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "I.2.9; I.2.10; I.2.11; I.4.7; I.4.8; I.4.9; I.4.10; I.5.1; I.5.2;  I.5.4; I.5.5"], "abstract": "We report on an extensive study of the current benefits and limitations of deep learning approaches to robot vision and introduce a novel dataset used for our investigation. To avoid the biases in currently available datasets, we consider a human-robot interaction setting to design a data-acquisition protocol for visual object recognition on the iCub humanoid robot. Considering the performance of off-the-shelf models trained on off-line large-scale image retrieval datasets, we show the necessity for knowledge transfer. Indeed, we analyze different ways in which this last step can be done, and identify the major bottlenecks in robotics scenarios. By studying both object categorization and identification tasks, we highlight the key differences between object recognition in robotics and in image retrieval tasks, for which the considered deep learning approaches have been originally designed. In a nutshell, our results confirm also in the considered setting the remarkable improvements yield by deep learning, while pointing to specific open challenges that need to be addressed for seamless deployment in robotics.", "text": "artiﬁcial intelligence recently progressed dramatically largely thanks advance deep learning. computational vision speciﬁcally object classiﬁcation perhaps obvious example deep learning achieved stunning results raise question whether problem actually solved case robotics would main ﬁeld beneﬁts could reaching eﬀect. indeed lack reliable visual skills largely considered main bottleneck successful deployment robotics systems everyday life perspective mind recently started eﬀort isolate quantify beneﬁts limitations deep learning approaches object recognition robotics clearly visual perception possible sensory modalities enabling object recognition robotics indeed comparison human intelligence suggests just vision object recognition nonetheless current deep learning based artiﬁcial vision systems perform well seems natural perceptual cues/modalities needed. work mainly focus object recognition tasks robotics using visual cues. investigate eﬀectiveness deep learning approaches robotic applications designed dataset tailored reﬂect prototypical visual experience humanoid robot. indeed remarkable performance deep learning methods object recognition abstract report extensive study current beneﬁts limitations deep learning approaches robot vision introduce novel dataset used investigation. avoid biases currently available datasets consider human-robot interaction setting design data-acquisition protocol visual object recognition icub humanoid robot. considering performance oﬀ-the-shelf models trained oﬀ-line large-scale image retrieval datasets show necessity knowledge transfer. indeed analyze diﬀerent ways last step done identify major bottlenecks robotics scenarios. studying object categorization identiﬁcation tasks highlight diﬀerences object recognition robotics image retrieval tasks considered deep learning approaches originally designed. nutshell results conﬁrm also considered setting remarkable improvements yield deep learning pointing speciﬁc open challenges need addressed seamless deployment robotics. primarily reported computer vision benchmarks essentially designed image retrieval tasks hardly representative robotics scenario. fact motivation common recent works datasets proposed. using icub robot devised human-robot interaction framework acquire corresponding dataset named icwt dataset rich easy expand include data complex perceptual scenarios. includes several object categories many instances category hence allowing test categorization identiﬁcation capabilities. notably dataset segmented diﬀerent sets views purpose testing speciﬁcally robustness invariance properties recognition systems within realistic robotic scenario. provided icwt dataset performed extensive empirical investigation using diﬀerent state convolutional neural networks architectures began checking extent systems already trained could directly tested icwt. obtaining results much better chance systems perform accurately enough case perhaps could expect. hence followed recent trend based general idea transfer learning pretrained networks adapted data hand. indeed results conﬁrm eﬀectiveness strategies obtaining substantial improvements. impressive methods quite provide close perfect accuracy would wish for. hence proceeded taking closer look results starting question whether missing could imputed lack data. indeed cnns known need massive amount data work data-augmentation often used improve results. discuss detail later paper investigating latter question highlighted diﬀerences icwt datasets imagenet generally identiﬁed clear diﬀerences object recognition task robotics respect scenarios typically considered learning vision. works quantify merits categorization also identiﬁcation. description discussion empirical ﬁndings concluded critical review main venue improvements pure machine learning perspective also taking extensive advantage robotic platform. indeed bridging performance appears exciting avenue future multidisciplinary research. next subsection discuss several related works rest paper organized follows sec. introduces icwt dataset acquisition setting. sec. review deep learning methods considered empirical analysis reported sec. categorization task sec. object identiﬁcation. sec. concludes study review possible directions improvement visual recognition robotics. deep learning methods receiving growing attention robotics adopted variety problems object recognition place recognition mapping object aﬀordances grasping tactile perception limit discussion work object recognition relevant work described paper. authors demonstrate transfer learning pre-trained deep convolutional neural networks propose include depth information rgb-d camera. main idea extract feature vector train cascade support vector machines discriminate objects’ class identity position. depth data encoded planar image using colorization scheme. authors report performance increase washington rgb-d benchmark work shows robot self-generated explorative actions autonomously extract training data train cnn. paper employ less constrained setting cnns trained data acquired robot natural interaction employ similar techniques transfer learning consider wider range architectures tuning techniques. contrast human teacher gives control object trasformations. clearly work could extended introducing self-supervision using explorative actions similar ones work work similar work presented paper investigate invariance properties cnns learning examples. focus however instance recognition whereas paper consider thus signiﬁcantly extending problem object categorization. addition perform detailed investigation various ﬁne-tuning techniques systematic evaluation recognition performance speciﬁc object transformations. literature several datasets visual recognition robotics proposed coil aloi washington rgb-d short- bigbird rutgers amazon picking challenge rgb-d dataset main characteristic datasets capture images object undergoes speciﬁc viewpoint transformations. however datasets usually acquired strictly controlled turntable settings aiming provide accurate annotations ground truth grasping manipulation rather substantial variations objects’ appearance. consequence mainly contain changes point view under-representing visual transformations scaling background changes forth fundamental benchmark visual recognition tasks. instance washington rgb-d pascal datasets order evaluate invariance deep learning methods analysis consequently focuses mainly viewpoint changes. moreover since object typically positioned turntable subsequently rotated dataset show natural object transformations often occur practice. review major limitations current datasets. norb dataset also acquired similar turntable setting ﬁrst released speciﬁcally support investigation invariance properties recognition methods fact images dataset artiﬁcially perturbed order increase variability. recently motivated similar considerations presented ilab-m dataset speciﬁcally authors create large-scale visual recognition benchmark which beyond representing high number object instances provides also suﬃcient number images object order study invariance properties deep learning methods. also ilabm acquired turntable setting order collect annotations image terms object’s pose. icwt dataset presented paper separates previous work objects captured undergoing natural transformations. acquisition performed semi-controlled setting intended benchmark reproducing typical uncertainties faced visual recognition system robot real-world task. following discuss icwt dataset related acquisition setting detail. section present novel dataset visual recognition icubworld transformations used empirical analysis work. icwt latest release icubworld project whose goal benchmark improve visual recognition systems robotics. icubworld datasets designed record prototypical visual experience robot humanoid icub performing vision-based tasks. devised implemented simple human-robot interaction application directly acquire images dataset robot’s cameras. remarkable advantage collecting icubworld directly robot platform. indeed resulting dataset oﬀers natural testbed visual recognition robotics close possible real application. particular ensures performance measured oﬀ-line icubworld expected generalize well system deployed actual robot. note aspect icubworld extremely relevant since visual biases make typically diﬃcult generalize prediction performances across diﬀerent datasets applications already well-known previous work shown empirically sec. paper. currently acquire icubworld releases make extensive robot’s physical capabilities done current deep learning methods achieve already remarkable performance relying solely visual cues goal evaluate accuracy isolation robotic setting. indeed exploiting robot body could provide dramatic advantages modeling recognition recently would also prevent correctly assess contribution purely visual components. possibly contribute icubworld. indeed datasets visual recognition literature acquired directly robotic platform common reason scaling dataset size extremely expensive time consuming using single robot. appealing solution proposed million object challenge project involve multiple robots collect shared dataset visual experiences. data acquisition icwt followed protocol similar adopted previous icubworld releases human teacher shows object robot pronounces associated label annotate subsequent images. robot exploits bottom-up visual cues track collect images object human actor moves shows diﬀerent poses. work decided adopt object tracker using depth information place exploiting detection independent motion scene fact shown allows extract precise bounding around object interest. developed application scale acquisition procedure hundreds objects collected multiple interactive sessions. icwt available on-line plan make also application publicly available order laboratories protocol collect also imagenet large-scale visual recognition challenge i.e. found semantically visually similar classes among classiﬁcation challenge. remaining categories appear ilsvrc belong synset larger imagenet dataset provide qualitative intuition semantic variability within given category namely diﬀerent visual appearance objects category fig. shows sample image instances category. refer reader supplementary material example images object instances icwt. peculiarity icwt motivating name object shown multiple image sequences undergoes speciﬁc visual transformations done test invariance visual models robotics. fig. semantic variability. sample images diﬀerent object instances category provide qualitative intuition semantic variability icubworld. fig. supplementary material examples. indeed since works study real visual transformations order evaluate invariance properties visual representations believe icwt could signiﬁcant contribution direction. knowledge icwt ﬁrst dataset address invariancerelated questions robotics accounts much wider range visual transformations respect previous datasets. object instance acquired diﬀerent image sequences human supervisor performed diﬀerent visual transformation object. fig. reports excerpts sequences contain respectively rotation similarly rotations object kept position scale. however time human applied generic rotation object consequence diﬀerent faces object shown camera robot keeping approximately distance pose object respect camera plane. acquisition process background changes object appearance remains approximately human moved object freely front robot person would naturally showing item child. sequence nuisances combinations appear sition objects split multiple sessions performed diﬀerent days. acquisition location always lighting condition artiﬁcially controlled since wanted investigate role nuisance acquired objects diﬀerent times diﬀerent days lighting conditions slightly changing across objects moreover repeated acquisition object diﬀerent days ended sequences object containing visual transformations diﬀerent light conditions. recorded centroid bounding provided tracker frame left right images acquired allow oﬄine computation disparity potentially improvement object’s localization segmentation. tab. summarizes main characteristics icubworld-transformations. networks describe architectures used analysis algorithms adopted train apply them. refer interested reader in-depth introduction cnns deep learning general. deep convolutional neural networks hierarchical models organized concatenation multiple processing layers. structure allows mapping input signal series subsequent representations progressively select features relevant considered task. prototypical structure performs layer following operations architecture characterized speciﬁc choice implementation operations above. ﬁlters locally processing input signal layer typically learned minimizing desired loss function overall classiﬁcation accuracy supervised settings reconstruction error unsupervised ones. spatial downsampling pooling operations make representation robust transformations input increasingly larger scales. non-linearities selectively retain features relevant task suppressing others. recently common strategy image classiﬁcation settings follow convolution layers fully connected layers namely standard multi-layer neural network. last years however architectures fewer even layer started preferred since achieve comparable better performance optimizing signiﬁcantly fewer parameters classiﬁcation settings ﬁnal layer network typically softmax function maps output individual class likelihood scores. modular structure cnns allows training parameters simultaneously layers back-propagation given large number parameters optimized cnns typically need large amounts training data achieve good performance. various forms data augmentation often needed artiﬁcially increase number training examples mitigate risk overﬁtting regularization techniques weights regularization dropout recently batch normalization proved helpful. work investigate performance modern cnns robotic setting icubworld. analysis selected recent architectures achieving highest accuracy imagenet large-scale visual recognition challenge used corresponding implementation trained ilsvrc publicly available within caffe framework. summarize structures fig. example convolutional neural network knowledge transfer approaches considered work feature extraction case response layers used feature vector shallow predictor like rlscs svms trained task. ﬁne-tuning case network trained end-to-end task replacing ﬁnal layer using original model warm-restart network image from. deep learning methods typically require large datasets successfully trained. could principle prevent applicability problems training data scarce recent empirical evidence shown knowledge acquired training network largescale problem transferred domain. diﬀerent approaches implement strategy proposed literature. section review well-established method empirically assess experiments namely feature extraction ﬁne-tuning. observed deeper layers selectively respond speciﬁc properties visual scene typically characteristic speciﬁc objects object parts. responses interpreted representations visual input responding features relevant task hand. following intuition shown cnns trained large-scale datasets indeed used feature extractors smaller datasets leading remarkable performance typically done training standard classiﬁer support vector machine regularized least squares classiﬁer feature vectors obtained multiple layer responses. strategy depicted fig. interpreted changing last layer trainimplementation details. empirical analysis work used feature extraction strategy transfer knowledge cnns trained imagenet icwt. speciﬁc layers used experiments reported tab. four architectures considered paper. used rlsc gaussian kernel classiﬁer cnn-extracted features. particular implemented nystrom sub-sampling approach considered computationally appealing large-scale settings indeed allowed signiﬁcantly speed experimental evaluation. refer reader supplementary material details model selection image preprocessing protocols adopted work. trained large-scale dataset used warm-start learning process domains. strategy known ﬁnetuning consists performing back-propagation training initializing parameters network previously learned settings necessary adapt ﬁnal layer task table feature extraction layers four architectures considered work. used notation adopted literature number identiﬁes layer number label speciﬁes type table fine-tuning protocols caﬀenet googlenet. base starting learning rate layers initialized original model. layers learned scratch indicated using names caffe models specifying starting learning rate used them. parameters refer reader caffe documentation. ever ﬂexibility comes price involved training process. indeed performance ﬁne-tuned network extremely dependent choice hyper-parameters available. fine-tuning recently used adapt network models learned imagenet robotic tasks implementation details. experiments performed ﬁne-tuning caﬀenet googlenet representative recent architectures signiﬁcantly faster ﬁne-tune vgg- resnet-. considered main regimes ﬁne-tuning network updating layers keeping others ﬁxed another aggressively adapting layers comprising layers training dataset. done selecting diﬀerent learning rates neurons layer i.e. size gradient steps iteration back-propagation. refer protocols conservative adaptive report corresponding parameters tab. reference. experiments discussed paper consider strategies since observe signiﬁcant performance diﬀerences following choices parameters. modern datasets visual recognition comprise extremely large number images depicting objects wide range natural scenes. extreme variability opens question whether datasets icubworld represent smaller reality could interpreted simply sub-domains larger ones. case deep learning models trained imagenet would achieve high recognition performance icubworld well without re-training adaptation. result would lead appealing scenario robot simply download visual classiﬁer oﬀ-the-shelf previous analysis shown computer vision datasets essentially biased ultimately preventing learning algorithm generalize domain similarly recently observed conventional non-deep models trained imagenet perform poorly applied robotic settings address question evaluated four oﬀ-theshelf cnns task image classiﬁcation icwt. experiments restricted test categories icwt appear also ilsvrc challenge reference compared results average accuracy networks corresponding categories imagenet dataset. test icwt composed category images object instances comprising transformations left camera viable alternative transfer knowledge essentially adapting networks trained imagenet setting idea knowledge transfer recently received considerable attention deep learning community provides robust approach apply deep learning methods smaller datasets. fig. report classiﬁcation performance achieved systems knowledge transfer applied experiments followed protocol described sec. rlsc predictors trained feature vectors extracted deeper layers cnns. created separate training icwt choosing instances category training keeping instance testing. repeated experiments trials order allow instance category used test set. fig. reports average accuracy trials. observe sharp improvement networks achieve remarkable classiﬁcadeep learning methods require large amounts data order trained eﬀectively. particularly true training network scratch valid also knowledge transfer techniques. indeed mentioned sec. common practice training perform data augmentation namely artiﬁcially increase dataset size applying synthetic transformations original images perspective robotic setting seems particularly favorable. indeed data augmentation performed simply acquiring frames depicting object viewpoint illumination change naturally. acquiring frames object particularly convenient robotics typically expensive gather instead images depicting diﬀerent objects instances since robot needs directly observe them. section investigate impact fig. average classiﬁcation accuracy oﬀ-the-shelf networks tested icubworld imagenet test sets datasets restricted shared categories reports classiﬁcation accuracy networks transferred icubworld shows recognition chance random classiﬁer. rather imagenet suggesting diﬀerences between datasets exist particular icubworld sub-domain imagenet. drop likely biases imagenet in-depth analysis bias outside scope work supplementary material provide evidence eﬀect. care point fair comparison restricted output oﬀ-the-shelf networks classes considered experiment icwt refer supplementary material experiments reporting performance considering entire dimensional prediction. knowledge transfer. drop performance observed fig. implies necessary train recognition system target domain. however experiment also shows networks performed substantially better chance suggests networks retained knowledge objects’ appearance. therefore rather training novel architecture from scratch accuracy caﬀenet googlenet models ﬁne-tuned according conservative adaptive strategies accuracy rlsc classiﬁers trained features representations extracted architectures considered work fig. recognition accuracy instances accuracy caﬀenet googlenet models ﬁne-tuned according conservative adaptive strategies accuracy rlsc classiﬁers trained features extracted architectures considered work aspects robot vision taking icubworld testbed analysis. note following term instance refer speciﬁc object belonging given category frame denotes single image depicting scene object. considered -class categorization task icwt compare performance learning models trained increasing number examples. created training sets diﬀerent size sampling respectively frames transformation sequence object. category used objects training validation test. validation test sets contained images available corresponding instances. order account statistical variability repeated experiments trials time leaving diﬀerent instance testing. experiment trained tested available days icwt fig. reports average classiﬁcation accuracy diﬀerent learning models training examples provided. surprisingly architecture achieve remarkably high accuracy already trained smallest training show little improvement data available. ﬁnding contrast expectations since increasing dataset size seem signiﬁcant improvement performance. evaluated impact adding object instances training recognition system. consider process increasing semantic variability training contrast increasing geometric variability showing frames known object instance. considered -class categorization task sec. created four training sets containing frames sampled increasing number instances category namely dataset training data randomly sub-sampled available transformation sequences achieve identical total size images category. validation test sets contained frames remaining instance repeated experiments trials. analysis shown limited availability semantic variation naturally occurs robotics applications dramatically aﬀect recognition accuracy learning system. moreover contrarily expectations observed limitation canalleviated simply feeding training data network. indeed classiﬁers trained datasets diﬀerent size achieved identical performance. extremely problematic since robotics overhead collecting acquiring examples novel instance extremely expensive collecting views object comes cost. unfortunately cannot adopt data augmentation strategies artiﬁcially increase semantic variability dataset done case view-point variability. ﬁndings appears clear bridging robot vision image retrieval needs deeper analysis. following deepen analysis performance cnns icwt particular focus concept invariance. sec. models trained frames achieve comparable even better accuracy trained larger datasets. results suggest corresponding networks could leverage robust data representation indeed section investigate extent models invariant visual transformations icwt. indeed invariance i.e. robustness identity-preserving visual transformations scene highly desirable behavior designing training visual recognition system since dramatically reduce complexity learning problem increasing capability generalizing visual appearance object category limited number examples even appealing robotics applications where usually robot required learn objects recognize reliably unpredictable conditions. test invariance cnns considered -class categorization problem introduced sec. used object instances category training validation testing. however examples available sequences semantic variability namely amount diﬀerent object instances available category amount frames instance. typically image retrieval settings large number images depicting diﬀerent object instances category available instance images provided. example ilsvrc fig. generalization performance across diﬀerent visual transformations. learning models trained transformations present icwt tested transformations. accuracy reported separately plot tested transformation. shaded areas represent improvement achieved including background mix). instead performed training using individual transformation tested learned model others. repeated experiments trials. considered regimes including images sequence another subsampling images factor since previously observed that including instances training important including frames not. sec. limit analysis available days icwt. indeed setting study eﬀect isolated viewpoint transformations without considering nuisances example changes light setting conditions appear another. experiment report accuracy approaches rely application rlsc feature representations extracted using oﬀ-theshelf models done isolate invariance properties networks trained imagenet. note however performed experiments also ﬁne-tuning networks previous tests observing similar results. fig. reports generalization capabilities models trained single transformation tested diﬀerent one. reference considered also training obtained randomly sampling points training sets keeping similar size. training analogous used experiments sec. size ﬁrst conﬁrm adding example frames transformation signiﬁcantly improves networks’ invariance notice overall models exhibit invariance transformations included training set. worth noticing cases best performance obtained training testing performed transformation transformations included training demonstrates enriching dataset include transformations degrade performance. moreover training achieves good performance tested every speciﬁc transformation. suggests showing object robot natural transformations appearing random combinations instead systematically collecting sequences comprising individual transformations feasible approach obtain predictors invariant transformations. interesting note rather diﬀerent models trained imagenet exhibit invariance pattern. particular performance drops substantially testing transformation present training set. expected transformations involving rotations object quite surprising aﬃne ones namely scale rotation background convolutional structure invariant design learned training imagenet. investigate fact sec. study invariance models context object identiﬁcation rather categorization. work focused visual categorization problems namely assign given object instance corresponding category. section move instead task object identiﬁcation consists labeling given image according object instance appearing problem indeed relevant robotics settings since reliable instance recognition skills would provide system ﬁner control tasks manipulation grasping etc. also considering domestic scenario robot could asked speciﬁc object rather object category therefore parallel experiments categorization work assessed performance deep learning methods task object identiﬁcation robotics. note recently problem approached methods based keypoints extraction template matching however recently observed appraoches relying holistic visual representations perform typically better low/mid-resolution settings icubworld following this section focus architectures considered sec. sec. seen knowledge acquired imagenet transferred icubworld domain successfully tackle categorization tasks. natural question whether strategy would similarly favorable object identiﬁcation. indeed object identiﬁcation settings semantic variability categorization observed semantic variability extremely useful learning process simply adding images given instance almost redundant addressed question considering object identiﬁcation task icwt compared models trained increasing number examples. setting similar used categorization -class object identiﬁcation problem chose object instances book ﬂower glass hairbrush hairclip categories appear ilsvrc dataset. created four training sets containing respectively fig. recognition accuracy frames accuracy caﬀenet googlenet models ﬁne-tuned according conservative adaptive strategies accuracy rlsc classiﬁers trained features representations extracted architectures considered work images object sampled randomly transformation sequences scale dataset images retained model selection. images sequence used test classiﬁcation accuracy methods considered. categorization experiment images single used experiments. fig. reports average accuracy architectures trained growing number examples. results stark contrast counterpart categorization clearly notice adding training data dramatically improves recognition capabilities networks. suggests access views instance allows learning systems create nuanced model object. apparently richer model provide advantage categorization settings extremely useful recognize object images. line observations notice also setting adaptive ﬁne-tuning strategy signiﬁcantly outperforms competitors suggesting representation learned categorization imagenet albeit already beneﬁcial task improved adapt object identiﬁcation setting. ﬁndings conﬁrm extend recent similar results instance retrieval literature fig. generalization performance across diﬀerent visual transformations rlsc trained transformations present icwt tested transformations. accuracy reported separately plot tested transformation. designed already invariant view-point transformations. addressed question considering learning scenario similar adopted sec. investigate invariance categorization settings focused object identiﬁcation. speciﬁcally considered -class identiﬁcation task restricted training contain images single transformation sequence available icubworld resulting models tested separately remaining transformations assess ability learning systems generalize viewpoints. similarly sec. considered evaluated learning methods based feature extraction order evaluate viewpoint invariance oﬀ-the-shelf networks. single transformation using respectively frames object tested another. notice performance varies remarkably tested transformation another suggesting representations considered experiments invariant transformations observed icwt. interestingly comparing training regimes notice remarkable improvement observed sec. adding frames conﬁrmed extended also setting system trained single transformation time. expected recent networks signiﬁcantly outperform previous models resnet- achieving highest accuracy large margin. similarly observed categorization setting fig. reports quite performance generalizing across transformations. following discuss possible strategies address problem. section consider diﬀerent ﬁne-tuning strategies improve invariance properties oﬀ-the-shelf cnns identiﬁcation settings. analysis motivated observation fig. adaptive ﬁnetuning performs signiﬁcantly better conservative feature extraction based strategies. indeed suggests adapting representation cnns identiﬁcation task helpful recognition possibly network actually learning transformations needs invariant. investigate whether ﬁne-tuning icubworld domain indeed improve/adapt invariance properties network identiﬁcation task. performed preliminary experiments related question considered smaller transformations ﬁne-tuning strategies focused caﬀenet architecture. contains images instances object categories icwt used previous experiments namely oven glove. squeezer sprayer body lotion soda bottle. fine-tuning performed adaptive strategy identiﬁcation task. training validation sets obtained following protocol sec. fig. experiment setting fig. using diﬀerent image representations provided caﬀenet googlenet network models ﬁne-tuned according diﬀerent strategies dataset contains images categories sampled icwt imagenet. note icwt categories appear ilsvrc synsets larger imagenet dataset images synset used. fine-tuning performed -class categorization task. dataset conceived test possibility directly learn relation imagenet examples originally trained icwt images. imagenet synsets corresponding object categories cnns later trained tested invariance namely book ﬂower glass hairbrush hairclip. fine-tuning performed -class categorization task. dataset allows network focus data available on-line task hand diﬀerent images robot observe. fig. reports accuracy rlsc classiﬁers trained features extracted cnns ﬁne-tuned datasets described above. training performed separately transformation similarly previous section dataset containing example images object instance. noticed pre-ﬁne-tuning identiﬁcation task particularly advantageous leading dramatic improvement model trained oﬀ-the-shelf features. particular comparing results fig. trained examples object performs oﬀ-the-shelf method trained images object. moreover performance general stable across different viewpoint transformations suggesting preliminary ﬁne-tuning could indeed allowed cnns become partially invariant transformations icubworld. interestingly cnns provide similar improvements identiﬁcation cases even worse performance oﬀthe-shelf based classiﬁer. conclude note related categorization. indeed following experiments could wonder whether similar pre-ﬁne-tuning strategy could improve invariance also categorization setting sec. however empirical evidence showed performing ﬁne-tuning datasets described above performance cnns change signiﬁcantly categorization task. refer supplementary material analysis speciﬁc setting. hypothesize possible reasons networks trained ilsvrc already highly optimized categorization gain adapting visual representation negative eﬀect limited semantic variability icubworld respect imagenet overcome potential beneﬁts increasing invariance viewpoint transformations. motivates proposed directions future investigation discuss following section. work studied application modern deep learning methods visual recognition tasks robotics. challenged deep learning methods object recognition task speciﬁcally designed represent prototypical visual recognition problem real robotics application. experiments show deep learning leads remarkable performance visual tasks object classiﬁcation instance recognition. proper adoption knowledge transfer strategies particular mixing deep learning conventional shallow classiﬁers based kernel methods plays fundamental role leverage visual representation learned large-scale datasets achieve high performance robotic domain. however substantial still exists performance obtained domains. analysis shows performance scarce semantic variability characterizes robotic domain intrinsic cost acquiring training samples. adoption robotic systems real applications requires push requirements achieve performance approaches indeed real-world robotics applications failures object mis-detection mis-classiﬁcation potentially lead dramatically critical harmful consequences standard image retrieval scenarios. therefore error rate systems need close possible zero considered production deployment environments shared humans. section consider possible directions future research aimed mitigating impact diﬀerences performing visual recognition robotics. goal discussion consider together picture present opinion robot vision problem could addressed. improving invariance. experiments sec. show models tested work mostly locally invariant namely representation robust small view-point changes. local invariance main interest computer vision past current research invariance instead focused designing learning systems able learn encode representations robust dramatic transformations object clearly robust visual representation viewpoint changes small deformations background variations etc. less corresponding model trained oﬀ-line large-scale dataset imagenet biased speciﬁc training conditions. research invariant representations could therefore lead robust oﬀ-the-shelf robotics systems need knowledge transfer also allow train on-the-ﬂy classiﬁer trained examples novel object reliably recognize different poses. multi-task learning. work followed typical approach categorization similarity relation assumed among diﬀerent classes. however visual recognition settings object categories often number visual features common incorporating information similarities across object classes within learning model could signiﬁcantly improve overaccuracy recognition system especially presence scarce training data. particular literature multi-task learning proposed several approaches model relations across tasks directly enforce learning problem available a-priori learn unknown augmenting semantic variability. simplest method increasing semantic variability share data acquired parallel diﬀerent robotic systems. indeed similar strategy used learn hand-eye coordination robotic setting training networks using data acquired parallel several robots similarly million object challenge aims acquire large dataset object models sharing data acquired diﬀerent laboratories owning baxter robot. along similar direction plan extend icubworld help community icub robot beyond expanding dataset direction semantic variability would also allow collect multiple acquisitions object diﬀerent conditions order extend analysis presented work nuisances like changes light setting. indeed another critical aspect started consider supplementary material work. alternative complementary approach data augmentation. method often adopted image retrieval settings increase viewpoint variability training applying synthetic transformations image dataset visual augmentation cope semantic variability much challenging problem although recent work inverse graphics starting point direction. exploiting contextual information. work visual recognition considers frames independently rarely uses contextual information. robot typically exposed continuous stream visual information information highly correlated. exploiting correlations done many ways ranging trivial solutions complex ones rely scene reconstruction object tracking integrating information. robotics problem integrating informationwith recently investigated within deep learning framework results although promising leverage oﬀ-the-shelf representations learned large-scaled datasets rgb-only images. future challenge determine best encode process information within also acquire large-scale datasets directly training rgbdbased systems. self-supervised learning. reduce cost supervision implement explorative strategies robot autonomously interacts environment collect training samples. training instances scenario could extracted autonomously detecting invariances data correspond physical entities bottom-up salency cues). strategies speciﬁc robotic domain could devised integrating multiple sensory modalities repertoire explorative paper described systematic study beneﬁts deep learning methods robot vision. tests devised prototypical vision task humanoid robot human-robot interaction exploited obtain realistic supervision train object recognition system. presented icwt dataset in-depth investigation performance state-of-the-art deep learning methods applied task. results conﬁrm deep learning remarkable step forward. however still needs done reach level robustness reliability required real applications. identiﬁed speciﬁc challenges possible directions research bridge gap. conﬁdent next years rich exciting progress robotics. acknowledgements work described paper supported center brains minds machines funded award ccf-; firb project rbfrmac funded italian ministry education university research. gratefully acknowledge nvidia corporation donation tesla used research.", "year": 2017}