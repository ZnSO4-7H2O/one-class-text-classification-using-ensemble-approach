{"title": "Off-policy evaluation for slate recommendation", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context---a common scenario in web search, ads, and recommendation. We build on techniques from combinatorial bandits to introduce a new practical estimator that uses logged data to estimate a policy's performance. A thorough empirical evaluation on real-world data reveals that our estimator is accurate in a variety of settings, including as a subroutine in a learning-to-rank task, where it achieves competitive performance. We derive conditions under which our estimator is unbiased---these conditions are weaker than prior heuristics for slate evaluation---and experimentally demonstrate a smaller bias than parametric approaches, even when these conditions are violated. Finally, our theory and experiments also show exponential savings in the amount of required data compared with general unbiased estimators.", "text": "paper studies evaluation policies recommend ordered items based context—a common scenario search recommendation. build techniques combinatorial bandits introduce practical estimator uses logged data estimate policy’s performance. thorough empirical evaluation real-world data reveals estimator accurate variety settings including subroutine learningto-rank task achieves competitive performance. derive conditions estimator unbiased—these conditions weaker prior heuristics slate evaluation—and experimentally demonstrate smaller bias parametric approaches even conditions violated. finally theory experiments also show exponential savings amount required data compared general unbiased estimators. recommendation systems e-commerce search news would like data collected operation test content-serving algorithms along metrics revenue number clicks task called off-policy evaluation. general approaches namely inverse propensity scores require unrealistically large amounts logged data evaluate whole-page metrics depend multiple recommended items happens showing ranked lists. challenge number possible lists combinatorially large. result policy evaluated likely choose different slates recorded logs time unless similar data-collection policy. challenge fundamental off-policy evaluation method works large slates needs make structural assumptions whole-page metric user behavior. previous work off-policy evaluation whole-page optimization improves probability match logging evaluation restricting attention small slate spaces introducing assumptions allow partial matches proposed observed slates assuming policies used logging evaluation similar another line work constructs parametric models slate quality approaches require less data large bias practice requires expensive trial-and-error cycle involving weeks-long tests develop policies paper figure off-policy evaluation whole-page user-satisfaction metrics proprietary search engine data. average rmse different estimators runs log-log scale. method achieves best performance moderate data sizes. unbiased method suffers high variance direct modeling metrics suffers high bias. onpolicy expensive choice deploying policy instance test. design method robust problems bias modest data requirements goal substantially shortening cycle accelerating policy development process. frame slate recommendation problem combinatorial generalization contextual bandits combinatorial contextual bandits context policy selects slate consisting component actions reward entire slate observed. search context search query augmented user proﬁle slate search results page consisting list retrieved documents example reward metrics page-level measures time-to-success ndcg measures user satisfaction. input receive contextual bandit data obtained logging policy goal estimate reward target policy. off-policy setup differs online learning contextual bandits goal adaptively maximize reward presence explore-exploit trade-off inspired work combinatorial linear bandits propose estimator makes weak assumption evaluated metric exponentially reducing data requirements comparison ips. speciﬁcally posit linearity assumption stating slate-level reward decomposes additively across actions action-level rewards observed. crucially action-level rewards allowed depend context require easily modeled features describing context. fact method completely agnostic representation contexts. make following contributions pseudoinverse estimator off-policy evaluation general-purpose estimator combinatorial bandit literature adapted off-policy evaluation. ranking items linearity assumption typically requires om/ε) samples achieve error ε—an exponential gain sample complexity ips. provide distribution-dependent bounds based overlap logging target policies. experiments real-world search ranking datasets strong performance estimator provides knowledge ﬁrst demonstration high-quality off-policy evaluation whole-page metrics comprehensively outperforming prior baselines off-policy optimization provide simple procedure learning rank using estimator impute action-level rewards context. allows direct optimization whole-page metrics pointwise approaches without requiring pointwise feedback. related work large state spaces typically studied online on-policy setting. works assume speciﬁc parametric models relating metrics features describing slate lead bias model inaccurate others posit linearity assumption assume semi-bandit feedback model rewards actions slate revealed much research focuses on-policy setting off-policy paradigm studied paper often preferred practice since might possible implement low-latency updates needed online learning might interested many different metrics require manual review trade-offs deploying policies. technical level estimator used online learning analysis tailored speciﬁc data collection policies used learner. contrast provide distribution-dependent bounds without assumptions logging target policy. combinatorial contextual bandits decision maker repeatedly interacts follows based context decision maker chooses slate consisting actions position called slot number slots actions position come space slate chosen allowed slates given context slate reward drawn distribution rewards context space inﬁnite actions ﬁnite. assume |aj| contexts deﬁne maxj maximum number actions slot. goal decision maker maximize reward. decision maker modeled stochastic policy speciﬁes conditional distribution value policy denoted deﬁned expected reward following example consider optimization news portal reward whole-page advertising revenue. context user proﬁle slate news-portal page slots corresponding news sections actions articles. valid slates cartesian example consider search ranking. context query along user proﬁle. actions correspond search items policy chooses items items context chosen corpus ﬁltering step allowed slates repetitions. number valid slates exponential mω). reward could negative time-to-success i.e. negative time taken user relevant item. off-policy setting access logged data collected using past policy called logging policy. off-policy evaluation task estimating value policy called target policy using logged data. off-policy optimization harder task ﬁnding policy achieves maximal reward. standard approaches off-policy evaluation. direct method uses logged data train model predicting expected reward given context slate. estimated direct method often biased mismatch model assumptions ground truth. second approach provably unbiased inverse propensity score estimator estimator re-weights logged data according ratios slate probabilities target logging policy. common variants estimator minimax optimal exponential variance unavoidable worst case. circumvent hardness positing assumption structure rewards. speciﬁcally assume slate-level reward unobserved action-level rewards depend context action position slate actions slate. formally consider slate indicator vectors whose components indexed pairs slots possible actions them. slate described indicator vector whose slate indicator vector viewed feature vector representing slate viewed context-speciﬁc weight vector. assumption refers fact value slate linear function feature representation. however note linear dependence allowed completely different across contexts make assumptions depends fact method even attempt accurately estimate agnostic form departure direct method parametric bandits. assumption rules interactions among different actions slate ability vary intrinsic rewards arbitrarily across contexts captures many common metrics information retrieval normalized discounted cumulative gain common metric ranking rel− relevance document query ndcg dcg/dcg maxs∈s ndcg takes values thus ndcg satisﬁes assumpaddition assumption also make standard assumption logging policy puts non-zero probability slates potentially chosen target policy. assumption also required otherwise unbiased off-policy evaluation impossible assumption off-policy evaluation problem satisﬁes absolute continuity assumption whenever probability pseudoinverse estimator using assumption apply techniques combinatorial bandit literature problem. particular estimator closely follows recipe cesa-bianchi lugosi albeit differences account off-policy contextual nature setup. assumption view recovery given context linear regression problem. covariates drawn according reward follows linear model conditional weight vector. thus write estimate es∼µer∼d using deﬁnition distribution triples estimate minimizer smallest moore-penrose pseudoinverse matrix note idealized estimator uses conditional expectations simplify notation rm×m denote covariance matrix write also introduce notation second term empirical estimate risi. thus regression estimator simply show call pseudoinverse estimator prove following unbiasedness property appendix proposition assumptions hold estimator ˆvpi unbiased i.e. expectation logged examples sampled i.i.d. prop. appendix note unlike divides probabilities whole slates estimator divides probabilities actions appearing individual slots. thus magnitude term outer summation whereas terms mω). example uniform logging). case shown unbiased assumptions overcomes deﬁciencies speciﬁc examples. derive ﬁnite-sample error bound based overlap bernstein’s inequality deﬁne variance range terms observe ﬁnite sample bound structurally different regret bounds studied prior works combinatorial bandits. bound incorporates extent overlap higher conﬁdence estimates logging evaluation policies similar—an important consideration off-policy evaluation. bound might look complicated simpliﬁes consider class ε-uniform logging policies. formally policy deﬁne uniform distribution suitably small logging policies widely used practice. following corollary policies proved appendix corollary settings example example logging done ˆvpi empirically evaluate performance pseudoinverse estimator ranking problems. ﬁrst show outperforms prior works comprehensive semi-synthetic study using public dataset. estimator off-policy optimization i.e. learn ranking policies competitively supervised learning uses information. finally demonstrate substantial improvements proprietary data search engine logs user-satisfaction metrics used practice timeto-success utility rate satisfy linearity assumption. detailed results deferred appendices code available online. semi-synthetic evaluation uses labeled data microsoft learning rank challenge dataset create contextual bandit instance. queries form contexts actions available documents. dataset contains queries judged documents query-document pairs judged -point scale pair feature vector partitioned title body features consider slate rewards ndcg example expected reciprocal rank satisfy linearity deﬁned derive several distinct logging target policies ﬁrst train lasso regression models called lassotitle lassobody regression tree models called treetitle treebody predict relevances ftitle fbody respectively. create logs queries sampled uniformly consists documents according treetitle. logging policy parametrized model either treetitle lassotitle scalar samples multinomial distribution documents −αlog rank rank rank document query according corresponding model. slates constructed slot-by-slot sampling without replacement according varying interpolates uniformly random deterministic logging. thus logging policies based models derived ftitle. consider deterministic target policies based models derived fbody i.e. treebody lassobody select documents according corresponding model. four base models fairly distinct average fewer documents overlap among figure rmse various estimators four experimental conditions middle normalized rmse samples; plot aggregates logging-target combinations; closer top-left better. bottom middle samples. compare weighted estimator direct method weighted implement variants regression trees lasso trained ﬁrst examples using remaining examples evaluation according also include aspirational baseline onpolicy corresponds deploying target policy test returning average observed rewards. expensive alternative wish avoid. evaluate estimators recording root mean square error function number samples averaged least independent runs. different experimental conditions considering reward metrics slate-space sizes combinations target logging policies fig. shows results four representative conditions middle bottom rows aggregate across conditions. produce aggregates shift rescale rmse methods samples best performance worst aggregate plots display cumulative distribution function normalized rmse values across target-logging combinations keeping metric slate-space size ﬁxed. pseudoinverse estimator easily dominates wips across experimental conditions seen fig. appendix wips unbiased even without linearity assumption suffer large variance caused slate size. variance hence mean square error wips grows exponentially slate size perform poorly beyond smallest slate sizes. performs well cases especially samples often plateaus degrades eventually overﬁts logging distribution different target. always outperform methods method works robustly across conditions seen aggregate plots. general choosing largely matter bias-variance tradeoff. particularly good small data sizes variance settings often best choice. however performs comprehensively better given enough data fig. that expected biased metric since satisfy linearity. right panels also demonstrate effect varying deteriorates somewhat larger slate space still gives meaningful estimate. contrast wips fails produce meaningful estimate larger slate space rmse barely improves data. finally left plots show fairly insensitive amount stochasticity logging whereas improves overlap logging target. show pseudoinverse estimator off-policy optimization. leverage pointwise learning rank algorithms learn scoring function query-document pairs ﬁtting relevance labels. call supervised approach requires relevance labels. instead requiring relevance labels pseudoinverse estimator convert page-level reward per-slot reward components—the estimates φx—and become targets regression. thus pseudoinverse estimator enables pointwise optimize whole-page metrics even without relevance labels. given contextual bandit dataset {}i≤n collected logging policy begin creating estimates turning i-th contextual bandit example regression examples. trained regression model used create slate starting highest scoring slot-action pair continuing greedily procedure detailed appendix note without linearity assumptions imputed regression targets might lead best possible learned policy still expect adapt somewhat slate-level metric. mslr-webk dataset compare approach benchmarked results ndcg dataset contains queries relevance judgments judged documents query. state-of-the-art listwise method dataset highly tuned variant lambdamart provided -fold split always train bandit data collected uniform logging four folds evaluating supervised data ﬁfth. compare approach titled computed using annotated relevance judgements training folds pi-opt train gradient boosted regression trees additionally also experimented metric. average test-set performance across -folds reported table method pi-opt competitive supervised baseline ndcg substantially superior err. different transformation instead gains might yield stronger supervised baseline illustrates beneﬁt pi-opt right pointwise targets automatically inferred whole-page metric. pi-opt slightly worse lambdamart ndcg arguably highly tuned pi-opt uses slate-level metric. table comparison approaches optimizing ndcg err. lambdamart tuned list-wise approach. pi-opt pointwise learner; uses relevance judgments pi-opt uses samples page-level rewards. dataset differs dataset mslr-webk used sec. goal study realistic problem dimensions e.g. constructing length- rankings candidates. here mslrwebk largest dataset public benchmark numbers state-of-the-art approaches size small pre-ﬁltered documents size preprocessing unique queries total examples meaning query logged impressions many available slates. before create logs sampling queries uniformly random using logging policy samples uniformly slates shown query. consider page-level metrics time-to-success utilityrate. measures number seconds presenting results ﬁrst satisﬁed click user deﬁned click user stays linked page sufﬁciently long. value capped scaled utilityrate complex page-level metric user satisfaction. captures interaction user page timeline events durations. events classiﬁed revealing positive negative utility user contribution proportional duration. utilityrate takes values evaluate target policy based logistic regression classiﬁer trained predict clicks using predicted probabilities score slates. restrict target policy pick among slates logs know ground truth slate-level reward. since know query distribution calculate target policy’s value exactly measure rmse relative true value. compare estimator three baselines similar sec. onpolicy. uses regression trees roughly slate-level features. fig. introduction shows provides consistent multiplicative improvement rmse suffers high variance. starting moderate sample sizes also outperforms suffers substantial bias. paper introduced estimator off-policy evaluation combinatorial contextual bandits linearity assumption slate-level rewards. theoretical empirical analysis demonstrates merits approach. empirical results show favorable bias-variance tradeoff. even datasets metrics assumptions violated estimator typically outperforms baselines. performance especially smaller sample sizes could improved designing doubly-robust variants possibly also incorporating weight clipping promising approach relax assumption posit decomposition pairs slots capture higher-order interactions diversity. generally could replace slate spaces arbitrary compact convex sets done linear bandits. settings pseudoinverse estimator could still applied tight sample-complexity analysis open future research. léon bottou jonas peters joaquin quiñonero-candela denis charles chickering elon portugaly dipankar patrice simard snelson. counterfactual reasoning learning systems example computational advertising. journal machine learning research georges dupret benjamin piwowarski. user browsing model predict search engine click data past observations. sigir conference research development information retrieval lihong john langford xuanhui wang. unbiased ofﬂine evaluation contextual-banditbased news article recommendation algorithms. international conference search data mining wang dawei pengyuan wang makoto yamada chang qiaozhu mei. beyond ranking optimizing whole-page presentation. international conference search data mining pages fact quantity centered directly prop. must compute second moment range apply bernstein’s inequality. independence focus term drop subscript first bound variance section show target policy coincides logging i.e. bound theorem independent number actions slots. indeed claim estimator actually simpliﬁes taking empirical average rewards bounded proving claim prove supporting claim claim policy context proof. simplify exposition write instead verbose γµx. bulk proof deriving explicit expression begin expressing suitable basis. since matrix second moments vector ﬁrst moments matrix written last step follows fact uλut uλ−ut ﬁnish proof study structure first denote block corresponding slot. entry corresponds probability since values conditionally independent conditioned covariance matrix takes form covariance matrix quantity viewed norm non-zero probability. used bound thus bound error ˆvpi proposition logging policy target policy absolutely continuous respect next derive bounds ¯ρµx uniformly-random policies ranking example. prove translation theorem allows translating bound uniform distributions bound ε-uniform distributions. finally results together prove corollary vector all-ones actions j-th position zeros elsewhere. similarly vector all-ones action positions zeros elsewhere. finally all-ones vector. also diag denote diagonal matrix all-ones actions j-th position zeros elsewhere. proposition consider ranking setting slates without repetitions legal. denote uniform logging policy slates. ¯ρνx next would like argue eigendecomposition. this need show three kronecker products equals projection matrix ranges projection matrices orthogonal. ﬁrst property follows projection matrices second property follows either ranges orthogonal ranges orthogonal obtain ranges ready derive pseudo-inverse. distinguish cases. ranking slate space using prop. also ¯ρνx product slate space ranking slate space obtain ¯ρµεx om/ε). finally plugging upper bound prop. statement theorem completes proof. proof claim square root matrix i.e. symmetric positive semideﬁnite matrix eigenvectors eigenvalues square root corresponding eigenvalues similarly square root matrix thus u†u† similarly denote projection onto range denote projection onto range since null null range range. prove claim follows substitute u†u† also fact range range. obtained substituting relaxing maximization substitute v†v† fact note sufﬁces consider null null null. fact bijection range orthogonal complement null substitute finally substitute fact range. experimented several conﬁgurations slate spaces logging target policies wholepage metrics semi-synthetic evaluation setup. section details plots conﬁgurations. parameters were metric ndcg err. ndcg satisﬁes linearity assumption not. slate space logging policy unif lassotitle treetitle target policy lassobody treebody temperature uniform slightly peaked peaked. uniform corresponds small slate spaces creates slightly peaked logging distribution creates severely peaked logging distribution. larger slate spaces moderately peaked severely peaked. off-policy optimization experiments compare methods pi-opt mslr-webk dataset. pi-opt uses features query-document pairs training fold. uses regression targets outlined section also experimented regression relevance judgments denoted sup-rel. pi-opt query-document-position triplet produces regression example concatenated feature vector -dimensional one-hot encoding position every logged sample query yields estimate every candidate document position natural regression targets. optimization computationally tractable queries ﬁnite. averaging estimated particular query create lower-variance target regression remains unbiased estimate pi-opt employ gradient boosted regression trees tree-ensembles leaves tree predict corresponding regression targets. trained model constructs slates straightforward input query score candidate documents using trained model score sort scores descending order. rankings constructed using top- scoring candidates order. pi-opt score every document-position pair {··· score. greedily pick highest scored pair insert document slot slate. eliminating invalid four different base-rankers lassotitle lassobody treetitle treebody semi-synthetic experiments instantiate logging target policies. table report similar top- rankings retrieved rankers are. report metrics every pair rankers average fraction documents retrieved common rankers kendall’s computed union documents retrieved either ranker ranking). table reporting difference base-rankers lassotitle lassobody treetitle treebody measured average overlap retrieved document sets kendall’s tau.", "year": 2016}