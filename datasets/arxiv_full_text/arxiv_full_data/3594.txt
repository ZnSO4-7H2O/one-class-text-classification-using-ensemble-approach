{"title": "Stochastic Deconvolutional Neural Network Ensemble Training on  Generative Pseudo-Adversarial Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "The training of Generative Adversarial Networks is a difficult task mainly due to the nature of the networks. One such issue is when the generator and discriminator start oscillating, rather than converging to a fixed point. Another case can be when one agent becomes more adept than the other which results in the decrease of the other agent's ability to learn, reducing the learning capacity of the system as a whole. Additionally, there exists the problem of Mode Collapse which involves the generators output collapsing to a single sample or a small set of similar samples. To train GANs a careful selection of the architecture that is used along with a variety of other methods to improve training. Even when applying these methods there is low stability of training in relation to the parameters that are chosen. Stochastic ensembling is suggested as a method for improving the stability while training GANs.", "text": "alexey chaplygin joshua chacksfield data science research group department analytics business intelligence corp. {alexey.chapligin chacksfieldj}gmail.com training generative adversarial networks difficult task mainly nature networks. issue generator discriminator start oscillating rather converging fixed point. another case agent becomes adept results decrease agent’s ability learn reducing learning capacity system whole. additionally exists problem ‘mode collapse’ involves generators output collapsing single sample small similar samples. train gans careful selection architecture used along variety methods improve training. even applying methods stability training relation parameters chosen. stochastic ensembling suggested method improving stability training gans. introduction deep networks made great advances area generative modes. advances result wide range training losses architectures including limited generative adversarial networks deep generative models trained models. used solve minimax ‘game’ generator sampling data discriminator classifying data real generated. theory models capable modeling probability distribution. ability train flexible generating functions made gans extremely successful image generation practice however gans suffer many issues particularly training. common failure mode involves generator collapsing produce single sample small family similar samples. another generator discriminator oscillating training rather converging fixed point. addition agent becomes much powerful other learning signal agent becomes useless system learn. attempts made minimise mode-collapse problem improve variety output however solutions computationally expensive treated mode-collapse problem symptomatically. assumption behind methodology described later architecture typical causes mode-collapse occur. discriminator portion network constantly requires samples generator defined never reaches state output satisfactory. turn results possible ways model evolve. firstly case overly powerful network start oscillating. slightest modification parameters result significantly different outputs discriminator cannot remember. situation output differs epoch epoch cost local variety inside epoch. call scenario soft-collapse model. however weaker oscillation scenario cannot occur. instance situation called hard-collapse manifest. small number attempts significantly modify output oscillation mode fails. discriminator becomes absolute certain believe mode-collapse unavoidable situation another synthetic solving issue suggested. proposed simple idea stochastic ensembling described random shuffling filters deep levels generator. comparable creating weak generators still suffer modecollapse problem still produce acceptable output variety. efficiency demonstrated another pseudo-gan role discriminator played pre-trained image classifier. seen state absolute mode-collapse beginning training. apart stochastic ensembling architecture rest built standard approach usage weight normalization parametric relu additions. training architecture standard cgan loss used", "year": 2018}