{"title": "The Neural Noisy Channel", "tag": ["cs.CL", "cs.AI", "cs.NE"], "abstract": "We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.", "text": "phil blunsom chris dyer edward grefenstette tom´aˇs koˇcisk´y university oxford deepmind lei.yucs.ox.ac.uk {pblunsomcdyeretgtkocisky}google.com formulate sequence sequence transduction noisy channel decoding problem recurrent neural networks parameterise source channel models. unlike direct models suffer explaining-away effects during training noisy channel models must produce outputs explain inputs component models trained paired training samples also unpaired samples marginal output distribution. using latent variable control much conditioning sequence channel model needs read order generate subsequent symbol obtain tractable effective beam search decoder. experimental results abstractive sentence summarisation morphological inﬂection machine translation show noisy channel models outperform direct models signiﬁcantly beneﬁt increased amounts unpaired output data direct models cannot easily use. recurrent neural network sequence sequence models excellent models provided sufﬁcient input–output pairs available estimating parameters. however many domains vastly unpaired output examples available input–output pairs classic strategy exploiting kinds data bayes’ rule rewrite pp/p factorisation called noisy channel model noisy channel model thus consists component models conditional channel model characterizes reverse transduction problem whose parameters estimated paired samples unconditional source model whose parameters estimated paired unpaired samples. beyond data omnivorousness noisy channel models beneﬁts. first component models mean different aspects transduction problem addressed independently. example many applications source models language models innovations leveraged obtain improvements system uses component. second component models complementary strengths since inference carried product space; simpliﬁes design single model everything perfectly right. third noisy channel operates selecting outputs priori likely explain input well. addresses failure mode occur conditional models inputs explained away highly predictive output preﬁxes resulting poor training since noisy channel formulation requires outputs explain observed input problem avoided. principle noisy channel decomposition straightforward; however practice decoding signiﬁcant computational challenge tractability concerns impose restrictions form component models take. illustrate appealing parameterization would attentional seqseq network model channel probability however seqseq models designed assumption complete conditioning sequence available preﬁx probabilities output sequence computed. assumption problematic channel models since means complete output sequence must constructed channel model evaluated therefore practical channel probability must decompose terms preﬁxes conditioning variable chain rule justiﬁes decomposing output variable probabilities terms successive extensions partial preﬁx convenience exists conditioning variables approximations must introduced. work variant newly proposed online seqseq model uses latent alignment variable enable probabilities factorize terms preﬁxes input output making appropriate channel model using channel model decoding problem becomes similar problem faced decoding direct models experiments abstractive summarization machine translation morphological inﬂection show noisy channel signiﬁcantly improve performance exploit unpaired output training samples models combine direct model noisy channel model offer improvements still model based segment segment neural transduction model high level model alternates encoding input sequence decoding output tokens encoded representation. presentation deviates al.’s presentation emphasize incremental construction conditioning context enabled latent variable. avoid observe complete input sequence making prediction beginning output sequence introduce latent alignment variable indicates token output sequence generated input sequence read. since assume input read left right restrict monotonically increasing alignment denotes output token position generated input sequence position read. ssnt model explain model terms components starting word generation term. ssnt input output sequences encoded separate lstms resulting sequences hidden states representing preﬁxes sequences. al.’s formulation input sequence encoder either unidirectional bidirectional lstm assume unidirectional lstm ensures function well channel model compute probabilities incomplete conditioning contexts represent input sequence encoding preﬁx since ﬁnal action timestep predict convenient denote hidden state excludes i.e. encoding preﬁx discuss sequence generated. first remark modelling distribution requires care avoid conditioning entire input sequence. illustrate might induce dependency entire input sequence model useful compare standard attention model. attention models operate computing score using representation alignment candidate followed strategy would necessary observe full input sequence making ﬁrst alignment decision. instead model alignment transition timestep decomposing sequence conditionally independent shift emit operations progressively decide whether read another token stop reading. input position model decides emit i.e. predict next output token word model decides shift i.e. read input token increment input position probability formulation probabilities aligning alignment candidate computed reading probabilities also independent contents sufﬁx x|x| ssnt probability generating depends current output position’s alignzj ment current output preﬁx depend history alignment decisions. likewise alignment decisions position also conditionally independent history alignment decisions. independence assumptions marginalised using time dynamic programming algorithm chart computing following marginal probabilities gradients objective respect component probability models computed using automatic differentiation using secondary dynamic program computes ‘backward’ probabilities. refer reader section details. paper slightly different objective described rather require full input consumed ﬁnal output symbol generated. constraint biases away predicting outputs without explaining using input sequence. using ssnt model described previous section channel model language model delivers prior probabilities output sequence left-to-right order i.e. however even simpliﬁcation search problem remains nontrivial. hand must search space possible outputs model makes markovian assumptions. similar decoding problem faced standard seqseq transducers. hand model computes probability given input conditional predicted output hypothesis. therefore instead relying single softmax provide probability every output word type must loop output word type softmax input vocabulary—a computational expense quadratic size vocabulary reduce computational effort make auxiliary direct model explore probable extensions partial hypotheses rather trying perform exhaustive search vocabulary time extend item beam. algorithm appendix describes decoding algorithm based formulation tillmann idea create matrix partial hypotheses. hypothesis cell covers ﬁrst words input corresponds output hypothesis preﬁx length hypothesis associated model score. cell direct proposal model ﬁrst calculates scores possible extensions previous cells could reach considering every token output vocabulary previous candidate cells gives partial output sequences. partial output sequences subsequently rescored noisy channel model best candidates kept beam used extension. beam size hyperparameters tuned experiments. decoder described makes auxiliary decoding model. means that generalisation capable decoding objective linear combination direct model channel model language model bias output length evaluate model three natural language processing tasks abstractive sentence summarisation machine translation morphological inﬂection generation. task compare performance direct model noisy channel model interpolation models. experiments marginalize probability direct model calculating general search objective. found marginalizing probability give better performance makes decoding extremely slow. channel bias channel bias direct channel bias direct channel bias channel bias channel bias direct bias direct channel bias direct channel bias chanel bias direct bias direct channel bias table rouge scores sentence summarisation test set. ‘uni’ ‘bi’ parentheses denote encoder model proposing candidates unidirectional lstm bidirectional lstm. rows marked denote models process input online. sentence summarisation problem constructing shortened version sentence preserving majority meaning. contrast extractive summarisation copy words original sentence abstractive summarisation permits arbitrary rewording sentence. dataset constructed pairing ﬁrst sentence headline article annotated gigaword corpus sentence pairs training validation test sets respectively. ﬁltered dataset restricting lengths input output sentences greater tokens respectively. ﬁltered data sampled million sentence pairs training. experimented training direct model channel model sampled million full million parallel data. language model trained target side parallel data i.e. headlines. evaluated generated summaries randomly sampled sentence pairs using full length rouge setup line previous work task conﬁguration used train direct model channel model. loss optimized adam initial learning rate lstms layer encoder decoders hidden units mini-batch size dropout applied input output lstms. language model -layer lstm hidden units dropout. learning rate hyperparameters optimised grid search perplexity validation set. decoding beam search employed number proposals generated direct model number best candidates selected noisy channel model table presents rouge-f scores test direct model noisy channel model interpolation direct model noisy channel model interpolation direct model language model trained different sizes data. noisy channel model language model trained target side million parallel data outperforms direct model approximately point. improvement indicates language model helps improve quality output sequence extra unlabelled data available. training language model headlines dataset i.e. million sentences gives boost rouge score. line expectation model beneﬁts adding large amounts unlabelled data. interpolation direct model channel model language model bias output length achieves best results rouge score close direct model trained parallel table overview results abstractive sentence summarisation task. abs+ attentive model bag-of-words encoder. ras-lstm ras-elman sequence sequence models attention cell implemented lstms elman architecture respectively. pointing unknown words uses pointer networks select output token input sequence order avoid generating unknown tokens. semi-supervised model based variational autoencoder. data. although still improvement direct model trained data direct model noisy channel model smaller. gains observed language model combined direct model. increase weight language model result getting worse. table surveys published results task places best models context current state-of-the-art results. abs+ ras-lstm ras-elman different variations attentive models. pointing unkown words uses pointer networks select output token input sequence order avoid generating unknown tokens. semi-supervised model based variational autoencoder. trained paired samples unpaired samples noisy channel achieves comparable better results models trained paired samples. compared miao blunsom whose models alternative strategy using unpaired data noisy channel signiﬁcantly effective versus rouge-. finally motivated qualitative observation noisy channel model outputs quite ﬂuent often used reformulations input rather strict compression carried human preference evaluation whose results summarised table conﬁrms noisy channel summaries strongly preferred direct model. next evaluate models chinese–english machine translation task. used parallel data sentence pairs monolingual data million english sentences training data preprocessed lowercasing english sentences replacing digits token replacing tokens appearing models trained using adam initial learning rate direct model channel model language model. lstms direct channel models hidden units layer layers hidden units layer language model. dropout input output lstms model training. noisy channel decoding uses beam sizes. table lists translation performance different models bleu scores. benchmarks train vanilla attentional sequence sequence models using parallel data. direct models leverage bidirectional lstms encoder task. vanilla sequence sequence model behaves poorly small amounts parallel data. contrast direct model attentional model work relatively well attentional model outperforming ssnt direct model. although models directly model result unsurprising ssnt direct model effective alignment sequences largely monotonic chinese–english translation word orders diverge considerably. however despite limitation noisy channel model approximately points higher bleu direct model combination noisy channel direct model gives extra boost. conﬁrming empirical ﬁndings prior work interpolation direct model language model effective. morphological inﬂection task generating target word source word given morphological attribute e.g. number tense person etc.. useful reducing data sparsity issues translating morphologically rich languages. transformation base form inﬂected form usually preﬁx sufﬁx character replacement. dataset experiments created wiktionary including inﬂections german nouns german verbs spanish verbs finnish noun adjective finnish verbs. experimented german nouns german verbs german nouns difﬁcult task direct model perform well state-of-theart systems german verbs. train/dev/test split german nouns german verbs inﬂection types german nouns german verbs respectively. following previous work learn separate model type inﬂection independent inﬂections. report results average accuracy across different inﬂections. language models trained word types extracted running morphological analysis tool monolingual data extracting examples appropriately inﬂected word forms. annotation number instances training language model ranged different inﬂection types german nouns german verbs. figure accuracy morphological inﬂection german nouns german verbs ftnd previous state-of-the-art task based feature engineering ftnd based neural networks. nck+ ftnd+ semi-supervised setups models. table summarises results models. datasets noisy channel model perform well direct model interpolation direct model noisy channel model signiﬁcantly outperforms direct model. interpolation direct model language model achieves better results direct model noisy channel model german nouns german verbs. comparison also included state-of-the-art results benchmarks. tackles task based three-stage approach align source target word extract inﬂection rules apply rule examples. ftnd based neural sequence sequence models. models rerank candidate outputs scores predicted n-gram language models together features. observing output generated direct model noisy channel model direct model leave information. contrast noisy channel model seem avoid issue. illustrate example table direct model ignores phrase ‘coping with’ resulting incomplete meaning noisy channel model covers similarly example direct model translate chinese word corresponding ‘investigation’. also observe direct model mostly copies words source sentence noisy channel model prefers generating paraphrases. instance example direct model copies word ‘accelerate’ generated output noisy channel model generate ‘speed instead. might argue copying preferable compression technique paraphrasing show power models. noisy channel decompositions successfully used variety problems including speech recognition machine translation spelling correction question answering idea adding language models monolingual data machine translation explored earlier work. g¨ulc¸ehre propose strategies combining language model neural sequence sequence model. shallow fusion decoding sequence sequence model proposes candidate outputs candidates reranked based scores calculated weighted probability translation model language model. deep fusion language model integrated decoder sequence sequence model concatenating hidden state time step. sennrich incorporate target language unpaired training data back-translation create synthetic parallel training data. technique quite effective practicality seems limited problems inputs outputs contain roughly information cheng leverages abundant monolingual data multitask learning autoencoding objective. number papers remarked tendency content dropped translation. propose translating left-to-right left-to-right direction seeking consensus. propose augmenting direct model’s decoding objective reverse translation model however work reranks complete translation hypotheses rather developing model permits incremental search. direct model shares idea introducing stochastic latent variables neural networks several papers marginalising training. examples include connectionist temporal classiﬁcation recent segmental recurrent neural networks compared models direct model advantage capturing unbounded dependencies output words. direct model closely related sequence transduction model modeling probability predicting output tokens marginalizing latent variables using dynamic programming. however rather modeling joint distribution outputs alignments inserting null symbols output sequence direct model deﬁnes separate latent alignment variable alignment distribution deﬁned neural networks. similar work model decomposed alignment model model word predictions. models trained separately combined decoding subsequent reﬁnements using viterbi-em approximation. contrast direct channel models latent observed components models trained jointly using dynamic program exactly marginalise unobserved variables. presented empirically validated noisy channel transduction model uses component models based recurrent neural networks. formulation lets unpaired outputs estimate parameters source model input-output pairs train channel model. despite channel model’s ability condition long sequences able maintain tractable decoding using latent segmentation variable breaks conditioning context series monotonically growing segments. experiments show model makes excellent unpaired training data. tamer alkhouli gabriel bretschner jan-thorsten peter mohammed hethnawi andreas guta hermann ney. alignment-based neural machine translation. proc. machine translation peter brown stephen della pietra vincent della pietra robert. mercer. mathematics statistical machine translation parameter estimation. computational linguistics alex graves santiago fern´andez faustino gomez j¨urgen schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. icml alvin grissom jordan boyd-graber john morgan daum´e iii. don’t ﬁnal verb wait reinforcement learning simultaneous machine translation. empirical methods natural language processing aglar g¨ulc¸ehre orhan firat kelvin kyunghyun lo¨ıc barrault huei-chi fethi bougares holger schwenk yoshua bengio. using monolingual corpora neural machine translation. corr abs/. courtney napoles matthew gormley benjamin durme. annotated gigaword. proceedings joint workshop automatic knowledge base construction web-scale knowledge extraction notation viterbi matrix backpointer stores predicted tokens refers vocabulary jmax denotes maximum number output tokens predicted. input source sequence output best output sequence initialisation ri×jmax×k ni×jmax×k ni×jmax×k european commission health consumers protection −lrb− −rrb− offered cooperation indonesia coping spread avian inﬂuenza country ofﬁcial news agency antara said wednesday offers indonesia cooperation avian eradication offers cooperation indonesia avian offers cooperation indonesia coping bird vietnam accelerate export industrial goods mainly developing auxiliary industries helping enterprises sharpen competitive edges according ministry industry thursday vietnam boost industrial goods export vietnam accelerate export industrial goods vietnam speed export industrial goods japan toyota team europe banned world rally championship year friday crushing ruling world council international automobile federation -lrbﬁa toyota banned year toyota banned world rally championship toyota europe banned world rally championship year prices roared higher towards dollars monday equity markets surged government action aimed tackling severe economic downturn prices soar towards dollars prices jump towards dollars prices climb towards dollars well iran attitude quite issue takes even ﬁrmer attitude issue iran attitude quite hard attitude united states still tougher issue iran taken tougher attitude toward however attitude united states even harder", "year": 2016}