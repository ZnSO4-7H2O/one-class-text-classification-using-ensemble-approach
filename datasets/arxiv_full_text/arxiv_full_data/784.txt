{"title": "Lie Access Neural Turing Machine", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "Following the recent trend in explicit neural memory structures, we present a new design of an external memory, wherein memories are stored in an Euclidean key space $\\mathbb R^n$. An LSTM controller performs read and write via specialized read and write heads. It can move a head by either providing a new address in the key space (aka random access) or moving from its previous position via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\" instructions of a traditional Turing Machine are generalized to arbitrary elements of a fixed Lie group action. For this reason, we name this new model the Lie Access Neural Turing Machine, or LANTM.  We tested two different configurations of LANTM against an LSTM baseline in several basic experiments. We found the right configuration of LANTM to outperform the baseline in all of our experiments. In particular, we trained LANTM on addition of $k$-digit numbers for $2 \\le k \\le 16$, but it was able to generalize almost perfectly to $17 \\le k \\le 32$, all with the number of parameters 2 orders of magnitude below the LSTM baseline.", "text": "following recent trend explicit neural memory structures present design external memory wherein memories stored euclidean space lstm controller performs read write specialized read write heads. move head either providing address space moving previous position group action instructions traditional turing machine generalized arbitrary elements ﬁxed group action. reason name model access neural turing machine lantm. tested diﬀerent conﬁgurations lantm lstm baseline several basic experiments. found right conﬁguration lantm outperform baseline experiments. particular trained lantm addition k-digit numbers recurrent neural networks powerful devices that unlike conventional neural networks able keep state across time. achieved great results diverse ﬁelds like machine translation speech recognition image captioning many others. however despite advances traditional rnns still trouble maintaining memory long periods time presenting obstacle attaining human-like general intelligence. following pioneering work graves weston researchers studied many variations external memories equipped rnns explicit memory structures ameliorate problem discussed obtained great results applications like question answering algorithm learning machine translation others. paper propose variation external memory. conventional used personal computers memory stored integer addresses access either random sequential. replace integers retrieve memory controller either issue brand address drag previous address chosen direction former analog random access latter analog sequential access. call latter access meaning parametrized group speciﬁes dragging done. call model built around concept access access neural turing machine lantm. give speciﬁc implementations section explore section several experiments. refer implementations also lantms want stress certainly ways instantiating access concept. assume reader basic knowledge groups group actions passing notion groups groups diﬀerentiable operations. background enable understand rest paper section defer readers need slightly exposition topics appendix connections. mathematically function input space output space space internal states. input initial state transitions states returns sequence deﬁned recursively work particular variant called long short term memory lstm’s hidden state consists variables also output ﬁlls role description). memory external world called input forget output gates respectively modulate multiplicatively diﬀerent quantities computation. weights trainable backpropagation time undashed parts ﬁgure show schematic equations above. models external memories lstm often serves controller means entire system carries state time lstm external memory lstm controller collects reading computes additional instructions external memory lstm possibly performs extra processing return desired output time point. dashed parts ﬁgure demonstrate typical arrangement represents state memory represents reading memory represents subroutine used reading writing memory. entire system described figure lstm schematics without external memory. plain lstm illustrated undashed part diagram. lstm controller external memory illustrated including dashed parts. gate indicates concatenating inputs applying linear transformation given weights network. gate indicates splitting vector. processing produce ﬁnal output e.g. softmax produce distribution vocabulary. access neural turing machine inspired external memory architecture neural turing machine neural network controller reads writes memory structure specially designed diﬀerentiable functions called heads. heads trainable parameters learning done controller entire network trained gradient descent. lantm memory structure dictionary keys euclidean space ﬁxed called space address space; values another euclidean space ﬁxed time step read head retrieves reading converts instructions controller read address memory weighted inverse squared elaborated below. write head converts instructions controller memory vector address along scalar called memory strength vector. triple essentially appended memory. important hyperparameter lantm choice group acts time controller emit addresses head issue piece paper read write heads stones placed paper. controller hand moves stones turn turn. sometimes lift stone place somewhere completely unrelated original position times drag stone along chosen direction thus access generalizes sequential access conventional memory array continuous setting. figure retrieval value memory key. weightings unit assigned diﬀerent memories depending distances addresses read key. weighted arithmetic mean emitted ﬁnal read value. invnorm softmax schemes follow method diﬀerent computing weightings. particular softmax scheme requires another input temperature denote memory vectors stored space time choose canonical ordering example time added write vector order. denote corresponding addresses corresponding memory strength section introduce weight schemes retrieving value memory address. main idea summarized ﬁgure given ubiquity softmax machine learning literature consider natural choice weight scheme. seen experiments invnorm crucial making euclidean space work address space. extra ingredient writing adding produced memory vector strength address collection memory vectors strengths addresses. ensure memory selection weighted average works well squash values tanh squashing logistic sigmoid function also conceivable. without squashing memory vector large values dominate output weight method despite weight controller emits things candidate mixing coeﬃcient gate action also call step. gate mixes previous candidate produce pre-action transformed produce ﬁnal fact canonical interpolate common groups including groups mentioned above based exponential baker-campbell-hausdorﬀ formula details outside scope paper computational cost acceptable control theory settings hefty interested readers referred experiments experiments group types lantm translation group acting used action interpolation speciﬁed above. outline important experimental setup main text defer details appendix section tested variations lantm along baseline lstm encoder-decoder setup copy reverse bigram tasks done well double addition tasks designed similar vein. table shows input/output templates permutation task. digits left zero padding. double task takes integer padded integers padded digits interleaved forming length input sequence early experimented scaling rotation group produced acceptable results ﬂanked start-of-input symbol repetition end-of-input symbol machines output correct sequence response phase starts receive ﬁrst repetition eﬀectively means correct symbols must correctly emit end-of-output symbol terminate answers. figure usual prediction performed argmax training done minimizing negative likelihood. evaluate performance models compute fraction characters correctly predicted fraction answers completely correctly predicted respectively called score coarse score following task parameters hyperparameters. trained models tasks input sizes summarized table tasks lantm single-layer -cell -cell lstm controller. memory width tasks lstm baseline layers cells. appendix exact parameters model task listed table experimental details given section notice lstm orders magnitude parameters lantm models. results. lantm-invnorm able master tasks generalize nearly perfectly training sizes shown table lantm-softmax well copy double tasks failed others performed worse lstm baseline. baseline learned tasks smaller training input sizes almost ﬂawlessly generalization training size inadequate tasks coarse score exceeding tested learned invnorm model larger arbitrarily selected input sizes. results summarized table permutation tasks generalized quite well challenged times training size able test problems correct. double task extrapolation performance similar coarse score training size. notice lantm-invnorm several tasks achieved high scores extrapolating large input sizes despite coarse scores. suggests extrapolation errors systematically occur output tasks. table permutation arithemetic task results. indicates tested sequence length compared trained length. values rounded nearest integer percent. supplementary materials details explained appendix appendix look behaviors trained lantm-invnorm read write locations gate values example input/output analyze exactly learned extrapolation errors come challenged extreme input lengths. problem setting highly structured favors design lantm. task trained models generated python programs following natural. dataset comprises types programs integers addition/subtraction identity multiplication small operand small loops variable substitution ternary else statements illustrated table models required read input program terminates print statement output correct integer response reverse sequence without correct answer used teacher forcing). performed curriculum learning using mixed strategy starting digits operands digits operands. evaluated models coarse scores randomly sampled digit programs. training done rmsprop learning rate multiplied whenever validation accuracy became lower highest last four. results summarized table noted small loop programs diﬃcult program type models predicted less half characters correctly trained separate experiment small loop programs. results given table advantage lantm lstm dramatic. memory access lantm nearly orderly neat previous experiment rather erratic looking. interactive plot example read write locations state data lantm-invnorm learning small loops found supplementary materials. finally tested models penn treebank corpus. train predict continuously whenever external memories lantms memory vectors oldest vectors discarded. last experiment lstm baseline single layer cells lantm models also controllers size. addition lantm model memory size unrolled bptt steps trained adagrad learning rate halved time validation perplexity exceeded previous epoch. observed lantm-invnorm read write locations distant clusters read weights diﬀuse across entire memory. repeated application single action long course training blowing magnitude keys degrades random access typical squashing functions controller limits range keys produce. means that rather storing useful information particular locations machine stored deltas time step whole memory averaged together gave desired information. lantm-softmax also exhibited behavior high ﬁdelity access required read closer desired much keys cannot immediately infer also stored deltas. zaremba taught lstm evaluate simple python programs curriculum learning formed basis experiments. kalchbrenner arranged lstm cells multidimensional grid form grid long short term memory learned copy addition tasks well. graves created inspired much design work. zhang found several tweaks improve convergence performance. grefenstette designed smooth versions stack queue deque external memories lstm controller. unbounded memory experimental setups direct inﬂuences paper. zaremba used reinforcement learning absolve need involve entire memory memory retrieval. weston came upon similar ideas memory network around time less focus sequence learning question answering tasks sukhbaater improved results give memory network trainable gradient descent end-to-end allowing multiple adaptive memory queries help complex relational reasoning. dynamic memory network kumar added episodic memory module similar multiple hops feature sukhbaatar al.’s model dynamically chose stop accessing memory rather ﬁxed number times. achieved state results several tasks sequence modelling. danihelka designed external memory based holographic reduced representations store unlimited memory larger size noisy retrieval. kaiser created neural based convolutional kernels learned long multiplication binary numbers bits able generalize bits. kurach generalized random access conventional rams create neural random access machine learned simple algorithm able generalize larger lengths memory access inference done constant time. neelakantan investigated adding gradient noise training found many models mentioned above method improved performance allowed greater percentage random initializations converge optimum. want stress model explained implement access memory. indeed euclidean space could generalized riemannian manifold equipped subgroup isometry group notion metric required access memory wants ability store retrieve information straight line suggests action invariant respect metric potentially useful riemannian manifold hyperbolic space speciﬁcally poincare disk model seen language modelling task repeated application action blow magnitude keys degrading random access. poincare disk model points unit ball prevents problem occurring. standard riemannian model sphere quite desirable setting wraps around confuse gradient descent. paper introduced access memory explored diﬀerent implementations experiments. lantm model invnorm weight scheme tasks performed better baseline spectacularly sequence addition tasks learned generalize extraordinary lengths whereas softmax weight scheme failed outperform baseline reverse bigramflip addition language modelling tasks. lantm-invnorm held largest advantage lstms case long structured tasks. python program experiment shows less structured environments environments redundant useless information lantm designs could utilize memory impressively structure environments. thus work needs done toward combining logical reasoning natural language processing. adopted simple turn episodic nature unbounded memory continuous perfect. language modelling experiment lantm models seem memory remarkable way. future work explore diﬀerent options adapting access memory continuous tasks example bounding memory using poincare disk model underlying manifold suggested section", "year": 2016}