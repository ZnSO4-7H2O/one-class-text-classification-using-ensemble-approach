{"title": "Gossip training for deep learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We address the issue of speeding up the training of convolutional networks. Here we study a distributed method adapted to stochastic gradient descent (SGD). The parallel optimization setup uses several threads, each applying individual gradient descents on a local variable. We propose a new way to share information between different threads inspired by gossip algorithms and showing good consensus convergence properties. Our method called GoSGD has the advantage to be fully asynchronous and decentralized. We compared our method to the recent EASGD in \\cite{elastic} on CIFAR-10 show encouraging results.", "text": "address issue speeding training convolutional networks. study distributed method adapted stochastic gradient descent parallel optimization setup uses several threads applying individual gradient descents local variable. propose share information different threads inspired gossip algorithms showing good consensus convergence properties. method called gosgd advantage fully asynchronous decentralized. compared method recent easgd cifar- show encouraging results. deep convolutional neural networks introduced computer vision tasks speciﬁcally image classiﬁcation made huge improvements last years following performances beneﬁt databases annotated images like trained optimizing loss function gradient descents computed random mini-batches. method called stochastic gradient descent proved efﬁcient train neural networks general. however current structures deep like layers network resnet contains parameters making training datasets slow. computation accelerates training still difﬁcult test many architectures. nevertheless mini-batch optimization seems suitable distributing training. many methods proposed like process parallel different threads optimize local neural network. different threads called workers. additionally workers periodically exchange information central network. role central variable essential share spread information well ensuring worker networks converge toward local minimum. indeed symmetry property neural networks well studied different workers could give different optimizations. consensus important fully beneﬁt parallelism information sharing. unfortunately proposed methods decentralized resulting loss time synchronizing updates central network. could result suboptimal distributed computation. well known example decentralized distributed algorithm gossip averaging. studied method fast make different agents converge toward consensus exchanging information peer peer way. gossip averaging already adapted machine learning algorithms kernel methods family algorithm presents many advantages like fully asynchronous totally decentralized require central variable. propose associate method order apply deep learning speciﬁcally cnn. call resulting optimization method gosgd gossip stochastic gradient descent. ﬁrst section introduces gosgd algorithm. experiments illustrate good convergence properties decentralized gosgd. gossip stochastic gradient descent objective minimize couple variable following natural image distribution parameters loss function. used discussed problem derived distributed fashion minimizing gosgd algorithm considers independent agents called workers. hosts architecture sets weights noted worker initialized value. training workers iteratively proceed steps described below. consisting local optimisation gradient descent aiming exchanging information order ensure consensus workers step gradient descent worker draws random bernoulli variable noted expectancy variable decides worker sharing information another worker chosen uniformly among others. share information update processes sum-weight gossip protocol sum-weight protocols sharing variable associated worker updated whenever information exchanged deﬁnes rate information mixed. push nature synchronization required workers. exchange worker drawing successful worker described algorithm iteration worker sends weights receive weights several others. case worker updates weights sequentially reception order performing gradient update. since agent perform update without waiting answer performs update delayed fashion agent ever idling computing resources always used remark update rules equivalent common sum-weight gossip rules main difference choose scale results complex update rule consequently several properties sum-weight protocols retained remark adjustable parameter algorithm. obviously bigger exchanges threads eventually closer workers’ weights experiment already ensures good consensus. need consensus contributes optimization must close direction true gradient point best ensure property keep close possible. requirement holds workers iteration initial optimization problem thus coupled consensus problem. compare convergence speed gosgd easgd version easgd version momentum namely measgd parameters suggested author. parameter equivalent inverse parameters controls frequency exchange worker. experiments done cifar- detailed presentation. network used described loss. data sampling protocol training learning rate constant equal weight decay batch contain images. eight workers. adjustable parameter probability control frequency exchange. implemented methods torch framework titan gpu. report figure evolution training losses different methods. baseline displayed \"naive\" scheme corresponding train without exchange workers. train loss depicted averaging train loss last batches taken regardless workers. ﬁrst curve left figure shows loss number images processed worker. clarity zoomed convergence. study beneﬁts exchanging information regardless communication time maximize number exchanges setting gosgd better exchanges easgd. signify gossip strategy implies better consensus training. second graph represents evolution loss time hours. small seems give good compromise communication costs consensus gosgd easgd. gosgd faster easgd. strategy converging hours easgd needs hours reach train loss score. shows distributing beneﬁt asynchronous strategies. paper introduced learning scheme deep architectures based gossip gosgd. experimentally validated approach. algorithm disposes several advantages compared methods. first fully asynchronous decentralized avoiding kind idling exchanges pairwise beneﬁt faster communication channel cpi. second theoretical aspects interesting discuss possible derive consensus convergence rate many gossip algorithms. could useful extend study gosdg order measure sensibility gossip averaging additional gradients. would provide insights optimize frequency exchange control possible without impacting much consensus threads.", "year": 2016}