{"title": "SMS Spam Filtering using Probabilistic Topic Modelling and Stacked  Denoising Autoencoder", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "In This paper we present a novel approach to spam filtering and demonstrate its applicability with respect to SMS messages. Our approach requires minimum features engineering and a small set of la- belled data samples. Features are extracted using topic modelling based on latent Dirichlet allocation, and then a comprehensive data model is created using a Stacked Denoising Autoencoder (SDA). Topic modelling summarises the data providing ease of use and high interpretability by visualising the topics using word clouds. Given that the SMS messages can be regarded as either spam (unwanted) or ham (wanted), the SDA is able to model the messages and accurately discriminate between the two classes without the need for a pre-labelled training set. The results are compared against the state-of-the-art spam detection algorithms with our proposed approach achieving over 97% accuracy which compares favourably to the best reported algorithms presented in the literature.", "text": "abstract. paper present novel approach spam ﬁltering demonstrate applicability respect messages. approach requires minimum features engineering small labelled data samples. features extracted using topic modelling based latent dirichlet allocation comprehensive data model created using stacked denoising autoencoder topic modelling summarises data providing ease high interpretability visualising topics using word clouds. given messages regarded either spam able model messages accurately discriminate classes without need pre-labelled training set. results compared state-of-the-art spam detection algorithms proposed approach achieving accuracy compares favourably best reported algorithms presented literature. short messaging service applications widely used applications smart phones surveyed users report used least survey. people worldwide expected send trillion text messages alone large volume traﬃc opening opportunity spammers move email spamming prior research shown eﬀective approach spam ﬁltering perform threat analysis message content level. problem principle similar email spam ﬁltering however diﬀers mainly nature messaging itself capped characters. users normally write idiosyncratic language subset abbreviations spelling slang internet acronyms. despite ﬁlters standard feature extraction methods direct n-gram characterbased word-based tokenisation supervised unsupervised machine learning techniques commonly trained using collection labelled messages spam non-spam trained model used predict labels previously unseen messages. work recently developed text mining method probabilistic topic modelling extract hidden topics statistically related sms. topic modelling advantage handling seamlessly robustly text size topics generated used unsupervised deep learning approach stacked denoising auto-encoders build data model. novel onset detection approach based built model used increase separation spam ﬁnally fisher’s linear discriminate analysis used classify data spam ham. results achieved using approach comparable best reported literature. ﬁrst step machine learning based spam ﬁlter feature extraction/engineering. classiﬁer must eﬀectively utilise features discrimination spam ham. means unique problem spam ﬁltering however limited available text makes feature space sparse. means samples input space fewer apart thus signiﬁcantly reducing data classiﬁer work hidalgo suggested diﬀerent features including normalised words character bitri-grams word bi-grams. novel approach based stylometry i.e. statistical analysis linguistic style presented goal identifying spam message style messages written. review email spam ﬁltering reported words common feature used literature. however argue greatest disadvantage approach features ﬁxed updated data changes nature spam threat changes. extracted features tend high dimensional requiring sort feature selection dimensionality reduction techniques features extracted selected machine learning method trained classify available data spam ham. early work suggested supervised machine learning methods e.g. unsupervised methods e.g. k-nn hidalgo evaluated number spam ﬁltering methods concluded svms suitable classiﬁcation approaches. number spam samples dataset much smaller samples classiﬁer must take consideration otherwise serious risk over-ﬁtting model class address issue bayesian approach naive bayes based classiﬁer used approach penalises false positives ensuring balanced performance spam higher spam precision. feature selection method features sparse limited size selected features normally hard-coded system hence hard adapt emerging spam patterns. address issues opted probabilistic topic modelling text mining technique models latent patterns messages models latent patterns text. approach automatically identiﬁes topics within messages assigns message topics. approach requires maximum number topics set. messages distributed among small number topics minimising eﬀect sparsity. importantly topic modelling work adaptively. topic modelling also requires basic pre-processing steps tokenisation stop words removal. limited availability labelled training data unsupervised learning realistic approach real-life applications. unsupervised deep neural network stacked denoising autoencoders sdas usually pre-trained using unsupervised approach supervised method used ﬁne-tuning. approach utilise pre-trained stage reconstruction error data sample given model used surrogate measure well sample represented model hence exploited identify outliers topic modelling text mining tool identify latent text patterns documents contents handling large volumes corpuses regardless size individual documents. describes statistical terms words documents generated based pre-deﬁned number topics using statistical sampling technique. commonly used topic modelling method latent dirichlet allocation documents represented predeﬁned number topics topic hidden variable characterised nominal distribution ﬁxed dictionary. represents document mixture diﬀerent topics prior assumptions distribution. topic occur diﬀerent documents diﬀerent probability word occur several topics diﬀerent probabilities. complete description found vocabulary consisting words topics documents arbitrary length. every topic distribution sampled known probability distribution gibbs sampling normally used inference lda. estimates distribution denotes main advantage unsupervised deep learning utilisation previously considered useless masses unlabelled data easy obtain order achieve better understanding emerging patterns data. unsupervised deep learning capable extracting high level feature representations autoencoder consists visible input layer hidden layer. learning goes phases construct phase maps input data hidden layer reconstruct phase maps back hidden layer’s data input layer. model converges reconstruction error input output minimum. normally tied weights regularisation constrains parameter search space reduces number parameters learn also known weight matrix. constructed representation input deﬁned reconstructed representation avoid over-ﬁtting i.e. learning identify function reduce information redundancy input features denoising autoencoder stochastic version corrupts input data adding noise allowing variance input space hence better generalisation model. paper adopt masking noise corruption forcing fraction input layer units weight stacked denoising autoencoder deep version single output input following one. network trained layer layer. fig. illustrates architecture. arrows indicate direction information ﬂow. construction data ﬂows input layer hierarchy layer. reconstruction data ﬂows back hidden layers input layer reconstructed data compared input data overall reconstruction error calculated. reconstruction error measure well models presented sample input layer. high suggests poor modelling input sample small indication accurate representation input. among layers used unsupervised pre-training optimise model parameters. work utilise overall novel measure detecting outliers majority available data model accurately spam. words spam higher making easier discriminate sets using simple linear classiﬁers like first text content messages tokenised stop words removed. stemming applied data aﬀect interpretability topic modelling results. pre-processed text used build dictionary words passed generate topic model. contains wide range topics irrelevant discrimination spam ham. hence data labelled spam employed building topic model. maximum topics used. optimal value identiﬁed varying maximum number topics model built messages passed model producing -feature vector message feature probability message contains topic uses input layer units hidden layers units respectively. units sigmoid activation functions learning rate corruption rate learning algorithm runs epochs. learnt model used calculate message followed classiﬁcation. properly evaluate performance methods -fold cross validation approach used. fold training data used build topic model generate feature vectors training testing data. built using training features used train tested testing set. process repeated times average accuracies reported. major advantages topic modelling ability visualise topics interpret meaning using word cloud presentation. figure demonstrates word cloud distinct topics generated topic figure plots histogram ﬁtted gaussian probability density function spam. ﬁgure clearly shows high separability classes using principal component analysis approach fails. shows ability build model data resulting small spam data well resulting higher res. keep evaluation metrics reported literature also report overall cross validated classiﬁcation accuracy spam caught accuracy blocked accuracy mathews correlation coeﬃcient table presents results tm+sda along commonly used methods literature ordered mcc%. topic modelling proposed feature extraction method tackles several disadvantages state-of-the-art methods. modelling abstract topics responsible generating text within given message limited number features used eliminating need feature selection. model also reduces sparsity input space making easier classiﬁer decode data. model adaptive cope newly emerging data samples without need major redesign system. this along ease interpretability topic model approach oﬀers allows argue approach signiﬁcant advantage many application areas. presented unsupervised technique model extracted topic modelling features. demonstrated successfully separate spam using structure data alone without need labelling. novelty approach reconstruction errors produced increase separability spam. classiﬁer trained eﬀective classifying classes. accuracy achieved proposed system comparable best results reported literature although scores higher spam caught scores worse blocked.", "year": 2016}