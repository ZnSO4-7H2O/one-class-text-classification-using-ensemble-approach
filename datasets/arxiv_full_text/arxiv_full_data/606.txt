{"title": "Improved Variational Autoencoders for Text Modeling using Dilated  Convolutions", "tag": ["cs.NE", "cs.CL", "cs.LG"], "abstract": "Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.", "text": "recent work generative text modeling found variational autoencoders lstm decoders perform worse simpler lstm language models negative result poorly understood attributed propensity lstm decoders ignore conditioning information encoder. paper experiment type decoder dilated cnn. changing decoder’s dilation architecture control size context previously generated words. experiments trade-off between contextual capacity decoder effective encoding information. show carefully managed vaes outperform lstm language models. demonstrate perplexity gains datasets representing ﬁrst positive language modeling result vae. further conduct in-depth investigation semi-supervised unsupervised labeling tasks demonstrating gains several strong baselines. generative models play important role language models ability effectively learn unlabeled data. parameterzing generative models using neural nets recent work proposed model classes particularly expressive pontentially model wide range phenomena language modalities. focus speciﬁc instance generative story behind simple first continuous latent representation sampled multivariate gaussian. then output sampled distribution parameterized neural decoder conditioned latent representation. latent representation intended give model expressive capacity compared simpler neural generative models–for example conditional language models. choice decoding architecture ﬁnal output distribution connect latent representation output depends kind data modeled. owes name accompanying variational technique successfully used train models image data application vaes text data less successful obvious choice decoding architecture textual lstm typical workhorse nlp. however bowman found using lstm-vae text modeling yields higher perplexity held-out data using lstm language model. particular observe lstm decoder make effective latent representation training result collapses simple language model. related work used simpler decoders model text words. results indicate better latent representations decoders cannot effectively model longer-range dependencies text thus underperform terms ﬁnal perplexity. motivated observations hypothesize contextual capacity decoder plays important role whether vaes effectively condition latent representation trained text data. propose dilated decoder inspired recent success using cnns audio image language state-of-the-art language models often parametrize conditional probabilities using rnns compute evolving hidden state text used predict approach though effective modeling text explicitly model variance higher-level properties entire utterances thus difﬁculty heterogeneous datasets. bowman propose different approach generative text modeling inspired related work vision instead directly modeling joint probability equation specify generative process marginal distribution. speciﬁcally ﬁrst generate continuous latent vector representation multivariate gaussian prior generate text sequence conditional distribution parameterized using neural model incorporates latent variable modulates entire generation whole utterance better able capture high-level sources variation data. speciﬁcally contrast equation generating distribution conditions latent vector representation here approximation true posterior parameterized like decoder choice neural architecture parameterize encoder. however unlike decoder choice encoder change model class changes variational approximation used training function model parameters approximation parameters training seeks optimize parameters jointly using stochastic gradient ascent. ﬁnal wrinkle training procedure involves stochastic approximation gradients variational objective omit details here noting ﬁnal distribution posterior approximation typically assumed gaussian re-parametrization trick used refer readers modeling contrast prior work extremely large cnns used exploit dilated ﬂexibility varying amount conditioning context. extremes depending choice dilation decoder reproduce simple using bags words representation text reproduce long-range dependence recurrent architectures conditioning entire history. thus choosing dilated decoder able conduct experiments vary contextual capacity ﬁnding sweet spot decoder accurately model text overpower latent representation. demonstrate trade-off correctly managed textual vaes perform substantially better simple lstm language models ﬁnding consistent recent image modeling experiments using variational lossy autoencoders show vaes carefully selected decoders quite effective semi-supervised classiﬁcation unsupervised clustering outperforming several strong baselines text categorization sentiment analysis. contributions follows first propose dilated decoder vae. empirically evaluate several dilation architectures different capacities ﬁnding reduced contextual capacity leads stronger reliance latent representations. picking decoder suitable contextual capacity performs better lstm language models data sets. also explore dilated vaes semi-supervised classiﬁcation perform better strong baselines finally verify framework used effectively unsupervised clustering. section begin providing background variational autoencoders language modeling. introduce dilated architecture decoder experiments. finally describe generalization conduct experiments semi-supervised classiﬁcation. together combination generative model variational inference procedure often referred variational autoencoder also view regularized version autoencoder. note however vaes valid probabilistic models whose likelihood evaluated held-out data autoencoders valid models. ﬁrst term variational bound eqφ] used objective variance posterior probability become small training procedure rekl-divergence term duces autoencoder. kl||pθ) discourages memorizing single latent point. term critical training vaes historically instability text evidenced term becoming vanishingly small training observed bowman training procedure collapses result encoder duplicated gaussian prior decoder completely ignores latent variable learned model reduces simpler language model. hypothesize collapse condition related contextual capacity decoder architecture. choice encoder decoder depends type data. images typically mlps cnns. lstms used text resulted training collapse discussed here propose dilated decoder instead. extreme effective contextual width large resembles behavior lstm. width small behaves like bag-ofwords model. architectural ﬂexibility dilated cnns allows change contextual capacity conduct experiments validate hypothesis decoder contextual capacity effective encoding information directly related. next describe details decoder. typical approach using cnns used text generation similar used images convolution applied dimension. take approach deﬁning decoder. dimensional convolution serve decoder text generation must condition past tokens x<t. applying traditional convolution break assumption tokens inputs predict decoder avoid simply shifting input several slots convolution ﬁlter size using layers effective ﬁlter size would hence ﬁlter size would grow linearly depth network. dilation dilated convolution introduced greatly increase effective receptive ﬁeld size without increasing computational cost. dilation convolution applied inputs skipped step. causal convolution seen special case dilation effective receptive size grows exponentially network depth. figure show dilation sizes ﬁrst second layer respectively. suppose dilation size i-th layer ﬁlter size dilations typically double every layer effective receptive ﬁeld size grow exponentially. hence contextual capacity controlled across greater range manipulating ﬁlter size dilation size network depth. approach experiments. residual connection nection speed convergence enable training deeper models. residual block similar three convolutional layers ﬁlter size respectively relu activation begi follows gumbel. approximation accurate smooth experiments gumbel-softmax approximate samples reduce computational cost. result directly back propagate gradients discriminator network. anneal sample variance small training starts gradually decrease unsupervised clustering section adapt framework unsupervised clustering. directly minimize objective consisted parts reconstruction loss regularization ﬁrst part encourages model assign label reconstruction loss low. model easily stuck local optimum term small close uniform distribution term large samples collapse class. order make model robust modify term since would like investigate vaes language modeling semi-supervised classiﬁcation data sets suitable purposes. large scale document classiﬁcation data sets yahoo answer yelp review representing topic classiﬁcation sentiment classiﬁcation data sets respectively original data sets contain millions samples sample training validation test respective partitions. detailed statistics data sets table yahoo answer contains topics including society culture science mathematics etc. yelp contains level rating higher rating better. tween convolutional layers. overall architecture architecture shown figure lstm encoder posterior probability assume diagonal gaussian. parametrize mean variance lstm output. sample decoder conditioned sample concatenating every word embedding decoder input. addition conducting language modeling experiments also conduct experiments semi-supervised classiﬁcation text using proposed decoder. section brieﬂy review semi-supervised vaes incorporate discrete labels additional variables. given labeled unlabeled proposed model whose latent representation contains continuous vector discrete label semi-supervised discriminative network inference network generative network jointly part optimizing variational lower bound similar basic vae. labeled data bound controls trade generative discriminative terms. gumbel-softmax jang maddison propose continuous approximation sampling categorical distribution. categorical distribution probabilities samples model lstm-lm lstm-vae∗∗ lstm-vae∗∗ init scnn-lm scnn-vae scnn-vae init mcnn-lm mcnn-vae mcnn-vae init lcnn-lm lcnn-vae lcnn-vae init vlcnn-lm vlcnn-vae vlcnn-vae init model lstm-lm lstm-vae∗∗ lstm-vae∗∗ init scnn-lm scnn-vae scnn-vae init mcnn-lm mcnn-vae mcnn-vae init lcnn-lm lcnn-vae lcnn-vae init vlcnn-lm vlcnn-vae vlcnn-vae init table language modeling results test set. report negative likelihood perplexity test set. component given parentheses. size indicates effective ﬁlter size. init indicates pretraining encoder using lstm lstm encoder explore lstms cnns decoders. cnns explore several different conﬁgurations. convolution ﬁlter size gradually increase depth dilation represent small medium large model name scnn mcnn lcnn. also explore large model dilations name vlcnn. effective ﬁlter size respectively. last hidden state encoder lstm feed though mean variance sample feed starting state decoder. lstm decoder follow initial state lstm feed every step lstm. decoder concatenate word embedding every decoder input. architecture semi-supervised basically follows vae. feed last hidden state encoder lstm layer softmax gumbel-softmax sample concatenate last hidden state encoder lstm feed throught mean variance together used starting state decoder. adam optimize models learning rate selected selected empirically learning rate perform best. select drop ratio lstms following also drop word lstm decoder drop word ratio selected decoder drop ratio layer. drop word decoders. batch size model trained epochs. start half learning rate every epochs epoch following cost annealing strategy. initial weight cost term increase linearly given iteration treat hyper parameter select results language modeling shown table report negative likelihood perplexity test set. vaes decompose reconstruction loss divergence report divergence parenthesis. better visualize results plot results yahoo data figure figure decomposition table group consists three bars representing vae+init. decompose loss reconstruction loss divergence shown blue respectively. subtract loss values better visualization. ﬁrst look results yahoo data set. gradually increase effective ﬁlter size scnn mcnn lcnn decreases lcnn-lm close lstm-lm vlcnn-lm little worse lcnn-lm indicates little over-ﬁtting. lstm-vae worse lstm-lm terms term nearly zero veriﬁes ﬁnding cnns decoders vaes improvement pure lms. scnn mcnn lcnn results improve results respectively. improvement small models gradually decreases increase decoder model contextual capacity. model large vlcnn improvement diminishes result almost result. also reﬂected term scnn-vae largest vlcnn-vae smallest lcnn used decoder obtain optimal trade using contextual information latent representation. lcnn-vae achieves improves lstm-lm initialize parameters lstm encoder parameters lstm language model improve results further. indicates better encoder model also factor vaes work well. combined encoder initialization lcnn-vae improves lstm-lm ppl. similar results sentiment data shown table lcnn-vae improves lstm-lm ppl. latent representation visualization order visualize latent representation dimension plot mean posterior probability shown figure distinct different characteristics topic sentiment representation. figure documents different topics fall different clusters figure documents different ratings form continuum continuously xaxis review rating increases. motivated success vaes language modeling continue explore vaes semi-supervised learning. following number labeled samples respectively. ablation study ﬁrst would like explore effect different decoders semi-supervised classiﬁcation. number labeled samples report classiﬁcation accuracy test yahoo data table. scnn-vaesemi best classiﬁcation accuracy accuracy decreases gradually increase decoder contextual capacity. hand lcnn-vae-semi best result. classiﬁcation accuracy trade veriﬁes conjecture small contextual window size decoder forced encoder information hence latent representation better comparing results table table improves. semisupervised improves simple scnn mcnn lcnn. improvement mainly comes divergence part indicates better latent representations decrease divergence improving results. comparison related methods compare semisupervised methods represent previous state-of-the-art semisupervised sequence learning. pre-trains classiﬁer initializing parameters classiﬁer language model sequence autoencoder. improves classiﬁcation accuracy signiﬁcantly. since scnn-vae-semi performs best according table decoder scnn part. detailed comparison table semisupervised performs better lm-lstm lalstm also initialize encoder parameters classiﬁcation accuracy improves. also advantage scnn-vae-semi lm-lstm greater number labeled samples smaller. advantage decreases increase number labeled samples. number labeled samples scnn-vae-semi achieves accuracy similar lm-lstm accuracy also scnn-vae-semi performs better yahoo data yelp data set. yelp scnn-vae-semi little worse lm-lstm number labeled samples greater becomes better initialize encoder. figure explains observation. shows documents coupled together harder classify. also latent representation contains information sentiment useful classiﬁcation. table unsupervised clustering results yahoo data set. model times report best results. lstm+gmm means extract features lstm language model. scnn-vae means mean feature. scnn-vae init means scnn-vae trained encoder initialization. tract feature existing models gaussian mixture model features. empirically simply using features perform well since features high dimensional. features dimension selected since easily stuck poor local optimum model times report best result. directly optimizing perform well unsupervised clustering need initialize encoder lstm language model. model works well yahoo data set. potentially figure shows sentiment latent representations fall clusters. equation sensitive parameter select range interval following evaluation protocol ﬁnish training cluster validation sample cluster best assign label samples cluster compute test accuracy based assignment. detailed results table scnn-vae-unsup init performs better baselines. lstm+gmm performs probably feature dimension high even though already used reduce dimension. conditional text generation semi-supervised able generate text conditional label. space limitation show example food good service horrible took forever food twice check food return food good service terrible took forever someone take drink order times check food nothing write came ﬁrst time last night food good service little slow food food good service little slow food pretty good grilled chicken sandwich really good deﬁnitely back food good service fast friendly food good well back work line previous works combining variational inferences text modeling ﬁrst work combine language model lstm decoder negative results. hand models text words though improvement found model used generate text. work ﬁlls gaps between them. applies variational inference dialogue modeling machine translation found improvement terms generated text quality language modeling results reported. embedded variational units every step different model using global latent variables learn high level features. decoder inspired recent success pixelcnn model images wavenet audios video pixel network video modeling bytenet machine translation contrast works showing using deep architecture leads better performance decoder used model control contextual capacity leading better performance. dict image pixels. conditioning smaller window pixels leads better results similar ﬁnding. much done come powerful prior/posterior distribution representations techniques normalizing ﬂows. treat future works. work largely orthogonal could potentially combined effective choice decoder yield additional gains. much previous work exploring unsupervised sentence encodings example skip-thought vectors paragraph vectors sequence autoencoders applies pretrained model semi-supervised classiﬁcation signiﬁcant gains baseline semi-supervised vae. showed controlling decoder’s contextual capacity improve performance language modeling semi-supervised classiﬁcation tasks preventing degenerate collapse training procedure. results indicate carefully characterizing decoder capacity understanding relates common variational training procedures represent important avenues unlocking future unsupervised problems. chen kingma diederik salimans duan dhariwal prafulla schulman john sutskever ilya abbeel pieter. variational lossy autoencoder. arxiv preprint arxiv. chung junyoung kastner kyle dinh laurent goel kratarth courville aaron bengio yoshua. recurrent latent variable model sequential data. advances neural information processing systems kingma diederik mohamed shakir rezende danilo jimenez welling max. semi-supervised advances learning deep generative models. neural information processing systems fraccaro marco sønderby søren kaae paquet ulrich winther ole. sequential neural models advances neural information stochastic layers. processing systems gregor karol danihelka graves alex rezende danilo jimenez wierstra daan. draw recurrent neural network image generation. arxiv preprint arxiv. gregor karol besse frederic rezende danilo jimenez danihelka wierstra daan. towards concepadvances neural information tual compression. processing systems kaiming zhang xiangyu shaoqing jian. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition kalchbrenner espeholt lasse simonyan karen oord aaron graves alex kavukcuoglu koray. neural machine translation linear time. arxiv preprint arxiv. kiros ryan yukun salakhutdinov ruslan zemel richard urtasun raquel torralba antonio fidler sanja. skip-thought vectors. advances neural information processing systems krizhevsky alex sutskever ilya hinton geoffrey imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems rezende danilo jimenez mohamed shakir wierstra daan. stochastic backpropagation approximate inference deep generative models. arxiv preprint arxiv. serban iulian vlad sordoni alessandro lowe ryan charlin laurent pineau joelle courville aaron bengio yoshua. hierarchical latent variable encoderdecoder model generating dialogues. arxiv preprint arxiv. oord a¨aron dieleman sander heiga simonyan karen vinyals oriol graves alex kalchbrenner senior andrew kavukcuoglu koray. wavenet generative model audio. corr abs/. oord aaron kalchbrenner espeholt lasse vinyals oriol graves alex conditional image generation pixelcnn decoders. advances neural information processing systems xinchen yang jimei sohn kihyuk honglak. attributeimage conditional image generation visual attributes. european conference computer vision springer yang zichao yang diyi dyer chris xiaodong smola alex hovy eduard. hierarchical attention networks document classiﬁcation. proceedings naacl-hlt anyone know info looking name anyone know picture friend cell phone anyone know biography anyone know copy anyone know name song sings tell boyfriend love best friend dont know tell please help politics osama laden food good service terrible three times time service horrible last time wait long time food come ﬁnally food food cold service terrible back place used favorite places area times food always good favorite places phoenix area food good service friendly star husband came brunch saturday night place packed able outside patio great view bellagio fountains great view bellagio fountains great view bellagio fountains star husband came ﬁrst time last night great time food amazing service great atmosphere perfect back worst place ever never back disappointed quality food service returning ﬁrst time location good experience great place grab bite friends family happy found great place nails done star star star star star star wife going restaurant years last times service terrible last time wait long time food arrive food good worth wait food good service leaves something desired times food consistently good service good service always great food good service terrible wait minutes food come cold back food good service terrible party food took forever come food good worth price food good service little slow wait food even busy times never disappointed food great service great back great service great deﬁnitely back could give place zero stars would recommend place anyone know hype place think back know hype place think back couple times never disappointed service friendly prices reasonable best ramen ever life never meal worst company ever dealt know worst buffet ever life food nothing write home place stay looking cheap place stay great place stay looking quick bite love place staff friendly helpful price right", "year": 2017}