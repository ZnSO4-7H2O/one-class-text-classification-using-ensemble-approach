{"title": "Generative Temporal Models with Memory", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.", "text": "consider general problem modeling temporal data long-range dependencies wherein observations fully partially predictable based temporally-distant past observations. sufﬁciently powerful temporal model separate predictable elements sequence unpredictable elements express uncertainty unpredictable elements rapidly identify novel elements help predict future. create models introduce generative temporal models augmented external memory systems. developed within variational inference framework provides practical training methodology methods gain insight models’ operation. show range problems sparse long-term temporal dependencies models store information early sequence reuse stored information efﬁciently. allows perform substantially better existing models based well-known recurrent neural networks like lstms. many data sets machine learning applications sequential whether natural language speech processing data streams high-deﬁnition video longitudinal timeseries medical diagnostics spatio-temporal data climate forecasting. generative temporal models core requirement applications. generative temporal models also important components intelligent agents permit counterfactual reasoning physical predictions robot localisation simulation-based planning among capacities tasks require models high-dimensional observation sequences contain complex long temporal dependencies—requirements available gtms unable fulﬁl. developing gtms paper. many gtms—whether linear nonlinear deterministic stochastic—assume underlying temporal dynamics governed low-order markov transitions ﬁxed-dimensional sufﬁcient statistics. examples models include hidden markov models linear dynamical systems kalman ﬁlters non-linear extensions ﬁxed-order markov assumption used models insufﬁcient characterising many systems practical relevance. bialek quantitatively show markov assumptions fail describe physical systems longrange correlations fail approximate long-distance dependencies written literature. models instead maintain information large variable-order histories e.g. recurrent neural networks signiﬁcant advantages ones constrained ﬁxed-order markov assumptions. recently proposed gtms like variational recurrent neural networks deep kalman filters built upon well-known recurrent neural networks like long short-term memory gated recurrent units principle recurrent networks solve variable-order markovian problems additive dynamics designed store protect information long intervals. practice scale poorly higher capacity storage required. rnns typically densely connected parametric complexity model grow quadratically memory capacity. furthermore recurrent dynamics must serve competing roles must preserve information stable state later retrieval must perform relevant computations distill information immediate use. limitations point need rnns separate memory storage computation. recurrent networks successfully separate memory storage computation developed several settings algorithm learning symbolic reasoning natural language processing recurrent networks store information memory buffer differentiable addressing mechanisms efﬁciently optimise reading writing memory. particular details system’s memory access mechanisms play critical role determining data efﬁciency. demonstrate generative temporal models memory exhibit signiﬁcantly enhanced capacity solve tasks involving complex long-term temporal dependencies. develop common architecture generative temporal models study four instantiations different type memory system. four models allow show different memory systems adapted different types sequential structure resulting impact modelling success data-efﬁciency generation quality. models distinct presented developed deep generative model images posessing attentional lookup mechanism. memory contains table parameters updated within sequence. instead table biases jointly optimised end-to-end performance. contrast systems dynamically update memory within sequence. structure discussion ﬁrst describing general approach designing generative temporal models performing variational inference compare gtmms vrnns visual sequence tasks designed stress different problems arise modelling information long time dependencies. finally make strides toward scaling models richer perceptual modelling three-dimensional environment. process make following technical contributions develop general architecture generative models memory. architecture allows develop gtmms based four memory systems positional memory architecture referred introspection network neural turing machine least-recently used access mechanism differentiable neural computer show variational inference makes easy train scalable models capable handling show models outperform current state-of-the-art gtms based several tasks range generative variants copy task one-shot recall across long time delays. show gtmms model realistic environments demonstrate models capture important aspects physical temporal consistency coherently generating ﬁrst-person views loop closure. generative temporal models kalman ﬁlters non-linear dynamical systems hidden markov models switching state-space models change-point models popular choice modeling temporal sequential data using latent variables. models explain observations corresponding latent variables figure variants generative temporal models. circled variables stochastic; boxed variables deterministic. solid lines show dependencies generative model; dashed lines show additional dependencies inference model. model parameters. formulation supports wide range models variants shown fig. particular examples include non-linear state space models deep kalman filters stochastic recurrent neural networks particular model speciﬁed ﬁxing distributions functional dependencies conditioned variables. distributions. typically assume prior distribution gaussian likelihood function distribution appropriate observed data gaussian continuous observations bernoulli binary data. conditional dependencies. models introduce deterministic hidden-state variable modiﬁed every time point using transition function prior describes non-linear dependence past observations latent variables using hidden state provides parameters latent variable distribution. nonlinear function observation provides parameters likelihood function depends latent variables state. functions speciﬁed using deep neural networks fully-connected convolutional recurrent networks. general model retains possible dependencies latent variables deterministic state variables maps transition parameterised lstm network depends history variable current observation current latent variable observation depends past history current latent variable. structure used chung variational rnns ﬁgure vrnns forms baseline comparisons since retains possible dependencies within model provides best existing models. dependency structures also considered although used paper gtms autoregressive dynamics transition depends visible variables i.e. statespace models observation maps depends latent variables speciﬁed model task infer posterior distribution latent variables learn model parameters. variational inference currently widelyused approaches since well suited problems high-dimensional observations highdimensional parameter spaces. variational inference also allows design fast scalable algorithms easily composed gradient-based learning systems provides tools principled model evaluation comparison. compute marginal probability observed data must integrate latent variables integration often intractable variational methods compute marginal probabilities transforming intractable integration rewrote expectation terms distribution variational parameters equation application jensen’s inequality obtained lower bound marginal likelihood; klp] kullback-leibler divergence distributions lower bound known negative free energy terms trade reconstruction accuracy complexity posterior approximation provides tractable objective function optimization. form distribution approximation true posterior distribution latent variables choose auto-regressive form distribution. recent approaches variational inference additional tools optimize free energy. first since expectations typically known closed form gradient computed using monte carlo estimator. continuous latent variables pathwise derivative used second approximate posterior distribution represented inference model whose outputs parameters posterior distribution. inference networks amortise cost inference across posterior computations make joint optimisation model variational parameters possible. inference model uses posterior speciﬁed deep network provides parameters q-distribution function current observation past history latent variables observations. latent variable models trained using amortised variational inference monte carlo gradient estimation referred variational auto-encoders generative temporal models referred temporal vaes. existing models temporal structure captured lstm networks state variables summarised introduction exhibit experiments lstms powerful sequence models suffer limitation strongly couple memory capacity recurrent processing number trainable parameters. limitation result slow learning demand large models achieve high capacity memory. overcome issue develop generative temporal models memory i.e. ones augmented external memory systems. prior diagonal gaussian depends memory context prior diagonal gaussian approximate posterior depends observation memory context posterior show stochastic computational figure components generative temporal model external memory. high-level structure model showing memory system connects generative model. green lines indicate writing reading operations respectively. update controller state time combined latent variable time produce attention weight. produces memory context function data memory time denote ψt−. schematic introspective memory system. schematic memory systems like dnc. graph modiﬁed generative process fig. structure generic ﬂexible allows type memory system used allowing remainder system unchanged since dependencies memory context external memory systems comprise components external memory stores latent variables controller implements addressing scheme informs memory storage retrieval. types addressing schemes possible contentbased addressing accesses memories based similarity given position-based addressing accesses memories based position within memory-store. expand four types memory systems different characteristics describing speciﬁc memory controllers used ﬁnal memory context computed. develop external memory system uses position-based addressing scheme fast learning temporal dependencies related pointer networks vinyals able effectively handle sequences temporally-extended dependency structures trains quickly robustly applied wide variety tasks refer memory system introspection network. memory. memory ﬁrst-in-ﬁrst-out buffer storage locations latent variables written generated time step. natural directly store latent variables since compressed representations data time point. type memory require model learn write memory read feature enables fast learning. controller. controller responsible memory retrieval. every time step controller ﬁrst updates hidden states using lstm network frnn fuses information previous hidden state previously generated latent variable zt−. additional context information available also included input. access memory softattention weights computed using attention network fatt based output controller attention weights used retrieve multiple memories time. attention weight used compute weighted average rows memory matrix produce retrieved memory vector number attention functions fatt used including softmax gaussian. make function deep feed-forward network. attention system proved easy require special initialisation. model found softmax attention slower convergence. gating mechanism. ability retrieve multiple memories makes possible latent variables depend variable number past latent variables. allow network adjust importance retrieved memories corrections passed sigmoid function element-wise multiplied context vector. ﬁnal memory context output memory system concatenation memory-contexts read-head forms memory context passed generative model. complete information introspection network shown stochastic computational graph fig. introspective gtmms learn fast simple memory structure limits range applications applied. develop gtmms three alternative types memory architectures neural turing machine combines content-based positional addressing; least-recently used access exclusively employs content-based addressing; differentiable neural computer uses content-based addressing mechanism positional addressing links positions memory based temporal adjacency writing. call models ntm-gtmm lru-gtmm dnc-gtmm respectively. describe high-level aspects memory controllers used defer detailed discussion properties alternative parameterisations graves santoro graves memory. unlike ﬁrst-in-ﬁrst-out buffer used previously memory ntms dncs generic storage allows information written read location. controller. controller uses lstm network frnn updates state-history external memory using latent variables previous time step additional context information generative model conditioned perform content-based read items memory controller generates compares memory using cosine similarity keys measure yield soft attention weights retrieved memory obtained weighted attention weights memory evaluated models qualitatively quantitatively. qualitative assessment involved visual inspection generated sequences; example task copy particular portion observed sequence number steps copy procedure evident sequences generated models. qualitative assessments revealed signiﬁcant differences models even differences variational lower bounds minimal. also used three quantitative metrics gather complete picture model behaviour ﬁrst variational lower bound objective function tracked across training; second kl-divergence particular time-point every training sequence tracked across training steps; third time-point kl-divergences averaged batch sequences training completed. time-step kl-divergences measure number bits additional information needed represent posterior distribution relative prior distribution latent variable used explain current observation. indicate amount prior knowledge model contains. kl-divergence close zero current observation fully-predictable previous information. tasks considered here deﬁned random sequences every episode would imply model stores information memory beginning episode construct predictive priors rest episode. calculating quantity across training sequences last time point sequence demonstrates quickly memory system becomes useful prediction across training viewing average time-step averaged batch sequences training indicates much information trained model gathers throughout sequence make predictions. posterior observation maps used convolutional deconvolutional networks cases residual skip connections refer appendix explicit details. models trained stochastic gradient descent variational lower bound using adam optimizer learning rate mini-batches training sequences used computing gradients tasks. tasks involving digits characters used latent variables size environment used latent variables size used read heads tasks. number memory slots used gtmms taken number steps training sequences task except lrugtmm used number slots times number time steps hidden state size lstm models chosen keep total number parameters within another model replicas hyperparameters. performed quantiﬁcations ﬁgures ﬁrst averaged example sequences mini-batch computed means standard errors across replicas model type. tested models seven tasks probed capacity learn make predictions temporal data complex dependencies. tasks involved image-sequence modelling offered tests deduction spatial reasoning one-shot generalisation. example training sequences provided task described below. appendix present generated samples tasks. tasks artiﬁcial data sets pseudo-code generate training sequences given appendix figure perfect recall task left average kullbackleibler divergence frame serving measure prediction error prior posterior. models learned repetition frame introspective gtmm exhibited lowest error. middle last frame also lowest introspective gtmm convergence. right introspective gtmm convergenced fastest lowest level negative variational lower bound close models. middle perfect recall task results roughly similar. bottom perfect recall task substantially larger time intervals models able detect regularity data sequences. training sequences consisted randomly sampled mnist digits remembered prerecall interval extended time steps. thus digits distractor stimuli. recall interval occurred steps ﬁrst images presented again. constructed variants task fig. shows typical training sequence task. succeed task models encode store ﬁrst images protect distractor interval retrieve recall interval. successful memory would elicit drop kl-divergence recall interval since information stored memory information extracted current observation would used image reconstruction. figure parity recall task difference models minimal here likely memory storage required solve task bits. middle task parameters bottom again models succeeded equally despite longer delay. performance task reported fig. models showed effect kldivergence prior posterior becomes reduced beginning recall phase. however gtmms especially introspective gtmm reduction signiﬁcant. implies models predicting arriving frames based memory. effect pronounced larger sequence length. contrast previous experiment exact recall images demanded parity recall task asked model identify report latent property data. recall interval model must generate sequence matched parity ﬁrst images. ﬁrst recalled digit zero ﬁrst digit initial sequence even. fig. shows typical training sequence. successful models then need implicitly classify input digits. although computation required task complicated perfect recall information content stored actually smaller i.e. single image. figure one-shot recall task experimental parameters again task similar perfect recall structure models performed comparable rank order introspective gtmm showing signiﬁcant reductions sequence predictable memory. less memory required parity recall perfect recall details image need retained; instead parity image tracked requiring bits total. models performed satisfactorily here fig. exhibiting reductions number possible digit classes drops models able contend long delays equally suggesting primary advantage gtmms vrnns tasks require storage large number bits. also examined abilities gtmms memorise novel information testing sequences data directly trained. typical training sequence shown fig. images every point time drawn omniglot data training data consisted alphabets three characters excluded alphabet. unseen characters used form unseen sequences test time. task otherwise perfect recall task demands generative model memory substantial. gtmms outperformed vrnn introspective gtmm showing signiﬁcant reductions divergence beginning recall phase notable memorised images entirely novel hold-outs training set. thus gtmms particular introspective gtmm able construct useful latent variable representations novel stimuli store memory predict future events recall phase. preceding tasks demanded ordered recall sequence. here tested whether recall complicated order possible. typical training sequence shown figure began sequence digits before. next digits generated index-andrecall game. ﬁgure example ﬁnal digit pre-recall interval numerical value digit indicates position sequence next digit copied. here position contains ﬁrst digit recall interval. position contains successful models therefore learn classify digits class labels images based temporal order presentation. figure training sequence dynamic dependency task following index-and-recall game image digit provided positional reference next digit sequence order. task requires models learn algorithmic addressing procedure current image indicates time point next image stored allowing memory address storing latent variables time point looked gtmms perform considerably better vrnn task substantial improvements achieved introspective gtmm dnc-gtmm last task probed positional indexing construct task demands content-based addressing. training sequence ﬁrst present random sequence digits time steps. digits recall interval randomly chosen contiguous sub-sequence length pre-recall interval solve task model must able ﬁrst image recall interval similar image seen previously produce temporal sequence followed similarity-cued recall played strongly advantages memory systems contentbased addressing. task required using image images sequence followed pre-recall exposure phase. perform operation memory systems content-based addressing could encode look similar feature encodings memory. long mechanism iterate subsequent latent variables task easily solved. dnc-gtmm could temporal transition links task described lacks content-based addressing introspective-gtmm perform better vrnn important motivation developing gtmms desire improve capacity agents understand spatial structure environments. example problem created environment represented grid grid cell contained random mnist digit case instead agent took random walk grid using actions down left right steps move neighbouring locations receiving observation corresponding current grid cell. actions always treated context variables predicted gtmms. expected gtmms conditioned random walk action sequences would able generate observation case grid cell revisited action sequence; gtmm maintained coherent environment. figure similarity-cued recall temporal dependencies length memory models content-based addressing i.e. ntm- lru- dnc-gtmm showed signiﬁcant reductions lowest task losses. since actions generated random walk process structure problem poorly captured memory addressing mechanisms based time positional order sequence. instead models used content-based addressing could encode sequence actions alongside latent variable representations images. necessary predict present already visited location content-based addressing could used look latent variables based action sequence. since grid location could reached multiple routes models also capture invariance action sequences converted displacements origin. dnclru-gtmm performed best task exhibiting signiﬁcant reductions environment explored. fig. also show generation sequences maze model. lru-gtmm dnc-gtmm consistently generated digits returning positions. although ntm-backed gtmm content-based addressing ability allocate free locations memory generally inferior abilities models using lrus dncs. lrudnc-based mechanisms could easily store memories also could collocate memories context information registering computed position grid image located. ultimately wish design agents operate realistic environments learn sequential information using memory form predictions. agents possess spatiotemporally coherent memories environments understanding walls typically shift undisturbing movements change camera angles scene arrangements. ﬁrst study problem test whether gtmms maintain consistent predictions provided frames in-place rotation camera full turns. experiments used procedurally-generated maze environment random wall conﬁgurations textures object positions. rotational dynamics environment included acceleration frame represent view angle equally distributed around unit circle. models cope structure. additionally frames captured discrete moments models learn interpolate past views full turn instead merely copying frames ﬁrst rotation. rotational period steps full episode took place steps. example training sequence shown figure successful generative model data create random environment panoramas generate views consistent panorama second full turn. figure random walk action sequence provided model along images found location. boundaries action sequence restricted stay bounds. action provided conditioning variable generation modeled itself. fig. vrnn lowest variational lower bound. however generative samples clear vrnn also incoherent across time forgot information paintings walls buildings skyline argue representative quantiﬁcation signiﬁcance training loss kullback-leibler divergence last frame well reduction time turn. indicators showed dnc-gtmm introspective-gtmm models able predict redundant frames memory. paper open research direction. seen that range tasks standard generative temporal models limited memory capacity memory access mechanisms. motivated design generative temporal models external memory systems whose performance cases qualitatively better. tried provide proofs concept without extraneous complication example variational distributions simple diagonal gaussians; could consider complex posterior distributions like used draw normalising ﬂows auxiliary variables models discrete variables also aimed explore advantages disadvantages variety external memory mechanisms generative temporal modelling. results suggest none architectures report uniformly dominant across tasks. however interesting direction research strongly suggested results. namely imagine model combining direct storage latent variables introspective-gtmm content-based addressing ntm- lru- dnc-gtmm could indeed prove performance uniformly dominant across tasks. furthermore many sparse memory access models efﬁciently implemented using fast k-nearest neighbour lookup though explore savings here. leave development sophisticated models future work. storage latent variables transformed latent variables memory additionally suggests several intriguing extensions framework. currently models based mathematics figure generation examples mnist maze. actions shown row. column last actions time point indicated yellow arrows. best four models type shown below. ﬁrst time grid cell visited pixel channel turned second time image generated superimposed green channels turned third time blue channel turned overlaid generated image white. vrnn inconsistent generations subsequent visits grid cells produced overlays different colours share shape. introspection models comparison better coherent generation lrudnc-based gtmms best here that example white yellow digits masked underlying digit. figure realistic environments task. vrnn lowest variational lower bound measure tells little overall story. instead dnc-gtmm introspective-gtmm showed signiﬁcant reductions sequence predictable memory measure translates good behaviour generation samples. optimal ﬁltering produce sample every time step drawn ﬁltering posterior latent variables stored memory formal distinction ﬁltering smoothing begins break down. imagine mechanisms modify previously written latent variables fact overwriting previously written locations. explicit storage latent variables memory supports this whereas would difﬁcult precisely modify component latent state encodes historical latent variable conventional densely connected rnn. j.-a. assael wahlstr¨om sch¨on deisenroth. data-efﬁcient learning feedback policies image pixels using deep dynamical models. arxiv preprint arxiv. chung kastner dinh goel courville bengio. recurrent latent variable model sequential data. advances neural information processing systems pages eslami heess weber tassa szepesvari hinton attend infer repeat fast scene understanding generative models. advances neural information processing systems pages stochastic gradient estimation. technical report dtic document ghahramani hinton. parameter estimation linear dynamical systems. technical report technical report crg-tr-- university totronto dept. computer science graves wayne danihelka. neural turing machines. arxiv preprint arxiv. graves wayne reynolds harley danihelka grabska-barwi´nska colmenarejo grefenstette ramalho agapiou hybrid computing using neural network dynamic external memory. nature kingma adam method stochastic optimization. corr kingma welling. auto-encoding variational bayes. iclr krishnan shalit sontag. deep kalman ﬁlters. arxiv preprint arxiv. kumar irsoy bradbury english pierce ondruska gulrajani socher. anything dynamic memory networks natural language processing. arxiv preprint arxiv. tornio honkela karhunen. time series prediction variational bayesian nonlinear state-space models. proc. european symp. time series prediction pages watter springenberg boedecker riedmiller. embed control locally linear latent dynamics model control images. advances neural information processing systems pages posterior mapping inputs latents implemented convolutional neural network consisted blocks arranged series ﬁrst block grey-scale image input saturation mapped range input passed four parallel dimension-preserving convolutional streams convolved input using kernels size respectively padding necessary dimension-preservation. outputs parallel streams passed batch-normalization layer rectiﬁed-linear layer concatenated total feature-maps. feature-maps served input dimension-halving convolution using kernel size followed batch normalization layer rectiﬁed linear layer. thus ﬁrst processing block functioned take image input return featuremaps size second identical block followed ﬁrst except differences batch normalization rectiﬁed layer ﬁnal convolution ﬁnal number kernels produce sample latent variable mapped kernels linear layer vector length dimensions used construct mean used construct vector parameters gaussian distribution. together sample standard normal latent generated σt\u0001t thus completing posterior map. observation mapping latents recurrent deterministic variables reconstructions identical posterior except convolution operations replaced deconvolution operations. three-dimensional environment visual model inputs included colour channels comprising values. used separate pathways roughly encode global information across image another roughly encode local textures. global information pathway convolved image using kernels stride padding. passed kernels stride without padding. feature-maps convolved kernels stride without padding. gave super-pixels. local texture pathway convolutional layer kernels size stride padding feature-maps passed another convolutional layer kernels size stride padding yielded block size linearised vector elements concatenated features global information pathway. thewallsﬂoorcolourandbuildingsontheskyline. duringasecondrotation.thevrnnmodelclearlyforgotinformationfromtheﬁrstrotationbuttheothermodelsclearlydemonstratedmemoryofpaintingson figurerotationinacomplexenvironment.themodelsobservedframesfromtheenvironmentduringonerotationthengeneratedcorrespondingframes algorithm training sequence generation dynamic dependency task procedure dynamicdependency generator instance generator dataset length random sequence length recall interval sequence labels", "year": 2017}