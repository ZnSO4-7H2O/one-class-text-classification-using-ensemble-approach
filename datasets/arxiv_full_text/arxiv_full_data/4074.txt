{"title": "Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image  Captioning", "tag": ["cs.CV", "cs.AI"], "abstract": "Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as \"the\" and \"of\". Other words that may seem visual can often be predicted reliably just from the language model e.g., \"sign\" after \"behind a red stop\" or \"phone\" following \"talking on a cell\". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.", "text": "question answering attend image every time step irrespective word going emitted next however words caption corresponding visual signals. consider example fig. shows image generated caption white bird perched stop sign. words corresponding canonical visual signals. moreover language correlations make visual signal unnecessary generating words like following perched sign following stop. fact gradients non-visual words could mislead diminish overall effectiveness visual signal guiding caption generation process. paper introduce adaptive attention encoderdecoder framework automatically decide rely visual signals rely language model. course relying visual signals model also decides image region attend ﬁrst propose novel spatial attention model extracting spatial image features. proposed adaptive attention mechanism introduce long short term memory extension produces additional visual sentinel vector instead single hidden state. visual sentinel additional latent representaattention-based neural encoder-decoder frameworks widely adopted image captioning. methods force visual attention active every generated word. however decoder likely requires little visual information image predict nonvisual words words seem visual often predicted reliably language model e.g. sign behind stop phone following talking cell. paper propose novel adaptive attention model visual sentinel. time step model decides whether attend image visual sentinel. model decides whether attend image where order extract meaningful information sequential word generation. test method coco image captioning challenge dataset flickrk. approach sets state-of-the-art signiﬁcant margin. source code downloaded https//github.com/jiasenlu/adaptiveattention images emerged prominent interdisciplinary research problem academia industry. visually impaired users make easy users organize navigate large amounts typically unstructured visual data. order generate high quality captions model needs incorporate ﬁne-grained visual clues image. recently visual attention-based neural encoder-decoder models explored attention mechanism typically produces spatial highlighting image regions relevant generated word. nonlinear function outputs probability visual context vector time extracted image hidden state time paper adopt long-short term memory instead vanilla rnn. former demonstrated state-of-the-art performance variety sequence modeling tasks. modeled commonly context vector important factor neural encoder-decoder framework provides visual evidence caption generation different ways modeling context vector fall categories vanilla encoder-decoder attentionbased encoder-decoder frameworks first vanilla framework dependent encoder convolutional neural network input image extracts last fully connected layer global image feature across generated words context vector keeps constant depend hidden state decoder. second attention-based framework dependent encoder decoder. time based hidden state decoder would attend speciﬁc regions image compute using spatial image features convolution layer cnn. show attention models signiﬁcantly improve performance image captioning. attention function spatial image features dimensional representation corresponding part image. hidden state time given spatial image feature rd×k hidden state lstm feed single layer neural network followed softmax function generate attention distribution regions image tion decoder’s memory provides fallback option decoder. design sentinel gate decides much information decoder wants image opposed relying visual sentinel generating next word. example illustrated fig. model learns attend image generating words white bird stop relies visual sentinel generating words sign. overall main contributions paper introduce adaptive encoder-decoder framework automatically decides look image rely language model generate next word. ﬁrst propose spatial attention model build design novel adaptive attention model visual sentinel. model signiﬁcantly outperforms state-of perform extensive analysis adaptive attention model including visual grounding probabilities words weakly supervised localization generated attention maps. start brieﬂy describing encoder-decoder image captioning framework given image corresponding caption encoder-decoder model directly maximizes following objective based visual sentinel propose adaptive attention model compute context vector. proposed architecture adaptive context vector deﬁned modeled mixture spatially attended image features visual sentinel vector. trades much information network considering image already knows decoder memory mixture model deﬁned follows sentinel gate time mixture model produces scalar range value implies visual sentinel information used means spatial image information used generating next word. compute sentinel gate modiﬁed spatial attention component. particular additional element vector containing attention scores deﬁned equation element indicates much attention network placing sentinel addition extra element summarized converting equation figure illustration soft attention model proposed spatial attention model vector elements rk×d parameters learnt. attention weight features based attention distribution context vector obtained different shown fig. current hidden state analyze look combine sources information predict next word. motivation stems superior performance residual network generated context vector could considered residual visual information current hidden state diminishes uncertainty complements informativeness current hidden state next word prediction. also empirically spatial attention model performs better illustrated table adaptive attention model spatial attention based decoders proven effective image captioning cannot determine rely visual signal rely language model. section motivated merity introduce concept visual sentinel latent representation decoder already knows. visual sentinel extend spatial attention model propose adaptive model able determine whether needs attend image predict next word. visual sentinel? decoder’s memory stores long short term visual linguistic information. model learns extract component model fall back chooses attend image. component called visual sentinel. gate decides whether attend image image captioning many important applications ranging helping visually impaired users human-robot interaction. result many different models general methdeveloped image captioning. divided categories template-based neural-based template-based approaches generate caption templates whose slots ﬁlled based outputs object detection attribute classiﬁcation scene recognition. farhadi infer triplet scene elements converted text using templates. kulkarni adopt conditional random field jointly reason across objects attributes prepositions ﬁlling slots. powerful language templates syntactically well-formed tree descriptive information output attribute detection. neural-based approaches inspired success sequence-to-sequence encoder-decoder frameworks machine translation view image captioning analogous translating images text. kiros proposed feed forward neural network multimodal log-bilinear model predict next word given image previous word. methods replaced feed forward neural network recurrent neural network vinyals lstm instead vanilla decoder. however approaches represent image last fully connected layer cnn. karpathy adopt result object detection r-cnn output bidirectional learn joint embedding space caption ranking generation. recently attention mechanisms introduced encoder-decoder neural frameworks image captioning. incorporate attention mechanism learn latent alignment scratch generating corresponding words. utilize high-level concepts attributes inject neural-based approach semantic attention enhance image captioning. yang extend current attention encoder-decoder frameworks using review network captures global properties compact vector representation usable attention mechanism decoder. present variants architectures augmenting high-level attributes images complement image representation sentence generation. formulation encourages model adaptively attend image visual sentinel generating next word. sentinel vector updated time step. adaptive attention model call framework adaptive encoder-decoder image captioning framework. encoder-cnn. encoder uses representation images. speciﬁcally spatial feature outputs last convolutional layer resnet used dimension represent spatial features grid locations. following global image feature obtained global image feature. modeling convenience single layer perceptron rectiﬁer activation function transform image feature vector vectors dimension decoder-rnn. concatenate word embedding vector global image feature vector input vector single layer neural network transform visual sentinel vector lstm output vector vectors dimension training details. experiments single layer lstm hidden size adam optimizer base learning rate language model cnn. momentum weightdecay respectively. ﬁnetune network epochs. batch size train epochs early stopping validation table performance flickrk coco test splits. indicates ensemble models. bleu score uses n-grams. higher better columns. future comparisons rouge-l/spice flickrk scores coco scores ./.. table leaderboard published state-of-the-art image captioning models online coco testing server. submission ensemble models trained different initialization. flickrk contains images collected flickr. images depict humans performing various activities. image paired crowd-sourced captions. publicly available splits containing images validation test each. coco largest image captioning dataset containing images training validation test respectively. dataset challenging since images contain multiple objects context complex scenes. image human annotated captions. ofﬂine evaluation data split containing images validation test each. online evaluation coco evaluation server reserve images validation development rest training. compared approaches ofﬂine evaluation flickrk coco ﬁrst compare full model ablated version performs spatial attention. goal comparison verify improvements result orthogonal contributions compare method deepvs hard-attention recently proposed best performed method online evaluation compare method google captivator m-rnn lrcn hard-attention attfcn quantitative analysis report results using coco captioning evaluation tool reports following metrics bleu meteor rouge-l cider also report results using metric spice found better correlate human judgments. figure visualization generated captions image attention maps coco dataset. different colors show correspondence attended regions underlined words. first columns success cases last columns failure examples. best viewed color. without visual sentinel veriﬁes effectiveness proposed framework. adaptive attention model signiﬁcantly outperforms spatial attention model improves cider score flickrk coco respectively. comparing previous methods single model signiﬁcantly outperforms previous methods metrics. coco approach improves state-of-the-art bleu- meteor cider similarly flickrk model improves state-of-the-art large margin. compare model state-of-the-art systems coco evaluation server table approach achieves best performance metrics among published systems. notably google inception-v encoder similar better classiﬁcation performance compared resnet- better understand model ﬁrst visualize spatial attention weight different words generated caption. simply upsample attention weight image size using bilinear interpolation. fig. shows generated captions spatial attention maps speciﬁc words caption. first columns success examples last column shows failure examples. model learns alignments correspond strongly human intuition. note even cases model produces inaccurate captions model look reasonable regions image seems able count recognize texture ﬁne-grained categories. provide extensive list visualizations supplementary material. visualize sentinel gate caption generated. word visual grounding probability. fig. visualize generated caption visual grounding probability spatial attention generated model word. model successfully learns attend image less generating non-visual words visual words like rose doughnuts woman snowboard model assigns high visual grounding probabilities note word assigned different visual grounding probabilities generated different contexts. example word usually high visual grounding probability beginning sentence since without language context model needs visual information determine plurality hand visual grounding probability phrase table much lower. since unlikely something table. section analysis adaptive attention generated methods. visualize sentinel gate understand when model attends image caption generated. also perform weakly-supervised localization coco categories using generated attention maps. help intuition where order assess whether model learns separate visual words captions non-visual words visualize visual grounding probability. word vocabulary average visual grounding probability generated captions containing word. fig. shows rank-probability plot coco flickrk. model attends image generating object words like dishes people boat; attribute words like giant metal yellow number words like three. word non-visual model learns attend image etc. abstract notions crossing during etc. model leans attend less visual words attend non-visual words. note model rely syntactic features external figure localization accuracy generated captions frequent coco object categories. spatial attention adaptive attention proposed spatial attention model adaptive attention model respectively. coco categories ranked based align results adaptive attention cover total matched regions spatial attention adaptive attention respectively. model cannot distinguish words truly non-visual ones technically visual high correlation words hence chooses rely visual signal. example words phone relatively visual grounding probability model. large language correlation word cell. also observe interesting trends model learns different datasets. example generating words model learns attend less image coco flickrk. words different forms also results different visual grounding probabilities. example crossing cross crossed cognate words similar meaning. however terms visual grounding probability learnt model large variance. model learns attend images generating crossing followed cross attend least image generating crossed. assess whether model attends correct spatial image regions. perform weakly-supervised localization using generated attention maps. best best knowledge previous works used weakly supervised localization evaluate spatial attention image captioning. given word attention ﬁrst segment regions image attention values larger per-class threshold estimated using coco validation split. take bounding covers largest connected component segmentation map. intersection union generated ground truth bounding localization accuracy. by-word match align generated words ground truth bounding box. object categories multiple words teddy bear take maximum score multiple words localization accuracy. able align regions captions generated spatial adaptive attention models respectively. average localization accuracy spatial attention model adaptive attention model. demonstrates byproduct knowing attend also helps attend. fig. shows localization accuracy generated captions frequent coco object categories. spatial attention adaptive attention models share similar trends. observe models perform well categories truck. smaller objects sink surfboard clock frisbee models perform relatively poorly. spatial attention maps directly rescaled coarse feature looses spatial resolution detail. using larger feature improve performance. paper present novel adaptive attention encoder-decoder framework provides fallback option decoder. introduce lstm extension produces additional visual sentinel. model achieves state-of-the-art performance across standard benchmarks image captioning. perform extensive attention evaluation analysis adaptive attention. though model evaluated image captioning useful applications domains. acknowledgements work funded part career award award sloan fellowship award allen distinguished investigrator award paul allen family foundation google faculty research award amazon academic research award", "year": 2016}