{"title": "End-to-end Training for Whole Image Breast Cancer Diagnosis using An All  Convolutional Design", "tag": ["cs.CV", "cs.AI", "stat.ML"], "abstract": "We develop an end-to-end training algorithm for whole-image breast cancer diagnosis based on mammograms. It requires lesion annotations only at the first stage of training. After that, a whole image classifier can be trained using only image level labels. This greatly reduced the reliance on lesion annotations. Our approach is implemented using an all convolutional design that is simple yet provides superior performance in comparison with the previous methods. On DDSM, our best single-model achieves a per-image AUC score of 0.88 and three-model averaging increases the score to 0.91. On INbreast, our best single-model achieves a per-image AUC score of 0.96. Using DDSM as benchmark, our models compare favorably with the current state-of-the-art. We also demonstrate that a whole image model trained on DDSM can be easily transferred to INbreast without using its lesion annotations and using only a small amount of training data.  Code and model availability: https://github.com/lishen/end2end-all-conv", "text": "develop end-to-end training algorithm whole-image breast cancer diagnosis based mammograms. requires lesion annotations first stage training. that whole image classifier trained using image level labels. greatly reduced reliance lesion annotations. approach implemented using convolutional design simple provides superior performance comparison previous methods. ddsm best single-model achieves per-image score three-model averaging increases score inbreast best single-model achieves per-image score using ddsm benchmark models compare favorably current state-of-the-art. also demonstrate whole image model trained ddsm easily transferred inbreast without using lesion annotations using small amount training data. code model availability https//github.com/lishen/endend-all-conv rapid advancement machine learning especially deep learning recent years keen interest medical imaging community apply techniques cancer screening. recently group researchers along sage bionetworks dream community organized challenge competing teams develop algorithms improve breast cancer diagnosis using large database digital mammograms main purpose challenge predict probability patient develop cancers within months based mammographic images. also participated challenge obtained receiver operating characteristic curve score ranked leaderboard however much still done improve result. mammography based breast cancer diagnosis challenging problem cannot simply treated normal image classification task. cancerous status whole image determined small regions need identified. example mammogram typically size pixels cancerous lesion region interest small pixels. resizing large mammogram common choice used image classification filed likely make hard detect and/or classify. mammographic database comes annotations images diagnostic problem conveniently solved object detection classification problem already well studied computer vision field. example region-based convolutional neural network approach variants applied here. many published works assume databases study fully annotated rois thus developed models cannot transferred another database lacks annotations. unrealistic require databases fully annotated time monetary costs obtain annotations require expertise radiologists. survey public databases want build breast cancer diagnostic system production consider situation datasets annotated rois datasets annotated whole image level. exactly situation participants faced challenge competitive phase. participants allowed public databases develop models. however challenge database contain annotations. although challenge boasts large database images women dataset highly imbalanced positive cases public train significantly reduces effective sample size. therefore directly training classification model dataset scratch unlikely give good performance. study propose approach utilizes fully annotated database train model recognize localized patches. that model converted whole image classifier trained end-to-end without annotations. approach based idea independently formed final stage competition time implement competition ended. original entry challenge used patch classification network built fully annotated public database predict overlapping patches mammogram generate called heatmap. random forest classifier used heatmap produce final label. however observed significant performance drop applied model challenge data patch classifier developed public database contains different color profile challenge data. difficult transfer patch classifier onto challenge data since annotations. whole image classifier end-to-end trainable involves sliding window step. later realized end-to-end training achieved using convolutional property change input size patch whole image create larger feature maps additional convolutional layers produce final labels. surprisingly found similar idea employed top-performing team final results came out. encouraged success want continue investigate line methods. propose convolutional design construct whole image classifier simply stacking convolutional blocks patch classifier provides competitive results. also present pipeline build whole image classifier scratch discuss pros cons important choices. notice method somewhat related recent publications breast cancer diagnosis. claim implementation multi-instance learning modified cost functions satisfy criterion. studies develop whole image models end-to-end trained completely ignore existence annotations databases train models using image level labels. seems focus layers summarize predictions local patches patch classifiers make accurate predictions. however show patch classifier critical performance whole image classifier. manuscript organized follows. section present method convert patch classifier whole image classifier network training algorithm. section parts. section develop whole image classifiers scratch evaluate public database annotations. section transfer whole image models developed section another database using image level labels. section provide discussion results future works. hope virtue study provide insights promising method whole image classification medical imaging community. perform classification segmentation large complex images commonly used approach first develop classifier recognize smaller patches classifier scan whole image using sliding window. example approach exploited competition automated detection metastatic breast cancer segment neuronal membranes microscopic images however want utilize patch classifier initialize weights whole image classification network latter trained without annotations. words want whole image classifier finetuned end-to-end fashion without explicit reliance patch image set. seem obvious surface actually achieved exploiting properties convolutional neural network idea learn filters small sizes apply much larger input using weight sharing greatly reduces number parameters learn. time convolutional layer added network layer effectively uses previous layers construct filters perform complex transformations precursors. reasoning patch classification network convolutional layer turns patch network part filters layer. although patch network trained recognize patches properties convolution allow change input patches whole images. modification convolutional operation layer becomes equivalent applying patch classifier patches whole image forward propagation apply convolutional operation patch classifier’s outputs. indeed design kind operations patch classifier’s output eventually connect image level labels. illustration method fig. notice variable input size feature supported major deep learning frameworks therefore easily implemented. advantage method network takes whole images input image level labels output cutting need patch image set. training method find many applications medical imaging. breast cancer diagnosis example. databases annotations rare expensive obtain. largest public database mammograms digital database screening mammography contains thousands images pixel-level annotations exploited construct meaningful patch classifier. patch classifier converted whole image classifier finetuned another database using whole image level labels. significantly reduce requirement annotations. classifications. therefore want focus structure layers study. propose convolutional design makes fully connected layers. briefly first remove last layers patch network last convolutional layer. convolutional layers residual blocks last convolutional layer feature reaches proper size. lastly global average pooling layer output complete network. notice deliberately discard output layer patch network. patch classifier typically small output create information bottleneck prevents layers fully utilizing information provided patch network. understand better let’s resnet example. assume trained recognize patch five categories background malignant mass benign mass malignant calcification benign calcification. last convolutional layer dimension residual block dimension patch classifier’s output creates bottleneck structure in-between patch network layers. network without patch classifier’s output structure allowing layers information patch network. contrast approach performing team challenge utilizes patch classifier’s output layer construct called heatmap represent likelihood patch input image five categories first version manuscript mistakenly used softmax activation heatmap generate probabilistic output layers. nonlinear transformation would certainly impede gradients flow. confirming performing team remove softmax activation heatmap construct networks similar theirs. however implement whole image classifiers find totally inactivated heatmap cause layers badly initialized leading divergence. hypothesize fully unbounded values make layers saturate early. therefore relu heatmaps instead study find convergence improve. another important difference method performing team’s method layers. argue layers poor image recognition require convolutional layer’s output flattened eliminates spatial information. design fully convolutional preserves spatial information every stage network. compare different strategies following sections. parts training whole image classifier scratch. first part train patch classification network. compare networks weights pretrained imagenet database randomly initialized ones. pretraining help speedup learning well improving generalization networks. pretrained network notice bottom layers represent primitive features tend invariant across datasets. layers represent higher representations related final labels therefore need trained aggressively. demands higher learning rate layers bottom layers. however layer-wise learning rate adjustment available keras deep learning framework work. therefore develop stage training strategy freeze parameter learning layers except last progressively unfreeze parameter learning bottom decrease learning rate time. details training strategy follows above epoch defined sweep train set. total number epochs early stopping used training improve validation loss. randomly initialized networks learning rate adam used optimizer batch size also adjust sample weights within batch keep classes balanced. second part create train whole image classifier patch classifier. done first altering input size patch whole image proportionally increases feature size every convolutional layer. study patch size whole image size resnet example. patch network last convolutional layer feature size whole image network layer’s feature size becomes convolutional design additional residual blocks stride reduce feature size global average pooling softmax output. alternatively average pooling applied convolutional layer produce feature size weight matrix output layer patch network copied applied onto feature generate heatmap replace softmax relu facilitate gradients flow. similar performing team heatmap first flattened layers shortcut connection added output. similar patch network training also develop -stage training strategy avoid unlearning important features patch network. trial-and-errors decide smaller learning rates usual prevent training diverging. details -stage training follows memory constraint small batch size whole image training. parameters patch classifier training. make easier convert patch classifier whole image classifier calculate pixel-wise mean mammograms train value pixel-wise mean centering patch whole image training. preprocessing applied. compensate lack sample size data augmentation on-the-fly patch whole image training. followings random image transformations horizontal vertical flips rotation degrees shear radians zoom ratio channel shift pixel values. ddsm contains digitized images scanned films uses lossless-jpeg format obsolete. modernized version database called cbis-ddsm contains images already converted standard dicom format. downloaded dataset cbis-ddsm’s website contains data patients mammograms represents subset original ddsm database. includes cranial cardo media lateral oblique views screened breasts. using views prediction shall provide better result using view separately. however treat view standalone image study limitation sample size. purpose predict malignancy status entire mammogram. perform split dataset train test sets based patients. train aside patients validation set. splits done stratified fashion positive negative patients proportions train validation test sets. total numbers images train validation test sets respectively. ddsm database contains pixel-level annotation rois pathology confirmed labeling benign malignant. contains type calcification mass. mammograms contain small portion contain rois. first convert mammograms format resize create several patch image sets sampling image patches rois background regions. patches size first dataset patches centered random background patch mammogram. second dataset sampled patches around minimum overlapping ratio number background patches mammogram. third dataset created increase number patches background mammogram. according annotations rois patches classified five different categories background calcification-benign calcification-malignant mass-benign mass-malignant. train network classify patch five categories three popular convolutional network structures resnet train networks sets using -stage training strategy epochs total. increase total number epochs adjust numbers training stage accordingly. evaluate models using test accuracy five classes. results summarized table randomly initialized pretrained resnet models achieve high accuracies pretrained network converges much faster cutting number epochs half. notice intrinsically easier classify sets. pretrained resnet performs much better randomly initialized resnet difference test accuracy. therefore conclude pretraining help train networks faster produce better models. pretrained networks rest study. sets resnet outperforms vgg. performs better dataset order reversed set. convert patch classifiers whole image classifiers testing many different configurations layers. evaluate different models using per-image scores test set. first test conversion based resnet patch classifiers. results summarized table notice original residual network design authors increased dimension residual block previous block times compensate reduction feature size. avoids creating bottlenecks computation. however able memory constraint. first test residual blocks dimension bottleneck design without repeating residual units. gives score resnet trained resnet trained large discrepancy score. hypothesize since larger contains much information variations benign malignant rois neighborhood regions. information helps training whole image classifier locate classify cancer-related regions diagnosis. focus patch classifiers trained sets rest study. vary design residual blocks reducing dimension last layer residual unit allows repeat residual unit twice without exceeding memory constraint i.e. blocks. design slightly increases score also reduce dimensions first second blocks find scores drop .-.. therefore conclude dimensions newly added residual blocks critical performance whole image classifiers. test conversion based patch classifier trained set. newly added blocks convolutions batch normalization results summarized table find structure likely suffer overfitting residual structure. however alleviated reducing model complexity newly added blocks. illustrate this plot train validation losses configurations training blocks dimensions repetitions dimensions repetition; first structure contains convolutional layers therefore higher complexity second one. seen first network difficulty identifying good local minimum suffers badly overfitting. second network smoother loss curves smaller differences train validation losses. reduce dimensions blocks find scores decrease small margin. line results resnet based models. overall resnet based whole image classifiers perform better based ones addition resnet based models seem achieve best validation score earlier based models. understand whether performance difference caused patch network bottom newly added layers residual blocks patch network create hybrid model. gives score line based networks points best resnet based models. suggests patch network part important encouraged performance leap going choose residual configurations configurations resnet patch classifiers trained respectively. expect obtain even performance gains. surprisingly performance decreases resnet based models indeed overfitting happens based model training early stage validation loss stops improving further. clear good patch classifier critical performance whole image classifier. however merely increasing number sampled patches necessarily lead better whole image classifiers. leave future work study sample patches efficiently effectively help building better image classifiers. test configurations inspired performing team heatmap followed layers shortcut connection patch classifier resnet patch classifiers trained set. choose different filter sizes pooling layer heatmap adjust layer sizes accordingly gradually decreases layer size output. also shortcut flattened heatmap output implemented layer. results summarized tables first version manuscript used heatmaps softmax activation generate probabilistic outputs found pooling layer destructive heatmap features therefore harm scores. version relu heatmaps instead find trend almost reversed pooling leads better score. could related difficulty training heatmap layers used. overall performance scores different activations other. therefore heatmap activation critical whole image classifier’s performance. obviously image classifiers design underperform ones convolutional design. notice implementation exact replicate work used modified network using instead blocks. design like hybrid designs compared using additional convolutional block followed layers. also want find whether heatmaps simply create bottleneck patch network layers. heatmap resnet patch classifier residual blocks design. network gives score lower design without heatmap. exclude possibility residual blocks overfit small heatmap input also residual blocks reduced size heatmap slightly improves score conclude heatmaps shall removed facilitate information flow training whole image networks. finally test strategy cutoff binarize heatmaps; extract regional features; train random forest classifier based features resnet trained used patch classifier. heatmaps softmax obtain probabilistic outputs help setting cutoffs. four different cutoffs combine features. random forest classifier trees. gives test score seems line score challenge. major reasons increase score study patch classifier study given training time produces better accuracy; train test sets database. clearly method performs worse end-to-end trained convolutional networks. pick models inference-level augmentation horizontal vertical flips create four predictions model take average. three best performing models resnet residual blocks blocks residual blocks selected. perform extended training hybrid model improve single-model score reason explained section augmented predictions three average three augmented predictions gives score notice created four network models augmented predictions predicted views breast took average predictions aggressive model averaging used study. deliberately avoid much model averaging better understand pros cons different network structures. whole image classifier developed finetune another database without using annotations. advantage offered end-to-end trained classifier. inbreast dataset another public database mammograms. recent ddsm contains full-field digital images opposed digitized images films. images different color profiles images ddsm visually confirmed looking example images databases therefore excellent source test transferability whole image classifier database another. inbreast database contains patients mammograms including views. treat views separate samples sample size limitation. includes bi-rads readings images lacks biopsy confirmation. therefore manually assign images bi-rads readings negative samples; positive samples; ignore bi-rads readings since clear designation negative positive class. excludes patients mammograms analysis. notice categorization based bi-rads readings makes inbreast dataset inherently easier classify ddsm dataset. mammograms different bi-rads readings already visually discernible according radiologists biases labeling. perform split dataset train validation sets based patients stratified fashion. total numbers images train validation sets respectively. exactly processing steps inbreast images ddsm images. although inbreast database contains annotations simply ignore test transferability whole image classifier. directly finetune whole image networks train evaluate model performance using per-image validation scores. best models resnet residual blocks blocks used transfer learning. adam optimizer learning rate number epochs weight decay finetuned resnet based model achieves score surprisingly finetuned based model achieves score better resnet based models. yaroslav argued structure better suited residual structure whole image classification residual networks reduce feature sizes aggressively damage features first layers. based this underperformance resnet based models likely bottom layers residual blocks. validate that hybrid model patch classifier bottom residual blocks finetune inbreast dataset. hybrid model achieves high score proves point. however structure indeed better residual structure bottom layers resnet beat vgg/ ddsm data? reason networks need trained longer residual networks reach full potentials. line observation based whole image classifiers achieve best validation scores ddsm later stage resnet based ones choice epochs ddsm mainly driven computational resource limitation. since residual networks powered shortcuts speed training able converge better networks within epochs. prove that perform another model training hybrid model ddsm data additional epochs. model improves test score good best resnet based models. perform additional training based models computational constraint. also want find much data required finetune whole image classifier reach satisfactory performance. important implications practice since obtaining labels even whole image level expensive. sample subset patients train finetuning evaluate model performance validation little patients images based hybrid models achieve scores respectively. scores seem quickly saturate increase train size. hypothesize hard part learning recognize textures benign malignant rois easy part adjust different color profiles. quick adjustment huge advantage end-to-end trained whole image networks. future works whole image classifier finetuned make predictions another database small amount training data. greatly reduce burden train construction. shown accurate whole-image breast cancer diagnosis achieved deep learning model trained end-to-end fashion independent annotations. network based convolutional design simple powerful. seen high-resolution mammograms critical accuracy diagnosis. however large image size easily lead explosion memory requirement. memory becomes available future shall return problem train models larger image sizes even original resolution without downsizing. provide much details rois potentially improve performance. decrease scores models based surprise. indicates intricacy training whole image classifier. research needed make whole image training robust divergence overfitting especially train large. patch sampling also made efficient focusing difficult cases easy ones. result supports yaroslav’s argument networks suitable residual networks breast cancer diagnosis. however also demonstrate superiority residual networks networks several aspects. problem residual structure first layers destroy fine details rois. future works modify original residual network make first layers less aggressive reducing feature sizes. shall lead improved performance residual networks. computational environment research study carried linux workstation cores single nvidia quadro memory. deep learning framework keras tensorflow backend. trained deep learning models medical image computing computer-assisted intervention miccai international conference munich germany october proceedings part navab hornegger wells frangi eds. cham springer international publishing convolutional network tumor detection classification breast mammography deep learning data labeling medical applications first international workshop labels second international workshop dlmia held conjunction miccai athens greece october proceedings carneiro mateus peter bradley tavares belagiannis papa nascimento loog cardoso cornebise eds. cham springer international publishing dhungel carneiro bradley automated learning deep features breast mass classification mammograms medical image computing computer-assisted intervention miccai international conference athens greece october proceedings part ourselin joskowicz sabuncu unal wells eds. cham springer international publishing mammogram classification abnormality detection nonlocal labels using deep multiple instance neural network presented eurographics workshop visual computing biology medicine ciresan giusti gambardella schmidhuber deep neural networks segment neuronal membranes electron microscopy images advances neural information processing systems pereira burges bottou weinberger eds. curran associates inc. chollet others keras. github martín abadi tensorflow large-scale machine learning heterogeneous systems. caffe convolutional architecture fast feature embedding arxiv. jun. michael heath kevin bowyer daniel kopans richard moore philip kegelmeyer digital database screening mammography proceedings fifth international workshop digital mammography", "year": 2017}