{"title": "Entropic one-class classifiers", "tag": ["cs.CV", "cs.LG", "stat.ML", "I.2.6; K.2.3"], "abstract": "The one-class classification problem is a well-known research endeavor in pattern recognition. The problem is also known under different names, such as outlier and novelty/anomaly detection. The core of the problem consists in modeling and recognizing patterns belonging only to a so-called target class. All other patterns are termed non-target, and therefore they should be recognized as such. In this paper, we propose a novel one-class classification system that is based on an interplay of different techniques. Primarily, we follow a dissimilarity representation based approach; we embed the input data into the dissimilarity space by means of an appropriate parametric dissimilarity measure. This step allows us to process virtually any type of data. The dissimilarity vectors are then represented through a weighted Euclidean graphs, which we use to (i) determine the entropy of the data distribution in the dissimilarity space, and at the same time (ii) derive effective decision regions that are modeled as clusters of vertices. Since the dissimilarity measure for the input data is parametric, we optimize its parameters by means of a global optimization scheme, which considers both mesoscopic and structural characteristics of the data represented through the graphs. The proposed one-class classifier is designed to provide both hard (Boolean) and soft decisions about the recognition of test patterns, allowing an accurate description of the classification process. We evaluate the performance of the system on different benchmarking datasets, containing either feature-based or structured patterns. Experimental results demonstrate the effectiveness of the proposed technique.", "text": "one-class classiﬁcation problem well-known research endeavor pattern recognition. problem also known diﬀerent names outlier novelty/anomaly detection. core problem consists modeling recognizing patterns belonging so-called target class. patterns termed non-target therefore recognized such. paper propose novel one-class classiﬁcation system based interplay diﬀerent techniques. primarily follow dissimilarity representation based approach; embed input data dissimilarity space means appropriate parametric dissimilarity measure. step allows process virtually type data. dissimilarity vectors represented weighted euclidean graphs determine entropy data distribution dissimilarity space time derive eﬀective decision regions modeled clusters vertices. since dissimilarity measure input data parametric optimize parameters means global optimization scheme considers mesoscopic structural characteristics data represented graphs. proposed one-class classiﬁer designed provide hard soft decisions recognition test patterns allowing accurate description classiﬁcation process. evaluate performance system diﬀerent benchmarking datasets containing either feature-based structured patterns. experimental results demonstrate eﬀectiveness proposed technique. index terms— one-class classiﬁcation; entropic spanning graph; modularity measure; dissimilarity representation; fuzzy set. pattern recognition problems involving processing patterns belonging class quite common interest type problems methodological application-oriented character. fact one-class classiﬁcation problems could used deal tasks involving recognition outliers data. application side instead many real-world scenarios possible obtain patterns so-called target class. instance cite problem determining whether given machine/device working properly. intuitively patterns representing correct functioning machine/device trivial informative sense anything diﬀerent observed faults deﬁnition instance correct functioning. consequence case would model patterns representing fault instances however modeling explicitly side decision boundary implies diﬃcult setting respect well-established multi-class problems. particular method evaluating performance one-class classiﬁer take account implicit uncertainty rooted resulting decisions one-class classiﬁcation setting adopted many real-world applications recognition faults smart electric grids raman spectroscopy events detection videos document classiﬁcation medical imaging spill detection paper propose novel based interplay diﬀerent techniques. ﬁrst objective make proposed applicable data type. develop approach basis dissimilarity space representation choice although increases overall computational complexity allows cover virtually application context regardless adopted representation input data then represent embedded data dissimilarity space complete euclidean graphs whose vertices denote input patterns edges mutual euclidean distance graph allows estimate informativeness represented data time construct decision regions deﬁne model. additionally representing data means euclidean graph provides bypass problems related high-dimensionality informativeness embedded data computed α-order r´enyi entropy estimated means entropic spanning graph technique proposed hero particular minimum spanning tree analyzed inducing partition graphs suitably compact separated clusters vertices. designed fast graph partitioning algorithm based concept modularity derived equipped suitable membership functions allow form hard soft decisions classiﬁcation. benchmark proposed test types data features labeled graphs based patterns. provide experiments oﬀer comparative analysis diﬀerent datasets well-known benchmarks coming repositories. paper structured follows. section oﬀers overview one-class classiﬁcation context hence providing also clear collocation proposed system. moreover section introduce main technical background material used proposed occ. successively section present details proposed occ. section show discuss experimental evaluations. finally section draw conclusions providing also pointers future directions. pimentel group current one-class methods diﬀerent categories probabilistic distance-based domain-based reconstruction-based ﬁnally information-theoretic techniques. probabilistic methods focused reconstruction generative probability density function underlying data hand. methods subdivided usual parametric non-parametric classes former include methods based identiﬁcation optimal parameters describing pre-deﬁned statistical model latter include methods based reconstruction directly data. distance-based methods operate essentially means suitable distance measure input space. techniques category grouped clusteringbased nearest neighbors based approaches. reconstruction-based methods include classical data-driven approaches neural networks subspace-based methods. domain-based methods revolve around well-established support vector data description case objective model target data suitable decision regions/surfaces optimizing speciﬁc convex optimization problem. finally information-theoretic methods rely information measures entropy divergence mutual information. intuitively non-target pattern identiﬁed alters signiﬁcantly information content data. stated before important class occs revolves around svdd method proposed duin elaborated taking inspiration well-known support vector machine classiﬁcation model svdd deﬁned terms hyper-spheres cover training data svm-like optimization problem svdd particularly exploitable since used jointly positive deﬁnite kernel functions allow generalization input domain. sch¨olkopf proposed alternative approach svdd employs hyperplane like conventional case. hyperplane positioned separated region input space containing patterns form region containing data. recent approaches include algorithms based minimum spanning tree gaussian processes random forests reader referred ref. comprehensive survey portraying state-of-the-art one-class classiﬁcation methods. according aforementioned systems categorization herein proposed could collocated intersection among probabilistic distance-based information-theoretic based approaches. notably proposed exhibits linkages system juszczak sense solution relies mst. however approach substantially diﬀerent since either information-theoretic fuzzy sets graph partitioning concepts exploit study. moreover approach dissimilarity-based opens multitude applications diﬀerent areas. last aspect recalls scheme p¸ekalska however authors diﬀerent techniques design system based linear programming prototype selection. graph-based particular minimum spanning tree based general clustering algorithms popular literature since fact graph provides powerful data abstraction sound mathematical framework. however best knowledge graph-based entropy estimation techniques design model missing literature. utilization fuzzy sets model establishes another connection so-called fuzzy one-class classiﬁers following subsections introduce main concepts used proposed paper dissimilarity representation entropy estimation modularity graph partition. detailed discussions dissimilarity representation refer reader graph-based entropy estimation ﬁnally modularity graph partition dissimilarity representation elements input dataset characterized dissimilarity measure charge synthesizing relevant commonalities among input patterns single real-valued number. prototypes called representation used develop dissimilarity matrix given every course determination suitable usually important objective. diﬀerent techniques discussed p¸ekalska duin span prototype selection strategies criteria related embedded data. using directly rows embedding vectors obtain so-called dissimilarity space representation fastest represent input data vectors starting dissimilarity values. addition common algebraic structure deﬁned dissimilarity space making approach ﬂexible. assume data sample i.i.d. realizations complete euclidean graph constructed edge connecting weighted using weight based distance |eij| α-order r´enyi entropy estimated according geometric interpretation entropic spanning graph examples graphs used literature graph steiner tree graph paper focus weighted length connecting points graph pair vertices edges. edge models binary relation among vertices. normally edge either exists exist. however case weighted graphs every edge relation. weighted adjacency matrix deﬁned wij. degree vertex aij. consider weighted graphs weights diversely partition order graph commonly intended partition vertex disjoint subsets ...ck}. well-established measure proposed one-class classiﬁer given input dataset design one-class classiﬁer applicable input domain fulﬁll requirement ﬁrst embed input data euclidean space. notably implement embedding constructing suitable dissimilarity measure. assume depends parameters alter resulting view input data derived accordingly denoted speciﬁc parameters instance embedded data represented constructing complete graph vertices denote patterns edges denote relations terms distance; edge weight given suitable euclidean metric. graph representations burden determining optimal size corresponds dimensionality subset since euclidean used also estimate α-order r´enyi entropy underlying data distribution computation entropic entropy powerful mesoscopic data descriptor also measure spread data together terms guide synthesis model. following denote one-class classiﬁcation setting model target class only. therefore need conceive inference mechanism takes account implicit uncertainty deﬁnition boundaries. synthesis optimize model searching best instances. provide objective functions ﬁrst considers linear convex combination entropy modularity calculated training favors general spread–separation data modularity constraints data group compact clusters combination yields solutions help magnifying structure second objective function designed train model cross-validation i.e. patterns. ﬁrst approach considerably faster concerns training stage although second provides eﬀective solution terms test recognition performance. figure block scheme training test stages occ; ﬁrst objective function assumed. used synthesize model. optimal parameters used generate fuzzy model ﬁrst embedded obtained means successively tested. model related testing test pattern classiﬁed considering optimal parameters yield graph constructed derived hard fuzzy partitions. fuzzy forms fuzzy described membership function quantity membership function parametrized explicitly denoted µfi. scalar determined considering statistics intra-cluster edge weights linear cost deriving input data denotes computational synthesis model cast synthesis proposed optimization w.r.t. parameters note paper make fair assumption number parameters characterizing idea determine best-performing parameters setting entropy term favors construction related graph overall distance among vertices/patterns maximized hand evaluating modularity derived partitioning constrains optimization search solutions magnify also community structure graph community term used denote compact populated cluster vertices. results graph suitably optimized derive terms compact separated clusters vertices. however evaluate modularity ﬁrst need generate partition next section describe algorithm designed derive partition whose quickly reasonable approximation optimal modularity following algorithm exploits fact need compute calculate α-order r´enyi entropy form clusters iteratively pruning edges higher euclidean distance values. construction edges likely bridges among well-separated components graph connected components used derive partition considering exactly grouping vertices. however full information edges compute modularity resulting partition terminate procedure exploit following greedy assumption. since connected ﬁrst removing edges maximum weight form interesting communities/clusters terms modularity. consequence iteration modularity value lower obtained iteration stop algorithm returning last computed partition. algorithm delivers pseudo-code herein described procedure. remove edge determine resulting connected components k))i derive corresponding partitioning considering vertex grouping k))i according evaluation overall computational overhead synthesizing using herein explained approach characterized following costs embedding graph construction entropy estimation determination graph partition ﬁnally generation membership functions. ﬁrst three components must considered suitable optimization loop last performed optimization cycle. ﬁrst term standing accounts generation computing respective euclidean distances edge weights second term quantiﬁes cost involved computation using well-known kruskal’s algorithm. last term concerns computation length. third cost given algorithm main cycle repeated maximum times. iteration derive connected components costs induce components. computed putting together algorithm dominated cubic computational complexity number training patterns. last cost depends order derived best partition. particular case average intra-cluster overall worst-case cost considering main parameter given determination partition best modularity. however since designed system operate actual cost depends also signiﬁcant impact case complex input data types. deﬁnes objective take account explicitly recognition capability synthesized model. blind partitioning derives according algorithm might suﬀer problem generating simple model i.e. drs. fact algorithm constraints partition formed clusters induce well-deﬁned community structure however since one-class setting synthesize model target class only well-formed cluster/community structure easy identify especially hard problems. relaxing imperative ﬁnding partition maximum modularity conceive another objective function therefore ﬁnal model necessarily imply best possible modularity entropy ˆhα) focusing instead solutions perform better also terms recognition. choice potentially implies obtaining complex model although observe experiments usually provides also eﬀective classiﬁcation system test set. algorithm shows pseudo-code herein described training scheme. derived exploiting basically mst-based graph partitioning approach fact derived incrementally ﬁrst removing edges likely induce well-formed clusters remove edge determine resulting connected components k))i derive considering vertex grouping k))i according evaluation generate fuzzy model evaluation objective function pmax fig. first modularity computed always exactly times. additionally fuzzy model must synthesized tested inside optimization cycle then sec. provide preliminary explanatory tests synthetically generated problems. sec. perform experiments diﬀerent datasets feature-based patterns taken repository lastly sec. discuss results obtained diﬀerent datasets containing patterns represented labeled graphs. non-synthetic datasets considered paper originally conceived multi-class classiﬁcation problems. accordingly convert one-class setting selecting class target class considering classes non-target. speciﬁed otherwise train proposed available non-target patterns. following shorten proposed system eocc variant operating training scheme described sec. denoted eocc- eocc- used refer variant operating described sec. performs roulette wheel selection two-point crossover random mutation parameters characterizing dissimilarity measure. addition genetic algorithm implements elitism strategy automatically imports ﬁttest individual next population; population size individuals mutation rate convergence criteria determined combining maximum number iterations/evolutions check evaluates best ﬁtness changed last evolutions. settings determined preliminary tuning system. fuzziﬁcation vertex clusters performed generating gaussian membership functions. width/size gaussian according computed intra-cluster average distance. gaussian membership functions symmetric described single parameter fact agreement single-parameter performed all). note performing input data necessary processing featurebased patterns. however embed also type data provide uniform view experimental results. again settings determined preliminary ﬁne-tuning stage. considering must rely appropriate test performance measure takes account scorings assigned test patterns. particular paper consider area curve computed according ref. robust statistics gives average probability target pattern ranked higher non-target one. using instead evaluate obtained confusion matrix analyzing standard measures accuracy recall precision f-measure performance fig. illustrates example target class distributed three well-separated spherical clusters. eocc trained target instances tested green blue instances simple instance fact eocc solves problem without errors obtained membership degrees target class plotted fig. fact eocc synthesizes three corresponds best solution terms modularity demonstrated fig. modularity value possible partitions derivable plotted note consider speciﬁc instance parameters input data dissimilarity measure finally fig. show entropy modularity trends iterations optimization since problem simple increments numerically small although monotonic trend clearly recognizable. worth noting early convergence iteration since fact entropy modularity hence ﬁtness stuck values. problem. pattern instances used testing centered narrow variance y-axis. target instances used testing blue violet represented green non-target instances fig. results achieved eocc- good although commits errors considering three clusters synthesized training. fig. reports membership degrees obtained eocc-. hand eocc- performs better terms since eocc- assigns always higher membership degrees target patterns. moreover also hard decisions better since zero errors committed. please note that clarity purpose validation data used eocc- shown examples. test exemplar diﬀerent views obtained considering soft binary decisions one-class classiﬁcation setting. fact still provide consistent picture correct labeling tested patterns even complex situations. additionally test yields ﬁrst insight possible diﬀerence terms recognition performance among eocc- eocc- eocc- optimized explicitly towards solutions perform better. finally fig. shows additional relevant properties eocc. fig. training/test target data distributed highly separated clusters denoting high variance y-axis. assuming weighted euclidean metric dissimilarity measure would consist real-valued vector eocc synthesizes model ﬁnding best-performing solution equal fact corresponds expected since x-coordinate plays important role magnifying separation y-coordinate compactness partition. fig. show situation training data contains isolated pattern interpretation one-class classiﬁcation setting pattern outlier since fact outliers identiﬁed test stage. consequence eocc synthesizes four drs. tab. presents details herein considered datasets juszczak provide results experimentations completed one-class setting comparison study; possible missing results retrieved please note provide consistent comparison ref. actually considered versions datasets downloaded consider versions data non-normalized normalized ensuring zero-mean unit variance component. datasets usually provide validation explicitly. since principal comparison proposed eocc results presented ref. need consider training/test split scheme. consequence case eocc- validation generated applying suitable zero-mean gaussian noise randomly selected target/non-target data sts. nonetheless demonstration reliability target non-target patterns since normalized euclidean metric weights number features characterizing dataset hand. settings herein considered reference systems refer reader ref. tab. show average results eocc- eocc- eocc- respectively considered lowhigh-dimensional datasets. results tab. show competitive performances eocc variants w.r.t. others; four seven seven datasets achieve highest considering either non-normalized normalized dataset instances. data normalization usually aﬀect results exception dataset eocc performances degrades signiﬁcantly; worth noting observed also competitors. results tab. still denote good eocc performances although score best datasets results datasets general comparable although observe performance degradation three systems however results never statistically worse competitors. nonetheless worth pointing that although eocc figure fig. instances used training green used testing. non-target instances represented blue. fig. shows calculated membership values target class assigned sdf. fig. shows trend modularity possible partitions derivable entropic mst. peak recognizable expected best partition order i.e. three. finally fig. shows entropy modularity trends iterations optimization relies entropy estimator turn based data observe severe performance breakdown processing high-dimensional data usually observed pdf-based methods follows fact initially perform whose experiments expect performance degradation training data size grows however contrary achieve good results datasets demonstrating eﬀectiveness combined dissimilarity graph based approach adopted eocc. standard deviations general denoting highly stable reliable results. repository variegated database containing many diﬀerent datasets labeled graphs consider datasets namely aids grec letter-low letter-high mutagenicity protein. tab. shows relevant information regarding data. datasets already contain suitable validation figure fig. blue violet test patterns belong target class. blue ones correctly classiﬁed violet misclassiﬁed terms hdf. fig. show respectively membership degrees assigned test patterns eocc- eocc-. table test results low-dimensional datasets. results show average standard deviations signiﬁcance test statistically signiﬁcant results bold; available denoted figure fig. depicts situation training/test target data shows high variance y-axis; best-performing parameters selected accordingly. fig. example training target data contains isolated pattern solves assignment problem vertices greedy strategy. twec characterized three parameters ranging controlling importance insertion deletion substitution edit operations. tab. show obtained results. knowledge results available comparison considering one-class classiﬁcation setting datasets. however report results demonstrate wide straightforward applicability eocc future experimental comparisons. herein reported results conﬁrm eocc- eﬀective eocc- especially considering harder datasets notably dataset known hard multi-class case); tab. obtained denotes nearly randomized classiﬁer. however case system correctly rejects non-target patterns rejects also target instance. general eocc- eocc- reduced accuracy. number synthesized lower eocc-. aspect magniﬁes potential eocc-. fact diﬀerent experiments results eocc- eocc- comparable although eocc- solves problem much lower computing time fewer i.e. less resources. important aspect evaluated basis speciﬁc application hand. standard deviations acceptable conﬁrming stability system. paper proposed evaluated novel one-class classiﬁcation system called eocc. classiﬁer designed making interplay diﬀerent techniques. dissimilarity representation exploited make system general-purpose. graph-based techniques employed estimating information-theoretic quantities deriving model classiﬁer. decision regions forming model obtained exploiting concept modularity graph partition. decision regions hence deﬁned clusters vertices equipped suitable membership functions. allows provide hard soft decisions recognition test patterns. validated system types benchmarks diﬀerent feature-based datasets datasets labeled graphs. overall comparisons made datasets demonstrate validity approach respect several state-of-the-art one-class classiﬁcation systems taken literature. results datasets prove versatility eﬀectiveness system processing labeled graphs one-class classiﬁcation setting useful situations patterns interest known. patterns termed target instances. solution applicable virtually context based dissimilarity representation. aspect particular interest since allows user model patterns according suitable representation application hand. future directions include usual improvements variants herein discussed system instance used complete euclidean graph representation estimate entropy data mapped dissimilarity space. sparse representation could become handy processing large volume data. therefore future evaluate entropic spanning graphs based so-called graphs. global optimization techniques course interest well membership function models decision regions particular attention devoted issue interpretability system. usually researchers focus improving performances pattern recognition systems pure technical viewpoint paying attention generalization capability computing speed. however facing applications true scientiﬁc interest systems satisfy requirement producing results using inference rules/functions easily understandable humans. fact would allow ﬁeld experts easily gather useful insights underlying problem. system suitable mission since conceived exploiting fuzzy sets based techniques. therefore important future goal evaluation system speciﬁc viewpoint.", "year": 2014}