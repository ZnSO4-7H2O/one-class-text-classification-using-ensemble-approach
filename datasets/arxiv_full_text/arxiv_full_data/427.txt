{"title": "Houdini: Fooling Deep Structured Prediction Models", "tag": ["stat.ML", "cs.AI", "cs.CR", "cs.CV", "cs.LG"], "abstract": "Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. We successfully apply Houdini to a range of applications such as speech recognition, pose estimation and semantic segmentation. In all cases, the attacks based on Houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.", "text": "generating adversarial examples critical step evaluating improving robustness learning machines. existing methods work classiﬁcation designed alter true performance measure problem hand. introduce novel ﬂexible approach named houdini generating adversarial examples speciﬁcally tailored ﬁnal performance measure task considered combinatorial non-decomposable. successfully apply houdini range applications speech recognition pose estimation semantic segmentation. cases attacks based houdini achieve higher success rate based traditional surrogates used train models using less perceptible adversarial perturbation. deep learning redeﬁned landscape machine intelligence enabling several breakthroughs notoriously difﬁcult problems image classiﬁcation speech recognition human pose estimation machine translation successful models permeating nearly segments technology industry self-driving cars automated dialog agents becomes critical revisit evaluation protocol deep learning models design ways assess reliability beyond traditional metrics. evaluating robustness neural networks adversarial examples step direction adversarial examples synthetic patterns carefully crafted adding peculiar noise legitimate examples. indistinguishable legitimate examples human demonstrated strong ability cause catastrophic failure state classiﬁcation systems existence adversarial examples highlights potential threat machine learning systems large limit adoption security sensitive applications. triggered active line research concerned understanding phenomenon making neural networks robust adversarial examples crucial reliably evaluating improving robustness models ideally must generated alter task loss unique application considered directly. instance adversarial example crafted attack speech recognition system designed maximize word error rate targetted system. existing methods generating adversarial examples exploit gradient given differentiable loss function guide search neighborhood legitimates examples unfortunately task loss several structured prediction problems interest combinatorial non-decomposable quantity amenable gradient-based methods generating adversarial example. example metric evaluating human pose estimation percentage correct keypoints automatic figure cause network generate minion segmentation adversarially perturbed version original image. note original perturbed image indistinguishable. speech recognition systems assessed using word error rate. similarly quality semantic segmentation measured intersection union ground truth prediction. evalutation measures non-differentiable. solutions obstacle supervised learning kinds. ﬁrst route consistent differentiable surrogate loss function place task loss surrogate guaranteed converge task loss asymptotically. second option directly optimize task loss using approaches direct loss minimization strategies severe limitations. differentiable surrogates satisfactory classiﬁcation relationship surrogates classiﬁcation accuracy well established picture different above-mentioned structured prediction tasks. indeed known consistency guarantee surrogates traditionally used problems designing surrogate nontrivial problem dependent. best expect high positive correlation proxy task loss. direct minimization approaches computationally involved require solving computationally expensive loss augmented inference parameter update. also notoriously sensitive choice hyperparameters. consequently harder generate adversarial examples structured prediction problems requires signiﬁcant domain expertise little guarantee success surrogate tightly approximate task loss. results. work introduce houdini ﬁrst approach fooling gradient-based learning machine generating adversarial examples directly tailored task loss interest combinatorial non-differentiable. show tight relationship houdini task loss problem considered. present ﬁrst successful attack deep automatic speech recognition system namely deepspeech- based architecture generating adversarial audio ﬁles distinguishable legitimate ones human also demonstrate transferability adversarial examples speech recognition fooling google voice black attack scenario adversarial example generated model distinguishable legitimate human leads invalid transcription google voice application also present ﬁrst successful untargeted targetted attacks deep model human pose estimation similarly validate feasibility untargeted targetted attacks semantic segmentation system show make system hallucinate arbitrary segmentation choice given image. figure shows experiment cause network hallucinate minion. cases approach generates better quality adversarial examples different surrogates without additional computational overhead thanks analytical gradient houdini. adversarial examples. empirical study szegedy ﬁrst demonstrated deep neural networks could achieve high accuracy previously unseen examples vulnerable small adversarial perturbations. ﬁnding recently aroused keen interest community several studies subsequently analyzed phenomenon various approaches proposed improve robustness neural networks closely related work different proposals aiming generating better adversarial examples given input example adversarial example perturbed version original pattern small enough undistinguishable represents strength adversary. assuming loss function differentiable shaham propose take ﬁrst order taylor expansion compute solving following simpler problem \u0001sign corresponds fast gradient sign method instead obtain often normalized. optionally perform iterations steps using smaller norm. involved strategy several variants methods concerned generating adversarial examples assuming differentiable loss function therefore directly applicable task losses interest. however used combination proposal derives consistent approximation task loss analytical gradient. task loss minimization. recently several works focused directly minimizing task loss. particular mcallester presented theorem stating certain perceptron-like learning rule involving feature vectors derived loss-augmented inference directly corresponds gradient task loss. algorithm performs well practice extremely sensitive choice hyper-parameter needs inference operations training iteration. generalized notion ramp loss binary classiﬁcation structured prediction proposed tighter bound task loss structured hinge loss. update rule structured ramp loss similar update rule direct loss minimization algorithm similarly needs inference operations training iteration. keshet generalized notion binary probit loss structured prediction case. probit loss surrogate loss function naturally resulted pac-bayesian generalization theorems. deﬁned follows d-dimensional isotropic normal random vector. stated ﬁnite sample generalization bounds structured probit loss showed strongly consistent. strong consistency critical property surrogate since guarantees tight relationship task loss. instance attacker given system expect deteriorate task loss deteriorates consistent surrogate gradient structured probit loss approximated averaging samples unit-variance isotropic normal distribution sample inference perturbed parameters computed. hundreds thousands inference operations required iteration gain stability gradient computation. hence update rule computationally prohibitive limits applicability structured probit loss despite desirable properties. propose loss named houdini. shares desirable properties structured probit loss suffering limitations. like structured probit loss unlike surrogates used structured prediction tightly related task loss. therefore allows reliably generate adversarial examples given task loss interest. unlike structured probit loss like smooth surrogates analytical gradient hence require single inference update rule. next section presents details proposal. consider neural network parameterized task loss given problem assume target score output network example network’s decoder predicts highest scoring target task loss often combinatorial quantity hard optimize hence replaced differentiable surrogate loss denoted different algorithms different surrogate loss functions structural uses structured hinge loss conditional random ﬁelds loss etc. propose surrogate named houdini deﬁned follows given example words houdini product terms. ﬁrst term stochastic margin probability difference score actual target predicted target smaller reﬂects conﬁdence model predictions. second term task loss given targets independent model corresponds ultimately interested maximizing. houdini lower bound task loss. indeed denoting difference scores assigned network ground truth prediction pγ∼n smaller hence probability goes equivalently score assigned network target grows without bound houdini converges task loss. unique property enjoyed surrogates used applications interest work. ensures houdini good proxy task loss generating adversarial examples. houdini place task loss problem following resort ﬁrst order approximation requires gradient houdini respect input latter obtained following chain rule equation provides simple analytical formula computing gradient houdini respect input hence efﬁcient obtain gradient respect input network backpropagation. gradient used combination gradient-based adversarial example generation procedure ways depending form attack considered. untargeted attack want change prediction network without preference ﬁnal prediction. case alternative target used targetted attack desired ﬁnal prediction. also note that score predicted target close ground truth small expect trained network want fool evaluate effectiveness houdini loss context adversarial attacks neural models human pose estimation. compromising performance systems desirable manipulating surveillance cameras altering analysis crime scenes disrupting human-robot interaction fooling biometrical authentication systems based gate recognition. pose estimation task formulated follows given single image person determine correct positions several pre-deﬁned points typically correspond skeletal joints. practice figure convergence dynamics pose estimation attacks perturbation perceptibility iterations pckh. iterations proportion re-positioned joints perceptibility. table human pose estimation comparison empirical performance adversarial attacks adversarial pose re-targeting. pckh= corresponds correctly detected keypoints pckhlim denotes value upon convergence iterations). performance frameworks measured percentage correctly detected keypoints i.e. keypoints whose predicted locations within certain distance corresponding target positions predicted desired positions given joint respectively head size person threshold number annotated keypoints. pose estimations good example problem observe discrepancy training objective ﬁnal evaluation measure. instead directly minimizing percentage correctly detected keypoints state-of-the-art methods rely upon dense prediction heat maps i.e. estimation probabilities every pixel corresponding keypoint locations. models trained binary cross entropy softmax losses applied every pixel output space separately plane corresponding every point. ﬁrst experiment attack state-of-the-art model single person pose estimation based hourglass networks minimize value pckh. metric given minimal perturbation. task choose pixel coordinate heatmap corresponding argmax value vector perform optimization iteratively till convergence update rule gradients respect input empirical comparison similar attack based minimizing training loss given part table perform evaluations validation subset mpii dataset consisting images deﬁned evaluate perceived degree perturbation perceptibility expressed addition report structural similarity index known correlate well visual perception image structure human. shown table adversarial examples generated houdini similar legitimate examples ones generated training proxy untargeted attacks optimized convergence perturbation generated houdini less perceptible obtained mse. measured number iterations required completely compromise pose estimation system depending loss function used. convergence dynamics illustrated fig. figure examples successful targetted attacks pose estimation system. despite important difference images selected possible make network predict wrong pose adding imperceptible perturbation. show employing houdini makes possible completely compromise network performance given target metric several iterations using training surrogate generating adversarial examples could achieve similar success level attacks even iterations optimization process. observation underlines importance loss function used generate adversarial examples structured prediction problems. show example figure interestingly predicted poses corrupted images still make overall plausible impression failing formal evaluation entirely. indeed mistakes made compromised network appear natural typical pose estimation frameworks imprecision localization confusion right left limbs combining joints several people etc. second experiment perform targetted attack form pose transfer i.e. force network hallucinate arbitrary pose experimental setup follows given pair images force network output ground truth pose picture input image vice versa. task challenging depends similarity original target poses. surprisingly targetted attacks still feasible even ground truth poses different. figure shows example model predicts pose human body horizontal position adversarially perturbed image depicting standing person similar experiment persons standing sitting positions respectively also shown ﬁgure semantic segmentation uses another customized metric evaluate performance namely mean intersection union measure deﬁned averaging classes stand true positive false positive false negative respectively taken separately class. compared per-pixel accuracy appears overoptimistic highly unbalanced datasets per-class accuracy under-penalizes false alarms non-background classes metric favors accurate object localization tighter masks table semantic segmentation comparison empirical performance targetted untargeted adversarial attacks segmentation systems. miou/ denotes performance drop according target metric mioulim corresponds convergence termination iterations. ssim higher better perceptibility lower better. houdini based attacks less perceptible. figure targetted attack semantic segmentation system switching target segmentation images cityscapes dataset last columns respectively zoomed-in parts perturbed image adversarial perturbation added original one. bounding boxes models trained per-pixel softmax multi-class cross entropy losses depending task formulation i.e. optimized mean per-pixel per-class accuracy instead miou. primary targets adversarial attacks group applications self-driving cars robots. previously explored adversarial attacks context semantic segmentation. however exploited proxy used training network. perform series experiments similar ones described sec. show targetted untargeted attacks semantic segmentation model. pre-trained dilation model semantic segmentation evaluate success attacks validation subset cityscapes dataset ﬁrst experiment directly alter target miou metric given test image targetted untargeted attacks. shown table houdini allows fooling model least well training proxy using less perceptible. indeed houdini based adversarial perturbations generated alter performance model less noticeable noise created nll. second experiments consists targetted attacks. altering input image obtain arbitrary target segmentation network response. figure show instance attack segmentation transfer setting i.e. target segmentation ground truth segmentation different image. clear type attack still feasible small adversarial perturbation figure depicts challenging scenario target segmentation arbitrary again make network hallucinate segmentation choice adding barely noticeable perturbation. evaluate effectiveness houdini concerning adversarial attacks automatic speech recognition system. traditionally systems composed different components component trained separately. recently research focused deep learning based end-to-end models. type models input speech segment output transcript additional post-processing. work deep neural network model similar architecture presented system composed convolutional layers followed seven layers bidirectional lstm fully connected layer. optimize connectionist temporal classiﬁcation loss function speciﬁcally designed systems. model gets input spectrograms outputs transcript. standard evaluating metric speech recognition word error rate character error rate metrics derived levenshtein distance number substitutions deletions insertions divided target length. model achieves word error rate character error rate librispeech dataset additional language modeling. order houdini attacking end-to-end model need scores predicting respectively. recall speech recognition target sequence characters. hence score possible paths output fortunately forward-backward algorithm core score given label experiment generated audio samples adversarial examples performed test humans. test standard assess detectable differences choices sensory stimuli. present human audio samples samples either legitimate adversarial version sound. samples followed third sound randomly selected either next human must decide whether every audio sample executed test least nine different persons. overall adversarial examples could distinguished original ones humans therefore generated examples statistically signiﬁcantly distinguishable human ear. subsequently indistinguishable adversarial examples test robustness systems. untargeted attacks ﬁrst experiment compare network performance attacking houdini ctc. generate adversarial example loss functions every sample clean test librispeech experienced different distortion levels adversarial examples orty \"answer ultimate question life universe everything.\" results summarizes notice houdini causes bigger decrease regarding distortions values tested. particular small adversarial perturbation word error rate caused attack houdini larger obtained ctc. similarly character error rate caused houdini-based attack larger ctc-based one. fig. shows original adversarial spectrograms single speech segment. shows spectrogram original sound shows spectrogram adversarial one. visually undistinguishable. targetted attacks push model towards predicting different transcription iteratively. case input model iteration adversarial example iteration corresponding transcription samples shown table notice setting phonetically model tends predict wrong transcriptions necessarily similar selected target. however picking phonetically close ones model acts expected predict transcription phonetically close overall targetted attacks seem much challenging dealing speech recognition systems consider artiﬁcial visual systems pose estimators semantic segmentation systems. figure examples iteratively generated adversarial examples targetted attacks. case model predicts exact target transcription original example. targetted attacks difﬁcult speech segments phonetically different. black attacks lastly experimented black attack attacking system access models’ gradients its’ predictions. figure show examples google voice application predict transcript original adversarial audio ﬁles. original audio adversarial versions generated deepspeech- based model distinguishable human according test. play audio clip front android based mobile phone report transcription produced application. seen google voice could almost transcriptions correct legitimate examples largely fails produce good transcriptions adversarial examples. images adversarial examples speech recognition also transfer models. introduced novel approach generate adversarial examples tailored performance measure unique task interest. applied houdini challenging structured prediction problems pose estimation semantic segmentation speech recognition. case houdini allows fooling state learning systems imperceptible perturbation hence extending adversarial examples beyond task image classiﬁcation. eyes ears hear mind believes. authors thank alexandre lebrun pauline camille couprie valuable help code experiments. also thank antoine bordes laurens maaten nicolas usunier christian wolf herve jegou yann ollivier neil zeghidour lior wolf insightful comments early draft paper. references amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english mandarin. international conference machine learning pages graves fernández gomez schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning pages keshet mcallester. generalization bounds consistency latent structural probit ramp loss. advances neural information processing systems pages keshet mcallester hazan. pac-bayesian approach minimization phoneme error rate. acoustics speech signal processing ieee international conference pages ieee panayotov chen povey khudanpur. librispeech corpus based public domain audio books. acoustics speech signal processing ieee international conference pages ieee papernot mcdaniel goodfellow berkay celik swami. practical black-box attacks deep learning systems using adversarial examples. arxiv preprint arxiv.", "year": 2017}