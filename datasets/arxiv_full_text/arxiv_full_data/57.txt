{"title": "DopeLearning: A Computational Approach to Rap Lyrics Generation", "tag": ["cs.LG", "cs.AI", "cs.CL", "cs.NE", "I.2.7; H.3.3"], "abstract": "Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.", "text": "distinguished music genres formal structure present lyrics makes lyrics rhyme well hence provides better music. literature professor adam bradley compares popular music traditional poetry stating popular lyrics lack much formal structure literary verse crafts intricate structures sound rhyme creating scrupulously formal poetry composed today approach problem lyrics creation information-retrieval perspective. assume access large repository rap-song lyrics. paper dataset containing half million lines lyrics diﬀerent artists. view lyrics-generation problem task identifying relevant next line. consider song partially constructed treat ﬁrst lines song query. task identify relevant next line collection candidate lines respect query. following approach lyrics constructed line line combining lyrics diﬀerent artists order introduce novelty. advantage approach evaluate performance generator measuring well predicts existing songs. conceptually could approach lyrics-generation problem word-by-word construction increase novelty approach would require signiﬁcantly complex models leave future work. work lies intersection areas computational creativity information retrieval. approach assume users certain concept mind formulated sequence lines information need missing lines composing song. information need factual answer; nevertheless users able assess relevance response provided system. relevance response depends factors include rhyming vocabulary unexpectedness semantic coherence humor. tony veale illustrates linguistically creative uses information retrieval e.g. metaphor generation. argues phrases extracted large corpora used readymade found objects like objets trouv´es arts writing lyrics requires creativity construct meaningful interesting story lyrical skills produce complex rhyme patterns form cornerstone good ﬂow. present lyrics generation method captures aspects. first develop prediction model identify next line existing lyrics candidate next lines. model based machinelearning techniques ranksvm algorithm deep neural network model novel structure. results show prediction model identify true next line among randomly selected lines accuracy i.e. times likely random. second employ prediction model combine lines existing songs producing lyrics rhyme meaning. evaluation produced lyrics shows terms quantitative rhyme density method outperforms best human rappers lyrics generator deployed online tool called deepbeat performance tool assessed analyzing usage logs. analysis shows machine-learned rankings correlate user preferences. emerging hobby african american youth music quickly evolved mainstream music genre several artists frequenting billboard rankings. objective study problem computational creation lyrics. interest problem motivated diﬀerent diﬀerent perspectives. first interested analyzing formal structure lyrics developing model lead generating artistic ∗when used adjective dope means cool nice awesome. take fresh meanings used context. computational perspective major challenge generating lyrics produce semantically coherent lines instead merely generating complex rhymes. paul edwards puts artist takes time craft phrases rhyme intricate ways still gets across message song usually seen mark highly skilled result record results computer vision deep neural networks become popular tool feature learning. avoid hand-crafting number semantic grammatical features introduce deep neural network model maps sentences highdimensional vector space. type vector-space representations attracted much attention recent years exhibited great empirical performance tasks requiring semantic analysis natural language features extract analyzed lyrics tailored lyrics similar approach could applied generate lyrics music genres. furthermore proposed framework could form basis several text-synthesis problems generation text conversation responses. practical extended applications include automation tasks customer service sales even news reporting. introduce several useful features predicting next line song hence generating lyrics. particular developed deep neural network model capturing semantic similarity lines. feature carries predictive power features studied. rest paper organized follows. start brief discussion relevant work. next discuss domain lyrics introduce dataset used describe rhymes analyzed. proceed describing task nextline approach solving including used features neural language model results experiments. apply resulting model task lyrics generation showing also examples generated lyrics. ﬁnal sections discuss tasks model conclusions work. study human-generated lyrics interest academics ﬁelds linguistics music artiﬁcial rhyme generation also relevant various subﬁelds computer science. relevant literature found under domains computational creativity information extraction natural language processing. additionally relevant hirjee brown develop probabilistic method inspired local alignment protein homology detection algorithms detecting rhymes. algorithm obtains high rhyme detection performance requires training dataset labeled rhyme pairs. introduce simpler rule-based approach section seemed suﬃcient purposes. hirjee brown obtain phonetic transcription lyrics applying pronouncing dictionary hand-crafted rules handle slang words text-to-phoneme rules handle out-ofvocabulary words whereas open-source speech synthesizer espeak produce transcription. computational generation lyrics previously studied works adopt machine-translation approach whereas view information-retrieval problem. furthermore deployed lyrics generator openly accessible tool assess performance wild. automated creation lyrics also viewed problem within research ﬁeld computational creativity i.e. study computational systems exhibit behaviors deemed creative according boden creativity ability come ideas artifacts surprising valuable three diﬀerent types creativity. work falls class combinatorial creativity creative results produced novel combinations familiar ideas. combinatorial approaches used create poetry predominantly based idea copying grammar existing poetry substituting content words ones shown document ranking accuracy signiﬁcantly improved combining multiple features machine-learning algorithms instead using single static ranking pagerank learning-to-rank approach popular nowadays many methods developed paper ranksvm algorithm combining diﬀerent relevance features next-line prediction problem basis rap-lyrics generator. neural networks recently applied various related tasks. instance recurrent neural networks shown promise predicting text sequences applications include tasks information extraction information retrieval indexing question answering also approached using deep learning questions answers latent semantic space either generating response selecting neural network approach similarity response selection also learn mapping hidden space. hand network architecture uses feed-forward net—a suitable choice context sentences often relatively short equal length. also generation lyrics typically considered responseselection task interpret one. section ﬁrst describe typical structure lyrics well diﬀerent rhyme types often used. information basis extracting useful features next-line prediction problem discuss next section. introduce method automatically detecting rhymes deﬁne measure rhyme density. also present experimental results assess validity rhyme-density measure. various diﬀerent rhyme types perfect rhyme alliteration consonance employed lyrics common rhyme type nowadays versatility assonance rhyme perfect rhyme words share exactly sound slang gang whereas assonance rhyme vowel sounds shared. example words crazy baby different consonant sounds vowel sounds seen phonetic representations kôeisi beibi. typical song follows pattern alternating verses choruses. turn consist lines break individual words ﬁnally syllables. line equals musical typically consists four beats setting limits many syllables single line. verses constitute main body song often composed lines. consecutive lines joined rhyme typically placed lines appear anywithin lines. rhyme maintained couple lines even throughout entire verse. verses algorithm generates rhyme kept ﬁxed four consecutive lines unless otherwise speciﬁed user automatically detect multisyllabic assonance rhymes lyrics given text. this ﬁrst obtain phonetic transcription lyrics applying textto-phonemes functionality open source speech synthesizer espeak. synthesizer assumes typical american– english pronunciation. phonetic transcription detect rhymes ﬁnding matching vowel phoneme sequences ignoring consonant phonemes spaces. order quantify technical quality lyrics rhyming perspective introduce measure rhyme density lyrics. simpliﬁed description computation measure provided below make rhyme densities diﬀerent artists comparable normalize lyrics removing duplicate lines within single song cases lyrics contain chorus repeated many times whereas cases might chorus depending user provided lyrics. songs like intro tracks often contain regular speech rather rapping hence removed songs whose title following words introoutroskit interlude. results surprising; instance rakim ranked second known pioneering internal rhymes multisyllabic rhymes. hand limitation results artists like eminem multisyllabic rhymes construct often bending words high list treat training data learn model consecutive lines lyrics. then given seed lines model identify best next line among candidate next lines taken lyrics repository. method used construct song line-by-line appending relevant lines diﬀerent songs. relevance score. performance method evaluated using standard information retrieval measures mean reciprocal rank. relevance score linear model similarity features query endrhyme number matching vowel phonemes lines i.e. last line spaces consonant phonemes ignored instance following phrases would three rhyme vowels common. published results shown table online artist called ahmen contacted asking compute rhyme density lyrics debut album. revealing rhyme densities individual songs sent asked rapper rank lyrics starting technical according think used longest rhymes. rankings produced artist algorithm shown table compute correlation artist produced algorithm produced rankings applying kendall rank correlation coeﬃcient. assuming ties indicated artist decided unfavorably algorithm correlation rankings would still null hypothesis rankings independent rejected suggests rhyme density measure adequately captures technical quality lyrics. bow. first tokenize lines represent last line words apply procedure obtain words candidate line measure semantic similarity lines computing jaccard similarity corresponding bags words incorporate longer context could relevant next line want identify. experimented various values found using previous lines gives best results. lsa. bag-of-word models able cope synonymy polysemy. enhance model capabilities simple latent semantic analysis approach. preprocessing step remove stop words words appear less three times. training data form line–term matrix compute rank- approximation matrix. line represented term vector projected low-rank matrix space. similarity lines computed cosine similarity projected vectors. knowledge possible features never comprehensive designing solid extractor features semantic grammatical similarity would extremely time-consuming. thus attempting learn additional features appears promising approach. since neural networks vast number available parameters learn complex nonlinear mappings experiment whether could used automatically extract relevant features. design neural network learns text sequences predict relevance candidate next line given previous lines. brief neural network starts ﬁnding distributed representations words. combined distributed representations lines combined vector representations multiple lines last line real line randomly sampled line. based vector representation text network learns predict whether believes last line suitable next line candidate not. network architecture. core predictor multi-layered fully-connected neural networks trained backpropagation. network structure originally inspired work collobert model diﬀers substantially theirs. particular included input transformations input consists multiple sentences. structure model illustrated figure start preprocessing text format easily handled neural network removing non-ascii characters one-word lines stemming lower-casing words. choice neural network architecture requires neural model small batches corresponding gradient descent update performed weights network. give network negative examples follow approach similar generate fake line examples choosing every second example candidate line uniformly random lyrics. technical choices necessary constructing training network. word-speciﬁc neural layers line-speciﬁc layer layers rectiﬁed linear units activation function. minibatch size adaptive learning rate adadelta regularize network dropout train network epochs machine taking advantage theano library analyzing hyperparameters improve results especially going previous line moderately large context using larger input window i.e. giving network words line. manual evaluation. understanding neural network learnt manually analyze random sentences random candidate line falsely classiﬁed real next line. lines diﬃcult distinguish whether real random line follow. often relates rapper changing topic especially moving verse another. remaining lines notice neural network succeeded identifying instances rhyme three instances repeating words instances consistent sentence styles instance consistent grammar instance pronunciation words similar previous next line. order detect rhymes network would need learn phonetic representation words problem developed features presented section ranksvm algorithm takes type data input tries learn linear model gives relevance scores candidate lines. relevance scores respect preference relations given training data algorithm. relevance scores given advantage simple linear model trained eﬃciently. employ svmrank software learning furthermore interpret weights importance values features. therefore ﬁxed line lengths also remove words exceeding words shorter lines. build samples format candidate next line previous lines lines previous lines paddings. ﬁrst layers network word-speciﬁc perform transformation word text regardless position. word start exponential moving average transformation turns character sequence vector. simpliﬁed example shown figure transformation compared one-hot encoding robust diﬀerent spellings coping well instance slang spelling errors. transformation works follows create zerovector length possible characters. starting ﬁrst character word choose corresponding location vector. increment value value proportional character’s position word counting beginning word. wordrepresentation avoid always giving larger weight beginning word also concatenate vector transformation backwards vector character counts word. following transformation feed word vector fully-connected neural network. word-speciﬁc vectors next concatenated word order single line-vector line-level network. next line-vectors concatenated candidate next-line placed ﬁrst preceding lines presented order occurrence. vector ﬁnal layer output softmaxfunction outputs binary prediction indicating whether believes line follows next not. ensemble model activation softmax corresponding conﬁdence network line next lyrics. while achieve best results using transformation traditional ‘one-hot’ vector approach yields nearly good results. model learns transformation recurrent neural network could also tested future work. experimental setup following. split lyrics artist randomly training validation test ranksvm model neural network model learned using training varyuse candidate containing true next line randomly chosen lines. performance measured using mean rank mean reciprocal rank recall value varied. results computed based random sample queries. table shows test performance diﬀerent feature sets. feature alone yields mean rank thus carries predictive power. best individual feature respect mean rank output neural network model. however look recall best performance obtained endrhyme works well cases overall inferior since cases consecutive lines rhyme all. combining features achieve mean rank pick true next line accuracy. probability pick true next line random features combined taking linear combination values according equation enable generation lyrics real time also tested employing features fast evaluate i.e. fastfeats linelength endrhyme endrhyme- fastfeatsnn fastfeats latter feature performance almost identical using full feature even fastfeats works relatively well. lyrics generation based idea selecting relevant line given previous lines repeated whole lyrics generated. algorithm deepbeat summarized algorithm relevance scores candidate lines computed using ranksvm algorithm described section instead selecting candidate strictly highest relevance score ﬁlter candidates according rhyme_ok function. checks consecutive lines song words. although rhyming words observed contemporary lyrics always considered valid technique straightforward produce long rhymes repeating phrases lead uninteresting lyrics. online demo lyrics generation algorithm available deepbeat.org. tool built order make generator available public provide users easy ways customizing generated lyrics collect limited usage logs order evaluate improve algorithm. website launched november june visited users. initial launch project started collaborating musicians record ﬁrst songs written deepbeat showed importance giving user suﬃcient customization capabilities instead merely outputting complete lyrics. mind designed online demo users deﬁne keywords must appear generated lyrics; algorithm give suggestions next line pick best suggestion manually; write lines themselves. user example write ﬁrst line algorithm generate remaining lines. interesting mode usage noticed users adopting write every line generate every other. ﬁrst music video available https//youtu.be/jshymhko collaboration read https//howwegettonext.com/deepbeat-what-happenswhen-a-robot-writes-rhymes-for-rappers-dcﬀ user generates lyrics line-by-line asking suggestions deepbeat selected lines. section show evaluate algorithm using data. conveniently also logs reﬁne learned models employing ranksvm approach done candidate retrieval. results table show endrhyme alone good predictor. therefore deﬁne candidate next lines best rhyming lines instead random lines. candidate retrieved quickly without evaluating lines database. conveniently formulated problem ﬁnding strings longest common preﬁx respect query string follows solve longest common preﬁx problem ﬁrst sorting phoneme string preprocessing step second employing binary search line longest common preﬁx long) third taking lines complexity approach online feature selection. default deepbeat.org employs fastfeats feature generation -line verse takes seconds. user additionally enable feature case algorithm retrieve top- lines based fastfeats rerank lines based fastfeatsnn. evaluate lines using since much heavier compute features since recall percentage points lower fastfeats compared fastfeatsnn. feature could used heavily acquiring server enables computation parallelization lyrics generated deepbeat evaluated introduced section rhyme density measure measuring correlation relevance scores assigned deepbeat human preferences recorded deepbeat.org. single generate hundred -bar verses random seed lines. randomly selected example verse shown appendix rhyme density hundred verses quite remarkably higher rhyme density ranked human rapper inspectah deck. candidate could found even faster building trie data structure phoneme strings. however experiments binary search approach fast enough memory-eﬃcient. order evaluate algorithm performed online experiment using demo. idea employ approach used optimizing search engines clicked search result logged following deepbeat.org user clicks suggest rhyming line button suggested candidate next lines shown user. wanted often line selected user assigned higher score lines selection. hypothesis larger absolute diﬀerence algorithm-assigned relevance scores lines likely user would pick line preferred algorithm. initial data collected website noticed users likely select line higher appears list suggestions. furthermore users tend prefer ﬁfth line since often appears location screen suggest rhyming line button user playing around tool withputting much thought content suggestions user likely select ﬁfth suggestion. challenging type biases entirely conducting uncontrolled experiment wild mitigate biases shuﬄed order suggested lines removed ﬁrst three selections user since assumed beginning users likely play tool without thinking much. moreover wanted create variability among suggestions might almost equally good deﬁned suggested lines lines ranked three randomly picked lines range avoid degrading usability tool much applied aforementioned manipulations enabled user. however also stored text selected line previous lines enable computation relevance scores fastfeatsnn post-processing step. total experiment resulted pairwise preferences users. results experiment shown figure conﬁrm hypothesis higher diﬀerence between relevance scores lines likely users select line evaluated suitable algorithm. furthermore including feature improves evaluations seen studying diﬀerence data series ﬁgure. conclude learned ranksvm model generalizes able successfully learn human preferences existing lyrics developed deep neural network important component model. developed deepbeat algorithm lyrics generation. lyrics generation formulated information retrieval task objective relevant next line given previous lines considered query. algorithm extracts three types features lyrics—rhyme structural semantic features—and combines employing ranksvm algorithm. semantic features developed deep neural network model single best predictor relevance line. quantitatively evaluated algorithm three measures. first evaluated prediction performance measuring well algorithm predicts next line existing song. true next line identiﬁed among randomly selected lines accuracy i.e. times likely random ranked accuracy. second introduced rhyme density measure showed deepbeat outperforms human rappers terms length frequency rhymes produced lyrics. validity rhyme density measure assessed conducting human experiment showed measure correlates rapper’s notion technically skilled lyrics. third lyrics generator deployed tool analysis usage logs showed machine evaluations candidate next lines correlate user preferences. would like thank stephen fenech developing front deepbeat.org jelena luketina miquel perell´o nieto vikram kamath useful comments manuscript. figure probability deepbeat.org user select line higher score pair lines given score diﬀerence lines. user preferences correlate scores assigned deepbeat. according boden creativity ability come ideas artifacts surprising valuable. here produced lyrics whole novel construction lines lyrics picked diﬀerent original lyrics even individual lines novel. lyrics produced method likely least surprising original lyrics. value lyrics diﬃcult estimate objectively. obvious factors contributing value poetic properties lyrics especially rhythm rhyme quality language meaning message lyrics. rhyme rhythm controlled relative ease even outperforming human rappers demonstrated experiments. quality individual lines exactly good dataset used. meaning semantics hardest part computational generation. applied standard bag-of-words methods additionally introduced deep neural network model order capture semantics lyrics. importance features proven experimental results next line prediction problem online experiment. features together contribute towards semantic coherence produced lyrics even full control meaning still missing. task predicting next line challenging even human model performs relatively well task. used models lyrics prediction components understand semantics could relevant also text processing tasks instance conversation prediction. work opens several lines future work. would interesting study automatic creation story lines analyzing existing songs novel lines modifying existing lines creating scratch. even more would exciting fully automatic would generate lyrics synthetically based input receives outside world. alternatively ﬁndings paper could transferred text processing tasks conversation predicinformation retrieval linguistic creativity. proceedings annual meeting association computational linguistics human language technologies–volume pages association computational linguistics addanki saers beloucif. learning freestyle challenge-response induction transduction rule segmentation. proceedings empirical methods natural language processing conference pages", "year": 2015}