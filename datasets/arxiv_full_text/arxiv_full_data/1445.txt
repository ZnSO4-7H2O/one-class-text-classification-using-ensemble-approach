{"title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "tag": ["cs.AI", "cs.CL", "stat.ML"], "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.", "text": "jason weston antoine bordes sumit chopra alexander rush bart merri¨enboer armand joulin tomas mikolov facebook research broadway york {jaseabordesspchopratmikolovsasharbartvm}fb.com long-term goal machine learning research produce methods applicable reasoning natural language particular building intelligent dialogue agent. measure progress towards goal argue usefulness proxy tasks evaluate reading comprehension question answering. tasks measure understanding several ways whether system able answer questions chaining facts simple induction deduction many more. tasks designed prerequisites system aims capable conversing human. believe many existing learning systems currently solve them hence classify tasks skill sets researchers identify failings systems. also extend improve recently introduced memory networks model show able solve some tasks. rich history synthetic tasks machine learning problem helped motivate neural networks circle ring datasets helped motivate well-known clustering semi-supervised learning algorithms mackey glass equations time series fact well known datasets synthetic well recent work continues trend. example area developing learning algorithms memory component synthetic datasets used help develop neural turing machine graves memory networks weston latter relevant work. reasons interest synthetic data easier develop techniques using well known working large amounts real data tends lead researchers simpler models simple models data trump elaborate models based less data example -grams language modeling work well relative existing competing methods model truly understands text. researchers become stuck local minima algorithm space; development synthetic data break that. work propose framework synthetic tasks goal helping develop learning algorithms text understanding reasoning. relatively difﬁcult automatically evaluate performance agent general dialogue long term-goal relatively easy evaluate responses input questions i.e. task question answering question answering incredibly broad less task think cast setup. enables propose wide ranging different tasks test different capabilities learning algorithms common framework. tasks built uniﬁed underlying simulation physical world akin classic text adventure game whereby actors move around manipulating objects interacting other. simulation runs grounded text question answer pairs simultaneously generated. goal categorize different kinds questions skill sets become tasks. hope analysis performance tasks help expose weaknesses current models help motivate algorithm designs alleviate weaknesses. envision feedback loop tasks designed response perhaps adversarial fashion order break models. tasks design detailed section simulation used generate section section give benchmark results standard methods tasks analyse successes failures. order exemplify kind feedback loop algorithm development task development envision section propose improvements recent memory network method shown give promising performance show proposed approach indeed give improved performance tasks still unable solve them consider open problems. several projects targeting language understanding using qa-based strategies recently emerged. unlike tasks like dialogue summarization easy evaluate hence makes appealing research avenue. difﬁculty lies deﬁnition questions must unambiguously answerable adult humans still require thinking. allen institute ai’s ﬂagship project aristo organized around collection tasks derived increasingly difﬁcult science exams grade levels. richardson proposed mctest stories associated questions intended research machine comprehension text. question requires reader understand different aspects story. initiatives promising direction interpreting results benchmarks remain complicated. indeed system able fully solve proposed tasks since many sub-tasks need solved answer questions difﬁcult clearly identify capabilities limitations systems hence propose improvements modiﬁcations. result conclusions drawn projects much clearer coming traditional works large-scale knowledge bases besides best performing systems based hand-crafted patterns features and/or statistics acquired large corpora. difﬁcult argue systems actually understand language simply light upgrades traditional information extraction methods system berant evolved since builds structured representation text question answer. despite potential method remains highly domain speciﬁc relies prior knowledge. based observations chose conceive collection much simpler tasks main objective failure success system unequivocally provide feedback capabilities. that close winograd schema challenge levesque organized around simple statements followed single binary choice question joan made sure thank susan help received. received help? joan susan?. challenge tasks straightforward interpret results. winograd challenge mostly centered around evaluating systems acquire make background knowledge expressed words statement tasks self-contained diverse. self-contained mean tasks come training data evaluation data rather latter case aristo winograd challenge. mctest train/test split training likely small capture reasoning needed well test set. setup assess amount training examples needed perform well commonsense knowledge reasoning required test contained training set. terms diversity tasks related existing setups also propose many additional ones; tasks inspired previous work lambda dependency-based compositional semantics instance. task checks skill system must principles main idea provide tasks similar software testing built computer science. ideally task leaf test case independent others possible tests simplest possible aspect intended behavior. subsequent tests build testing combinations well. tasks publicly available http//fb.ai/babi. source code generate tasks available https//github.com/facebook/babi-tasks. task provides training test data intention successful model performs well test data. following weston supervision training given true answers questions relevant statements answering given question used learner. tasks correct answers limited single word else list words evaluation clear-cut measured simply right wrong. tasks noiseless human able read language potentially achieve accuracy. tried choose tasks natural human based simple usual situations background areas formal semantics machine learning logic knowledge representation required adult solve them. data produced using simple simulation characters objects moving around interacting locations described section simulation allows generate data many different scenarios true labels known grounding simulation. task describe giving small sample dataset including statements questions true labels tables single supporting fact task consists questions previously given single supporting fact potentially amongst irrelevant facts provides answer. ﬁrst test simplest cases this asking location person e.g. mary travelled ofﬁce. mary?. kind task already employed weston considered simplest case real world datasets fader three supporting facts harder task answer questions supporting statements chained answer question task answer question where football? combine information sentences john playground john picked football. again kind task already used weston similarly make task three supporting facts given task whereby ﬁrst three statements required answer question where apple kitchen?. three argument relations answer questions ability differentiate recognize subjects objects crucial. task consider extreme case sentences feature reordered words i.e. bag-of-words work. example questions what north bedroom? what bedroom north exactly words different order different answers. step further sometimes needs differentiate three separate arguments. task involves statements like jeff given milk bill queries giver receiver object involved. counting lists/sets task tests ability system perform simple counting operations asking number objects certain property e.g. many objects daniel holding?. similarly task tests ability produce single word answers form list e.g. what daniel holding?. tasks seen tasks related basic database search operations. simple negation indeﬁnite knowledge tasks test slightly complex natural language constructs. task tests simplest forms negation supporting facts imply statement false e.g. fred longer ofﬁce rather fred travelled ofﬁce. prerequisite task.) task tests model statements describe possibilities rather certainties e.g. john either classroom playground. case answer maybe question john classroom?. basic coreference conjunctions compound coreference task tests simplest type coreference detecting nearest referent e.g. daniel kitchen. went studio.. real-world data typically addresses labeling problem studies sophisticated phenomena whereas evaluate tasks question answering problem. task tests referring multiple subjects single statement e.g. mary jeff went kitchen.. task tests coreference case pronoun refer multiple actors e.g. daniel sandra journeyed ofﬁce. went garden. time reasoning tasks included time implicitly order statements task tests understanding time expressions within statements e.g. afternoon julie went park. yesterday julie school. followed questions order events where julie park?. real-world datasets address task evaluating time expressions typically labeling rather task e.g. uzzaman basic deduction induction task tests basic deduction inheritance properties e.g. sheep afraid wolves. gertrude sheep. gertrude afraid of?. task similarly tests basic induction inheritance properties. full analysis induction deduction clearly beyond scope work future tasks analyse further deeper aspects. positional size reasoning task tests spatial reasoning many components classical shrdlu system asking questions relative positions colored blocks. task requires reasoning relative size objects inspired commonsense reasoning examples winograd schema challenge path finding goal task path locations given description various locations asks another? related work chen mooney effectively involves search problem. agent’s motivations finally task questions simplest possible agent performs action. addresses case actors given state actions take e.g. learn hungry people might kitchen already stated tasks meant foster development understanding machine learning algorithms. single model evaluated across tasks model tested additional real-world tasks. data release addition providing tasks english also provide hindi; shufﬂed english words longer readable humans. good learning algorithm perform similarly three would likely case method using external resources setting intended mimic learner ﬁrst presented language learn scratch. tasks generated simulation behaves like classic text adventure game. idea generating text within simulation allows ground language used coherent controlled world. simulation follows bordes weston somewhat complex. simulated world composed entities various types various actions operate entities. entities internal states location whether carry objects inside mental state actors well properties size color edibility. locations nearby places connected encoded. actors pre-speciﬁed rules actor also speciﬁed control behavior e.g. hungry food. random valid actions also executed rule e.g. walking around randomly. actions actor execute simulation consist following <location> <object> <object> <object> <object> in/on <object> give <object> <actor> drop <object> <entitity> <state> look inventory examine <object>. universal constraints imposed actions enforce coherence simulation. example actor cannot something someone else already cannot place connected current location cannot drop something already have using underlying actions rules actors constraints deﬁnes actors act. task limit actions needed task e.g. task needs whereas task uses drop. write commands gives simple story executable simulation e.g. playground; ofﬁce; football. example corresponds task system questions state simulation e.g. john? football? easy calculate true answers questions access underlying world. produce natural looking text lexical variety statements questions employ simple automated grammar. verb assigned synonyms e.g. simulation command replaced either picked grabbed took drop replaced either dropped left discarded down. similarly object actor replacement synonyms well e.g. replacing daniel task adverbs crucial tasks time reasoning task great many aspects language modeled. example sentences relatively short contain little nesting. further entities vocabulary size small hope deﬁning well deﬁned tasks help evaluate models controlled within simulated environment hard real data. tasks substitute real data complement them especially developing analysing algorithms. compared following methods tasks gram classiﬁer baseline lstms memory networks extensions memory networks detail; structured incorporates external labeled data existing tasks. models belong three separate tracks. weakly supervised models given question answer pairs training time whereas strong supervision provides supporting facts training time well. strongly supervised ones give accuracy upper bounds weakly supervised models i.e. performance superior given model class. methods last external resources track labeled data sources rather training provided e.g. coreference semantic role labeling tasks well strong supervision. task questions training testing report test accuracy. consider task successfully passed accuracy obtained. table test accuracy tasks various methods proposed extensions memnns columns adaptive memory n-grams nonlinear matching function combinations thereof. bold numbers indicate tasks extensions achieve accuracy original memnn model weston not. last columns give extra analysis memnn method. column gives amount training data task needed single supporting fact supporting facts three supporting facts arg. relations three arg. relations yes/no questions counting lists/sets simple negation indeﬁnite knowledge basic coreference conjunction compound coref. time reasoning basic deduction basic induction positional reasoning size reasoning path finding agent’s motivations mean performance methods -gram classiﬁer baseline inspired baselines richardson applied case producing -word answer rather multiple choice question construct bag-of-n -grams sentences story share least word question learn linear classiﬁer predict answer using features. lstms popular method sequence prediction outperform standard rnns tasks similar work reading story point reach question output answer. note weakly supervised answers only hence disadvantage compared strongly supervised methods methods external resources. memnns recently proposed class models shown perform well work controller neural network performing inference stored memories consist previous statements story. original proposed model performs hops inference ﬁnding ﬁrst supporting fact maximum match score question second supporting fact maximum match score question ﬁrst fact found. matching function consists mapping bag-ofwords question facts embedding space summing word embeddings. word embeddings learnt using strong supervision optimize task. ﬁnding supporting facts ﬁnal ranking performed rank possible responses given facts. also consider extensions model adaptive memories performing variable number hops rather model trained predict special stop class. similar procedure applied output multiple tokens well. finally built classical cascade system baseline using structured support vector machine incorporates coreference resolution semantic role labeling preprocessing steps trained large amounts costly labeled data. stanford coreference system senna semantic role labeling system used build features input trained strong supervision supporting facts e.g. features based words word pairs verb verb-argument pairs. ﬁnding supporting facts build similar structured response stage features tuned goal well. details sec. appendix. learning rates hyperparameters methods chosen using training set. summary experimental results tasks given table give results tasks separately well mean performance number failed tasks ﬁnal rows. results standard memnns generally outperform -gram lstm baselines consistent results weston however still fail number tasks; test accuracy less failures expected insufﬁcient modeling power described detail sec. e.g. facts single word answers bag-ofwords succeed tasks however also failures tasks ﬁrst expect example yes/no questions indeﬁnite knowledge given hindsight realize linear scoring function standard memnns cannot model match query supporting fact yes/no answer requires three-way interactions. columns table give results memnn extensions adaptive memories -grams nonlinearities plus combinations thereof. adaptive approach gives straight-forward improvement tasks require supporting facts also gives improvements require multi-word outputs hence model combination extensions subsequent experiments. memnns -gram modeling yield clear improvements word order matters e.g. tasks however -grams seem substitute nonlinearities embedding function model outperforms -grams average especially yes/no indefinite tasks explained before. hand method cannot model word order fails e.g. task obvious step thus combine complimentary approaches indeed am+ng+nl gives improved results both total tasks upgraded failure success compared original memnn model. structured despite access external resources perform better still failing tasks. perform better vanilla memnns tasks hand-built feature conjunctions capture necessary nonlinearities. however compared memnn seems signiﬁcantly worse tasks requiring three supporting facts presumably ranking many possibilities introduces mistakes. however non-greedy search seem help tasks path ﬁnding search important. since relies external resources speciﬁcally designed english unsure would perform well languages like hindi external resources might worse quality. ﬁnal columns give analysis am+ng+nl memnn method. second last column shows minimum number training examples required achieve accuracy fail achieved examples. important desirable perform well task also using fewest number examples succeeding tasks require examples. task requires examples requires hence labeled fail. latter task presumably solved adding times object picked subtracting times dropped seems possible memnn perfectly. tasks positional reasoning path ﬁnding cannot solved even examples seems require general search algorithm built inference procedure memnn lacking. last column shows performance am+ng+nl memnns training tasks jointly rather single one. performance generally encouragingly similar showing model learn many aspects text understanding reasoning simultaneously. main issues models still fail several tasks stronger form supervision typically realistic. prerequisite developed tasks believe prerequisite full language understanding reasoning. learner solve tasks necessarily close full reasoning learner fails tasks likely real-world tasks fail even situations language tasks artiﬁcial believe mechanisms required learn solve part towards text understanding reasoning. ﬂexible framework tasks deﬁnitive set. purpose simulation-based approach provide ﬂexibility control tasks’ construction. grounded tasks language easier understand usefulness tasks interpret results. however primary goal models able learn detect combine patterns symbolic sequences. might even want decrease intrinsic difﬁculty removing lexical variability ambiguity reason bare symbols stripped linguistic meaning. could also decorrelate long-term memory reasoning capabilities systems instance arranging supporting facts closer questions. opposing view could instead want transform tasks realistic stories using annotators complex grammars. tasks presented subset achieved simulation. chose offer variety skills would like text reasoning model have hope researchers community develop tasks varying complexity order develop analyze models solve them. transfer learning across tasks also important goal beyond scope paper. thus made simulator code tasks publicly available purposes. testing learning methods tasks designed test-bed learning methods provide training test sets intend evaluate capability models discover reason patterns hidden within them. could tempting hand-code solutions existing large-scale systems like might succeed solving them even structured results show straightforward; however tasks’ purpose since approaches would learning solve them. experiments show existing machine learning methods successful tasks particular memory networks introduced useful extensions however models still fail several tasks stronger form supervision typically realistic. datasets solved. future research minimize amount required supervision well number training examples needed solve task move closer task transfer capabilities humans. weakly supervised case training examples less known general method solves tasks. further importantly hope feedback loop developing challenging tasks algorithms solve them leads fruitful research directions. note tasks substitute real data complement them especially developing analysing algorithms. many complementary real-world datasets example hermann bordes hill even method works well tasks shown useful real data well. impact since online babi tasks already directly inﬂuenced development several promising algorithms including weakly supervised end-to-end memory networks sukhbaatar dynamic memory networks kumar neural reasoner memnn since shown perform well real-world tasks berant jonathan srikumar vivek chen pei-chun huang brad manning christopher vander linden abby harding brittany clark peter. modeling biological processes reading comprehension. proc. emnlp collobert ronan weston jason bottou l´eon karlen michael kavukcuoglu koray kuksa pavel. natural language processing scratch. journal machine learning research fader anthony zettlemoyer luke etzioni oren. open question answering curated extracted knowledge bases. proceedings sigkdd international conference knowledge discovery data mining hill felix bordes antoine chopra sumit weston jason. goldilocks principle reading children’s books explicit memory representation arxiv preprint arxiv. kumar ankit irsoy ozan jonathan bradbury james english robert pierce ˜brian ondruska peter gulrajani ishaan socher richard. anything dynamic memory networks natural language processing. http//arxiv.org/abs/. m¨uller smola alex r¨atsch gunnar sch¨olkopf bernhard kohlmorgen jens vapnik vladimir. predicting time series support vector machines. artiﬁcial neural networksicann’ springer raghunathan karthik heeyoung rangarajan sudarshan chambers nathanael surdeanu mihai jurafsky manning christopher. multi-pass sieve coreference resolution. proceedings conference empirical methods natural language processing association computational linguistics uzzaman naushad llorens hector allen james derczynski leon verhagen marc pustejovsky james. tempeval- evaluating events time expressions temporal relations. arxiv preprint arxiv. memory networks weston promising class models shown perform well apply tasks. consist memory four potentially learnable components executed given input potentially component make standard pre-processing e.g. parsing entity resolution simplest form processing all. simplest form store incoming example empty memory slot leave rest memory untouched. thus weston actual implementation used exactly simple form bulk work components. former responsible reading memory performing inference e.g. calculating relevant memories answer question latter producing actual wording answer given finally needs produce textual response authors also consider recurrent neural networks standard setup limits responses single word ranking them matrix number features embedding dimension. role original text d-dimensional feature space. choose words representation i.e. every word dictionary three different representations depending whether words input arguments actual input supporting memories modeled differently. consider various extensions model particular modeling write time modeling unseen words. discuss former also use. order model work tasks stories needs know order sentences uttered available model directly. thus extra write time extra features take value indicating sentence older another compared compare triples pairs sentences question itself. training carried stochastic gradient descent using supervision question answer pairs supporting memories weston details. consider variable number supporting facts automatically adapted dependent question asked. consider scoring special fact computation supporting memories becomes keep predicting supporting facts conditioning step previously found facts predicted point stop. unique embedding vector also learned. practice still impose hard maximum number loops experiments avoid fail cases computation never stops multiple answers similar trick response module well order output multiple words. special word dictionary predict word iteration conditional previous words i.e. predict several ways modeling sentences beyond bag-of-words explore three variants here. simplest bag-of-n-grams consider bag. main disadvantage method dictionary grows rapidly therefore consider alternative neural network approach call multilinear map. word sentence binned positions ⌈/l)⌉ position word sentence length position employ matrix model matching score with whereby apply linear word dependent position followed tanh nonlinearity mappings. note related model consider tags rather positions. results method shown main paper space restrictions performs similarly well -grams useful real-world cases -grams cause dictionary large. comparing table memnn adaptive memories multilinear obtains mean performance memnns am+ng+nl matrix. similar classical two-layer neural network applied sides also consider straight-forward combination bag-of-n -grams followed nonlinearity. also built classical cascade system baseline using structured incorporates coreference resolution semantic role labeling preprocessing steps trained large amounts costly labeled data. ﬁrst stanford coreference system stories mention replaced ﬁrst mention entity class. second senna semantic role labeling system collect arguments verb. deﬁne ranking task ﬁnding supporting facts given question three supporting facts indices facts story linear scoring function parameters computing argmax requires exhaustive search unlike e.g. memnn method greedy. scalability thus prune possible matches requiring facts share common non-determiner word match constructed indicator features. simplicity features looks pairs sentences i.e. feature function made following feature types shown word pairs indicator variable pair words pair distance indicator distance sentence i.e. pair order indicator order sentence i.e. verb pair indicator variables pair verbs verb-arg pair indicator variables pair arguments corresponding verbs. ﬁnding supporting facts build similar structured response stage also features tuned goal words indicator word word pairs indicator pair words supporting facts similar verb verb-arg pair features before. results given table structured despite access external resources perform better memnns overall still failing tasks. perform well tasks where hand-built feature conjunctions capture necessary nonlinearities original memnns not. however seems signiﬁcantly worse tasks requiring three supporting facts presumably ranking many possibilities introduces mistakes. however non-greedy search seem help tasks path ﬁnding search important.", "year": 2015}