{"title": "On the Effects of Batch and Weight Normalization in Generative  Adversarial Networks", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at https://github.com/stormraiser/gan-weightnorm-resnet", "text": "generative adversarial networks highly effective unsupervised learning frameworks generate sharp data even data images complex highly multimodal distributions. however gans known hard train suffering problems mode collapse disturbing visual artifacts. batch normalization techniques introduced address training. though accelerates training beginning experiments show unstable negatively impact quality trained model. evaluation numerous recent schemes improving training hindered lack effective objective quality measure models. address issues ﬁrst introduce weight normalization approach training signiﬁcantly improves stability efﬁciency quality generated samples. allow methodical evaluation introduce squared euclidean reconstruction error test objective measure assess training performance terms speed stability quality generated samples. experiments standard dcgan architecture commonly used datasets indicate training using generally superior gans achiving lower mean squared loss reconstruction signiﬁcantly better qualitative results demonstrate stability -layer resnet trained celeba data set. despite prevalent effects batch normalization generative adversarial networks examined carefully. popularized inﬂuential dcgan architecture gans typically justiﬁed perceived training speedup stability generated samples often suffer visual artifacts limited variations lack evidence always improves training partly unavailability quality measures models. puzzled technique propose methodical evaluation models assess abilities generate large variations samples idea hold portion dataset test dataset latent code generates closest approximation test images. test image optimize latent code gradient descent ﬁxed number iterations. average squared euclidean distance test samples reconstructed ones used measure quality gans. experiments show reconstruction error correlates visual quality generated samples still time consuming approach efﬁcient existing log-likelihood-based evaluation methods. evaluation technique therefore convenient monitoring progress training. show generally accelerates training early stages increase success rate training certain datasets network structures model without normalization could often fail. many cases though cause stability generalization power model decrease drastically. following work salimans kingma arpit introduce modiﬁed weight normalization technique training. using sets experiments found approach achieve faster stable training well generate equal higher quality samples models without normalization. believe proposed technique superior context gans. various quantitative measures since proposed. commonly used estimating log-likelihood training generator’s distribution generating large amount samples ﬁtting gaussian parzen window discussed theis particularly effective amount samples need generated accurate log-likelihood estimation intractable. another measure inception score proposed salimans based assumption good generative model able generate meaningful objects. limitation approach that inception model pretrained another image classiﬁcation task usually natural objects. thus useful measure quality trained images similar objects. quality gans also evaluated indirectly e.g. measuring classiﬁcation accuracy using features extracted discriminator proposed measure reconstruction loss similar used metz discuss differences section propose modiﬁed formulation weight normalization approach introduced salimans kingma notable deﬁciency original technique that simplest form normalize mean value input. solved augmenting version normalizes mean input variance. experiments showed improved performance cifar- classiﬁcation task compared plain gave worse results several experiments hence chose include augmentation approach. simplicity consider single output neuron. training data normalized input network zero mean unit variance. mean variance output nonlinear layer evaluated closed form assumption input preceding linear layer multivariate standard normal distribution. mean variance used correct distribution output shown effective various applications. context gans ﬁrst appeared lapgan denton made popular inﬂuential dcgan architecture radford since become common practice listed overview techniques used many architectures ebgan summarize takes batch samples computes following means standard deviations input batch learned parameters. result output always mean standard deviation regardless input distribution. importantly gradients must back-propagated computation although presented reparameterization modiﬁes curvature loss function main idea simply divide weight vectors norms. similar idea proposed around time under normalization propogation arpit effectiveness technique illustrated various experiments investigate acceleration approach gans. detailed section propose modiﬁed version weight normalization improve training models. evaluation. earlier gan-related works lack quantitative measures visual inspection commonly used method. addition inspecting visual quality also used show model overﬁt interpolation latent space ﬁnding closest training sample generated samples point difference learned parameter. commonly referred threshold layer deﬁned max{x threshold learned. chose name reﬂect fact relu-like nonlinear functions used give translated leaky parametric relu layers. here translate data apply nonlinear function translate data back using trelu instead adding bias previous layer prevent introduction large mean distribution. simpliﬁcation effectively negates learned afﬁne transformation seemingly would reduce functions represented network. argue however allowing learning afﬁne transformation last weight-normalized layer recovers expressiveness entire stack layers strict weight-normalized layers refer layers without afﬁne transformations layers learned afﬁne transformation many generative models reconstruction error training often explicitly optimized form even case gans natural evaluate model reconstruction loss measured test set. case gans given generator test samples reconstruction loss deﬁned case images normalize different image sizes considering pixel color channel reconstruction loss thus divide loss width height training images. since notice ad-hoc really achieve goal. firstly mentioned closed form mean variance approximation since correctness derivation requires input normal distributed strictly hold beyond ﬁrst layer. critically deriving equation ﬁxing akin batch normalization argue afﬁne transformation needs learned weight-normalized linear layer succeeding non-linear layer order avoid decreasing functions represented network. formulation becomes derivation restricted case learned afﬁne transformation i.e. restriction relaxed result would invalid even i.i.d. normal condition hold. could make functions error back-propagation computation would overly complex since functions also need taken account. cannot hope strictly enforce zero-mean unitvariance distribution propose simpler approximation instead. note relu-like nonlinearity relu relu equation always invert direction take negative hence without loss generality assume equivalently equation written directly infer optimal alternative method starting all-zero vector perform gradient descent latent code minimizes squared euclidean distance sample generated code target one. code optimized instead computed feed-forward network evaluation process time-consuming. thus avoid performing evaluation every training iteration monitoring training process reduced number samples gradient descent steps. ﬁnal trained model perform extensive evaluation larger test larger number steps. method similar proposed metz important differences samples used reconstruction come training take samples separate test set. intuitively order generate test samples training generator must learn distribution training samples memorize overﬁt them. effect would achieved test samples come training set. furthermore uses l-bfgs optimization latent code. l-bfgs known give good fast optimization problems high-dimension suits setting problem well. however effectiveness sensitive many parameters. able combination parameters consistently work well various experiment settings. also made harder justify choice parameters since different models would like compare best parameters different. instead rmsprop. fastest optimization method problem found work well settings altering parameters generally affect reconstruction result different models makes comparison easier. conducted experiments image generation tasks quantitative analysis dcgan-based architecture celeba lsun bedroom cifar- datasets qualitative results -layer resnet celeba. celeba experiments detailed here. limited space show generated reconstructed samples lsun cifar- here discuss settings qualitative quantitative results along samples appendices without normalization reference batch normalization formulation weight normalization network structured following discriminator successive convolution layers kernel size stride padding output features doubling previous layer starting features ﬁrst layer. convolution layers spatial size feature sufﬁciently small ﬁnal convolution layer stride zero padding kernel size generator reverse structure transposed convolution layers. common practice batch normalization applied ﬁrst layer discriminator last layers discriminator generator. weight normalization used every layer. last layer discriminator generator afﬁne weight-normalized layers every layer strict weight-normalized layers parametric relu used vanilla batch-normalized models translated parametric relu weight-normalized models. slope bias parameters learned per-channel. length code models. architectures summarized table additional details regarding implementation weight normalized layers discussed appendix models optimized rmsprop learning rate batch size speciﬁcally model separate batches true samples generated samples training discriminator suggested parameter update clip learned slope parametric relu layers total images celeba dataset. randomly selected images evaluation used rest training. training perform running evaluation every training iterations randomly selected ﬁxed subset test samples. optimal code found performing gradient descent steps starting zero vector. rmsprop learning rate model best performing network training saved used ﬁnal evaluation. ﬁnal evaluation test samples perform gradient descent steps. model inference mode. addition also converged model evaluation case model converge gives notably worse running reconstruction optimal recorded model. however occur main experiment. consider training converged running reconstruction loss generated samples stay stable sufﬁcient amount time. table network structure discriminators generators first three columns type layers vanilla models respectively. fourth column kernel size stride padding number output channels convolution layer. observed mode collapse issues vanilla models. reduce possibility observations caused random factors repeat training procedure models three times. present results best training instances additional ones vanilla models found appendix running reconstruction loss three models shown figure ﬁrst iterations. generated samples vanilla models collapsed. model trained iterations considered converged much faithfully. samples reconstructed model signiﬁcantly blurrier affected artifacts. shown figure reconstruction vanilla models started worse relatively early training achieving optimal reconstruction loss. vanilla model loss went slowly relatively short time around iteration generator collapses produces output caused reconstruction loss increase suddenly. model around iterations loss started show excessive ﬂuctuation. model however kept improving steadily iterations remained largely stable. figure selected ﬁnal reconstruction results. left right group test sample vanilla reconstruction reconstruction reconstruction. images best viewed enlarged. also visualize stability checking samples generated code different iterations shown figure model noticeably stable samples generated code remain mostly constant across time scale iterations generated samples slowly improving models produce random variations. additional visual analysis samples found appendix compare training speed three models assessing generated samples early stages training illustrated figure evident batch normalization accelerate training effect weight normalization comparable. notice model already produce human face iterations. accelerated training mostly useful fast sanity check monitoring training progress deep neural networks. shown figure visual quality samples generated three models comparable iterations none models achieve noticeably faster progression other. addition ability generate visually plausible samples earlier necessarily translate overall faster improvement reconstruction. notice allows higher learning rate. training vanilla models often fail learning rate model still trainable learning rate however found increased learning rate accelerate training model. instead harms stability model. figures show random generated reconstructed samples three models cifar- lsun bedroom datasets. vanilla model failed train lsun results models shown. gans setting usually image-to-image translation task e.g. image super-resolution direct image generation noise resnet particularly successful. test method -layer residual network. block structure follows base design basic blocks shortcut branch optional average pooling present stride followed optional convolution kernel size present number input features equal number output features. residue branch convbn-prelu-conv-bn structure model rewn model convolutions replaced strict weight normalized version prelu layers replaced translated version. complication summing branches model appendex details. ﬁrst convolution residue branch kernel size stride respectively. second convolution always kernel size convolutions always padding generator convolutions replaced transposed convolutions average pooling shortcut branch replaced nearest neighbour upscaling. discriminator network consists levels level consisting stride block followed stride block number output features total residue blocks thus layers. ﬁnal convolution kernel size padding added dcgan models above total layers. since network much deeper save computation time reduced number features dimension latent space again generator mirror image discriminator. also reduced batch size learning rate iterations training vanilla model model never able generate handful different samples extremely unstable model trained iteration without major issues able generate samples high quality diversity. best running reconstruction loss achieved iteration random samples iteration shown ﬁgure point however continued training around iteration observe degradation sample quality weight normalized resnet model similar ways vanilla dcgan model examined appendix indicates weight normalization sufﬁcient guarantee stability network. goal compete techniques complete solution instability training. rather since method propose different training loss protocol favour particular architecture method complementary existing training improvement techniques combined improve quality gans. figure evolution samples resnet training. rows vanilla; middle rows bottom rows columns samples iterations intervals iterations. hardly recognizable samples indeed generated code. weights notably weight clipping wasserstein particular case weight normalization provides alternative weight clipping enforce lipschitz continuity discussed appendix inal work achieves superior training performance. also presented evaluation method gans based mean squared euclidean distance test samples closest generated ones synthesized gradient descent latent code. trained analyzed variants dcgan different normalization methods image generation datasets multiple scales. found batch-normalized models perform worse reconstructing test samples less stable training. particular reconstruction errors visual quality deteriorated however formulation weight normalization improves reconstruction quality training stability considerably. demonstrate stabilizing power weight normalization successful training residual considerably deeper. based extensive evaluations believe weight normalization used instead batch normalization training generative adversarial networks.", "year": 2017}