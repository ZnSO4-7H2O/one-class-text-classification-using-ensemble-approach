{"title": "Scalable Out-of-Sample Extension of Graph Embeddings Using Deep Neural  Networks", "tag": ["stat.ML", "cs.LG", "cs.NE", "stat.ME"], "abstract": "Several popular graph embedding techniques for representation learning and dimensionality reduction rely on performing computationally expensive eigendecompositions to derive a nonlinear transformation of the input data space. The resulting eigenvectors encode the embedding coordinates for the training samples only, and so the embedding of novel data samples requires further costly computation. In this paper, we present a method for the out-of-sample extension of graph embeddings using deep neural networks (DNN) to parametrically approximate these nonlinear maps. Compared with traditional nonparametric out-of-sample extension methods, we demonstrate that the DNNs can generalize with equal or better fidelity and require orders of magnitude less computation at test time. Moreover, we find that unsupervised pretraining of the DNNs improves optimization for larger network sizes, thus removing sensitivity to model selection.", "text": "several popular graph embedding techniques representation learning dimensionality reduction rely performing computationally expensive eigendecompositions derive nonlinear transformation input data space. resulting eigenvectors encode embedding coordinates training samples only embedding novel data samples requires costly computation. paper present method out-of-sample extension graph embeddings using deep neural networks parametrically approximate nonlinear maps. compared traditional nonparametric out-of-sample extension methods demonstrate dnns generalize equal better ﬁdelity require orders magnitude less computation test time. moreover unsupervised pretraining dnns improves optimization larger network sizes thus removing sensitivity model selection. manifold learning popular data analysis framework attempts compact low-dimensional embeddings high-dimensional datasets. several manifold learning algorithms— including isomap locally linear embedding diﬀusion maps laplacian eigenmaps derive coordinate representations encode local neighborhood structure unlabeled data sample. techniques found considerable success wide array application domains including computer vision speech processing natural language processing shown algorithms members general graph embedding framework transformations derived generalized eigendecomposition graph laplacian matrix operator algorithmspeciﬁc graph construction methodologies. basic form graph embedding techniques provide transformations training samples used construct graph. thus even large training used computing output estimated novel test sample possible. address shortcoming nonparametric out-of-sample extension technique based nystr¨om sampling developed leverages input target representation pairs training sample approximate would generated arbitrary test point generally eﬀective nystr¨om extension kernel-based method time complexity scales linearly number training samples. increase computational cost especially problematic manifold methods eﬀective provided beneﬁt large training sets representation learning. would highly beneﬁcial remove trade-oﬀ between representation quality extension feasibility eﬃciently scaleable method out-ofsample extension. neural networks long known learning framework classiﬁcation powerful regression capable distilling large training sets eﬃciently evaluated parametric models thus natural choice modeling manifold embeddings. seminal paper hornik proved feedforward neural networks approximate virtually arbitrary deterministic high-dimensional spaces indicating would also ideally suited out-of-sample extension problem. however caveats neural networks universal approximators must suﬃcient hidden units turn require additional data samples training without overﬁtting; non-convexity objective function grows number model parameters making search reliable global solutions increasingly diﬃcult. considerations mind explore application recent advances deep neural network training methodology out-of-sample extension problem. first stabilizing lanczos eigendecomposition algorithm able produce exact graph embeddings training sets millions data samples. permits extensive study deeper architectures previously considered task. second motivated success supervised classiﬁcation setting consider unsupervised pretraining procedures improve optimization larger training samples support commensurate increases model complexity. work follows compare performance parametric approach nystr¨om sampling baseline terms approximation ﬁdelity test runtime. dnns match outperform approximation ﬁdelity nystr¨om method training sample sizes. furthermore since approach parametric test-time complexity ﬁxed network size constant training sample size producing orders-ofmagnitude speedup nystr¨om sampling larger training sizes. remainder paper organized follows. begin overview prior work out-of-sample extension graph embeddings. describe strategy stabilizing eigendecompositions large training sets followed description process training out-of-sample extension approximate embedding unseen data. finally analyze reconstruction accuracy computation speed nystr¨om baseline approach. popular methods extending graph embeddings unseen data based nystr¨om sampling thus serve baseline experiments. nonparametric kernel-based technique approximates embedding test sample computing weighted interpolation embeddings training samples nearby original input space. formally thus bogged feed data graph embedding training. begin section simple trick eigendecomposition large graph laplacians permits larger training sets motivates need computationally eﬃcient extension methods. followed presentation deep neural network architecture propose eﬃciently extend embedding arbitrary test points. suggested stability lanczos eigendecomposition algorithm greatly increased reformulating eigenproblem recover largest eigenvalues. exploit observing eigenguaranteed less equal thus small redeﬁnition eigenproblem recover eigenvectors considering largest eigenvalue criterion. note using arpack implementation similar eﬀect also accomplished searching smallest trick means fundamental theoretical innovation part eﬀects proven dramatic. past eﬀorts solve smallest magnitude eigenvalues graph laplacian exceeded hardware memory limits graphs reached order nodes million edges. employing simple trick succeeded processing graphs order million nodes order billion edges conventional hardware stably solving eigenvectors days using cores ram. problem size even exceeds reported using approximate singular value decomposition solvers past million node graphs consider experiments described below method adequate embedding training needs. complexity extension linear size training set. practice approximate nearest neighbor techniques used speed minimal loss ﬁdelity algorithmic complexity still increases training size. finally note nearly equivalent formulation based reproducing kernel hilbert space theory presented kernelization introduced objective function eigendecomposition performed. formulation scalability limitations nystr¨om extension. computational diﬃculties motivate exploration dnns model embeddings out-of-sample extension. traditional neural networks also considered out-of-sample extension past limited studies involving small datasets model architectures idea introduced study failed include meaningful quantitative evaluation. experiments predated advent recent deep learning training methodologies found neural networks worst performing methods. however similar motive computational eﬃciency explored dnns approximating expensive sparse coding transformations produced compelling results. truly scalable out-of-sample extension must simultaneously consume large amount training data detailed modeling provide test-time complexity strongly depend training size. nonparametric nature nystr¨om method leads linear dependence training size encoding layers decoding layer. thus initialize model parameters approximately recover identity mapping consider unsupervised pretraining procedure here introduce hidden layer time performing several epochs stochastic gradient descent minimize mean squared error training samples intermediate network depth. layer discard linear decoding weights previous optimization previous hidden representation input hidden layer reoptimize layer parameters. early stopping used prevent exact recovery identity layer. using layer-wise pretraining procedure initialized parameters network. remains perform several epochs stochastic gradient descent reoptimize network parameters minimize mean squared error exact predicted embedding pairs according equation training complete dnn-based out-of-sample extension arbistandard neural network forward pass deﬁned equation amounts matrix-vector multiplies vector additions plus evaluation hidden unit. ﬁxed network architecture computation constant number training samples. of-sample points whose mapping estimated interpolation instead seek estimate mapping itself. consider feedforward neural architectures hidden layers containing hidden units. l-th hidden layer nonlinearly maps output previous critical training procedure safeguards overﬁtting training sample especially deeper architectures required order approximate detailed graph embeddings wish extend. training procedure also employed diﬀerent application steps unsupervised stacked autoenaudio recordings typically produce highdimensional data samples known frames large datasets readily available; manifold learning techniques shown learn representations eﬀective keyword discovery search timit corpus evaluation given past success manifold embeddings speech recognition data consists hours prompted american english speech recordings split training consisting roughly million data sion method. thus strategy perform exact graph embedding entire corpus using method described section call reference embeddings. deﬁne outof-sample extension using input frames corresponding reference embeddings training set. allows reference embeddings training approximate embeddings test comparison true reference embeddings test set. measure approximation ﬁdelity terms normalized root mean squared error predicted reference test embeddings; here normalization constant taken root mean squared error test reference embeddings random permutation samples. thus perfect outof-sample extension nrmse extension random mapping empirical output distribution nrmse addition deﬁning extensions entire training also consider input features -dimensional homomorphically smoothed mel-scale power spectrograms sampled every using hamming window. construct graph laplacian using symmetrized -nearest neighbor adjacency graph cosine distance metric binary edge weights. amounts laplacian eigenmaps specialization graph embedding framework. finally keep eigenvectors largest eigenvalues produce graph embedding dimension input space. present focus out-of-sample extension ﬁdelity relevant note reference embeddings match best downstream performance reported used identical embedding strategy. baseline nystr¨om method compute equation using nearest neighbor approximation radial basis function kernel. approximation facilitated preprocessing training samples tree data structure eﬃcient retrieval near-neighbor sets. note tried matching nystr¨om kernel used graph construction prescribed performed substantially worse introducing weights. consider kernel squared-bandwidths first consider nrmse performance vary amount training samples used outof-sample extension. drew random subsets million sample training sizes subsets used computation equation nystr¨om method supervised ﬁne-tuning case method. table lists training subset size nrmse test runtime nystr¨om using optimal hyperparameters smaller layers units each larger layers units. larger matches outperforms nystr¨om method training sizes demonstrating power deep learning accurately approximating complex nonlinear functions. small matches nystr¨om smaller training sets suﬃcient parameters keep pace training data becomes available. results emphasize importance method’s sensitivity choice hyparameters specify complexity out-of-sample extension function. especially true fully unsupervised representation learning settings cross-validation possible. explore consideration nystr¨om method vary number nearest neighbors contribute test sample well kernel bandwidth. proposed method vary number layers number hidden units layer figure shows heatmaps indicating nrmse hyperparameters considered methods four training sizes. nystr¨om method performance relatively stable across range kernel bandwidths sensitive number neighbors used computation. moreover optimal number neighbors increases approach reaches near-optimal performance training sizes provided include least layers least twice many units input dimension. critically performance penalty overshooting network size suggests extension would require less tuning nystr¨om method achieve optimal approximation applications. typical machine learning scenarios increasing number model parameters ﬁxed training size opens method overﬁtting poor generalization. trends methods figure defy conventional wisdom loss approximation ﬁdelity training corpus size move deeper wider network architectures. indeed unsupervised pretraining responsible regularizing parameter estimates preventing overspecialization even smallest training considered. seen clearly scatterplots figure represents single model architecture. total number parameters increases test nrmse pretrained models decays roughly monotonically. architectures without pretraining track similarly smaller model sizes overﬁtting test performance degrades parameters made available. behavior especially clear case limited training examples though finally consider computational eﬃciency applying out-of-sample extension methods test corpus. table lists nrmse values corresponding test times seconds nystr¨om method optimal hyperparameters architectures. expected runtimes nonparametric nystr¨om method increase training sample gets bigger since nearest neighbor retrieval remains expensive even using tree data structure. meanwhile runtimes virtually constant ﬁxed number parameters. small consume full training produce extensions times faster nystr¨om method smallest training simultaneously reducing nrmse relative. moreover best roughly matches approximation ﬁdelity best nystr¨om nrmse speech processing applications interactivity often critical even largest networks process test samples faster real-time. moreover large consisting layers hidden units optimal nrmse achieved speeds times faster real-time using single processor. comparable extraction speed traditional acoustic front-ends state-of-the-art implementations work used modern deep learning methodologies perform out-of-sample extensions graph embeddings. compared standard nystr¨om sampling-based out-of-sample extension dnns approximate embeddings higher ﬁdelity substantially computationally eﬃcient. using unsupervised pretraining parameter initialization improves generalization making approach highly stable across wide variety hyperparameter settings. results support deep neural networks unsupervised pretraining ideal choice out-of-sample extensions learned manifold representations.", "year": 2015}