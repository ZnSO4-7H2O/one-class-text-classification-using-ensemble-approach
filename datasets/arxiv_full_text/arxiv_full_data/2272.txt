{"title": "Learning General Latent-Variable Graphical Models with Predictive Belief  Propagation and Hilbert Space Embeddings", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, we propose a new algorithm for learning general latent-variable probabilistic graphical models using the techniques of predictive state representation, instrumental variable regression, and reproducing-kernel Hilbert space embeddings of distributions. Under this new learning framework, we first convert latent-variable graphical models into corresponding latent-variable junction trees, and then reduce the hard parameter learning problem into a pipeline of supervised learning problems, whose results will then be used to perform predictive belief propagation over the latent junction tree during the actual inference procedure. We then give proofs of our algorithm's correctness, and demonstrate its good performance in experiments on one synthetic dataset and two real-world tasks from computational biology and computer vision - classifying DNA splice junctions and recognizing human actions in videos.", "text": "order overcome shortcomings recently researchers proposing studying family non-parametric learning algorithms called spectral learning based idea method moments spectral learning algorithms seek learn alternative parameterizations latent-variable graphical models purely based observable variables eﬃciently recovered low-order observable moments tensor decompositions. therefore algorithms enjoy advantages local-minimum-free provably consistent also often orders magnitude faster search-based methods algorithm. although spectral learning algorithms recently successfully applied tackle learning problems types latent-variable graphical models still several severe limitations problems spectral algorithms need solved. first spectral learning algorithms proposed restricted learning several speciﬁc types latent-variable graphical models thus making themselves hard generalize types latentvariable graphical model structures second current spectral learning algorithms deal discrete-valued random variables moderate cardinalities parameterization assumptions thus cannot easily applied scenarios variables graphical models continuous-valued large cardinalities quite often encountered many important real-world applications. third current spectral algorithms generally idiosyncratic speciﬁc graphical model structures targeted learn thus cannot provide ﬂexible learning framework template users easily incorporate diﬀerent prior knowledge probabilistic assumptions facing diﬀerent learning problems. paper propose algorithm learning general latent-variable probabilistic graphical models using techniques predictive state representation instrumental variable regression reproducing-kernel hilbert space embeddings distributions. learning framework ﬁrst convert latent-variable graphical models corresponding latent-variable junction trees reduce hard parameter learning problem pipeline supervised learning problems whose results used perform predictive belief propagation latent junction tree actual inference procedure. give proofs algorithm’s correctness demonstrate good performance experiments synthetic dataset real-world tasks computational biology computer vision classifying splice junctions recognizing human actions videos. many important problems modern machine learning artiﬁcial intelligence gracefully captured probabilistic graphical models latent variables. however learning general latent-variable probabilistic graphical models remained diﬃcult largely intractability introduced latent variables possibly complicated loopy structures graphical models. previously latentvariable graphical models would resort local search heuristics expectation maximization algorithm turn parameter learning problem maximization problem non-convex objective function. however methods based iterative local search lead local optima slow converge. limitations paper develop algorithmic framework learning general latent-variable probabilistic graphical models based ideas predictive belief propagation reproducing-kernel hilbert space embeddings distributions. learning framework built latent junction tree graphical representation latent-variable probabilistic graphical models reduces hard parameter learning problem pipeline supervised learning problems named two-stage regression ﬂexibly insert supervised learning algorithms procedure dealing continuous variables embed distributions elements reproducing-kernel hilbert spaces kernel trick propagate integrate embeddings latent junction tree tensor operations non-parametric fashion thus seamlessly extending algorithm discrete domain continuous domain general framework. result learning framework general enough apply diﬀerent structures latent-variable graphical models handle discrete variables continuous variables allow users easily incorporate prior information probabilistic assumptions learning. original formulation predictive state representation compact model state controlled dynamical system using vector consisting predictions observable future experiments performed system. representation state dynamical system comkey insight idea predictive state representations actually carried characterize information ﬂows message passing belief propagation probabilistic graphical models. essence state system time bottleneck compactly summarizes everything need know past order predict future illustrated figure below. figure classical interpretation state compact time bottleneck dynamical systems. analogy predictive state belief propagation latent-variable graphical models blue nodes indicate observable variables white nodes indicate latent variables. across separator order compute posterior distribution downstream variables using evidence information upstream variables. therefore belief propagation agent visits separator message sending essentially viewed representation current state junction tree inference system paticular separator set. important analogy illustrated figure below. analogy state built junctions trees generalize idea onto junction trees. instead using partial result sumproduct calculations past part junction tree state well message send across current separator hugin algorithm predictions success probabilities core future tests consisting observations future part junction tree represent state junction tree current separator send prediction vector message inside tree belief propagation concretely separator junction tree among observable variables inside tree always exists minimal subset observable variables whose posterior joint probabilistic distributions given evidence information passed outside tree would completely determine posterior joint distribution observable variables inside tree. minimal subset observable variables inside tree would designated core variables particular separator thus possible joint value realizations variables deﬁned core tests latent junction tree. therefore predictive state separator deﬁned posterior joint probability distribution core variables inference instead passing partial sum-product results around messages pass predictive state vectors around form predictive messages calculate desired posterior. learning process two-stage instrumental regression learn operators separator process incoming predictive messages distribute outgoing predictive messages purely based observed quantities training dataset. name novel message passing framework predictive belief propagation. determining form messages passing around inference process need ﬁgure diﬀerent messages relate could learn inter-message relationships training data. without loss generality let’s consider non-leaf separator latent junction tree connected figure indicate core observable variables associated separator according deﬁnition latent junction trees knowing posterior joint probability distribution would completely determine posterior joint probability distribution observable variables inside tree denote suﬃcient statistics feature vector posterior joint distribution separator denote evicorrelated other. powerful method overcome problem correlated noise modern statistics econometrics instrumental variable regression current setting valid instrumental variable must satisfy criteria correlated input variable corresection present main algorithm learning general latent-variable graphical models consecutive parts learning algorithm followed inference algorithm. describe basic version algorithm case variables discrete-valued extension handle models continuous-valued variables given section give full proof consistency algorithm appendix. model construction convert latent-variable graphical model appropriate corresponding latent-variable junction tree observable variable junction tree associated leaf clique example.) some mathematical notations denotes observed variables x|o|}; denotes hidden variables {x|o|+ x|o|+|h denotes number possible realization values discrete random variable; denotes separator sets connected clique node junction tree. figure example latent-variable graphical model. blue nodes indicate observable variables white nodes indicate latent variables. corresponding latent junction tree graphical model converted pink squares indicate clique nodes yellow squares indicate separator sets. variable nodes circles around associated current leaf clique nodes. non-root parent clique node separated separator parent node received messages children nodes separated separator sets repectively compute upward message root covariance estimation root clique node latent junction tree estimate expectation outer product inside tree feature vectors adjecent separator sets connected taking average across i.i.d. training samples initial leaf message generation leaf clique node latent junction tree indicate observable variables contained indicate separator right deﬁne function observable variable evaluates all-one vector observed evidence evaluates onehot-encoding value indicator vector observed value evidence. non-root parent clique node separated parent node separator separated children nodes separator sets respectively received downward message mcp→c parent node compute biggest advantages learning framework seamlessly entended discrete domain continuous domain general non-parametric fashion. previously existing algorithms learning graphical models continuous variables would make certain parametric assumptions function forms using model approximate continuous distributions variables. however many real-world data underlying continuous distributions highly complex irregular parametric approach would severely limit modeling expressiveness graphical models often deviate true underlying distributions. contrast algorithm simply reproducing-kernel hilbert space embeddings distributions suﬃcient statistic features express parameter learning belief propagation operations tensor algebra inﬁnite-dimensional hilbert space employ kernel trick computing query result query node locate leaf clique node associated call call cq’s parent node separator ﬁrst design tensor moore-penrose psuedoinverse transform downward incoming message mcp→cq upward outgoing message mcq→cp respectively compute hadamard product transformed versions messages obtain estimate unnormalized conditional probtransform operations back tractable ﬁnitedimensional linear algebra calculations gram matrices. explain formulate learning algorithm continuous domain rkhs embeddings. representing continuous distributions variables points embedded corresponding rkhs easily perform two-stage regression linear algebra operations hilbert space. example using kernel ridge regression remaining steps message-passing inference procedure follow exactly form discrete case eventually computation involved nicely carried ﬁnite-dimensional gram matrices. knowledge ﬁrst algorithm ability learn continuous-valued latent-variable graphical models completely nonparametric fashion without making assumptions function forms variables’ distributions. designed three sets experiments evaluate performance proposed algorithm including synthetic data real data ranging discrete domain continuous domain. experiment test performance algorithm task learning running inference discrete-valued latent-variable graphical model depicted figure using artiﬁcally generated synthetic data compare algorithm. gorithm average across possible joint realizations variables report results figure below. plot average divergence algorithm’s results ground truth posterior quickly decreases approaches size training data increases. result demonstrates algorithm learns quickly perform accurate inference latent graphical models. also algorithm learn model synthetic training dataset compare performance training time algorithm. results plotted figure algorithm achieves comparable learning performance much faster train consider computational biology task classifying splice junctions sequences test learning algorithm using splice dataset machine learning repository dataset contains total length- sequences. sequence labeled three diﬀerent categories intron/exon site exon/intron site neither. goal learn classify unseen instances. adopt generative approach second-order nonhomogeneous hidrandomly local conditional probability tables directed graphical model ground truth model parameters sample dataset sets joint observations observable variables. next apply algorithm learn model evaluate performance task inferring posterior distribution variable given observed values variables compute kullback-leibler divergence algorithm’s inferred posterior ground truth posterior calculated using exact shafer–shenoy alcal models loopy structures since feature descriptors continuous-valued complex distributions. discussed section algorithm smoothly handle learning problem models rkhs embeddings. experiment using gaussian radial basis function kernels bandwidth parameter regularization parameter model length able achieve overall recognition accuracy figure plots recognition accuracies normalized confusion matrix results. developed algorithm learning general latent-variable graphical models using predictive belief propagation two-stage instrumental regression rkhs embeddings distributions. proved algorithm gives consistent estimator inference results. evaluate algorithm’s learning performance synthetic real datasets showing learns diﬀerent types latent graphical model eﬃciently achieves good inference results discrete continuous domains. believe algorithm provides powerful ﬂexible learning framework latent graphical models. dern markov models model splice junction sequences. three categories algorithm learn diﬀerent second-order nonhomogeneous hmm. test time compute probabilities test instance generated three second-order hmms choosing highest probability predicted category. able achieve overall classiﬁcation accuracy detailed results reported figure experiment consider computer vision problem recognizing human actions videos using classic human action dataset dataset contains total video sequences diﬀerent categories actions boxing handclapping handwaving jogging running walking. choose second-order non-homogeneous state space model model generative process human action videos depicted figure video episode dataset take frames evenly spaced across whole time frame extract -dimensional histogram oriented gradients feature vector -dimensional histogram optical feature vector frames. training concatenate feature vectors -dimensional vector summarizes frame dimensionality reduction reduce dimensions. dimensional feature vectors observable variables latent graphical model. action categories algorithm learn diﬀerent second-order nonhomogeneous state space model. test time compute probabilities test video sequence generated second-order state space models choosing highest probability predicted action category. formulation poses diﬃcult problem learning continuous-valued latent-variable graphisingh james rudary predictive state representations theory modeling dynamical systems. proceedings conference uncertainty artiﬁcial intelligence", "year": 2017}