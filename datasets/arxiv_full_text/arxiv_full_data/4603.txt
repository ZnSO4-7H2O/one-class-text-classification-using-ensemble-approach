{"title": "Model-Based Bayesian Exploration", "tag": ["cs.AI", "cs.LG"], "abstract": "Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information - the expected improvement in future decision quality arising from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper we investigate ways of representing and reasoning about this uncertainty in algorithms where the system attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q-values based on these. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation.", "text": "reinforcement balancing exploitation benefit notion value information expected provement future decision quality arising tnformation estimating agent's uncer­ quantity requires tainty paper investigate uncertainty attempts learn model environment. explicitly model build probability values based these. distributions compute myopic approximation infor­ mation action hence select action best balances exploration reinforcement agent learn dynamic environments. important learning paradigm domains agent must consider sequences actions made throughout framework underlying much reinforcement cesses processes tions stochastic various states environments. compute choice actions maximizes expected future reward. task reinforcement learning achieve level performance known advance. models model-free optimal policies surrounding directly approximating desirability model-based environment's expected value actions environment. quantify perform. gives handle exploitation exploration volves dilemma whether exploreperform actions lead uncharted ploit-perform actions cording current knowledge. model expectations sible results duce bayesian model-free q-values actions represented distributions. q-values poorly known regions state space. approach based decision-theoretic tion agent choose actions based value information expect learn performing dearden eta. propose measure bal­ bution possible models estimate distribution possible models allows avoid problem faced model-free methods used dearden neect perform repeated actions propagate main ques­ tion estimate q-values distribu­ tion possible models. present several methods stochastic tions. evaluate performance bayesian learning signed fool many exploration paper focus infinite-horizon mops discount factor agent's maximize pected discounted equivalently compute optimal value function q-function functions optimal policy agent know since know dynamics underlying cannot compute q-value function estimate timates treating underlying used performing bellman equations. usually directly standard tions correct planning +-maxae.a ls'es fitv agent's approximate agent's approximation dated. prioritized sweeping method estimates value consequence dynamics previous value propagations. signed priorities values states highest perform value propagation. assumed transition d�richlet prior represent state observed executing similarly parameters rior distribution predict probability ward also compute probability sample distribution prior posterior infinite number mops. show possible adopting sults bayesian learning models bayesian networks also satisfies eter independence parameter independence. consequence posterior bitrarily form learning local learning problems. probabil­ distribution states rewards. question learn distributions. well-known bayesian methods learning standard distributions multinomials rameterizations transition reward models. example state described several attributes might bayesian network capture dynam­ ics. structure thus learn fewer examples. nonetheless much discussion independence methods maintain point estimate estimates often close mean prediction bayesian method. however point estimates capture uncertainty model. paper examine knowledge uncertainty improve exploration. value information recent paper dearden free bayesian reinforcement builds notion q-value expected reward execute continue optimal selection since dur­ learning model dis­ tribution bution induced belief state possible mdps q-values mdps. model­ free case dearden propose approach estimat­ q-value distributions approach makes several strong assumptions violated representation estimates briefly review dearden q-value distri­ butions selecting current work. notation. denote possible value mdp. treat quantities ran­ variables tion following ence belief state mathematical consider gained learning ture rewards? clearly knowledge change agent's policy future rewards would change. thus interesting knowledge change agent's policy. cases knowledge shows action previously best choice knowledge indicates previously considered actions. where again actions best second best expected values respectively. since agent know advance value revealed need compute expected b�liefs. hence expected value perfect information estimating estimate amine several methods different naive sampling perhaps simplest approach simulate definition q-value distribution. possible each. instead solve distribution standard techniques linear pro­ gramming). state action samp optima value given i'th mdp. sample estimate properties if)' denote weight sample given initially efficient sampling procedure. made priors helps prior form sample distribution problem reduces sampling \"simple\" posterior dis­ tributions. methods. sparse-multinomials complex solvable. sampling methods. importance sampling immediate problem naive sampling approach requires value functions agent. clearly expensive. possible avoiding repeated computations reuse sampled mdps several steps. ideas imponance weighted samples estimate mean different actions. weighted sample leads correct prediction large number samples. practice importance sampling depends difference distributions. cording probability small even ifpr high. term easily computed ill) easily computed based posteriors. experience models choosing actions. fast since already computed q-value pair models additional compu­ tations needed. comes irrelevant mdp. total weight sampled mdps track unlikely ini­ tially weight learn usually comes smaller. kmin sample kmin mdps current belief state assigning weight thus bringing total weight sample again. solve newly sampled mdps. q-values estimate bution. were-weight newly gained knowledge. finally method detecting global sampling repair previous section global sampling approach serious deficiency. involves computing global solu­ although tions mdps expensive. reuse mdps previous steps approach still quires based learning prioritized ning instantiation stantiations pair sampled sample current tions prioritized puted mdps good approximation true value. ples simplest ples representation bution global samples re-sample proach. points. tation choosing sampling sults method similar recent methods used successfully monitoring namic processes proaches. proximate sians fixed variance sample. proach effective careful choosing variance parameter. small variance lead spiky distribution overly smooth flat distribution. estimating kernel estimation width) sample values used generate distributions gaussian approximation proximation skewed better approximation reason expect kernel gaussian approximation figure shows domains type tested algorithms. four action maze domain agent begins collect ceives reward flag collects moves agent goal state problem reset. enters square marked receives action future discounted reward received agent. measure rather value learned policy exploratory follow either greedy policy discovered current exploration prioritized sweeping tbored parameter optimized estimation graph shows techniques sweeping process. avoiding widely find three techniques global requirements performs considerable sampling repair. global sampling converge paper makes main contributions. show maintain show ideas appear bayesian els. second choose actions ploitation. dearden approximate making choices. work kearns singh approach divides ones learner quite confident transition estimate transition true distribution. known states. constructs account better highly-rewarding). state proceeds recomputes singh's proposal bayesian fident state (i.e. also chooses actions based known rewards main difference fication states takes account possible ration. tremely teresting\" grateful stuart russell. killam predoctoral project done friedman friedman david andre supported muri program \"integrated intelligent grant number n---. friedman ichael sacher trust. david national defense science", "year": 2013}