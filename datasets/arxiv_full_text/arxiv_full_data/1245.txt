{"title": "Learning the Number of Neurons in Deep Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80\\% while retaining or even improving the network accuracy.", "text": "nowadays number layers neurons layer deep network typically manually. deep wide networks proven effective general come high memory computation cost thus making impractical constrained platforms. networks however known many redundant parameters could thus principle replaced compact architectures. paper introduce approach automatically determining number neurons layer deep network learning. propose make group sparsity regularizer parameters network group deﬁned single neuron. starting overcomplete network show approach reduce number parameters retaining even improving network accuracy. thanks growing availability large-scale datasets computation power deep learning recently generated quasi-revolution many ﬁelds computer vision natural language processing. despite progress designing deep architecture task essentially remains dark art. involves deﬁning number layers neurons layer which together determine number parameters complexity model typically manually trial error. recent trend avoid issue consists building deep ultra deep networks proven expressive. this however comes signiﬁcant cost terms memory requirement speed prevent deployment networks constrained platforms test time complicate learning process exploding vanishing gradients. automatic model selection nonetheless studied past using constructive destructive approaches. starting shallow architecture constructive methods work incrementally incorporating additional parameters recently layers network main drawback approach stems fact shallow networks less expressive deep ones thus provide poor initialization adding layers. contrast destructive techniques exploit fact deep models include signiﬁcant number redundant parameters thus given initial deep network reducing keeping representation power. originally achieved removing parameters neurons little inﬂuence output. effective requires analyzing every parameter/neuron independently e.g. network hessian thus scale well large architectures. therefore recent trends performing network reduction focused training shallow thin networks mimic behavior large deep ones approach however acts post-processing step thus requires able successfully train initial deep network. paper introduce approach automatically selecting number neurons layer deep architecture simultaneously learn network. speciﬁcally method require training initial network pre-processing step. instead introduce group sparsity regularizer parameters network group deﬁned parameters neuron. setting parameters zero therefore amounts canceling inﬂuence particular neuron thus removing entirely. consequence approach depend success learning redundant network later reduce parameters instead jointly learns number relevant neurons layer parameters neurons. demonstrate effectiveness approach several network architectures using several image recognition datasets. experiments demonstrate method reduce number parameters compared complete network. furthermore reduction comes loss recognition accuracy; even typically yields improvement complete network. short approach lets automatically perform model selection also yields networks that test time effective faster require less memory. model selection deep architectures precisely determining best number parameters number layers neurons layer widely studied. currently mostly achieved manually tuning hyper-parameters using validation data relying deep networks proven effective many scenarios. large networks however come cost high memory footprint speed test time. furthermore well-known parameters networks redundant thus compact architectures could good deep ones. sparse literature model selection deep learning nonetheless exists. particular forerunner approach presented dynamically nodes existing architecture. similarly introduced constructive method incrementally grows network adding neurons. recently similar constructive strategy successfully employed ﬁnal deep network built adding layers initial shallower architecture. constructive approach however drawback shallow networks known handle non-linearities effectively deeper ones therefore initial shallow architectures easily trapped optima thus provide poor initialization constructive steps. contrast constructive methods destructive approaches model selection start initial deep network reducing keeping behavior unchanged. trend started cancel individual parameters recently comes removing entire neurons. core idea methods consists studying saliency individual parameters neurons remove little inﬂuence output network. analyzing individual parameters/neurons however quickly becomes computationally expensive large networks particularly procedure involves computing network hessian repeated multiple times learning process. consequence techniques longer pursued current large-scale era. instead recent take destructive approach consists learning shallower thinner network mimics behavior initial deep ultimately also reduces number parameters initial network. main motivation works however truly model selection rather building compact network. matter fact designing compact models also active research focus deep learning. particular context convolutional neural networks several works proposed decompose ﬁlters pre-trained network low-rank ﬁlters thus reducing number parameters however approach similarly destructive methods mentioned above acts post-processing step thus requires able successfully train initial deep network. note that general context shown two-step procedure typically outperformed one-step direct training direct approach employed developed regularizers favor eliminating parameters network thus leading lower memory requirement. regularizers minimized simultaneously network learned thus pre-training required. however individual parameters. therefore similarly parameter regularization techniques methods perform model selection; number layers neurons layer determined manually won’t affected learning. contrast paper introduce approach automatically determine number neurons layer deep network. design regularizer-based formulation therefore rely pre-training. words approach performs model selection produces compact network single coherent learning framework. best knowledge three works studied similar group sparsity regularizers deep networks. however focuses last fully-connected layer obtain compact model considered small networks. approach scales datasets architectures orders magnitude larger last works minimum training overhead. furthermore three methods deﬁne single global regularizer. contrast work per-layer fashion found effective reduce number neurons large factors without accuracy drop. introduce approach automatically determining number neurons layer deep network learning network parameters. describe framework general deep network discuss speciﬁc architectures experiments section. general deep network described succession layers performing linear operations input intertwined non-linearities rectiﬁed linear units sigmoids potentially pooling operations. layer consists neurons encoded parameters bias. }≤n≤nl. given altogether parameters form parameter {θl}≤l≤l input signal image output network written encodes succession linear non-linear pooling operations. given training consisting input-output pairs {}≤i≤n learning parameters network expressed solving optimization form loss compares network prediction ground-truth output logistic loss classiﬁcation square loss regression regularizer acting network parameters. popular choices regularizer include weight-decay i.e. -norm sparsity-inducing norms e.g. -norm. recall goal automatically determine number neurons layer network. propose starting overcomplete network canceling inﬂuence neurons. note none standard regularizers mentioned achieve goal former favors small parameter values latter tends cancel individual parameters complete neurons. fact neuron encoded group parameters goal therefore translates making entire groups zero. achieve this make notion group sparsity particular write regularizer weight different layer practice however found effective different weights relatively small ﬁrst layers larger weight remaining ones. effectively prevents killing many neurons ﬁrst layers thus retains enough information remaining ones. group sparsity lets effectively remove neurons exploiting standard regularizers individual parameters proven effective past generalization purpose leverage idea within automatic model selection approach propose exploit sparse group lasso idea lets write regularizer sets relative inﬂuence terms. note brings back regularizer practice experimented solve problem regularizer deﬁned either follow proximal gradient descent approach context proximal gradient descent only resulting solution applying proximal operator regularizer. case since groups non-overlapping apply proximal operator group independently. speciﬁcally single group translates updating parameters learning algorithm therefore proceeds iteratively taking gradient step based loss only updating variables groups according practice follow stochastic gradient descent approach work mini-batches. setting apply proximal operator epoch algorithm ﬁxed number epochs. learning terminates parameters neurons gone zero. thus remove neurons entirely since effect output. furthermore considering fully-connected layers neurons acting output zeroed-out neurons previous layer also become useless thus removed. ultimately removing neurons yields compact architecture original overcomplete one. section demonstrate ability method automatically determine number neurons task large-scale classiﬁcation. study three different architectures analyze behavior method three different datasets particular focus parameter reduction. below ﬁrst describe experimental setup discuss results. additional experiments character recognition dataset imagenet contains million labeled images split categories. used ilsvrc- subset consisting categories million training images validation images. places- large-scale dataset speciﬁcally created high-level visual understanding tasks. consists million images unique scene categories. training comprises images category. finally icdar character recognition dataset consists training test samples split categories. training samples depict characters collected text observed number scenes synthesized datasets test comes icdar training removing non-alphanumeric characters. architectures imagenet places- architectures based vgg-b network decomposeme bnet consists convolutional layers followed three fully-connected layers. experiments removed ﬁrst fully-connected layers. shown results reduces number parameters maintains accuracy original network. below refer modiﬁed architecture bnetc. following idea low-rank ﬁlters consists convolutional layers kernels effectively modeling convolutional layers. icdar used architecture similar original architecture consists three convolutional layers maxout layer convolution followed fully-connected layer. ﬁrst trained network decomposed convolution kernels. here instead directly start convolutional layers. furthermore replaced maxout layers max-pooling. shown below architecture referred yields similar results original referred maxout. implementation details comparison fair models including baselines trained scratch computer using random seed framework. speciﬁcally imagenet places- used torch- multi-gpu framework dual xeon -core using three kepler tesla gpus parallel. models trained total epochs batches epoch batch size bnet respectively. variations batch size mainly memory runtime limitations bnet. learning rate initial value multiplied data augmentation done random crops random horizontal ﬂips probability icdar trained network single tesla total epochs batch size iterations epoch. case learning rate initial value multiplied second seventh ﬁfteenth epochs. used momentum terms hyper-parameters large-scale classiﬁcation used ﬁrst three layers remaining ones. icdar used ﬁrst layer remaining ones. evaluation measure classiﬁcation performance top- accuracy using center crop referred top-. compare results approach obtained training architectures without model selection technique. also provide results additional standard architectures. furthermore since approach determine number neurons layer also computed results method starting different number neurons referred below overcomplete network. addition accuracy also report convolutional layers percentage neurons approach corresponding percentage zero-valued parameters total percentage parameters additionally includes parameters non-completely zeroed-out neurons total percentage zero-valued parameters induced zeroed-out neurons additionally includes neurons layer including last fully-connected layer rendered useless zeroed-out neurons previous layer. figure parameter reduction imagenet using bnetc. comparison number neurons layer original network obtained using approach. percentage zeroed-out neurons parameters accuracy network original one. note outperform original network requiring much fewer parameters. evaluated additional versions that instead neurons layer original architecture neurons layer respectively. finally case evaluated group sparsity regularizer sparse group lasso regularizer table compares top- accuracy approach original architectures baselines. note that exception dec- methods yield improvement original network difference bnetc dec-. additional baseline also evaluated naive approach consisting reducing layer model constant factor corresponding instances dec% importantly figure figure report relative saving obtained approach terms percentage zeroed-out neurons/parameters bnetc respectively. bnetc figure approach reduces number neurons improving generalization ability indicated accuracy bottom table. seen bar-plot reduction number neurons spread layers largest difference last layer. direct consequence number neurons subsequent fully connected layer signiﬁcantly reduced leading reduction total number parameters. figure that considering original architecture neurons layer approach yields small reduction parameter numbers minimal gain performance. however increase initial number neurons layer beneﬁts approach become signiﬁcant. using group sparsity regularizer reduction number parameters improved generalization ability. reduction even larger using sparse group lasso regularizer. case managed remove neurons translates parameters. while here accuracy slightly lower initial network fact higher original network seen table interestingly learning also noticed signiﬁcant reduction training-validation accuracy applying regularization technique. instance dec- zeroes parameters found training-validation smaller original network believe indicates networks trained using approach better generalization ability even fewer parameters. similar phenomenon also observed architectures used experiments. analyze sensitivity method respect considered varied value parameter range speciﬁcally considered different pairs values former applied figure parameter reduction using imagenet. note signiﬁcantly reduce number parameters almost cases improve recognition accuracy original network. ﬁrst three layers latter remaining ones. details experiment reported supplementary material. altogether observed small variations validation accuracy number zeroed-out neurons icdar finally evaluate approach smaller dataset architectures heavily tuned. dataset used architecture last layers initially contain neurons. goal obtain optimal architecture dataset. figure summarizes results using regularization compares state-of-the-art baselines. comparison maxpooldneurons learning ﬁlters leads better performance equivalent network kernels. importantly algorithm reduces number parameters improving performance original network. believe results evidence algorithm effectively performs automatic model selection given task. discuss beneﬁts algorithm test time. simplicity implementation remove neurons training. however neurons effectively removed training thus yielding smaller network deploy test time. entail beneﬁts terms memory requirement illustrated looking reduction number parameters also leads speedups compared complete network. demonstrate this table report relative runtime speedups obtained removing zeroed-out neurons. bnet speedups obtained using imagenet tested icdar. note signiﬁcant speedups achieved depending architecture. instance using bnetc achieve speedup imagenet icdar speedup reaches almost right-hand side table shows relative memory saving networks. numbers computed actual memory requirements networks. terms parameters imagenet dec- yields reduction increases saving looking actual features computed layer network reach memory saving dec- saving dec. believe numbers clearly evidence beneﬁts approach terms speed memory footprint test time. note also that models trained additional parameters pruned using level individual parameters regularization threshold imagenet table gain runtime memory requirement reduced networks. note that conﬁgurations ﬁnal networks achieve speedup close similarly achieve memory savings terms parameters terms features computed network. runtimes obtained using single tesla memory estimations using rgb-images size ours-bnetc ours-dec-gs ours-dec-gs gray level images size ours-dec−gs. introduced approach automatically determining number neurons layer deep network. proposed rely group sparsity regularizer allowed jointly learn number neurons parameter values single coherent framework. approach estimate number neurons also yields compact architecture initial overcomplete network thus saving memory computation test time. experiments demonstrated beneﬁts method well generalizability different architectures. current limitation approach number layers network remains ﬁxed. address this future intend study architectures layer potentially bypassed entirely thus ultimately canceling inﬂuence. furthermore plan evaluate behavior approach types problems regression networks autoencoders. authors thank john taylor helpful discussions continuous support using csiro high-performance computing facilities. authors also thank nvidia generous hardware donations. wang huang zhang xue. learning block group sparse representation combined convolutional neural networks rgb-d object recognition. journal fiber bioengineering informatics", "year": 2016}