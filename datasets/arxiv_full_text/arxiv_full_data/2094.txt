{"title": "Encapsulating models and approximate inference programs in probabilistic  modules", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "This paper introduces the probabilistic module interface, which allows encapsulation of complex probabilistic models with latent variables alongside custom stochastic approximate inference machinery, and provides a platform-agnostic abstraction barrier separating the model internals from the host probabilistic inference system. The interface can be seen as a stochastic generalization of a standard simulation and density interface for probabilistic primitives. We show that sound approximate inference algorithms can be constructed for networks of probabilistic modules, and we demonstrate that the interface can be implemented using learned stochastic inference networks and MCMC and SMC approximate inference programs.", "text": "paper introduces probabilistic module interface allows encapsulation complex probabilistic models latent variables alongside custom stochastic approximate inference machinery provides platform-agnostic abstraction barrier separating model internals host probabilistic inference system. interface seen stochastic generalization standard simulation density interface probabilistic primitives. show sound approximate inference algorithms constructed networks probabilistic modules demonstrate interface implemented using learned stochastic inference networks mcmc approximate inference programs. present probabilistic module interface allows encapsulation complex latent variable models custom stochastic approximate inference machinery. modules interface seen generalization previously proposed interfaces elementary random procedures probabilistic programming languages require module author specify marginal input-output density. instead module authors obligated provide stochastically regenerate traces internal latent variables subject constraints module’s output provide calculate weight regeneration. show sufﬁcient constructing sound approximate inference algorithms networks modules including metropolis-hastings procedure seen module-level analogue single-site metropolis-hastings procedures commonly used lightweight implementations probabilistic programming languages paper illustrates module networks deﬁning mathematical interface providing example application linear regression outliers. application contains modules complex prior binary model selection variable determining prior prevalence outliers using learned bottom-up network regeneration linear regression model binary outlier indicators using sequential monte carlo regeneration outlier indicators several existing probabilistic programming systems probabilistic modeling primitives implement simulator procedure samples outputs given inputs distribution log-density evaluation procedure evaluates together procedures enable inference programs valid approximate inference algorithms mcmc composite probabilistic model. interface summarized figure figure encapsulating latent variables probabilistic model probabilistic module shows original probabilistic model latent variables abstracted away dashed box. shows model latent variables made internal auxiliary variables probabilistic module output input shows declarative semantics resulting module network. propose stochastic generalization interface called probabilistic modules interface replaces logpdf stochastic generalization called regenerate. unlike simulate logpdf interface probabilistic modules interface summarized figure able represent probabilistic computations involve internal ‘auxiliary’ random variables made possible implementing sampler samples values auxiliary variables given inputs outputs regeneration distribution denoted figure shows standard simulate logpdf interface elementary random procedures inputs outputs shows stochastic generalization probabilistic modules interface module inputs module auxiliary variables module outputs auxiliary variables regenerate procedure reduces deterministic logpdf procedure. presence auxiliary variables regenerate understood using unbiased single-sample importance sampling estimate output probability importance distribution eu|xz∼q indeed extreme setting regeneration distribution identical conditional distribution auxiliary variables given inputs outputs estimate deterministic exact regenerate identical logpdf. finally note probabilistic modules interface require auxiliary variables stored memory once. useful log-weight log/q) incrementally computed sampling cases discussed section compose probabilistic modules directed acyclic graph resulting probabilistic module network declarative semantics bayesian network nodes module outputs module network bayesian network represent joint distribution module outputs module auxiliary variables marginalized out. existence auxiliary variables modules changes approximate inference performed. valid mcmc algorithms easily constructed probabilistic module networks. fact existing standard metropolis-hastings algorithms inference bayesian networks need slight modiﬁcation modules change required storage current log-weight probabilistic module. current log-weight module accessed lookup-log-weight updated update-log-weight) mcmc inference. values initialized running simulate module whose output observed regenerate module whose output oberved following topological ordering nodes network. note single-site bayesian network lookup-log-weight call regenerate call algorithm replaced logpdf. markov chain constructed mixtures cycles update algorithm admits posterior marginal stationary distribution deﬁned space unobserved module outputs module auxiliary variables algorithm single-site metropolis-hastings update probabilistic module network require module whose output update proposal distribution children uniform show encapsulate probabilistic model internal latents outputs probabilistic module declarative semantics marginal distribution outputs shown figure useful high dimensional analytically intractable unable implement logpdf marginalizing exactly. begin deﬁning module auxiliary variables model’s internal latents then probabilistic module interface requires construct sampler regeneration distribution approximation efﬁciently sometimes possible learn sampler compute log-weight log/q). pioneering example approach ‘stochastic inverses’ learn model regeneration sampler time log-weight tractable. illustrate approach module figure uses learned stochastic inverse network trained using samples prior model described log-weight tractable learned contains additional random variables beyond model however wish generally applicable stochastic inference programs implementing mcmc sequential monte carlo regeneration distribution possible compute log/q) marginal output density stochastic inference program intractable. handle cases augment auxiliary variables module include execution history stochastic inference program deﬁne distribution sampled regenerate joint distribution stochastic inference program execution history output denoted extend distribution sampled simulate also sample execution history alonside model latents using ‘meta-inference’ program samples inference execution history given inference output distribution approximates conditional distribution inference execution histories shown possible construct meta-inference programs sequential variants mcmc using detailed balance transition kernels multiple-particle optional detailed balance transition kernels log-weight logm/q) efﬁciently computed sampling accuracy improves shows latent variable models encapsulated probabilistic modules composed probabilistic module network. module encodes prior distribution determines prior prevalence outliers. module encodes linear regression outlier model. treat modules like single node bayesian network lack marginal output density perform stochastic inversion module. shows data regeneration modules. module uses learned stochastic inverse network module uses sequential monte carlo regeneration. despite approximate permit valid metropolis-hastings exposed latent variable marginal output densities modules. shows traces total log-weight mcmc using algorithm model observed values total log-weight varies stochastically even static. shows rendering latent variables model encapsulated module dataset point chain shows internal probabilistic model module research supported darpa iarpa ofﬁce naval research army research ofﬁce gifts analog devices google. research conducted government support awarded force ofﬁce scientiﬁc research national defense science engineering graduate fellowship david wingate andreas stuhlmüller noah goodman. lightweight implementations probabilistic programming languages transformational compilation. aistats pages vikash mansinghka daniel selsam yura perov. venture higher-order probabilistic programming platform programmable inference. arxiv preprint arxiv. vikash mansinghka richard tibbetts baxter shafto baxter eaves. bayesdb probabilistic programming system querying probable implications data. arxiv preprint arxiv. quaid morris. recognition networks approximate inference networks. proceedings seventeenth conference uncertainty artiﬁcial intelligence pages morgan kaufmann publishers inc. marco cusumano-towner vikash mansinghka. measuring non-asymptotic convergence sequential monte carlo samplers using probabilistic programming. submitted nips workshop advances approximate bayesian inference appendix deriving single-site metropolis hastings module network consider network probabilistic modules indexed denote internal auxiliary variables module denote output module denote inputs module module input tuple module outpus sequence parent module indices module denote children module denote simulation distribution module denote regeneration distribution. deﬁne collection module auxiliary variables collection module outputs declarative semantics module network derived marginal distribution outputs suppose subset modules observed meaning output constrained value target distribution network deﬁned pj∈o|i∈o). derive metropolis-hastings algorithm stationary distribution distribution pj∈o|i∈o). algorithm converges joint distribution marginal converges network’s target distribution. consider targeting distribution pj∈o|i∈o) propose value output module denote state prior update. propose value modules perform accept/reject step using proposal ‘local’ posterior pj∈ci|xi j∈ci j∈ci target distribution. acceptance ratio acceptance ratio used algorithm therefore algorithm corresponds valid update used compose valid algorithms converge posterior pj∈o|i∈o) single-site random-scan mixture algorithm applications unobserved modules", "year": 2016}