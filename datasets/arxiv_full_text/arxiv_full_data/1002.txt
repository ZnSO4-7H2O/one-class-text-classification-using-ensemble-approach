{"title": "Single-Solution Hypervolume Maximization and its use for Improving  Generalization of Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper introduces the hypervolume maximization with a single solution as an alternative to the mean loss minimization. The relationship between the two problems is proved through bounds on the cost function when an optimal solution to one of the problems is evaluated on the other, with a hyperparameter to control the similarity between the two problems. This same hyperparameter allows higher weight to be placed on samples with higher loss when computing the hypervolume's gradient, whose normalized version can range from the mean loss to the max loss. An experiment on MNIST with a neural network is used to validate the theory developed, showing that the hypervolume maximization can behave similarly to the mean loss minimization and can also provide better performance, resulting on a 20% reduction of the classification error on the test set.", "text": "paper introduces hypervolume maximization single solution alternative mean loss minimization. relationship problems proved bounds cost function optimal solution problems evaluated other hyperparameter control similarity problems. hyperparameter allows higher weight placed samples higher loss computing hypervolume’s gradient whose normalized version range mean loss loss. experiment mnist neural network used validate theory developed showing hypervolume maximization behave similarly mean loss minimization also provide better performance resulting reduction classiﬁcation error test set. many machine learning algorithms including neural networks divided three parts model used describe approximate structure present training data set; loss function deﬁnes well instance model samples; optimization method adjusts model’s parameters improve error expressed loss function. obviously three parts related generalization capability obtained solution depends individual merit three parts also interplay. different appli; koller friedman cations data types optimization methods allow faster convergence robustness better chance escape poor local minima. hand many cost functions come statistical models quadratic error cross-entropy variational bound although terms regularization necessarily statistical basis building total cost sample frequently costs sample plus regularization terms whole dataset. although methodology sound problematic real-world applications involving complex models. speciﬁcally learning problem viewed multi-objective optimization perspective minimization cost sample individually every efﬁcient solution achieved convex combination costs pareto-based solutions might provide better results alternative minimizing convex combination maximize metric known hypervolume used measure quality samples. algorithms usually search many solutions different trade-offs objectives time used ensemble instance ability evaluate whole solutions instead single made metric widely used computation hypervolume np-complete making hard used many objectives candidate solutions. nonetheless particular case single candidate solution being used computed linear time number objectives makes computing time equal objectives minima redundant objectives ignored optimization. however minima different example single optimal point different trade-offs objectives. solution establishes optimal tradeoff impossible reduce objectives without increasing another said efﬁcient. efﬁcient solutions called pareto counterpart objective space called pareto frontier. approach frequently found learning regularization objective decrease loss training another decrease model complexity multiple objectives combined weights regularization terms balance trade-off. examples technique include softmargin support vector machines semi-supervised models adversarial examples among many others. although optimal solution linearly combined problem guaranteed efﬁcient possible achieve efﬁcient solution objectives convex means desired solutions achievable performing linear combination objectives pareto-based approaches used creation hypervolume indicator moo. since linear combination objectives going work properly non-convex objectives desirable investigate forms transforming multi-objective problem single-objective allows standard optimization tools used. perspective single objective function sample paper develop theory linking maximization single-solution hypervolume minimization mean loss average cost training samples considered. provide theoretical bounds hypervolume value neighborhood optimal solution mean loss vice-versa showing bounds made arbitrarily small that limit optimal value problem also optimal other. moreover analyze gradient hypervolume showing places importance samples higher cost. since gradient optimization iterative process hypervolume maximization implies automatic reweighing samples iteration. reweighing allows hypervolume gradient range maximum loss’ mean loss’ gradient changing hyperparameter. also different optimizing linear combination mean maximum losses also considers losses intermediary samples. conjecture gradient obtained hypervolume guides improved models able express compromise ﬁtting well average sample worst samples. automatic reweighing prevents learning algorithm pursuing quick reduction mean loss requires signiﬁcant increase loss already badly represented samples. perform experiment provide empirical evidence conjecture showing using hypervolume maximization reduces classiﬁcation error compared mean loss minimization validation theory developed. paper organized follows. section provides overview multi-objective optimization properly charactering maximization hypervolume performance indicator learning system. section presents theoretical developments paper section describes experiment performed validate theory provides evidence conjectures developed paper. finally section outlines concluding remarks future research directions. multi-objective optimization generalization traditional single-objective optimization problem composed multiple objective functions using standard notation problem learning linear combination objectives discussed sec. however also discussed sec. approach limits number solutions obtained losses convex motivates hypervolume indicator. since objectives differ samples used loss function considering samples equal importance nadir point value objectives solution found balanced losses. value given parameter problem becomes maximizing relation vector ones size deﬁned section develop theory linking singlesolution hypervolume maximization minimization mean loss common optimization objective. first deﬁne requirements loss function must satisfy describe optimization problems sec. then given optimal solution problem show sec. bounds loss optimality problem neighborhood given solution show bounds made arbitrarily small changing reference point. finally sec. show transform gradient hypervolume format similar convex combination gradients loss used experiments sec. show advantage using hypervolume maximization. reference point vector obtained stacking objectives dominance operator similar comparator deﬁned indicator operator. problem becomes maximizing hypervolume domain optimization able achieve larger number efﬁcient points without requiring convexity pareto frontier although hypervolume frequently used analyze candidate solutions state-of-the-art algorithms expensive compute np-complete however single solution computed linear time logarithm written among many properties hypervolume must highlighted paper. first hypervolume monotonic objectives means reduction objective causes hypervolume increase turn aligned loss minimization. maximum single-solution hypervolume point pareto means solution efﬁcient. second property that like linear combination also maintains shape information objectives. objectives convex linear combination convex hypervolume concave since concave logarithm concave function concave. common objective machine learning minimization loss function given data note notation includes supervised unsupervised learning space include samples targets. simplicity speciﬁc data considered. note eqs. multiplying regular bounds continuous differentiability functions. knowledge given optimal problem provide additional information. however made arbitrarily small making large increasing allows information shared among problems loss surfaces become closer. practical application theorems that given value region check whether reference point large enough optimizing similar optimizing bound around optimal solution viceversa difference bound real value vanishing gets smaller gets larger. conjecture automatic control relevance beneﬁcial learning function better generalization model forced focus capacity samples able represent well. moreover hyperparameter provides control much difference importance placed samples shown below. similar soft-margin support vector machines change regularization hyperparameter control much margin reduced order better misclassiﬁed samples. provide empirical evidence conjecture sec. given gradient change changing avoided optimization real scenarios. order stabilize gradient make similar gradient convex combination objective’s gradients justed closed points border allowed inﬁnite loss. deﬁnition open subset loss functions deﬁned mean loss associated minimization problem deﬁned using deﬁnitions last section state bounds applying optimal solution problem other. proofs present section order avoid cluttering provided appendix theorem open subset loss functions deﬁned local minimum k∇lik shown sec. mean loss hypervolume problems become closer increases. limit normalized gradient hypervolume becomes equal gradient mean loss. hand close lower bound becomes mean gradient loss functions maximum value. particular loss maximal value normalized gradient hypervolume becomes equal gradient maximum loss. validate single-solution hypervolume instead mean loss training neural networks used lenet-like network mnist dataset network composed three layers relu activation ﬁrst layers convolutions ﬁlters respectively size followed max-pooling size last layer fully connected composed hidden units dropout probability learning performed gradient descent base learning rate momentum selected using validation provide best performance mean loss optimization minibatches samples. iterations without improvement validation error learning rate reduced factor reaches value kept constant iterations occurred. follow improvement loss functions samples mini-batch parameters’ gradients backpropagated value provides valid reference point larger values make problem closer using mean loss. tested represents mean loss allowed scheduling case decreasing learning rate learning stalled incremented next value also considered possibility evaluate effect using mean together schedule. figure shows results scenario considered. first note using provided results close mean loss throughout iterations scenarios empirically validates theory large values makes maximization hypervolume similar minimization mean loss provides evidence large comparison loss functions happen. moreover figs. similar values provides evidence large enough approximate mean loss well including schedule change performance. hand able provide good classiﬁcation itself requiring values achieve error rate similar mean loss. although able better results help schedule shown figs. probably figure.mean number misclassiﬁed samples test runs different initial values training strategies representing mean loss representing schedule values used improvement observed. values instead providing direct beneﬁt achieved error similar mean loss schedule except used shown fig. indicates much pressure samples high loss beneﬁcial explained optimization becoming closed minimizing maximum loss discussed sec. thus ignoring samples. optimizing hypervolume starting scenarios showed improvements classiﬁcation error differences convergence optimizing hypervolume mean loss statistically signiﬁcant. moreover better results obtained schedule started smaller value provides evidence conjecture sec. placing higher pressure samples high loss represented higher values beneﬁcial might help network achieve higher generalization also warns much pressure prejudicial results show. furthermore fig. indicates that even pressure kept throughout training might improve results compared using mean loss reducing pressure training progresses improves results. suspect reducing pressure allows rare samples cannot well learned model less relevant favor common samples improves generalization overall initial pressure allowed model learn better representations data forced consider samples. presence rare samples also explain provided results learning algorithm focused mainly samples cannot appropriately learnt model instead focusing representative ones. table provides errors mean loss represented hypervolume presented best improvements. used classiﬁcation error validation select parameters used computing classiﬁcation error test set. used alone either scheduling mean both maximizing hypervolume leads reduction least paper introduced idea using hypervolume single solution optimization objective presented theory use. showed optimal solution hypervolume relates mean loss problem minimize average losses sample vice-versa providing bounds neighborhood optimal point. also showed gradient hypervolume behaves changing reference point stabilize practical applications. analysis raised conjecture using hypervolume machine learning might result better models hypervolume’s gradient composed automatically weighted average gradient sample higher weights representing higher losses. weighting makes learning algorithm focus samples well represented current parameters even means slower reduction mean loss. hence forces learning algorithm search regions samples well represented avoiding early commitment less promising regions. theory conjecture validated experiment mnist using hypervolume maximization reduction classiﬁcation errors comparison mean loss minimization. future research focus studying theoretical empirical properties single-solution hypervolume maximization provide solid explanation improvement mean loss scenarios could expected. robustness method also investigated much noise presence outliers might cause large losses opens possibility inducing learner place high importance losses detriment common cases. rifai vincent muller glorot bengio contractive auto-encoders explicit invariance feature extraction. proceedings international conference machine learning pareto- aggregation- indicator-based methods manyobjective optimization. obayashi poloni hiroyasu murata evolutionary multi-criterion optimization volume lecture notes computer science springer zitzler thiele laumanns fonseca fonseca performance assessment multiobjective optimizers analysis review. evolutionary computation ieee transactions zitzler brockhoff thiele hypervolume indicator revisited design pareto-compliant indicators weighted integration. evolutionary multi-criterion optimization springer", "year": 2016}