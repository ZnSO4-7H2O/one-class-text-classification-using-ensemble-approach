{"title": "A Method of Generating Random Weights and Biases in Feedforward Neural  Networks with Random Hidden Nodes", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Neural networks with random hidden nodes have gained increasing interest from researchers and practical applications. This is due to their unique features such as very fast training and universal approximation property. In these networks the weights and biases of hidden nodes determining the nonlinear feature mapping are set randomly and are not learned. Appropriate selection of the intervals from which weights and biases are selected is extremely important. This topic has not yet been sufficiently explored in the literature. In this work a method of generating random weights and biases is proposed. This method generates the parameters of the hidden nodes in such a way that nonlinear fragments of the activation functions are located in the input space regions with data and can be used to construct the surface approximating a nonlinear target function. The weights and biases are dependent on the input data range and activation function type. The proposed methods allows us to control the generalization degree of the model. These all lead to improvement in approximation performance of the network. Several experiments show very promising results.", "text": "domly resulting optimization task becomes convex formulated linear least-squares problem methods applied three broad families fnns recurrent randomized kernel approximations many simulation studies reported literature show high performance randomized models compared fully adaptable ones. randomization cheaper optimization provides simplicity implementation faster training. feedforward neural networks random hidden nodes learning process require iterative tuning weights. weights biases hidden neurons need adjusted. randomly selected intervals according continuous sampling distribution remain fixed. parameters need learned output weights linking hidden output nodes. thus fnnrhn considered linear system output weights analytically determined simple generalized inverse operation hidden layer output matrices. reason learning speed thousands times faster classical gradient descent-based learning. theoretical studies shown parameters hidden nodes randomly generated uniform distribution within proper range resulting neural network universal approximator continuous function bounded finite dimensional efficient convergence rate. husmeier proved universal approximation property also holds symmetric interval setting random parameter scope function approximated meets lipschitz condition. however select range random parameters remains open question. issue considered important research gaps field randomized algorithms training algorithms suffer design choices translated free parameters difficult optimally require many trials cross-validation find good projection space authors solutions randomization give hints ranges random parameters applications fnnrhns classification regression problems ranges random parameters hidden nodes selected without scientific justification could ensure universal approximation property network. usually intervals assigned fixed weights biases) regardless bstract—neural networks random hidden nodes gained increasing interest researchers practical applications. unique features fast training universal approximation property. networks weights biases hidden nodes determining nonlinear feature mapping randomly learned. appropriate selection intervals weights biases selected extremely important. topic sufficiently explored literature. work method generating random weights biases proposed. method generates parameters hidden nodes nonlinear fragments activation functions located input space regions data used construct surface approximating nonlinear target function. weights biases dependent input data range activation function type. proposed methods allows control generalization degree model. lead improvement approximation performance network. several experiments show promising results. index terms—activation functions function approximation feedforward neural networks neural networks random hidden nodes randomized learning algorithms eedforward neural networks extensively used regression classification applications adaptive nature universal approximation property. fnns able learn observed data generalize well unseen examples. inner parameters i.e. weights biases adjustable learning process. layered structure network process complicated inefficient requires activation functions neurons differentiable. training algorithms involves optimization non-convex objective function usually employ form gradient descent method known time consuming sensitive initial values parameters converging local minima. moreover parameters number hidden nodes learning algorithm parameters tuned manually. learner model generated incrementally stochastic configuration algorithms random parameters generated inequality constraint adaptively selecting scope them ensuring universal approximation property model. authors adopt symmetric interinput weights biases generated symmetric interval. conclusion work input weights biases generated different ranges different meaning. method proposed work generate separately depending data activation function type. experimental part work compare results proposed method stochastic configuration network previous works looked inside fnnrhn studied neurons compose fitting curve ranges weights biases randomly generated affect approximation ability network. works provide guidance randomly generate weights biases good performance approximation functions variable. work focus multidimensional cases. method randomly generating fnnrhn parameters nonlinear fragments input space regions containing data points proposed. method allows control flatness steepness input hypercube hence degree generalization network. remainder paper organized follows. section briefly presents fnnrhn learning algorithm. section intervals random parameters determined basis theoretical analysis one-dimensional case. analysis performed four popular sigmoidal gaussian softplus sine/cosine. similar analysis performed multidimensional case section section reports simulation study compare results proposed method newest results literature. section concludes paper. architecture fnnrhn singlehidden-layer feedforward neural network. output considered hidden neurons inputs. training hidden nodes learning algorithm consists three steps. randomly generate hidden node parameters weights problem selection appropriate ranges random parameters hidden nodes solved till today. however many works concerning fnnrhns attention drawn significance intervals random weights biases selected. conclusion rightly pointed network nodes chosen random subsequently trained usually placed accordance density input data. case training linear parameters becomes ineffective reducing errors. moreover number nodes needed approximate nonlinear grows exponentially model sensitive random parameters. improve effectiveness network authors advice combining unsupervised placement network nodes according input data density subsequent supervised reinforcement learning values linear parameters approximator. work motivated authors highlight risky aspects caused randomness fnnrhn illogical simply selecting trivial range random assignment input weights biases. analyze impacts scope random parameters model performance empirically show widely used setting scope misleading. although observe specific scopes network performs better learning generalization scopes give tips select appropriate scopes. tips also authors investigate range random parameters introducing scaling factor control range. work concluded scaling randomization range avoid saturating neurons risk degenerating discrimination power random features. scaling range enhance discrimination power random features risk saturating neurons. papers find suggestions generate random parameters hidden neurons. early work fnns randomization parameters hidden nodes uniform random values authors suggest optimize range appropriate range specified application. author suggests symmetric \"large enough\" boundaries hidden node parameters advices optimize training process. valuable remark selection hidden node parameters appears generated random scaled avoid saturation unfortunately tips scaling given. details generating random parameters hidden nodes given. weights chosen normal distribution zero mean specified variance adjusted obtain input-to-node values saturate sigmoids. biases computed center sigmoid training points. distributes sigmoids across input space suggested nguyen–widrow weight initialization algorithm weight decides slope sigmoid bias shifts function along x-axis positive slope sigmoid positive negative slope negative. hidden neurons represents combined linearly produce results curve fitting using single-hidden layer hidden neurons fig. shown. trained using levenberg-marquardt algorithm middle chart shows hidden neurons parameters parameters determined learning process well output weights bottom chart shows multiplied output weights curves gives drawn solid line upper chart. note nonlinear steep fragments inside fragments used compose expresses complex behavior function distributed steep fragments correspond steep fragments i-th column i-th hidden node output vector respect inputs hidden neurons m-dimensional feature space thus nonlinear feature mapping. output matrix remains unchanged parameters fixed. node. presented network popular solution fnnrhn. mentioned prototype randomization i.e. random vector functional link network proposed takefji direct links input layer output one. variation function increases along left border flat towards right border expresses increasing oscillations. training contains points uniformly randomly distributed distorted adding uniform noise distributed fnnrhn hidden neurons fitting curve generate randomly weights biases interval typical fnnrhn fig. fragments flat cannot combined another example fig. shown. weights generated biases figure steep fragments left border flat. hand right border requires steep fragments flat fragments. results poor fitting. examples show problem definition appropriate intervals random weights biases. determine interval sigmoid inflection point sigmoid value note case nonlinear steepest fragment sigmoid around inflection point inside parameter lower completely flat function. decreases toward sigmoid steep thus controls flatness inflection point sigmoid value shift parameter equation shift parameter sigmoid consequently interval bias generated results fitting using gaussian hidden nodes described approach generating random weights biases fig. shown. assumed value parameters border interval blim second border interval blim note interval shift parameter dependent value slope parameter thus biases generated individually i-th interval fig. results fitting shown fnnrhn hidden nodes described approach used generating random weights biases. small value leads underfitting high value leads overfitting recommended select parameter experimentally e.g. cross-validation procedure well parameter assume gaussian building flatter thus slope parameters satisfy condition defines steepest gaussian possible. thus slope parameter i-th generated ranges substituting simplifying notation obtain assume flatter means slope parameters satisfy condition case sigmoid gaussian assume slope parameter steepest hidden nodes leads following interval slope parameter i-th obtain equation interval i-th bias sine exactly equations weight interval biases interval used. sine function shifted version cosine function. input vectors normalized belong hypercube similarly one-dimensional case weights decide slopes bias shifts along x-axes. goal find method generating random weights biases ensure inside hypercube nonlinear steep fragments afs. variation function lowest corner increases towards corner training contains points components independently uniformly randomly distributed distorted adding uniform noise distributed testing size created similarly without noise. outputs normalized range shift bias determination step randomly selected point training point. ensures nonlinear fragments regions containing data. modification method generating biases random integer uniformly distributed alternative choosing select regions input space variable another idea calculate biases group training points clusters. prototypes clusters taken points shifted analysis multidimensional case table summarized. proposed process generating random weights biases fnnrhn shown algorithm section proposed method fnnrhn random parameters generation illustrated several examples. results compared state-of-the-art method proposed recently well modified quickprop incremental random vector functional link network results comparative models taken well regression problems including function approximation three real-world modeling tasks tober aerospace companies. task approximate price company given prices rest. samples composed nine input variables output variable. whole data divided training containing samples selected randomly test containing remaining samples. strength ingredients cement blast furnace slag water superplasticizer coarse aggregate fine aggregate. task approximate highly nonlinear relationship concrete compressive strength ingredients age. samples composed eight input variables output variable. whole data divided training test parts manner stock data set. compactiv computer activity dataset collection computer systems activity measures. data collected sparcstation mbytes memory running multi-user university department. task predict portion time cpus user mode. samples composed input variables output variable. whole data divided training test parts manner stock data set. datasets stock concrete compactiv downloaded keel dataset repository input output variables normalized results reported work take averages independent trials. root mean squares error used measure modeling accuracy. iteratively finds appropriate parameters hidden node added incremental procedure. parameters authors follows learning rate maximum iterative number irvfl incremental random vector functional link network model built incrementally random assignment input weights biases constructive evaluation output weights using least squares method random parameters taken authors uniform distribution property built randomized learner model. among three algorithmic implementations accurate chosen signed sc-iii output weights recalculated together solving global least squares problem time hidden node added. sigmoidal activation function used hidden nodes. parameters selected authors ensure best performance. table shows results errors standard deviations fnnrhn different parameters generated using well comparative models optimal parameter values proposed method also shown table selected grid search using -fold cross-validation. procedure training parts datasets used. number hidden neurons fnnrhn cases. table proposed method allows fnnrhn achieve results worse sophisticated comparative model outperforms irvfl terms learning generalization. approximation highly nonlinear spikes method shows power achieves significantly better results models. case shown fig. irvfl shown). worse performance algorithm results method computing weights nodes added hidden layer. purpose gradient-ascent algorithm applied uses also second-order information optimizaleast squares. algorithms issue properly generate random parameters hidden neurons find orthogonal projection input space irvfl random parameters taken fixed range guarantee appropriate regression problem. results irvfl even worse searches random parameter tion objective function. problematic exploring region plateau error surface first second derivatives function optimized respect parameters nearly zero. although algorithm equipped escape mechanism regions always work. thus optimal solution cannot guaranteed optimization non-convex i.e. nonlinear hidden layer parameters. case randomized algorithms training optimization problem linear parameters thus optimization convex analytic solution husmeier \"random vector functional link networks\" neural networks conditional probability estimation forecasting beyond point predictions chapter springer g.-b. huang q.-y. c.-k. siew \"extreme learning machine theory applications\" neurocomputing vol. g.-b. huang insight extreme learning machines random neurons random features kernels\" cognitive computation vol. nguyen widrow \"improving learning speed -layer neural networks choosing initial values adaptive weights\" proc. int. joint conf. neural networks vol. ranges node added hidden layer. thus ranges optimized neuron. translates much better results fixed ranges withscientific justification. light considerations carried work assigning ranges weights biases questionable. problem strongly nonlinear \"flat\" without spikes sudden jumps. function approximated using flat fragments ranges random parameters important. problem arises approximate strongly nonlinear function. proposed method generates random parameters nonlinear steepest fragments inside region data. allows model approximate strongly nonlinear functions function spikes. weights corresponding slope parameters generated different ranges biases corresponding shift parameters different meaning parameters. mechanism selecting random weights biases simple needs tune parameters control bias-variance tradeoff network. select parameters used cross-validation. competitive algorithm works incremental mode time consuming needs five work demonstrate intervals random weights biases fnnrhn extremely important approximation properties network. activation functions hidden neurons basis functions linear combination forms surface fitting data. nonlinear target function deliver nonlinear fragments model target function nonlinear regions required accuracy. main contribution work propose practical method randomly generating weights biases fnnrhn nonlinear fragments input space region containing data points. analyzes carried lead conclusion parameters hidden nodes dependent input data range activation function type. ranges weights biases considered separately because parameters different meaning. moreover range bias i-th hidden node strictly dependent weights node. proposed method allows control flatness steepness hence degree generalization network. experimental results demonstrate approach promising. shows remarkable improvement accuracy compared existing methods modified quickprop incremental rvfl. also competitive latest solutions stochastic configuration network less complex algorithm.", "year": 2017}