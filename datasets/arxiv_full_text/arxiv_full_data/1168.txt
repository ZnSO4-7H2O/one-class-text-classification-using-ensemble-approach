{"title": "Parameter Compression of Recurrent Neural Networks and Degradation of  Short-term Memory", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "The significant computational costs of deploying neural networks in large-scale or resource constrained environments, such as data centers and mobile devices, has spurred interest in model compression, which can achieve a reduction in both arithmetic operations and storage memory. Several techniques have been proposed for reducing or compressing the parameters for feed-forward and convolutional neural networks, but less is understood about the effect of parameter compression on recurrent neural networks (RNN). In particular, the extent to which the recurrent parameters can be compressed and the impact on short-term memory performance, is not well understood. In this paper, we study the effect of complexity reduction, through singular value decomposition rank reduction, on RNN and minimal gated recurrent unit (MGRU) networks for several tasks. We show that considerable rank reduction is possible when compressing recurrent weights, even without fine tuning. Furthermore, we propose a perturbation model for the effect of general perturbations, such as a compression, on the recurrent parameters of RNNs. The model is tested against a noiseless memorization experiment that elucidates the short-term memory performance. In this way, we demonstrate that the effect of compression of recurrent parameters is dependent on the degree of temporal coherence present in the data and task. This work can guide on-the-fly RNN compression for novel environments or tasks, and provides insight for applying RNN compression in low-power devices, such as hearing aids.", "text": "abstractâ€”the significant computational costs deploying neural networks large-scale resource constrained environments data centers mobile devices spurred interest model compression achieve reduction arithmetic operations storage memory. several techniques proposed reducing compressing parameters feed-forward convolutional neural networks less understood effect parameter compression recurrent neural networks particular extent recurrent parameters compressed impact short-term memory performance well understood. paper study effect complexity reduction singular value decomposition rank reduction minimal gated recurrent unit networks several tasks. show considerable rank reduction possible compressing recurrent weights even without tuning. furthermore propose perturbation model effect general perturbations compression recurrent parameters rnns. model tested noiseless memorization experiment elucidates short-term memory performance. demonstrate effect compression recurrent parameters dependent degree temporal coherence present data task. work guide on-the-fly compression novel environments tasks provides insight applying compression low-power devices hearing aids. considerable interest deployment artificial neural network models wide range applications including biomedical devices drones mobile phones autonomous vehicles however models often extremely computationally complex hundreds millions parameters recurrent convolutional neural networks complexity neural networks imperative improved efficiency enabling novel applications resource constrained environments hearing aids. recent progress complexity reduction compression feed-forward neural networks demonstrated reductions least parameters fully connected layers. related methods applied convolutional neural networks demonstrated reduction recurrent neural networks especially interesting low-power mobile applications since often involve information. investigating pruning although compression feed-forward convolutional neural networks well understood complexity reduction impacts recurrent neural networks memory-cell based architectures long short-term memory gated recurrent unit recently proposed minimal gated recurrent unit work geras investigated using model compression train lstm always possible desirable transform practical theoretical reasons nevertheless complexity reduction rnns witnessed large-scale adoption industrial systems contrast feed-forward neural networks cnns information feedback connections indeterminate number cycles. general inference often known advance long must unfolded image captioning object detection capability makes rnns extremely powerful expressive applying understanding complexity reduction challenging result compression parameters becomes dependent temporal dependencies embedded data task fully known inference change time. paper show recurrent neural networks including using memory cell based architecture mgru achieve significant complexity reduction feedforward classification language modeling sequence prediction tasks. fundamental understanding complexity reduction viewed general perturbation corruption impacted temporal dependency. therefore devise perturbation model effect general compression method singular value decomposition rank reduction short-term memory performance recurrent networks. model tested noiseless memorization task elucidate conditions scaling short-term memory performance agrees. shown architectures demonstrate superior long short-term memory performance variety tasks. consequently reasonable presume less susceptible perturbations recurrent weights rank reduction since information could remain cell state less influence successive perturbations recurrent weights. mgru described eqn. differs additional gate switches output state linear combination input prior state represents simplest form differentiable memory cell lacks several features lstm complexity output nonlinearity input gate output gate peephole connections. eqn. represents sigmoid function forward connections input control units respectively. similarly represent recurrent connections. practice combine forward recurrent matrices single matrices forward recurrent connections. effective form complexity reduction demonstrated feed-forward convolutional neural networks rank reduction singular value decomposition network parameters. rnns forward recurrent matrix weights individually decomposed singular values orthonormal bases respectively. eliminating smallest singular values order greatest optimal reduced rank compressed representation parameters rank original dimensions particular weight matrix. rank reduced matrix-vector product viewed decomposing single layer long short-term memory networks extremely popular considerable practical success. however recently shown redundancy lstm structure architectures also renewed interest rnns owing powerful optimization algorithms hessian-free optimization suggested core attribute lstm memory cell architecture comparable performance obtained fewer possibly even single flow control gate mgru therefore best capture underlying dynamics analysis performed mgru architectures. mgru represents fundamental incarnation gated differentiable memory cell valuable understanding compression impacts gdmc-based networks. here general non-linear function hyperbolic tangent rectifying nonlinearity matrix feed-forward connections matrix recurrent connections activations previous layer. recurrent matrix transforms output layer previous time step fig. minimal gated recurrent unit cell simplest gated differentiable memory cell architecture consisting single control gate switch controls storage information cell. contrast lstm switch prevents memory cell value exploding reduces need additional output nonlinearity. recurrent connection switch shown green arrow. minimal gated recurrent unit recently proposed reduction shown fig. mgru single gate controlling memory cell state. contrast basic differentiable memory cell iii. pertubation model short-term memory strengths recurrent neural networks ability learn temporal sequence tasks requiring degree short-term memory capability learned directly data. contrast feed-forward networks recurrent neural network also unfolded deep network time shared recurrent weights every time step. information repeatedly transformed recurrent weights. result especially sensitive corruptions perturbations performance suffers dropout applied naively recurrent connections long sequence problems information corrupted perturbation recurrent weights steps. consequently reasonable suspect recurrent connections cannot benefit significantly compression. compounding problem propagation length information unfolded unknown inference time dependent specific task data encountered. therefore wish describe perturbation model understanding general perturbations recurrent weights rank reduction impact fundamental performance scaling recurrent networks. consider standard tanh activation nonlinearity biased near zero small activations perhaps sparsity activation penalty case reasonable linearize activation function within regime eqn. furthermore simplify effect arbitrary compression scheme rank reduction perturbation original weight matrix understand effect small perturbation shortterm memory noiseless memorization experiment described martens sutskever shown fig. task presented sequence bits long unfolded time-steps. t-nb network asked reproduce sequence initially presented stop-bit presented. task elucidates short-term memory performance scaling network allows model degradation perturbation complexity reduction gain fundamental insight. performance task evaluated difference ground-truth output actual output successive unfoldings. since input output weights unperturbed neglect simplicity. neglecting higher order terms eqn. model error effect perturbation. clearly regime assumptions remain valid error scales linearly temporal coherence magnitude perturbation also spectral radius recurrent weight matrix error blow nevertheless tradeoff encourage short-term memory desire reduce amplification error perturbation fig. unfolded noiseless memorization experiment described initially sequence bits presented network point unfolded time-steps. unfolding stop-word presented asks network reproduce bits initially present. performance task elucidates scaling short-term memory performance. based model expect complexity reduction network performance dependent degree temporal coherence data task often determined environment stochastic. general known inference stationary. examples image caption generation object detection crowded scenes illustrate point since output sequence highly dependent input data found field. three different tasks mgru networks. goal first experiments understand complexity reduction rank reduction separately impacts performance feed-forward recurrent connections. particular interested real-time complexity reduction rnns without benefit additional fine-tuning. lastly perform understand regime perturbation model applicable applies mgru networks. three models mgru cells compression applied single matrix recurrent connections including control gates. first experiment train recurrent language model predict next word sequence minimizing crossentropy error full vocabulary described complete works shakespeare corpus task stanford treebank tokenizer library tokenize corpus resulting corpus vocabulary words. models experiments consist single recurrent layer trained using adam optimizer dropout applied hidden layer outputs. language modeling task batch size train network continuous ordered passes corpus epochs. mgru networks single recurrent layer units word embedding matrix dimensions word. performance task measured mean perplexity full vocabulary distribution. fig. plots show performance task verses rank recurrent forward matricies rank reduction eliminating smallest singular values order least greatest. models recurrent rank rank matrix forward rank matrix. similary forward rank rank rank matrix mgru models recurrent rank rank combined matrix. show logarithmically scaled performance verses rank feed-forward recurrent connections mgru combined models. perplexity language modeling task isolines shown levels show performance mnist classification units accuracy scaling rank. incoming recurrent layer recurrent connections within recurrent layer compressed. ranks separately swept perplexity recorded entire epoch fine-tuning performed rank reduction. isolines contour plots fig. shown logarithmic increase perplexity thus increase perplexity approximately experiment relatively greater rank reduction possible feed-forward connections recurrent connections factor â€”for mgru models. without fine tuning significant rank reduction possible minor degradation performance. moreover observed practical models even greater redundancy tend highly over-parameterized comparison simplified example. second experiment performed single-layer recurrent mnist classifier. case data presented -dimension column vector time-step time-steps. effect observes image scan line time must make sense total image. output recurrent hidden layer also units temporally mean-pooled sent fully-connected output layer softmax activation cross-entropy loss classes. rank feed-forward connections dimensionality input recurrent connections. contrast language model rank feed-forward recurrent weights reduced similar ratios along isoline. however shall next experiment degree reduction dependent task data always possible achieve significant compression recurrent weights long short-term memory performance critical. understanding fig. performance root mean square error noiseless memorization task mgru networks. show error recurrent connect rank temporal delay vertical axis rescaled rank equivalent error approximated recurrent weight marix. note corresponds ranks mgru respectively. lastly show collapsed view error integrated vertical axis performance scaling perturbation model short-term memory. trained mgru noiseless memorization models fig. described recurrent hidden units. cases sequence bits drawn bernoulli distribution presented network. silent period steps observed stop-word presented network must recall sequence. period corresponds trivial case network must immediately reproduce input within single time step similarly would network remember single time step. mgru networks trained minibatches randomly drawn batch. mgru models fully converge however mgru much faster. surface average trials. large approximation breaks down error saturates almost immediately. here root mean squared error rank approximation recurrent weight matrix. estimate error scales duration plot collapsed integrating linear regime thus eqn. mean integrated error peak perturbation point perturbation model longer valid regime. results shown bottom fig. confirm error scales linearly supporting proposed perturbation model. interestingly memory cell architecture mgru exhibits similar behavior less sensitive small. indicate moderate temporal durations mgru able accurately retain shortterm memory cell state without subjecting repeated perturbations. however beyond duration becomes sensitive degradation perhaps model trained durations contrast error scales quite linearly even short durations results demonstrate feed-forward recurrent connections differentiable memory-cell architectures benefit parameter compression considerable practical benefit low-power resource constrained operating environments. unlike strictly feed-forward networks cnns compression recurrent connections impacts performance temporal domain dependent sequencial coherence data task. temporal dependency often unknown inference vary time. results suggest mgru less sensitive recurrent parameter compression faced varying temporal depenence data. finally proposed experimentally validated scaling short-term pertubation model governing parameter compression. memory performance consequently real-time compression practical applications deploying trained models field. instance estimating temporal coherence data adjusting compression real-time minimal resource utilization achieved applications ranging hearing aids mobile devices. iandola moskewicz ashraf dally keutzer squeezenet alexnet-level accuracy fewer parameters and< model size arxiv prepr. arxiv sutskever vinyals sequence sequence learning neural networks advances neural information processing systems denton zaremba bruna lecun fergus exploiting linear structure within convolutional networks efficient evaluation advances neural information processing systems", "year": 2016}