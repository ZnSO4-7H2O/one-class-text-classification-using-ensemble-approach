{"title": "Denoising random forests", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "This paper proposes a novel type of random forests called a denoising random forests that are robust against noises contained in test samples. Such noise-corrupted samples cause serious damage to the estimation performances of random forests, since unexpected child nodes are often selected and the leaf nodes that the input sample reaches are sometimes far from those for a clean sample. Our main idea for tackling this problem originates from a binary indicator vector that encodes a traversal path of a sample in the forest. Our proposed method effectively employs this vector by introducing denoising autoencoders into random forests. A denoising autoencoder can be trained with indicator vectors produced from clean and noisy input samples, and non-leaf nodes where incorrect decisions are made can be identified by comparing the input and output of the trained denoising autoencoder. Multiple traversal paths with respect to the nodes with incorrect decisions caused by the noises can then be considered for the estimation.", "text": "abstract paper proposes novel type random forests called denoising random forests robust noises contained test samples. noise-corrupted samples cause serious damage estimation performances random forests since unexpected child nodes often selected leaf nodes input sample reaches sometimes clean sample. main idea tackling problem originates binary indicator vector encodes traversal path sample forest. proposed method eﬀectively employs vector introducing denoising autoencoders random forests. denoising autoencoder trained indicator vectors produced clean noisy input samples non-leaf nodes incorrect decisions made identiﬁed comparing input output trained denoising autoencoder. multiple traversal paths respect nodes incorrect decisions caused noises considered estimation. keywords random forests regression denoising autoencoder random forests constitute ensemble learning method using multiple decision trees successful history machine learning many appealing properties attract wide range researchers engineers. ﬂexible framework makes possible apply classiﬁcation problems also regression density estimation semi-supervised learning manifold learning random forests also experimentally demonstrated perform well even handling highdimensional data problems addition inherently distributable parallel hardware architectures close ideal learner random forests also used various computer vision tasks including character recognition object recognition semantic segmentation human pose recognition fundamental problems random forests noise robustness. random forests believed robust noises contained training samples thanks ensemble properties. however work well training samples clean test samples contaminated noises. figure shows example consider regression forest decision trees -dimension features several dimensions corrupted noises noise-corrupted samples cause unexpected decisions nodes leaf nodes sample reached diﬀerent sometimes completely unlike clean samples results large estimation errors. possible solution remove noises test samples applying denoising autoencoders test samples. zhang discussed types prediction errors bias variance regression forests proposed method estimating prediction biases diﬃcult remove prediction variances. also several previous researches detect noise-corrupted test samples outliers however strategies might work well large bursty noises. insight focusing managing issue leaf decision tree stores estimation decision tree returns estimation result samples felled leaf node. readily implies reproduce estimation results noiseless samples wrong decisions nodes caused noises correct ones. recovering noiseless samples noisy ones remains diﬃcult even fully exploit various techniques brought recent progress deep learning. meanwhile detecting nodes incorrect decisions much easier approaches. paper proposes novel method boosting resistance random forests noises test samples eﬀectively incorporating insight. employ binary indicator vectors dimension number leaf nodes forest indicate leaf nodes input sample arrived. figure bottom shows examples binary indicator vector. binary indicator vector input sample encodes trajectory sample root leaf node decision tree. also nature decision trees leaf node forest corresponds part feature space thus binary indicator vectors strong correlation. shown figure proposed method called denoising random forests employs denoising autoencoders detect correct noise-corrupted decision nodes. denoising autoencoder trained indicator vectors produced clean noise-contaminated training samples enables detect nodes incorrect decisions made. element binary indicating vectors assumed generated bernoulli distribution independently clean binary indicating vectors clean training samples cannot recovered even denoising autoencoders. however described above binary indicating vectors strong correlation thus clean indicating vectors recovered denoising autoencoders. detect nodes multiple traversal paths namely left right child nodes respect nodes considered estimation. proposed method applied type decision forests however concentrate regression forests since sensitive noises types forests. following ﬁrst describe procedure regression forests. feature vector target value m-th sample respectively. random forests usually adopt bootstrap sampling create subsets thus subsets might share several samples. next t-th decision tree regression built t-th subset training samples. node tree selects feature dimension threshold maximizes following objective function depending index samples arrive n-th node selected dimension threshold average target value across samples then samples divided left right child nodes whose sample sets respectively. node reaches pre-deﬁned maximum depth target estimate ˆytn samples arrive node computed testing stage decision tree receives test sample xtest transfers leaf node according splitting functions nodes returns target estimate stored leaf node. ﬁnal output average outputs obtained test sample xtest trees forest follows described introduction figure noises contained test samples cause undesirable decisions nodes results large regression errors. empirically evaluated regression errors respect amount noises test samples using public dataset named concrete compressive strength dataset machine learning repository. experiment used trees depth node trees forest checked single dimension splitting samples. table gives result. indicates noises brought signiﬁcant impact regression errors negative impact rapidly increased noises increase. describing framework proposed method introduce main idea behind proposed method present preliminary experimental result supports idea. main idea originates binary indicator vectors indicate leaf nodes input sample arrived note leaf node decision tree directly corresponds region feature space thus binary indicator vector obtained single decision tree roughly encodes position given feature feature space. readily implies representation feature space binary indicator vectors obtained whole forest becoming redundant binary indicator vector strong correlation suﬃciently large number trees forest tree suﬃciently deep. conﬁrm assumption performed preliminary experiment compute cross correlations among decision trees. measure evaluating correlations used cross entropy computed aggregation binary indicator vectors. speciﬁcally follow scheme first entropy t-th tree given sample calculated follows number leaf nodes t-th tree subset whose samples reach n-th leaf node t-th tree small means outputs t-th tree highly biased samples cross entropy t-th t-th trees given sample average t-th tree trees computed follows small means samples felled speciﬁc leaf node tree fall nodes tree thus outputs trees highly correlated other. cross entropy inﬂuenced ratio overlapping samples used training t-th t-th trees since random forests usually adopt bootstrap sampling generate subsets training samples shown section figure shows normalized mean cross entropy averaged trees forest controlling ratio overlapping training samples bootstrap sampling. almost experimental settings section prepared another subset samples computing cross entropy diﬀerent training samples. result indicates increase ratio contributed greatly decrease cross entropy among trees. usually control ratio overlapping training samples bootstrap sampling training random forests namely training samples used bootstrap sampling general. thus experimental result shown figure supports assumption trees forest correlated other. figure shows overview proposed method called denoising random forests. note assume training samples clean test samples might corrupted noises. first training stage regression forest trained given training samples. section procedure training regression forests. next mean cross entropy deﬁned section computed tree trees entropy scores selected form reﬁned regression forest. general random forests expected high-entropy trees improving generalization performances. hand step selecting low-entropy trees signiﬁcant improve noise robustness. show eﬀectiveness step experimentally section binary indicator vectors represents traversal path training sample computed clean noisy training samples train denoising autoencoders. usual denoising autoencoders generate noisy training samples clean training samples. testing stage binary indicator vector ﬁrst extracted given test sample trained regression forest trained denoising autoencoder. detect nodes produce incorrect decisions caused noises denoising autoencoder. details detect noise-corrupted nodes described section estimate possibly correct traversal path detected nodes. estimate corrected traversal path original recovered traversal paths considered regression. details described section denoising autoencoder trained indicator vectors produced clean noisy samples assume training samples noiseless. paper used simple architecture denoising autoencoder consists input layer bottleneck layer output layer number units bottleneck layer half number units input output layers binary indicator vector highly sparse especially deep trees makes diﬃcult train denoising autoencoder. therefore develop another type indicator vector encodes inter-node distances. shown figure element indicator vector corresponds leaf improve performance recovering traversal paths preparing denoising autoencoders leaf depth also intermediate node depths. this identify nodes layers possibly incorrect decisions. testing stage traversal paths recovered trained denoising autoencoder indicator vectors might corrupted noises following steps proposed indicator vector ﬁrst computed test sample passed trained autoencoder obtain reﬁned indicator vector reﬁned indicator vector binarized element tree positive value reﬁned traversal path recovered binarized indicator vector. nodes incorrect decisions identiﬁed comparing diﬀerences original reﬁned traversal paths. introducing multiple autoencoders depth-wise recovery process starts leaf depth moves shallower depths shown figure assume perfectly recover true traversal paths denoising autoencoders simply replace original traversal paths reﬁned one. however denoising autoencoders sometimes fail undesirable traversal paths recover. instead propose estimation process considers multiple namely original reﬁned traversal paths. figure shows outline proposed multiple path estimation. already obtained nodes might yield incorrect decisions employing denoising autoencoders multiple traversal paths generated obtained nodes incorrect decisions estimation result obtained traversal path. ﬁnal estimation result weighted estimation results obtained individual traversal path. used public datasets machine learning repository namely physicochemical properties protein tertiary structure concrete compressive strength geographical origin music energy eﬃciency airfoil self-noise created noisy test samples intentionally injecting noises original features meanwhile used original data model training. although several types noises considered replaced several elements samples average value across samples. parameters decision forests optimized obtain best regression performance baseline. speciﬁcally number depth trees dataset follows protein trees depth concrete trees depth music trees depth energy trees depth airfoil trees depth first evaluated performance noise correction denoising autoencoders described section measures evaluation used precision rate namely ratio samples correct indicator vectors could recovered cross entropy ground-truth recovered indicator vectors. note adopt standard deﬁnition cross entropy evaluation diﬀerent shown section precision rate rather conservative since gains estimated indicator vector strictly equal ground-truth. meanwhile cross entropy enables measure detailed diﬀerence estimated ground-truth indicator vector. fig. shows results indicate denoising performance proposed method improved number overlapping samples training subsets increased. also proposed method recovered average true indicator vectors. next evaluated regression performances proposed method. measure evaluations used l-distance normalized ground-truth. compared proposed method standard regression forest clean test samples oracle standard regression forest noisy test samples baseline multi-path regression forest noisy test samples known ground-truth noise patterns. figure show results. naive regression forest provided best regression performances test samples noise free however performance worsened noisy test samples multi-path regression forest recovered regression performance knew true noise patterns however real-world applications cannot obtain groundtruth noise patterns. hand proposed method exhibited regression performance better naive regression forest comparable multi-path regression forest true noise patterns even though know true noise patterns. shown figure denoising autoencoders cannot perfectly recover true traversal paths. implies completely rely reﬁned traversal paths adopt multiple path estimation proposed section conﬁrm discussion perform another experiment comparing regression forest reﬁned traversal paths proposed method utilizing multiple paths. figure shows result protein dataset solid dashed lines correspond regression forest reﬁned traversal paths proposed method respectively. ﬁgure experimental result supports suggestion. think denoising autoencoders applied input features traversal patterns. answer concern comparing regression performances denoising autoencoders applying test features indicator vectors figure show experimental results. conﬁrmed proposed method applies denoising autoencoders correcting traversal paths outperformed method applied denoising autoencoders removing noises input features many datasets although gain large improvement recovering true traversal paths introducing emerging deep learning technologies extension proposed method machine learning tasks classiﬁcation density estimation. amit geman shape quantization recognition randomized trees. neural criminisi decision forests uniﬁed framework classiﬁcation regression density estimation manifold learning semi-supervised learning. foundations trends® computer graphics vision gall razavi gool lempitsky hough forests object detection tracking action recognition. ieee transactions pattern analysis machine intelligence shotton fitzgibbon cook sharp finocchio moore kipman blake real-time human pose recognition parts single depth images. studies computational intelligence wakayama murata kimura yamashita yamauchi fujiyoshi distributed forests mapreduce-based machine learning. proceedings iapr asian conference pattern recognition acpr", "year": 2017}