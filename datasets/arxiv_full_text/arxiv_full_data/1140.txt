{"title": "Improving Graph Convolutional Networks with Non-Parametric Activation  Functions", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "Graph neural networks (GNNs) are a class of neural networks that allow to efficiently perform inference on data that is associated to a graph structure, such as, e.g., citation networks or knowledge graphs. While several variants of GNNs have been proposed, they only consider simple nonlinear activation functions in their layers, such as rectifiers or squashing functions. In this paper, we investigate the use of graph convolutional networks (GCNs) when combined with more complex activation functions, able to adapt from the training data. More specifically, we extend the recently proposed kernel activation function, a non-parametric model which can be implemented easily, can be regularized with standard $\\ell_p$-norms techniques, and is smooth over its entire domain. Our experimental evaluation shows that the proposed architecture can significantly improve over its baseline, while similar improvements cannot be obtained by simply increasing the depth or size of the original GCN.", "text": "graph-frequency domain. however approach scalable scaled linearly size graph. defferard extended approach using polynomial ﬁlters frequency components rewritten directly graph domain avoiding graph fourier transform. modiﬁcations proposed kipf welling resulting generic graph convolutional network successfully applied several real-world scenarios including semi-supervised learning matrix completion program induction modeling relational data several others. like neural networks gcns interleave linear layers wherein information adaptively combined according topology graph nonlinear activation functions applied element-wise. information nodes combined obtain graph-level prediction kept separate obtain node-speciﬁc inference literature gcns worked simple choices activation functions rectiﬁed linear unit however well known choice function large impact ﬁnal performance neural network. particularly large literature standard neural networks design ﬂexible schemes adapting activation functions training data techniques range simple parametrizations known functions sophisticated non-parametric families including maxout network adaptive piecewise linear unit recently proposed kernel activation function contribution paper paper conjecture choosing properly activation function improve performance gcns possibly signiﬁcant margin. enhance basic model extending kafs activation functions ﬁlters. models one-dimensional activation function terms simple kernel expansion properly choosing elements expansion beforehand represent function small mixing coefﬁcients adapted together weights convolutional layers using standard back-propagation. apart ﬂexibility abstract—graph neural networks class neural networks allow efﬁciently perform inference data associated graph structure e.g. citation networks knowledge graphs. several variants gnns proposed consider simple nonlinear activation functions layers rectiﬁers squashing functions. paper investigate graph convolutional networks combined complex activation functions able adapt training data. speciﬁcally extend recently proposed kernel activation function non-parametric model implemented easily regularized standard p-norms techniques smooth entire domain. experimental evaluation shows proposed architecture signiﬁcantly improve baseline similar improvements cannot obtained simply increasing depth size original gcn. efﬁcient processing graph-structured data range different applications going bioinformatics text analysis sensor networks among others. particular importance design learning methods able take account numerical characteristics node graph inter-connections graph machine learning techniques long history recently witness renewed interest models deﬁned operate directly graph domain instead including graph information posteriori optimization process pre-processing phase graph embeddings examples native graph models include graph linear ﬁlters kernel counterparts graph neural networks gnns particularly interesting promise bring performance deep learning models graphbased domains. particular convolutional neural networks nowadays de-facto standard processing image data. cnns exploit image structure performing spatial convolutions image thus increasing parameter sharing lowering complexity. number authors recently explored possibility extending cnns graph domain several generalizations convolution operator trend generically called ‘geometric deep learning’ ﬁrst proposals graph fourier transform used every layer network perform ﬁltering operations resulting scheme number advantages including smoothness domain possibility implementing algorithm using highly vectorized operations. compare benchmarks semi-supervised learning showing proposed kafs signiﬁcantly outperform competing baselines marginal increase computational complexity structure paper section introduces problem inference graphs generic model. proposed extension kafs described section iii. evaluate compare model section concluding section consider generic undirected graph vertices edges connecting vertices. graph equivalently described adjacency matrix rn×n generic element nodes connected. graph signal function associating vertex -dimensional vector subset vertices also available nodespeciﬁc label either real number discrete quantity task correct labels nodes contained note graph unknown and/or must inferred data setup equivalent standard semi-supervised learning ignoring moment graph information could solve problem training standard predict label input composed stacking layers operation layer described rnl− input layer rnl−×nl trainable weights element-wise nonlinear function known activation function takes input providing ﬁnal prediction. number techniques used include unlabeled information training process including ladder networks pseudo-labels manifold regularization neural graph machines stated introduction however gnns include proximity information contained directly inside processing layers order improve performance models. shown experimental section working directly graph domain obtain vastly superior performances compared standard semi-supervised techniques. standard convolutional operations common cnns order extend idea general unweighted graphs need additional tools theory graph signal processing. order deﬁne convolutional operation graph normalized laplacian matrix diagonal matrix att. denote eigendecomposition uλut matrix collecting column-wise eigenvectors diagonal matrix associated eigenvalues. denote rn×f matrix collecting input signal across nodes column. graph fourier transform deﬁned eigenvectors form orthonormal basis also deﬁne inverse fourier transform ˆxi. exploit graph fourier transform deﬁne convolutional layer operating graphs ﬁltering operation acting eigenvalues adaptable parameters previous operation iterated successive representations ﬁnal node-speciﬁc predictions. choice determines complexity training. particular proposed make problem tractable working polynomial ﬁlters eigenvalues deﬁned advantageous ﬁlter parameterized values additionally easy show ﬁlter localized graph sense output given node depends nodes maximum hops thanks choice polynomial ﬁlter also avoid expensive multiplications rewriting ﬁltering operation directly original graph domain order avoid need recurrence relation proposed simplify expression setting channels assuming λmax backsubstituting original expression obtain known problem kernel methods elements computing kernel values extremely hard select. insight exploit fact working one-dimensional functions ﬁxing dictionary beforehand sampling uniformly elements around zero adapting linear coefﬁcients kernel expansion parameter controls ﬂexibility approach increasing increase ﬂexibility ﬁlter cost larger number parameters per-ﬁlter. elements dictionary selected according before mixing coefﬁcients {αi}d initialized every ﬁlter adapted independently together generic kernel function {θl}l gaussian kernel experiments order avoid numerical problems initial phase learning initialize mixing coefﬁcients mimic close possible exponential linear unit following strategy denoting sampling positions dictionary initialize coefﬁcients details refer original publication here brieﬂy comment advantages using non-parametric formulation. first implemented easily deep learning libraries using vectorized operations second functions deﬁned smooth entire domain. finally mixing parameters {αi}d handled like parameters particularly applying standard regularization techniques regularization). initialized randomly trained minimizing suitable loss labeled examples crossentropy classiﬁcation problems single layer combine information coming immediate neighbors node using multiple layers allows information several hops. practice layers found sufﬁcient many benchmark problems depth provide beneﬁt note however previous expression requires choose proper activation function. subject next section. point assumed activation functions given. note functions different layers need same. particularly outer function generally chosen task-dependent fashion however choosing different activation functions hidden layers vastly change performance resulting models ﬂexibility. generic element function applied. easy small amount ﬂexibility introducing simple parametrization function. example parametric relu adds adaptable slope negative part function stated introduction parametric functions might provide signiﬁcant increase performance general literature devoted designing non-parametric activation functions adapt large family shapes. simple technique would project activation high-dimensional space adapt different linear function feature space every ﬁlter. however method becomes easily unfeasible large dimensionality results proposed kafs given table also report performance additional baseline semi-supervised algorithms taken including manifold regularization semisupervised embedding label propagation skip-gram based graph embeddings iterative classiﬁcation planetoid references full description baselines found original papers. note cases proposed kaf-gcn able outperform baselines stable fashion particular results cora dataset represent stateof-the-art results using small labeled dataset. important underline performance kaf-gcn cannot reduced merely increasing depth former since architecture already ﬁne-tuned speciﬁc datasets. particular reports results showing neither variant residual connections obtain higher accuracy adding layers. opinion points importance adaptable activation functions able efﬁciently process information coming different ﬁlters. lack space able show shapes functions resulting optimization process although found similar obtained interested reader referred there. terms training time implementation kaf-gcn requires roughly computation epoch standard cora dataset citeseer one. however show fig. loss evolution cora dataset models. kaf-gcn generally converges faster possibly higher ﬂexibility allowed network. this kaf-gcn requires lower number epochs achieve convergence requiring average epochs less converge compensate increased computational time per-epoch. evaluate proposed model semi-supervised learning benchmarks taken whose characteristics reported table represents citation networks vertices documents edges links document belong given class. last column table percentage labeled nodes datasets. baseline network two-layer proposed method simply replace inner activation functions kafs dictionary elements sampled uniformly optimize cross-entropy loss given loss optimized using adam algorithm batch fashion. labeled dataset split following procedure nodes class training elements validation remaining elements testing accuracy. validation used evaluating loss epoch stop soon loss decreasing respect last epochs. hyper-parameters selected accordance already optimized cora dataset ﬁlters hidden layer initial learning rate additional regularization weights weighting factor addition dropout probability splits datasets taken average different initializations networks. implementation extend original code gcn. speciﬁcally multiplication adjacency matrix done using efﬁcient sparse data structures allowing perform linear time respect number defferrard bresson vandergheynst convolutional neural networks graphs fast localized spectral ﬁltering advances neural information processing systems zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proc. ieee int. conf. comput. vision goodfellow warde-farley mirza courville bengio maxout networks proc. international conference machine learning d.-h. pseudo-label simple efﬁcient semi-supervised learning method deep neural networks workshop challenges representation learning icml vol. paper shown performance graph convolutional networks signiﬁcantly improved ﬂexible activation functions hidden layers. speciﬁcally evaluated kernel activation functions semi-supervised benchmark tasks resulting faster convergence higher classiﬁcation accuracy. limitations current work undirected graph topologies training batch regime. handles former case rewriting directed graph equivalent bipartite graph operation computationally expensive. plan investigating recent developments graph fourier transforms directed graphs deﬁne simpler formulation. general test non-parametric activation functions classes graph neural networks would allow process sequences graphs multiple graphs time. sandryhaila moura data analysis signal processing graphs representation processing massive data sets irregular structure ieee signal process. mag. vol.", "year": 2018}