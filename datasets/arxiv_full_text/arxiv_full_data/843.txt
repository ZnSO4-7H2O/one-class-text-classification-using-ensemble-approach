{"title": "Locally Imposing Function for Generalized Constraint Neural Networks - A  Study on Equality Constraints", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "This work is a further study on the Generalized Constraint Neural Network (GCNN) model [1], [2]. Two challenges are encountered in the study, that is, to embed any type of prior information and to select its imposing schemes. The work focuses on the second challenge and studies a new constraint imposing scheme for equality constraints. A new method called locally imposing function (LIF) is proposed to provide a local correction to the GCNN prediction function, which therefore falls within Locally Imposing Scheme (LIS). In comparison, the conventional Lagrange multiplier method is considered as Globally Imposing Scheme (GIS) because its added constraint term exhibits a global impact to its objective function. Two advantages are gained from LIS over GIS. First, LIS enables constraints to fire locally and explicitly in the domain only where they need on the prediction function. Second, constraints can be implemented within a network setting directly. We attempt to interpret several constraint methods graphically from a viewpoint of the locality principle. Numerical examples confirm the advantages of the proposed method. In solving boundary value problems with Dirichlet and Neumann constraints, the GCNN model with LIF is possible to achieve an exact satisfaction of the constraints.", "text": "abstract—this work study generalized constraint neural network model challenges encountered study embed type prior information select imposing schemes. work focuses second challenge studies constraint imposing scheme equality constraints. method called locally imposing function proposed provide local correction gcnn prediction function therefore falls within locally imposing scheme comparison conventional lagrange multiplier method considered globally imposing scheme added constraint term exhibits global impact objective function. advantages gained gis. first enables constraints locally explicitly domain need prediction function. second constraints implemented within network setting directly. attempt interpret several constraint methods graphically viewpoint locality principle. numerical examples conﬁrm advantages proposed method. solving boundary value problems dirichlet neumann constraints gcnn model possible achieve exact satisfaction constraints. artiﬁcial neural networks received signiﬁcant progresses proposal deep learning models anns formed mainly based learning data. hence considered data-driven approach blackbox limitation feature provides ﬂexiblility power anns modeling miss functioning part top-down mechanisms seems necessary realizing human-like machines. furthermore ultimate goal machine learning study insight machine itself. current anns including deep learning models fail present interpretations learning processes well associated physical targets human brains. adding transparency anns proposed generalized constraint neural network approach also viewed knowledge-and-data-driven modeling approach submodels formed coupled shown fig. simplify discussions later refer gcnn kddm approaches gcnn models developed based previously existing modeling approaches hybrid neural network model chose generalized constraint descriptive terms mathematical meaning stressed terms generalized constraint ﬁrstly given zadeh describing wide variety constraints probabilistic fuzzy rough forms. consider concepts generalized constraint provide critical step construct human-like machines. implications behind concepts least challenges follows. ﬁrst challenge aims mimic behavior human beings decision making. deduction induction inferences employed daily life. second challenge attempts emulate synaptic plasticity function human brain. still away understanding mathematically human brain select change couplings. challenges lead difﬁculty stated confronting large diversity unstructured representations prior knowledge would rather difﬁcult build rigorous theoretical framework already done elegant treatments bayesian neuro-fuzzy ones. difﬁculty implies need study gcnn approaches class-by-class basis. work extends previous study gcnn models class equality constraints focuses locality principle second challenge. main progress work twofold below. novel proposal locally imposing scheme presented resulting alternative solution different globally imposing scheme lagrange multiplier method. numerical examples shown class equality constraints including derivative form conﬁrm speciﬁc advantages given examples. study regression problems equality constraints. remaining paper organized follows. section discusses differences machine learning problems optimization problems. based discussions main idea behind presented. conventional rbfnn model learning brieﬂy introduced section iii. section demonstrates proposed model learning process. numerical experiments synthetic data sets presented section discussions locality principle coupling forms given section section presents ﬁnal remarks work. mathematically machine learning problems equivalent optimization problems. compare reﬂecting differences. optimization problem equality constraints expressed following form expectation prediction function formed composition radical basis functions equality constraint. presents several differences comparing better understanding explain imaging mountain first conventional optimization problem search optimization point well-deﬁned mountain machine learning problem tries form unknown mountain minimum error observation data. second equality constraints optimizations imply solution located constraints. otherwise exist feasible solutions. machine learning problem equality constraints suggest unknown mountain surface given form described function and/or value. approximation made minimum error sense. third machine learning produces larger variety constraint types encountered conventional optimization problems. main reason comes prior describe unknown real-system function. sometimes well deﬁned shows partially known relationship terms generalized constraints used machine learning problems. reason rewrite form highlight meaning machine learning problems based discussions above present proposal namely locally imposing scheme dealing equality constraints machine learning problems. main idea behind realized following steps. step removing jump switching step locally imposing function weight constraint term complementary weight ﬁrst term continuity property held modiﬁed prediction function idea ﬁrst steps reported previous studies particularly boundary value problems used different methods realize step polynomial methods methods length methods equality constraints given interpolation points methods shown suggested neural-network-based models enhanced integrating conventional approximation tools. showed example realize step apply lagrange interpolation method. followingstudy elimination method used methods fact fall category. applied method realize step demonstrated equality function constraints satisﬁed completely exactly given dirichlet boundary smooth work. location parameter deﬁnes peak distribution scale parameter describes width half maximum. cauchy distribution smooth inﬁnitely differentiable property. smooth function also used lif. denotes distance variable constraint location. ψnorm normalized parameter ensures normalization monotonically decreasing function respect distance call locality parameter controls locality property lif. decreases becomes sharper function shape. generally preset parameter constant trial error way. hence drop describe denotes constraint numerical value function. note bvps dirichlet form special case given regions without limitation boundary. facing following constrained minimization problem conventional lagrange multiplier method belongs globally imposing scheme lagrange multiplier term exhibits global impact objective function. heuristic justiﬁcation analogy locality principle brain functioning memory constraints viewed memory. principle provides time efﬁciency energy efﬁciency implies constraints better imposed local means. together open direction study coupling forms towards braininspired machines. given training data desired outputs input vector denotes vector desired network output input number training data. output rbfnn calculated according represents model parameter number neurons hidden layer. terms feature mapping function centers rm×d widths easily determined using method proposed section focus gcnn equality constraints using lif. note special method within category include several methods. ﬁrst describe locally imposing function used gcnn models. gcnn designs direct derivative constraints discussed respectively. simplifying presentations consider single constraint design process steps clear individual constraint. multiple sets combinations direct derivative constraints extended directly. variable determined solution. different lagrange multiplier method imposes constraint global manner objective function solve constrained optimization problem. modiﬁed prediction function deﬁned gcnn general case non-integrable derivative constraints general case explicit form cannot derived given neumann constraint. modiﬁed loss function including terms given following form constraint approximately satisﬁed much possible exactly known integration term constant neglected gcnn includes term already. hence substituting solve neumann constraint like dirichlet constraint. however distinguishing gcnn model general case denote gcnn model special case neumann constraint. numerical examples shown section comparisons gis. gcnn model within models gcnn lagrange bvc-rbf rbfnn lagrange interpolation gcnn-lp considered gis. ﬁrst example interpolation point constraints. consider problem approximating sinc sin/x function based equality constraints function corrupted additive gaussian noise optimization problem represented training data instances generated uniformly along variable intervals testing data randomly generated intervals. table shows performances methods rbfnn belong constraint method. examine performances constraints testing data. observe among constraint methods rbfnn lagrange multiplier presents excellent approximation constraints others produce exact satisfaction constraints. gaussian noise added onto original function training data instances selected evenly within testing data instances instances randomly sampled within instances selected evenly boundary rbfnn+lagrange multiplier bvcrbf gcnn+lagrange interpolation applicable solving problem transferring continuous constrain point-wise constraints. reason select points evenly according along boundary them. table lists ﬁtting performances boundary testing data. gcnn satisfy dirichlet boundary condition exactly continuous function constrain. constraint methods reach satisfaction point-wise constraint location moreover gcnn performs much better methods testing data. case gcnn+lagrange interpolation methods fail without transferring point-wise constraints. gcnn gcnn solve constraint problem compare performances. rbfnn also given without using constraint. training data instances selected evenly within testing data instances instances randomly sampled within instances selected evenly boundary table shows performance boundary testing data neumann boundary condition. speciﬁc examination made constraint boundary. fig. depicts plots three methods neumann boundary condition. obviously gcnn satisfy constraint exactly boundary neumann constraint integrable achieving explicit expression. gcnn best solving problem however sometimes explicit expression unavailable impossible gcnn also good choice. note neumann constraint difﬁcult satisﬁed dirichlet one. gcnn presents reasonable approximation except ending ranges boundary. section attempt discuss locality principle viewpoint constraint imposing anns provide graphical interpretations differences lis. typical question likes discover lagrange multiplier method lis?. answer question however interpretations coupling-form dependent. show original coupling form three methods table lagrange multiplier method gcnn-lp. ﬁnal prediction output contains terms output superposition constraint. methods alternative coupling form shown table alternative coupling term different expressions. speciﬁc forms bvc-rbf gcnn lagrange interpolation discussed respectively. form gcnn equal better understanding differences among given three methods sinc function example interpolation point constraints enforced without additive noise. fig. shows original coupling function fig. shows output alternative coupling function together. keep parameters bvc-rbf reason good performance data. performance becomes poor. within either coupling forms gcnn presents best terms locality plots conﬁrm locality interpretations coupling-form dependent. isons made rbfnn lagrange multiplier method gcnn fig. shows plots normalized weight changes rbfnn lagrange multiplier gcnn numerical tests indicate behavior locality property plots dependent parameters networks. reaching meaningful plots nrbf ntrain center parameters generated uniformly along variable intervals center interval constant given values respectively. decreased performance becomes poor rbfnn lagrange multiplier gcnn fig. observe that rbfnn lagrange multiplier gcnn show locality property constraint locations. rbfnn lagrange multiplier loses locality property gcnn less degree. numerical tests imply gcnn holds locality property better rbfnn lagrange multiplier. work study constraint imposing scheme gcnn models. ﬁrst discuss geometric differences conventional optimization problems machine learning problems. based discussions method within proposed gcnn models. gcnn transfers equality constraint problems unconstrained ones solves linear approach convexity constraints issue. present method able process interpolation function constraints cover constraint types bvps. numerical study made including constraints forms dirichlet neumann bvps. gcnn achieves exact satisfaction equality constraints either dirichlet neumann types expressed explicit form approximations obtained neumann constraint integrable explicit form additively original output form ﬁnal prediction output constraint methods examined however examination basically numerical requires extra calculation fwc. fig. shows plots rbfnn+lagrange multiplier gcnn models. observe signiﬁcant differences locality behaviors. work represent neural networks without constraints respectively. brain memory attributed changes synaptic strength connectivity propose following steps designs networks. first number neurons applied share connectivity terms neurons second preset values parameters given respectively networks. step weight parameters gcnn gained solving linear problem guarantees unique solution. lagrange multiplier method take weights obtained initial condition updating updating operation emulate brain memory change. steps enable examine changes synaptic strengths networks. figs. provide locality interpretation signal function sense another interpretation explored plots weight changes networks number neurons weight parameters denote weight changes. normalized weight changes achieved ∆w/|∆w |max |max normalization scalar. still take sinc function example. compar b.-g. generalized constraint neural network regression model subject equality function constraints proc. international joint conference neural networks olden jackson illuminating black randomization approach understanding variable contributions artiﬁcial neural networks ecological modelling vol. x.-r. m.-z. kang heuvelink reffye b.-g. knowledge-and-data-driven modeling approach simulating plant growth case study tomato growth ecological modelling vol. zadeh outline computational approach meaning knowledge representation based concept generalized assignment statement proc. international seminar artiﬁcial intelligence man-machine systems. springer fuzzy logic computing words ieee transactions mcfall mahan artiﬁcial neural network method solution boundary value problems exact satisfaction arbitrary boundary conditions ieee transactions neural networks vol. lecun boser denker henderson howard hubbard jackel handwritten digit recognition backpropagation network advances neural information processing systems locality principle brain study wider meaning anns. apart local properties coupling forms knowledge data another locality source studies. believe locality principle steps anns realize braininspired machine. present work indicates direction advancing technique. lagrange multiplier standard method machine learning show alternative solution performance better given problems. need explore together understand conditions selected.", "year": 2016}