{"title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box  Machine Learning Models", "tag": ["stat.ML", "cs.CR", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .", "text": "wieland brendel∗ jonas rauber∗ matthias bethge werner reichardt centre integrative neuroscience eberhard karls university t¨ubingen germany {wielandjonasmatthias}bethgelab.org many machine learning algorithms vulnerable almost imperceptible perturbations inputs. unclear much risk adversarial perturbations carry safety real-world machine learning applications methods used generate perturbations rely either detailed model information conﬁdence scores class probabilities neither available real-world many cases currently needs retreat transfer-based scenarios. attacks rely cumbersome substitute models need access training data defended against. emphasise importance attacks solely rely ﬁnal model decision. decision-based attacks applicable real-world black-box models autonomous cars need less knowledge easier apply transfer-based attacks robust simple defences gradientscore-based attacks. previous attacks category limited simple models simple datasets. introduce boundary attack decision-based attack starts large adversarial perturbation seeks reduce perturbation staying adversarial. attack conceptually simple requires close hyperparameter tuning rely substitute models competitive best gradient-based attacks standard computer vision tasks like imagenet. apply attack black-box algorithms clarifai.com. boundary attack particular class decision-based attacks general open avenues study robustness machine learning models raise questions regarding safety deployed machine learning systems. implementation attack available part foolbox figure taxonomy adversarial attack methods. boundary attack applicable realworld algorithms needs access ﬁnal decision model rely model information like gradient conﬁdence scores. application clarifai brand recognition model. many high-performance machine learning algorithms used computer vision speech recognition areas susceptible minimal changes inputs concrete example modern deep neural network like vgg- trained object recognition might perfectly recognize main object image tiger pixel values slightly perturbed speciﬁc prediction network drastically altered so-called adversarial perturbations ubiquitous many machine learning models often imperceptible humans. algorithms seek adversarial perturbations generally denoted adversarial attacks. adversarial perturbations drawn interest different sides. side worrisome integrity security deployed machine learning algorithms autonomous cars face recognition systems. minimal perturbations street signs street lights severe consequences. hand adversarial perturbations provide exciting spotlight sensory information processing humans machines thus provide guidance towards robust human-like architectures. adversarial attacks roughly divided three categories gradient-based score-based transfer-based attacks gradient-based score-based attacks often denoted white-box oracle attacks respectively explicit possible information used category. severe problem affecting attacks categories surprisingly straight-forward defend against gradient-based attacks. existing attacks rely detailed model information including gradient loss w.r.t. input. examples fast-gradient sign method basic iterative method deepfool jacobian-based saliency attack houdini carlini wagner attack defence simple defend gradient-based attacks mask gradients example adding non-differentiable elements either implicitly means like defensive distillation saturated non-linearities explicitly means like non-differentiable classiﬁers score-based attacks. attacks agnostic rely predicted scores model. conceptual level attacks predictions numerically estimate gradient. includes black-box variants jsma carlini wagner attack well generator networks predict adversarials defence straight-forward severely impede numerical gradient estimate adding stochastic elements like dropout model. also many robust training methods introduce sharp-edged plateau around samples masks gradients also numerical estimate. transfer-based attacks. transfer-based attacks rely model information need information training data. data used train fully observable substitute model adversarial perturbations synthesized rely empirical observation adversarial examples often transfer models. adversarial examples created ensemble substitute models success rate attacked model reach certain scenarios defence recent defence method transfer attacks based robust training dataset augmented adversarial examples ensemble substitute models proven highly successful basically attacks kaggle competition adversarial attacks. fact many attacks easily averted makes often extremely difﬁcult assess whether model truly robust whether attacks weak lead premature claims robustness dnns delineation category justiﬁed following reasons first compared score-based attacks decision-based attacks much relevant real-world machine learning applications conﬁdence scores logits rarely accessible. time decision-based attacks potential much robust standard defences like gradient masking intrinsic stochasticity robust training attacks categories. finally compared transferbased attacks need much less information model much simpler apply. currently exists effective decision-based attack scales natural datasets imagenet applicable deep neural networks relevant prior work variant transfer attacks training needed learn substitute model replaced synthetic dataset synthetic dataset generated adversary alongside training substitute; labels synthetic sample drawn black-box model. approach works well datasets intra-class variability shown scales complex natural datasets cifar imagenet. decision-based attacks speciﬁc linear convex-inducing classiﬁers applicable machine learning models. work basically stands transfer attacks decision-based attacks substitute model trained dataset labels observed black-box model. attack still requires knowledge data distribution black-box models trained don’t consider pure decision-based attack. finally naive attacks line-search along random direction away original sample qualify decision-based attacks induce large visible perturbations orders magnitude larger typical gradient-based score-based transfer-based attacks. throughout paper focus threat scenario adversary aims change decision model particular input sample inducing minimal perturbation sample. adversary observe ﬁnal decision model arbitrary inputs knows least perturbation however large perturbed sample adversarial. introduce ﬁrst effective decision-based attack scales complex machine learning models natural datasets. boundary attack conceptually surprisingly simple extremely ﬂexible requires little hyperparameter tuning competitive best gradient-based attacks targeted untargeted computer vision scenarios. basic intuition behind boundary attack algorithm depicted figure algorithm initialized point already adversarial performs random walk along boundary adversarial non-adversarial region stays adversarial region distance towards target image reduced. words perform rejection sampling suitable proposal distribution progressively smaller adversarial perturbations according given adversarial criterion basic logic algorithm described algorithm individual building block detailed next subsections. data original image adversarial criterion decision model result adversarial example distance initialization s.t. adversarial; maximum number steps boundary attack needs initialized sample already adversarial. untargeted scenario simply sample maximum entropy distribution given valid domain input. computer vision applications below input constrained range pixel sample pixel initial image uniform distribution reject samples adversarial. targeted scenario start sample classiﬁed model target class. proposal distribution efﬁciency algorithm crucially depends proposal distribution i.e. random directions explored step algorithm. optimal proposal distribution generally depend domain model attacked vision-related problems tested simple proposal distribution worked surprisingly well. basic idea behind proposal distribution follows k-th step want draw perturbations maximum entropy distribution subject following constraints figure essence boundary attack performs rejection sampling along boundary adversarial non-adversarial images. step draw random direction drawing gaussian projecting sphere making small move towards target image. step-sizes dynamically adjusted according local geometry boundary. practice difﬁcult sample distribution resort simpler heuristic ﬁrst rescale clip sample sample gaussian distribution hold. second step project onto sphere around original image hold. denote orthogonal perturbation later hyperparameter tuning. last step make small movement towards original image hold. high-dimensional inputs small constraint also hold approximately. typical criterion input classiﬁed adversarial misclassiﬁcation i.e. whether model assigns perturbed input class different class label original input. another common choice targeted misclassiﬁcation perturbed input classiﬁed given target class. choices include top-k misclassiﬁcation thresholds certain conﬁdence scores. outside computer vision many choices exist criteria worderror rates. comparison attacks boundary attack extremely ﬂexible regards adversarial criterion. basically allows criterion long criterion initial adversarial found boundary attack relevant parameters length total perturbation length step towards original input adjust parameters dynamically according local geometry boundary. adjustment inspired trust region methods. essence ﬁrst test whether orthogonal perturbation still adversarial. true make small movement towards target test again. orthogonal step tests whether step-size small enough treat decision boundary adversarial non-adversarial region approximately linear. case expect around orthogonal perturbations still adversarial. ratio much lower reduce step-size close higher increase orthogonal perturbation still adversarial small step towards original input. maximum size step depends angle decision boundary local neighbourhood success rate small decrease large increase typically closer original image ﬂatter decision boundary becomes smaller still make progress. attack converged whenever converges zero. quantify performance boundary attack three different standard datasets mnist cifar- imagenet- make comparison previous results easy transparent possible mnist cifar networks carlini wagner nutshell mnist cifar model feature nine layers four convolutional layers max-pooling layers fully-connected layers. details including training parameters refer reader imagenet pretrained networks vgg- resnet- inception-v provided keras. evaluate boundary attack settings untargeted setting adversarial perturbation ﬂips label original sample label targeted setting adversarial ﬂips label speciﬁc target class. untargeted setting compare boundary attack three gradient-based attack algorithms fast-gradient sign method fgsm among simplest widely used untargeted adversarial attack methods. nutshell fgsm computes gradient maximizes loss true class-label seeks smallest o+\u0001·g still adversarial. implementation foolbox deepfool. deepfool simple effective attack. iteration computes class minimum distance takes reach class boundary approximating model classiﬁer linear classiﬁer. makes corresponding step direction class smallest distance. implementation foolbox carlini wagner. attack carlini wagner essentially reﬁned iterative gradient attack uses adam optimizer multiple starting points tanh-nonlinearity respect box-constraints max-based adversarial constraint function. original implementation provided authors hyperparameters left default values. evaluate success attack following metric adversarial perturbation attack ﬁnds model i-th sample total score median squared l-distance across samples untargeted setting adversarial image predicted label different label original image. show adversarial samples synthesized boundary attack dataset figure score attack dataset follows figure adversarial examples generated boundary attack mnist cifar imagenet network. mnist difference shows positive negative changes. cifar imagenet take norm across color channels. differences scaled improved visibility. figure example untargeted attack. goal synthesize image close possible original image misclassiﬁed image report total number model calls point mean squared error adversarial original despite simplicity boundary attack competitive gradient-based attacks terms minimal adversarial perturbations stable choice initial point ﬁnding quite remarkable given gradient-based attacks fully observe model whereas boundary attack severely restricted ﬁnal class prediction. compensate lack information boundary attack needs many iterations converge. rough measure run-time attack independent quality implementation tracked number forward passes backward passes network requested attacks adversarial resnet- averaged samples conditions before deepfool needs forward backward passes carlini wagner attack requires forward number backward passes boundary attack uses forward passes zero backward passes. makes boundary attack expensive important note boundary attacks needs much fewer iterations interested imperceptible perturbations ﬁgures also apply boundary attack targeted setting. case initialize attack sample target class correctly identiﬁed model. sample trajectory starting point original sample shown figure around calls model figure example targeted attack. goal synthesize image close possible given image tiger classiﬁed dalmatian dog. image report total number model calls point. order compare boundary attack carlini wagner deﬁne target target label sample following mnist cifar sample label gets target label modulo imagenet draw target label randomly consistent across attacks. results follows discussed introduction many attack methods straight-forward defend against. common nuisance gradient masking model implicitely explicitely modiﬁed yield masked gradients. interesting example saturated sigmoid network additional regularization term leads sigmoid activations saturate turn leads vanishing gradients failing gradient-based attacks initial results promising success rate gradient-based attacks dropped close later became clear distilled networks appeared robust masked gradients cross-entropy loss temperature softmax decreased test time input softmax increases factor probabilities saturate leads vanishing gradients cross-entropy loss w.r.t. input gradient-based attacks rely. attacks instead applied logits success rate recovers almost decision-based attacks immune defences. demonstrate apply boundary attack distilled networks trained mnist cifar. architecture section implementation training protocol available https//github.com/carlini/nn_robust_attacks. importantly operate logits provide class label maximum probability boundary attack. results follows size adversarial perturbations boundary attack ﬁnds fairly similar distilled undistilled network. demonstrates defensive distillation signiﬁcantly increase robustness network models boundary attack able break defences based gradient masking. many real-world machine learning applications attacker access architecture training data observe ﬁnal decision. true security systems autonomous cars speech recognition systems like alexa cortana. section apply boundary attack models cloud-based computer vision clarifai. ﬁrst model identiﬁes brand names natural images recognizes brands. second model identiﬁes celebrities recognize individuals. multiple identiﬁcations image possible consider highest conﬁdence score. important note clarifai provide conﬁdence scores identiﬁed class however experiments provide conﬁdence score boundary attack. instead attack receives name identiﬁed object selected several samples natural images clearly visible brand names portraits celebrities. make square crop resize image pixels. sample make sure brand celebrity clearly visible corresponding clarifai model correctly identiﬁes content. adversarial criterion misclassiﬁcation i.e. clarifai report different brand celebrity none adversarially perturbed sample. show samples model alongside adversarial image generated boundary attack figure generally observed clarifai models difﬁcult attack imagenet models like vgg- samples succeed adversarial perturbations order section adversarial perturbations order resulting slightly noticeable noise adversarial examples. nonetheless samples original adversarial image close perceptually indistinguishable. paper emphasised importance mostly neglected category adversarial attacks— decision-based attacks—that adversarial examples models ﬁnal decision observed. argue category important three reasons ﬁrst attacks class highly relevant many real-world deployed machine learning systems like autonomous cars internal decision making process unobservable. second attacks class rely substitute models trained similar data model attacked thus making real-world applications much straight-forward. third attacks class potential much robust common deceptions like gradient masking intrinsic stochasticity robust training. also introduced ﬁrst effective attack category applicable general machine learning algorithms complex natural datasets boundary attack. core boundary attack follows decision boundary adversarial non-adversarial samples using simple rejection sampling algorithm conjunction simple proposal distribution dynamic step-size adjustment inspired trust region methods. basic operating principle— starting large perturbation successively reducing it—inverts logic essentially previous adversarial attacks. besides surprisingly simple boundary attack also extremely ﬂexible terms possible adversarial criteria performs gradient-based attacks standard computer vision tasks terms size minimal perturbations. mere fact simple constrained gaussian distribution serve effective proposal perturbation step boundary attack surprising sheds light brittle information processing current computer vision architectures. nonetheless many ways boundary attack made even effective particular learning suitable proposal distribution given model conditioning proposal distribution recent history successful unsuccessful proposals. decision-based attacks highly relevant assess robustness machine learning models highlight security risks closed-source machine learning systems like autonomous cars. hope boundary attack inspire future work area. work supported carl zeiss foundation bosch forschungsstiftung international planck research school intelligent systems german research foundation intelligence advanced research projects activity department interior/interior business center contract number dpc. u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. disclaimer views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied iarpa doi/ibc u.s. government. references battista biggio igino corona davide maiorca blaine nelson nedim ˇsrndi´c pavel laskov giorjoint giacinto fabio roli. evasion attacks machine learning test time. european conference machine learning knowledge discovery databases springer wieland brendel matthias bethge. comment biologically inspired protection deep networks adversarial attacks. corr abs/. http//arxiv.org/ abs/.. pin-yu chen huan zhang yash sharma jinfeng cho-jui hsieh. zeroth order optimization based black-box attacks deep neural networks without training substitute models. corr abs/. http//arxiv.org/abs/.. nilesh dalvi pedro domingos mausam sumit sanghai deepak verma. adversarial classiﬁcation. proceedings tenth sigkdd international conference knowledge discovery data mining york acm. isbn ---. ./.. http//doi.acm.org/./ deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee daniel lowd christopher meek. adversarial learning. proceedings eleventh sigkdd international conference knowledge discovery data mining york acm. isbn ---x. ./.. http//doi.acm.org/./.. seyed-mohsen moosavi-dezfooli alhussein fawzi pascal frossard. deepfool simple accurate method fool deep neural networks. corr abs/. http //arxiv.org/abs/.. blaine nelson benjamin rubinstein ling huang anthony joseph steven satish tygar. query strategies evading convex-inducing classiﬁers. mach. learn. res. issn http//dl.acm.org/citation. cfm?id=.. nicolas papernot patrick mcdaniel somesh matt fredrikson berkay celik ananthram swami. limitations deep learning adversarial settings. corr abs/. http//arxiv.org/abs/.. nicolas papernot patrick mcdaniel somesh ananthram swami. distillation defense adversarial perturbations deep neural networks. security privacy ieee symposium ieee nicolas papernot patrick mcdaniel goodfellow somesh berkay celik ananthram swami. practical black-box attacks machine learning. proceedings asia conference computer communications security nicolas papernot patrick mcdaniel goodfellow somesh berkay celik ananthram swami. practical black-box attacks machine learning. proceedings asia conference computer communications security asia york acm. isbn ----. ./.. http//doi.acm.org/./.. jonas rauber wieland brendel matthias bethge. foolbox python toolbox benchmark robustness machine learning models. corr abs/. http//arxiv.org/abs/.. christian szegedy wojciech zaremba ilya sutskever joan bruna dumitru erhan goodfellow fergus. intriguing properties neural networks. corr abs/. http//arxiv.org/abs/.. christian szegedy vincent vanhoucke sergey ioffe jonathon shlens zbigniew wojna. rethinking inception architecture computer vision. corr abs/. http//arxiv.org/abs/.. florian tramer alexey kurakin nicolas papernot boneh patrick mcdaniel. ensemble adversarial training attacks defenses. corr abs/. http //arxiv.org/abs/..", "year": 2017}