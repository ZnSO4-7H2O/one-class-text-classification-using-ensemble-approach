{"title": "Identity Matters in Deep Learning", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "An emerging design principle in deep learning is that each layer of a deep artificial neural network should be able to easily express the identity transformation. This idea not only motivated various normalization techniques, such as \\emph{batch normalization}, but was also key to the immense success of \\emph{residual networks}.  In this work, we put the principle of \\emph{identity parameterization} on a more solid theoretical footing alongside further empirical progress. We first give a strikingly simple proof that arbitrarily deep linear residual networks have no spurious local optima. The same result for linear feed-forward networks in their standard parameterization is substantially more delicate. Second, we show that residual networks with ReLu activations have universal finite-sample expressivity in the sense that the network can represent any function of its sample provided that the model has more parameters than the sample size.  Directly inspired by our theory, we experiment with a radically simple residual architecture consisting of only residual convolutional layers and ReLu activations, but no batch normalization, dropout, or max pool. Our model improves significantly on previous all-convolutional networks on the CIFAR10, CIFAR100, and ImageNet classification benchmarks.", "text": "emerging design principle deep learning layer deep artiﬁcial neural network able easily express identity transformation. idea motivated various normalization techniques batch normalization also immense success residual networks. work principle identity parameterization solid theoretical footing alongside empirical progress. ﬁrst give strikingly simple proof arbitrarily deep linear residual networks spurious local optima. result linear feed-forward networks standard parameterization substantially delicate. second show residual networks relu activations universal ﬁnite-sample expressivity sense network represent function sample provided model parameters sample size. directly inspired theory experiment radically simple residual architecture consisting residual convolutional layers relu activations batch normalization dropout pool. model improves signiﬁcantly previous all-convolutional networks cifar cifar imagenet classiﬁcation benchmarks. traditional convolutional neural networks image classiﬁcation alexnet parameterized trainable weights convolutional layer represents -mapping. moreover weights initialized symmetrically around standard parameterization makes non-trivial convolutional layer trained stochastic gradient methods preserve features already good. differently convolutional layers cannot easily converge identity transformation training time. shortcoming observed partially addressed batch normalization i.e. layer-wise whitening input learned mean covariance. idea remained somewhat implicit residual networks explicitly introduced reparameterization convolutional layers trainable weights layer represents identity function. formally input residual layer form rather simple reparameterization allows much deeper architectures largely avoiding problem vanishing gradients. residual networks subsequent architectures parameterization since consistently achieved state-of-the-art results various computer vision benchmarks cifar imagenet. work consider identity parameterizations theoretical perspective translating theoretical insight back experiments. loosely speaking ﬁrst result underlines identity parameterizations make optimization easier second result shows true representation. linear residual networks. since general non-linear neural networks beyond reach current theoretical methods optimization consider case deep linear networks simpliﬁed model. linear network represents arbitrary linear sequence matrices objective function unknown linear transformation drawn distribution. linear networks studied actively recent years stepping stone toward general non-linear case even though linear optimization problem factored variables non-convex. give intuition depth large enough hope target function factored representation matrix small norm. symmetric positive semideﬁnite matrix example written product close identity large small spectral norm. ﬁrst prove analogous claim true linear transformations positive determinant. speciﬁcally prove every linear transformation exists global optimizer large enough depth here denotes spectral norm constant factor depends conditioning give formal statement theorem theorem interesting consequence depth increases smaller norm solutions exist hence regularization offset increase parameters. established existence small norm solutions main result linear residual networks shows objective function fact easy optimize matrices sufﬁciently small norm. formally letting denote objective function show gradients vanish provided maxi theorem result implies linear residual networks critical points global optimum. contrast standard linear neural networks know work networks don’t local optima except global optimum doesn’t rule critical points. fact setting always lead critical point standard parameterization. universal ﬁnite sample expressivity. going back non-linear residual networks relu activations expressive deep neural networks solely based residual layers relu activations? answer question give simple construction showing residual networks perfect ﬁnite sample expressivity. words residual network relu activations easily express functions sample size provided sufﬁciently parameters. note requirement easily practice. cifar example successful residual networks often parameters. formally data size classes construction requires parameters. theorem gives formal statement. residual layer construction form relu linear transformations. layers signiﬁcantly simpler standard residual layers typically relu activations well instances batch normalization. power all-convolutional residual networks. directly inspired simplicity expressivity result experiment similar architecture cifar cifar imagenet data sets. architecture merely chain convolutional residual layers single relu activation without batch normalization dropout pooling common standard architectures. last layer ﬁxed random projection trained. line theory convolutional weights initialized near using gaussian noise mainly symmetry breaker. regularizer standard weight decay -regularization) need dropout. despite simplicity architecture reaches top- classiﬁcation error cifar benchmark competitive best residual network reported achieved moreover improves upon performance previous best all-convolutional network achieved unlike ours previous all-convolutional architecture additionally required dropout non-standard preprocessing entire data set. architecture also improves signiﬁcantly upon cifar imagenet. since advent residual networks state-of-the-art networks image classiﬁcation adopted residual parameterization convolutional layers. impressive improvements reported variant residual networks called dense nets. rather adding original input output convolutional layer networks preserve original features directly concatenation. dense nets also able easily encode identity embedding higher-dimensional space. would interesting theoretical results also apply variant residual networks. recent progress understanding optimization landscape neural networks though comprehensive answer remains elusive. experiments suggest training objectives limited number local minima large function values. work draws analogy optimization landscape neural nets spin glass model physics showed -layer neural networks differentiable local minima didn’t prove good differentiable local minimum exist. show linear neural networks local minima. contrast show optimization landscape deep linear residual networks critical point stronger desirable property. proof also notably simpler illustrating power re-parametrization optimization. results also indicate deeper networks desirable optimization landscapes compared shallower ones. optimization landscape linear residual networks consider problem learning linear transformation noisy measurements d-dimensional spherical gaussian vector. denoting distribution input data ex∼d] covariance matrix. course many ways solve classical problem goal gain insights optimization landscape neural nets particular residual networks. therefore parameterize learned model sequence weight matrices rd×d easy model express linear transformation shorthand weight matrices d-dimensional tensor contains slices. objective function maximum likelihood estimator ﬁrst theorem section states objective function optimal solution small |||·|||-norm inversely proportional number layers thus architecture deep shoot fairly small norm solutions. deﬁne max{| σmax|| σmin|}. σmin σmax denote least largest singular values respectively. theorem suppose then exists global optimum solution population risk norm ﬁrst note condition without loss generality following sense. given linear transformation negative determinant effectively determinant augmenting data label additional dimension independent random variable then det) second note thought constant since large scale data properly σmin σmax. concretely σmax/σmin scaling outputs properly σmin remain small constant fairly large condition number also point made attempt optimize constant factors analysis. proof theorem rather involved deferred section landscape |||·|||-norm less r×d×d |||a||| using theorem radius thought order main theorem section claims critical point domain recall critical point vanishing gradient. theorem critical point objective function inside domain must also global minimum. equation says gradient fairly large norm compared error guarantees convergence gradient descent global minimum iterates stay inside domain guaranteed theorem itself. towards proving theorem start simple claim simpliﬁes population risk. also denote frobenius norm matrix. claim setting section have ready prove theorem observation matric small norm cannot cancel identity matrix. therefore gradients equation product non-zero matrices except error matrix therefore gradient vanishes possibility matrix vanishes turns implies optimal solution. section characterize ﬁnite-sample expressivity residual networks. consider residual layers single relu activation batch normalization. basic residual building block function tuvs parameterized weight matrices rk×k rk×k bias vector residual network composed sequence residual blocks. comparison full pre-activation architecture remove batch normalization layers relu layer building block. assume data labels encoded standard basis vectors denoted training examples denotes i-th data denotes i-th label. without loss generality assume data normalized also make mild assumption data points close other. theorem suppose training examples satisfy assumption then exists residual network parameters perfectly expresses training data i.e. network maps common practice example case imagenet construct following residual using building blocks form tuvs deﬁned equation network consists hidden layers output denoted ﬁrst layer weights matrices maps d-dimensional input k-dimensional hidden variable apply layers building block weight matrices rk×k. finally apply another layer hidden variable label mathematically note rk×r rr×r dimension compatible. assume number labels input dimension smaller safely true practical applications. hyperparameter chosen number layers chosen n/k. thus ﬁrst layer parameters middle building blocks contains parameters ﬁnal building block parameters. hence total number parameters towards constructing network form data ﬁrst take random matrix rk×d maps data points vectors denote j-th layer hidden variable i-th example. johnson-lindenstrauss theorem good probability resulting vectors remain satisfy assumption vectors correlated. every vectors clustered groups according labels though instead desired. concretely design cluster centers picking random unit vectors view surrogate label vectors dimension high dimensions random unit vectors pair-wise uncorrelated inner product less associate i-th example target surrogate label vector deﬁned follows putting together deﬁnition equation every label equation ta+b+b+ hence obtain part plan construction middle layers weight matrices encapsulate following informal lemma. formal statement full proof deferred section corresponds hidden variable block corresponds after. claim arbitrary sequence vectors exists tuvs operation transforms vectors arbitrary vectors freely choose maintain value rest vectors. concretely subset size desired vector exist claim formalized lemma repeatedly construct layers building blocks transforms subset vectors corresponding vectors maintains values others. recall layers therefore layers vectors transformed complete proof sketch. architectures cifar cifar identical except ﬁnal dimension corresponding number classes respectively. table outline architecture. residual block form x+c) convolutions speciﬁed dimension second convolution block always stride ﬁrst stride indicated. cases transformation dimensionality-preserving original input adjusted using averaging pooling padding standard residual layers. trained models tensorﬂow framework using momentum optimizer momentum batch size convolutional weights trained weight decay initial learning rate drops factor steps. model reaches peak performance around steps takes single nvidia tesla gpu. code easily derived open source implementation removing batch normalization adjusting residual components model architecture. important departure code initialize residual convolutional layer kernel size output channels using random normal initializer standard deviation rather used standard convolutional layers. substantially smaller weight initialization helped training affecting representation. notable difference standard models last layer trained simply ﬁxed random projection. hand slightly improved test error hand means trainable weights model convolutions making architecture all-convolutional. interesting aspect model despite massive size million trainable parameters model seem overﬁt quickly even though data size contrast found difﬁcult train model batch normalization size without signiﬁcant overﬁtting cifar. imagenet ilsvrc data data points classes. image resized pixels channels. experimented all-convolutional variant -layer network original model achieved classiﬁcation error. derived model trainable parameters. trained model momentum optimizer learning rate schedule decays factor every epochs starting initial learning rate training distributed across machines updating asynchronously. machine equipped gpus used batch size split across gpus updated batches size contrast situation cifar cifar imagenet allconvolutional model performed signiﬁcantly worse original counterpart. specifically experienced signiﬁcant amount underﬁtting suggesting larger despite issue model still reached top- classiﬁcation error test top- test error steps longer state-of-the-art performance signiﬁcantly better reported well best all-convolutional architecture believe quite likely better learning rate schedule hyperparameter settings model could substantially improve preliminary performance reported here. theory underlines importance identity parameterizations training deep artiﬁcial neural networks. outstanding open problem extend optimization result non-linear case residual single relu activiation expressivity result. conjecture result analogous theorem true general non-linear case. unlike standard parameterization fundamental obstacle result. hope theory experiments together help simplify state deep learning aiming explain success fundamental principles rather multitude tricks need delicately combined. believe much advances image recognition achieved residual convolutional layers relu activations alone. could lead extremely simple architectures match state-of-the-art image classiﬁcation benchmarks. acknowledgment thank jason qixing huang helpful discussions pointing error earlier version paper. tengy would like thank support dodds fellowship siebel scholarship. yann dauphin razvan pascanu caglar gulcehre kyunghyun surya ganguli yoshua bengio. identifying attacking saddle point problem high-dimensional non-convex optimization. advances neural information processing systems pages kaiming xiangyu zhang shaoqing jian sun. identity mappings deep residual networks. computer vision eccv european conference amsterdam netherlands october proceedings part pages sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning icml lille france july pages proof theorem turns proof signiﬁcantly easier assumed symmetric positive semideﬁnite matrix allow variables complex matrices. ﬁrst give proof sketch ﬁrst special case. readers skip jumps full proof below. also prove stronger results namely |||a||| special case. block diagonal situation apply claim above. proof theorem singular value decomposition orthonormal matrices diagonal matrix nonnegative entries diagonal. since properly since normal matrix claim block-diagnolaized orthonormal matrix sds− diag real block diagonal matrix block size using claim exists moreover diagonal matrix diagonal. since wij’s orthonormal matrix determinant even number diagonal. group blocks. note rotation matrix thus write concatenation diagonal block applying claim obtain section provide full proof theorem start following lemma constructs building block transform vectors arbitrary sequence vectors arbitrary vectors main value others. better abstraction denote sequence vectors. lemma size suppose sequences vectors satisfying every contains least arbitrary sequence vectors. then exists rk×k every tuvs) moreover every tuvs) different writing equation proof lemma without loss generality suppose construct follows. i-th denotes vector. i-column next verify correctness construction. ﬁrst consider vector i-th coordinate equal j-th coordinate equal upperbounded using assumption lemma vector. follows relu finally consider similarly computation equation vector coordinates less therefore vector negative entries. hence relu implies relu lemma repeatedly construct building blocks bksj thus prove lemma building block bksj takes subset vectors among convert maintaining vectors ﬁxed. since totally layers ﬁnally maps target vectors v’s. construct layers inductively. construct layers every hidden variable layer satisﬁes every assume constructed ﬁrst layer next lemma construct layer. argue choice satisﬁes assumption lemma indeed qi’s chosen uniformly randomly w.h.p every thus since also doesn’t correlate apply lemma conclude exists taj+bj+bj+) taj+bj+bj+) taj+bj+bj+ imply proof theorem formalize intuition discussed theorem first take sufﬁciently large absolute constant johnson-lindenstrauss theorem random matrix standard normal entires high probability pairwise distance vectors preserved factor. every section state folklore linear algebra statements. following claim known can’t literature. provide proof completeness. claim rd×d real normal matrix uu). then exists orthonormal matrix rd×d proof. since normal matrix unitarily diagonalizable backgrounds). therefore exists unitary matrix cd×d diagonal matrix cd×d eigen-decomposition since real matrix eigenvalues come conjugate pairs eigenvectors group columns pairs corresponding eigenvalues real matrix rank-. orthonormal basis column span matrix. written sidis finally diag complete proof.", "year": 2016}