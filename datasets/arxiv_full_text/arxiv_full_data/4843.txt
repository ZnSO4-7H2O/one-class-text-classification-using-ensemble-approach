{"title": "Model-Free Imitation Learning with Policy Optimization", "tag": ["cs.LG", "cs.AI"], "abstract": "In imitation learning, an agent learns how to behave in an environment with an unknown cost function by mimicking expert demonstrations. Existing imitation learning algorithms typically involve solving a sequence of planning or reinforcement learning problems. Such algorithms are therefore not directly applicable to large, high-dimensional environments, and their performance can significantly degrade if the planning problems are not solved to optimality. Under the apprenticeship learning formalism, we develop alternative model-free algorithms for finding a parameterized stochastic policy that performs at least as well as an expert policy on an unknown cost function, based on sample trajectories from the expert. Our approach, based on policy gradients, scales to large continuous environments with guaranteed convergence to local minima.", "text": "imitation learning agent learns beenvironment unknown cost function mimicking expert demonstrations. existing imitation learning algorithms typically involve solving sequence planning reinforcement learning problems. algorithms therefore directly applicable large high-dimensional environments performance signiﬁcantly degrade planning problems solved optimality. apprenticeship learning formalism develop alternative model-free algorithms ﬁnding parameterized stochastic policy performs least well expert policy unknown cost function based sample trajectories expert. approach based policy gradients scales large continuous environments guaranteed convergence local minima. reinforcement learning learner needs access cost reward signal identify desirable outcomes. dependence cost function corresponding optimal policy however generally complex involves planning. practice eliciting cost function achieves desired behavior difﬁcult alternative often practical approach called imitation learning encode preferences differentiate desirable undesirable outcomes using demonstrations provided expert problem conceptually simple theoretically sound small inaccuracies learned model compound time lead situations quite different ones encountered training. often referred problem cascading errors related covariate shift inverse reinforcement learning methods successful approaches imitation learning assume behavior learner desires imitate generated expert behaving optimally respect unknown cost function. algorithms train models entire trajectories behavior instead individual actions hence suffer cascading error problems. furthermore assumption expert optimality acts prior space policies algorithms allow learner generalize expert behavior unseen states much effectively learner tried produce policy value function instead unfortunately assumption expert optimality leads expensive design choices algorithms. iteration determine whether certain cost function expert policy algorithm must compare return return possible policies. algorithms running reinforcement learning algorithm because reinforcement learning must iteration algorithms extremely expensive large domains. forgo learning cost function. propose method directly learns policy expert trajectories exploiting learning signal class cost functions distinguish expert policy others. ﬁrst develop simple uniﬁed view certain class imitation learning algorithms called apprenticeship learning algorithms view naturally leads development gradient-based optimization formulation parameterized policies apprenticeship learning. provide model-free realizations optimization algorithms based standard policy gradient algorithm based recently developed policy gradient algorithm incorporates trust region constraints stabilize optimization. demonstrate effectiveness approach control problems high-dimensional observations train neural network control policies scratch. begin deﬁning basic notions reinforcement learning. given environment consisting state space action space dynamics model initial state distribution agents according stationary stochastic policies specify action choice probabilities state. work ﬁnite methods extend continuous spaces. respect cost function discount factor policy stateπ state value function action value function deﬁned ea∼π expected cost es∼p clarity parameterized function written parameter vector replace names quantities—for example etc. deﬁne γ-discounted state visitation distribution γtppπ probability landing state time following starting convenient overload notation state-action visitation distributions allowing address imitation learning problem adopt apprenticeship learning formalism learner must policy performs least well expert unknown true cost function ctrue formally learner’s goal policy ηctrue ηctrue given dataset trajectory samples ctrue assumption satisfying family constraints ensures successful apprenticeship learning. reformulate constraint satisfaction problem optimization problem deﬁning objective intuitively cost functions distinguish expert policies assigning high expected cost nonexpert policies expected cost expert policy. exists cost performs worse πe—in case poor solution apprenticeship learning problem. hand performs least well costs therefore satisﬁes apprenticeship learning constraints described general framework deﬁning apprenticeship learning algorithms. instantiate framework ingredients must provided cost function class optimization algorithm solve goal paper address optimization ingredient linearly parameterized cost functions experiments although development method agnostic particulars cost function class. section develop method approximately solving class parameterized stochastic policies assuming generic access method solving maximization costs ﬁxed policies method perform gradient-based stochastic optimization policy parameters—we refer strategy policy optimization. delving method ﬁrst review prototypical examples apprenticeship learning algorithms. show fall framework detailed section brieﬂy describe solution techniques solving differ vastly policy optimization method. structure clinear allows expected costs respect written inner product certain feature expectation vector deﬁned γtφ] ck]t cost clinear written linearity expectation based observation abbeel propose match feature expectations; policy thereby guaranteeing cost functions clinear. understand feature expectation matching minimization δclinear solve problem abbeel propose incrementally generate policies inverse reinforcement learning iteration algorithm ﬁnds cost function assigns expected cost expert high expected cost previously found policies. then adds policies optimal policy cost computed reinforcement learning. steps repeated cost function makes expert perform much better previously found policies. ﬁnal policy minimizes produced stochastically mixing generated policies using weights calculated quadratic program. algorithm quite expensive large mdps requires running reinforcement learning iteration. game-theoretic approaches syed schapire syed proposed apprenticeship learning algorithms multiplicative weights apprenticeship learning linear programming apprenticeship learning also basis cost functions weights restricted give convex combination cconvex mwal uses multiplicative weights update method solve resulting optimization problem like abbeel ng’s method requires running reinforcement learning inner loop. syed address computational complexity lpal method. notice restricting weights basis functions simplex allows maximization costs performed instead ﬁnite problem cconvex written minπ maxi∈{...k} ηci. syed therefore able formulate single linear program state-action visitation frequencies simultaneously encodes bellman constraints frequencies ensure generated policy environment. inspired lpal formulate unconstrained optimization approach apprenticeship learning. propose optimize directly parameterized stochastic policies instead state-action visitation distributions allowing scale large spaces without keeping variables state action. keep formulation general enough allow cost function class focus paper optimization policies linearly parameterized cost classes clinear cconvex experiments. reviewed existing algorithms solving various settings apprenticeship learning delve policy optimization strategies directly operate stochastic optimization problem. allow scale large continuous environments ﬁrst class smoothly parameterized stochastic policies valid parameter vectors. class policies goal solve optimization problem policy parameters supc∈c propose local minimum problem using gradient-based stochastic optimization. ﬁrst examine gradient respect letting denote cost function achieves supremum formula dictates basic structure gradient-based algorithm must take minimize δc—it must ﬁrst compute ﬁxed must improve next iteration. cost effectively deﬁnes reinforcement learning problem current policy interpret gradient-based optimization procedure alternates ﬁtting local reinforcement learning problem generate learning signal imitation improving policy respect local problem. discuss section discuss strategies policy improvement. classic policy gradient formula cost function estimate samples propose following algorithm called im-reinforce parallels development reinforce reinforcement learning. input im-reinforce given expert trajectories iteration current parameter vector trajectories sampled using then cost attaining supremum empirical estimate calculated satisfy here denotes empirical expectation using rollout samples. describe compute detail section depends im-reinforce estimates gradient ∇θηˆc using formula state-action value estimated using discounted future sums costs along rollouts finally complete iteration im-reinforce takes step resulting gradient direction producing policy parameters ready next iteration. steps summarized algorithm im-reinforce straightforward implement gradient estimator exhibits extremely high variance making algorithm slow converge even diverge reasonably large step sizes. variance issue unique apprenticeship learning formulation hallmark difﬁculty policy gradient algorithms reinforcement learning reinforcement learning literature contains vast number techniques calculating high-quality policy parameter steps based monte carlo estimates gradient. make attempt fully review techniques here. instead directly draw inspiration recently developed algorithm called trust region policy optimization model-free policy search algorithm capable quickly training large neural network stochastic policies complex tasks trpo reinforcement learning section review trpo reinforcement learning next develop analogous algorithm apprenticeship learning setting. drop cost function superscript cost ﬁxed reinforcement learning setting. es∼ρπ ea∼π policies parameterized matches ﬁrst order therefore taking small gradient step guarantees improvement however little guidance large step cases gradient estimated required step size might extremely small compensate noise. schulman address showing minimizing certain surrogate loss function guarantee policy improvement large step size. deﬁne following penalized variant divergence zero arguments equal inequality shows majorizes using majorizer majorization-minimization algorithm leads algorithm guaranteeing monotonic policy improvement iteration. unfortunately currently deﬁned computing maximum-kl divergence term whole state space intractable. schulman propose relax average state space approximated samples average-kl formulation works well empirically algorithm’s stability could improved reformulating cost trust region constraint. leads trpo step computation s.t. constants equation folded predeﬁned trust region size solve step computation problem objective divergence constraint must approximated using samples minimized gradient-based constrained optimization. sample approximation done using similar strategy described section discussion sampling methodologies effective optimization algorithms solving constrained problem found schulman trpo apprenticeship learning describe adapt trpo apprenticeship learning reintroducing superscripts wish compute improvement step optimization problem wish derive majorizer objective analogous majorizer ﬁxed cost function. observe {fα} {gα} families functions majorizes supα majorizes supα therefore derive trpo-style algorithm apprenticeship learning follows. first remove dependence particular cost function assume cost functions bounded cmax said majorize equality practice easy satisfy clinear cconvex enhence apply empirically justiﬁed transformation trpo replacing maximum-kl cost average-kl constraint. therefore propose compute steps apprenticeship learning setting solving following trust region subproblem again constants folded solve trust region subproblem ﬁnite-sample regime approximate constraint using samples trpo trust region problem objective however warrants attention interplay maximization minimization objective wish derive ﬁnite-sample approximation suitable objective subproblem computational reasons would like avoid trajectory sampling within optimization subproblem. replacing expectations empirical ones gives ﬁnal form objective deﬁne ﬁnitesample trust region subproblem. solving trust region subproblems yields ﬁnal algorithm call im-trpo computational power needed minimize trust region cost much greater trpo subproblem assuming supremum easily computable. show next section solving im-trpo subproblems indeed poses signiﬁcant difﬁculty computation necessary im-reinforce. deferred discussion ﬁnding cost functions achieve supremum apprenticeship learning objective address issue feature expectation matching setting described section cost functions clinear parameterized linearly -bounded weight vectors case cconvex derived similarly omitted space reasons. mentioned section apprenticeship learning algorithm access sample trajectories consider ﬁnding cost achieves supremum empirical apprenticeship learning objective using denote optimal cost empirical objective closed-form expression ˆφ)/ inserted directly reinforce. however sufﬁce imtrpo which objective trust region subproblem requires maximizer must recomputed every optimization step subproblem. recomputation difﬁcult expensive demonstrate. linear costs empirical trust region subproblem objective given straightforward compute. note vector depends changes trust region subproblem optimized must recomputed step algorithm solving trust region subproblem. however construction empirical expectations taken respect change changes hence simulations environment required recomputations. evaluated approach variety scenarios ﬁnite gridworlds varying sizes continuous planar navigation task levine koltun family continuous environments varying numbers observation features variation levine koltun’s highway driving simulation agent receives high-dimensional egocentric observation features. continuous environments used policies constructed according schulman policies gaussian action distributions mean given multi-layer perceptron taking observations input standard deviations given extra parameters. details environments training methodology supplement. comparing globally optimal methods mentioned section lpal ﬁnds global optimum apprenticeship problem cconvex ﬁnite state action spaces. contrast approach scales high-dimensional spaces guaranteed local optimum described section evaluate quality local optima tested im-reinforce using cconvex learn tabular boltzmann policies value iteration exact gradient evaluation lpal behavioral cloning baseline. evaluated learned policies three algorithms gridworlds varying amounts expert data. trial randomly generated costs world generated expert data sampling behavior optimal policy computed value iteration. evaluate algorithm computed ratio learned policy performance expert’s performance. also timing test evaluated computation time algorithm varying gridworld sizes ﬁxed dataset sizes trials each. results displayed figure found despite local optimality guarantee im-reinforce learned policies achieving least performance policies learned lpal similar sample complexity. imreinforce’s training times also scaled favorably compared lpal. large gridworld states lpal took average minutes train large variance across instantiations expert whereas algorithm consistently took around minutes. comparing continuous next evaluated algorithms small continuous environment objectworld environment levine koltun agent moves plane seek gaussianshaped costs given expert data generated either globally locally optimal expert policies. compared trajectories produced im-trpo clinear produced trajectory optimization cost learned levine koltun’s cioc algorithm model-based method designed continuous settings full knowledge dynamics derivatives. basis functions used clinear used cioc deﬁne found even though method model-free dynamics derivatives consistently learned policies achieving zero excess cost matching performance optimal trajectories cost functions learned cioc. varying dimension evaluate algorithms’ performance varying environment dimension used family environments inspired karpathy environments agent moves plane populated colored moving targets either captured avoided depending color. action space two-dimensional allowing agent apply forces move direction. agent number sensors nsensors facing outward uniform angular spacing. sensor detects presence target continuous features indicating nearest target’s distance color relative velocity agent. sensor features along indicators whether agent currently capturing target lead observation features dimension nsensors policy. varying nsensors yields family environments differing observation dimension. nsensors ﬁrst generated expert data executing policies learned reinforcement learning true cost linear combination basis functions indicating control effort intersection targets. then im-reinforce im-trpo using clinear basis functions measured excess cost learned policy. found im-trpo achieved nearly perfect imitation setting performance significantly affected dimensionality space. imreinforce’s learning also progressed outpaced im-trpo also veriﬁed im-trpo’s overhead computing step solving trust region subproblem negligible verifyhighway driving finally im-trpo variation highway driving task levine koltun task learner must imitate driving behaviors continuous driving simulation. observations original driving task actual states whole environment including positions velocities cars points road. introduce realism modiﬁed environment providing policies egocentric observation features readings equally spaced rangeﬁnders detect road edges readings equally spaced rangeﬁnders detect cars speed angular velocity agent’s car. observations effectively form depth image nearby cars lane markings within agent’s ﬁeld view; figure aggregated readings window timesteps yielding dimensional partial observations. im-trpo clinear using basis functions representing quadratic features derived levine koltun. despite fact provided highdimensional partial observations model-free approach learned policies achieving behavior comparable trajectories generated cioc provided full state features full environment model. policies learned algorithm generated behavior qualitatively showed carefully blending state-of-the-art policy gradient algorithms reinforcement learning local cost function ﬁtting lets successfully train neural network policies imitation high-dimensional continuous environments. method able identify locally optimal solution even settings optimal planning reach. signiﬁcant advantage competing algorithms require repeatedly solving planning problems inner loop. fact inner planning problem approximately solved competing algorithms even provide local optimality guarantees approach expert interaction reinforcement signal ﬁtting family approaches includes apprenticeship learning inverse reinforcement learning. either additional resources provided alternative approaches sample efﬁcient investigating ways combine resources framework interesting research direction. focused policy optimization component apprenticeship learning rather design appropriate cost function classes. believe important area future work. nonlinear cost function classes successful well machine learning problems reminiscent ours particular training generative image models. language generative adversarial networks policy parameterizes generative model state-action pairs cost function serves adversary. apprenticeship learning large cost function classes capable distinguishing between arbitrary state-action visitation distributions would enticingly open possibility exact imitation. thank john schulman valuable conversations trpo. work supported grant sailtoyota center research national science foundation graduate research fellowship references abbeel pieter andrew apprenticeship learning proceedings inverse reinforcement learning. international conference machine learning ermon stefano yexiang toth russell dilkina bistra bernstein richard damoulas theodoros clark patrick degloria steve mude andrew barrett christopher learning large-scale dynamic discrete choice models spatio-temporal preferences application migratory pastoralism east africa. aaai goodfellow pouget-abadie jean mirza mehdi bing warde-farley david ozair sherjil courville aaron bengio yoshua. generative adversarial nets. advances neural information processing systems levine sergey koltun vladlen. continuous inverse optimal control locally optimal examples. proceedings international conference machine learning levine sergey popovic zoran koltun vladlen. nonlinear inverse reinforcement learning gaussian processes. advances neural information processing systems ross st´ephane gordon geoffrey bagnell drew. reduction imitation learning structured prediction no-regret online learning. international conference artiﬁcial intelligence statistics schulman john levine sergey abbeel pieter jordan michael moritz philipp. trust region policy optimization. proceedings international conference machine learning sutton richard mcallester david singh satinder mansour yishay. policy gradient methods reinforcement learning function approximation. advances neural information processing systems syed umar bowling michael schapire robert apprenticeship learning using linear programming. proceedings international conference machine learning here give extra information regarding environment algorithm setups experiments. gridworld used tabular policies im-reinforce parameters action probabilities exp; used value iteration obtain values gradient formula solved linear programs lpal gurobi deﬁned policies learned behavioral cloning simple lookups expert data timing tests performed -core .ghz intel cpu. gridworlds used resembled abbeel square grid states actions fail probability result random move. test consisted trials. costs generated non-overlapping regions gridworld giving basis function cconvex region. waterworld ﬁrst trpo various iteration counts obtain expert policies achieving various expected costs according true cost function penalized application control assigned differing cost values targets different colors. then executed expert policy yield trajectory samples im-reinforce im-trpo iterations imitate expert policy. trajectories timesteps long discount factor gave algorithms rollouts iteration. excess costs computed averaging rollouts. highway driving style im-trpo iterations collecting state-action pairs simulation. datasets dynamics model identical ones used levine koltun evaluated policies measurements used levine koltun averaged rollouts.", "year": 2016}