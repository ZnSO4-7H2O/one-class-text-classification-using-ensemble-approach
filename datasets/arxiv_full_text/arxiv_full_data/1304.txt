{"title": "Network In Network", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.", "text": "propose novel deep network structure called network network enhance model discriminability local patches within receptive ﬁeld. conventional convolutional layer uses linear ﬁlters followed nonlinear activation function scan input. instead build micro neural networks complex structures abstract data within receptive ﬁeld. instantiate micro neural network multilayer perceptron potent function approximator. feature maps obtained sliding micro networks input similar manner cnn; next layer. deep implemented stacking mutiple described structure. enhanced local modeling micro network able utilize global average pooling feature maps classiﬁcation layer easier interpret less prone overﬁtting traditional fully connected layers. demonstrated state-of-the-art classiﬁcation performances cifar- cifar- reasonable performances svhn mnist datasets. convolutional neural networks consist alternating convolutional layers pooling layers. convolution layers take inner product linear ﬁlter underlying receptive ﬁeld followed nonlinear activation function every local portion input. resulting outputs called feature maps. convolution ﬁlter generalized linear model underlying data patch argue level abstraction glm. abstraction mean feature invariant variants concept replacing potent nonlinear function approximator enhance abstraction ability local model. achieve good extent abstraction samples latent concepts linearly separable i.e. variants concepts live side separation plane deﬁned glm. thus conventional implicitly makes assumption latent concepts linearly separable. however data concept often live nonlinear manifold therefore representations capture concepts generally highly nonlinear function input. replaced micro network structure general nonlinear function approximator. work choose multilayer perceptron instantiation micro network universal function approximator neural network trainable back-propagation. resulting structure call mlpconv layer compared figure linear convolutional layer mlpconv layer local receptive ﬁeld output feature vector. mlpconv maps input local patch output feature vector multilayer perceptron consisting multiple fully connected layers nonlinear activation functions. shared among local receptive ﬁelds. feature maps obtained sliding figure comparison linear convolution layer mlpconv layer. linear convolution layer includes linear ﬁlter mlpconv layer includes micro network layers local receptive ﬁeld conﬁdence value latent concept. input similar manner next layer. overall structure stacking multiple mlpconv layers. called network network micro networks composing elements overall deep network within mlpconv layers instead adopting traditional fully connected layers classiﬁcation directly output spatial average feature maps last mlpconv layer conﬁdence categories global average pooling layer resulting vector softmax layer. traditional difﬁcult interpret category level information objective cost layer passed back previous convolution layer fully connected layers black between. contrast global average pooling meaningful interpretable enforces correspondance feature maps categories made possible stronger local modeling using micro network. furthermore fully connected layers prone overﬁtting heavily depend dropout regularization global average pooling structural regularizer natively prevents overﬁtting overall structure. classic convolutional neuron networks consist alternatively stacked convolutional layers spatial pooling layers. convolutional layers generate feature maps linear convolutional ﬁlters followed nonlinear activation functions using linear rectiﬁer example feature calculated follows linear convolution sufﬁcient abstraction instances latent concepts linearly separable. however representations achieve good abstraction generally highly nonlinear functions input data. conventional might compensated utilizing over-complete ﬁlters cover variations latent concepts. namely individual linear ﬁlters learned detect different variations concept. however many ﬁlters single concept imposes extra burden next layer needs consider combinations variations previous layer ﬁlters higher layers larger regions original input. generates higher level concept combining lower level concepts layer below. therefore argue would beneﬁcial better abstraction local patch combining higher level concepts. recent maxout network number feature maps reduced maximum pooling afﬁne feature maps maximization linear functions makes piecewise linear approximator capable approximating convex functions. compared conventional convolutional layers perform linear separation maxout network potent separate concepts within convex sets. improvement endows maxout network best performances several benchmark datasets. however maxout network imposes prior instances latent concept within convex input space necessarily hold. would necessary employ general function approximator distributions latent concepts complex. seek achieve introducing novel network network structure micro network introduced within convolutional layer compute abstract features local patches. sliding micro network input proposed several previous works. example structured multilayer perceptron applies shared multilayer perceptron different patches input image; another work neural network based ﬁlter trained face detection however designed speciﬁc problems contain layer sliding network structure. proposed general perspective micro network integrated structure persuit better abstractions levels features. ﬁrst highlight components proposed network network structure convolutional layer global averaging pooling layer sec. sec. respectively. detail overall sec. given priors distributions latent concepts desirable universal function approximator feature extraction local patches capable approximating abstract representations latent concepts. radial basis network multilayer perceptron well known universal function approximators. choose multilayer perceptron work reasons. first multilayer perceptron compatible structure convolutional neural networks trained using back-propagation. second multilayer perceptron deep model itself consistent spirit feature re-use type layer called mlpconv paper replaces convolve input. figure illustrates difference linear convolutional layer mlpconv layer. calculation performed mlpconv layer shown follows cross channel pooling point view equation equivalent cascaded cross channel parametric pooling normal convolution layer. pooling layer performs weighted linear recombination input feature maps rectiﬁer linear unit. cross channel pooled feature maps cross channel pooled next layers. cascaded cross channel parameteric pooling structure allows complex learnable interactions cross channel information. cross channel parametric pooling layer also equivalent convolution layer convolution kernel. interpretation makes straightforawrd understand structure nin. comparison maxout layers maxout layers maxout network performs pooling across multiple afﬁne feature maps feature maps maxout layers calculated follows maxout linear functions forms piecewise linear function capable modeling convex function. convex function samples function values speciﬁc threshold form convex set. therefore approximating convex functions local patch maxout capability forming separation hyperplanes concepts whose samples within convex mlpconv layer differs maxout layer convex function approximator replaced universal function approximator greater capability modeling various distributions latent concepts. conventional convolutional neural networks perform convolution lower layers network. classiﬁcation feature maps last convolutional layer vectorized fully connected layers followed softmax logistic regression layer structure bridges convolutional structure traditional neural network classiﬁers. treats convolutional layers feature extractors resulting feature classiﬁed traditional way. however fully connected layers prone overﬁtting thus hampering generalization ability overall network. dropout proposed hinton regularizer randomly sets half activations fully connected layers zero training. improved generalization ability largely prevents overﬁtting paper propose another strategy called global average pooling replace traditional fully connected layers cnn. idea generate feature corresponding category classiﬁcation task last mlpconv layer. instead adding fully connected layers feature maps take average feature resulting vector directly softmax layer. advantage global average pooling fully connected layers native convolution structure enforcing correspondences feature maps categories. thus feature maps easily interpreted categories conﬁdence maps. another advantage parameter optimize global average pooling thus overﬁtting avoided layer. futhermore global average pooling sums spatial information thus robust spatial translations input. global average pooling structural regularizer explicitly enforces feature maps conﬁdence maps concepts made possible mlpconv layers makes better approximation conﬁdence maps glms. layers maxout networks. figure shows three mlpconv layers. within mlpconv layer three-layer perceptron. number layers micro networks ﬂexible tuned speciﬁc tasks. evaluate four benchmark datasets cifar- cifar- svhn mnist networks used datasets consist three stacked mlpconv layers mlpconv layers experiments followed spatial pooling layer downsamples input image factor two. regularizer dropout applied outputs last mlpconv layers. unless stated speciﬁcally networks used experiment section global average pooling instead fully connected layers network. another regularizer applied weight decay used krizhevsky figure illustrates overall structure network used section. detailed settings parameters provided supplementary materials. implement network super fast cuda-convnet code developed alex krizhevsky preprocessing datasets splitting training validation sets follow goodfellow adopt training procedure used krizhevsky namely manually proper initializations weights learning rates. network trained using mini-batches size training process starts initial weights learning rates continues accuracy training stops improving learning rate lowered scale procedure repeated ﬁnal learning rate percent initial value. cifar- dataset composed classes natural images training images total testing images. image image size dataset apply global contrast normalization whitening used goodfellow maxout network last images training validation data. number feature maps mlpconv layer experiment number corresponding maxout network. hyper-parameters tuned using validation i.e. local receptive ﬁeld size weight decay. hyper-parameters ﬁxed re-train network scratch training validation set. resulting model used testing. obtain test error dataset improves percent compared state-of-the-art. comparison previous methods shown table method stochastic pooling spearmint conv. maxout dropout dropout spearmint data augmentation conv. maxout dropout data augmentation dropconnect networks data augmentation dropout data augmentation turns experiment using dropout mlpconv layers boosts performance network improving generalization ability model. shown figure introducing dropout layers mlpconv layers reduced test error observation consistant goodfellow thus dropout added mlpconv layers models used paper. model without dropout regularizer achieves error rate cifar- dataset already surpasses many previous state-of-the-arts regularizer since performance maxout without dropout available dropout regularized version compared paper. consistent previous works also evaluate method cifar- dataset translation horizontal ﬂipping augmentation. able achieve test error sets state-of-the-art performance. cifar- dataset size format cifar- dataset contains classes. thus number images class tenth cifar- dataset. cifar- tune hyper-parameters setting cifar- dataset. difference last mlpconv layer outputs feature maps. test error obtained cifar- surpasses current best performance without data augmentation percent. details performance comparison shown table svhn dataset composed color images divided training testing extra set. task data classify digit located center image. training testing procedure follow goodfellow namely samples class selected training samples class extra used validation. remainder training extra used training. validation used guidance hyper-parameter selection never used training model. preprocessing dataset follows goodfellow local contrast normalization. structure parameters used svhn similar used cifar- consist three mlpconv layers followed global average pooling. dataset obtain method stochastic pooling rectiﬁer dropout rectiﬁer dropout synthetic translation conv. maxout dropout dropout multi-digit number recognition dropconnect mnist dataset consists hand written digits size. training images testing images total. dataset network structure used cifar- adopted. numbers feature maps generated mlpconv layer reduced. mnist simpler dataset compared cifar-; fewer parameters needed. test method dataset without data augmentation. result compared previous works adopted convolutional structures shown table global average pooling layer similar fully connected layer perform linear transformations vectorized feature maps. difference lies transformation matrix. global average pooling transformation matrix preﬁxed non-zero block diagonal elements share value. fully connected layers dense transformation matrices values subject back-propagation optimization. study regularization effect global average pooling replace global average pooling layer fully connected layer parts model remain same. evaluated model without dropout fully connected linear layer. models tested cifar- dataset comparison performances shown table regularizer applied. adding dropout fully connected layer reduced testing error global average pooling achieved lowest testing error among three. explore whether global average pooling regularization effect conventional cnns. instantiate conventional described hinton consists three convolutional layers local connection layer. local connection layer generates feature maps fully connected layer dropout. make comparison fair reduce number feature local connection layer since feature allowed category global average pooling scheme. equivalent network global average pooling created replacing dropout fully connected layer global average pooling. performances tested cifar- dataset. model fully connected layer achieve error rate dropout added achieve similar performance reported hinton replacing fully connected layer global average pooling model obtain error rate percent improvement compared without dropout. veriﬁes effectiveness global average pooling layer regularizer. although slightly worse dropout regularizer result argue global average pooling might demanding linear convolution layers requires linear ﬁlter rectiﬁed activation model conﬁdence maps categories. explicitly enforce feature maps last mlpconv layer conﬁdence maps categories means global average pooling possible stronger local receptive ﬁeld modeling e.g. mlpconv nin. understand much purpose accomplished extract directly visualize feature maps last mlpconv layer trained model cifar-. figure shows examplar images corresponding feature maps categories selected cifar- test set. expected largest activations observed feature corresponding ground truth category input image explicitly enforced global average pooling. within feature ground truth category observed strongest activations appear roughly region object original image. especially true structured objects second figure note feature maps categories trained category information. better results expected bounding boxes objects used grained labels. visualization demonstrates effectiveness nin. achieved stronger local receptive ﬁeld modeling using mlpconv layers. global average pooling enforces learning category level feature maps. exploration made towards general object detection. detection results achieved based category level feature maps ﬂavor scene labeling work farabet proposed novel deep network called network network classiﬁcation tasks. structure consists mlpconv layers multilayer perceptrons convolve input global average pooling layer replacement fully connected layers conventional cnn. mlpconv layers model local patches better global average pooling acts structural regularizer prevents overﬁtting globally. components demonstrated state-of-the-art performance cifar- cifar- svhn datasets. visualization feature maps demonstrated feature maps last mlpconv layer conﬁdence maps categories motivates possibility performing object detection nin.", "year": 2013}