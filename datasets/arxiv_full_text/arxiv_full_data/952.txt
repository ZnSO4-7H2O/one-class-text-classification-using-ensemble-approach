{"title": "Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks", "tag": ["cs.NE", "cs.LG", "stat.ML"], "abstract": "In this paper we propose and investigate a novel nonlinear unit, called $L_p$ unit, for deep neural networks. The proposed $L_p$ unit receives signals from several projections of a subset of units in the layer below and computes a normalized $L_p$ norm. We notice two interesting interpretations of the $L_p$ unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the $L_p$ unit is, to a certain degree, similar to the recently proposed maxout unit (Goodfellow et al., 2013) which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the $L_p$ unit is more efficient at representing complex, nonlinear separating boundaries. Each $L_p$ unit defines a superelliptic boundary, with its exact shape defined by the order $p$. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few $L_p$ units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed $L_p$ units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the $L_p$ units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed $L_p$ unit on the recently proposed deep recurrent neural networks (RNN).", "text": "abstract. paper propose investigate novel nonlinear unit called unit deep neural networks. proposed unit receives signals several projections subset units layer computes normalized norm. notice interesting interpretations unit. first proposed unit understood generalization number conventional pooling operators average root-mean-square pooling widely used instance convolutional neural networks hmax models neocognitrons. furthermore unit certain degree similar recently proposed maxout unit achieved state-of-the-art object recognition results number benchmark datasets. secondly provide geometrical interpretation activation function based argue unit eﬃcient representing complex nonlinear separating boundaries. unit deﬁnes superelliptic boundary exact shape deﬁned order claim makes possible model arbitrarily shaped curved boundaries eﬃciently combining units diﬀerent orders. insight justiﬁes need learning diﬀerent orders unit model. empirically evaluate proposed units number datasets show multilayer perceptrons consisting units achieve state-of-the-art results number benchmark datasets. furthermore evaluate proposed unit recently proposed deep recurrent neural networks importance well-designed nonlinear activation functions building deep neural network become apparent recently. novel nonlinear activation functions unbounded often piecewise linear continuous rectiﬁed linear units rectiﬁer maxout units found particularly well suited deep neural networks many object recognition tasks. employ pooling achieved state-of-the-art recognition performances various benchmark datasets also biologically inspired models hmax employed pooling pooling operator context understood summarize high-dimensional collection neural responses produce features invariant variations input recently authors proposed understand pooling operator nonlinear activation function. proposed maxout unit pools group linear responses outputs neurons overall acts piecewise linear activation function. approach achieved many state-of-the-art results various benchmark datasets. paper attempt generalize approach noticing pooling operators including pooling well maxout units understood special cases computing normalized norm outputs ﬁlter outputs. unlike conventional pooling operators however claim beneﬁcial estimate order norm instead ﬁxing certain predeﬁned value pooling. beneﬁt learning order thereby neural network units diﬀerent orders understood geometrical perspective. unit deﬁnes spherical shape non-euclidean space whose metric deﬁned norm combination multiple units leads non-trivial separating boundary input space. particular learn highly curved boundary eﬃciently taking advantage diﬀerent values contrast using conventional nonlinear activation function rectiﬁer results boundaries piece-wise linear. approximating curved separation classes would expensive case terms number hidden units piece-wise linear segments. sec. basic description multi-layer perceptron given followed explanation pooling operator considered nonlinear activation function mlp. propose novel unit generalizing pooling operators norms sec. sec. proposed unit analyzed geometrical perspective. describe proposed unit used recurrent neural networks sec. sec. provides empirical evaluation unit number object recognition tasks. pooling operators widely used convolutional neural networks reduce dimensionality high-dimensional output convolutional layer. used group spatially neighboring neurons operator summarizes group neurons lower layer able achieve property translation invariance. various types pooling operator proposed used successfully average pooling root-of-mean-squared pooling pooling pooling operator viewed instead nonlinear activation function. receives input signal layer below returns scalar value. output result applying nonlinear function diﬀerence traditional nonlinearities pooling operator applied element-wise lower layer rather groups hidden units. maxout nonlinear activation function proposed recently representative example pooling respect. recent success maxout motivated consider general nonlinear activation function rooted pooling operator. section propose discuss nonlinear activation function called unit replaces operator maxout unit norm. fig. illustration single unit sets incoming signals. clarity biases division number ﬁlters omitted. symbol block represents input signal speciﬁc block only. illustration eﬀect shape ellipsoid. ﬁrst quadrant shown. indicates order norm diﬀer neuron. noticed deﬁnition norm anymore violation triangle inequality. practice re-parameterize satisfy constraint. parameters layers units estimated using backpropagation particular adapt order norm. experiments theano compute partial derivatives update orders usual parameters. thanks deﬁnition proposed unit based norm straightforward many previously described nonlinear activation functions pooling operators closely related special cases unit. discuss them. function projection lower layer activation reduced computing average projections. form average pooling non-linear projections represent pooled layer. single ﬁlter equivalent absolute value rectiﬁcation proposed instead root-of-mean-squared pooling recovered. short proposed unit interpolates among diﬀerent pooling operators choice order noticed earlier well however stopped analyzing norm pooling operator ﬁxed order comparing conventional pooling operators other. authors investigated similar nonlinear activation function inspired cells primary visual cortex. possibility learning investigated probabilistic setting computer vision. hand paper claim order needs learned like parameters deep neural network. furthermore conjecture optimal distribution orders units diﬀers dataset another unit requires diﬀerent order properties also distinguish proposed unit conventional radial-basis function network fig. visualization separating curves obtained using diﬀerent activation functions. underlying data distribution mixture gaussian distributions. green dots samples classes respectively black curves separating curves found mlps. purple lines indicate axes subspace learned unit. best viewed color. fig. visualization separating curves obtained using diﬀerent orders units. underlying data distribution mixture three gaussian distributions. blue curves show shape superellipse learned unit. green blue dots samples. otherwise color convention fig. used. represents i-th column matrix equation eﬀectively says unit computes p-th norm projection input subspace spanned vectors wn}. assume vector euclidean space. space onto projected spanned linearly dependent vectors wi’s. possible lack linear independence among vectors span subspace dimensionality subspace origin impose non-euclidean geometry subspace deﬁning norm space potentially geometrical object particular value unit corresponds forms superellipse projected back original input space. superellipse centered since superellipse degenerate sense axes width become inﬁnitely large. however invalidate inverse projection euclidean input space. shape varies according order unit potentially linearly-dependent bases. long shape remains convex. fig. draws superellipses diﬀerent orders function unit partitions input space regions inside outside superellipse. unit uses curved boundary learned curvature divide space. contrast instance maxout unit uses piecewise linear hyperplanes might require linear pieces approximate curved segment. dimensionality input space receives input signals visualize partitions input space obtained using units well conventional nonlinear activation functions. here examine artiﬁcially generated cases space. classes single unit fig. shows case classes corresponds gaussian distribution. trained mlps single hidden neuron. mlps unit ﬁxed either fig. unit divides input space regions inside outside rotated superellipse. superellipse correctly identiﬁed classes case degenerate rectangle case rectiﬁer units could correct separating curve clear single rectiﬁer unit partition input space linearly unlike units. combination several rectiﬁer units result nonlinear boundary speciﬁcally piecewise-linear though claim need rectiﬁer units arbitrarily shaped curve whose curvature changes highly nonlinear way. three classes units similarly previous experiment trained mlps units data generated mixture three gaussian distribution. again mixture component corresponds class. units ﬁxed respectively. unit deﬁnes usual superellipse unit deﬁnes rectangle. separating curves constructed combination translated superellipse rectangle non-trivial curvature fig. furthermore clear plots fig. curvature separating curves change input space. easier model non-stationary curvature using multiple units diﬀerent p’s. decision boundary non-stationary curvature representational eﬃciency order test potential eﬃciency proposed unit ability learn order designed binary classiﬁcation task decision boundary non-stationary curvature. data points subset shown fig. classes marked dataset trained mlps either units units maxout units rectiﬁers logistic sigmoid units. varied number parameters correspond number units case rectiﬁers logistic sigmoid units number inputs signals hidden layer case units units maxout units setting trained randomly initialized mlps. order reduce eﬀects optimization diﬃculties used cases natural conjugate gradient fig. clear mlps units outperform others terms representing speciﬁc curve. able achieve zero training error three units random runs achieved lowest average training error even less units. importantly comparison performance mlps units shows beneﬁcial learn orders units. example units none random runs succeed least succeeds units. mlps especially ones rectiﬁers maxout units model decision boundary piecewise linear functions able achieve similar eﬃciency mlps units fig. also shows decision boundary found units training. observed shapes units unit learned appropriate order enables model non-stationary decision boundary. fig. shows boundary obtained rectiﬁer model four units. linear segments compose boundary resulting perfectly solving task. rectiﬁer model represented mistakes versus obtained model. fig. visualization data decision boundary learned units shapes corresponding orders learned units visualization done using four rectiﬁers. failure rates computed mlps using diﬀerent numbers diﬀerent nonlinear activation functions maxout green dashed curve green rectiﬁer cyan dash-dot curve cyan sigmoid purple dashed curve purple curves show proportion failed attempts random trials either number units sigmoid rectiﬁer model total number linear projection going maxout units units fig. distributions initial learned orders mnist pentomino. x-axis y-axis show order number units corresponding order. note diﬀerence scales x-axes y-axes logarithmic scale. although low-dimensional artiﬁcially generated example demonstrates proposed units eﬃcient representing decision boundaries non-stationary curvatures. conventional recurrent neural network mostly uses saturating nonlinear activation functions tanh compute hidden state time step. prevents possible explosion activations hidden states time general results stable learning dynamics. however time allow build recently proposed non-saturating activation functions rectiﬁers maxout well proposed units. authors recently proposed three ways extend conventional shallow deep rnn. among three proposals notice possible non-saturating activations functions deep deep transition without causing instability model saturating non-linearity applied sandwich associated step. usual saturating nonlinear activation function used activations hidden state bounded. allows potentially non-saturating nonlinear function simply layer proposed unit place input previous summary highly nonlinear beneﬁt proposed unit existing conventional activation functions feedforward neural networks naturally translate deep rnns well. show eﬀect empirically later training deep output deep transition proposed units. section provide empirical evidences showing advantages utilizing units. order clearly distinguish eﬀect employing units introducing data-speciﬁc model architectures experiments section performed neural networks densely connected hidden layers. ﬁrst claim states universally optimal order train mlps number benchmark datasets resulting distribution pj’s. distributions similar tasks claim would rejected. naturally connects second claim. orders estimated learning unlikely orders units convergence single value expect response unit specialize using distinct order. inspection trained mlps conﬁrm ﬁrst claim validate claim well. claims expect units parameters including orders units well estimated achieve highly competitive classiﬁcation performance. addition classiﬁcation tasks using feedforward neural networks anticipate recurrent neural network beneﬁts units intermediate layer consecutive hidden states well. feedforward neural networks mlps used four datasets; mnist pentomino toronto face database forest covertype mnist forest covertype three representative benchmark datasets pentomino relatively recently proposed dataset known induce diﬃcult optimization challenge deep neural network. used three music datasets evaluating eﬀect units deep recurrent neural networks. understand estimated orders proposed unit distributed trained mlps single layer mnist pentomino. measured validation error search good hyperparameters including number units number ﬁlters unit. however pentomino simply ﬁxed size layer unit received signals hidden units below. table averages standard deviations estimated orders units single-layer mlps listed mnist pentomino. clear distribution orders depend heavily dataset conﬁrms ﬁrst claim described earlier. fig. clearly even single model estimated orders vary quite conﬁrms second claim. interestingly case pentomino distribution orders consists distinct modes. plots fig. clearly show orders units change signiﬁcantly initial values training. although initialized orders units around datasets resulting distributions orders signiﬁcantly diﬀerent among three datasets. conﬁrms claims. simple empirical conﬁrmation tried experiment ﬁxed achieved worse test error ultimate goal novel nonlinear activation function achieve better generalization performance. conjectured learning orders units layers achieve highly competitive classiﬁcation performance. table generalization errors three datasets obtained mlps using proposed units. previous state-of-the-art results obtained others also presented comparison. used previous experiment evaluate generalization performance. achieved recognition rate although neither pretraining unlabeled samples result close current state-of-the-art rate permutation-invariant version task reported pretrained models large amount unlabeled samples. used ﬁve-fold cross validation optimal hyperparameters able investigate variance estimations values. table shows averages standard deviations estimated orders mlps trained folds using best hyperparameters. clear cases orders ended similar region near without much diﬀerence variance. similarly trained randomly initialized mlps mnist observed similar phenomenon resulting mlps similar distributions orders. standard deviation averages learned orders mean single layer able classify test samples pentomino error rate. best result reported pentomino dataset without using kind prior information task forest covertype three layers trained. able classify test samples error. improvement large compared previous state-of-the-art rate achieved manifold tangent classiﬁer four hidden layers logistic sigmoid units result obtained comparable obtained maxout units. experiments optimized hyperparameters initial learning rate scheduling minimize validation error using random search generally eﬃcient grid search number hyperparameters tiny. trained stochastic gradient descent. experiments paper done using pylearn library tried polyphonic music prediction tasks three music datasets; nottingam musedata dot-rnns trained deep transition units tanh units deep output function maxout intermediate layer coarsely optimized size models initial leaning rate well schedule maximize performance validation sets. also chose whether threshold norm gradient based validation performance models trained dropout shown table able achieve state-of-the-art results three datasets. results much better achieved dot-rnns using logistic sigmoid units deep transition deep output suggests superiority proposed units conventional saturating activation functions. suggests proposed units well suited feedforward neural networks also recurrent neural networks. however acknowledge investigation applying units needed future draw concrete conclusion beneﬁts units recurrent neural networks. paper proposed novel nonlinear activation function based generalization widely used pooling operators. proposed nonlinear activation function computes norm several projections lower layer. max- averageroot-of-mean-squared pooling operators special table negative log-probability test sets computed trained dot-rnns. results achieved using dot-rnns logistic sigmoid units reported previous best results achieved using conventional rnns obtained cases proposed activation function naturally recently proposed maxout unit closely related assumption non-negative input signals. important diﬀerence unit conventional pooling operators order unit learned rather pre-deﬁned. claimed estimation orders important optimal model units various orders. analysis shown unit deﬁnes non-euclidean subspace whose metric deﬁned norm. projected back input space unit deﬁnes ellipsoidal boundary. conjectured showed small scale experiment combination curved boundaries eﬃciently model separating curves data non-stationary curvature. claims empirically veriﬁed training deep feedforward neural networks deep recurrent neural networks. tested feedforward neural network four benchmark datasets; mnist toronto face database pentomino forest covertype tested recurrent neural networks task polyphonic music prediction. experiments revealed distribution estimated orders units indeed depends highly dataset away dirac delta distribution. additionally conjecture deep neural networks units able achieve competitive generalization performance empirically conﬁrmed. would like thank developers pylearn theano would also like thank cifar canada research chairs funding compute canada calcul qu´ebec providing computational resources.", "year": 2013}