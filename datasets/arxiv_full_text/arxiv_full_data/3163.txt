{"title": "A Linear Approximation to the chi^2 Kernel with Geometric Convergence", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We propose a new analytical approximation to the $\\chi^2$ kernel that converges geometrically. The analytical approximation is derived with elementary methods and adapts to the input distribution for optimal convergence rate. Experiments show the new approximation leads to improved performance in image classification and semantic segmentation tasks using a random Fourier feature approximation of the $\\exp-\\chi^2$ kernel. Besides, out-of-core principal component analysis (PCA) methods are introduced to reduce the dimensionality of the approximation and achieve better performance at the expense of only an additional constant factor to the time complexity. Moreover, when PCA is performed jointly on the training and unlabeled testing data, further performance improvements can be obtained. Experiments conducted on the PASCAL VOC 2010 segmentation and the ImageNet ILSVRC 2010 datasets show statistically significant improvements over alternative approximation methods.", "text": "propose analytical approximation kernel converges geometrically. analytical approximation derived elementary methods adapts input distribution optimal convergence rate. experiments show approximation leads improved performance image classiﬁcation semantic segmentation tasks using random fourier feature approximation kernel. besides out-of-core principal component analysis methods introduced reduce dimensionality approximation achieve better performance expense additional constant factor time complexity. moreover performed jointly training unlabeled testing data performance improvements obtained. experiments conducted pascal segmentation imagenet ilsvrc datasets show statistically signiﬁcant improvements alternative approximation methods. fuxin school interactive computing georgia institute technology atlanta lebanon department computational science engineering georgia institute technology atlanta cristian sminchisescu center mathematical sciences lund university lund sweden. histograms important tools constructing visual object descriptors. many visual recognition approaches utilize similarity comparisons histogram descriptors extracted training testing images. widely used approaches k-nearest neighbors support vector machines compare testing descriptor multiple training descriptors make predictions weighted comparison scores. important metric compare histograms exponential-χ kernel derived classic pearson test utilized many state-of-the-art object recognition studies excellent performance. however current data training sets often contains millions billions examples. training testing hundreds thousands comparisons using nonlinear metric often time-consuming. main approaches approximate exp−χ facilitate fast linear time training testing. approach devise transformation function represented inner product vectors. transformation random fourier features methodology used approximate gaussian kernel. full exp−χ kernel approximated inner products vector transformations different approach nystr¨om method directly takes subset training examples apply comparison metric example subset output feature vector paper pursue research line. interested potential representing complicated functions nystr¨om approach conﬁned summations kernel comparisons hard approximate functions type. besides provides ﬁxed basis regardless input data could valuable online settings large training available sampling. however able outperform nystr¨om especially image data exp−χ approximation. believe partial reason suboptimal previous performance exp−χ kernel inaccuracy approximation metric. signiﬁcant contribution paper analytic series approximate kernel. series derived using elementary techniques enjoys geometric convergence rate. therefore orders magnitudes better terms approximation error previously proposed approaches experiments show better approximation quality directly translate better classiﬁcation accuracy using conjunction method approximate exp−χ kernel. also developed another analytical approximation using techniques chebyshev polynomials. however approximation slower linear convergence rate fourier transform non-differentiable function. chebyshev approximation derivations also listed paper record. another research question pursue whether also improve empirical performance applying generated features. applying theoretical convergence rate longer conﬁned monte carlo rate number dimensions used better raising least level nystr¨om approach. question whether applying would translate comparable empirical performance. question exploit out-of-core versions little computational overhead approximation especially combined least squares quadratic losses e.g. group lasso. allows reduce number dimensions required classiﬁcation relaxes memory constraints multiple kernels approximated also explore unlabeled data order better estimate covariance matrix pca. turns improve performance better selecting effective frequency components. paper organized follows section summarizes related work. section describes kernel elaborate connection exp−χ kernel test. section present analytical approximation geometric convergence rate. section describes semantic segmentation problems size shape object interest. recent secondorder pooling scheme proposes alternative shown successful results semantic segmentation problem. pooling schemes orthogonal feature approximation problem could potentially used conjunction. kernel relationship test paper denote element-wise throughout products vectors identity matrix column vector zeros column vector ones. denotes element-wise division kernel derived pearson’s test. original pearson test testing whether empirical histogram estimate matches probability distribution. given histogram estimate test statistic virtue harmonic mean approach lies removing singular points kernel value original test goes inﬁnity using harmonic mean approach function well-deﬁned cases. degree freedom distribution regularized gamma function. pvalue minus cumulative distribution function test statistic. p-value small means observed statistic unlikely happen hypothesized distribution. usual criterion decide disagrees distribution speciﬁed case test special case exp. constructed exp−χ kernel used svm-based image classiﬁcation. hypothesized exponential mercer kernel real proof available appendix kernel exp−χ used number visual classiﬁcation object detection systems shown best performances among histogram kernels proposes extension kernel normalizes cross different bins. metrics histogram comparison include histogram intersection efﬁcient speed-up testing also proposed hellinger kernel earth mover distance jenson-shannon. summary comparisons. random fourier features proposed translation-invariant kernels. generalizes exp−χ kernel aforementioned two-steps approach. several studies linear kernel approximations also used ideas nystr¨om method sub-samples training operate reduced kernel matrix. asymptotic convergence rate long known slow recent papers proved actually faster monte carlo rate speed-ups kernel methods based low-rank approximations kernel matrix proposed topic recent interest methods coding image features goal achieve good performance using linear learners following feature embedding hierarchical coding schemes based deep structures also proposed sparse dense coding schemes proven successful supervector coding fisher kernels best performers imagenet large-scale image classiﬁcation challenge dictionaries inﬂuential coding schemes usually extremely large fisher kernel supervector coding usually require dimensions training dictionary often time-consuming. nystr¨om require training hence interesting alternatives methods. crucial component many coding algorithms max-pooling approach uses maximum coded descriptors spatial range features. since case informative small patch could descriptor whole image desirable image classiﬁcation undesirable object detection kernel parameter. note although kernel used many papers enjoys excellent results found elaboration analogy p-value test literature. since p-value relevant metric comparing distributions exp−χ kernel considered intuitively better function similarity metric comparing histogram distributions. empirically tested kernels different degrees freedom found exp−χ works similarly outperforming others degrees freedom. excellent convergence rate histogram approximation full domain note convergence rate dominated case convergence rate slow close examples even cases geometric convergence slow. example single choice achieves good convergence rate entire input domain solution utilize multiple different parameters cover different regions combining parameter choice input distribution data achieve optimal convergence rate entire domain input. input distribution estimated particular dataset. choice reduces error mode input distribution empirically tested superior greedy schemes minimizing mean error step. practice estimated using histogram estimate logarithmically spaced bins chosen centers. algorithm implementation shown algorithm note kernel form coincides harmonic mean vectors. therefore approach could also linear approximation harmonic mean vectors. however currently know applications that. analytical approximation kernel geometric convergence following show analytical approximation kernel referred direct approximation later paper. start onedimensional case. kernel dimension form refer approximation chebyshev approximation draws ideas chebyshev polynomials clenshaw-curtis quadrature central idea clenshawcurtis quadrature change variable arccos order convert aperiodic integral periodic making possible apply fourier techniques. variable substitution arctan serves similar purpose. technique applied principle kernels represents dimension example denote vector constructed concatenating construct matrix entry sampled normal distribution construct vector sampled main advantage using reduce memory footprint. known performance improves random dimensions used. however speed learning algorithms usually deteriorates quickly data cannot load memory would case multiple kernels concatenated e.g. kernels dimensions kernel learning phase following needs operate dimensional feature vector. using eigenvectors also approaches could provide better asymptotic convergence rate monte carlo case means fewer approximation dimensions quality. many techniques like quasi-monte carlo suffer curse dimensionality convergence rate decreases exponentially number input dimensions generally makes unsuitable supposed work high-dimensional problems. another interesting aspect rf-pca bring unexpected ﬂavor semi-supervised learning unlabeled test data improve classiﬁcation accuracy. rf-pca amounts selecting relevant dimensions frequency domain considering training testing data frequencies help discriminate test data likely selected. experiments strategy shown improve performance computation training data. main problem large training feature matrix cannot fully loaded memory. therefore needs performed out-of-core high-performance computing term depicting situation linear time singular value decomposition features rather performing eigenvalue decomposition centered covariance matrix convenient perform regression quadratic loss since hessian needed optimization. applies traditional least squares regression also lasso group lasso composite regularization approaches. case projections need performed explicitly. instead notice section present simple analysis asymptotic convergence rate chebyshev approximation. since exact apply standard results fourier series coefﬁcients state convergence rate depends smoothness function approximated. converges slower series experiments also shown inferior results direct approximation. however convergence different analytical series function lead mathematical equalities therefore still listed chebyshev approximation paper. another rather orthogonal strategy pursue principal component analysis obtaining random features solving regression problems pca. care needs exercised performed extremely large-scale dataset conjunction multiple kernels. similar approaches discussed extensively high-performance computing literature approach data loaded compute hessian. additional complexity necessary matrix decomposition ridge regression used decomposition diagonal therefore needed obtain regression results. case additional constant factor quite small. bottleneck algorithm large-scale problems undoubtedly computation initial hessian involves reading multiple chunks disk. sophisticated case needs performed separately multiple different kernel approximators i.e. feature embedding kernel. time need compute rules tricks simple computation. data needs read twice ﬁrst perform transform chunks order obtain full computation still linear number training examples. cases projection required testing examples. whenever obtained weight vector original input addition constant term. worth noting out-of-core least squares ridge regression scales extremely well number output dimensions used solve one-against-all classiﬁcation problems classes. out-of-core case computed time along hessian algorithm inverse hessian obtained matrix-vector multiplication costing needed obtain solutions without dependency thus total time approach classes scales nicely especially compared algorithms need perform full training procedure class. although loss optimal classiﬁcation large-scale problems classes out-of-core ridge regression still used generate fairly good baseline result quickly. experiments conducted challenging datasets pascal imagenet ilsvrc challenging benchmarks reveal subtle performance differences among approximation methods would otherwise difﬁcult observe simple datasets. conduct experiments medium-scale pascal data order compare exact kernel methods. dataset exclusively train datasets images around objects each. classiﬁcation results also shown imagenet dataset demonstrate efﬁciency kernel approximations. experiments conducted using intel xeon .ghz cores memory. algorithm parallelized using openmp take advantage cores. comparing approximations test different approximations consider small sample pascal segmentation dataset. training image segments best match ground truth segment terms overlap train plus ground truth segments. best-matching segments used test. creates medium-scale problem training test segments. approximations tested experiments chebyshev direct. reference also report classiﬁcation results kernel without exponentiating well skewed kernel proposed chi-skewed. monte carlo approximation different random seeds lead quite signiﬁcant performance variations. therefore experiments averaged trials different random seeds. within trial random seeds used methods. pca-chebyshev initial sampling done using three times ﬁnal approximating dimensions performed reduce dimensionality level methods. test classiﬁcation performance kernels different types features sift words feature dimensions histogram gradient feature dimensions. classiﬁcation done linear using libsvm library parameter libsvm validated kernel approximated expχ period parameter optimal speciﬁed kernel dimensions used approximate distance fig. effect classiﬁcation accuracy dimensional rf-approximated exp−χ kernel using different approximations various number dimensions approximate function. direct approximation works already well terms original input dimension approximation schemes need terms. results multiple kernels pascal segmentation challenge section consider semantic segmentation task pascal need recognize objects images generate pixelwise segmentations objects. ground truth segments objects paired category labels available training. recent state-of-the-art approach trains scoring function class many putative ﬁgureground segmentation hypotheses obtained using cpmc creates large-scale learning task even original image database moderate size segments image training images creates learning problem around training examples. input scale still tractable exact kernel approaches directly compare them. experiments conducted using multiple kernel approximations exp-χ kernels. different image descriptors include hogs different scales sift foreground table classiﬁcation accuracy exp-χ kernel function estimated different approximations bow-sift descriptor. results chi-skewed kernels also shown reference. table classiﬁcation accuracy exp-χ kernel function approximated different approximations descriptor. results chi-skewed kernels also shown reference. background color sift foreground background segmentation measure used compare different approaches. measure average pixel-wise average precision classes plus background. avoid distraction fair comparison post-processing step performed result obtained reporting segment highest score image. method used nonlinear estimation one-against-all support vector regression method linear estimation one-against-all ridge regression. latter used since fast solutions linear problems available out-of-core dense features. avoided stochastic gradient methods since difﬁcult tune convergence effects potentially bias results. average trials different random seeds. table segmentation performance measured pixel segment output image averaged random trials. upper part table shows results bow-sift features extracted foreground background. lower part shows results based combining different descriptors. imagenet ilsvrc challenging classiﬁcation dataset million images separated different categories. show experiments performed using original feature provided authors. goal primarily compare among different approximations hence generate multiple image descriptors spatial pyramid compatible framework could improve results signiﬁcantly. since regression used resulting scores well-calibrated across categories. therefore conclusion goes here. conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here.the conclusion goes here. conclusion goes here.the conclusion goes table performance linear classiﬁers well non-linear approximation methods imagenet ilsvrc data. notice signiﬁcant boost provided non-linear approximations first concern symbolic integration software com√ pute sech) antisymmetric symmetric even antisymmetric argument gets therefore need solve coefﬁcients even therefore start integration acknowledgments authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank...the authors would like thank... zhang lazebnik schmid local features kernels classiﬁcation texture object categories comprehensive study international journal computer vision vol. marszalek laptev schmid actions context ieee conference computer vision pattern recognition gonfaus boix weijer bagdanov serrat gonzlez harmony potentials joint classiﬁcation segmentation ieee conference computer vision pattern recognition vedaldi zisserman efﬁcient additive kernels explicit feature maps ieee transaction pattern analysis machine intelligence vol. lebanon sminchisescu chebyshev approximations histogram chi-square kernel ieee conference computer vision pattern recognition pele werman quadratic-chi histogram distance family european conference computer vision maji berg malik efﬁcient classiﬁcation additive kernel svms ieee transaction pattern analysis machine intelligence vol. yang y.-f. mahdavi z.-h. zhou nystrom method random fourier features theoretical empirical comparison advances neural information processing systems grosse ranganath convolutional deep belief networks scalable unsupervised learning hierarchical representations proceedings international conference machine learning carreira sminchisescu cpmc automatic object segmentation using constrained parametric min-cuts ieee transactions pattern analysis machine intelligence july john biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here. text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography jane biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.biography text here.", "year": 2012}