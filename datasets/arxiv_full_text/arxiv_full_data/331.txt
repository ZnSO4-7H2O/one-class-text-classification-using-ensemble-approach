{"title": "Deep Reconstruction-Classification Networks for Unsupervised Domain  Adaptation", "tag": ["cs.CV", "cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper, we propose a novel unsupervised domain adaptation algorithm based on deep learning for visual object recognition. Specifically, we design a new model called Deep Reconstruction-Classification Network (DRCN), which jointly learns a shared encoding representation for two tasks: i) supervised classification of labeled source data, and ii) unsupervised reconstruction of unlabeled target data.In this way, the learnt representation not only preserves discriminability, but also encodes useful information from the target domain. Our new DRCN model can be optimized by using backpropagation similarly as the standard neural networks.  We evaluate the performance of DRCN on a series of cross-domain object recognition tasks, where DRCN provides a considerable improvement (up to ~8% in accuracy) over the prior state-of-the-art algorithms. Interestingly, we also observe that the reconstruction pipeline of DRCN transforms images from the source domain into images whose appearance resembles the target dataset. This suggests that DRCN's performance is due to constructing a single composite representation that encodes information about both the structure of target images and the classification of source images. Finally, we provide a formal analysis to justify the algorithm's objective in domain adaptation context.", "text": "abstract. paper propose novel unsupervised domain adaptation algorithm based deep learning visual object recognition. speciﬁcally design model called deep reconstructionclassiﬁcation network jointly learns shared encoding representation tasks supervised classiﬁcation labeled source data unsupervised reconstruction unlabeled target data. learnt representation preserves discriminability also encodes useful information target domain. drcn model optimized using backpropagation similarly standard neural networks. evaluate performance drcn series cross-domain object recognition tasks drcn provides considerable improveinterestingly also observe reconstruction pipeline drcn transforms images source domain images whose appearance resembles target dataset. suggests drcn’s performance constructing single composite representation encodes information structure target images classiﬁcation source images. finally provide formal analysis justify algorithm’s objective domain adaptation context. important task visual object recognition design algorithms robust dataset bias dataset bias arises labeled training instances available source domain test instances sampled related diﬀerent target domain. example consider person identiﬁcation application unmanned aerial vehicles essential variety tasks surveillance people search remote monitoring critical tasks identify people bird’s-eye view; however collecting labeled data viewpoint challenging. desirable trained already available on-the-ground labeled images e.g. people photographs social media successfully applied actual view traditional supervised learning algorithms typically perform poorly setting since assume training test data drawn domain. domain adaptation attempts deal dataset bias using unlabeled data target domain task manual labeling target data reduced. unlabeled target data provides auxiliary training information help algorithms generalize better target domain using source data only. successful domain adaptation algorithms large practical value since acquiring huge amount labels target domain often expensive impossible. although domain adaptation gained increasing attention object recognition recent overview problem remains essentially unsolved since model accuracy reach level satisfactory real-world applications. another issue many existing algorithms require optimization procedures scale well size datasets increases earlier algorithms typically designed relatively small datasets e.g. oﬃce dataset consider solution based learning representations features data. ideally learned feature model label distribution well reduce discrepancy source target domains. hypothesize possible approximate feature learning source label distribution learning target data distribution. spirit multi-task learning learning auxiliary tasks help main task learned better goal paper develop accurate scalable multi-task feature learning algorithm context domain adaptation. contribution achieve goal stated above propose deep learning model unsupervised domain adaptation. deep learning algorithms highly scalable since linear time handle streaming data parallelized gpus. indeed deep learning come dominate object recognition recent years propose deep reconstruction-classiﬁcation network convolutional network jointly learns tasks supervised source label prediction unsupervised target data reconstruction. encoding parameters drcn shared across tasks decoding parameters separated. learned label prediction function perform well classifying images target domain data reconstruction thus viewed auxiliary task support adaptation label prediction. learning drcn alternates unsupervised supervised training diﬀerent standard pretraining-ﬁnetuning strategy furthermore perform visual analysis reconstructing source images learned reconstruction function. found reconstructed outputs resemble appearances target images suggesting encoding representations successfully adapted. finally present probabilistic analysis show relationship drcn’s learning objective semisupervised learning framework also soundness considering data target domain data reconstruction training. domain adaptation large ﬁeld research related work several names class imbalance covariate shift sample selection bias considered special case transfer learning. earlier work domain adaptation focused text document analysis recent years gained attention computer vision community mainly object recognition application references therein. domain adaptation problem often referred dataset bias computer vision paper concerned unsupervised domain adaptation labeled data target domain available range approaches along line research object recognition proposed designed speciﬁcally small datasets oﬃce dataset furthermore usually operated surf-based features extracted pixels. essence unsupervised domain adaptation problem remains open needs powerful solutions useful practical situations. deep learning plays major role advancement domain adaptation. early attempt addressed large-scale sentiment classiﬁcation concatenated features fully connected layers stacked denoising autoencoders found domain-adaptive visual recognition fully connected shallow network pretrained denoising autoencoders shown certain level eﬀectiveness widely known deep convolutional networks natural choice visual recognition tasks achieved signiﬁcant successes recently convnets pretrained large-scale dataset imagenet shown reasonably eﬀective domain adaptation provide signiﬁcantly better performances surf-based features oﬃce dataset earlier approach using convolutional architecture without pretraining imagenet dlid also explored performs better surf-based features. improve domain adaptation performance pretrained convnets ﬁne-tuned particular constraint related minimizing domain discrepancy measure deep domain confusion utilizes maximum mean discrepancy measure additional loss function ﬁne-tuning adapt last fully connected layer. deep adaptation network ﬁne-tunes last fully connected layer also convolutional fully connected layers underneath outperforms ddc. recently deep model proposed extends idea adding criterion guarantee class alignment diﬀerent domains. however limited semi-supervised adaptation setting small number target labels acquired. algorithm proposed refer reversegrad handles domain invariance binary classiﬁcation problem. thus optimizes contradictory objectives minimizing label prediction loss maximizing domain classiﬁcation loss simple gradient reversal strategy. reversegrad eﬀectively applied pretrained randomly initialized deep networks. randomly initialized model also shown perform well crossdomain recognition tasks oﬃce benchmark i.e. large-scale handwritten digit recognition tasks. work paper similar spirit reversegrad necessarily require pretrained deep networks perform well tasks. however proposed method undertakes fundamentally diﬀerent learning algorithm ﬁnding good label classiﬁer simultaneously learning structure target images. section describes proposed deep learning algorithm unsupervised domain adaptation refer deep reconstruction-classiﬁcation networks ﬁrst brieﬂy discuss unsupervised domain adaptation problem. present drcn architecture learning algorithm useful aspects. deﬁne domain probability distribution input space output space. denote source domain target domain source domain classify well source domain labeled data reconstruct well target domain unlabeled data viewed approximate ideal discriminative representation. model based convolutional architecture pipelines shared encoding representation. ﬁrst pipeline standard convolutional network source label prediction second convolutional autoencoder target data reconstruction convolutional architectures natural choice object recognition capture spatial correlation images. model optimized multitask learning jointly learns source label prediction target data reconstruction tasks. encoding shared representation learn commonality tasks provides useful information cross-domain object recognition. figure illustrates architecture drcn. fig. illustration drcn’s architecture. consists pipelines label prediction data reconstruction pipelines. shared parameters pipelines indicated color. describe drcn formally. label prediction pipeline data reconfeature mapping genc decoder gdec feature labeling glab m-class classiﬁcation problems output glab i.e. softmax output. given input decompose supervised unsupervised model. θenc shared parameters feature mapping genc. note θenc θdec θlab encode parameters multiple layers. goal seek single feature mapping genc model supports using stochastic gradient descent implementation used rmsprop variant gradient normalization current gradient divided moving average previous root mean squared minimization eﬀective reduce overﬁtting. note dropout regularization applied fully-connected/dense layers only figure stopping criterion algorithm determined monitoring average reconstruction loss unsupervised model training process stopped average reconstruction loss stabilizes. training completed optimal parameters ˆθenc ˆθlab used form classiﬁcation model expected perform well data augmentation denoising well-known strategies improve drcn’s performance data augmentation denoising. data augmentation generates additional training data supervised training respect plausible transformations original data improves generalization e.g. denoising involves reconstructing clean inputs given noisy counterparts. used improve feature invariance denoising autoencoders generalization feature invariance properties needed improve domain adaptation. since drcn classiﬁcation reconstruction aspects naturally apply tricks simultaneously training stage. transformations data augmentation either zero-masked noise gaussian noise used denoising strategy. work combine fore-mentioned types noise denoising geometric transformations data augmentation. section reports evaluation results drcn. divided parts. ﬁrst part focuses evaluation large-scale datasets popular deep learning methods second part summarizes results oﬃce dataset ﬁrst experiments investigates empirical performance drcn widely used benchmarks mnist usps street view house numbers cifar corresponding references detailed conﬁgurations. task perform cross-domain recognition taking training dataset source domain test another dataset target domain. evaluate algorithm’s recognition accuracy three cross-domain pairs mnist usps svhn mnist cifar stl. mnist usps contains grayscale handwritten digit images classes. preprocessed follows. usps images rescaled pixels normalized values. pair crossdomain recognition tasks performed svhn mnist pair mnist images rescaled architecture learning setup drcn architecture used experiments adopted label prediction pipeline three convolutional layers ﬁlters ﬁlters ﬁlters respectively max-pooling layers size ﬁrst second convolutional layers three fully-connected layers output layer. number neurons treated tunable hyper-parameter range chosen according best performance validation set. shared encoder genc thus conﬁguration conv-pool-convpool-conv-fc-fc. furthermore conﬁguration decoder gdec inverse genc. note unpooling operation gdec performs upsampling-by-duplication inserting pooled values appropriate locations feature maps remaining elements pooled values. employ relu activations hidden layers linear activations output layer reconstruction pipeline. updates classiﬁcation reconstruction tasks computed rmsprop learning rate moving average decay control penalty selected according accuracy source validation data typically optimal value range benchmark algorithms compare drcn following methods. convnetsrc supervised convolutional network trained labeled source domain only network conﬁguration drcn’s label prediction pipeline scae convnet preceded layer-wise pretraining stacked convolutional autoencoders unlabeled data scaet similar scae unlabeled data target domain used pretraining sdash deep network three fully connected layers successful domain adaptation model sentiment classiﬁcation subspace alignment reversegrad recently published domain adaptation model based deep convolutional networks provides state-of-the-art performance. deep learning based models architecture drcn label predictor. reversegrad also evaluated original architecture devised chose whichever performed better original architecture architecture. finally applied data augmentation models similarly drcn. ground-truth model also evaluated convolutional network trained tested images target domain measure diﬀerence cross-domain performance ideal performance. accuracy algorithms independent runs. drcn perart reversegrad. notably task drcn outperforms reversegrad accuracy gap. drcn also provides considerable improvement reversegrad reverse task failed case case performance drcn almost drcn also convincingly outperforms greedy-layer pretraining-based algorithms indicates eﬀectiveness simultaneous reconstruction-classiﬁcation training strategy standard pretraining-ﬁnetuning context domain adaptation. independent runs. bold underline indicate best second best domain adaptation performance. convnettgt denotes ground-truth model training testing target domain only. comparison diﬀerent drcn ﬂavors recall drcn uses unlabeled target images unsupervised reconstruction training. verify importance strategy compare diﬀerent ﬂavors drcn drcns drcnst. algorithms conceptually diﬀerent utilizing unlabeled images unsupervised training. drcns uses unlabeled source images whereas drcnst combines unlabeled source target images. experimental results table conﬁrm drcn always performs better drcns drcnst. drcnst occasionally outperforms reversegrad overall performance compete drcn. case drcns drcnst ﬂavors closely match drcn suggests unlabeled source data recondata reconstruction useful insight found reconstructing source images reconstruction pipeline drcn. speciﬁcally observe visual appearance display original source main ﬁnding observation depicted figure reconstructed images produced drcn given svhn images source inputs. found reconstructed svhn images resemble mnist-like digit appearances white stroke black background figure remarkably drcn still produce correct reconstructions noisy svhn images. example svhn digits displayed figure clearly reconstructed drcn fourth figure drcn tends pick digit middle ignore remaining digits. explain superior cross-domain recognition performance drcn task. however cross-reconstruction appearance happen reverse also conduct diagnostic reconstruction algorithms reconstruction pipeline. figure depicts reconstructions svhn images produced convae trained mnist images only. appear digits suggesting convae recognizes svhn images noise. figure shows reconstructed svhn images produced drcnst. look almost identical source images shown figure surprising since source images included reconstruction training. mnist images encoding parameters initialized convnetsrc updated training. refer model convae+convnetsrc. reconstructed images visualized figure although resemble style mnist images drcn’s case source images correctly reconstructed. summarize results diagnostic data reconstruction correlate cross-domain recognition performance. visualization cross-domain cases found supplemental materials. second experiment evaluated drcn standard domain adaptation benchmark visual object recognition office consists three diﬀerent domains amazon dslr webcam office labeled images total distributed across object categories. number images thus relatively small compared previously used datasets. applied drcn algorithm ﬁnetune alexnet done diﬀerent methods previous work ﬁne-tuning performed fully connected layers alexnet last convolutional layer conv. speciﬁcally label prediction pipeline drcn contains conv-conv-f c-label data reconstruction pipeline convconv-f c-conv-conv denotes inverse layer) thus reconstruct original input pixels. learning rate selected following strategy devised cross-validating base learning rate multiplicative step-size followed standard unsupervised domain adaptation training protocol used previous work using labeled source data unlabeled target data. table summarizes performance accuracy drcn based protocol comparison state-of-the-art algorithms. found drcn competitive reversegrad performance either best second best except case. particular drcn performs best convincing situations target domain relatively many data i.e. amazon target dataset. section provides ﬁrst step towards formal analysis drcn algorithm. demonstrate optimizing drcn relates solving semi-supervised learning problem target domain according framework proposed analysis suggests unsupervised training using unlabeled target data suﬃcient. adding unlabeled source data might improve domain adaptation. model learned supervised convolutional network second term represents model learned unsupervised convolutional autoencoder. note discriminative model observes labeled data source distribution objectives recall semi-supervised learning problem formulated suppose labeled unlabeled samples taken target domain probabilities respectively. theorem maximum theorem holds satisﬁes following assumptions consistency model contains true distribution consistent; smoothness measurability given target data ﬁrst term. notice would approximately equal ratio constant fact becomes objective drcnst. although constant ratio assumption strong hold practice comparing suggests reasonable approximation converge hence adding unlabeled source data result constant. implies optimization procedure equivalent explain uselessness unlabeled source data context domain adaptation. note latter analysis necessarily imply incorporating unlabeled source data degrades performance. fact drcnst performs worse drcn could e.g. model capacity depends choice architecture. proposed deep reconstruction-classiﬁcation network novel model unsupervised domain adaptation object recognition. model performs multitask learning i.e. alternately learning label prediction data reconstruction using shared encoding representation. shown drcn provides considerable improvement crossdomain recognition tasks state-of-the-art model. also performs better deep models trained using standard pretraining-ﬁnetuning approach. useful insight eﬀectiveness learned drcn obtained data reconstruction. appearance drcn’s reconstructed source images resemble target images indicates drcn learns domain correspondence. also provided theoretical analysis relating drcn algorithm semi-supervised learning. analysis used support strategy involving target unlabeled data learning reconstruction task.", "year": 2016}