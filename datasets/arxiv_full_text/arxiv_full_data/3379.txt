{"title": "Information-theoretical label embeddings for large-scale image  classification", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "We present a method for training multi-label, massively multi-class image classification models, that is faster and more accurate than supervision via a sigmoid cross-entropy loss (logistic regression). Our method consists in embedding high-dimensional sparse labels onto a lower-dimensional dense sphere of unit-normed vectors, and treating the classification problem as a cosine proximity regression problem on this sphere. We test our method on a dataset of 300 million high-resolution images with 17,000 labels, where it yields considerably faster convergence, as well as a 7% higher mean average precision compared to logistic regression.", "text": "present method training multi-label massively multi-class image classiﬁcation models faster accurate supervision sigmoid cross-entropy loss method consists embedding high-dimensional sparse labels onto lower-dimensional dense sphere unit-normed vectors treating classiﬁcation problem cosine proximity regression problem sphere. test method dataset million high-resolution images labels yields considerably faster convergence well higher mean average precision compared logistic regression. consider problem predicting classes image belongs number classes large image typically belongs multiple classes properly identiﬁed multi-label massively multi-class classiﬁcation. classiﬁcation problems best practice deep convolutional neural network ones described culminating logistic regression layer sigmoid cross-entropy loss target labels encoded high-dimensional sparse binary vectors. logistic regression implies important overlooked assumption made label space classes considered statistically independent class treated independent dimension label space. generally case practice mirroring statistical dependencies found real world label spaces often well-deﬁned internal structure labels likely co-occur labels. instance beach frequently co-occurring labels crane manta rarely co-occurring. sigmoid cross-entropy loss sparse binary targets allow leverage observations structure label space. therefore opportunity exploit internal structure label space gains training speed precision recall. simple achieve project labels onto lower-dimensional manifold embedding space– distance function embedded labels would capture useful statistical dependencies. appropriate loss function allow parametric model trained stochastic gradient descent beneﬁt structure manifold training inference. several approaches problem learning label embeddings proposed past. generally fall three broad categories embeddings capturing label co-occurrences visual similarity-based embeddings semantic similarity-based embeddings leveraging external sources data. approaches aimed label co-occurrence mapping include compressed sensing error correcting output codes framework well randomized linear algebra methods outlined mineiro approaches leverage image similarity together external sources semantic knowledge word embeddings. include approach focused purely semantic label embeddings. additionally information-theoretical word embedding techniques strongly related approach long natural language processing community. learning word embeddings factorizing pairwise matrix correlation-like metric initially proposed latent semantic analysis explored instance levy goldberg showed wordvec algorithm equivalent factorizing shifted pointwise mutual information matrix speciﬁc paper drew inspiration proposed method. unlike approaches aiming capturing either semantic visual similarity interested purely concept visual co-occurrence labels separate notion. frequently co-occurring items visually dissimilar semantically dissimilar approach therefore conceptually related although method diﬀers signiﬁcantly algorithmically application scope deﬁne basic notations. consider instances classes size image associate non-empty subset classes belongs encode binary vector non-zero values. class associate binary variable presence indicator class sets. vectors thus observations binary variables dataset diagonal matrix eigenvalues diagonal ranked construction every projection kronecker vector onto latent space dimension independent factor variation pairwise mutual informations. dimensions ranked much variance explain matrix restricted ﬁrst columns minimal loss variance explanation. restriction ﬁrst columns note embedding matrix. representation class latent space. distance choice space cosine distance diﬀerent points computationally eﬃcient. class added class co-occurrence information class known classes available possible give class position embedding space cosine proximity class embedding embeddings known classes approximates measured mutual information classes. then simply taking account class prediction time possible start generating predictions class without modify trained network embedding acts linking function labels hard-coding statistical dependencies labels directly learning process. train network associate certain visual feature embedding face automatically learn associate features labels embedded nearby nose eyes. acts label enriching process label denoising process useful presence unreliable annotations. logistic regression target space continuous loss i.e. small change ground-truth annotations cause large change corresponding loss value computed network. consider following example picture tree might annotated tree branch trunk noise inherent groundtruth gathering process. model predicts equal presence probability three classes adding removing trunk label would cause change cross-entropy loss value. argue might reasonably expect lack continuity detrimental smooth gradient descent process. meanwhile proposed method embedding labels dense continuous space rather discrete sparse space makes target space continuous loss function. small changes ground-truth annotations would result correspondingly small movement annotations embedding thus small change cosine proximity loss. consider previous example embedding three classes tree branch trunk close removing adding three labels signiﬁcantly impact embedding ﬁnal labels thus small impact cosine proximity loss. table relative cosine proximities labels. datasets publicly available large-scale image dataset multi-label massively multi-class classiﬁcation. however google inc. maintains internal dataset approximately million images nicknamed ﬁrst introduced hinton images fairly high-resolution annotated labels approximately classes. labels annotated hand generated variety heuristics based semantic context therefore somewhat noisy. associated dataset smaller less noisy datasets calibrationm dataset million images approximately million annotated hand fastevalk dataset picture high-quality dense label annotations calibrationm ground-truth annotations computing label embeddings data training cosine proximity regression model evaluate model fastevalk. table look label density statistics calibrationm fastevalk. fastevalk evaluation dataset contains annotations approximately common labels. hence although embeddings model technically trained labels results show include labels. additionally worth noting distribution labels dataset embeddings computed somewhat diﬀerent distribution labels evaluation dataset. mean results could improved calibrating label distribution ground-truth used embedding computation. moreover care model performance commonly occurring classes rare classes associate class importance weight computed based occurrence frequency class separate dataset social media images capped minimum maximum value multiply contributions associated labels weight. reason highest achievable experiments higher base model slightly modiﬁed version inception architecture described szegedy implemented tensorflow framework trained rmsprop optimizer modiﬁcations architecture remove dropout layer well auxiliary loss associated tower regularization mechanisms necessary large dataset threat overﬁtting reasonable amount time. networks trained nvidia gpus several weeks. longest-running experiment weeks. models used completely identical conﬁgurations architecture initialization optimization –except last layer associated loss function. ﬁrst compute embedding matrix based ground-truth annotation calibrationm convert ground-truth annotation normalized embedded vectors following method described earlier. train modiﬁed inception predict ground-truth embeddings images using cosine proximity loss function described earlier. inference time network predict embedding vectors test images convert embeddings ranked label predictions taking taking closest labels cosine proximity. baseline model modiﬁed inception model trained logistic regression i.e. ending -way sigmoid layer supervised binary crossentropy loss sparse binary ground-truth vectors network predictions. resources standpoint memory footprint training-time computational cost cosine proximity regression model noticeably smaller since features k-dimensional last layer instead -dimensional last layer. qualitative exploration embedding space shows that expected classes positioned close together correspond commonly co-occurring labels either synonym labels labels naturally co-occurring embedding thus ground-truth completion enrichment mechanism. interestingly embeddings capable performing concept arithmetic somewhat analogous mikolov presented adding embeddings labels produce point close embeddings labels also appear present instance horse equestrianism. cosine proximity regression model converges towards higher classweighted considerably faster reaches ﬁnal performance logistic regression model million iterations i.e. approximately less iterations baseline. believe faster convergence explained ground-truth enrichment mechanism implemented embedding space well continuity target space regard loss however claims cannot rigorously proven point. presented training method leveraging label-space structure multilabel massively multi-class image classiﬁcation problems. method several theoretical advantages logistic regression practice proves perform signiﬁcantly better logistic regression dataset leading signiﬁcantly faster convergence higher mean average precision. results suggest method strong alternative logistic regression cases label space large highly structured.", "year": 2016}