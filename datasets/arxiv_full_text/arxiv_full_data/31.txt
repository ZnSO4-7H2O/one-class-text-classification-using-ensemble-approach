{"title": "Clustering with Deep Learning: Taxonomy and New Methods", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML", "62H30, 62M45, 91C20", "H.3.3; I.2.6; I.5; I.5.3; I.5.4"], "abstract": "Clustering is a fundamental machine learning method. The quality of its results is dependent on the data distribution. For this reason, deep neural networks can be used for learning better representations of the data. In this paper, we propose a systematic taxonomy for clustering with deep learning, in addition to a review of methods from the field. Based on our taxonomy, creating new methods is more straightforward. We also propose a new approach which is built on the taxonomy and surpasses some of the limitations of some previous work. Our experimental evaluation on image datasets shows that the method approaches state-of-the-art clustering quality, and performs better in some cases.", "text": "clustering fundamental machine learning method. quality results dependent data distribution. reason deep neural networks used learning better representations data. paper propose systematic taxonomy clustering deep learning addition review methods ﬁeld. based taxonomy creating methods straightforward. also propose approach built taxonomy surpasses limitations previous work. experimental evaluation image datasets shows method approaches state-of-the-art clustering quality performs better cases. clustering fundamental unsupervised machine learning problems. main goal separate data clusters similar data points. besides applications beneﬁcial multiple fundamental tasks. instance serve automatic data labeling supervised learning pre-processing step data visualization analysis. however performance clustering algorithms dependent type input data different problems datasets could require different similarity measures different separation techniques. result dimensionality reduction representation learning extensively used alongside clustering order input data feature space separation easier respect problem’s context. using deep neural networks possible learn non-linear mappings allowing transform data clustering-friendly representations. past dimensionality reduction clustering treated separately sequentially applied data however recent research shown jointly optimizing problems achieve decent results main contributions formulation taxonomy methods deep learning clustering. taxonomy facilitates overview existing methods creation ones using best properties existing ones modular manner. based taxonomy propose method combines advantageous properties existing methods. autoencoder-based method learning better representations data clustering-friendly state-of-the-art training procedure. training phases ﬁrst standard autoencoder training mean squared error reconstruction loss second based loss function combining reconstruction loss clustering-speciﬁc loss. moreover second phase jointly optimize network model clustering assignments. rest paper organized follows taxonomy clustering deep learning corresponding building blocks described section section several related methods brieﬂy described compared based taxonomy. subsequently section method proposed discussed based building blocks taxonomy. results proposed method shown section followed conclusions section successful methods clustering deep neural networks work following principle representation learning using dnns using representations input speciﬁc clustering method. every method consists following parts several options choose from deep learning methods clustering main branch neural network used transform inputs latent representation used clustering. following neural network architectures previously used purpose convolutional neural network inspired biology precisely organization animal visual cortex. useful applications regular-grid data images locality shift-equivariance/invariance feature extraction desired. deep belief network generative graphical model consisting several layers latent variables. composed several shallow networks restricted boltzmann machines hidden layer sub-network serves visible layer next sub-network. dnns serve clustering mappings better representations. features representations drawn different layers network even several ones. possible separate choice categories non-clustering loss independent clustering algorithm usually enforces desired constraint learned model. following possible options non-clustering loss functions cases network model constrained clustering loss requirements. clustering losses non-clustering loss danger worse representations/results theoretically even collapsing clusters latter rarely occurs practice. autoencoder reconstruction loss autoencoder consists parts encoder decoder. encoder maps input representation latent space training decoder tries reconstruct making sure useful information lost encoding phase. context clustering methods training done decoder part longer used encoder left mapping input latent space applying procedure autoencoders successfully learn useful representations cases output’s dimensionality different input’s random noise injected input additionally also used dimensionality reduction goals generally reconstruction loss distance measure dae) input autoencoder corresponding reconstruction particular formulation using mean squared error variables input autoencoder reconstruction. loss function guarantees learned representation preserves important information initial reconstruction possible. tasks additional information training samples available form targets even perfectly suitable dictate clustering used non-clustering loss encourage meaningful feature extraction. second type functions speciﬁc clustering method clustering-friendliness learned representations therefore functions called clustering loss functions. following options clustering loss functions clustering loss even neural network non-clustering losses features extracts used clustering training neural network serves case changing representation input instance changing dimensionality. transformation could beneﬁcial clustering sometimes using clustering loss usually yields better results k-means loss assures representation k-means-friendly i.e. data points evenly distributed around cluster centers. order obtain distribution neural network trained following loss function embedded data point cluster center boolean variable assigning minimizing loss respect network parameters assures distance data point assigned cluster center small. that applying k-means would result better clustering quality. cluster assignment hardening requires using soft assignments data points clusters. instance student’s t-distribution used kernel measure similarity points centroids. distribution formulated follows embedded data point cluster centroid constant e.g. normalized similarities points centroids considered soft cluster assignments. cluster assignment hardening loss enforces making soft assignment probabilities stricter. letting cluster assignment probability distribution approach auxiliary distribution guarantees constraint. propose following auxiliary distribution squaring original distribution normalizing auxiliary distribution forces assignments stricter probabilities aims improve cluster purity emphasis data points assigned high conﬁdence prevent large clusters distorting hidden feature space formulate divergence probability distributions using kullback–leibler divergence formulated follows minimizing equation probability assigning data point certain cluster uniform across possible clusters important note property always desired. thus case prior known still possible replace uniform distribution known prior one. group sparsity loss inspired spectral clustering block diagonal similarity matrix exploited representation learning group sparsity effective feature selection method. huang hidden units divided groups assumed number clusters. given data point obtained representation form {φg}g thus loss deﬁned follows cluster classiﬁcation loss cluster assignments obtained cluster updates used mock class labels classiﬁcation loss additional network branch order encourage meaningful feature extraction network layers agglomerative clustering loss agglomerative clustering merges clusters maximum afﬁnity step stopping criterion fulﬁlled. neural network loss inspired agglomerative clustering computed several steps. first cluster update step merges several pairs clusters selecting pairs best afﬁnity network training retrospectively even optimizes afﬁnity already merged clusters next cluster update step network training switches retrospectively optimizing afﬁnity newest newly merged cluster pairs. cluster merging retrospective latent space adjustments hand hand. optimizing network parameters loss function would result clustering space suitable clustering. clustering loss non-clustering loss constant specifying weighting functions. additional hyperparameter network training. also changed training following schedule. following methods assign schedule values pre-training ﬁne-tuning first i.e. network trained using nonclustering loss only. subsequently i.e. non-clustering network branches removed clustering loss used train obtained network. constraint forced reconstruction loss could lost training network long enough clustering only. cases losing constraints lead worse results clustering methods broadly categorized hierarchical partitional approaches hierarchical clustering combines methods build hierarchy clusters data points. hand partitional clustering groups methods create cluster centers metric relations assign data points cluster similar center. context deep learning clustering dominant methods categories used. agglomerative clustering hierarchical clustering method used deep learning algorithm brieﬂy discussed jointly updated network model cluster assignments formulated probabilities therefore continuous values case included parameters network optimized back-propagation. alternatingly updated network model clustering assignments strict updated different step network model updated. case several scenarios possible dependent main factors number iterations number iterations chosen clustering algorithm executed every cluster update step. instance cluster update step algorithm runs ﬁxed percentage points change assignments consecutive iterations. training converges network learned mapping input space clustering-friendly space respect dataset trained words training performed digit images pixel size network able images space clustering easier. mapping makes sense clustering algorithm desired dataset. however majority presented methods performs clustering training obtain clustering results last training iteration. therefore following reasons re-running clustering training done clustering similar dataset general trivial case reuse learned features representation mapping another dataset similar used different data. obtaining better results certain circumstances possible results clustering training better ones obtained learning procedure. instance yang behavior reported. possible reason happen cluster update step training doesn’t till meaning older steps used older representations might worse. therefore cluster merging steps performed less optimal feature representation clustering training performed better. clustering extensively studied researched. application deep neural networks gained additional interest last years success supervised deep learning. however cases clustering handled unsupervised fashion making application deep learning less trivial requiring modeling effort theoretical analysis. therefore several approaches presented last years trying representational power dnns preprocessing clustering inputs. approaches used different network architectures structures loss functions training methods order achieve results improve clustering quality. following interesting methods previously introduced. training neural network method ﬁrst pretrains model using standard input reconstruction loss function. secondly network’s model ﬁne-tuned using cluster assignment hardening loss clustering centers updated. clusters iteratively reﬁned learning high conﬁdence assignments help auxiliary target distribution. consequence method showed decent results later used reference compare methods performances. another autoencoder-based method uses k-means clustering similar ﬁrst phase network pretrained using autoencoder reconstruction loss. however second phase different. contrast network jointly trained using mathematical combination autoencoder reconstruction loss k-means clustering loss function. thus fact strict cluster assignments used training method required alternation process network training cluster updates. method performed well even better results mnist dataset. respect presented taxonomy approach almost identical except using convolutional autoencoders. namely also uses k-means clustering training method pretraining autoencoder reconstruction loss tuning using cluster assignment hardening loss. additionally advantages disadvantages shared methods. thus fact uses convolutional layers outperformed dec’s clustering quality image datasets obviously expected. jule uses convolutional neural network representation learning. clustering hierarchical approach used speciﬁcally agglomerative clustering method employed. concerning training method uses clustering loss speciﬁcally agglomerative loss. additionally method period hyper-parameter training behavior altered. namely hyper-parameter speciﬁes number model updates applied clustering algorithm executes clustering iteration instance learning sessions followed fusing clusters one. experiments method showed great results example mnist performed better methods. however disadvantages lack nonclustering loss particularly pronounced least theory ccnn uses clustering achieve joint clustering representation learning. internal layers ccnn forms feature space. time ccnn’s softmax layer predicts cluster labels. initially features random images dataset used initialize cluster centers. k-means performed features extracted input dataset corresponding cluster labels. based assigned labels labels predicted softmax layer ccnn parameters updated using clustering classiﬁcation loss discussed section extracted features minibatch used update corresponding cluster centroids. besides described methods multiple attempts made ﬁeld clustering deep learning. interesting work saito standard autoencoder used without additional clustering loss functions. however outputs several layers network beside last used ﬁnal feature representation. layer concatenation superior results even compared methods included clustering-speciﬁc loss function. moreover huang joint training performed combination autoencoder reconstruction loss locality-preserving loss group sparsity loss. another work dizaji similar except adding additional term clustering loss balanced assignments loss. addition alleviate danger obtaining degenerate solutions introduce need alternating network training clustering updates. addition mentioned methods multiple others exist rather directly using neural network extract high-level features samples inﬁnite ensemble clustering uses neural networks generate inﬁnite ensemble partitions fuse consensus partition obtain ﬁnal clustering. identifying taxonomy clustering deep learning comparing methods ﬁeld based creating improved methods became straightforward. instance looking table could notice combinations method properties could lead methods. cases combinations could also surpass limitations previous approaches lead better results. procedure followed work. namely picked interesting combination taxonomy features came method method uses convolutional architecture since target clustering datasets image datasets. additionally network training phases. ﬁrst pretraining autoencoder reconstruction loss. second phase autoencoder loss cluster assignment hardening jointly optimized. second phase different cluster figure proposed method. fully convolutional autoencoder reconstruction cluster hardening loss discussed section respectively results cluster friendly feature space without risk collapsing degenerate solutions assignment hardening loss level. omitting reconstruction loss phase network training could lead worse representations solutions therefore combining reconstruction loss cluster assignment hardening loss makes sense. phase also different joint training property uses k-means loss. k-means loss forces alternate joint training clustering updates hard cluster assignments. using cluster assignment hardening loss alternation procedure longer needed approach since loss uses soft assignments jointly updated network updates. training phases done network able input clustering-friendly space. based assumption output network input k-means method produces ﬁnal clustering results. experimental setup training network involved trying several architectures network sizes. addition required tuning learning hyper-parameters learning rate initialization parameters mini-batch size others. particular learning rate momentum addition batch normalization regularization. presented results best obtained ones experimentation phase. performance table shows clustering performance terms accuracy various clustering approaches. results methods borrowed respective publications. table seen proposed algorithm performs comparable better state approaches. figure show clustering spaces different stages training proposed network true cluster labels shown using different colors. clustering spaces -dimensional dimensional mnist coil respectively. seen visualizations proposed method results much clustering-friendly spaces original image space autoencoder space. figure t-sne visualizations clustering mnist dataset original pixel space autoencoder hidden layer space autoencoder hidden layer space proposed method. figure t-sne visualizations clustering coil dataset original pixel space autoencoder hidden layer space autoencoder hidden layer space proposed method. work present taxonomy clustering deep learning identifying general framework discussing different building blocks possible options. addition summary methods ﬁeld speciﬁc taxonomy presented alongside general comparison many methods. using taxonomy summary previous methods generating methods clearer easier done creating combinations taxonomy’s building blocks. moreover present method ﬁeld based combination. method overcomes limitations several previous ones approaches state-ofthe-art performance performs better cases. lukic vogt d¨urr stadelmann speaker identiﬁcation clustering using convolutional neural networks. international workshop machine learning signal processing pages ieee. premachandran yuille unsupervised learning using generative adversarial training clustering. proceedings international conference learning representations trigeorgis bousmalis zafeiriou schuller deep semi-nmf model learning hidden representations. proceedings international conference machine learning pages vincent larochelle lajoie bengio manzagol p.-a. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research vinh epps bailey information theoretic measures clusterings comparison variants properties normalization correction chance. journal machine learning research", "year": 2018}