{"title": "Stochastic Gradient Descent, Weighted Sampling, and the Randomized  Kaczmarz algorithm", "tag": ["math.NA", "cs.CV", "cs.LG", "math.OC", "stat.ML", "65B99, 52A99, 60G99, 62L20"], "abstract": "We obtain an improved finite-sample guarantee on the linear convergence of stochastic gradient descent for smooth and strongly convex objectives, improving from a quadratic dependence on the conditioning $(L/\\mu)^2$ (where $L$ is a bound on the smoothness and $\\mu$ on the strong convexity) to a linear dependence on $L/\\mu$. Furthermore, we show how reweighting the sampling distribution (i.e. importance sampling) is necessary in order to further improve convergence, and obtain a linear dependence in the average smoothness, dominating previous results. We also discuss importance sampling for SGD more broadly and show how it can improve convergence also in other scenarios. Our results are based on a connection we make between SGD and the randomized Kaczmarz algorithm, which allows us to transfer ideas between the separate bodies of literature studying each of the two methods. In particular, we recast the randomized Kaczmarz algorithm as an instance of SGD, and apply our results to prove its exponential convergence, but to the solution of a weighted least squares problem rather than the original least squares problem. We then present a modified Kaczmarz algorithm with partially biased sampling which does converge to the original least squares solution with the same exponential convergence rate.", "text": "abstract. obtain improved ﬁnite-sample guarantee linear convergence stochastic gradient descent smooth strongly convex objectives improving quadratic dependence conditioning linear dependence l/µ. furthermore show reweighting sampling distribution necessary order improve convergence obtain linear dependence average smoothness dominating previous results. also discuss importance sampling broadly show improve convergence also scenarios. results based connection make randomized kaczmarz algorithm allows transfer ideas separate bodies literature studying methods. particular recast randomized kaczmarz algorithm instance apply results prove exponential convergence solution weighted least squares problem rather original least squares problem. present modiﬁed kaczmarz algorithm partially biased sampling converge original least squares solution exponential convergence rate. keywords. distribution reweighting importance sampling kaczmarz method stochastic gradient descent paper connects algorithms remained remarkably disjoint literature randomized kaczmarz algorithm solving linear systems stochastic gradient descent method optimizing convex objective using unbiased gradient estimates. connection enables make contributions borrowing body literature other. particular helps highlight role weighted sampling obtain tighter guarantee linear convergence regime sgd. recall stochastic gradient descent method minimizing convex objective based access unbiased stochastic gradient estimates i.e. estimate gradient given point viewing expectation unbiased gradient estimate obtained drawing using gradient ∇fi. originated stochastic approximation pioneering work robbins monroe recently received renewed attention confronting large scale problems especially context machine learning classical analysis shows polynomial date january department mathematical sciences claremont mckenna college toyota technological institute chicago department mathematics university texas austin ensured even iterates necessarily converge unique optimum might case strongly convex. focus strongly convex case optimum unique convergence iterates optimum bach moulines recently provided non-asymptotic bound convergence iterates strongly convex improving previous results kind particular showed smooth minimizer i.e. ekxk goes zero exponentially rather polynomially reaching desired accuracy ekxk requires number steps scales logarithmically bach moulines’s bound required number iterations depends average squared conditioning number lipschitz constant l-smooth) µ-strongly convex. exact minimizer bound degrades gracefully function ek∇fik includes unavoidable term behaves σ/k. seemingly independent line research kaczmarz method proposed iterative method solving systems linear equations simplicity method makes useful wide array applications ranging computer tomography digital signal processing recently strohmer vershynin proposed variant kaczmarz method using random selection method select rows probability proportional squared norm showed using selection strategy desired accuracy reached noiseless setting number steps scales like linearly condition number. importance sampling stochastic optimization. birds-eye perspective paper aims extend notion importance sampling stochastic sampling methods numerical linear algebra applications general stochastic convex optimization problems. strohmer vershynin’s incorporation importance sampling kaczmarz setup example closely related set-up. importance sampling also considered stochastic coordinate-descent methods also weights proportional power lipschitz constants importance sampling also played role designing sampling-based low-rank matrix approximation algorithms row/column based sampling entry-wise sampling goes name leverage score sampling. resulting sampling methods proportional squared euclidean norms rows columns underlying matrix. references therein applications column subset selection problem matrix completion. applications importance sampling nystr¨om method. importance sampling also introduced compressive sensing framework translates sampling rows orthonormal matrix proportionally squared inner products rows second orthonormal matrix underlying signal assumed sparse. details. contributions. inspired analysis strohmer vershynin bach moulines prove convergence results stochastic gradient descent well variants gradient estimates chosen based weighted sampling distribution highlighting role importance sampling sgd. ﬁrst show without perturbing sampling distribution obtain linear dependence uniform conditioning possible obtain linear dependence average conditioning quadratic improvement previous results regimes components similar lipschitz constants. turn importance sampling using weighted sampling distribution. show weighting components proportionally lipschitz constants essentially done strohmer vershynin reduce dependence conditioning linear dependence average conditioning however comes increased dependence residual show partially biasing sampling towards enjoy best worlds obtaining linear dependence average conditioning without amplifying dependence residual. thus using importance sampling obtain guarantee dominating improving previous best-known results section consider beneﬁts reweighted also scenarios regimes. show also smooth not-strongly-convex objectives importance sampling improve dependence uniform bound smoothness dependence average smoothness e—such improvement possible without importance sampling. non-smooth objectives show importance sampling eliminate dependence variance lipschitz constants components. parallel work recently became aware zhao zhang also consider importance sampling non-smooth objectives including composite objectives suggesting reweighting obtain here. finally section turn kaczmarz algorithm explain instantiation using partially biased sampling improves known guarantees context well. show randomized kaczmarz method uniform i.i.d. selection recast instance preconditioned stochastic gradient descent acting re-weighted least squares problem connection provide exponential convergence rates algorithm. also consider kaczmarz algorithm corresponding hybrid selection strategy shares exponential convergence rate strohmer vershynin also sharing small error residual term algorithm. presents clear tradeoff convergence rate convergence residual present results method. form ei∼dfi smooth functionals endowed standard euclidean norm hilbert space norm k·k. drawn source distribution arbitrary probability space. throughout manuscript unless explicitly speciﬁed otherwise expectations respect indices drawn source distribution write ei∼dfi. also denote residual quantity minimum denote supremum support i.e. smallest a.s. unbiased gradient estimate obtained drawing using estimate. updates step size based gradient estimates given iterations form appropriate step-size sufﬁcient ensure ekxk expectations random sampling. long i.e. minimizer minimizes components yields linear convergence graceful degradation however linear convergence regime number required iterations scales expected squared conditioning paper reduce quadratic dependence linear dependence. begin guarantee ensuring linear dependence though dependence rather eli/µ theorem convex lipschitz constant a.s. µ-strongly convex. ek∇fik argminx suppose iterates given satisfy bach moulines’s results somewhat general. lipschitz requirement weaker complicated terms yields also study polynomial decaying step-sizes lead improved runtime target accuracy known ahead time. lipschitz constant component used current iterate. signiﬁcant difference factors third term inside parenthesis replaced complete proof found appendix. comparison results bach moulines. bound replaces dependence average square conditioning linear dependence uniform conditioning lipschitz constants similar magnitude quadratic improvement number required iterations. however different components widely different scaling i.e. highly variable supremum might larger average square conditioning. tightness. considering above might hope obtain linear dependence average conditioning eli/µ. however following example shows possible. consider uniform source distribution quadratics ﬁrst quadratic method must examine order recover within error less uniformly sampling indices takes iterations expectation. easy verify case large linear dependence would mean constant number iterations sufﬁce method sampled uniformly must consider least samples expectation non-trivial error. note li/µ indeed correspond correct number iterations required sgd. therefore choice dependence average quadratic conditioning linear dependence uniform conditioning unavoidable. linear dependence average conditioning possible method samples source distribution next section show obtain linear dependence average conditioning using importance sampling i.e. sampling modiﬁed distribution. event indicator function. discrete distribution probability mass function corresponds weighting probabilities obtain probability mass function construct weighted distribution sample rejection sampling sample accept probability supi otherwise reject continue re-sample suggestion accepted. accepted samples distributed according ei∼d denote expectation indices sampled weighted distribution important property expectation quantity depends representation equivalent equally valid stochastic representation objective well base representation. case iteration w∇fi unbiased gradient estimate. iterates based representation also refer w-weighted given {ik} drawn i.i.d. important observation guarantees equally valid w-weighted updates –the objective objective sub-optimality same minimizer same. need however calculate relevant quantities controlling convergence respect modiﬁed components weighted distribution strongly convex smooth optimization using weighted sgd. return analysis strongly convex smooth optimization investigate re-weighting yield better guarantee. must analyze relevant quantities involved. partially biased sampling. i.e. realizable situation true linear convergence also case already obtain desired guarantee linear convergence linear dependence average conditioning strictly improving bach moulines. however inequality might tight presence components small contribute towards residual error therefore dissatisfying scaling second term relative bach moulines factor l/inf fortunately easily overcome factor. consider sampling distribution mixture original source distribution re-weighting using weights sampling using weights obtain desired linear scaling without introducing additional factor residual term except constant factor two. thus obtain result dominates bach moulines substantially improves upon might also whether previous best known result could improved using weighted sampling. relevant quantity consider average square lipschitz constant weighted representation also ensures instead indeed improvement. thus bach moulines guarantee also however relying bach moulines still quadratic dependence opposed linear dependence obtain corollary implementing importance sampling. discussed above magnitudes highly variable importance sampling necessarily order obtain dependence average rather worst-case conditioning. applications especially lipschitz constants known advance easily calculated bounded importance sampling might possible directly sampling case example trigonometric approximation problems linear systems need solved repeatedly lipschitz constant easily computed data multiple passes data needed anyway. acknowledge regimes data presented online fashion sampling access source distribution importance sampling might difﬁcult. option could considered light results rejection sampling simulate sampling weights done accepting samples probability proportional overall probability accepting sample introducing additional factor l/l. results sample complexity linear dependence corollary ﬁrst accept probability accept perform procedure). thus presented samples cost obtaining sample dominates cost taking gradient step gain rejection sampling. might still gain rejection sampling cost operating sample dominates cost obtaining lipschitz constant. family partially biased schemes. choice weights corresponds equal uniform fully biased sampling. generally could consider sampling according family weights interpolate uniform fully biased sampling previous section considered smooth strongly convex objectives particularly interested regime residual linear convergence term dominant. weighted could course relevant also scenarios brieﬂy survey them well relate main scenario interest. iterations appropriately chosen step-size appropriate averaging iterates srebro relevant quantity determining iteration complexity furthermore srebro relying example foygel srebro point dependence supremum unavoidable cannot replaced average lipschitz constant sample gradients according source distribution must linear dependence quantity bound changes re-weighting l—all quan sub-optimality invariant re-weightings. therefor tities using weighted already calculated optimal weights given using case need partially biased sampling obtain appropriate step-size non-smooth objectives. turn non-smooth objectives components might smooth component gi-lipschitz. roughly speaking bound ﬁrst derivative bound second derivatives here performance depends second moment precise iteration complexity depends whether objective strongly convex whether bounded either case depends linearly parallel work zhao zhang also consider importance sampling stochastic optimization non-smooth objectives. zhao zhang consider general setting composite objective partially linearized. also there iteration complexity depends second moment gradient estimates analysis performed applies non-realizable regime. returning smooth strongly convex setting sections quantity deﬁni consider carefully residual term ek∇fik tively depends weighting analysis section avoided increasing much introducing partial biasing purpose. however dominant term might want choose weights minimize term. optimal weights would proportional k∇fik. problem know minimizer cannot calculate weights. approaches dynamically update weights based current iterates surrogate possible beyond scope paper. alternative approach bound k∇fik taking bound back quantity non-smooth case optimal weights proportional note different weighting using weights proportional optimize linear-convergence term studied section understand weighting according different consider generalized linear objective scalar function weighting according lipschitz constants kzik kzik gradients i.e. smoothness parameters versus weighting according lipschitz n-dimensional vector matrix rows argminx least-squares solution. writing least squares problem form source distribution uniform components lipschitz constants nkaik strong convexity parameter fk−k residual npikaik bi|. note case full-rank instead replace smallest nonzero eigenvalue case instead write randomized kaczmarz method solving least squares problem begins arbitrary estimate iteration selects i.i.d. random matrix iterates strohmer vershynin provided ﬁrst non-asymptotic convergence rates showing drawing rows proportionally kaik leads provable exponential convergence expectation full-rank case method easily extended case matrix full-rank yield convergence solution e.g. recent works acceleration techniques improve convergence rates however easily verify iterates precisely weighted iterates reduction quadratic dependence conditioning linear dependence theorem well biased sampling investigate motivated strohmer vershynin’s analysis randomized kaczmarz method. indeed applying theorem weighted iterates weights stepsize yields precisely strohmer vershynin guarantee. using step-sizes. shown strohmer vershynin extended needell randomized kaczmarz method weighted sampling exhibits exponential convergence within radius convergence horizon least-squares solution. stepsize used second term vanish. shown changing step size allow convergence inside convergence horizon although non-asymptotic results difﬁcult obtain. results allow ﬁniteiteration guarantees arbitrary step-sizes immediately applied setting. indeed applying theorem weights gives recover exponential rate strohmer vershynin factor nearly convergence horizon. arbitrary corollary implies tradeoff smaller convergence horizon slower convergence rate. uniform selection. kaczmarz variant strohmer vershynin calls weighted sampling thus requires pre-computing norms. although certainly possible applications cases might better avoided. understanding randomized kaczmarz allows apply theorem also uniform weights obtain randomized kaczmarz using uniform sampling converges leastsquares solution enjoys ﬁnite-iteration guarantees note randomized kaczmarz algorithm uniform selection converges exponentially weighted least-squares solution within arbitrary accuracy choosing sufﬁciently small stepsize thus general randomized kaczmarz algorithms uniform biased selection converge towards different solutions. partially biased sampling. analysis using partially biased sampling weights applicable also randomized kaczmarz method. applying theorem using weights gives partially biased randomized kaczmarz method described compared standard update equation yields convergence rate fully biased randomized kaczmarz method gives better dependence residual error fully biased sampling ﬁnal term smaller ﬁnal term section present numerical results randomized kaczmarz algorithm partially biased sampling applying algorithm least squares problem considering recall corkax responds randomized kaczmarz algorithm strohmer vershynin fully weighted sampling corresponds partially biased randomized kaczmarz algorithm outlined corollary demonstrate behavior algorithm depends conditioning system residual error least squares solution. focus exploring role convergence rate algorithm various types matrices consider types systems described below using matrix setting create vector standard normal entries. described matrix residual create system randomized kaczmarz method various choices experiment consists independent trials uses optimal step size corollary plots show average behavior trials. settings show various types behavior kazcmarz method exhibit. figure shows convergence behavior randomized kaczmarz method settings. expected rows normalized case different behavior varies here weighted sampling signiﬁcantly outperforms uniform sampling trend monotonic hand rows close normalized case various give rise similar convergence rates expected. tested choice gave worst convergence rate purely weighted sampling gives best. still worst-case convergence rate much worse opposed situation uniform sampling case cases matrices varying norms cover high medium noise regimes respectively. high noise regime fully weighted sampling relatively slow converge theory suggests hybrid sampling outperforms weighted uniform selection. medium noise regime hybrid sampling still outperforms weighted uniform selection. again surprising since hybrid sampling allows balance small convergence horizon convergence rate. decrease noise level weighted sampling preferred. figure shows number iterations randomized kaczmarz method needed obtain ﬁxed approximation error. choice case number iterations point desired approximation error still attained. seen also figure case exhibits monotonic improvements scale cases optimal choice pure weighted sampling whereas cases prefer intermediate values consider paper making three contributions improved dependence conditioning smooth strongly convex discussion importance sampling connection randomized kaczmarz method. simplicity considered iterates ﬁxed step-size enough getting optimal iteration complexity target accuracy known advance approach paper. easy adapt analysis using standard techniques incorporate decaying step-sizes appropriate don’t know advance. figure number iterations needed randomized kaczmarz method partially biased sampling various values obtain approximation error cases described above case case case case case finally discussion importance sampling limited static reweighting sampling distribution. sophisticated approach would update sampling distribution dynamically method progresses gain information relative importance components. although dynamic importance sampling sometimes attempted heuristically aware rigorous analysis dynamic biasing scheme. would like thank anonymous reviewers useful feedback signiﬁcantly improved manuscript. would like thank chris white pointing simpliﬁed proof corollary partially supported simons foundation collaboration grant career alfred sloan fellowship. partially supported google research award. supported part grant n--- afosr young investigator program award career award. cenker feichtinger mayer steier strohmer. variants pocs method using afﬁne subspaces ﬁnite codimension applications irregular sampling. proc. spie visual communications image processing pages mathematics computerized tomography volume classics society industrial applied mathematics applied mathematics. philadelphia isbn ---. ./.. http//dx.doi.org/./.. reprint original. popa. extensions block-projections methods relaxation parameters inconsistent rank-deﬁcient least-squares problems. issn ./bf. http//dx.doi.org/./bf. strohmer vershynin. randomized kaczmarz algorithm exponential convergence. fourier anal. appl. issn s---. http//dx.doi.org/./s---. tanabe. projection method solving singular system linear equations applimain results utilize elementary fact smooth functions lipschitz continuous gradient called co-coercivity gradient. state lemma recall proof completeness. employed jensen’s inequality ﬁrst inequality co-coercivity lemma ﬁnal line. next take expectation respect choice assumption ek∇fik. e∇fi obtain", "year": 2013}