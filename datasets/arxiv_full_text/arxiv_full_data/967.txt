{"title": "Learning with Pseudo-Ensembles", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout (Hinton et. al, 2012) in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of (Socher et. al, 2013) into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.", "text": "formalize notion pseudo-ensemble collection child models spawned parent model perturbing according noise process. e.g. dropout deep neural network trains pseudo-ensemble child subnetworks generated randomly masking nodes parent network. examine relationship pseudo-ensembles involve perturbation model-space standard ensemble methods existing notions robustness focus perturbation observation-space. present novel regularizer based making behavior pseudo-ensemble robust respect noise process generating fully-supervised setting regularizer matches performance dropout. unlike dropout regularizer naturally extends semi-supervised setting produces state-of-the-art results. provide case study transform recursive neural tensor network pseudo-ensemble signiﬁcantly improves performance real-world sentiment analysis benchmark. ensembles models long used obtain robust performance presence noise. ensembles typically work training several classiﬁers perturbed input distributions e.g. bagging randomly elides parts distribution trained model boosting re-weights distribution training adding model ensemble. last years dropout methods achieved great empirical success training deep models leveraging noise process perturbs model structure itself. however much analysis relating approach classic ensemble methods approaches learning robust models. paper formalize notion pseudo-ensemble collection child models spawned parent model perturbing noise process. sec. deﬁnes pseudoensembles sec. discusses relationships pseudo-ensembles standard ensemble methods well existing notions robustness. pseudo-ensemble framework deﬁned leveraged create algorithms. sec. develop novel regularizer minimizes variation output model subject noise inputs internal state also discuss relationship regularizer standard dropout methods. sec. show regularizer reproduce performance dropout fullysupervised setting also naturally extending semi-supervised setting produces state-of-the-art performance real-world datasets. sec. presents case study extend recursive neural tensor network converting pseudo-ensemble. generate pseudo-ensemble using noise process based gaussian parameter fuzzing latent subspace sampling empirically show types perturbation contribute signiﬁcant performance improvements beyond original model. conclude sec. consider data distribution want approximate using parametric parent model pseudo-ensemble collection ξ-perturbed child models comes noise process dropout provides clearest existing example pseudo-ensemble. dropout samples subnetworks source network randomly masking activity subsets input/hidden layer nodes. parameters shared subnetworks common source network learned minimize expected loss individual subnetworks. pseudoensemble terms source network parent model sampled subnetwork child model noise process consists sampling node mask using extract subnetwork. noise process used generate pseudo-ensemble take fairly arbitrary forms. requirement sampling noise realization imposing parent model computationally tractable. generality allows deriving variety pseudo-ensemble methods existing models. example gaussian mixture model could perturb means mixture components with e.g. gaussian noise covariances with e.g. wishart noise. goal learning pseudo-ensembles produce models robust perturbation. formalize this general pseudo-ensemble objective supervised learning written follows pair drawn data distribution noise realization represents output child model spawned parent model ξ-perturbation true label loss predicting instead generality pseudo-ensemble approach comes broad freedom describing noise process mechanism perturbs parent model many useful methods could developed exploring novel noise processes generating perturbations beyond independent masking noise considered neural networks feature noise considered context linear models. example develops method learning ordered representations applying dropout/masking noise deep autoencoder enforcing particular nested structure among random masking variables relies heavily random perturbations training generative stochastic networks. pseudo-ensembles closely related traditional ensemble methods well methods learning models robust input uncertainty. optimizing expected loss individual ensemble members’ outputs rather expected loss joint ensemble output pseudo-ensembles differ boosting iteratively augments ensemble minimize loss joint output meanwhile child models pseudo-ensemble share parameters structure parent model tend correlate behavior. distinguishes pseudo-ensembles traditional independent member ensemble methods like bagging random forests typically prefer diversity behavior members provides bias variance reduction outputs members averaged fact regularizers introduce sec. explicitly minimize diversity behavior pseudo-ensemble members. deﬁnition pseudo-ensembles strongly motivated intuition models trained robust noise generalize better models sensitive small perturbations. previous work robust learning overwhelmingly concentrated perturbations affecting inputs model. example optimization community produced large body theoretical empirical work addressing stochastic programming robust optimization stochastic programming seeks produce solution e.g. linear program performs well average respect known distribution perturbations parameters problem deﬁnition. robust optimization generally seeks produce solution e.g. linear program optimal worst case performance given possible perturbations parameters problem deﬁnition. several well-known machine learning methods shown equivalent certain robust optimization problems. example shows using lasso regularization) linear regression model equivalent robust optimization problem. shows learning standard regularization corresponding rkhs) also equivalent robust optimization problem. supporting notion noise-robustness improves generalization prove many statistical guarantees make svms appealing directly properties robust optimization equivalents rather using complicated proofs involving e.g. vc-dimension. closely related pseudo-ensembles recent works consider approaches learning linear models inputs perturbed different sorts noise. shows efﬁciently learn linear model optimizes expected performance w.r.t. certain types noise inputs marginalizing noise. particularly relevant work studies dropout closely shows effects well-approximated tikhonov regularization term estimated labeled unlabeled data. authors leveraged label-agnosticism achieve state-of-the-art performance several sentiment analysis tasks. work described considers noise input-space pseudo-ensembles involve noise model-space. actually seen superset input-space noise model always extended initial identity layer copies noise-free input. noise input-space reproduced noise initial layer part model-space. present pseudo-ensemble agreement regularization used fairly general class computation graphs. concreteness present case deep layered neural networks. regularization operates controlling distributional properties random vectors gives activities layer response layers perturbed layer left unperturbed. fig. illustrates construction random vectors. assume layer output layer i.e.f gives output unperturbed parent model response gives response child model generated ξ-perturbing given random vectors parent model regularize unlabeled observation variance penalty imposed distribution activities layer pseudo-ensemble spawned controls relative importance note variance approximation holds reasonably well many useful neural network architectures experiments actually compute penalties independently-sampled pairs child models. consider several different measures variance penalize introduce needed. original motivations dropout helps prevent feature co-adaptation dropout encourages individual features remain helpful least become harmful features removed local context. provide support claim examining following optimization objective supervised loss depends parent model pseudo-ensemble appears regularization term. simplicity dkl|| softmax) softmax standard softmax kl-divergence xent) loss xent cross-entropy predicted distribution true distribution never explicitly passes label information ξ-perturbed network acts effects distribution parent model’s predictions subjected ξ-perturbation. case trades accuracy feature co-adaptation measured degree feature activity distribution layer affected perturbation feature activity distributions layers test regularizer empirically sec. observed ability regularizer reproduce performance beneﬁts standard dropout supports notion discouraging co-adaptation plays important role dropout’s empirical success. also acting strictly make output parent model robust ξ-perturbation performance regularizer rebuts claim noise-robustness plays minor role success standard dropout. relating regularization standard dropout authors show that assuming noise process logistic regression inﬂuence dropout optimizes following objective regularization semi-supervised learning regularization works as-is semi-supervised setting penalties require label information. train networks semi-supervised learning ways apply objective labeled examples regularization unlabeled examples. ﬁrst applies tanh-variance penalty second applies xent-variance penalty deﬁne follows denotes entropy. thus combines kl-divergence penalty entropy penalty shown perform well semi-supervised setting recall non-output layers regularize direction penalty masking noise also apply zero-mean gaussian noise input biases nodes. experiments chose output-layer penalties based observed performance. supervised learning mnist digtested regularization three scenarios learning semi-supervised learning mnist digits semi-supervised transfer nips workshop challenges learning hierarchical moddataset scripts/instructions reproducing results section available online http//github.com/philip-bachman/pseudo-ensembles. mnist dataset comprises grayscale hand-written digit images training images testing. supervised tests used hyperparameters roughly following trained networks hidden layers nodes each using rectiﬁed-linear activations -norm constraint incoming weights node. standard dropout used softmax xent loss output layer. initialized hidden layer biases output layer biases inter-layer weights zero-mean gaussian noise trained networks epochs early-stopping obtained error averaged random initializations. using penalty output layer computing classiﬁcation loss/gradient unperturbed parent network obtained averaged error. ξ-perturbation involved node masking bias noise. thus training network used dropout ignoring effects masking noise classiﬁcation loss encouraging network robust masking noise matched performance dropout. result supports equivalence dropout particular form regularization derived section tested semi-supervised learning mnist following protocol described tests split mnist’s training samples labeled/unlabeled subsets labeled sets containing samples. labeled sets size full training data randomly split times labeled/unlabeled sets results averaged splits. labeled sets size averaged random splits. labeled sets number examples class. tested regularization without denoising autoencoder pre-training pre-trained networks always pea-regularized penalty figure performance regularization semi-supervised learning using mnist dataset. ﬁlter blocks result training ﬁxed network architecture labeled samples using weight norm constraints standard dropout standard dropout regularization unlabeled data preceded pre-training denoising autoencoder bottom ﬁlter block result training labeled samples. shows test error course training raw/sde/pea averaged random training sets size output layer hidden layers. non-pre-trained networks used output layer except labeled size used. latter case gradually increased course training suggested generated pseudoensembles tests using masking noise gaussian input+bias noise network hidden layers nodes. weight norm constraints hyperparameters supervised learning. table compares performance regularization previous results. aside methods table general i.e. convolutions image-speciﬁc techniques improve performance. main comparisons interest methods semi-supervised learning neural networks i.e. e-nn mtc+ pl+. e-nn uses nearest-neighbors-based graph laplacian regularizer make predictions smooth respect manifold underlying data distribution mtc+ regularizes predictions smooth respect data manifold penalizing gradients learned approximation tangent space data manifold. uses joint-ensemble predictions unlabeled data pseudo-labels treats like true labels. classiﬁcation losses true labels pseudo-labels balanced scaling factor carefully modulated course training. regularization outperforms previous methods every setting except labeled samples performs better beneﬁt pre-training. adding pretraining achieve two-fold reduction error using labeled samples. table performance semi-supervised learning methods mnist varying numbers labeled samples. left-to-right methods transductive neural convolutional neural embednn manifold tangent classiﬁer pseudo-label standard dropout plus fuzzing dropout plus fuzzing pre-training pre-training. methods used contractive denoising autoencoder pre-training testing protocol results left mtc+ presented mtc+ results respective papers remaining results own. trained using network/sgd hyperparameters pea. difference former regularize pseudo-ensemble agreement unlabeled examples. measured performance standard test samples mnist training samples included given labeled training made available without labels. best result training size bold. organizers nips workshop challenges learning hierarchical models proposed challenge improve performance target domain using labeled unlabeled data related source domains. labeled data source cifar- contains color images classes. unlabeled data source collection color images taken tiny images target domain comprised color images divided unevenly among classes. neither classes images target domain appeared either source domains. winner challenge used convolutional spike slab sparse coding followed pooling linear pooled features labels source data ignored source data used pre-train large convolutional features. applying pre-trained feature extractor training images method achieved accuracy target domain best published result dataset. applied semi-supervised regularization ﬁrst using cifar- data train deep network comprising three max-pooled convolutional layers followed fully-connected hidden layer softmax xent output layer. afterwards removed hidden output layers replaced pair fully-connected hidden layers feeding -hinge-loss output layer trained non-convolutional part network training images target domain. ﬁnal training phase involved three layers tried standard dropout dropout regularization source data. standard dropout achieved accuracy improved added regularization source data. improvement previous state-of-the-art dropout improved training strategy controlling feature activity output distributions pseudo-ensemble unlabeled data allowed signiﬁcant improvement. show recursive neural tensor network adapted using pseudo-ensembles evaluate stanford sentiment treebank task. task involves predicting sentiment short phrases extracted movie reviews rottentomatoes.com. ground-truth labels phrases sub-phrases produced processing standard parser generated using amazon mechanical turk. addition pseudoensembles used compact bilinear form function rntn applies recursively shown figure computation dimension original indicates matrix slice tensor indicates vector matrix original rntn parameters rntns transform matrix rn×n+ classiﬁcation matrix rc×n+; rntn outputs class probabilities vector using softmax). indicates vertical vector stacking. initialized model pre-trained word vectors. pre-training used wordvec training three modiﬁcations dropout/fuzzing applied pre-training vector norms constrained pre-trained vectors standard deviation tanh applied wordvec code required experiments publicly available online. generated pseudo-ensembles parent rntn using types perturbation subspace sampling weight fuzzing. performed subspace sampling keeping randomly sampled latent dimensions parent model processing given phrase tree. using sampled dimensions full phrase tree reduced computation time signiﬁcantly parameter matrices/tensor could sliced include relevant dimensions. this allowed train signiﬁcantly larger models over-ﬁtting offset increased model capacity. training larger models would tedious without parameter slicing permitted subspace sampling feedforward rntn training sampled subspace time phrase tree processed computed testtime outputs phrase tree averaging randomly sampled subspaces. performed weight fuzzing training perturbing parameters zero-mean gaussian noise processing phrase tree applying gradients w.r.t. perturbed parameters unperturbed parameters. fuzz testing. weight fuzzing interesting interpretation implicit convolution objective function isotropic gaussian distribution. case recursive/recurrent neural networks prove quite useful convolving objective gaussian reduces curvature thereby mitigating problems stemming ill-conditioned hessians description model training/testing process supplementary material code http//github.com/philip-bachman/pseudo-ensembles. table fine-grained binary root-level prediction performance stanford sentiment treebank task. rntn original full model presented compact tensor network model. +f/s indicates augmenting base model weight fuzzing/subspace sampling. paragraph vector model dcnn dynamic convolutional neural network model following protocol suggested measured root-level prediction accuracy tasks ﬁne-grained sentiment prediction binary sentiment prediction. ﬁne-grained task involves predicting classes indicating strongly negative sentiment indicating strongly positive sentiment. binary task similar ignores neutral phrases considers whether phrase generally negative positive table shows performance compact rntn four forms include none subspace sampling weight fuzzing. using regularization parameters compact rntn approached performance full rntn roughly matching performance second best method tested adding weight fuzzing improved performance past full rntn. adding subspace sampling improved performance adding noise types pushed rntn well past full rntn resulting state-ofthe-art performance binary task. figure feedforward recursive neural tensor network. first tree structure generated parsing input sentence. then vector node computed look-up leaves tensor-based transform node’s children’s vectors otherwise. proposed notion pseudo-ensemble captures methods dropout feature noising linear models recently drawn signiﬁcant attention. using conceptual framework provided pseudo-ensembles developed applied regularizer performs well empirically provides insight mechanisms behind dropout’s success. also showed pseudo-ensembles used improve performance already powerful model competitive real-world sentiment analysis benchmark. anticipate idea uniﬁes several rapidly evolving lines research used develop several novel successful algorithms especially semi-supervised learning. bergstra breuleux bastien lamblin pascanu desjardins turian warde-farley bengio. theano math expression compiler. python scientiﬁc computing conference hastie friedman tibshirani. elements statistical learning g.e. hinton srivastava krizhevsky sutskever r.r. salakhutdinov. improving neural networks preventing co-adaptation feature detectors. arxiv.v ranzato salakhutdinov tenenbaum. workshop challenges learning hierarchical models transfer learning optimization. nips d.-h. lee. pseudo-label simple efﬁcient semi-supervised learning method deep", "year": 2014}