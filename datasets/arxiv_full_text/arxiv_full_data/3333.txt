{"title": "Random Feature Maps via a Layered Random Projection (LaRP) Framework for  Object Classification", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "The approximation of nonlinear kernels via linear feature maps has recently gained interest due to their applications in reducing the training and testing time of kernel-based learning algorithms. Current random projection methods avoid the curse of dimensionality by embedding the nonlinear feature space into a low dimensional Euclidean space to create nonlinear kernels. We introduce a Layered Random Projection (LaRP) framework, where we model the linear kernels and nonlinearity separately for increased training efficiency. The proposed LaRP framework was assessed using the MNIST hand-written digits database and the COIL-100 object database, and showed notable improvement in object classification performance relative to other state-of-the-art random projection methods.", "text": "approximation nonlinear kernels linear feature maps recently gained interest applications reducing training testing time kernel-based learning algorithms. current random projection methods avoid curse dimensionality embedding nonlinear feature space dimensional euclidean space create nonlinear kernels. introduce layered random projection framework model linear kernels nonlinearity separately increased training efﬁciency. proposed larp framework assessed using mnist hand-written digits database coil- object database showed notable improvement object classiﬁcation performance relative state-of-the-art random projection methods. approximation nonlinear kernels recently gained popularity ability implicitly learn nonlinear functions using explicit linear feature spaces linear feature spaces typically high dimensional pose referred curse dimensionality. avoid cost explicitly working feature spaces well-known kernel trick employed rather directly learning classiﬁer nonlinear mapping considered kernel classiﬁer learned appears solve curse dimensionality leads entirely problem referred curse support. support undergo unbounded growth increasing training data size resulting increased training testing time though kernel approximation work supported natural sciences engineering research council canada ontario ministry economic development innovation canada research chairs program. authors also thank nvidia hardware used study nvidia hardware grant program. methods used successfully variety data analysis tasks issue scalability becoming crucial advent data applications initially proposed rahimi recht previous kernel approximation methods attempted address curse support low-distortion embedding nonlinear feature space dimensional euclidean inner product space randomized feature rahimi recht proposed method extracting random features mapping input data randomized low-dimensional feature space applying fast linear methods. designed approximate user speciﬁed shift-invariant kernel rahimi recht evaluated sets random features showing linear machine learning methods outperformed state-of-the-art large-scale kernel machines large-scale classiﬁcation regression tasks. inspired karnick presented feature maps approximating positive deﬁnite product kernels low-distortion embeddings product kernels linear euclidean spaces. karnick demonstrated approach using homogeneous non-homogeneous polynomial kernels well generalization approach compositional kernels. experiments resulted moderate decrease classiﬁcation accuracy authors noted decrease almost always accompanied signiﬁcant increase training test speeds. similarly based proposed fastfood approximation method focuses signiﬁcantly decreasing computational memory costs associated kernel methods. using hadamard matrices combined diagonal gaussian matrices place dense gaussian random matrices kernel approximation shown variance unbiased. achieved similar accuracy full kernel expansions approximately faster using less memory. pham pagh presented novel fast scalable randomized tensor product technique called tensor sketching efﬁciently approximating polynomial kernels. images data randomly projected without computing corresponding coordinates polynomial feature space allowing fast computation unbiased estimators. similar pham pagh demonstrated increase accuracy fig. proposed layered random projection framework. framework comprised alternating layers linear localized random projection ensembles non-saturating global nonlinearities allow complex nonlinear random projections. recently hamid proposed compact random feature maps concise representation random features maps accurately approximate polynomial kernels. taking advantage rank deﬁciency spaces constructed random feature maps craftmaps achieves concise representation projecting original data nonlinearly linearly projecting vectors capture underlying structure random feature space. hamid demonstrated rank deﬁciency kernel approximations presented showed improved test classiﬁcation errors multiple datasets using craftmaps comparison original random feature maps. state-of-the-art nonlinear random projection methods demonstrated provide signiﬁcantly improved accuracy reduced computational costs largescale real-world datasets primarily focused embedding nonlinear feature spaces dimensional spaces create nonlinear kernels. such alternative strategies achieving complexity nonlinear random projection beyond kernel methods wellexplored strong potential improved accuracy reduced complexity. work propose novel method modelling nonlinear kernels using layered random projection framework. contrary existing kernel methods larp models nonlinear kernels alternating layers linear kernel ensembles nonlinearities. strategy allows proposed larp framework overcome curse dimensionality producing compact discriminative random features. work consists alternating layers linear localized random projection ensembles non-saturating global nonlinearities combination layers allows complex nonlinear random projections produce discriminative features provided existing linear random projection approaches. data projected onto alternative feature space using random matrices. lrpe sequencing layer consists ensemble localized random projections project input feature maps previous layer xki− random feature spaces banded toeplitz matrices resulting output feature maps sliding window used nonlinearly enforce spatial consistency within rectiﬁed feature sliding window used study rect empirically shown provide good balance spatial consistency feature information preservation. random projection kernel characterizing random projection matrices proposed larp framework parameters needs trained upper lower bounds uniform distribution total number number layers. work larp framework trained iterative scaled conjugate gradient optimization using cross-entropy objective function. assess efﬁcacy proposed larp framework object classiﬁcation method compared state-ofthe-art random projection methods mnist hand-written digits database coil- object database mnist database divided training testing sets speciﬁed similar coil- database divided equally sized partitions training testing; training consisted views object intervals remaining views used testing. figure shows sample images mnist hand-written digits database coil- object database fig. sample images databases used testing. ﬁrst shows sample images mnist handwritten digits database second shows samples coil- object database. fig. localized random projection ensemble sequencing layer consists ensemble localized random projections project input feature maps previous layer random feature spaces banded toeplitz matrices. output feature maps lrpe layer nonlinearity layer. nonl layer consists absolute value rectiﬁcation followed slidingwindow median regularization fig. nonlinearity layer consists absolute value rectiﬁcation introduce non-saturating nonlinearity layered random projection framework followed sliding-window median regularization improve robustness uncertainties data. table test classiﬁcation errors mnist coil- databases best classiﬁcation error database boldface. proposed larp framework compared fastfood applied note fastfood used features proposed larp framework used random features. table summarizes number localized random projections supports random projection matrices used lrpe sequencing layer proposed larp framework study. proposed larp framework characterizes given image using random features. test classiﬁcation errors state-of-the-art methods obtained test classiﬁcation errors state-of-the-art methods unavailable features classiﬁcation errors state-of-the-art methods using features used comparison. table shows test classiﬁcation errors proposed larp framework state-of-the-art methods fastfood applied using mnist coil- databases. test classiﬁcation errors clearly indicate proposed larp framework outperformed state-of-the-art methods. proposed larp framework achieved test classiﬁcation error improvements best stateof-the-art results using features respectively mnist database. similarly larp outperformed state-of-the-art methods using coil- database showing test classiﬁcation error improvements best state-of-the-art results using features respectively. addition notable test classiﬁcation error improvements also observed proposed larp framework achieved results using signiﬁcantly fewer random features. state-of-the-art methods require features generate comparable test classiﬁcation errors larp framework attained better test classiﬁcation errors using features. indicates larp framework capable generating compact discriminative random features relative state-of-the-art methods. novel layered random projection framework presented overcome curse dimensionality model linear kernels nonlinearity separately. done alternating layers linear localized random projection ensembles non-saturating global nonlinearities allow complex nonlinear random projections. proposed larp framework evaluated state-of-the-art random kernel approximation methods using mnist coil- databases. generating random features larp framework achieved lowest test classiﬁcation errors databases compared state-of-the-art methods using random features. indicates potential proposed larp framework producing useful compact feature maps object classiﬁcation. future work includes investigation inter-kernel information effects generated random features maps. well comprehensive evaluation investigation larp framework different image recognition processing tasks applications e.g. saliency segmentation etc. conducted. chitta havens jain approximate kernel k-means solution large scale kernel clustering proceedings sigkdd international conference knowledge discovery data mining. pham pagh fast scalable polynomial kernels explicit feature maps proceedings sigkdd international conference knowledge discovery data mining.", "year": 2016}