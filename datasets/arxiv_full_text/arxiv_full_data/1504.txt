{"title": "Interpretable Counting for Visual Question Answering", "tag": ["cs.AI", "cs.CL", "cs.CV"], "abstract": "Questions that require counting a variety of objects in images remain a major challenge in visual question answering (VQA). The most common approaches to VQA involve either classifying answers based on fixed length representations of both the image and question or summing fractional counts estimated from each section of the image. In contrast, we treat counting as a sequential decision process and force our model to make discrete choices of what to count. Specifically, the model sequentially selects from detected objects and learns interactions between objects that influence subsequent selections. A distinction of our approach is its intuitive and interpretable output, as discrete counts are automatically grounded in the image. Furthermore, our method outperforms the state of the art architecture for VQA on multiple metrics that evaluate counting.", "text": "questions require counting variety objects images remain major challenge visual question answering common approaches involve either classifying answers based ﬁxed length representations image question summing fractional counts estimated section image. contrast treat counting sequential decision process force model make discrete choices count. speciﬁcally model sequentially selects detected objects learns interactions objects inﬂuence subsequent selections. distinction approach intuitive interpretable output discrete counts automatically grounded image. furthermore method outperforms state architecture multiple metrics evaluate counting. visual question answering important benchmark test context-speciﬁc reasoning complex images. ﬁeld seen substantial progress counting-based questions seen least improvement intuitively counting involve ﬁnding number distinct scene elements objects meet criteria fig. example. contrast predominant approach involves representing visual input ﬁnal feature convolutional neural network attending regions based encoding question classifying answer attention-weighted image features intuition counting seems odds effects attention weighted average obscures notion distinct elements. such motivated re-think typical approach counting propose method embraces discrete nature task. approach partly inspired recent work represents images distinct objects identiﬁed object detection making relationships objects experiment counting systems build vision module used works represents image detected objects. training evaluation create dataset howmany-qa. taken countingspeciﬁc union visual genome introduce interpretable reinforcement learning counter treats counting sequential decision process. treat learning count learning enumerate relevant objects scene. result irlc returns count also objects supporting answer. output produced iterative method. step sequence stages first object selected added count. second model adjusts priority given unselected objects based conﬁguration selected objects supervise ﬁnal count train decision process using reinforcement learning additional experiments highlight importance iterative approach using manner weak supervision. furthermore train current state model howmanyqa irlc achieves higher accuracy lower count error. lastly compare figure irlc takes input counting question image. detected objects added returned count sequential decision process. example illustrates actual model behavior training. visual representations counting. standalone problem counting images received attention typically within speciﬁc problem domains. segui explore training count directly synthetic data. counts also estimated learning produce density maps category interest lempitsky zisserman o˜noro-rubio l´opez-sastre zhang density estimation simpliﬁes challenging approach counting instance-by-instance detection methods detect objects bounding boxes advanced considerably tuning redundancy reduction steps order count unreliable here overcome limitation allowing ﬂexible question speciﬁc interactions counting. alternative approaches attempt model subitizing describes human ability quickly accurately gauge numerosity objects present. zhang demonstrates cnns trained towards similar ability estimating number salient objects scene. approach extended counting classes objects simultaneously chattopadhyay model trained estimate counts within subdivision full image local counts typically within subitizing range. studies apply counting ﬁxed object categories. contrast interested counting visual question answering criteria counting change question question arbitrarily complex. places work different setting count image alone. example chattopadhyay apply trained models subset questions analysis limited speciﬁc subset examples question answer labels agreed object detection labels training. here overcome limitation learning count directly question/answer pairs. visual question answering. potential deep learning fuse visual linguistic reasoning recognized time visual question answering poses challenge retrieving question-speciﬁc information associated image often requiring complex scene understanding ﬂexible reasoning. recent years number datasets introduced studying problem majority recent progress aimed so-named datasets early baseline represents question image coarse granularity respectively using words embedding along spatially-pooled outputs classify answer similar ﬁxed-length image representation fused question embeddings input recurrent neural network answer classiﬁed. attention. recent variants chosen represent image ﬁner granularity omitting spatial pooling feature instead attention focus relevant image regions producing answer works spatially-tiled feature vectors output represent image; others follow intuition meaningful representation come parsing feature according locations objects scene notably using object detection design choice winning submission challenge work directed synthetic images demonstrated utility relationships provide additional form image annotation interpretable vqa. scene graphs real-image would desirable property intermediate model variables would grounded concepts explicitly step towards making neural reasoning transparent. conceptual parallel found neural module networks gain interpretability grounding reasoning process deﬁned concepts. general concept interpretable subject recent interest. park extends task include generating explanations produced answers. chandrasekaran take different approach asking well humans learn patterns answers failures trained model. humans indeed identify patterns gain apparent insight knowing intermediate states model light this motivated goal developing transparent address level counting vqa. show that despite challenge presented particular task intuitive approach gains performance interpretability state art. within ﬁeld majority progress aimed dataset recently expands total number questions dataset attempts reduce bias balancing answers repeated questions. consists questions pertaining images coco examples divided according ofﬁcial coco splits. addition incorporate visual genome dataset visual genome consists images roughly half part coco. includes visual question answering dataset. include examples dataset pertain image training set. order evaluate counting speciﬁcally deﬁne subset pairs refer howmany-qa. inclusion criteria designed ﬁlter pairs question asks count opposed simply answer form number ﬁrst condition require question contains following phrases many number amount count also reject question contains phrase number since phrase frequently refers printed number rather count lastly require ground-truth answer number original train includes roughly pairs labeled figure examples question-answer pairs excluded howmany-qa. selection exempliﬁes common types number questions require counting therefore distract objective time general number-based answers ballparking reading numbers images. importantly standard evaluation metrics distinguish counting questions; instead performance reported number questions whole. ﬁlter focus counting questions cannot make ofﬁcial test data since annotations available. hence divide validation data separate development test sets. speciﬁcally apply criteria ofﬁcial validation data select resulting pairs serve test data. remaining pairs used development set. mentioned above howmany-qa training data augmented available pairs visual genome selected using criteria. breakdown size composition howmany-qa provided table models compared work trained evaluated howmany-qa. facilitate future comparison work made training development test question available download. work focus speciﬁcally counting setting visual question answering addition interested model interpretability. explore notion experimenting models capable producing question-guided counts visually grounded object proposals. rather substantially modifying existing counting approaches chattopadhyay compare three models whose architectures naturally within experimental scope. models produce count outputs object detection module identical strategies encode question compare detected objects. models differ terms components used produce count approach inspired strategy anderson teney model represents current state infers objects input questionanswering system. inference performed using faster r-cnn architecture faster r-cnn proposes regions corresponding objects image. figure model includes three basic modules vision language counting text shaded regions describes aspects modules shared across models. language model embeds question compares object using scoring function jointly trained caption grounding; irlc counting module. rather train vision module scratch make publicly available object proposals learned anderson provide rich object-centric representations image dataset. representations ﬁxed learning count shared across models experiment with. architecture encodes question compares detected object scoring function. deﬁne ﬁnal hidden state lstm processing question compute score vector object here denotes word embedding question token position denotes score vector encoding relevance object question. following anderson implement scoring function layer gated tanh units denotes vector concatenation. experiment jointly training scoring function perform caption grounding supervise using region captions visual genome. region captions provide linguistic descriptions localized regions within image goal caption grounding identify object given caption describes caption grounding uses strategy identical question answering lstm used encode caption scoring function used encode relevance detected object. weights scoring function tied counting caption grounding include results experiments caption grounding ignored. interpretable counter proposed model learn count learning count. assume counting question implicitly refers subset objects within scene meet variable criteria. sense goal model enumerate subset objects. implement sequential decision process need represent probability selecting given action action affects subsequent choices. project object scores vector logits representing likely object counted number detected objects compute matrix interaction terms used update logits value represents selecting object change calculate interaction compressed representation question product normalized object vectors action expressed index selected object. denotes indexed object allowed counted once. deﬁne count timestep terminal action selected approach bears similarity non-maximal suppression staple technique object detection suppress redundant proposals. however approach less rigid allows question determine similar and/or overlapping objects interact. training irlc. process generating count requires making discrete decisions training requires techniques reinforcement learning. given formulation natural choice apply reinforce calculate distribution action probabilities generate count iteratively sampling actions distribution calculate reward using self-critical sequence training variation policy gradient. deﬁne count error deﬁne reward egreedy egreedy baseline count error obtained greedy action selection this deﬁne counting loss additionally include auxiliary objectives learning. sampled sequence measure total negative policy entropy across observed time steps. also measure average interaction strength time step collect total huber loss including entropy objective common strategy using policy gradient used improve exploration. interaction penalty motivated priori expectation interactions sparse. training minimize weighted three losses normalized number decision steps. before provide training implementation details appendix softcount. baseline approach train model count directly outputs scoring function. object project score vector scalar value apply sigmoid nonlinearity denoted assign object count value total count fractional object-speciﬁc count values. train model minimizing huber loss associated absolute difference predicted count ground truth count evaluation round estimated count nearest integer limit output maximum ground truth count attention baseline second baseline re-implement architecture introduced anderson authors refer updown also teney additional details. focus architecture three main reasons. first represents current state second designed visual representations employ. third exempliﬁes common two-stage approach deploying question-based attention image regions ﬁxed-length visual representation denotes matrix score vectors detected objects denotes attention weights. here function implemented layer denotes element-wise multiplication. training cross entropy loss target given ground-truth count. test time probable count given metrics evaluation. consistency past work report standard test metric accuracy. since accuracy measure degree error also report root-meansquared-error captures typical deviation estimated ground-truth count emphasizes extreme errors. details provided appendix better understand performance models also report performance non-visual baselines. ﬁrst baseline shows performance estimated count always second baseline learns predict count directly linear projection question embedding irlc achieves highest overall accuracy lowest overall rmse test interestingly softcount clearly lags accuracy competitive rmse arguing accuracy rmse redundant. observe result fact irlc less prone small errors slightly prone large errors however whereas updown improves accuracy cost rmse irlc substantially accurate without sacriﬁcing overall rmse. gain insight performance models calculate metrics within development separating data according common subject count training. break questions roughly equal-sized bins representing increasingly uncommon subjects. include subjects never seen training. accuracy rmse across development reported bins figure organizing data reveals main trends. first models perform better asked count subjects common training. second performance improvements offered irlc updown persist groupings development data. introduce novel analysis quantify well counted objects match subject question. perform analysis form generic questions refer object categories coco object detection dataset. take object proposals counted response given question compare ground truth coco labels determine relevant counted object proposals are. metric takes value counted objects perfectly onto category question refers. values around indicate counted objects relevant question. section appendix details grounding quality metric calculated. perform analysis coco categories using images howmany-qa development set. figure compares grounding quality softcount irlc point represents average grounding quality particular coco category. previous metrics grounding quality highest coco categories common training. observe irlc consistently grounds counts objects relevant question softcount. paired t-test shows trend statistically signiﬁcant figure average grounding quality coco object categories measured softcount irlc. point represents coco category colored according common category training histogram showing difference grounding quality irlc softcount. figure examples failure cases common rare subjects example shows output irlc boxes correspond counted objects output updown boxes shaded according attention weights. design irlc inspired ideal interpretable hallmark interpretability ability predict failure modes. argue made approachable requiring irlc identify objects scene chooses count. figure illustrates failure cases exemplify observed trends irlc. particular irlc little trouble counting people encounters difﬁculty referring phrases asked count ties irlc includes sleeve output demonstrating tendency misidentify objects training examples. failures obvious virtue grounded counts point exactly objects irlc counted. comparison attention focus updown identify pattern. attention weights unclear scene elements form basis returned count. indeed models share similar deﬁcits. observe that many cases produce similar counts. however stress without irlc chance observe similarities deﬁcits updown model would difﬁcult identify. appendix includes visualizations comparisons model output including examples irlc uses iterative decision process produce discrete grounded counts present interpretable approach counting visual question answering based learning enumerate objects scene. using able train model make binary decisions whether detected object contributes ﬁnal count. experiment additional baselines control variations visual representations mechanism visuallinguistic comparison. approach achieves state evaluation metrics. addition model identiﬁes objects contribute count. groundings provide traction identifying aspects task model failed learn thereby improve performance also interpretability. references aishwarya agrawal jiasen stanislaw antol margaret mitchell lawrence zitnick devi parikh dhruv batra. visual question answering. international journal computer vision yash goyal tejas khot douglas summers-stay dhruv batra devi parikh. making matter elevating role image understanding visual question answering. cvpr ranjay krishna yuke oliver groth justin johnson kenji hata joshua kravitz stephanie chen yannis kalantidis li-jia david shamma michael bernstein fei-fei visual genome connecting language vision using crowdsourced dense image annotations. international journal computer vision tsung-yi michael maire serge belongie lubomir bourdev ross girshick james hays pietro perona deva ramanan lawrence zitnick piotr doll´ar. microsoft coco common objects context. eccv volodymyr minh adri`a puigdom`enech badia mehdi mirza alex graves harley timothy lillicrap david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. icml dong park lisa anne hendricks zeynep akata bernt schiele trevor darrell marcus rohrbach. attentive explanations justifying decisions pointing evidence. arxiv jianming zhang shugao mehrnoosh sameki stan sclaroff margrit betke xiaohui shen brian price radom´ır mˇech. salient object subitizing. international journal computer vision figure example outputs produced model. softcount objects shaded according fractional count updown similarly shade objects attention focus determine opacity. irlc plot boxes objects selected part count. figure sequential counting irlc. timestep illustrate unchosen boxes pink shade according also show already-selected boxes blue. questions counting sequence terminates meaning returned count questions correct answer. example right ‘correct failure’ case correct answer returned counted objects related question. kinds subtle failures revealed grounded counts. experiment jointly training counting caption grounding. goal caption grounding given objects caption identify object caption describes. identical ﬁrst stages answering counting question lstm encode caption compare objects using scoring function embedding token timestep caption caption length scoring function embedding object denoted relevance object caption encoded score vector project score vector scalar logit apply softmax nonlinearity estimate number object proposals denotes probability caption describes object proposal training randomly select four images batch examples caption grounding create training data region captions visual genome assign caption detected object proposals. compute intersection union ground truth region caption describes coordinates object proposal. assign object proposal largest caption. maximum given caption less ignore training. compute grounding probability caption successfully assign detection train using cross entropy loss averaged captions. weight loss associated caption grounding relative counting loss. considered counting models makes basic architecture encoding question comparing detected objects. model initialized word embeddings glove encoded question lstm hidden size differences model-speciﬁc implementations language module hidden size scoring function determined speciﬁcs optimal settings observed initial experiments. hidden size softcount updown hidden size irlc. observed former models prone overﬁtting whereas irlc beneﬁted increased capacity. training counting optimize using adam softcount updown learning rate decay learning rate training accuracy plateaus. irlc learning rate decay learning rate every iteration. models regularize using dropout apply early stopping based development accuracy training irlc apply sampling procedure times question average losses. weight entropy penalty interaction penalty relative counting loss. penalty weights yield best development accuracy within hyperparameter search performed irlc auxiliary loss. performed grid search determine optimal setting weights auxiliary losses training irlc. observations entropy penalty important balance exploration model training. addition interaction penalty prevents degenerate counting strategies. results grid search suggest auxiliary losses improve performance become unhelpful given much weight case irlc outperforms baseline models across range settings explored. data augmentation visual genome. here compare performance howmanyqa test models trained without additional data visual genome cases performance beneﬁts additional training data average excluding visual genome training data decreases accuracy increases rmse interestingly performance irlc robust loss training data. ordinality updown output. whereas training objectives softcount irlc intrinsically reﬂect ordinal nature counts true updown. example loss experienced softcount irlc reﬂect degree error estimated count ground truth target; however updown trained place high probability mass ground truth value examine patterns output count probabilities updown whether model learns ordinal representation despite non-ordinal training objective. figure illustrates trends. estimated count less second-most probable count frequently adjacent probable count. estimated count larger probability distribution less smooth figure average count probability updown grouped according estimated count. cumulative distribution absolute difference predicted counts shown likely count less greater equal probability distributions much less smooth estimated count large. second-most probable count often considerably different probable count. result suggests updown learns ordinality lower count values fails generalize concept larger counts accuracy. dataset includes annotations human reviewers question. accuracy given answer depends many provided answers agrees with. scored correct least humans answers agree answer’s accuracy averaged -choose- human answers. described main text consider examples consensus answer range labels calculate accuracy regardless whether individual labels deviate range. thee accuracy values report taken average accuracy examples. rmse. metric simply quantiﬁes typical deviation model count groundtruth. across calculate metric predicted ground truth counts respectively question rmse measurement error lower better. grounding quality. introduce evaluation method quantifying relevant objects counted model type object asked count. evaluation metric takes advantage ground truth labels included coco dataset. labels annotate object instance different categories images development set. make glove embeddings compute semantic similarity. glove denote glove embedding category first assign coco categories object proposals used counting. object proposal object coco labels largest iou. assign object proposal category coco object otherwise assign object proposal background. below denote category assigned object proposal image second coco categories present image category build question softcount irlc count returned response question object proposal’s inferred count value number object proposals image count value given proposal count values compute weighted semantic similarity assigned object proposal categories question category semantic similarity estimated product embeddings assigned category question category. corresponds background category replace embedding vector zeros.", "year": 2017}