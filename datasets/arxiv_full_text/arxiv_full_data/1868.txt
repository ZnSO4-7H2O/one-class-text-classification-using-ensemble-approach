{"title": "Fine-tuned Language Models for Text Classification", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Transfer learning has revolutionized computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Fine-tuned Language Models (FitLaM), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a state-of-the-art language model. Our method significantly outperforms the state-of-the-art on five text classification tasks, reducing the error by 18-24% on the majority of datasets. We open-source our pretrained models and code to enable adoption by the community.", "text": "transfer learning revolutionized computer vision existing approaches still require task-speciﬁc modiﬁcations training scratch. propose fine-tuned language models effective transfer learning method applied task introduce techniques ﬁne-tuning state-of-the-art language model. method signiﬁcantly outperforms state-of-the-art text classiﬁcation tasks reducing error majority datasets. opensource pretrained models code enable adoption community. transfer learning large impact computer vision applied models rarely trained scratch instead ﬁne-tuned models pretrained imagenet ms-coco datasets text classiﬁcation category natural language processing tasks many important real-world applications spam fraud detection emergency response commercial document classiﬁcation legal discovery learning lagged behind transfer fine-tuning pretrained word embeddings simple transfer learning technique targets model’s ﬁrst layer outsized impact practice used state-of-theart models. light beneﬁts pretraining able better randomly initializing remaining parameters models. recent approaches concatenate embeddings derived tasks language modeling machine translation input different layers approaches however still train main task model scratch treat pretrained embeddings ﬁxed parameters limiting usefulness. arguably successful transfer learning technique fulﬁll similar criteria counterpart method able leverage large amounts available data; utilize task optimized independently leading downstream improvements; rely single model used as-is tasks; easy practice. propose fine-tuned language models effective form transfer learning fulﬁlls aforementioned criteria. method uses language modeling task almost inﬁnite amounts data stream recent advances pushing state-of-the-art. seamlessly integrates large amounts external data well in-domain data pretraining. fitlam relies simple recurrent neural network without additional modiﬁcations; augment model taskspeciﬁc linear layers accounting small number parameters relative existing approaches. propose ﬁne-tuning technique discriminative ﬁne-tuning ﬁne-tunes lower layers lesser extent higher layers order retain knowledge acquired language modeling. furthermore introduce several techniques good ﬁne-tuning performance faster training. evaluate transfer learning method widely-studied text classiﬁcation tasks various sizes types signiﬁcantly outperform highly task-speciﬁc previous work existing transfer learning approaches. transfer learning features deep neural networks observed transition task-speciﬁc general ﬁrst last layer reason work focuses transferring last layers model sharif razavian achieve state-of-theart results using features imagenet model input simple classiﬁer. recent years hypercolumns recently methods proposed beyond transferring word embeddings. prevailing approach pretrain embeddings capture additional context tasks. embeddings concatenated either word embeddings inputs intermediate layers. approach known hypercolumns used peters anonymous mccann language modeling machine translation respectively pretraining. similar approach used conneau learning sentence representations. hypercolumns nearly entirely superseded end-to-end ﬁne-tuning multi-task learning related direction multi-task learning approach taken language modeling objective model trained jointly main task model. could potentially combined fitlam. fine-tuning fine-tuning ﬁne-tunes pretrained model target task used successfully certain tasks sentiment analysis distant supervision following show ﬁne-tuning superior hypercolumns general-purpose transfer learning method nlp. transfer instantiations here interested general inductive transfer learning setting given static source task target task would like improve performance main questions transfer learning method use; source task transfer. settings insights inform reasoning. case ﬁne-tuning facilitate wide adoption transfer learning method practical efﬁcient effective. requires tasks trained scratch every time often requires careful weighting task-speciﬁc objective functions success shown ﬁne-tuning clearly superior hypercolumns. given sufﬁcient data available ﬁne-tuning representations outperforms freezing them; similar observations made word embeddings furthermore pretrained weights capturing certain properties natural language available using random weights initialization appears suboptimal choice case language modeling ideal pretraining task similar properties imagenet provide abundant data enable induce representations helpful many tasks. formally pretraining task allow induce hypothesis space useful many tasks possibly task largest amount labeled data reliance labeled data imposes upper bound precludes using data target task distribution training order bridge domain shift. language modeling seen prototypical task language models capture many facets language long-term dependencies sentiment relevant downstream tasks addition data available near-unlimited quantities majority target domains. thus argue pretraining language model ﬁne-tuning target task useful transfer learning method nlp. empirically validate section approach—which introduce next section—outperforms existing transfer learning methods signiﬁcantly across text classiﬁcation tasks. propose fine-tuned language models pretrains highly-optimized language model large general-domain corpus ﬁne-tunes target task. regular lstm various highly tuned regularization strategies. analogous downstream performance improved using higherperformance language models future. fitlam consists following steps general-domain pretraining; target task ﬁne-tuning; target task classiﬁer ﬁnetuning. discuss following sections. pretrain language model wikitext- consisting preprocessed wikipedia articles. pretraining additional diverse datasets boost performance. stage expensive needs performed once vastly beneﬁts following stages improving performance convergence downstream models. make weights pretrained model available facilitate experimentation. matter diverse general-domain data used pretraining data target task likely come different distribution. thus ﬁne-tune language model training examples target task. given pretrained general-domain stage converges much faster needs adapt idiosyncrasies target data allows train robust even small datasets. gradual unfreezing could ﬁne-tune parameters pretrained language model time found useful gradually unfreeze model starting last layer contains least general knowledge ﬁrst unfreeze last layer ﬁne-tune unfrozen layers. additionally unfreeze next lower frozen task classiﬁer critical part transfer learning method. overly aggressive ﬁne-tuning cause catastrophic forgetting eliminating beneﬁt information captured language modeling; cautious ﬁne-tuning lead slow convergence different layers capture different types information ﬁnetuned different extents. ﬁne-tuning entire model costly networks layers common practice ﬁne-tune initially last hidden layer unfreeze additional layers ﬁne-tuning. simplest approach treating different layers appropriately ﬁne-tune model layer time analogous greedy layer-wise training ‘chain-thaw’ however introduces sequential requirement hindering parallelism requires multiple passes dataset resulting overﬁtting small datasets. reason propose efﬁcient ﬁne-tuning method discriminative ﬁne-tuning. instead using learning rate layers model discriminative ﬁne-tuning allows tune layer different learning rates. context regular stochastic gradient descent update model’s parameters time step looks like following learning rate gradient regard model’s objective function. discriminative ﬁne-tuning split parameters contains parameters model l-th layer number layers model. similarly obtain learning rate l-th layer. fine-tuning annealing observed best results ﬁne-tuning aggressive cosine annealing schedule lowering learning rate several epochs train epoch lower learning rate batch following schedule warm-up reverse annealing also found useful ﬁrst train layers gradually increasing i.e. reverse annealed learning rate epoch warm-up gradual unfreezing. similar approach used target task classiﬁer ﬁne-tuning augment pretrained language model additional linear blocks. block uses batch normalization dropout relu activations intermediate layers softmax activation outputs probability distribution target task classes last layer. note parameters task-speciﬁc classiﬁer layers ones need learned scratch. ﬁrst linear layer takes input pooled last hidden layer states. concat pooling signal text classiﬁcation tasks often contained words occur anywhere document. input documents consist hundreds words information lost consider last hidden state model. reason concatenate hidden state last time step document max-pooled mean-pooled representation hidden states many time steps memory language models trained backpropagation time enable gradient propagation large input sequences. order make ﬁne-tuning classiﬁer large documents feasible propose bptt text classiﬁcation divide document ﬁxedlength batches size beginning batch model initialized ﬁnal state previous batch; keep track hidden states mean max-pooling; gradients back-propagated batches whose hidden states contributed classiﬁer prediction document. practice variable length backpropagation sequences similar existing work limited ﬁnetuning uni-directional language model. experiments pretrain forward backward ﬁne-tune classiﬁer independently using bptc average predictions. datasets tasks evaluate method widely-studied datasets different sizes used state-of-the-art text classiﬁcation transfer learning approaches instances three common text classiﬁcation tasks sentiment analynote already achieve state-of-the-art results datasets unidirectional lms. bidirectional generally improve performance. report detailed ablations future work. pre-processing pre-processing earlier work addition allow language model capture aspects might relevant classiﬁcation special tokens upper-case words elongation repetition. hyperparameters awd-lstm language model embedding size layers hidden activations layer bptt batch size apply dropout layers layers input embedding layers embedding layers weight dropout hidden-to-hidden matrix. classiﬁer hidden layer size adam instead default similar otherwise hyperparameters practices used of-the-art transfer learning method nlp. ag-news yelp dbpedia datasets compiled zhang compare state-of-the-art text categorization method johnson zhang show test accuracy scores imdb trec- datasets used mccann table method outperforms cove state-of-the-art transfer learning method based hypercolumns well state-of-theart datasets. imdb reduce error dramatically regard cove state-of-the-art respectively. promising particular existing state-of-the-art requires complex architectures multiple forms attention sophisticated embedding schemes method employs standard bi-lstm dropout. trec- improvement—similar improvements state-of-the-art approaches—is statistically signiﬁcant owing small size test consisting examples. recommend cease using dataset evaluation text classiﬁcation algorithms. however competitive performance small trec dataset still demonstrates ﬁne-tuning language model target task classiﬁer feasible even small datasets. note despite pretraining orders magnitude less data million sentence pairs used mccann consistently outperform approach datasets. show test error rates larger ag-news dbpedia yelp-bi datasets used johnson zhang table method outperforms state-of-the-art signiﬁcantly. ag-news observe similarly dramatic error reduction compared state-of-the-art. dbpedia yelp-bi reduce error respectively. method still requires tricks manual learning rates dropout weights achieve best performance analogous alexnet necessary ﬁrst step lead wave innovation. conﬁdent ﬁne-tuning language models become robust research focuses improving transfer learning nlp. important step path careful ablation studies understand impact component models training procedures described here. given transfer learning under-explored many future directions possible. possible direction improve language model pretraining task make scalable imagenet predicting fewer classes incurs small performance drop focusing predicting frequent words might retain performance speeding training. language modeling could also augmented additional tasks multi-task learning fashion enriched additional supervision e.g. syntax-sensitive dependencies create model general better suited certain downstream tasks. another direction apply method novel tasks models. extension sequence labeling tasks straightforward tasks entailment question answering employ complex interactions require novel ways pretrain ﬁne-tune. finally ﬁne-tuning integral component transfer learning received scarce attention even common benchmarks still evaluate ability train model scratch rather ﬁne-tuning pretrained model. creating benchmarks ﬁne-tuning enable develop sophisticated ﬁne-tuning methods allow unlock full potential pretrained models novel tasks. proposed fitlam effective transfer learning method discriminative ﬁnetuning efﬁcient ﬁne-tuning method tunes different layers different extents order avoid catastrophic forgetting. introduced bptc method back-propagate classiﬁer loss output sequence size well several techniques good ﬁnetuning performance fast training. method signiﬁcantly outperformed existing transfer learning techniques state-of-the-art representative text classiﬁcation tasks. total demonstrated beneﬁt transfer learning hope results catalyze developments transfer learning nlp.", "year": 2018}