{"title": "Decomposition Bounds for Marginal MAP", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.ML"], "abstract": "Marginal MAP inference involves making MAP predictions in systems defined with latent variables or missing information. It is significantly more difficult than pure marginalization and MAP tasks, for which a large class of efficient and convergent variational algorithms, such as dual decomposition, exist. In this work, we generalize dual decomposition to a generic power sum inference task, which includes marginal MAP, along with pure marginalization and MAP, as special cases. Our method is based on a block coordinate descent algorithm on a new convex decomposition bound, that is guaranteed to converge monotonically, and can be parallelized efficiently. We demonstrate our approach on marginal MAP queries defined on real-world problems from the UAI approximate inference challenge, showing that our framework is faster and more reliable than previous methods.", "text": "marginal inference involves making predictions systems deﬁned latent variables missing information. signiﬁcantly difﬁcult pure marginalization tasks large class efﬁcient convergent variational algorithms dual decomposition exist. work generalize dual decomposition generic power inference task includes marginal along pure marginalization special cases. method based block coordinate descent algorithm convex decomposition bound guaranteed converge monotonically parallelized efﬁciently. demonstrate approach marginal queries deﬁned real-world problems approximate inference challenge showing framework faster reliable previous methods. probabilistic graphical models bayesian networks markov random ﬁelds provide useful framework powerful tools machine learning. given graphical model inference refers answering probabilistic queries model. three common types inference tasks. ﬁrst max-inference maximum posteriori tasks probable state joint probability; exact approximate inference widely used structured prediction sum-inference tasks include calculating marginal probabilities normalization constant distribution play central role many learning tasks finally marginal tasks mixed inference problems generalize ﬁrst types marginalizing subset variables optimizing remainder. tasks arise latent variable models many decision-making problems three inference types generally intractable; result approximate inference particularly convex relaxations upper bounding methods great interest. decomposition methods provide useful computationally efﬁcient class bounds inference problems. example dual decomposition methods give class easy-toevaluate upper bounds directly optimized using coordinate descent subgradient updates methods easy ensure convergence objective monotonically decreasing resulting bounds used either stand-alone approximation methods component search summation problems notable decomposition bound tree-reweighted bounds partition function combination trees bounds useful joint inference learning frameworks allowing learning approximate inference framed joint optimization model parameters decomposition bound often leading efﬁcient learning however fewer methods developed marginal problems. work deveop decomposition bound number desirable properties generality bound sufﬁciently general applied easily marginal map. any-time yields bound point optimization used anytime way. monotonic convergent computational effort gives strictly tighter bounds; note particularly important high-width approximations expensive represent update. allows optimization parameters including weights fractional counting numbers approximation; parameters often signiﬁcant effect tightness resulting bound. compact representation within given class bounds using fewer parameters express bound reduces memory typically speeds optimization. organize rest paper follows. section gives background notation followed connections related work section derive decomposed bound section present coordinate descent algorithm monotonically tightening section report experimental results section conclude paper section subsets variables associated factor partition function. associate undirected graph mapping node adding edge exists node neighbors then subset cliques evaluation given often involves different types inference tasks. marginalization sum-inference tasks perform conﬁgurations calculate partition function marginal probabilities probability observed evidence. hand maximum posteriori max-inference tasks perform joint maximization conﬁgurations highest probability maxx generalization maxsuminference marginal mixed-inference interested ﬁrst marginalizing subset variables maximizing remaining variables non-negative function temperature weight parameter. power reduces standard approaches maxx deﬁne power equal operator. power helpful unifying maxsuminference well marginal speciﬁcally apply power sums different weights variable along predeﬁned elimination order deﬁne weighted partition function note value depends elimination order unless weights equal. obviously includes marginal special case setting weights representation provides useful tool understanding deriving algorithms general inference tasks especially marginal relatively efﬁcient algorithms exist. variational upper bounds partition function along algorithms providing fast convergent optimization widely studied last decade. dual decomposition linear programming methods become dominating approach numerous optimization techniques methods tighten approximations summation problems upper bounds derived tree-reweighted family convex bounds generally conditional entropy decompositions bounds framed optimizing convex combination tree-structured models dual representation message-passing belief propagation algorithm. illustrates basic tension resulting bounds primal form inefﬁcient maintains weight parameters tree large number trees required obtain tight bound; uses memory makes optimization slow. hand dual free energy form uses parameters optimize possible spanning trees resulting optimization guaranteed bound convergence making difﬁcult anytime fashion. similarly gradient weights correct convergence making difﬁcult optimize parameters; implementations simply adopt ﬁxed weights. thus algorithms satisfy desirable properties listed introduction. example many works developed convergent message-passing algorithms convex free energies however optimizing dual provide bound convergence representation constraints counting numbers facilitate optimizing bound parameters. optimize counting numbers adopt restrictive free energy form requiring positive counting numbers entropies; cannot represent marginal whose free energy involves conditional entropies hand working primal domain ensures bound usually cost enumerating large number trees. heuristically select small number trees avoid inefﬁcient focus trying speed updates given collection trees. another primal bound weighted mini-bucket represent large collection trees compactly easily applied marginal using weighted partition function viewpoint however existing optimization algorithms non-monotonic often fail converge especially marginal tasks. focus variational bounds many non-variational approaches marginal well. provide upper bounds marginal reordering order variables eliminated using exact inference reordered join-tree; however exponential size treewidth easily become intractable. give approximation closely related mini-bucket bound marginal map; however unlike mini-bucket bounds cannot improved iteratively. true algorithm also strong dependence treewidth. examples marginal algorithms include local search markov chain monte carlo methods section develop general form upper bound provide efﬁcient monotonically convergent optimization algorithm. bound based fully decomposing graph disconnected cliques allowing efﬁcient local computation still tight bound large collection spanning trees weights shifting variables chosen optimized properly. bound reduces dual decomposition inference applicable general mixed-inference settings. despite term dual decomposition used tasks work refer decomposition bounds primal bounds since viewed directly bounding result variable elimination. contrast example linear programming relaxation bounds result optimization. ranked increasing index consisting elimination order used left-hand side. proof details found section supplement. advantage bound decomposes joint power product independent power sums smaller cliques signiﬁcantly reduces computational complexity enables parallel computation. order increase ﬂexibility upper bound introduce cost-shifting reparameterization variables variable-factor pair optimized provide much tighter upper bound. note rewritten bound convex w.r.t. cost-shifting variables weights enabling efﬁcient optimization algorithm present section discuss section shifting variables correspond lagrange multipliers enforce moment matching condition. straightforward bound reduces dual decomposition applied inference hence hand connection sum-inference bounds seen clearly dual representation marginal conditional entropies global elimination order proof details found section supplement. useful compare theorem dual representations. non-negatively weighted conditional entropies bound clearly convex within general class conditional entropy decompositions unlike generic simple efﬁcient primal form comparing figure illustrating bound grid. uses covering tree minimal number splits cost-shifting. decomposition splits graph small cliques introducing additional cost-shifting variables allowing easier monotonic optimization. primal splits graph many spanning trees requiring even cost-shifting variables. note three bounds attain tightness optimization. dual form theorem bound tight hence class bounds attainable duality-based forms rather conditional entropies; converted resulting counting numbers differences weights obfuscates convexity makes harder maintain relative constraints counting numbers optimization makes counting numbers negative finally like variational bounds dual form inner maximization hence guaranteed bound optimum. contrast primal bound similar primal form except individual regions single cliques rather spanning trees graph fraction weights associated region vectors rather single scalar. representation’s efﬁciency seen example figure shows grid model three relaxations achieve bound. assuming states variable ignoring equality constraints decomposition figure uses cost-shifting parameters weights. slightly efﬁcient parameters weights lack decomposition makes parallel monotonic updates difﬁcult. hand equivalent primal uses spanning trees shown figure parameters weights. increased dimensionality optimization slows convergence updates non-local requiring full message-passing sweeps involved trees section propose block coordinate descent algorithm minimize upper bound w.r.t. shifting variables weights algorithm monotonic convergence property allows efﬁcient distributable local computation full decomposition bound. framework allows generic powered-sum inference including max- sum- mixed-inference special cases setting different weights. start deriving gradient w.r.t. show zero-gradient equation w.r.t. simple form moment matching enforces consistency singleton beliefs related clique beliefs weights enforces consistency marginal conditional entropies. theorem zero-gradient w.r.t. proof details found section supplement. matching condition enforces belong local consistency polytope deﬁned theorem similar moment matching results appear commonly variational inference algorithms also derive gradient weights based free energy form correct optimization; form holds point enabling efﬁcient joint optimization derive block coordinate descent method algorithm minimize bound sweep nodes update block neighborhood parameters ﬁxed. algorithm applies update types depending whether variables zero weight nodes derive closed-form coordinate descent rule associated shifting variables δni; nodes require optimize since ﬁxed zero. nodes lack closed form update optimize local gradient descent combined line search. lack closed form coordinate update nodes mainly order power sums different weights cannot exchanged. however gradient descent inner loop still efﬁcient gradient evaluation involves local variables clique closed-form update. associated zero gradient equation analogous star update dual decomposition detailed derivation shown proposition supplement. update calculated cost number states clique size computing saving shared updating δni. furthermore updates different nodes independent directly connected clique makes easy parallelize coordinate descent process partitioning graph independent sets parallelizing updates within set. nodes closedlocal gradient descent. minimize upper bound. however form solution fully decomposed form gradient w.r.t. evaluated efﬁciently local computation parallelized nonadjacent nodes. handle normalization constraint exponential gradient descent ={j∈ implementation gradient step size steps backtracking line search using armijo rule works well practice. advanced optimization methods l-bfgs newton’s method also applicable. section demonstrate algorithm real-world graphical models recent inference challenges including diagnostic bayesian networks variables domain sizes respectively several mrfs pedigree analysis variables domain size clique size construct marginal problems models randomly selecting half variables nodes rest nodes. implement several algorithms optimize primal marginal bound including algorithm ibound uses cliques ﬁxed point heuristic optimization off-the-shelf l-bfgs implementation directly optimizes decomposed bound. comparison also computed several related primal bounds including standard mini-bucket elimination reordering limited computational limits also tried found bounds extremely loose. decoding difﬁcult marginal joint map. local decoding procedure standard dual decomposition however evaluating objective involves potentially difﬁcult making hard score decoding. reason evaluate score decoding show recent decoding rather best simulate behavior practice. figure figure compare convergence different algorithms deﬁne iteration algorithm correspond full sweep graph order time complexity iteration deﬁned algorithm full forward backward message pass algorithm l-bfgs joint quasi-newton step variables. elimination order obtained weighted-min-ﬁll heuristic constrained eliminate nodes ﬁrst. diagnostic bayesian networks. figure shows converges quickly monotonically networks converge without proper damping; http//graphmod.ics.uci.edu/uai/evaluation/report/benchmarks. instances tested many zero probabilities make ﬁnding lower bounds difﬁcult; since mas’ figure marginal results randomly selected max-nodes plot upper bounds different algorithms across iterations; objective function decoded solutions also shown beginning equal zero probabiliy. experimented different damping ratios found slower even best damping ratio found works best damping ratio still signiﬁcantly slower gdd). also gives better decoded marginal solution provide much tighter bound non-iterative mini-bucket elimination reordered elimination methods. genetic pedigree instances. figure shows similar results pedigree instances. again outperforms even best possible damping out-performs non-iterative bounds iteration work propose class decomposition bounds general powered-sum inference capable representing large class primal variational bounds much computationally efﬁcient. unlike previous primal bounds bound decomposes computations small local cliques increasing efﬁciency enabling parallel monotonic optimization. derive block coordinate descent algorithm optimizing bound cost-shifting parameters weights generalizes dual decomposition enjoy similar monotonic convergence property. taking advantage monotonic convergence algorithm widely applied building block improved heuristic construction search efﬁcient learning algorithms. acknowledgments work sponsored part grants iis- iis-. alexander ihler also funded part united states force contract fa--c- darpa ppaml program. dechter rish. mini-buckets general scheme bounded inference. jacm domke. dual decomposition marginal inference. aaai doucet godsill robert. marginal maximum posteriori estimation using markov chain hardy littlewood polya. inequalities. cambridge university press hazan peng shashua. tightening fractional covering upper bounds partition function ihler flerova dechter otten. join-graph based cost-shifting schemes. jancsary matz. convergent decomposition solvers free energies. aistats kiselev poupart. policy optimization marginal probabilistic inference generative ihler. bounding partition function using h¨older’s inequality. icml ihler. variational algorithms marginal map. jmlr marinescu dechter ihler. and/or search marginal map. maua campos. anytime marginal maximum posteriori inference. icml meek wexler. approximating max-sum-product problems using multiplicative error bounds. sontag jaakkola. tree block coordinate descent graphical models. aistats sontag meltzer globerson jaakkola weiss. tightening relaxations using werner. linear programming approach max-sum problem review. tpami yarkony fowlkes ihler. covering trees lower-bounds quadratic assignment. cvpr experiment ising grid directly optimizes primal bound thus guaranteed upper bound partition function even algorithm converges enabling desirable any-time property. contrast typical implementations tree reweighted belief propagation optimize dual free energy function guaranteed bound convergence. illustrate point using experiment ising grid parameters generated normal ditribution half nodes selected max-nodes marginal map. figure shows free energy objective upper bounds across iterations; violate upper bound property convergence always give valid upper bounds. figure sum-inference marginal results ising model iteration different algorithms corresponds full sweep graph. note dual formulation bound convergence; example iteration objective function true results diagnostic bayesian networks addtion marginal results main text vary percentage max-nodes generating marginal problems; reported results figure best bound obtained different algorithms ﬁrst iterations. cases gdd’s results good better wmb. wmb-. appears work well sum-only max-only problems i.e. percentage max-nodes equals respectively performs poorly intermediate settings. heavily damped wmb-. wmb-. work better average much slower convergence. results pedigree linkage analysis test algorithm additional models pedigree linkage analysis inference challenge. construct marginal problems randomly selected nodes max-nodes report results figure algorithm consistently outperforms best possible damping ratio. extensions junction graph bound main text uses standard factor graph representation cost-shifts deﬁned variable-factor pair functions single variables extend bound general shifting parameters using junction graph representation; allows exploit higher order clique structures leading better performance. junction graph clusters separators. assume reparameterized form figure marginal results diagnostic bayesian networks. report best results obtained iterations marginal problems constructed randomly selecting different percentages max-nodes. figure marginal inference additional pedigree linkage analysis models. randomly selected nodes max-nodes models. tune damping rate omit wmb-. plot wmb-. already diverged. clusters include node obviously separators include node earlier bound main text viewed special case special junction graph whose separators consist single variables block coordinate descent algorithm similar algorithm derived optimize junction graph bound. case sweep separators perform block coordinate update s|∀c iteration. similarly algorithm derive close form update separators all-zero weights perform local gradient descent otherwise. elements independently optimized. then tightening reparameterizationθ {θiθα} order commuted according strong duality concave inner minimization minθθ linear program turns solved analytically. this given relationship betweenθ rewrite linear program weights optimized bound matches bound optimum weights. simple weight initialization method matches bound uniform weights mini-bucket often gives satisfactory result; similar procedure used match bound general weights section ﬁrst nodes visit nodes along elimination order divide xi’s neighborhood cliques {α|α groups children cliques xα\\i already eliminated other parent cliques children cliques across parent cliques.", "year": 2015}