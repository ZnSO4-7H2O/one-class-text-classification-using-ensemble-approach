{"title": "Local minima in training of neural networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "There has been a lot of recent interest in trying to characterize the error surface of deep models. This stems from a long standing question. Given that deep networks are highly nonlinear systems optimized by local gradient methods, why do they not seem to be affected by bad local minima? It is widely believed that training of deep models using gradient methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minimum. It is known that such results hold under very strong assumptions which are not satisfied by real models. In this paper we present examples showing that for such theorem to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. We look at the particular case of finite size datasets. We demonstrate that in this scenario one can construct counter-examples (datasets or initialization schemes) when the network does become susceptible to bad local minima over the weight space.", "text": "training neural network often formulated task ﬁnding good minimum error surface graph loss expressed function weights. growing popularity deep learning classical problem studying error surfaces neural networks focus many researchers. stems long standing question. given deep networks highly nonlinear systems optimized local gradient methods seem affected local minima? much often observed practice training deep models using gradient methods works well little understood happens. research efforts dedicated recently proving good behavior training neural networks. paper adapt complementary approach studying possible obstacles. present several concrete examples datasets cause error surface strongly suboptimal local minimum. deep learning fast growing subﬁeld machine learning many impressive results. images classiﬁed super-human accuracy szegedy quality machine translation reaching heights bahdanau reinforcement learning deep architectures successfully used learn play atari games game always case fast-progressing domain practical application theoretical understanding moving forwards slower fast forefront empirical success. training complex models spite fact training relies non-convex functions optimized using local gradient descent methods. light empirical results many efforts made explain training deep networks works well authors believe equally important understand training neural networks behaves well also understand wrong. empirical examples things always working well. learning susceptible adversarial examples neural networks identifying road stop signs interior refrigerator invisible human eyes perturbations make perfectly good model suddenly misclassify well known examples. additionally numerous tricks trade like batch normalization skip connections invented address slow convergence poor results learning. because despite optimism gradient descent often working well enough. approach paper look fundamental reasons training behaving well. goal construct small possible datasets lead emergence local minima. error surface extremely complicated mathematical object. authors believe strategy improving understanding structure error surface build knowledge base around constructing examples ones presented serves main purposed. first help formulate necessary assumptions behind theorems showing convergence neural network good minima assumptions exclude datasets. second goal practical inspire design better learning algorithms. hypothesis learning well behaved neural networks forward dauphin refer local minima hypothesis. observation work intuitions low-dimensional spaces usually misleading moving high-dimensional spaces. work makes connection profound results obtained statistical physics. particular fyodorov williams bray dean showed using replica theory random gaussian error functions particular friendly structure. namely looks critical points function plots error versus index critical point points align nicely monotonically increasing curve. points index roughly performance critical points high error implicitly large number negative eigenvalue means saddle points. claim dauphin structure holds neural networks well become large enough. provides appealing conjecture learning results well performing models also reliably. similar claim forward sagun intuitions also traced back earlier work baldi hornik shows single linear intermediate layer local minima saddle points global minimum. extensions early results found saxe choromanska provides study conjecture rests recasting neural network spin-glass model. obtain result several assumptions need made authors work time acknowledged realistic practice. line attack taken kawaguchi derivations hold practical case ﬁnite size datasets ﬁnite size models. goodfellow argues provides empirical evidence moving original initialization model along straight line solution loss seems monotonically decreasing speaks towards apparent convexity problem. soudry carmon safran shamir also look error surface neural network providing theoretical arguments error surface becoming well-behaved case overparametrized models. different view presented tegmark shamir underlying easiness optimizing deep networks simply rest emerging structures highdimensional spaces rather tightly connected intrinsic characteristics data models section present examples local minima. speaking precisely present examples datasets architectures training using gradient descent converge suboptimal local minumum. main goals authors show sigmoid-based neural network suboptimal ﬁnite local minimum. turned example widely known community agreement even whether minimum could exist all. ﬁnite local minimum understand local minimum produced ﬁnite weights. minimum caused sigmoids saturating trying become step-function. sense minima presented section ﬁnite minima. task constructing example turned surprisingly difﬁcult. ability sigmoidbased neural network wiggle sophisticated traps authors creating impressive challenging. ﬁrst failed attempts made realize nature successful example would geometric analytic time. deadlocks sigmoid-based neural network geometric conﬁguration points also precise cross-ratios distances them. successful construction presented example combination studying failed attempts generated guesswork trying block escape routes gradient descent data space. close enough conﬁguration points deduced gradient descent applied datapoints order minimize length gradient weights space loss function. procedure modiﬁes dataset weights becomes critical point error surface starting randomly chosen dataset almost surely produces saddle point instead minimum. using close enough conﬁguration yields higher chance ﬁnding true local minimum. authors constructed several examples local minima sigmoid-based neural network using datapoints. today know different examples -point datasets lead suboptimal minimum. authors conjecture minimum amount points required deadlock architecture possible construct example using less points. -point examples geometric conﬁguration resembling ﬁgure shape presented figure remark worth noting claiming global minimum. fact even minimum all. note high values happens ﬁnal sigmoid struggling approximate step function. common phenomenon training neural networks training converge minimum gets stopped trying converge point inﬁnity. remark point rectiﬁer-based models facto standard applications neural networks. section present examples local minima regression using single layer hidden rectiﬁer units -dimensional data remark relu-s activation function simply linear projection input. hence modes operation either linear regime saturated regime. obviously gradient ﬂows saturated unit hence particular simple mechanism locking network suboptimal solution subset datapoints units saturated gradient ﬁtting points. refer points blind spot model explore phenomenon properly section remark examples presented section beyond relying solely blind-spots model. proposition dataset deﬁned equation point local minimum global minimum. proof. holds ++++ ++++ thus cannot global minimum. remains prove local minimum i.e. |δw| |δb| |δv| |δc| sufﬁciently small. need consider cases relu activated case again). note assumption |δw| |δb| |δv| |δc| sufﬁciently small relu always activated deactivated remark point minimum strict minimum isolated lies -dimensional manifold instead. remark following examples show blind spots reason model stuck suboptimal solution. even surprisingly also show blind spots completely absent local optima time present global solution. proposition consider dataset given points then rectiﬁer network hidden units squared error loss weights global minimum weights region region solutions proposition constructed best points assigned given region difference number hidden units used describe them. local optimum neurons used describe region describes region symmetrically better solution assigns neurons region region conjecture conjecture core idea behind construction generalized high-dimensional problems. section look slight variation theoretically well-studied datasets problem. exploiting observations made failure modes observed problem able construct similar dataset ﬂattened results suboptimal learning dynamics. dataset formed four datapoints positive class given negative figure analyze ﬁrst observation solve task hidden units full batch methods always succeed. replacing gradient descent aggressive optimizers like adam seem help rather tends make likely stuck suboptimal solutions table convergence rate network random initializations simple -dimensional datasets using either adam gradient descent optimizer. comparison regular fxor ﬂattened compared problem seems ﬂattened problem poses even issues especially relu units hidden units still gets runs training error particular observation contrast good solutions model fails dataset behaviour close datapoints almost linear. argue hence failure mode might come datapoints concentrated linear region model hence forcing model suboptimally points. remark examples used relu sigmoid activation functions common used practice. similar examples constructed different activation functions however constructions need modiﬁcations technically complicated. section prove formally seemingly obvious often overlooked fact regression dataset rectiﬁer model least local minimum. construction relies fact dataset ﬁnite. such bounded compute conditions weights given layer model datapoint units layer saturated. furthermore show obtain better solution reached state. formalization result follows. combining lemmata yields corollary holds wnhn− then training model output y+...+yn denote a+...+al deﬁnition dataset decent exists theorem wnhn i-s. proof. claim direct consequence corollary remains prove ii). sufﬁcient show example weighs n=). point exists assumption dataset decent. hyperplane passing none points lies exists vector deﬁne ﬁrst second third again ﬁrst layer neurons remaining rows equal zero. choose ﬁrst three biases respectively. denote xr}) choose matrix whose ﬁrst rows equal finally choose bias vector network layer output relu relu relu every yields either yielding yielding case network hidden layer denote previous results provide insightful description error surface deep models general assumptions divorced speciﬁcs architecture data. analysis valuable building intuition also development tools studying neural networks provides facade problem. work focused constructing scenarios learning fails hope help setting right assumptions convergence theorems neural networks practical scenarios. similar tegmark forward hypothesis learning well behaved conditioned structure data. understanding structure error surface extremely challenging problem. believe such agreement scientiﬁc tradition approached gradually building related knowledge base trying obtain positive results studying obstacles limitations arising concrete examples. dauphin yann pascanu razvan gulcehre caglar kyunhyun ganguli surya bengio identifying attacking saddle point problem high dimensional non-convex fawzi alhussein moosavi-dezfooli seyed-mohsen frossard pascal. robustness classiﬁers adversarial random noise. sugiyama luxburg guyon garnett advances neural information processing systems fyodorov williams ian. replica symmetry breaking condition exposed random matrix calculation landscape complexity. journal statistical physics kaiming zhang xiangyu shaoqing jian. delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation. ieee international conference computer vision iccv santiago chile december mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg humanlevel control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. nguyen yosinski jason clune jeff. deep neural networks easily fooled high conﬁdence predictions unrecognizable images. ieee conference computer vision pattern recognition cvpr boston june saxe andrew mcclelland james ganguli surya. learning hierarchical category structure deep neural networks. proceedings annual meeting cognitive science society saxe andrew mcclelland james ganguli surya. exact solutions nonlinear dynamics learning deep linear neural network. international conference learning representations silver david huang maddison chris guez arthur sifre laurent driessche george schrittwieser julian antonoglou ioannis panneershelvam veda lanctot marc dieleman sander grewe dominik nham john kalchbrenner sutskever ilya lillicrap timothy leach madeleine kavukcuoglu koray graepel thore hassabis demis. mastering game deep neural networks tree search. nature january sutskever ilya vinyals oriol quoc sequence sequence learning neural networks. proceedings international conference neural information processing systems nips’ szegedy christian vanhoucke vincent ioffe sergey shlens jonathon wojna zbigniew. rethinking inception architecture computer vision. ieee conference computer vision pattern recognition cvpr vegas june yonghui schuster mike chen zhifeng quoc norouzi mohammad macherey wolfgang krikun maxim yuan macherey klaus klingner jeff shah apurva johnson melvin xiaobing kaiser lukasz gouws stephan kato yoshikiyo kudo taku kazawa hideto stevens keith kurian george patil nishant wang young cliff smith jason riesa jason rudnick alex vinyals oriol corrado greg hughes macduff dean jeffrey. google’s neural machine translation system bridging human machine translation. corr abs/. http//arxiv.org/abs/.", "year": 2016}