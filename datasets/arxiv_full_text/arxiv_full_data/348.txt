{"title": "Unsupervised Learning of Disentangled Representations from Video", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "We present a new model DrNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.", "text": "present model drnet learns disentangled image representations video. approach leverages temporal coherence video novel adversarial loss learn representation factorizes frame stationary part temporally varying component. disentangled representation used range tasks. example applying standard lstm time-vary components enables prediction future frames. evaluate approach range synthetic real videos demonstrating ability coherently generate hundreds steps future. unsupervised learning video long-standing problem computer vision machine learning. goal learn without explicit labels representation generalizes effectively previously unseen range tasks semantic classiﬁcation objects present predicting future frames video classifying dynamic activity taking place. several prevailing paradigms ﬁrst known self-supervision uses domain knowledge implicitly provide labels using feature tracks allows problem posed classiﬁcation task self-generated labels. second general approach relies auxiliary action labels available real simulated robotic environments. either used train action-conditional predictive models future frames inversekinematics models attempt predict actions current future frame pairs. third general approaches predictive auto-encoders attempt predict future frames current ones. learn effective representations kind constraint latent representation required. paper introduce form predictive auto-encoder uses novel adversarial loss factor latent representation video frame components roughly time-independent another captures dynamic aspects sequence thus varying time. refer content pose components respectively. adversarial loss relies intuition content features distinctive given clip individual pose features not. thus loss encourages pose features carry information clip identity. empirically training loss crucial inducing desired factorization. explore disentangled representation produced model call disentangledrepresentation variety tasks. ﬁrst predicting future video frames something straightforward using representation. apply standard lstm model pose features conditioning content features last observed frame. despite simplicity model relative video generation techniques able generate convincing long-range frame predictions hundreds time steps instances. signiﬁcantly existing approaches real video data. also show drnet used classiﬁcation. content features capture semantic content video thus used predict object identity. alternately pose features used action prediction. related work account natural invariances image data naturally lends explicit what where representation. capsule model hinton performed separation explicit auto-encoder structure. zhao proposed multi-layered version similarities ladder networks methods operate static images whereas approach uses temporal structure separate components. factoring video time-varying time-independent components explored many settings. classic structure-from-motion methods explicit afﬁne projection model extract point cloud camera homography matrices contrast slow feature analysis model instead simply penalizing rate change time-independent components encouraging decorrelation. closely related villegas uses unsupervised approach factoring video content motion. architecture also broadly similar ours loss functions differ important ways. rely pixel/gradient space p-norm reconstructions plus term encourages generated frames sharp. also pixel-space reconstruction. however pixel-space loss applied combination novel adversarial term applied pose features learn disentangled representation. contrast forward model acts latent pose vectors rather predicting pixels directly. approaches explore general methods learning disentangled representations video. kulkarni show explicit graphics code learned datasets systematic dimensions variation. whitney gating principle encourage dimension latent representation capture distinct mode variation. range generative video models based deep nets recently proposed. ranzato adopt discrete vector quantization approach inspired text models. srivastava lstms generate entire frames. video pixel networks models conditional manner generating pixel time raster-scan order finn lstm framework model motion transformations groups pixels. cricri ladder stacked-autoencoders. works predict optical ﬂows ﬁelds used extrapolate motion beyond current frame e.g. contrast single pose vector predicted model rather spatial ﬁeld. chiappa focus prediction video game environments known actions frame permit action-conditional generative models give accurate long-range predictions. contrast works whose latent representations combine content motion approach relies factorization predictive model applied latter. furthermore attempt predict pixels directly instead applying forward model latent space. chiappa like approach produces convincing long-range generations. however video game environment somewhat constrained real-world video consider since actions provided generation. several video prediction approaches proposed focus handling inherent uncertainty predicting future. mathieu demonstrate loss based gans produce sharper generations traditional -based losses. train series models span possible outcomes select likely given instant. considered ganbased losses constrained nature model fact forward model directly generate pixel-space meant standard deterministic losses worked satisfactorily. approach model separate encoders produce distinct feature representations content pose frame. trained requiring content representation frame pose representation future frame xt+k combined decoded predict pixels future frame xt+k. however reconstruction constraint alone insufﬁcient induce desired factorization encoders. thus introduce novel adversarial loss pose features prevents discriminable video another thus ensuring cannot contain content information. constraint motivated notion content information vary slowly time encourages temporally close content vectors similar another. formally denote sequence images video subsequently drop index brevity. denote neural network maps image content representation captures structure shared across time. denote neural network maps image pose representation capturing content varies time. denote pose representation decoder network maps content representation frame future time step prediction future frame ˜xt+k. finally scene ht+k discriminator network takes pairs pose vectors outputs scalar probability came video not. loss function used training several terms reconstruction loss standard per-pixel loss predicted future frame ˜xt+k actual future frame xt+k random frame offset note many recent works video prediction rely complex losses capture uncertainty gans similarity loss ensure content encoder extracts mostly time-invariant representations neighboring frames penalize squared error content features adversarial loss introduce novel adversarial loss exploits fact objects present typically change within video different videos. desired disenanglement would thus content features constant within clip distinct them. implies pose features carry information identity objects within clip. impose adversarial framework scene discriminator network pose encoder shown fig. latter provides pairs pose vectors either computed video video discriminator attempts classify pair same/different video using cross-entropy loss thus pose encoder encouraged produce features discriminator unable classify come clip not. doing pose features cannot carry information object content yielding desired factorization. note assume object’s pose distinctive particular clip. adversarial training also used gans setup purely considers classiﬁcation; generator network example. overall training objective training minimize losses respect lreconstruction+αlsimilarity+β+ladversarial) hyper-parameters. ﬁrst three terms jointly optimized discriminator updated parts model held constant. overall model shown fig. details training procedure model architectures given section training pose content encoders provide representation enables video prediction straightforward manner. given frame encoders produce respectively. generate next frame input lstm model predict next pose features passed decoder generates pixel-space prediction ˜xt+ figure left discriminator trained binary cross entropy loss predict pair pose vectors comes different scenes. denote frames different sequences frame offset sampled uniformly range note trained pose encoder ﬁxed. right overall model showing terms loss function. note pose encoder updated scene discriminator held ﬁxed. note pose estimates generated recurrent fashion content features remain ﬁxed last observed real frame. relies nature lreconstruction ensured content features combined future pose vectors give valid reconstructions. lstm trained separately main model using standard loss ˜ht+ note generative model simpler many recent approaches e.g. largely forward model applied within disentangled representation rather directly pixels. another application disentangled representation classiﬁcation tasks. content features trained invariant local temporal changes used classify semantic content image. conversely sequence pose features used classify actions video sequence. either case train layer classiﬁer network either output predicting class label evaluate model synthetic real video datasets. explore several tasks model ability cleanly factorize content pose components; forward prediction video frames using approach section using pose/content features classiﬁcation tasks. explored variety convolutional architectures content encoder pose encoder decoder mnist dcgan architecture |hp| |hc| encoders consist convolutional layers subsampling. batch normalization leaky relu’s follow convolutional layer except ﬁnal layer normalizes pose/content vectors unit norm. decoder mirrored version encoder deconvolutional layers sigmoid output layer. norb suncg dcgan architecture resnet- architecture ﬁnal pooling layer |hp| |hc| uses resnet- architecture |hp| uses architecture ﬁnal pooling layer |hc| decoder mirrored version content encoder pooling layers replaced spatial up-sampling. style u-net skip connections content encoder decoder enabling model easily generate static background features. experiments scene discriminator fully connected neural network hidden layers units. trained models adam optimizer learning rate used mnist norb suncg experiments. used datasets. future prediction experiments train layer lstm cells using adam optimizer. mnist train model observing frames predicting frames. train model observing frames predicting frames. mnist start dataset consisting mnist digits bouncing around image. video sequence consists different pair digits independent trajectories. fig. shows content vector frame pose vector another generate examples transfer content pose original frames. demonstrates clean disentanglement produced model. interestingly data found necessary different color digits. adversarial term aggressive prevents pose vector capturing content information thus without color model unable determine pose information associate digit. fig. perform forward modeling using representation demonstrating ability generate crisp digits time steps future. norb apply model norb dataset converted videos taking sequences different azimuths holding object identity lighting elevation constant. fig. shows figure left demonstration content/pose factorization held mnist examples. image grid generated using pose content vectors taken corresponding images ﬁrst column respectively. model clearly learned disentangle content pose. right shows forward modeling time steps future given initial frames. generation note pose part representation predicted previous time step content vector ﬁxed frame. generations remain crisp despite long-range nature predictions. figure left factorization examples using drnet model held norb images. image grid generated using pose content vectors taken corresponding images ﬁrst column respectively. examples found suplemental material. center examples drnet trained without adversarial loss term. note content pose longer factorized cleanly pose vector contains content information ends dominating generation. right factorization examples mathieu figure left examples linear interpolation pose space examples right factorization examples held images suncg dataset. image grid generated using pose content vectors taken corresponding images ﬁrst column respectively. note even complex objects model able rotate accurately. model able factor content pose cleanly held data. fig. train version model without adversarial loss term results signiﬁcant degradation model pose vectors longer isolated content. comparison also show factorizations produced mathieu less clean terms disentanglement generation quality approach. table shows classiﬁcation results norb following training classiﬁer pose features also content features. adversarial term used content features perform well. without term content features become less effective classiﬁcation. suncg rendering engine suncg dataset generate sequences camera rotates around range chair models. drnet able generate high quality examples data shown fig. action dataset finally apply drnet dataset simple dataset real-world videos people performing actions fairly uniform backgrounds. fig. show forward generations different held examples comparing baselines mcnet villegas which best knowledge produces current best quality generations real-world video baseline auto-encoder lstm model essentially ours single encoder whose features thus combine content pose also similar figure qualitative comparison drnet model mcnet ae-lstm baseline. models conditioned ﬁrst video frames generate frames. display predictions every frame. video sequences taken held examples dataset classes walking running fig. shows examples generations time steps. actions sufﬁcient time person left frame thus generations would ﬁxed background. fig. attempt quantify ﬁdelity generations comparing inception score approach mcnet metric used assessing generations gans appropriate scenario traditional metrics psnr ssim curves show mean scores generations decaying gracefully mcnet examples generated movies viewed appendix also https//sites.google.com/view/drnet-paper//. natural concern high capacity models might memorizing training examples. probe fig. show nearest neighbors generated frames training set. fig. uses pose representation produced drnet train action classiﬁer examples. extract pose vectors video sequences length train fully connected classiﬁer vectors predict action class. compare autoencoder baseline single encoder whose features thus combine content pose. factorization signiﬁcantly boosts performance. table classiﬁcation results norb dataset with/without adversarial loss using content pose representations adversarial term crucial forcing semantic information content vectors without performance drops significantly. figure frame generated drnet show nearest-neighbor images training based pose vectors content pose vectors evident model simply copying examples training data. furthermore middle shows pose vector generalizes well independent background clothing. paper introduced model based pair encoders factor video content pose. seperation achieved training novel adversarial loss term. resulting representation versatile particular allowing stable coherent long-range prediction nothing standard lstm. generations compare favorably leading approaches despite simple model e.g. lacking losses probabilistic formulations video generation approaches. source code available https//github.com/edenton/drnet.", "year": 2017}