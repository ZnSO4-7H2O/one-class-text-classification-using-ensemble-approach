{"title": "Neural Network Regularization via Robust Weight Factorization", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Regularization is essential when training large neural networks. As deep neural networks can be mathematically interpreted as universal function approximators, they are effective at memorizing sampling noise in the training data. This results in poor generalization to unseen data. Therefore, it is no surprise that a new regularization technique, Dropout, was partially responsible for the now-ubiquitous winning entry to ImageNet 2012 by the University of Toronto. Currently, Dropout (and related methods such as DropConnect) are the most effective means of regularizing large neural networks. These amount to efficiently visiting a large number of related models at training time, while aggregating them to a single predictor at test time. The proposed FaMe model aims to apply a similar strategy, yet learns a factorization of each weight matrix such that the factors are robust to noise.", "text": "rudy weiguang ding daniel jiwoong graham taylor school engineering university guelph guelph ontario canada {jrudyimjwdinggwtaylor}uoguelph.ca regularization essential training large neural networks. deep neural networks mathematically interpreted universal function approximators effective memorizing sampling noise training data. results poor generalization unseen data. therefore surprise regularization technique dropout partially responsible now-ubiquitous winning entry imagenet university toronto. currently dropout effective means regularizing large neural networks. amount efﬁciently visiting large number related models training time aggregating single predictor test time. proposed fame model aims apply similar strategy learns factorization weight matrix factors robust noise. much recent surge popularity neural networks especially application classiﬁcation visual data advances regularization. winning entry imagenet lsvrc- challenge krizhevsky used deep convolutional neural network surpass competition margin nearly top- test error rate. partially attribute performance regularization technique called dropout large neural networks extremely powerful avoiding overﬁtting crucial increasing generalization performance. dropout elegant simple solution equivalent training exponential number models. test time models ‘averaged’ single ‘mean’ predictor generalizes better unseen test data. however models rectiﬁed linear activations known lead sparse activations percent units true zero activation experiments shown number high training dropout. although sparse representations desirable general reduce effective number size models dropout visits training. words multiplicative noise applied dropout effect sparse activations. address adding post-activation bias relu activation function making explicit distinction between sparse units units masked dropout. propose related regularization procedure call factored mean training high-level fame similar dropout. methods address problem overﬁtting efﬁciently training ensemble models averaged together test time. dropout achieves randomly masking units fame learning factorization weight matrix factors robust noise. leads accurate model averaging procedure without sacriﬁcing fame’s ability make shared information models visited training time. section brieﬂy review traditional methods improve generalization discussing modern advances dropout drop-connect. also introduce concept gating networks achieve weight modulation multiplicative interactions among variables. traditional approaches improving generalization neural networks seen means limiting capacity model. include early stopping weight decay weight constraints addition noise training. weight decay involves adding proportional norm weights objective weight constraints limits norm incoming weight vector unit addition noise training also effective regularizer adding small amounts noise input vector shown equivalent tikhonov regularization denoising autoencoders apply either additive multiplicative noise input signal. model must learn ‘denoise’ input reconstruct uncorrupted input. denoising criterion permits overcomplete models learn useful representations predictive opposition reconstruction distribution regularizer dropout motivated idea sexual reproduction increases overall ﬁtness species preventing complex co-adaptations genes. likewise dropout aims increase generalization performance preventing complex co-adaptations hidden units. formally consider feed forward neural network hidden layers. deﬁne output vector hidden layer learning procedure learns parameterized function input vector hidden vector output vector number input hidden output dimensions. convention deﬁne next deﬁne rnl×nl− weights hidden biases layer hidden unit output activations written hidden output activation functions. weight matrices bias vectors randomly initialized learned optimization cost function typically gradient descent. dropout multiplicative noise applied hidden activations training presentation input. accomplished stochastically dropping individual hidden units layer formally deﬁne bernoulli i.e. vector independent bernoulli variables probability training equation becomes denotes elementwise product. masking performed test time. order compensate lack masking weights scaled training complete recent results shown using gaussian noise leads improved test performance test time such weights testing model require scaling. bernoulli dropout procedure interpreted means training different sub-network training example sub-network contains subset connections model. exponential number networks many visited training. however extensive amount weight sharing networks allows make useful predictions regardless fact trained explicitly interpretation test procedure seen approximation geometric average sub-networks dropout inspired variety theoretical experimental research. subsequent theoretical work found training dropout equivalent adaptive version weight decay traditional penalty spherical weight space dropout akin axis aligned scaling penalty takes curvature likelihood function account extension dropout dropconnect applies mask hidden units connections units found outperform dropout certain image recognition tasks maxout networks attempt design type activation function exploits beneﬁts dropout training procedure. classical neural networks contain ﬁrst order interactions among input variables hidden variables gated models permit tri-partite graph connects hidden variables pairs input variables. messages sent networks involve multiplicative interactions among variables permit learning structure relationship inputs rather structure inputs recent applications models include modeling transformations images depth time-series data objective typical ﬁrst order neural network learn mapping function input output words training involves updating model weights model learns function instead learning mapping single input output vector gated models conditional second input learned function refer context. instead ﬁxed weight matrix classical neural networks weights gated model interpreted function context formally hidden activation single layered factored gated model input vectors context factor size proposed factored mean training procedure aims make weight modulation property gated networks means regularization. fame architecture like neural network trained dropout aims learn mapping input output vector dropout applies multiplicative noise hidden activations training fame training weight matrix decomposed matrices multiplicative noise applied directly input vector projected onto ﬁrst matrices. figure comparison classical feed forward neural network fame. ﬁgure right depicts connectivity hidden layers typical feed forward neural network. left connectivity hidden layers fame model vector independent samples noise distribution. biases omitted clarity. formally given feed forward neural network layers described section instead single weight matrix layer deﬁne matrices rnl×f free parameter. thus hidden activation becomes instead model note similarity equation gated model equation weights function secondary input vector fame training procedure modulates weights random vector like dropout fame model viewed training unique model training example epoch. words fame model represents manifold models settings thought coordinates given model manifold. dropout computes average mean network scaling weights test time calculate mean network learned fame training setting expectation sampling distribution. similar equation secondary input gated model. however deﬁne hidden activations mean network test time simpliﬁed rnl×n). equivalent equation classical feed forward network. effect fame training procedure learns decomposition weight matrix robust noise. notice process learning decomposition rank bounded choice order restrict rank introduce unnecessary parameters min). parameters fame model learned using gradient-based optimization methods classical neural network. similar technique applied convolution layers single convolution step decomposed linear convolution operations. fame training multiplicative noise applied ﬁrst convolution. similar network network model ﬁlter seen small non-linear fame convolution layer ﬁlters layer linear networks multiplicative noise added ﬁrst layer. mnist dataset consists training test examples greyscale image handwritten digit. training randomly partitioned image training image validation set. experiments implemented python using theano library model hyper-parameters chosen based performance validation set. training multiplicative gaussian noise applied input linear factor layers incoming weight vectors unit constrained maximum norm training performed using mini-batch gradient descent crossentropy loss batch size learning rates annealed factor epoch. nesterov accelerated gradient used optimization initial value increasing linearly number epochs. ﬁnal momentum value along number epochs reaching ﬁnal momentum value chosen based validation performance. training performed weight updates hyper-parameters resulted lowest validation error used train ﬁnal model training examples. test model trained total weight updates. results mnist data summarized table using best settings hyper-parameters found validation test model trained random initializations average error models reported. fame training outperforms dropout maxout achieving best test error found restricting rank beneﬁcial larger hidden layer sizes layer model restricted size linear factor layer additionally training model fewer parameters achieves similar results. consider size test model fame model hidden layers units layer effective parameters. maxout model million free parameters gaussian dropout million parameters. cifar cifar datasets contain color images training examples remaining used testing. cifar- dataset contains images classes cifar- contains classes. hyper-parameter selection followed similar procedure used mnist classiﬁcation partitioning training training validation images. cifar models consist three convolutional fame layers followed fully connected fame layers. results summarized table image preprocessing follow procedure srivastava figure training cost test cost training progresses example training fame dropout. notice fame training test cost continues decrease entire duration training. although fame fails outperform dropout maxout competitive recent results cifar- cifar-. discrete subset parameters considered hyper-parameter cross-validation substantially smaller considered mnist task. expect time optimize parameters potentially using tools bayesian hyper-parameter optimization performance improve. conducted separate experiments mnist probe learning dynamics fame dropout. experiments described section performed using model non-linear hidden layers multiplicative gaussian noise input hiddens factors. fame dropout training hyper-parameters chosen based validation performance. similar dropout training fame training prevents overﬁtting such require early stopping. seen figure test cost continues decrease entirety training. dropout also successful avoiding overﬁtting i.e. test cost increase continued training. however test cost plateaus earlier fame training. figure comparison fame dropout test procedure true arithmetic geometric means estimated sampling outputs random subnetworks. fame training prediction testing procedure fact give good estimate true geometric mean prediction. full mathematical analysis mean test procedure given equation difﬁcult non-linearity hidden units. such experimentally verify testing procedure indeed approximating geometric mean predictions noisy subnetworks visited training. using test data input generate samples output noisy networks. mean prediction calculated ﬁrst computing geometric mean sample outputs comparing output given deterministic testing procedure. figure demonstrates fame testing procedure gives accurate estimate true geometric mean subnetworks. comparison even though dropout test procedure outperforms estimated geometric mean subnetworks provide good estimate true mean network. note fame dropout models contained non-linear hidden layers trained multiplicative gaussian noise. interestingly arithmetic mean prediction fame subnetworks appears slightly outperform geometric mean prediction warrant investigation. wager shown dropout equivalent adaptive weight decay. examine link weight decay fame dropout training monitor norms weight matrices training regimes. figure depicts evolution norm weight matrix training relative norm initial value note fame trianing plot norm implied weight matrix factored weights actually learned model. although explicit penalty used fame dropout training seem impose implicit penalty norm weight matrices. however effect pronounced fame training. regularization essential training large neural networks. power universal function approximators allows memorize sampling noise training data leading poor generalization unseen data. traditional forms generalization means limiting model’s capacity. include early stopping weight decay weight constraints addition noise. currently dropout effective means regularizing large neural networks. amount efﬁciently visiting large number related models training time aggregating single mean model test time. proposed figure norm weight matrices training. visualization norms scaled value initialization. training procedures exhibit behavior similar weight decay result pronounced case fame training. like dropout fame visits family models training allowing efﬁcient testing procedure making predictions. models trained fame outperform dropout training even models order magnitude fewer effective parameters. additionally restricting rank factor loadings used means controlling number free parameters. supported recent work looked closely signiﬁcant redundancy parameterization deep learning architectures proposed low-rank weight matrices massively reduce parameters preserving predictive accuracy bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas bengio yoshua. theano features speed improvements. deep learning unsupervised feature learning nips workshop bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. scipy june oral presentation. glorot xavier bordes antoine bengio yoshua. deep sparse rectiﬁer networks. proceedings international conference artiﬁcial intelligence statistics. jmlr w&cp volume volume hinton geoffrey camp drew. keeping neural networks simple minimizing description length weights. proceedings sixth annual conference computational learning theory hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv preprint arxiv. srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research vincent pascal larochelle hugo lajoie isabelle bengio yoshua manzagol pierre-antoine. stacked denoising autoencoders learning useful representations deep network local denoising criterion. journal machine learning research", "year": 2014}