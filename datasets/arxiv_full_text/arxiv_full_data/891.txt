{"title": "Gradient Descent for Spiking Neural Networks", "tag": ["q-bio.NC", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (~millisecond) spike-based interactions for efficient encoding of information, and a delayed memory XOR task over extended duration (~second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.", "text": "much studies neural computation based network models static neurons produce analog output despite fact information processing brain predominantly carried dynamic neurons produce discrete pulses called spikes. research spike-based computation impeded lack efﬁcient supervised learning algorithm spiking networks. here present gradient descent method optimizing spiking network models introducing differentiable formulation spiking networks deriving exact gradient calculation. demonstration trained recurrent spiking networks dynamic tasks requires optimizing fast spike-based interactions efﬁcient encoding information delayed-memory task extended duration results show method indeed optimizes spiking network dynamics time scale individual spikes well behavioral time scales. conclusion result offers general purpose supervised learning algorithm spiking neural networks thus advancing investigations spike-based computation. brain executes highly dynamic computation multiple time-scales individual neurons exhibit spiking dynamics millisecond resolution recurrent network connections produce slower dynamics order seconds minutes. brain organizes spiking neuron dynamics form basis computation central problem neuroscience. nonetheless analysis modeling neural computation assume static neuron models produce analog output called rate-based neurons simpliﬁed models compatible advanced tools deep learning ﬁeld efﬁciently optimize large scale network models perform complex computational tasks however rate-based neural network models fail describe fast dynamics spike-based computation brain. main difﬁculty optimizing spiking neural networks stems discrete all-or-none nature spikes spiking neuron generates brief spike output membrane voltage crosses threshold silent times. non-differentiable behavior incompatible standard gradient-based supervised learning methods. consequently previous learning methods spiking neural networks explored various ways circumvent non-differentiability problem. spikeprop considered spike times neurons state variables used differentiable relationship input output spike times minimize difference actual desired output spike times. however creation deletion spikes nondifferentiable number output spikes must pre-speciﬁed. memmesheimer considered problem generating spikes desired times remaining silent times binary classiﬁcation problem applied perceptron learning rule pﬁster considered stochastic spiking neurons maximized smooth likelihood function neurons spiking desired times. biologically inspired methods based spike-time-dependentplasticity also proposed methods however require target spiking activity individual neurons desired times signiﬁcantly limit range applicability. alternative methods also proposed instead directly optimizing spiking network methods optimize network static analog neurons replicate optimized solution spiking network. hunsberger eliasmith used analog units closely approximated ﬁring rate individual spiking neurons. instead replicating individual neuron’s dynamics abbott proposed replicating entire network dynamics using recently developed methods predictive coding although approaches applicable wider range problems replicated spiking networks mimic solutions rate-based networks rather exploring larger space spike-time based solutions. paper introduce novel differentiable formulation spiking neural networks derive exact gradient calculation gradient based optimization. method optimizes recurrent network dynamics time scale individual spikes general supervised learning problems. spiking networks transmission neural activity mediated synaptic current. models describe synaptic current dynamics linear ﬁlter process instantly activates presynaptic membrane voltage crosses threshold e.g. dirac-delta function denotes time threshold-crossing. thresholdtriggered dynamics generates discrete all-or-none responses synaptic current nondifferentiable. here replace threshold gate function non-negative unit integral time derivative pre-synaptic membrane voltage. term required dimensional consistency term dimension dirac-delta impulses since gate function dimension dimension hence time integral synaptic current i.e. charge dimensionless quantity. consequently depolarization event beyond active zone induces constant amount total charge regardless time scale depolarization since therefore generalizes threshold-triggered synapse model preserving fundamental property spiking neurons i.e. supra-threshold depolarizations induce amount synaptic responses regardless depolarization rate depolarizations active zone induce synaptic responses depolarizations within active zone induce graded responses contrasts threshold-triggered synaptic dynamics causes abrupt non-differentiable change response threshold note term reduces dirac-delta impulses zero-width limit active zone reduces back threshold-triggered synapse model gate function without term previous used differentiable model synaptic connection model however spike event delivers varying amount charge depending depolarization rate slower presynaptic depolarization greater amount charge delivered post-synaptic targets. figure differentiability synaptic current dynamics synaptic current traces shown corresponding membrane voltage traces here gate function within active zone width otherwise. pre-synaptic membrane voltage depolarizes beyond active zone. despite different rates depolarization events incur amount charge complete input-output dynamics spiking neuron synaptic current dynamics must coupled presynaptic neuron’s internal state dynamics. simplicity consider differentiable neural dynamics depend membrane voltage input current recurrent connectivity weight matrix input weight matrixi input signal network tonic current. note formulation describes general fully connected networks; speciﬁc network structures imposed constraining connectivity e.g. triangular matrix structure multi-layer feedforward networks. lastly deﬁne output network linear readout synaptic current spiking neural network model optimized gradient descent. general exact gradient dynamical system calculated using either pontryagin’s minimum principle also known backpropagation time real-time recurrent learning yield identical results. present former approach here scales better network size instead latter approach also straightforwardly implemented. links backpropagating dynamics individual neurons. here ∂l/∂sj. interestingly coupling term backpropagating dynamics form coupling term forward-propagating dynamics. thus gating mechanism mediates spiked-based communication signals also controls propagation error sparse compressed manner. given adjoint state vectors satisfy gradient total cost respect network parameters calculated ∂l/∂oj. note gradient calculation procedure involves multiplication presynaptic input source postsynaptic adjoint state driven term i.e. product postsynaptic spike activity temporal difference error. analogous reward-modulated spike-time dependent plasticity demonstrate method training spiking networks dynamic tasks require information processing time. tasks deﬁned relationship time-varying input-output signals used training examples. draw mini-batches training examples signal distribution calculate gradient average total cost stochastic gradient descent optimization. here cost function penalizes readout error overall synaptic activity ﬁrst consider predictive coding tasks optimize spike-based representations accurately reproduce input-ouput behavior linear dynamical system full-rank input output matrices. analytical solutions class problems obtained form non-leaky integrate neural networks although insigniﬁcant amount leak current often added. solutions also require networks equipped instantaneously fast synapses addition regular synapses ﬁnite time constant membrane voltage dynamics neuron given figure balanced dynamics spiking network trained auto-encoding task. readout signals actual desired input current components single neuron external input current blue) fast reccurent synaptic current red). total input current single neuron wfsf single neuron membrane voltage traces actual voltage trace driven external input fast reccurent synaptic current virtual trace driven external input fast recurrent weight trained predicted diagonal elements zero avoid self-excitation/inhibition. readout weight input weight ensure membrane voltage stays within ﬁnite range impose thresholds reset voltage vreset simplicity allow threshold trigger negative synaptic responses turned desired. also introduce additional fast synaptic current proposed modiﬁes input current vector wfsf recurrent weight matrix associated fast synapses. however assigning zero time constant fast synapses often causes unstable dynamics could lead spike immediately triggering spikes neurons. here assign ﬁnite time constants types synapses fast synapses regular synapses. despite simplicity predictive coding framework reproduces important features biological neural networks balance excitatory inhibitory inputs efﬁcient coding also analytical solutions provide benchmark assessing results optimization. synaptic time constant goal accurately represent analog signals using least number spikes. used network neurons input output signals. randomly generated sum-of-sinusoid signals period used input. used training zero post-training simulations. output trained network accurately tracks desired output analysis simulation reveals network operates tightly balanced regime fast recurrent synaptic input wfsf provides opposing current mostly cancels input current external signal neuron generates greatly reduced number spike outputs network structure also shows close agreement prediction. optimal input weight matrix equal transpose readout matrix optimal fast recurrent weight approximately product input readout weights close agreement regular recurrent connection needed task hence zero. network structures shown maintain tight input balance remove redundant spikes encode signals efﬁcient manner representation error scales number involved spikes compared general predictive coding task generally predictive coding tasks involve linear dynamic relationships desired input-output signals following form constant matrix. here trained spiking network neurons input signals sums-of-sinusoid strongly modulates desired output signal dynamics. similar result shown figure trained network exhibits tightly balanced input current network output accurately tracking desired output. optimal regular recurrent weight approximately also close agreement prediction network structures similar case auto-encoding task. results show algorithm accurately optimizes millisecond time-scale interaction neurons efﬁcient spike-time-based encoding scheme. moreover also shows efﬁcient coding robustly achieved without introducing instantaneously fast synapses previously considered necessary. major challenge spike-based computation bridging wide divergence timescales behavior spikes millisecond spikes perform behaviorally relevant computations order seconds? figure delayed-memory task panel shows single-trial input go-cue output traces spike raster optimized neural network. y-axis raster plot neuron note similarity initial portion spike patterns trials ﬁrst input pulses contrast spike patterns go-cue signal similar trials desired output pulses here consider delayed-memory task performs exclusive-or operation input history stored extended duration. speciﬁcally network receives binary pulse signals input channel go-cue another channel. network receives input pulses since last go-cue signal generate output pulse next go-cue i.e. positive output pulse input pulses opposite signs negative output pulse input pulses equal signs additionally generate null output input pulse received since last go-cue signal. variable time delays introduced input pulses go-cues. simpler version task proposed whose solution involved ﬁrst training analog rate-based neural network replicating learned network dynamics larger network spiking neurons using method predictive coding also required dendritic nonlinearity function match transfer function rate neurons. trained network quadratic integrate neurons whose dynamics also known theta neuron model threshold reset voltage vreset time constants used whereas time-scale task much longer time constants. intrinsic nonlinearity spiking dynamics proves sufﬁcient solving task without requiring extra dendritic nonlinearity. trained network successfully solves delayed-memory task spike patterns exhibit time-varying sustained activities maintain input history generate correct outputs triggered go-cue signal return background activity. analysis needed understand exact underlying computational mechanism. result shows algorithm indeed optimize spiking networks perform nonlinear computations extended time. presented novel differentiable formulation spiking neural networks derived gradient calculation supervised learning. unlike previous learning methods method optimizes spiking network dynamics general supervised tasks time scale individual spikes well behavioral time scales. exact gradient-based learning methods inevitably involve discrepancies biological learning processes. nonetheless methods provide solid theoretical ground understanding principles biological learning rules. example result shows gradient update occurs sparsely compressed manner near spike times bearing close resemblance reward-modulated stdp. moreover analysis reveal certain aspects gradient calculation approximated biologically plausible manner without signiﬁcantly compromising efﬁciency optimization. example recently shown biologically implausible aspects backpropagation method resolved feedback alignment rate-based multi-layer feedforward networks approximations could also apply spiking neural networks. here coupled synaptic current model differentiable single-state spiking neuron models. however synapse model coupled neuron models including realistic multi-state neuron models detailed action potential dynamics including hodgkin-huxley model morris-lecar model fitzhugh-nagumo model; even models internal adaptation variables. also coupled neuron models non-differentiable reset dynamics leaky integrate model exponential integrate model izhikevich model although gradient calculation models would require additional procedures. examined future work. daniel yamins hong charles cadieu ethan solomon darren seibert james dicarlo. performance-optimized hierarchical models predict neural responses higher visual cortex. proceedings national academy sciences jean-pascal pﬁster taro toyoizumi david barber wulfram gerstner. optimal spike-timing-dependent plasticity precise action potential ﬁring supervised learning. neural computation robert legenstein dejan pecevski wolfgang maass. learning theory reward-modulated spike-timing-dependent plasticity application biofeedback. plos comput biol bard ermentrout. ermentrout-kopell canonical model. scholarpedia timothy lillicrap daniel cownden douglas tweed colin akerman. random synaptic feedback ¯pvi ¯psi adjoint state variables membrane voltage synaptic current neuron respectively cost function. back-propagating dynamics adjoint state variables", "year": 2017}