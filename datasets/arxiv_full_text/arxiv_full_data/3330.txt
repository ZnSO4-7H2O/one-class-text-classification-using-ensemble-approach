{"title": "Improved graph-based SFA: Information preservation complements the  slowness principle", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Slow feature analysis (SFA) is an unsupervised-learning algorithm that extracts slowly varying features from a multi-dimensional time series. A supervised extension to SFA for classification and regression is graph-based SFA (GSFA). GSFA is based on the preservation of similarities, which are specified by a graph structure derived from the labels. It has been shown that hierarchical GSFA (HGSFA) allows learning from images and other high-dimensional data. The feature space spanned by HGSFA is complex due to the composition of the nonlinearities of the nodes in the network. However, we show that the network discards useful information prematurely before it reaches higher nodes, resulting in suboptimal global slowness and an under-exploited feature space.  To counteract these problems, we propose an extension called hierarchical information-preserving GSFA (HiGSFA), where information preservation complements the slowness-maximization goal. We build a 10-layer HiGSFA network to estimate human age from facial photographs of the MORPH-II database, achieving a mean absolute error of 3.50 years, improving the state-of-the-art performance. HiGSFA and HGSFA support multiple-labels and offer a rich feature space, feed-forward training, and linear complexity in the number of samples and dimensions. Furthermore, HiGSFA outperforms HGSFA in terms of feature slowness, estimation accuracy and input reconstruction, giving rise to a promising hierarchical supervised-learning approach.", "text": "slow feature analysis unsupervised-learning algorithm extracts slowly varying features multi-dimensional time series. supervised extension classiﬁcation regression graph-based gsfa based preservation similarities speciﬁed graph structure derived labels. shown hierarchical gsfa allows learning images high-dimensional data. feature space spanned hgsfa complex composition nonlinearities nodes network. however show network discards useful information prematurely reaches higher nodes resulting suboptimal global slowness under-exploited feature space. counteract problems propose extension called hierarchical information-preserving gsfa information preservation complements slowness-maximization goal. build -layer higsfa network estimate human facial photographs morph-ii database achieving mean absolute error years improving state-of-the-art performance. higsfa hgsfa support multiple-labels oﬀer rich feature space feed-forward training linear complexity number samples dimensions. furthermore higsfa outperforms hgsfa terms feature slowness estimation accuracy input reconstruction giving rise promising hierarchical supervised-learning approach. frequently used pre-processing step solve classiﬁcation regression problems high-dimensional input. goal supervised extract low-dimensional representation input samples contains predictive information labels advantage dimensions irrelevant supervised problem discarded resulting compact representation accurate estimations. slowness principle requires extraction slowly changing features used algorithms. shown principle might explain part neurons brain self-organize compute invariant representations. slow features computed using methods online learning rules slow feature analysis closed-form algorithm speciﬁc task biologically feasible variants incremental-learning version optimization objective minimization squared output diﬀerences consecutive pairs samples. although unsupervised order samples regarded weak form supervised information allowing supervised learning tasks. graph-based extension explicitly designed solution classiﬁcation regression problems yields accurate label estimations sfa. gsfa used estimate gender synthetic face images recently traﬃc sign classiﬁcation face detection gsfa trained training graph vertices samples edges represent similarities corresponding labels. optimization objective gsfa minimization weighted squared output diﬀerences samples connected edge edges carefully chosen reﬂect label similarities. figure combination three extensions gives rise diﬀerent versions sfa. article propose versions information preservation higsfa signiﬁcant one. applied data directly cubic complexity w.r.t. number dimensions. however processing high-dimensional data still practical resorts hierarchical gsfa instead extracting slow features single step features computed applying gsfa lower dimensional data chunks repeatedly. advantages hgsfa direct gsfa include lower memory requirements complex global nonlinearity composition nonlinear transformations layers. moreover local extraction slow features lower-dimensional data chunks typically results less overﬁtting. article show hgsfa suﬀers drawback separate instances gsfa process low-dimensional data chunks network also called gsfa nodes prematurely discard features slow local level would useful improve global slowness combined features nodes. drawback call unnecessary information loss leads suboptimal feature space comprised network also aﬀects estimation accuracy underlying supervised learning problem. reduce unnecessary information loss hgsfa propose complement slowness information preservation simplicity eﬃciency implement idea minimization reconstruction error. resulting network called hierarchical information-preserving gsfa algorithm constituting node network called information-preserving gsfa features computed igsfa divided parts slow part linear transformation features computed gsfa input-reconstructive part. compute input-reconstructive part input data approximated using slow part resulting residual data processed pca. construction ensures parts decorrelated features comexperiments show advantages higsfa hgsfa slower features better generalization unseen data much better input reconstruction improved accuracy supervised learning problem. furthermore computational memory requirements higsfa asymptotically hgsfa. higsfa main extension proposed work belongs supervised algorithms supervised include fisher discriminant analysis local pairwise constraints-guided feature projection semi-supervised dimensionality reduction semi-supervised lfda existing extensions include extended generalized graph-based higsfa extends hierarchical gsfa adding information preservation. adaptations shown successful classiﬁcation regression mathematical point view gensfa gsfa belong family optimization problems solved generalized eigenvalues. conversions four algorithms possible. diﬀerences gsfa gsfa vertex weights independent edge weights gsfa invariant scale weights providing normalized objective function. spite closely related mathematically gensfa gsfa originate diﬀerent backgrounds ﬁrst motivated different goals mind. therefore usually diﬀerent applications trained diﬀerent similarity matrices resulting features diﬀerent properties. instance originates ﬁeld manifold learning transitions typically deﬁned input similarities originates unsupervised learning transitions typically deﬁned temporal ordering samples. gensfa typically combines input similarities class information. gsfa transitions typically reﬂect label similarities. gsfa input similarities might disadvantageous might compromise invariance features central goals. rest article organized follows. next section review gsfa. section analyze advantages limitations hierarchical networks slow feature extraction. section propose isfa algorithm. section evaluate higsfa experimentally using problem estimation facial photographs. conclude discussion section. edges pairs samples index substitutes time variable sfa. edges undirected symmetric weights indicate similarity connected vertices; also vertex associated weight used reﬂect frequency. representation includes standard time series special case constraints called weighted zero mean weighted unit variance weighted decorrelation. factors provide invariance scale edge weights well scale vertex weights. typically linear feature space used input samples preprocessed nonlinear expansion function. depending training graph chosen properties features extracted gsfa quite diﬀerent. training graphs classiﬁcation favor connections samples class whereas graphs regression favor connections samples similar labels. section show combine graphs classiﬁcation regression eﬃcient graph learn gender race simultaneously. motivation hisfa higsfa correct certain disadvantages hsfa hgsfa preserving advantages. analysis advantages disadvantages topic next section. section analyze hsfa networks point view advantages particularly computational complexity limitations particularly unnecessary information loss. focus hsfa hgsfa covered extension. understanding drawbacks hsfa crucial justify extensions information preservation proposed section various contributions continued biological interpretation. hsfa used learn invariant features simulated view moves inside box. conjunction sparseness post-processing step extracted features similar responses place cells hippocampus. works focused computational eﬃciency compared direct sfa. hsfa used object recognition images estimate pose parameters single objects moving rotating homogeneous background. hgsfa network layers used accurately horizontal position faces photographs sub-problem face detection. general hsfa networks directed acyclic arbitrary edges otherwise usually composed multiple layers regular structure. hsfa networks literature similar composition. typical diﬀerences data split smaller chunks particular processing done nodes themselves. structure network usually follows structure data. instance networks data dimension typically -dimensional structure networks data dimensions -dimensional structure. idea extends voxel data dimensions beyond. simplicity refer input data layer important parameters deﬁne structure network include output dimensionality nodes. fan-in nodes number nodes previous layer feed them. receptive ﬁelds nodes refer elements input data provide input particular node. stride layer tells apart inputs adjacent nodes layer are. stride smaller fan-in least node previous layer feed nodes current layer. called receptive ﬁeld overlap. parameters learned. explained fact input node hierarchical network number samples original input much smaller dimensionality frequently leading good generalization unseen data individual nodes well whole network. diﬀerence generalization hsfa direct evident polynomial expansions involved larger expanded dimensionality direct translates stronger overﬁtting. another interesting property hsfa nonlinearity nodes accumulates across layers even using simple expansions network whole describe highly nonlinear feature space depending data complex feature space necessary extract slowest hidden parameters solve supervised problem good accuracy. motivation preferring hsfa direct computational eﬃciency. focus complexity training rather feature extraction former relevant applications latter relatively lightweight cases. training linear computational complexity large direct gsfa therefore ineﬃcient. complexity reduced using hsfa hgsfa. exact complexity depends structure parameters hierarchical network. prove below linear certain networks. although existing applications exploited speed hsfa aware analysis actual complexity. compute computational complexity concrete quadratic hsfa network data layers illustrated figure node network performs quadratic receptive ﬁeld fan-in stride nodes layer input values ﬁxed parameter. rest layers fan-in stride nodes nodes. assume total input dimensionality every node reduces dimensionality input components components. problem still feasible small enough might apply singular value decomposition methods. however small number samples usually results pronounced overﬁtting. input qsfa node dimensions increased dimensions quadratic expansion. afterwards linear reduces dimensionality hence complexity training single node since layer qhsfa network quadratic general output features layer written polynomials degree particular output features network polynomials degree however actual feature space span polynomials degree subset restricted connectivity network. contrast direct qsfa contains quadratic polynomials figure example qhsfa network binary fan-ins overlap. node performs quadratic reducing dimensionality components. small fan-in results network large number layers useful build deep networks. possible design sophisticated networks qhsfa preserve similar computational complexity. example network proposed section overlapping receptive ﬁelds larger fan-ins structure complexity also linear section analyze limitations hsfa networks general network nodes criterion namely slowness maximization. otherwise nodes unrestricted; might linear nonlinear include additive noise clipping various passes etc. spite remarkable advantages hsfa show relying slowness determine aspects data preserved results three disadvantages unnecessary information loss poor input reconstruction feature garbling. unnecessary information loss. disadvantage occurs nodes discard dimensions data signiﬁcantly slow locally would useful slowness optimization higher layers network preserved. duces number dimensions since ﬁrst bottom node computes qsfa since second bottom node computes qsfa node extracts qsfa therefore network misses slowest feature even though belongs feature space spanned network. denotes mutual information entropy. thus impossible locally rule feature contains information might yield slow feature unless observes whole data available network. unnecessary information loss also aﬀect applications practice. example figure shows values slowest features extracted ﬁrst layer hgsfa network trained estimation human face images. values approximately less feature transformation input data transformation inherent noise mixture both. fact feasible features value outputs arbitrary rotation them even though might prefer features transformations input rather noise. features preserved. discarded features might still contain useful information giving place information loss. poor input reconstruction. goal input reconstruction generate input given slow feature representation. interesting task studied help determine features network sensitive inputs certain properties. ﬁeld image processing reconstruction might relevant image morphing interpolation. here consider morphing task ﬁnding input must modiﬁed reﬂect modiﬁcations introduced output features. example assume trained extract facial figure values ﬁrst slow features averaged nodes ﬁrst layer hgsfa network trained estimation training graph employed serial graph groups. values close indicating early stage nodes small experiments shown input reconstruction top-level features extracted hsfa challenging task also conducted experiments various nonlinear methods input reconstruction including local global methods conﬁrming diﬃculty. show cause poor input reconstruction weakness reconstruction algorithms employed insuﬃcient reconstructive information slow features. extracted features ideally depend hidden slow parameters invariant factor. estimation example extracted features close features predicted theory would strongly related harmonic functions thus would mostly invariant factors identity person facial expression background etc. therefore theory would available reconstruction. practice features usually contain residual information input data. however cannot rely information partial making reconstructions unique highly nonlinear making diﬃcult untangle even features extracted linear typically result inaccurate reconstructions. using many layers make problem serious hsfa. exception reconstruction possible trained artiﬁcial data generated slowly changing parameters. case slow output features encode generative parameters allowing input reconstruction reconstruction method powerful enough. feature garbling. even node incur information loss features might still represented useful extraction slow features next nodes. feature garbling occurs features extracted complex necessary without slower. complicates extraction slow features higher layers feature space available nodes might suﬃce transform representation slow features. feature vectors encryption function unknown assume valid output signals belong feature space. since signals ∆values might output them. signal might useful slowness maximization next layers features simpler directly connected inputs might useless next layers unless decryption function belongs feature space. feature garbling results information loss. informative garbled feature likely discarded later hierarchy. might attempt reduce feature garbling using mild nonlinear expansion sfa. however might compromise slowness extracted features. similarly might preserve large number features reduce information loss. however might impractical would increase computational cost contradicts goal dimensionality reduction. problems poor input reconstruction unnecessary information loss also connected. evident distinguishes types information information full input data information global slow parameters. losing results poor input reconstruction whereas losing results unnecessary information loss. course contains therefore problems originate losing diﬀerent related types information. feature garbling theoretical open issue still needs formalized whose relevance practice unknown. therefore attempt counteract proposed extensions. section propose information-preserving counteract problems unnecessary information loss poor input reconstruction. write isfa lowercase distinguish independent extension also applied gsfa called informationpreserving gsfa simplicity ﬁrst focus isfa. isfa combines learning principles slowness principle information preservation without compromising former way. information preservation requires maximization mutual information output features input data. however ﬁnite discrete typically unique data samples diﬃcult measure maximize mutual information unless assumes probability model. therefore interpret information preservation practically minimize reconstruction error. closely related concept explained variance avoid term typically restricted linear transformations. isfa used hierarchical networks concept information preservation might also called information propagation. rest section ﬁrst present high-level description algorithm then describe construction detail show approximate inverse transformation discuss main properties. goal isfa improve feature extraction hsfa networks improving feature extraction node level achieved replacing nodes isfa nodes. therefore structure hsfa network preserved. roughly speaking slow part captures slow aspects data basically composed standard features except additional linear mixing step explained sections reconstructive part ignores slowness criteria instead focuses describing input linearly closely possible section show that although reconstructive features particularly slow contribute global slowness maximization. proposed algorithm takes care following important considerations. given output dimension decides many features slow reconstructive part contain. minimizes redundancy slow reconstructive part allowing output features compact higher information content. corrects amplitudes slow features make compatible figure block diagram isfa node showing components used training feature extraction linear input reconstruction. blocks signals explained text. def= i-dimensional training data output dimensionality expansion function first average sample def= removed training samples resulting centered data def= def= then expanded resulting vectors dimensionality afterwards instance linear created trained expanded data def= {zn}. slow features extracted expanded data denoted ﬁrst components def= denoted unit variance. centered data approximated linearly computing matrix vector follows. contribution slow part approximation centered data vector length since centered could discarded however gsfa used slow features weighted zero mean might improve approximation decomposition computed computed i.e. data remains data linearly reconstructed removed centered data. afterwards instance trained {un}. then reconstructive part deﬁned def= finally algorithm returns trained instances. output features usually computed feature extraction still keep simplify understanding signals involved. learned training data. algorithm shows single input sample processed however easily eﬃciently adapted process multiple input samples taking advantage matrix operations. interesting property isfa features nonlinear w.r.t. input data slow reconstructive part. slow part nonlinear expansion function. residual data nonlinear computed using slow part centered data. reconstructive part thus linear w.r.t residual data nonlinear w.r.t. input data. transformed scaled features. transformation necessary make amplitude slow features compatible amplitude features processing sets features together possible next layers. scaling used algorithm well sensitivity-based scaling explained below. methods ensure amplitude slow features approximately equal reduction reconstruction error allow. practice lower bound scales ensures features amplitudes even contribute reconstruction. scaling method ideally oﬀer properties pca. adds unit-variance noise principal components variance reconstruction error also increases unit. adds independent noise principal components simultaneously variance reconstruction error increases additively. mixing i.e. sfa) sfa) diagonal matrix diagonal elements sfa) sfa) general. .exp expansion motivated model slow features noisy harmonics increasing frequency hidden parameter applied slow features directly. thus mixture slow features would break assumed model might compromise slowness extraction next layers. technically mixing could reverted next layer would unnecessary complexity. reason besides scaling propose second scaling method. sensitivity based scaling scales slow features without mixing them follows. clearly transformation slow features scales them. reconstruction properties mentioned ﬁrst fulﬁlled columns unit norm. second fulﬁlled general orthogonal. ﬁrst sight seems like multiple-view learning might alternative scaling methods used here. however actually solves diﬀerent problem. moreover suitable task would slow reconstructive parts might expensive computationally cubic complexity isfa allows linear approximation input even though features computed nonlinear. contrast standard standard input reconstruction method although various gradient-descent vector-quantization methods tried limited success. linear reconstruction algorithm interesting properties. firstly simpler feature extraction algorithm nonlinear expansion wsfa needed. secondly computational complexity consists matrix-vector multiplications couple vector additions d-dimensional. linear reconstruction simple eﬀective isfa. describe nonlinear reconstruction algorithm. assume isfa feature representation sample denote isfa. since unknown generic nonlinear minimization algorithms. frequently algorithms require ﬁrst approximation conveniently provided linear reconstruction algorithm. although nonlinear reconstruction methods might result higher reconstruction accuracy typically expensive computationally. moreover discussion explain minimizing efeat necessarily improve reconstruction error unless aspects considered. clearly computational complexity isfa least because isfa consists additional computations. however none additional computations done expanded i-dimensional data d-dimensional data complexity respectively). therefore isfa slightly slower presentation focuses isfa. obtain information-preserving gsfa needs substitute gsfa inside isfa algorithm provide gsfa corresponding training graph training phase. notice gsfa features weighted zero-mean instead simple zero-mean enforced sfa. small diﬀerence already compensated vector section evaluate higsfa using problem estimation human face photographs. higsfa employed instead hisfa labels boost estimation accuracy. however close connection many aspects evaluation also extend hisfa. first estimation previous work introduced. then preprocessing images described. afterwards training graph learning race gender proposed. then higsfa network described evaluated according three criteria feature slowness estimation error linear reconstruction error study estimation photographs relatively recent useful applications human-computer interaction group-targeted advertisement demographics face recognition control age-related policies security. however estimation challenging task probably diﬀerent persons experience facial aging diﬀerently depending factors gender race life style nutrition health exposure weather creams cosmetics operations accidents even psychological traits. types distinguished. real chronological person apparent conveyed solely information present image. clearly algorithm cannot determine real exactly might determine apparent age. diﬀerent methods estimation proposed. comprehensive literature review refer aging pattern subspace proposed based temporal sequences images individual persons so-called aging patterns. images represented appearance model combines geometric texture information. system subspace constructed aging pattern. given image subspace providing best possible reconstruction found. then position image within aging pattern determined. proposed bio-inspired features architecture consists two-layers units ﬁrst layer compute gabor functions inspired simple cells whereas units second layer compute standard-deviation operation inspired complex cells. then applied reduce dimensionality data fewer features. finally provides ﬁnal estimate. ﬁrst architecture estimation four-layer hsfa network applied images directly without prior feature extraction input images synthetic created using special software dface modeling. however complexity face model probably simple allowed linear achieve good performance left open question whether could also successful real photographs. race gender factors inﬂuence accuracy estimation idea exploited system faces ﬁrst classiﬁed according race gender estimated particular race/gender group. algorithms allow estimation race gender simultaneously. proposed kernel partial least squares regression features. recently various methods based canonical correlation analysis proposed particularly regularized kernel features framework joint estimation race gender. morph-ii database large database available symbolic suitable estimation. contains images diﬀerent persons ages ranging years. images taken partially controlled conditions include variations head pose expression. database includes annotations stating persons gender race black white asian hispanic other coordinates eyes. procedure used assign race label unknown images black white races probably making diﬃcult generalize races asian. chose database large number images. follow evaluation method used many works. input images partitioned disjoint sets images images. racial gender composition same times images males females number white black people. races omitted. exactly pre-process input images steps pose normalization face sampling pose-normalization step ﬁxes position eyes ensuring that line horizontal inter-eye distance constant deﬁne dr-dataset train higsfa s-dataset train supervised step higsfa tdataset testing. s-datasets created training images t-dataset corresponding test images either s-test s-test. sampling include small random translation pixels rotation degrees rescaling well small ﬂuctuations average color contrast. exact transformations distributed uniformly corresponding intervals. although distortions frequently imperceptible teach higsfa become invariant small errors image normalization necessary feature speciﬁcity improve generalization test data. algorithms particular structures features mostly invariant small transformations construction distortions allow increase number images used training. images dr-dataset distorted times diﬀerent random distortions s-dataset times resulting images respectively. images t-dataset distorted used once. clustered training graph. graph generates features useful classiﬁcation equivalent illustrated figure optimization problem associated graph explicitly demands samples class typically mapped figure illustration clustered training graph gender classiﬁcation images males females. vertex represents image edges represent transitions. pairs serial training graph. graph consistently given good results diﬀerent regression problems. eﬃcient training graphs regression sliding window mixed graphs gsfa complementary explicit regression step features solves original regression problem. figure illustration serial training graph estimation. training images ﬁrst ordered increasing grouped groups samples each. represents image edges represent connections ovals represent groups. images ﬁrst last group weight remaining images weight weight edges theory unrestricted gsfa predicts mapping slowest feature suﬃce solve regression problem accurately requires slowest hidden parameter data strictly monotonic function label. practice however mapping slow features frequently gives better results. diﬀerent approaches implementing mapping proposed eﬃcient graph race gender estimation. based recent analysis optimal free responses gsfa propose combination eﬃcient graphs learn label single graph learns labels. compute combined graph must vertex edge weights. approach mathematically sound require conditions graphs samples consistent graphs node weights optimal free responses slow graph fast graph. requirements guarantee slowest optimal free responses combined graph span slowest optimal free responses original graphs. example combine clustered graph gender estimation another race ﬁrst features resulting graph enough gender race classiﬁcation. alternatively could create clustered graph four classes features would needed classiﬁcation instead however would impractical larger numbers classes. example original numbers classes would need extract learn race gender labels propose graph combines serial graph estimation clustered graph gender clustered graph race. serial graph node weights quite clustered graphs might aﬀect accuracy combined graph signiﬁcantly. comparison also serial graph learns age. years. compute average ages well order samples serial graph persons resolution however evaluation integer ground-truth labels integer estimates. computed using class membership probabilities. class probability input sample feature representation belongs group average label then estimated ﬁnal step keeps integer part equation particularly suited minimize root mean squared error although incurs error discretization labels soft nature estimation provided good accuracy robustness. besides evaluating higsfa resort hgsfa state-of-theart age-estimation algorithms comparison purposes. igsfa gsfa used directly higsfa hgsfa used take advantage beneﬁts hierarchical processing. structure higsfa hgsfa networks described table networks nodes composed igsfa gsfa various types nonlinear expansion functions. ﬁrst layer applied pixel data prior igsfa/gsfa preserving principal components. scale slow features sensitivity method section hyper-parameters hand-tuned achieve best accuracy estimation using educated guesses random sets diﬀerent used evaluation fewer image multiplicities speed process. hgsfa/higsfa networks diﬀer several aspects networks used literature example improve feature speciﬁcity lowest layers weight sharing used. moreover input nodes expansion functions mixture diﬀerent nonlinear functions subsets input vectors including identity function quadratic terms def= {xixj}n ij=. normalized version def= useful improve generalization resistance outliers table description higsfa hgsfa networks. networks number nodes general structure diﬀer type nodes number features preserve. layer denotes input image whereas layer node. size slow part layers depend ﬁxed features resp. parameter layers used layers number slow features ﬁxed resp. number features given supervised algorithm shown table tuned algorithm supervised problem. weighted values gsfa denote ∆drg depend graph turn depends training data labels. measure slowness test data compute standard values images ordered increasing label def= ∆tlin graph. cases scale features normalized variance before computing values prevent alterations feature scaling method. table average delta values ﬁrst three features extracted hgsfa higsfa training test data ﬁrst feature extracted stable according age-ordered linear graph indicates main feature coding age. comparison value unit-variance i.i.d. noise table shows higsfa outperforms hgsfa slowness maximization. ∆tlin values features larger surprising optimize slowness. since ∆drg ∆tlin computed diﬀerent graphs compared other. ∆tlin considers transitions images similar ages arbitrary race gender. ∆drg considers transitions images least gender race diﬀerent consecutive groups. real-life applications need coarse categorization broad groups. however applications beneﬁt precise estimation making convenient treat estimation regression problem requiring concrete numerical estimation usually expressed integer number years. three metrics measure estimation accuracy mean absolute error frequent metric estimation root mean squared error common loss function regression machine learning cumulative scores indicate fraction images estimation error given threshold. instance fraction estimates error years w.r.t. real age. include various thresholds facilitating future comparisons methods. although rmse sensitive outliers almost used literature estimation applications might beneﬁt stronger penalization larger estimation errors. accuracies summarized table table accuracy years state-of-the-art algorithms estimation morphii database classiﬁcation rates race gender estimation also provided. chance level best possible performance estimation constant. mistake evaluation protocol mcnn made training test data disjoint thus actual accuracy might diﬀer http//www.cbsr.ia.ac.cn/users/dyi/agr.html. hgsfa years better bif+step bif+kpls bif+rkcca similar bif+rkcca+svm worse mcnns however higsfa years better previous algorithms found published. contrast years largest mae. detailed cumulative scores higsfa hgsfa provided table poor accuracy estimation surprising principal components might lose wrinkles skin imperfections information could reveal adult persons might also explained principal components unstructured properly untangled soft method contrast slow features speciﬁc simple structure. behavior estimation errors function real plotted figure average older persons estimated much younger really are. part small number older persons database oldest class used supervised step average years making largest estimated. surprisingly persons years. -year-old persons years. reconstruction error provides indication much information output features retain original input. order compute assume linear global model input reconstruction. input data corresponding extracted features. matrix vector learned dr-dataset using linear regression def= approximates closely possible vector ones. matrix containing reconstructed samples def= reconstruction input given feature figure provides examples face reconstructions diﬀerent features. since model linear global output features mapped input linearly. gives result usual multiplication transposed projection matrix plus image average. alternative approach higsfa would pseudo-inversion algorithm perform reconstruction network bottom node time. normalized reconstruction error computed t-dataset reconstruction errors hgsfa higsfa using features given table largest reconstruction error results constant reconstruction expected hgsfa slightly better chance level worse higsfa closer pca. yields best possible features given linear global reconstruction method better higsfa higsfa output features slow features remaining reconstructive. using features yields reconstruction error hgsfa simply diﬀerent hyper-parameters evaluated performance higsfa network using hyper-parameters hgsfa network hyper-parameter present hgsfa tuned higsfa network. expected performance higsfa aﬀected increased years rmse increased years. reconstruction error improved slightly although suboptimal hyper-parameters affected higsfa still clearly superior hgsfa. evaluated inﬂuence estimation accuracy numerical stability testing diﬀerent values simplicity used layers experiment performance algorithm function shown table yielding minimum used optimized architecture table performance higsfa morph-ii database using diﬀerent results reported estimation errors reconstruction error erec percentile classiﬁcation rate race gender average number features nodes third higsfa layer. error measures computed test data. average number slow features third layer changed moderately depending ranging features diﬀerences ﬁnal error measures small. shows parameter critical easily tuned. investigated capability higsfa generalize diﬀerent database. used higsfa network trained images morph-ii database tested using images fg-net database test images taken uncontrolled conditions excluded original range years. protocol used create training test data frequently used estimation. however disadvantages. thus also adopt leave person protocol. eﬃciency reasons leave persons training instead one. test data created images persons chosen random whereas training data simply remaining images available times larger might improve generalization. since realistic applications might involve estimations unknown persons restrict test images persons appearing training data. test images belong persons seen training. improve evaluation accuracy repeating protocol several times original protocol repeated twice. alternative protocol training images. thus distort image times create dr-dataset create s-dataset. notice races appear training test data. therefore training consider classes virtual class r=w+a+h+o preserve binary clustered graph race estimation balance size classes. however better comparison compute race races considered. results shown table article propose extension hgsfa called hierarchical information-preserving gsfa complements slowness principle information preservation improving global slowness input reconstruction estimation accuracy supervised learning problems. analyze advantages limitations hsfa networks particularly phenomena unnecessary information loss poor input reconstruction. unnecessary information loss occurs node network prematurely discards information would useful slowness maximization another node higher hierarchy. poor input reconstruction written table average runs standard error mean. -fold cross validation. †used larger version database training images. alternative protocol ensured refers diﬃculty approximating input accurately feature representation. show phenomena result locally optimizing slowness yielding suboptimal global features. address shortcomings improve feature extraction local level. feature vectors computed igsfa nodes higsfa divided parts slow reconstructive part. features slow part follow slowness optimization goal slow features transformed linear scaling. features reconstructive part follow principle information preservation implement practice minimization also present method combine various eﬃcient training graphs single eﬃcient training graph. combined graph learn several labels simultaneously. construction simple; node edge weights graphs requirements graphs consistent proportional node weights. experimental results show higsfa better hgsfa terms feature slowness input reconstruction estimation accuracy. moreover higsfa oﬀers higher accuracies current state-of-the-art algorithms estimation including approaches based features convoluone cannot always identify information local level. therefore resort reconstruction goal preserve much information local input possible likely also include information relevant extract global slow parameters. addressed feature garbling brieﬂy aware complex problem needs better formalization. higsfa partially reduce problem reconstructive features might simpler slow features. however feature garbling still present slow part. features extracted higsfa better hgsfa quantitatively qualitatively. even unlimited training data computational resources available features extracted hgsfa necessarily converge higsfa. overﬁtting-free scenario information loss would decrease partially hgsfa main cause problem overﬁtting local optimization slowness. another combine slowness principle information preservation optimize single objective function integrates criteria favoring directions slow large variance. however found previous experiments balancing criteria diﬃcult practice. higsfa inherits close connection unsupervised learning. like gsfa emulated higsfa approximated training hisfa unsupervisedly data generated particular markov chain. emulation would incur small error unaware sample weights could ﬁxed using node weights weighting factors samples computation covariance matrix pca. selecting network structure appropriately computational complexity higsfa linear w.r.t. number samples dimensionality oﬀering feasible training times. hours whereas hisfa takes hours single computer without computing. however algorithm implemented gpus using distributed processing nodes layer trained independently. comparison system takes hours higsfa accurate hgsfa output dimensionalities network hyper-parameters. however higsfa yields even higher accuracies larger output dimensionalities used hgsfa network. explained various factors networks input dimensionality whereas higsfa features overﬁt reconstructive features setup. hgsfa features nodes attracted optimal free responses whereas higsfa reconstructive part attracted local principal components. thus hgsfa overﬁtting might accumulate layers higsfa. hgsfa network beneﬁt less input features faster features might noise-like result overﬁtting without providing much additional information. reconstructive. otherwise output features would reconstructive part removing advantages method. might happen many slow parameters mixtures higher-frequency harmonics present data. might avoid problem setting smaller directly controlling number slow features using training graphs small number optimal free responses choose problem estimation adult facial photographs because appears ideal problem test capabilities higsfa. estimation useful wrinkles skin texture higher-frequency features poorly represented. therefore obvious even counter-intuitive feature slowness improves incorporating higsfa. improvements feature slowness using supervised learning problems gender race horizontal-position estimation would inconclusive problems code discriminative information relatively well. estimate race gender simultaneously propose graph combines three graphs encoding sensitivity particular labels favoring invariance factor. graphs used serial graph estimation groups clustered graphs clusters race gender estimation. number features extracted graphs respectively since node weights graphs proportional exactly combined graph optimal responses still ﬁrst features contain relevant information method works well practice. reconstructive part used supervised step ﬁrst slow features output. shows higsfa hgsfa concentrate label information ﬁrst features. actually replace igsfa node higsfa network regular gsfa node features slow without aﬀecting performance. superiority estimation higsfa hgsfa thus principal components ﬁnal supervised step higher quality slow features. performance higsfa estimation highest reported morph-ii database years. previous state-of-the-art results years using multi-scale using bif+rkcca+svm even though system performs slightly better state-of-the-art algorithms focus here. goal improve hgsfa. thus claim higsfa better hgsfa regarding feature slowness input reconstruction estimation accuracy. experiments conﬁrm accurate higsfa reconstruction using features expected features optimal reconstruction linear. higsfa features reconstructive computed hierarchically contrast features global. thus encouraging between higsfa input reconstruction moderate. turn higsfa accurate hgsfa reconstruction secondary goal higsfa whereas hgsfa pursue reconstruction. improved reconstruction capability higsfa might facilitate certain applications morphing. since higsfa network implements nonlinear transformation nonlinear reconstruction algorithms also reasonable. nonlinear reconstruction might provide accurate reconstructions theory able train type algorithms well enough perform better test data simpler global linear reconstruction algorithm. algorithm nonlinear reconstruction minimizes feature error. however since number dimensions reduced network expect many samples feature error efeat diﬀer substantially valid input sample looking like face. correct problem might need consider input distribution select appropriate reconstruction. although generative adversarial networks originally designed generate random inputs might possible adapt reconstruction higsfa networks. estimation accuracy improved using complex hierarchical networks instance increasing overlap receptive ﬁelds using complex nonlinearities. usual training images might also improve accuracy could approximated implementing true face-distortion methods simple transformations image level. factor performance multi-scale receptive ﬁelds centered speciﬁc facial points idea could also applied higsfa might particularly boost generalization. informally tested higsfa problems good results. therefore interested formal systematic evaluation higsfa problems. idea complementing gsfa information preservation hierarchical networks also applied improved feature slowness hisfa hsfa might useful improve simulations based neuroscience. moreover existing neural models based slowness principle might beneﬁt incorporating information preservation. believe possible develop successful learning algorithms based simple strong learning principles heuristics approach pursue higsfa. much interested developing algorithm might strong cannot understood analytically. slow reconstructive parts extracted features seen information channels ﬁrst codes information connected slow parameters second codes information representing input. although information slow part somewhat mixed decomposed three channels. slowest feature mostly related race gender remaining ones age. therefore higsfa follows suggestions based ﬁndings neuroscience primate visual system successful computer vision namely hierarchical processing information-channel separation. proposed algorithm general purpose still capable outperforming special-purpose state-of-the-art algorithms least estimation problem. shows improved versatility robustness algorithm makes good candidate many problems computer vision high-dimensional data particularly lying intersection image analysis nonlinear feature extraction supervised learning.", "year": 2016}