{"title": "Unified Spectral Clustering with Optimal Graph", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.MM", "stat.ML"], "abstract": "Spectral clustering has found extensive use in many areas. Most traditional spectral clustering algorithms work in three separate steps: similarity graph construction; continuous labels learning; discretizing the learned labels by k-means clustering. Such common practice has two potential flaws, which may lead to severe information loss and performance degradation. First, predefined similarity graph might not be optimal for subsequent clustering. It is well-accepted that similarity graph highly affects the clustering results. To this end, we propose to automatically learn similarity information from data and simultaneously consider the constraint that the similarity matrix has exact c connected components if there are c clusters. Second, the discrete solution may deviate from the spectral solution since k-means method is well-known as sensitive to the initialization of cluster centers. In this work, we transform the candidate solution into a new one that better approximates the discrete one. Finally, those three subtasks are integrated into a unified framework, with each subtask iteratively boosted by using the results of the others towards an overall optimal solution. It is known that the performance of a kernel method is largely determined by the choice of kernels. To tackle this practical problem of how to select the most suitable kernel for a particular data set, we further extend our model to incorporate multiple kernel learning ability. Extensive experiments demonstrate the superiority of our proposed method as compared to existing clustering approaches.", "text": "spectral clustering found extensive many areas. traditional spectral clustering algorithms work three separate steps similarity graph construction; continuous labels learning; discretizing learned labels k-means clustering. common practice potential ﬂaws lead severe information loss performance degradation. first predeﬁned similarity graph might optimal subsequent clustering. well-accepted similarity graph highly affects clustering results. propose automatically learn similarity information data simultaneously consider constraint similarity matrix exact connected components clusters. second discrete solution deviate spectral solution since k-means method well-known sensitive initialization cluster centers. work transform candidate solution better approximates discrete one. finally three subtasks integrated uniﬁed framework subtask iteratively boosted using results others towards overall optimal solution. known performance kernel method largely determined choice kernels. tackle practical problem select suitable kernel particular data extend model incorporate multiple kernel learning ability. extensive experiments demonstrate superiority proposed method compared existing clustering approaches. clustering fundamental technique machine learning pattern recognition data mining past decades variety clustering algorithms developed k-means clustering spectral clustering. beneﬁts simplicity effectiveness k-means clustering algorithm often adopted various real-world problems. deal nonlinear structure many practical data sets kernel k-means algorithm developed data points mapped nonlinear transformation higher dimensional feature space data points linearly separable. usually achieves better performance standard k-means. cope noise outliers robust kernel k-means algorithm proposed. approach squared norm error construction term replaced norm. rkkm demonstrates superior performance number benchmark data sets. performance model-based methods heavily depends whether data model. unfortunately cases know distribution data advance. extent problem alleviated multiple kernel learning. moreover theoretical result choose similarity graph spectral clustering another widely used clustering method enjoys advantage exploring intrinsic data structures exploiting different similarity graphs data points three kinds similarity graph constructing strategies k-nearest-neighborhood \u0001-nearest-neighborhood; fully connected graph. here open issues arise choose proper neighbor number radius select appropriate similarity metric measure similarity among data points; counteract adverse effect noise outliers; tackle data structures different scales size density. unfortunately issues heavily inﬂuence clustering results nowadays many data often high dimensional heterogeneous without prior knowledge therefore fundamental challenge deﬁne pairwise similarity graph effective spectral clustering. recently construct robust afﬁnity graphs spectral clustering identifying discriminative features. adopts random forest approach based motivation tree leaf nodes contain discriminative data partitions exploited capture subtle weak data afﬁnity. approach shows better performance state-of-the-art methods including euclidean-distance-based dominant neighbourhoods consensus non-metric based unsupervised manifold forests second step spectral clustering spectrum similarity graph reveal cluster structure data. discrete constraint cluster labels problem np-hard. obtain feasible approximation solution spectral clustering solves relaxed version problem i.e. discrete constraint relaxed allow continuous values. ﬁrst performs eigenvalue decomposition laplacian matrix generate approximate indicator matrix continuous values. then k-means often implemented produce ﬁnal clustering labels although approach widely used practice exhibit poor performance since kmeans method well-known sensitive initialization cluster centers address aforementioned problems paper propose uniﬁed spectral clustering framework. jointly learns similarity graph data discrete clustering labels solving optimization problem continuous clustering labels serve intermediate products. best knowledge ﬁrst work combine three steps single optimization problem. show later trivial unify them. contributions work follows rather using predeﬁned similarity metrics similarity graph adaptively learned data kernel space. combining similarity learning subsequentl clustering uniﬁed framework ensure optimality learned similarity graph. unlike existing spectral clustering methods work three separate steps simultaneously learn similarity graph continuous labels discrete cluster labels. leveraging inherent interactions three subtasks boosted other. based single kernel model extend ability learn optimal combination multiple kernels. notations. given data denote rm×n features samples. sample element matrix denoted respectively. -norm vector deﬁned means transpose. squared frobenius norm denoted -norm matrix deﬁned absolute summaj |xij|. denotes identity matrix. trace operator. means elements nonnegative. sparse representation recently sparse representation assumes data point reconstructed linear combination data points shown power many tasks often solves following problem s.t. diag balancing parameter. simultaneously determines neighboring samples data point corresponding weights sparse reconstruction remaining samples. principle similar points receive bigger weights weights smaller less similar points. thus also called similarity graph matrix addition sparse representation enjoys nice properties e.g. robustness noise datum-adaptive ability hand model drawback i.e. consider nonlinear data sets data points reside union manifolds spectral clustering spectral clustering requires laplacian matrix rn×n input computed rn×n diagonal matrix i-th diagonal ele. traditional spectral clustering methods similarity graph rn×n often constructed three ways aforementioned. supposing clusters data spectral clustering solves following problem rn×c cluster indicator matrix represents clustering label vector point contains element indicate group membership discrete constraint problem np-hard. practice relaxed allow continuous values solve rn×c relaxed continuous clustering label matrix orthogonal constraint adopted avoid trivial solutions. optimal solution obtained eigenvectors corresponding smallest eigenvalues. obtaining traditional clustering method e.g. k-means implemented obtain discrete cluster labels although three-steps approach provides feasible solution comes potential risks. first since similarity graph computation independent subsequent steps optimal. discussed before clustering performance largely determined similarity graph. thus ﬁnal results degraded. second ﬁnal solution unpredictably deviate ground-truth discrete labels address problems propose uniﬁed spectral clustering model. model drawback assumes points union independent disjoint subspaces noiseless. presence dependent subspaces nonlinear manifolds and/or data errors select points different structures represent data point makes representation less informative recognized nonlinear data represent linearity mapped implicit higher-dimensional space kernel function. fully exploit data information formulate general manner kernelization framework. kernel mapping data samples input space reproducing kernel hilbert space transformed kernel similarity data samples deﬁned predeﬁned kernel kxixj applying kernel trick need know transformation space becomes model recovers linear relations among data space thus nonlinear relations original representation. general supposed learn arbitrarily shaped data structure. moreover goes back linear kernel applied. penalty parameters rotation matrix. spectral solution invariance property solution another solution. purpose last term proper orthonormal resulting close real discrete clustering labels. similarity graph ﬁnal discrete clustering labels automatically learned data. ideally whenever data points belong different clusters must also true vice versa. data points cluster equivalently therefore uniﬁed framework exploit correlation similarity matrix labels. feedback inferred labels induce similarity matrix vice versa clustering framework self-taught property. fact simple uniﬁcation pipeline steps. learns similarity graph optimal structure clustering. ideally exactly connected components clusters data laplacian matrix zero eigenvalues i.e. summation smallest eigenvalues zero. ensure optimali= information. multiple kernel learning ability integrate complementary information identify suitable kernel given task. present learn appropriate consensus kernel convex combination number predeﬁned kernel functions. note convex combination positive semideﬁnite kernel matrices still positive semideﬁnite kernel matrix. thus combined kernel still satisﬁes mercer’s condition. proposed method spectral clustering multiple kernels formulated model learn similarity graph discrete clustering labels kernel weights itself. iteratively updating iteratively reﬁned according results others. algorithm algorithm scsk input kernel matrix parameters initialize random matrices repeat update according diag) max. update according update solving problem update according update according stopping criterion met. complexity analysis optimization strategy updating requires complexity. quadratic program solved polynomial time. solution involves complexity update need complexity note number clusters often small number. therefore main computation load solving involves matrix inversion. fortunately solved parallel. model although model automatically learn similarity graph matrix discrete cluster labels performance strongly depend choice kernels. often impractical exhaustively search suitable kernel. moreover real world data sets often generated different sources along heterogeneous features. single kernel method able fully utilize database coil binary alpha digits data speciﬁcally coil contains images objects. object images taken degrees apart object rotating turntable. images object. image represented dimensional vector. consists digits letters capital examples class. yale jafee contain images individuals. image different facial expressions conﬁgurations times illumination conditions glasses/no glasses. kernel design assess effectiveness multiple kernel learning adopted kernels. include seven gaussian kernels form expx max)) dmax maximal distance samples varies linear kernel four polynomial kernels furthermore kernels rescaled dividing element largest pair-wise squared distance. comparison algorithms single kernel methods downloaded kernel kmeans spectral clustering robust kernel k-means scsk kernel separately. demonstrate advantage uniﬁed framework also implement three separate steps method i.e. learn similarity matrix spectral clustering k-means report best average results kernels. addition also implement recent simplex sparse representation method robust afﬁnity graph construction methods using random forest approach clustrf-u clustrf-a clustrf-u assumes tree nodes uniformly important clustrf-a assigns adaptive weight node. note three methods process data original feature space. moreover clusterf high demand memory cannot process high dimensional data directly. thus follow authors’ strategy perform reduce dimension. different numbers dominant components report best clustering results. nevertheless still cannot handle data them. algorithm algorithm scmk input kernel matrix initialize random matrices repeat calculate steps algorithm calculate calculate stopping criterion met. data sets altogether real benchmark data sets used experiments. table summarizes statistics data sets. among them ﬁrst image data four text corpora. http//www-users.cs.umn.edu/ han/data/tmdata.tar.gz http//www.cad.zju.edu.cn/home/dengcai/data/textdata.html http//www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html http//vision.ucsd.edu/content/yale-face-database http//www.ece.ohio-state.edu/ aleix/ardatabase.html http//www.kasrl.org/jaffe.html table clustering results obtained benchmark data sets. ’-m’ denotes average performance kernels. best results single kernel multiple kernel methods highlighted boldface. method need once. methods involve k-means follow strategy suggested i.e. repeat clustering times present results best objective values. number clusters true number classes clustering algorithms. results present clustering results different methods benchmark data sets table terms accuracy purity proposed methods obtain superior results. difference best average results conﬁrms choice kernels huge inﬂuence performance single kernel methods. motivates extended model multiple kernel learning. besides extended model multiple kernel clustering usually improves results model single kernel clustering. although best results three separate steps approach sometimes close proposed uniﬁed method average values often lower method. notice random forest based afﬁnity graph method achieves good performance image data sets. observation explained fact clustrf suitable handle ambiguous unreliable features caused variation illumination face expression pose data sets. hand effective text data sets. cases clustrf-a behaves better clustrf-u. justiﬁes importance considering neighbourhood-scaleadaptive weighting nodes. work address problems existing classical spectral clustering algorithms i.e. constructing similarity graph relaxing discrete constraints continuous one. alleviate performance degradation propose uniﬁed spectral clustering framework automatically learns similarity graph discrete labels data. cope complex data develop method kernel space. multiple kernel approach proposed solve kernel dependent issue. extensive experiments nine real data sets demonstrated promising performance methods compared existing clustering approaches. paper part supported grants natural science foundation china national high technology research development program china project uestc fundamental research fund central universities china", "year": 2017}