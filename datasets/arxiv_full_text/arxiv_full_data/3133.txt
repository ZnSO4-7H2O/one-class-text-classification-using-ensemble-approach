{"title": "Further heuristics for $k$-means: The merge-and-split heuristic and the  $(k,l)$-means", "tag": ["cs.LG", "cs.CV", "cs.IR", "stat.ML"], "abstract": "Finding the optimal $k$-means clustering is NP-hard in general and many heuristics have been designed for minimizing monotonically the $k$-means objective. We first show how to extend Lloyd's batched relocation heuristic and Hartigan's single-point relocation heuristic to take into account empty-cluster and single-point cluster events, respectively. Those events tend to increasingly occur when $k$ or $d$ increases, or when performing several restarts. First, we show that those special events are a blessing because they allow to partially re-seed some cluster centers while further minimizing the $k$-means objective function. Second, we describe a novel heuristic, merge-and-split $k$-means, that consists in merging two clusters and splitting this merged cluster again with two new centers provided it improves the $k$-means objective. This novel heuristic can improve Hartigan's $k$-means when it has converged to a local minimum. We show empirically that this merge-and-split $k$-means improves over the Hartigan's heuristic which is the {\\em de facto} method of choice. Finally, we propose the $(k,l)$-means objective that generalizes the $k$-means objective by associating the data points to their $l$ closest cluster centers, and show how to either directly convert or iteratively relax the $(k,l)$-means into a $k$-means in order to reach better local minima.", "text": "k-means clustering problem asks partition data clusters minimize squared euclidean distances data points closest cluster center. finding optimal k-means clustering d-dimensional data np-hard general many heuristics designed minimizing monotonically k-means objective function. heuristics trapped local minima thus heavily depend initial seeding cluster centers. celebrated k-means++ algorithm randomized seeding method guarantees probabilistically good initialization respect global minimum. paper ﬁrst show extend lloyd’s batched relocation heuristic hartigan’s single-point relocation heuristic take account empty-cluster single-point cluster events respectively. events tend increasingly occur increases performing several restarts k-means heuristic diﬀerent seeding round order keep best clustering lot. show special events blessing allow partially re-seed cluster centers minimizing k-means objective function. second describe novel heuristic called merge-and-split k-means consists merging clusters splitting merged cluster centers provided improves k-means objective. hartigan’s heuristic improve lloyd’s heuristic reaches local minimum similarly novel heuristic improve hartigan’s k-means converged local minimum. show empirically merge-and-split k-means improves hartigan’s heuristic facto method choice. finally propose -means objective generalizes k-means objective associating data points closest cluster centers show either directly convert iteratively relax -means k-means order reach better local minima. polynomial using dynamic programming setting center mass. note exponential number optimal k-means clustering yielding optimal objective function indeed consider equilateral triangle thus equivalent optimal clustering related rotational symmetries. make away separated copies consider optimal k-means clustering. minimizing k-means function equivalent minimizing intra-cluster squared distances maximizing inter-cluster squared distances many heuristics proposed overcome np-hardness k-means. classiﬁed main groups local search heuristics global heuristics used initialize local heuristics. example following four heuristics classically implemented totypes inducing partition. yields -approximation factor compared ordinary k-means using proof contradiction based variance-bias decomposition intra-cluster variances point cluster another cluster k-means cost strictly decreases reiterate single-point relocations convergence reached. note point maybe assigned cluster center closest center improvement heuristic cannot improve k-means score. pfqlh denotes maximum number stable k-means partitions obtained forgy’s macqueen’s lloyd’s hartigan’s schemes respectively. hartigan’s single-point relocation heuristic improved lloyd’s clustering converse note lloyd’s heuristic require exponential number iterations converge open question bound maximum number hartigan’s iterations. hand local heuristics performing pivots voronoi partitions using primitives initialization crucial obtain good clustering several restarts denoted mstart performed practice choose best clustering. practice forgy’s initialization replaced k-means++ provides expected competitive initialization. however shown exits point sets probability good initialization exponentially hand global k-means builds incrementally clustering adding -means objective function. thus initialization limited choosing ﬁrst point points considered ﬁrst starting point. however global k-means requires computation. paper address problem choosing appropriate number clusters model selection problem investigated also consider squared euclidean distance although results apply bregman divergence paper organized follows investigate blessing empty-cluster exceptions lloyd’s heuristic section single-point-cluster exceptions hartigan’s scheme section section describe novel heuristic merge-split-cluster k-means report performances respect hartigan’s heuristic. section present generalization k-means objective function point associated closest clusters means clustering. show directl convert iteratively relax sequence -means k-means compare experimentally solutions direct k-means. finally section wrap contributions discusses perspectives. euclidean distance relocates cluster centers centroids. batched assignment/relocation iterations repeated convergence reached k-means cost monotonically decreases guaranteed convergence ﬁnite number iterations complexity lloyd’s k-means denotes number iterations. proved lloyd’s k-means performs maximum number iterations exponential polynomial spread point point reported take iterations even ﬁrst report lower bound number lloyd’s stable optima centroids depicted large colored disks. bottom lloyd’s k-means local optimization technique produce empty cluster exceptions. consider points clusters random forgy initialization initial k-means cost ﬁrst iteration yields cost second iteration empty cluster exception green cluster. data data size number clusters result clustering partition ...ck point belongs exactly cluster initialization cluster centers choosing cluster prototypes random iter seeding empty cluster exception times) choose seeds empty clusters using k-means++ global k-means etc.; check convergence checking least diﬀerent previous iteration; iter maxiter hartigan’s heuristic proceeds relocating single point clusters provided k-means cost function decreases. thus decrease k-means score lloyd’s batched algorithm stuck local minimum recently hartigan’s heuristic suggested replace lloyd’s heuristic basis hartigan’s local minima subset lloyd’s optima argue true empty cluster exceptions lloyd’s iterations. figure illustrates data lloyd’s k-means meets empty-cluster exception. general relocation stage points assigned closest current centroids figure left graph plot frequency empty-cluster exceptions lloyd’s k-means using forgy’s initialization normalized iris data computed averaging million runs. right number eces depend initialization method observe frequency empty cluster empty clusters etc. forgy’s seeding k-means++ initialization produces less exceptions. however empty-cluster exceptions blessing seeds decrease signiﬁcantly cost k-means partial re-seeding. thus extended lloyd’s heuristic assignment relocation partial reseeding keep exactly non-empty clusters next stage. various heuristics partially re-seeding like evaluate frequency empty-cluster exceptions occur number take iris data repository consists samples features ﬁrst renormalize data-set coordinates dimension zero mean unit standard deviation. lloyd’s k-means random seed initialization mstart count number empty cluster exceptions report frequency graph figure observe larger frequent exceptions. phenomenon also noticed furthermore increases dimension however note tendency number empty-cluster exceptions vary data another mstart k-means report empirical frequency simultaneous empty-cluster exceptions. values). empty-cluster frequency depends initialization scheme higher using forgy’s heuristic lower using k-means++ global k-means. table demonstrates empirically observation. noticed number cluster-empty exceptions rise authors avoided problem setting minimum input table comparing lloyd’s k-means heuristics without without partial reseeding meeting empty-cluster exceptions iris dataset million restart using forgy’s initialization round. observe better local minima reached using partial reseeding empty-cluster exceptions. finally compare best minimum k-means score performing lloyd’s heuristic extended lloyd’s heuristic partially reseeds current clustering algorithm meets empty-cluster exceptions. partial reseeding done many ways starting current number cluster centers usual seeding methods table presents results proof concept using forgy’s re-seeding observe partial reseeding eces allows reach better local minima hartigan’s heuristic consider relocating single-point provided decreases k-means objective function. synthetic noisy data-set built probability tending initial random partition stable wrt. lloyd’s k-means hartigan’s converges correct solution. recall hartigan’s local minima subset lloyd’s minima provided lloyd’s heuristic encounter empty-cluster exceptions. note single-point cluster cannot relocated clusters since necessarily increases k-means energy relocation merge&re-seed operation decreases k-means loss. example classical hartigan’s best clustering k-means score heuristic partial reseeding novel heuristic merge-and-split-cluster k-means novel heuristic proceeds considering pairs clusters corresponding centers basic local search primitive consists computing best k-means score diﬀerence merging splitting centers since clusters denote voronoi partition induced untouched diﬀerence k-means score written -means brute-force method computes hyperplanes passing points induced variances below-above clusters o-time. using topological sweep reduced time. note unﬁxed -means np-hard also coresets -approximation -means linear time need compute explicitly equation hyperplane since clockwise/counterclockwise orientation predicates used instead. predicates rely computing sign matrix determinant. table average performance trials merge-and-split k-means heuristic compared hartigan’s discrete hartigan’s heuristics. common forgy’s initialization k-means implemented using optimal discrete -means. bottom common k-means++ initialization k-means implemented using -means++ observe experimentally heuristic yields always better performance hartigan’s discrete single-point relocation heuristic often signigicantly better hartigan’s heuristic. note k-means++ seeding performs better forgy’s seeding lower k-means score. heuristic classiﬁed macro kind hartigan-type heuristic based local voronoi assignment. indeed hartigan’s heuristic moves point and-split k-means converges ﬁnite number iterations. compare heuristic hartigan’s ordinary discrete variants consists moving point another cluster recomputed medoids selected clusters yield better k-means score. heuristic performances compared initialization averaging number rounds observe table heuristic always outperforms discrete hartigan’s method suprisingly. although number basic primitives lower operation costly. thus k-means overall time consuming gets better local optima solutions. note discrete -means medoid splitting procedure well suited k-modes algorithm k-means extension working categorical data sets. clustering -means objective function generalize k-means objective function follows data associate nearest cluster centers minimize following -means objective function stages. initial cost have assignment stage point assigned nearest neighbor centers nnl. c∈nnl ct−. relocation stage stop batched iterations. since iterations strictly decreases score function algorithm converges. moreover since number diﬀerent cluster sets induced means upper bounded cluster sets cannot repeated follows -means converges ﬁnite number iterations. bound improved considering l-order weighted voronoi diagrams similarly note basic lloyd’s -means also produce empty-cluster exceptions although become rarer increase although -means interesting also used k-means. indeed instead running local search k-means heuristic trapped soon local minimum prefer means prescribed convert -means assigning point closest neighbor -means) compute centroids launch regular lloyd’s k-means ﬁnalize ↓-means denote conversion. example converted -means beats k-means time mstart using forgy’s initialization iris. table shows experimentally converted -means beats average regular k-means phenomenon increases surprisingly however best minimum score often obtained classical k-means. thus suggests performs better number restarts limited. fact -means tend smooth k-means optimization landscape produce less local minima also smooth best minima. figure -means data point associated closest cluster center neighbors. converging relax -means solution keeping closest neighbor current centroids classic k-means. alternatively relax iteratively reached -means initialize means dropping point farthest cluster perform lloyd’s -means reiterate scheme means ordinary k-means. -means denote scheme. table presents performance comparisons regular lloyd’s k-means lloyd’s -means extended classical lloyd’s hartigan’s heuristics partial re-seeding proposed local heuristics k-means. summarize contributions follows first showed blessing empty-cluster events lloyd’s heuristic single-point-cluster events hartigan’s heuristic. events happen increasingly number cluster dimension increase running heuristics given number trials choose best solution. second proposed novel merge-and-split-cluster k-means heuristic improves hartigan’s heuristic currently facto method choice showed experimentally method brings better k-means result expense computational cost. third generalized k-means objective function -means objective function show directly convert iteratively relax -means heuristic k-means avoiding potentially trapped many local optima. -means another exploratory clustering technique browsing space hard clustering partitions. example k-means trapped consider -means local minimum convert -means k-means explore minimum.", "year": 2014}