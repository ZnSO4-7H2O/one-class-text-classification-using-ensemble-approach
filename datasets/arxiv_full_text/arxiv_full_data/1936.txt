{"title": "Self-Guiding Multimodal LSTM - when we do not have a perfect training  dataset for image captioning", "tag": ["cs.CV", "cs.CL", "cs.LG"], "abstract": "In this paper, a self-guiding multimodal LSTM (sg-LSTM) image captioning model is proposed to handle uncontrolled imbalanced real-world image-sentence dataset. We collect FlickrNYC dataset from Flickr as our testbed with 306,165 images and the original text descriptions uploaded by the users are utilized as the ground truth for training. Descriptions in FlickrNYC dataset vary dramatically ranging from short term-descriptions to long paragraph-descriptions and can describe any visual aspects, or even refer to objects that are not depicted. To deal with the imbalanced and noisy situation and to fully explore the dataset itself, we propose a novel guiding textual feature extracted utilizing a multimodal LSTM (m-LSTM) model. Training of m-LSTM is based on the portion of data in which the image content and the corresponding descriptions are strongly bonded. Afterwards, during the training of sg-LSTM on the rest training data, this guiding information serves as additional input to the network along with the image representations and the ground-truth descriptions. By integrating these input components into a multimodal block, we aim to form a training scheme with the textual information tightly coupled with the image content. The experimental results demonstrate that the proposed sg-LSTM model outperforms the traditional state-of-the-art multimodal RNN captioning framework in successfully describing the key components of the input images.", "text": "athe department computer science graduate center city university york york bthe department electrical engineering city college city university york york paper self-guiding multimodal lstm image captioning model proposed handle uncontrolled imbalanced real-world image-sentence dataset. collect flickrnyc dataset flickr testbed images original text descriptions uploaded users utilized ground truth training. descriptions flickrnyc dataset vary dramatically ranging short term-descriptions long paragraph-descriptions describe visual aspects even refer objects depicted. deal imbalanced noisy situation fully explore dataset itself propose novel guiding textual feature extracted utilizing multimodal lstm model. training m-lstm based portion data image content corresponding descriptions strongly bonded. afterwards training sg-lstm rest training data guiding information serves additional input network along image representations ground-truth descriptions. integrating input components multimodal block form training scheme textual information tightly coupled image content. experimental results demonstrate proposed sg-lstm model outperforms traditional state-of-the-art multimodal captioning framework successfully describing components input images. recent popularized language-vision community image captioning important task. involves generating textual description describes image analyzing visual content. automatic image captioning able assist solving computer vision challenges including image retrieval image understanding object recognition navigation blind many others. ambiguous. countless ways describe input image high-level descriptions explanations details semantically correct. fundamental cause principle descriptions image talk visual aspects varying object attributes scene features even refer objects depicted hidden interaction connection requires common sense knowledge analyze general image captioning data-driven task. descriptions query images normally deﬁned training data. therefore uncommon birth dataset task. recently amazon mechanical turk involved dataset description generation process. diﬀerent sets descriptions generated depending instructions provided speciﬁc captioning task. since indicating location date information. users input descriptions album instead photo. therefore multiple images look visually diﬀerent description. unlike labeling process performed workers content descriptions strictly controlled semantically syntactically. foreign languages exist along personal information including copyright statement camera information links personal social media accounts. existing natural language processing tools provide limited solution preparing training data. becomes tedious ﬁltering criteria regular expressions generate ‘perfect’ training dataset. however despite characteristics listed flickr data meets criterion image captioning task comes millions users describe anything related images upload. importantly real-world uncontrolled valuable resource. paper ‘new york city’ test case i.e. ‘new york city’ employed keyword query process build flickrnyc dataset. observed flickrnyc descriptions shorter lengths strongly correlated image content mainly related locations events activities. occur repetitively compared longer descriptions e.g. user uploaded several images related walk central park description ‘central park’. hand descriptions longer sentences paragraphs reveal syntactical details provide concepts implicitly related images weaker correlation image content. examples flickrnyc found fig. proposed framework self-guiding multimodal long short-term memory framework presented leverage portions data datas datal make part dataset reliable information guide training process caption generation. demonstrated fig. direct training utilizing state-of-the-art multimodal captioning method fails capture core event revealed image fact that flickrnyc noisy figure example description generated proposed sglstm image captioning framework compared result generated traditional multimodal rnn. frameworks trained flickrnyc dataset proposed paper. expensive process majority image captioning frameworks focus exploring existing datasets tend provide sentence description embedded objects attributes reactions scene image. frameworks tackle problem diﬀerent angle unambiguous descriptions image stream descriptions etc. paper work flickrnyc imagesentence dataset collected directly flickr. original descriptions provided users utilized training data. flickr data widely used dataset collection availability billions images. however descriptions provided users rarely used captioning purpose directly several characteristics flickr text data lengths descriptions vary dramatically image. users talk paragraphs details including possible background directly related image others describe words figure sample images corresponding descriptions flickrnyc dataset. examples datas images short descriptions. examples datal images long descriptions. real-world dataset multiple images labeled ‘thanksgiving’ visually diﬀerent. moreover thanksgiving celebration frequently seen chinese year celebration within training dataset. however proposed framework manages generate accurate description semantically syntactically correct. contributions proposed framework threefold novel image captioning framework proposed deal uncontrolled image-sentence dataset descriptions could strongly weakly correlated image content arbitrary lengths. self-guiding process looks learning process global balance syntactic correctness semantic details revealed images. scheme extended handle tasks imperfect training data. personal information including copyright camera info. urls social network accounts etc. different majority captioning datasets descriptions flickrnyc come original flickr users. experimental results demonstrate proposed guiding textual feature manages provide additional text information strongly correlates image content. compared existing traditional multimodal captioning framework self-guiding scheme able recover accurate descriptions given input image. rest paper organized follows related work discussed sec. sec. provides detailed description proposed image captioning framework based self-guiding multimodal lstm. collected dataset including experimental results presented sec. conclusions drawn sec. based underlying models utilized recent image captioning frameworks classiﬁed three categories. ﬁrst group approaches casts problem retrieval problem description test image generated searching similar images database. group models employs visual space measure similarity image search. descriptions similar images transferred obtain target description. yagcioglu utilized activations last layer visual geometry group convolutional neural network trained imagenet represent image features. description query image represented weighted average distributed representations retrieved descriptions. diﬀerent yagcioglu devlin employed n-gram overlap f-score descriptions measure description similarity. deep learning based approaches traditional machine learning techniques also utilized task second group methods adopts pre-deﬁned sentence templates generate image descriptions. missing components sentence structures ﬁlled based image understanding objects attributes correlations objects scene. elliott keller proposed sentence generation model parses query image visual dependency representation traversed missing slots templates. linguistically sophisticated approaches proposed tackle sentence generation. third group approaches integrates image understanding natural language generation uniﬁed pipeline. general image content terms objects actions attributes represented based visual features. later content information utilized drive language generation system e.g. recurrent neural network output image descriptions. frameworks model image text jointly multimodal space later joint representation space used perform cross-modal retrieval based query image. karpathy presented alignment model uses structured object align modalities multimodal embedding. encoder-decoder framework presented kiros utilizing joint multimodal space lstm success. another represented work category m-rnn model multimodal component introduced explicitly connect language model vision model one-layer representation. driven technical trials improvements computer vision also importantly availability relevant datasets. traditional image captioning task eﬀorts made special captioning tasks. modiﬁed m-rnn address task learning novel visual concepts. hendricks incorporated unpaired image data labeling unpaired text data address concept limitations image-sentence paired dataset. similarly venugopalan proposed novel object captioner describe object categories present existing image-sentence datasets. ‘referring expression’ explored generate unambiguous descriptions. park presented coherence recurrent convolutional network describe image stream storytelling manner utilizing blog data. later authors brought personalized image captioning framework counting users’ vocabulary previous documents ﬁll-in-the-blank image captioning task introduced proposed sg-lstm captioning framework falls third category. multimodal component utilized connect visual textual spaces. different existing methods novel guiding textual feature proposed emphasize correlation description image content. guiding text extracted separate m-lstm model serves additional input multimodal component. sizes formats descriptions collection process. earliest benchmark datasets pascalk proposed consists images selected pascal object recognition dataset image associated sentence descriptions generated amt. later based pascal action recognition dataset elliott keller introduced visual linguistic treebank images. utilized speciﬁc instructions generate three two-sentence descriptions image. object annotation available small subset images vdrs created manually images. flickrk flickrk roots images flickr. although images collected based user queries speciﬁc objects actions descriptions generated manner similar pascalk dataset workers provide captions image. original titles descriptions flickr directly utilized generate captions datasets. hand user-provided descriptions employed sbum contains approximately million captioned images flickr. strict ﬁltering applied downloaded image contain least noun verb predeﬁned control lists. coco dataset widely used recently image captioning evaluation images accompanied descriptions image. extensions coco dataset available meet speciﬁc needs various tasks e.g. question answering unambiguous descriptions text detection recognition d´ej`a image captions dataset makes existing data without additional human eﬀorts. consists million images unique captions lemmatization stop word removal employed normalize captions create corpus near-identical texts. although various datasets collected recently expensive human label speciﬁc instructions strict ﬁltering often required especially large datasets. however mentioned image descriptions come diﬀerent degrees abstraction i.e. descriptions could abstract several words short term detailed paragraph storytelling way. flickrnyc dataset collected real-world user data meets requirements image description naturally additional human eﬀorts dataset generation minimized without nlp-based normalization. successful combination especially lstm widely experimented image captioning related tasks. however observed generated sentence sometimes weakly coupled provided image strongly correlated high frequency sentences training dataset. fact generated sentence drifted away sequence prediction process. problem exists especially long sentences generation carried almost blindly towards sentence. address issue alternative extensions proposed adding attention mechanism modifying lstm cell however still challenging uncontrolled dataset descriptions arbitrary lengths abstraction levels. section ﬁrst introduce basic multimodal lstm image captioning framework fuses information input sentences corresponding image features multimodal component. works eﬀectively input sources strongly bonded. however case diﬃcult maintain correlation sentence generation goes especially training dataset ideal image captioning task. observed flickrnyc dataset descriptions shorter sentences tend stronger bond image content compared longer descriptions. although syntactically sound form sentence short descriptions tend accurately describe locations activities objects events images taken. examples found fig. core information images conveyed corresponding descriptions. hand long descriptions valuable users state feelings reasoning personal experiences objects figure systematic ﬂowchart proposed self-guiding multimodal lstm captioning framework. basic multimodal lstm captioning framework trained subset flickrnyc dataset short descriptions denotes t-th word sentence words ranging start sign wstart sign wend added training instances datas datal. guiding text feature extraction extract text feature self-guiding generate descriptions utilizing m-lstm followed sentence vectorizer. illustration sg-lstm architecture. compared m-lstm additional textual feature multimodal block encodes language information connected image content. figure best viewed color. order generate image captions adequate details related image content separate data based diﬀerent characteristics revealed. flicknyc divided subsets datas descriptions short sentences terms datal descriptions long sentences paragraphs start training m-lstm captioning model based datas. captioning model aims extract textual information provided input image. information later utilized guide training sg-lstm based datal better link description image content. guiding information represented sentence vectorizer another input multimodal component sg-lstm. simplicity. gated recurrent unit replaced lstm proposed m-lstm model. lstm network widely used model temporal dynamics sequences. compared traditional better addresses issue exploding vanishing gradients. basic lstm block consists memory cell stores state time gates control update state cell. illustrated fig. m-lstm composed word embedding layer lstm layer multimodal layer softmax layer. takes training images corresponding descriptions inputs. word sentences encoded one-hot representation m-lstm training. word embedding layer aims one-hot vector compact representation shown randomly initialize embedding layer learn training. subsection describe detail training sg-lstm datal. mentioned training instances datal strong connection textual description image content. words additional textual features needed training. therefore presented fig. guiding textual feature extractor proposed connects m-lstm captioning model trained datas sentence vectorizer. guidance feature aims provide additional textual information training instance datal tends emphasize correlation textual visual domains. compared basic m-lstm architecture sg-lstm carries additional information multimodal component. image feature guiding textual features multimodal component timestep auxiliary information. additional textual feature implicitly encodes semantic information related image location activity etc. sg-lstm architecture composed four layers timestep similar m-lstm. embedding layer encodes one-hot word representation dense word representation. weights embedding layer learnt training data aiming encoding syntactic semantic meaning words. word representation embedding layer serves input lstm layer. m-lstm adopt basic lstm block peepholes. layer multimodal layer connect cnn-based image feature dense word representation recurrent layer output proposed guiding texture feature. activations four inputs mapped multimodal feature space activation multimodal layer representation word embedding representation proposed mlstm model later sg-lstm adopt lstm peepholes memory cell gates within lstm block deﬁned denotes element-wise product. sigmoid nonlinearity-introduce function. basic reprehyperbolic tangent function. sent state values input gate output gate forget gate cell state hidden state respectively. denote weight matrices bias vectors corresponding gates states. word sequence lstm network iterating recurrence connection shown fig. inception used extract image features. connected language inputs though multimodal component. multimodal part fuses language information represented dense word embedding lstm activation image information represented using shown below element-wise scaled hyperbolic tangent function leads faster training process basic hyperbolic tangent function. indicate mapping weights learn during training. textual feature). beam search adopted process avoid exhaustive search exponential search space. widely used rnn-based captioning models eﬃciency eﬀectiveness. ranked sentence selected vectorization. fig. presents examples guiding texts generated m-lstm trained datas. images datal therefore original descriptions relatively long. demonstrated guiding texts either provide core information conveyed original descriptions e.g. authorship info landmark name season info emphasize image content buried long sentences e.g. event location also observe interesting results reveal underneath feelings images themselves e.g. ‘snow dirt love loneliness’. group sentence vectorizers investigated vectorize sentence term generated m-lstm. general adopt wordvec fusion scheme i.e. word sentence vectorized word vectors combined produce ﬁnal output. three wordvec schemes experimented wordvec-glove adopt pre-trained glove i.e. global vectors word representation word vectorizer. word vectors trained aggregated global wordword co-occurrence statistics corpus combining wikipedia gigaword test diﬀerent feature dimensions wordvec-nyc compared wordvec-glove wordvec-nyc local word vectorizer trained textual data flickrnyc. model trained utilizing gensim -dimensional vector generated word. word embedding mapping learnt training m-lstm datas. word vectorizer representation word embedding layer employed directly word -dimensional vector. various vectorization methods look mapping problem diﬀerent angles utilizing global corpus local dataset diﬀerent dimensionality. later shown table sg-lstm based wordvecglove tf-idf weighting feature dimension works best among vectorization schemes m-lstm log-likelihood cost function related perplexity utilized training sg-lstm shown normalization regarding number words corrects bias shorter sentences caption generation process therefore suitable flickrnyc images various lengths. section eﬀectiveness proposed selfguiding strategy veriﬁed experimentally flickrnyc. start deeper introduction flickrnyc dataset followed implementation details proposed system. afterwards experimental evaluation results presented analyzed. flickrnyc dataset composed images total collected flickr word ‘new york city’. speciﬁcally flickr search employed crawl image-description data based word i.e. photos whose title description tags contain ‘new york city’ fetched. capturing images corresponding metadata image accompanied reference description provided original user. images without valid descriptions discarded. perform light pre-processing utilizing nltk figure examples guiding text generated m-lstm compared original descriptions provided flickr users. guiding text provides supplementary information strongly related image content. figure best viewed color. textual pre-processing dataset divided based number words descriptions. images descriptions shorter form dataset datas images training testing. rest images form datal used training validation testing. table provides statistics distributions based description lengths flickrnyc. sample images diﬀerent traditional create vocabulary removes words contain nonalphanumeric characters even non-alphabetic characters vocabulary build-up process flickrnyc tricky since dataset based upon york city multiple landmark names contain combinations alphanumeric characters therefore numeric alphanumeric words eliminated vocabulary. moreover words contain connected punctuations also considered e.g. ‘macy’s’ ‘it’s’ ‘let’s’ ‘sight-seeing’ ’african-americans’ etc. although image accompanied description description restricted sentence. observed descriptions long containing multiple sentences. better model continuity paragraph sentences punctuations considered part vocabulary list. flickrnyc utilizes uncontrolled real-world text data indicates usage words informal. however sometimes informality valuable since reveals emotions users emoticons etc.) exaggerated expressions therefore order keep information mentioned above tokenization converted lowercase words appear least times training kept create vocabulary. ﬁnal vocabulary size proposed framework built upon m-rnn tensorflow inception pretrained imagenet used compute features image representation. feature dimension image representation m-lstm sg-lstm word embedding layer dimension. lstm layer multimodal layer dimensions. assign dropout rate three layers. m-lstm sg-lstm models trained rmsprop optimizer apply stochastic gradient descent mini-batches beam search size ranked sentence generated m-lstm based training data datas utilized guiding textual feature extraction. mentioned three wordvec schemes tested i.e. wordvec-glove wordvec-nyc wordvec-short. diﬀerent sets pre-trained word vectors tested wordvec-glove dimensions dimensions wordvec-nyc wordvec-short based order select best vectorization scheme guiding textual feature certain objective criterion needed evaluate scheme. popular evaluation metrics image captioning tasks include bleu meteor rouge-l cider however none criteria listed perfect metric evaluation task case since ground-truth descriptions flickrnyc dataset noisy. example found bottom left image fig. original description ‘december th’. hand proposed sg-lstm framework outputs description ‘boaters lake central park near bridge’ much better description compared original given image content. however superiority reﬂected numerical metrics listed above. despite challenges evaluating vectorization schemes proposed framework still exists large portion data flickrnyc suits ‘perfectly’ captioning task. therefore small validation dataset separated datal utilized evaluate different vectorization schemes. based experimental results sglstm-glove-tﬁdf- achieves best performance quantitatively. thus adopt sg-lstm based wordglove tf-idf weighting dimension ﬁnal setting results reported paper based setting unless stated otherwise. table presents numerical results based testing images datal. proposed sg-lstm framework compared m-rnn mlstm among diﬀerent vectorization settings. results performers previous veriﬁcation step included table. m-lstm-long represents m-lstm captioning model trained datal. shown table sglstm-glove-tﬁdf- gives best performance numerically almost among evaluated methods consistent observation veriﬁcation step. zero numbers shown table m-rnn might better explained looking results fig. direct training whole dataset tends preference high frequency sentences training dataset figure descriptions generated proposed framework compared m-rnn m-lstm-long m-lstm-full original descriptions provided flickr users. guiding texts also provided. help evaluation ground-truth locations marked image figure best viewed color. figure results generated proposed framework compared original descriptions provided flickr users. blue indicates precise description image content even appear original descriptions. green shows successful recovery landmarks. figure best viewed color. figure results generated proposed framework could improved. purple indicates wrong unrelated phrases. shows wrong location activity based original descriptions provided users. however cases locations activities cannot recovered solely based image content. figure best viewed color. unrelated test image itself. therefore comes numerical evaluations total miss core concept image content leads score. hand integrating guiding textual features training process proposed sg-lstm model manages generate accurate descriptions related image content sometimes generated descriptions meaningful original ones provided flickr users demonstrated fig. fig. provides examples comparing results generated proposed framework original descriptions provided flickr users. sglstm model accurately generates descriptions closely related image content successfully recovers image features landmarks. fig. several results presented could improved. additional preprocessing steps could performed training remove terms ‘more blog’ shown ﬁgure. techniques applied avoiding repetitive pattern shown ‘bear’ example. certain cases solely based image content diﬃcult generate accurate descriptions even yorkers differentiate ‘sunset’ ‘sunrise’ ‘brooklyn’ ‘bronx’ decide name certain building little information extent would better remove ambiguous information cannot predicted based image content ﬁnal description. correlates image content utilized train mlstm model extract textual features. afterwards additional features utilized guide training process caption generation based rest data. experimental results demonstrate eﬀectiveness proposed framework generating descriptions syntactically correct semantically sound.", "year": 2017}