{"title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular  Functions", "tag": ["cs.LG", "cs.AI"], "abstract": "Submodular function minimization is a fundamental optimization problem that arises in several applications in machine learning and computer vision. The problem is known to be solvable in polynomial time, but general purpose algorithms have high running times and are unsuitable for large-scale problems. Recent work have used convex optimization techniques to obtain very practical algorithms for minimizing functions that are sums of ``simple\" functions. In this paper, we use random coordinate descent methods to obtain algorithms with faster linear convergence rates and cheaper iteration costs. Compared to alternating projection methods, our algorithms do not rely on full-dimensional vector operations and they converge in significantly fewer iterations.", "text": "submodular function minimization fundamental optimization problem arises several applications machine learning computer vision. problem known solvable polynomial time general purpose algorithms high running times unsuitable large-scale problems. recent work used convex optimization techniques obtain practical algorithms minimizing functions sums simple\" functions. paper random coordinate descent methods obtain algorithms faster linear convergence rates cheaper iteration costs. compared alternating projection methods algorithms rely full-dimensional vector operations converge signiﬁcantly fewer iterations. past decades signiﬁcant progress minimizing submodular functions leading several polynomial time algorithms problem despite intense focus running times algorithms high-order polynomials size data designing faster algorithms remains central challenging direction submodular optimization. time technological advances made possible capture store data ever increasing rate level detail. natural consequence data\" phenomenon machine learning applications need cope data quite large growing fast pace. thus increasing need algorithms fast scalable. general purpose algorithms submodular minimization designed provide worst-case guarantees even settings structure exploit submodularity. extreme graph algorithms eﬃcient cannot handle general submodular functions. many applications functions strike middle ground extremes becoming increasingly important special structure obtain signiﬁcantly faster algorithms. following consider problem minimizing decomposable submodular functions expressed simple functions. term simple refer functions eﬃcient algorithm minimizing linear function. assume given black-box access minimization procedures simple functions. decomposable functions fairly rich class functions arise several applications machine learning computer vision. example model higher-order potential functions inference markov random ﬁelds cost functions models examples small number features graph hypergraph functions image segmentation. recent work developed several algorithms good empirical performance exploit special structure decomposable functions. particular shown problem minimizing decomposable submodular functions formulated distance minimization problem polytopes. formulation coupled powerful convex optimization techniques gradient descent projection methods yields algorithms fast practice simple implement theoretical side convergence behaviour methods well understood. recently nishihara made signiﬁcant progress direction. work shows classical alternating projections method applied distance minimization formulation converges linear rate. contributions. work random coordinate descent methods order obtain algorithms minimizing decomposable submodular functions faster convergence rates cheaper iteration costs. analyze standard accelerated random coordinate descent algorithm show achieve linear convergence rates. compared alternating projection methods algorithms rely full-dimensional vector operations faster factor equal number simple functions. moreover accelerated algorithm converges much smaller number iterations. experimentally evaluate algorithms image segmentation tasks show perform well converge much faster alternating projection method. submodular minimization. ﬁrst polynomial time algorithm submodular optimization obtained grötschel using ellipsoid method. several combinatorial algorithms problem among combinatorial methods orlin’s algorithm achieves best time complexity size ground maximum amount time takes evaluate function. several algorithms proposed minimizing decomposable submodular functions stobbe krause gradient descent methods sublinear convergence rates minimizing sums concave functions applied linear functions. nishihara give algorithm based alternating projections achieves linear convergence rate. function submodular sets function simple fast subroutine minimizing modular function paper consider problem minimizing submodular function form discrete problem admits exact convex programming relaxation based lovász extension submodular function. lovász extension written support function base polytope even though base polytope exponentially many vertices lovász extension evaluated eﬃciently using greedy algorithm edmonds given point edmonds’ algorithm evaluates using time time needed evaluate submodular function lovász showed function submodular lovász extension convex thus relax problem minimizing following non-smooth convex optimization problem lovász extension relaxation exact. given fractional solution lovász relaxation best threshold cost important drawback lovász relaxation objective function smooth. following previous work consider proximal version problem section give algorithm problem based random coordinate gradient descent method algorithm given figure algorithm easy implement uses oracles problems form miny∈b since function simple oracles eﬃcient. remainder section analyze convergence rate rcdm algorithm. emphasize objective function strongly convex thus cannot black-box nesterov’s analysis rcdm method minimizing strongly convex functions. instead exploit special structure problem achieve convergence guarantees match rate achievable strong convex objectives strong convexity parameter analysis shows rcdm algorithm faster factor alternating projections algorithm outline analysis analysis main components. first build work order prove theorem theorem exploits special structure problem allows overcome fact objective function strongly convex. second modify nesterov’s analysis rcdm algorithm minimizing strongly convex functions replace strong convexity guarantee guarantee given theorem start introducing notation; part follow notation write vector block n-dimensional constraint objective function partial derivatives. denote i-th block coordinates theorem follows theorem remainder section nesterov’s analysis conjunction theorem order show rcdm algorithm converges linear rate. recall optimal solutions devote rest section proof theorem recall following well-known lemma refer ﬁrst-order optimality condition. lemma diﬀerentiable convex function closed convex set. point solution problem minx∈q third line used fact agree coordinate blocks except ik-th block. fourth line used inequality last line used inequality rearrange terms inequality take expectation substitute obtain section give accelerated random coordinate descent algorithm algorithm uses approx algorithm fercoq richtárik subroutine. approx algorithm applied problem yields algorithm figure acdm algorithm runs sequence epochs epoch algorithm starts solution previous epoch runs approx algorithm iterations. solution constructed approx algorithm starting point next epoch. note that gradient dominated time compute projection. remainder section analysis together theorem order show acdm algorithm converges linear rate. follow notation used section theorem epochs acdm algorithm iterations) following lemma show objective function satisﬁes assumption thus convergence analysis given applied setting. lemma random subset coordinate blocks property independently random probability vectors rnr. vector block otherwise. proof follows lemma objective function random blocks used approx algorithm satisfy assumption thus apply theorem consider epoch solution constructed approx algorithm iterations starting miny∈e y‘+k optimal solution closest y‘+. denote random choices made epoch theorem algorithms. empirically evaluate compare following algorithms rcdm described section acdm described section alternating projections algorithm algorithm solves following best approximation problem equivalent algorithm starts point iteratively constructs sequence {}k≥ projecting onto projection operator onto minz∈k since subspace straightforward project onto projection onto implemented using oracles projections onto base polytopes functions three algorithms iteration cost dominated cost projecting onto base polytopes therefore total number projections suitable measure comparing algorithms. iteration rcdm algorithm performs single projection random block acdm algorithm performs single projection expectation. algorithm performs projections iteration block. image segmentation experiments. evaluate algorithms graph problems arise image segmentation inference tasks markov random fields. experimental setup similar image segmentation problems -neighbor grid graph unary potentials derived gaussian mixture models color features weight graph edge pixels function color vector pixel optimization problem solve segmentation task problem grid graph. function decomposition partition edges grid small number matchings decompose function using functions matchings. note straightforward project onto base polytopes functions using sequence projections onto line segments. duality gaps evaluate convergence behaviours algorithms using following measures. feasible solution proximal problem. deﬁne smooth duality diﬀerence additionally compute discrete duality discrete problem dual lovász relaxation; latter problem maxz∈b− applied elementwise solution discrete problem feasible solution dual lovász relaxation. deﬁne discrete duality diﬀerence objective values solutions evaluated algorithms four image segmentation instances figure shows smooth discrete duality gaps four instances. figure shows segmentation results instances.", "year": 2015}