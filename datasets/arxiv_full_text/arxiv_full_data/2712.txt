{"title": "Independently Controllable Factors", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "It has been postulated that a good representation is one that disentangles the underlying explanatory factors of variation. However, it remains an open question what kind of training framework could potentially achieve that. Whereas most previous work focuses on the static setting (e.g., with images), we postulate that some of the causal factors could be discovered if the learner is allowed to interact with its environment. The agent can experiment with different actions and observe their effects. More specifically, we hypothesize that some of these factors correspond to aspects of the environment which are independently controllable, i.e., that there exists a policy and a learnable feature for each such aspect of the environment, such that this policy can yield changes in that feature with minimal changes to other features that explain the statistical variations in the observed data. We propose a specific objective function to find such factors and verify experimentally that it can indeed disentangle independently controllable aspects of the environment without any extrinsic reward signal.", "text": "postulated good representation disentangles underlying explanatory factors variation. however remains open question kind training framework could potentially achieve that. whereas previous work focuses static setting postulate causal factors could discovered learner allowed interact environment. agent experiment diﬀerent actions observe eﬀects. speciﬁcally hypothesize factors correspond aspects environment independently controllable i.e. exists policy learnable feature aspect environment policy yield changes feature minimal changes features explain statistical variations observed data. propose speciﬁc objective function factors verify experimentally indeed disentangle independently controllable aspects environment without extrinsic reward signal. whether static dynamic environments decision making real world problems often confronted hard challenge ﬁnding good representation problem. context supervised semi-supervised learning argued good representations separate underlying explanatory factors causes observed data. problems feature learning often involves mechanisms autoencoders latent features explain observed data. interactive environments temporal dependency successive observations creates opportunity notice causal structure data apparent using observational studies. need experiment order discover causal relationships already well explored psychology reinforcement learning several approaches explore mechanisms push internal representations learned models good sense provide better *equal contribution random order mila université montréal elementai paris mcgill university école polytechnique université québec montréal cifar senior fellow propose explore direct mechanism representation learning explicitly links agent’s control environment internal feature representations. speciﬁcally hypothesize factors explaining variations data correspond aspects world controlled agent. example object could pushed around picked independently others independently controllable aspect environment. approach therefore aims jointly discover features policies policy controls associated feature leaving features unchanged much possible. explain mechanism show experimental results simplest instantiation principle. discuss principle could applied generally research challenges emerge. make intuitions concrete assume factors variation underlying observations coming interactive environment independently controllable. controllable factor variation exists policy modify factor only others. example object associated pixels could acted independently objects would explain variations pose scale move around leaving others generally unchanged. object position case factor variation. poses challenge discovering mapping factors computed features fact factors explicitly observed. goal agent autonomously discover factors call independently controllable features along policies control them. seem like strong assumptions nature environment argue assumptions similar regularizers meant make diﬃcult learning problem better constrained. many possible ways express preference learning independently controllable features objective. proposes objective simple scenario. illustrates eﬀect objective features environment simple controllable agent. moreover show objective propose strong enough recover underlying factors variation without additional reconstruction loss. generalize objective continuous representation factors policies. present experiments mazebase domain shows that using continuous embeddings able disentangle latent space controllable factors. show learnt representations used planning inferring sequence actions performed states. capturing main factors variation since factors variation present data controllable propose combine objectives encourage learned representation capture main factors variation encourage representation structured controllable factors disentangled factors. common method representation learning could used simplicity simple autoencoder framework throughout paper encoder decoder autoencoder viewed function approximators parameters maps input space latent space maps back input space autoencoders trained minimize discrepancy a.k.a. reconstruction error e.g. common case vanilla autoencoder assume causes perform dimensionality reduction i.e. compression since dimension bottleneck information input data must pass. often bottleneck forces optimization procedure uncover principal factors variation data trained. however necessarily imply diﬀerent components vector individually meaningful. fact note bijective transformation could obtain reconstruction error replacing expect form disentangling factors variation unless additional constraints penalties imposed motivates approach present. speciﬁcally preference policies separately inﬂuence coordinates want express preference learning representations make policies possible. note several ways discover disentangle underlying factors variation. many deep generative models including variational autoencoders descendants helmholtz machine generative adversarial networks non-linear versions attempt disentangle underlying factors variation assuming joint distribution factorizes i.e. marginally independent. explore another direction trying exploit ability learning agent world order impose constraint representation. disentangling independently controllable factors simplest case consider following simple scenario train autoencoder producing latent features tandem features train policies denoted agent’s observation categorical distribution actions autoencoders learn relatively arbitrary feature representations would like many features correspond controllable factors learner’s environment. speciﬁcally would like policy cause change features. think feature-policy pair. successive state representations action environment transition distribution action normalization factor denominator equation ensures selectivity maximal single feature changes result action. think reward signal control problem expected reward ea∼πk maximized ﬁnding optimal policies note many variations objective possible. example also possible directed selectivity using max{ instead absolute value numerator policies must learn increase learned latent feature rather simply change useful policy gradually increase feature distinct policy decreases using log-selectivity sharpened form log) also lead easier optimization. gradients lines computed exactly backpropagation. experiments gradient line also computed backpropagation sampling expectation gradient line computed reinforce estimator limitation approach algorithm requires potentially controllable factors small enumerated. makes sense simple environment always objects scene. realistic environments number possible objects present combinatorially large individual scene comprise ﬁnite number instances objects. therefore instead indexing possible factors integer propose index embedding i.e. real-valued vector. last section enforced variations environment captured coordinate view attribute variations inﬂuenced separately policies relax assumption indexing learned real-valued vector leading continuous attributes idea mapping symbolic entities distributed representation ingredients success deep learning exploited well. selecting attributes conditioned scene representation distribution policies feasible. samples distribution represent ways modify scene thus trigger internal selectivity reward signal. instance might represent room objects light switch. thought distributed representation name underlying factor associated policy value. setting light room could factor could either could associated policy turn binary value referring state called attribute feature value.we wish jointly learn policy modiﬁes scene control corresponding value attribute scene whose variation computed attribute variation selector function order distribution embeddings compute function random noise scenario strategy determine whether selected attribute variation evolves independently attributes variations compare value values obtained factors. thus compute following selectivity acts intrinsic reward signal generalizing approximate expectation sampling ﬁxed number factor embeddings. model trained jointly minimizing autoencoder reconstruction cost disentanglement objective lsel depicted figure ideally could arbitrary function e.g. implementing attribute selector neural network function harder optimize. instead observe discrete case mentioned previously using select attribute equivalent one-hot vector index simple step towards continuous embeddings relax constraint function random vector drawn uniform distribution compute however experiments used gaussian kernel φ||/) better numerical stability provides. unlike ﬁnite case sampling uniformly policies neural network choose probability distribution. could lead exploration issues. demonstrate simple strategies allow network learn simple distributions experiments experimental results order validate method learns independently controllable features perform several experiments. first basic gridworld-like setting agent allowed move around four directions. basic domain allows verify whether discrete case learning process disentangles underlying features recovers ground truth properties environment. simple gridworld ﬁrst experiment performed gridworld-like setting illustrated figure agent sees square pixel grid actions move down left right. interacting environment autoencoder directed selectivity without absolute value numerator) learns latent features position square without ever explicit access values reconstructing input properly. contrast plain autoencoder also reconstructs properly without learning latent features explicitly. note setting learning process robust stochastic version environment probability either action taken random action taken. successfully trained models recovering using architecture smaller learning rate. following architecture relu convolutional layers stride followed fully connected relu layer units tanh layer features; transpose architecture softmax policy actions computed output relu fully connected layer. adam perform gradient descent. figure simple gridworld actions push square left right down. left example ground truth right reconstruction model trained selectivity. slope linear regression true features function latent feature. white correlation blue indicate strong negative positive slopes respectively. recover recover policy column corresponds action cell average selectivity objective also experimentally training discrete independently controllable features without training autoencoder objective correctly recovers ground truth features associated control policies. albeit slower jointly training autoencoder shows objective propose strong enough provide learning signal discovering disentangled latent representation. train model gridworld mnist environment instead square mnist digits digits moved grid directional actions ﬁrst digit always second digit always even distiguishable. figure plot latent feature curve function ground truth. example black feature recovers horizontal position ﬁrst digit purple feature recovers vertical position second digit. figure gridworld environment objects know underlying features position digit four plots represents evolution fk’s function underlying feature left right them least recovers almost linearly pixels only. experiments mazebase mazebase assess performance continuous embeddings approach complex well-known environment. mazebase contains diﬀerent games agent solve speciﬁc task solve game deal one-step policies. setting agent move small environment perform actions down left right complexify disentanglement task redundant action well action down+left. agent anywhere except orange blocks. figure show learned representation underlying factor variation learned representation clusters vectors possible decompose variation arbitrary state representations small variations along trajectory continuous policy embeddings consider model described architecure follows encoder mapping pixel state latent representation -layer convolutional neural network batch normalization leaky relu activations. decoder uses transposed architecture relu activations. noise sampled -dimensional gaussian distribution generator policy neural networks consisting fullyconnected layers. attribute selector gaussian kernel. practice minibatch vectors sampled step. agent randomly choses φbehavior samples action model parameters updated using policy gradient importance sampling. selectivity reward term φ)|] estimated φi)|. jointly training reconstruction selectivity losses algorithm disentangles four directed factors variations seen figure ±x-position ±y-position agent. visualization purposes rest section chose bottleneck autoencoder size disentanglement appears clearly latent features corresponding position orthogonal latent space. moreover notice algorithm assigns actions feature. also create signifant mode feature corresponding action down+left feature already explained features left. figure sampling variations kernel density estimation encountered sampling random controllable factors observe algorithm disentangles representations main modes corresponding action actually taken agent. disentangled stucture latent space. axis disentangled recover position agent observation simply looking latent encoding missing point grid position agent cannot reach lies orange block. towards planning policy inference disentangled structure could used address many challenging issues reinforcement learning. give examples ﬁgure simpliﬁed deterministic policy inference problem given initial state sstart terminal state sgoal suitable action sequence a{t−} sgoal reached sstart following tanh activation last layer diﬀerent factors variation placed vertices hypercube dimension think policy inference problem ﬁnding path simpler space starting point hstart goal hgoal. believe could prove much easier problem solve. figure predicting eﬀect cause mazebase. leftmost image visual input environment agent round circle switch states represented shades green. training able distinguish cluster variation obtained performing action independently position therefore able move agent adding corresponding latent representation second image reconstruction obtained feeding resulting decoder. given starting state goal state able decompose diﬀerence representations sequence movements. however disentangled representation alone cannot solve completely issues arbitrary environment. indeed factors able disentangle factors directly controllable agent thus able account ambiant dynamics agents’ inﬂuence. related work large body work learning features focusing indirectly learning good internal representations. jaderberg agents learn oﬀ-policy control pixel inputs forcing learn features help control environment propose models learn predict future conditioned action sequences push agent capture temporal features. many works direction successor feature representations options framework used conjunction neural networks approach similar spirit horde architecture scenario agents learn policies maximize speciﬁc inputs whereas learn policies control simultaneously learned features input. predictions policies become features agent. objective deﬁned speciﬁcally context autoencoders generalized representation-learning frameworks. unlike recent work predictron approach focused solving planning task goal simply learn agents control environment. introduced novel type clue aiming learning representations disentangle underlying factors variation. main assumption factors correspond independently controllable aspects environment. leads training frameworks learns jointly exploratory policies corresponding features learned representation disentangle controlled aspects. ﬁrst step towards training agents learn control environments time learning good representations focused simpler setups environment made static objects. case objective posited learned correctly assume feature representation unambiguously refer controllable property speciﬁc object environment. example agent’s world might contain circle green rectangle aﬀected actions agent change positions colours objects trial next. hence speciﬁc feature learn unambiguously refer position colour objects. reality environments stochastic objects given scene drawn distribution. number objects vary types diﬀerent. becomes less obvious feature could refer clear feature objects particular scene. instances objects diﬀerent types addressing naming scheme required refer particular objects present scene match policy particular attribute particular object selectively modify. proposed distributed alternative attempt address this fundamental representational problem remains. connected binding problem neuro-cognitive science represent objects diﬀerent attributes confuse example {red circle blue square} {red square blue circle}. binding problem received attention representation learning literature still remains mostly unsolved. jointly considering problem learning controllable features prove fruitful. ideas also lead interesting ways performing exploration. exploration process could driven notion controllability predicting interestingness objects scene choosing features associated policies attempt controlling ideas brieﬂy explored literature humans choose object play? attracted objects know control them process critical learn world works. yoshua bengio. learning deep architectures publishers matteo hessel schaul arthur guez harley gabriel dulac-arnold david reichert neil rabinowitz andre barreto thomas degris david silver hado hasselt. predictron end-to-end learning planning. arxiv klaus greﬀ antti rasmus mathias berglund tele harri valpola juergen schmidhuber. tagger deep unsupervised perceptual grouping. advances neural information processing systems pages sergey ioﬀe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. international conference machine learning pages jaderberg volodymyr mnih wojciech marian czarnecki schaul joel leibo david silver koray kavukcuoglu. reinforcement learning unsupervised auxiliary tasks. arxiv preprint arxiv. yann lecun. mnist database handwritten digits. http//yann.lecun.com/exdb/mnist/ alexey minin alois knoll hans-georg zimmermann siemens siemens. complex valued artiﬁcial recurrent neural network novel approach model perceptual binding problem. esann. citeseer junhyuk xiaoxiao honglak richard lewis satinder singh. action-conditional video prediction using deep networks atari games. advances neural information processing systems pages modayil delp degris pilarski white precup-d. sutton horde scalable real-time architecture learning knowledge unsupervised sensorimotor interaction. aamas richard sutton doina precup satinder singh. mdps semi-mdps framework temporal abstraction reinforcement learning. artiﬁcial intelligence pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting", "year": 2017}