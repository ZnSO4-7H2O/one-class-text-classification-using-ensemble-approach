{"title": "What to talk about and how? Selective Generation using LSTMs with  Coarse-to-Fine Alignment", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "We propose an end-to-end, domain-independent neural encoder-aligner-decoder model for selective generation, i.e., the joint task of content selection and surface realization. Our model first encodes a full set of over-determined database event records via an LSTM-based recurrent neural network, then utilizes a novel coarse-to-fine aligner to identify the small subset of salient records to talk about, and finally employs a decoder to generate free-form descriptions of the aligned, selected records. Our model achieves the best selection and generation results reported to-date (with 59% relative improvement in generation) on the benchmark WeatherGov dataset, despite using no specialized features or linguistic resources. Using an improved k-nearest neighbor beam filter helps further. We also perform a series of ablations and visualizations to elucidate the contributions of our key model components. Lastly, we evaluate the generalizability of our model on the RoboCup dataset, and get results that are competitive with or better than the state-of-the-art, despite being severely data-starved.", "text": "domainindependent neural encoder-aligner-decoder model joint task content selection surface realization. model ﬁrst encodes full over-determined database event records lstm-based recurrent neural network utilizes novel coarse-to-ﬁne aligner identify small subset salient records talk about ﬁnally employs decoder generate free-form descriptions aligned selected records. model achieves best selection generation results reported to-date benchmark weathergov dataset despite using specialized features linguistic resources. using improved k-nearest neighbor beam ﬁlter helps further. also perform series ablations visualizations elucidate contributions model components. lastly evaluate generalizability model robocup dataset results competitive better state-of-the-art despite severely data-starved. consider important task producing natural language description rich world state represented over-determined database event records. task refer selective generation often formulated subproblems content selection involves choosing subset relevant records talk exhaustive database surface realization concerned generating natural language descriptions subset. learning perform tasks jointly challenging ambiguity deciding records relevant complex dependencies selected records multiple ways records described. previous work made signiﬁcant progress task however approaches solve content selection surface realization subtasks separately manual domain-dependent resources features employ template-based generation. limits domain adaptability reduces coherence. take alternative neural encoder-aligner-decoder approach free-form selective generation jointly performs content selection surface realization without using specialized features resources generation templates. enables approach generalize domains. further memorybased model captures long-range contextual dependencies among records descriptions integral task formulate model encoder-alignerdecoder framework uses recurrent neural networks long short-term memory units together coarse-to-ﬁne aligner select translate rich world state natural language description. model ﬁrst encodes full over-determined event records using bidirectional lstm-rnn. novel coarse-to-ﬁne aligner reasons multiple abstractions input decide records discuss. model next employs lstm decoder generate natural language descriptions selected records. lstms proven effective similar long-range generation tasks allows model capture longrange contextual dependencies exist selective generation. further introduction proposed variation alignment-based lstms enables model learn perform content selection surface realization jointly aligning generated word event record decoding. novel coarse-to-ﬁne aligner avoids searching full over-determined records employing stages increasing complexity pre-selector reﬁner acting multiple abstractions record input. end-to-end nature framework advantage trained directly corpora record sets paired natural language descriptions without need ground-truth content selection. evaluate model benchmark weather forecasting dataset achieve best results reported to-date content selection language generation despite using domain-speciﬁc resources. also perform series ablations visualizations elucidate contributions primary model components also show improvements simple k-nearest neighbor beam ﬁlter approach. finally demonstrate generalizability model directly applying benchmark sportscasting dataset results competitive better state-of-the-art despite extremely data-starved. selective generation relatively research area attention paid individual content selection selective realization subproblems. regards former barzilay model content structure unannotated documents apply application text summarization. barzilay lapata treat content selection collective classiﬁcation problem simultaneously optimize local label assignment pairwise relations. liang address related task aligning records given textual description clauses. propose generative semi-markov alignment model jointly segments text sequences utterances associates corresponding record. surface realization often treated problem producing text according given grammar. soricut marcu propose language generation system uses widl-representation formalism used compactly represent probability distributions ﬁnite sets strings. wong mooney synchronous context-free grammars generate natural language sentences formal meaning representations. similarly belz employs probabilistic context-free grammars perform surface realization. effective approaches include tree conditional random ﬁelds template extraction within log-linear framework recent work seeks solve full selective generation problem single framework. chen mooney chen learn alignments comments corresponding event records using translation model parsing generation. mooney implement two-stage framework decides discuss using combination methods liang produces text based generation system wong mooney angeli propose uniﬁed conceptto-text model treats joint content selection surface realization sequence local decisions represented log-linear model. similar work train model using external alignments liang generation follows inference model ﬁrst choose event record record’s ﬁelds ﬁnally templates words selected ﬁelds. ability model long-range dependencies relies choice features log-linear model template-based generation employs konstas lapata propose alternative method simultaneously optimizes content selection surface realization problems. employ probabilistic context-free grammar speciﬁes structure event records treat generation ﬁnding best derivation tree according grammar. however method still selects orders records local fashion markovized chaining records. konstas lapata improve upon approach global document representations. however approach also requires alignment training estimate using method liang treat problem selective generation end-to-end learning recurrent neural network encoder-aligner-decoder model enables jointly learn content selection surface realization directly database-text pairs without need external aligner ground-truth selection labels. lstm-rnns enables model capture long-range dependencies exist among records natural language output. additionally model rely manually-selected domain-dependent features templates parsers thereby generalizable. alignment-rnn approach recently proven successful generation-style tasks e.g. machine translation image captioning since selective generation requires identifying small number salient records among over-determined database avoid performing exhaustive search full record instead propose novel coarse-toﬁne aligner divides search complexity pre-selection reﬁnement stages. consider problem generating natural language description rich world state speciﬁed terms over-determined records problem requires deciding records discuss discuss training data consists scenario pairs complete records natural language description robocup evaluate model’s generalizability sportscasting dataset chen mooney consists pairs temporally ordered robot soccer events commentary drawn four-game robocup ﬁnals scenario contains average event records word natural language commentary. multi-level representation input compute selection decision decoding step model employs decoder arrive word likelihood function multi-level input hidden state decoder time step order model long-range dependencies among records descriptions model employs lstm units nonlinear encoder decoder functions. encoder lstm-rnn encoder takes input records represented sequence returns sequence hidden annotations annotation summarizes record results representation models dependencies exist among records database. afﬁne transformation logistic sigmoid restricts input input forget output gates memory cell actilstm respectively vation vector. memory cell summarizes current inlstm’s previous memory modulated forget input gates respectively. encoder operates bidirectionally encoding records forward backward directions provides better summary input records. hid) concatenate forden annotations goal inference generate natural language description given records. effective means learning perform generation encoder-aligner-decoder architecture recurrent neural network proven effective related problems machine translation image captioning propose variation general model novel components well-suited selective generation problem. model ﬁrst encodes input record hidden state using bidirectional recurrent neural network novel coarse-to-ﬁne aligner acts concatenation record hidden state these records take form unordered natural ordering order make model generalizable treat sequence order speciﬁed dataset. note possible different ordering yield improved performance since ordering shown important operating sets pre-selector assigns large values small subset salient records small values rest. modulates standard aligner assign large weight order select j-th record time learned prior makes difﬁcult alignment distracted nonsalient records. further relate output pre-selector number records selected. speciﬁcally output expresses extent j-th record selected. regarded real-valued approximation total number pre-selected records regularize towards based validation decoder architecture uses lstm decoder takes input current context vector last word lstm’s previous hidden state st−. decoder outputs conditional probability distribution next word represented deep output layer training inference train model using database-record pairs training corpora maximize likelihood ground-truth language description additionally introduce regularization term enables model inﬂuence pre-selector weights based aforementioned relationship output preselector number selected records. moreover also introduce term accounts fact least record pre-selected. note equal model seeks select content time step used generation. model performs content selection using extension alignment mechanism proposed bahdanau allows selection generation independent ordering input. given event records over-determined small subset salient records relevant output natural language description. standard alignment mechanisms limit accuracy selection generation scanning entire range overdetermined records. order better address selective generation task propose coarse-toﬁne aligner prevents model distracted non-salient records. model aligns based multiple abstractions input original input record well hidden annotations approach previously shown yield better results aligning based hidden state coarse-to-ﬁne aligner avoids searching full over-determined records using stages increasing complexity pre-selector reﬁner pre-selector ﬁrst assigns record probability selected standard aligner computes alignment likelihood records time step decoding. next reﬁner produces ﬁnal selection decision re-weighting aligner weights pre-selector probabilities tanh sigmoid vtanh learned parameters. ideally selection decision would based highestvalue alignment maxj αtj. however weighted average soft approximation maintain differentiability entire architecture. pre-selector forced select records coarse-to-ﬁne alignment reverts standard alignment introduced bahdanau together negative loglikelihood ground-truth description loss function becomes trained model generate natural language description ﬁnding maximum posteriori words learned model inference perform greedy search starting ﬁrst word beam search offers perform approximate joint inference however empirically found beam search perform better greedy search datasets consider observation shared previous work later discuss alternative k-nearest neighbor-based beam ﬁlter datasets analyze model benchmark weathergov dataset data-starved robocup dataset demonstrate model’s generalizability. following angeli weathergov training development test splits size respectively. robocup follow evaluation methodology previous work performing three-fold cross-validation whereby train three games test fourth. within split hold training data development tune early-stopping criterion report standard average performance four splits. choose units robocup tune development choose however retune number hidden units robocup. iteration randomly sample mini-batch scenarios back-propagation adam optimization. training typically converges within epochs. select model according bleu score development set. evaluation metrics consider metrics means evaluating effectiveness model selective generation subproblems. content selection score selected records deﬁned harmonic mean precision recall respect ground-truth selection record set. deﬁne selected records consisting record largest selection weight computed aligner decoding step evaluate quality surface realization using bleu score generated description respect human-created reference. comparable previous results weathergov also consider modiﬁed bleu score penalize numerical deviations robocup also evaluate bleu score case groundtruth content selection known comparable previous work. analyze effectiveness model benchmark weathergov robocup datasets. also present several ablations illustrate contributions primary model components. primary results report performance content selection surface realization using bleu scores respectively table compares test results previous methods include method achieves best results reported to-date three metrics relative improvements previous state-of-the-art. beam filter k-nearest neighbors considered beam search alternative greedy search primary setup performs worse similar previous work found dataset alternative consider beam ﬁlter based knearest neighborhood. supplementary material details. table shows k-nn beam ﬁlter improves results primary greedy results. aligner ablation first evaluate contribution proposed coarse-to-ﬁne aligner comparing model basic encoder-alignerintroduced bahdanau decoder model encoder ablation next consider effectiveness encoder. table compares results without encoder development demonstrates signiﬁcant gain encoding event records using lstmrnn. attribute improvement lstmrnn’s ability capture relationships exist among records known essential selective generation qualitative analysis output examples fig. shows example record output description recordword alignment heat map. shown model learns align records corresponding words also learns subset salient records talk also word-level mismatch e.g. cloudy misaligns temp precipchance attribute high correlation types records embeddings capture semantic relationships among training words. table presents nearest neighbor words common words weathergov dataset details embedding approaches tried discussed supplementary material section. out-of-domain results robocup dataset evaluate domain-independence model. dataset severely data-starved training pairs much smaller typically necessary train rnns. results higher variance trained model distributions thus adopt standard denoising method ensembles following previous work perform experiments robocup dataset ﬁrst considering full selective generation second assuming ground-truth content selection test time. former obtain standard bleu score exceeds best score additionally achieve selection score also best result reported to-date. case assumed ground-truth content selection model attains sbleug score competitive state-of-the-art. tion templates. model employs bidirectional lstm-rnn model novel coarse-toﬁne aligner jointly learns content selection surface realization. evaluate model benchmark weathergov dataset achieve state-of-the-art selection generation results. achieve improvements k-nearest neighbor beam ﬁlter. also present several model ablations visualizations elucidate effects primary components model. moreover model generalizes different data-starved domain achieves results competitive better state-of-the-art. beam filter k-nearest neighbors perform greedy search approximation full inference decision variables considered beam search alternative previous work dataset found greedy search still yields better bleu performance alternative consider beam ﬁlter based k-nearest neighborhood. first generate m-best description candidates given input record using standard beam search. next nearest neighbor database-description pairs training data based cosine similarity neighbor database given input record. compute bleu score description candidates relative nearest neighbor descriptions select candidate highest bleu score. tune development report results table table presents test results tuned setting achieve bleu scores better primary greedy results. training decoder effect learning embeddings words training here explore extent learned embeddings capture semantic relationships among training words. table presents nearest neighbor words common words weathergov dataset also consider different ways using pretrained word embeddings bootstrap quality learned embeddings. approach initializes embedding matrix pre-trained vectors reﬁnes embedding based training corpus. second concatenates learned embedding matrix pre-trained vectors effort simultaneously exploit general similarities well learned domain. shown previously tasks pre-trained embeddings results negligible improvements references gabor angeli percy liang klein. simple domain-independent probabilistic approach proceedings conference generation. empirical methods natural language processing pages regina barzilay mirella lapata. collective content selection concept-to-text generation. proceedings human language technology conference conference empirical methods natural language processing pages regina barzilay lillian lee. catching drift probabilistic content models applications proceedings generation summarization. conference north american chapter association computational linguistics human language technologies pages fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. nips workshop deep learning unsupervised feature learning. james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math expression david chen raymond mooney. learning sportscast test grounded language acquisition. proceedings international conference machine learning pages david chen joohyun raymond mooney. training multilingual sportscaster using perceptual context learn language. journal artiﬁcial intelligence research alex graves mohamed abdel-rahman geoffrey hinton. speech recognition deep recurrent proceedings ieee interneural networks. national conference acoustics speech signal processing pages andrej karpathy fei-fei. deep visualsemantic alignments generating image descriptions. proceedings ieee conference computer vision pattern recognition pages joohyun raymond mooney. generative alignment semantic parsing learning proceedings ambiguous supervision. international conference computational linguistics pages diederik kingma jimmy ioannis konstas mirella lapata. unsupervised concept-to-text generation hypergraphs. proceedings conference north american chapter association computational linguistics human language technologies pages ioannis konstas mirella lapata. inducing docproument plans concept-to-text generation. ceedings conference empirical methods natural language processing pages percy liang michael jordan klein. learning semantic correspondences less superviproceedings joint conference sion. annual meeting association computational linguistics international joint conference natural language processing pages hwee luke zettlemoyer. generative model parsing natural language meaning representations. proceedings conference empirical methods natural language processing pages hwee lee. natural language generation tree conditional random ﬁelds. proceedings conference empirical methods natural language processing pages tomas mikolov chen greg corrado jeffrey dean. efﬁcient estimation word represenproceedings intations vector space. ternational conference learning representations kishore papineni salim roukos todd ward weijing zhu. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics pages razvan pascanu caglar gulcehre kyunghyun yoshua bengio. construct deep reproceedings incurrent neural networks. ternational conference learning representations radu soricut daniel marcu. stochastic language generation using widl-expressions application machine translation summarization. proceedings international conference oriol vinyals alexander toshev samy bengio dumitru erhan. show tell neural image proceedings ieee concaption generator. ference computer vision pattern recognition pages wong raymond mooney. generation inverting semantic parser uses statistical machine translation. proceedings conference north american chapter association computational linguistics human language technologies pages kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual atproceedings international confertention. ence machine learning", "year": 2015}