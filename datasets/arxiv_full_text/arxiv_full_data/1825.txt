{"title": "An online sequence-to-sequence model for noisy speech recognition", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Generative models have long been the dominant approach for speech recognition. The success of these models however relies on the use of sophisticated recipes and complicated machinery that is not easily accessible to non-practitioners. Recent innovations in Deep Learning have given rise to an alternative - discriminative models called Sequence-to-Sequence models, that can almost match the accuracy of state of the art generative models. While these models are easy to train as they can be trained end-to-end in a single step, they have a practical limitation that they can only be used for offline recognition. This is because the models require that the entirety of the input sequence be available at the beginning of inference, an assumption that is not valid for instantaneous speech recognition. To address this problem, online sequence-to-sequence models were recently introduced. These models are able to start producing outputs as data arrives, and the model feels confident enough to output partial transcripts. These models, like sequence-to-sequence are causal - the output produced by the model until any time, $t$, affects the features that are computed subsequently. This makes the model inherently more powerful than generative models that are unable to change features that are computed from the data. This paper highlights two main contributions - an improvement to online sequence-to-sequence model training, and its application to noisy settings with mixed speech from two speakers.", "text": "abstract—generative models long dominant approach speech recognition. success models however relies sophisticated recipes complicated machinery easily accessible non-practitioners. recent innovations deep learning given rise alternative discriminative models called sequence-to-sequence models almost match accuracy state generative models. models easy train trained end-to-end single step practical limitation used ofﬂine recognition. models require entirety input sequence available beginning inference assumption valid instantaneous speech recognition. address problem online sequence-to-sequence models recently introduced. models able start producing outputs data arrives model feels conﬁdent enough output partial transcripts. models like sequence-to-sequence causal output produced model time affects features computed subsequently. makes model inherently powerful generative models unable change features computed data. paper highlights main contributions improvement online sequence-to-sequence model training application noisy settings mixed speech speakers. traditional speech recognition techniques. using models transcription typically performed maximum-aposteriori estimation word sequence given trained generative model acoustic observation. gaussian mixture models dominant models instantaneous emission distributions coupled hidden markov models model dynamics. posteriors gmm’s supplanted deep neural networks lately recognition model essentially retains generative interpretation. recent developments deep learning given rise powerful alternative discriminative models called sequenceto-sequence models trained model conditional probability distribution output transcript sequence given input acoustic sequence directly without inverting generative model. sequence-to-sequence models general model family solving supervised learning problems inputs outputs sequences. performance original sequence-to-sequence model greatly improved invention soft attention made possible sequence-to-sequence models generalize better achieve excellent results using much smaller networks long sequences. sequenceto-sequence model attention considerable empirical success machine translation speech recognition image caption generation question answering although remarkably successful sequence-to-sequence model attention must process entire input sequence producing output. however tasks useful start producing outputs entire input processed. tasks include speech recognition machine translation simultaneous speech recognition translation model recently models developed overcome shortcomings. models call online sequence-to-sequence models property produce outputs inputs received retaining causal nature sequence-to-sequence models. paper model previously introduced model uses binary stochastic variables select timesteps produce outputs. call model neural autoregressive transducer stochastic variables trained policy gradient method. however unlike work modiﬁed method training improves training results. further explore model noisy input present single channel mixed speech speakers different mixing proportions input model. models uniquely suited task causal model trained discriminatively. show results model task call multitimit shows model able handle noisy speech quite well. speculate model multiple microphone arrangement lead strong results mixed noisy speech. sequence sequence models recently applied phoneme recognition speech recognition models input acoustics form ﬁlter banks processed encoder neural network usually bidirectional neural network. decoder produces output tokens symbol time using next step prediction. step decoder uses soft attention encoder time steps create context vector summary features encoder. context vector decoder used make prediction time step. idea soft attention currently understood ﬁrst introduced graves ﬁrst truly successful formulation soft attention bahdanau used neural architecture implements search query ﬁnds relevant element input picks out. soft attention quickly become method choice various settings easy implement state results various tasks. example neural turing machine memory network attention mechanism similar bahdanau implement models learning algorithms question answering. soft attention immensely ﬂexible easy assumes test sequence provided entirety test time. inconvenient assumption whenever wish produce relevant output soon possible without processing input sequence entirety ﬁrst. useful context speech recognition system runs smartphone especially useful combined speech recognition machine translation system. model thought extending previous models connectionist temporal classiﬁcation sequence transducer models used speech recognition previously. however neither sequence transducer causal models models compute features data independently time step feature computation unaffected tokens output previously. note language model sequence transducer computes predictions causally impact local class predictions made acoustics independent others causal. producing output without consuming input entirety. include work mnih zaremba sutskever used reinforce algorithm learn location consume input emit output. finally jaitly used online sequenceto-sequence method conditioning partial inputs yielded encouraging results timit dataset. work technically extension prior work policy gradients continuous rewards used train model. paper similar ideas instead using single sample reinforce model parameteric baseline centering training stochastic model multi-sample training baseline average leave-one-out samples. section describe details autoregressive sequence transducer. includes recurrent neural network architecture reward function training inference procedure. much description borrowed heavily description refer reader ﬁgure details model. begin describing probabilistic model used work. time step recurrent neural network decides whether emit output token. decision made stochastic binary logistic unit bernoulli bernoulli distribution model outputs vector softmax distribution possible tokens. current position output sequence written incremented every time model chooses emit. model’s goal predict fig. impact entropy regularization emission locations. line shows emission predictions made example input utterance symbol representing input time steps. indicates model chooses emit output time steps whereas indicates otherwise. line without entropy penalty model emits symbols either start input unable meaningful gradients learn model. middle line entropy regularization model avoids clustering emission predictions time learns spread emissions meaningfully learn model. bottom line using divergence regularization emission probability also mitigates clustering problem albeit effectively entropy regularization. step binary decision previous timestep ˜bi− corresponding previous target ˜pi− model input. feedback ensures model’s outputs causally dependent model’s previous outputs thus model sequence sequence family. train model estimating gradient probability target sequence respect parameters model. model fully differentiable uses non-diffentiable binary stochastic units estimate gradients respect model parameters using policy gradient method discussed detail schulman used zaremba sutskever detail supervised learning train network make correct output predictions reinforcement learning train network decide emit various outputs. assume input sequence given desired sequence special end-of-sequence token assume probability model given following equations equations position model position advances model makes prediction. note deﬁne special beginning-of-sequence symbol. equations also suggest model easily implemented within static graph neural library tensorflow even though model conceptually dynamic neural network architecture. following zaremba sutskever modify model equations forcing equal whenever ˜pi. ensures model forced predict entire target sequence able learn degenerate solution chooses never make prediction therefore never experience prediction error. elaborate manner gradient computed. clear given value binary decisions compute ∂r/∂θ using backpropagation algorithm. figuring learn slightly challenging. understand factor reward expression distribution binary vectors derive gradient estimate respect parameters model since gradient equation policy gradient high variance variance reduction techniques must applied. common problems centering rao-blackwellization reduce variance models. mnih gregor however ignores fact internal state different samples same. ideally would average multiple trajectories starting state computationally expensive. result imbalance samples emitted symbols others thus future rewards directly comparable. residual term address this finally note reinforcement learning models often trained augmented objectives entropy penalty actions conﬁdent found crucial models train successfully. light regularization term augmented reward time steps without regularization model emits symbols clustered time either start input sequence end. model difﬁcult time recovering conﬁguration since gradients noisy biased. however penalty model successfully navigates away parameters lead clustered predictions eventually learns sensible parameters. alternative explored divergence predictions target bernouilli rate emission every step. however helped model successful entropy regularization. ﬁgure example clustering problem regularization ameliorates baselines commonly used reinforcement learning literature reduce variance estimators relying identity thus gradient better estimated following well chosen baseline function vector side information happens input outputs timestep where superscript indicates sample index. previous work used single sample estimate neural network parametric baseline estimate computed using linear projection hidden state lstm layer i.e. recent work reinforcement learning variational methods shown advantage multi-sample estimates paper thus explore multi-sample estimate further used baseline leave average explain next. conducted experiments different speech corpora using model. initial experiments conducted timit assess hyperparameters could lead stable behavior model. second experiments conducted speech mixed different speakers male speaker female speaker different mixing proportions. call experiments multi-timit. timit data phoneme recognition task phoneme sequences inferred input audio utterances. training dataset contains different audio clips target phonemes. scoring collapsed standard phoneme levenshtein edit distance computed phoneme error rate models trained timit layers units layer. model trained adam) used learning rate used asynchronous replicas tensorﬂow neural network framework training models gpus used training. produce best results emissions clumping beginning utterances entropy penalty used. started weight entropy penalty decayed linearly began decaying entropy penalty steps experimented ending decay steps ﬁnding step worked best. step entropy penalty weight kept also regularized models variational weight noise tested values standard deviation noise found worked best. started standard deviation variational noise increased linearly step value step experiment entropy penalty stopped decaying step variational noise ﬁnished increasing. lastly note input ﬁlterbanks processed three continuous frames ﬁlterbanks representing total speech concatenated input model. results smaller number input steps allows model learn hard alignments much faster would otherwise. figure example training curve. seen model requires larger number updates meaningful models learnt. however learning starts steady process achieved even though model trained policy gradient. table shows summary results achieved timit method other mature models. seen model compares favorably unidirectional models dnn-hmm’s etc. combining sophisticated features convolutional models produce better results. moreover model capacity absorb language models result suited training dnnhmm based models cannot inherently capture language models predict tokens independently other. generate data mixing male voice female voice original timit data. utterance original timit data pairs utterance coming opposite gender. wave signal utterances fig. emission distributions multi-timit ﬁgure shows probability emitting tokens case clean utterance timit corresponding noisy utterance multi-timit. seen multi-timit utterances model chooses emit symbols slightly later would timit utterances. ﬁrst scaled range signal scale second utterance reduced smaller volume mixing utterances. explored different scale mixing second utterance created three sets experiments. feature generation method described used resulting dimensional input frame. transcript speaker used ground truth transcript utterance. data follow train test speciﬁcation timit. result mixed data number train test utterances original timit also sets target phonemes. table results multi-timit table show phoneme error rate achieved models different proportions mixing distracting speech. also shown results deep lstms rnntransducer using implementation provided alex graves. table shows results using different mixing proportions confounding speaker. seen increasing mixing proportion model’s results worse expected. experiments audio input always paired confounding audio input. interestingly found pairing audio multiple confounding audio inputs produced worse results much worse overﬁtting. presumably happens model powerful enough memorize entire transcripts. figure shows example model emits symbols example multi-timit utterance. also shows comparison emissions clean model. generally speaking model chooses emit later multi-timit compared emits timit. paper introduced train online sequence-to-sequence models showed application noisy input. models result causal models incorporate language models also generate multiple different transcripts audio input. makes powerful class models. even dataset small timit model able adapt mixed speech. experiments speaker coupled distracting speaker hence dataset size limited. pairing speaker multiple speakers predicting outputs able achieve greater robustness. capability would like apply models multi-channel multi-speaker recognition future. work presented training online sequence sequence model. model allows exploit modelling power sequence-to-sequence problems withneed process entire input sequence ﬁrst. show results training model timit corpus acheived results comparable state results uni-directional models. merrienboer gulcehre bahdanau bougares schwen bengio learning phrase representations using encoder-decoder statistical machine translation conference empirical methods natural language processing chorowski bahdanau bengio end-to-end continuous speech recognition using attention-based recurrent first results neural information processing systems workshop deep learning representation learning workshop kiros courville salakhutdinov zemel bengio show attend tell neural image caption generation visual attention international conference machine learning abadi agarwal barham brevdo chen citro corrado davis dean devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. graves mohamed hinton speech recognition deep recurrent neural networks acoustics speech signal processing ieee international conference ieee chung-cheng chiu chung-cheng chiu software engineer google brain working deep learning models. received university southern california supervision stacy marsella. interests deep learning speech recognition reinforcement learning. dieterich lawson dieterich lawson brain resident google working sequential latent variable models methods variational inference. undergrad masters stanford computer science computational math respectively. interests include deep learning reinforcement learning optimization. yuping yuping student tsinghua university majoring computer science. interests deep learning optimization related theory. worked streaming algorithms online learning tsinghua university supervision periklis papakonstantinou. interned google brain working speech recognition supervised ilya sutskever navdeep jaitly. george tucker george tucker research software engineer google brain working deep learning models sequences. prior joining google research scientist amazon working deep acoustic models small-footprint keyword spotting. received supervision bonnie berger. interests deep learning variational inference reinforcement learning. kevin swersky kevin swersky research scientist google brain. received university toronto supervision richard zemel. research interests include deep learning graphical models generative models meta-learning. kevin co-founded whetlab online hyperparameter tuning service subsequently acquired twitter. ilya sutskever ilya sutskever research director openai. previously research scientist google brain. also co-founder dnnresearch acquired google. sutskever made many contributions ﬁeld deep learning including ﬁrst large scale convolutional neural network convincingly outperformed previous vision systems winning imagenet competition. listed technology reviews innovators navdeep jaitly navdeep jaitly research scientist nvidia research. previously research scientist google brain. interests endto-end models speech recognition speech synthesis reinforcement learning models sequences using deep learning.", "year": 2017}