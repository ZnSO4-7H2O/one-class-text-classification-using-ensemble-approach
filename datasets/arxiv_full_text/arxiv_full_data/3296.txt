{"title": "Aligning where to see and what to tell: image caption with region-based  attention and scene factorization", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Recent progress on automatic generation of image captions has shown that it is possible to describe the most salient information conveyed by images with accurate and meaningful sentences. In this paper, we propose an image caption system that exploits the parallel structures between images and sentences. In our model, the process of generating the next word, given the previously generated ones, is aligned with the visual perception experience where the attention shifting among the visual regions imposes a thread of visual ordering. This alignment characterizes the flow of \"abstract meaning\", encoding what is semantically shared by both the visual scene and the text description. Our system also makes another novel modeling contribution by introducing scene-specific contexts that capture higher-level semantic information encoded in an image. The contexts adapt language models for word generation to specific scene types. We benchmark our system and contrast to published results on several popular datasets. We show that using either region-based attention or scene-specific contexts improves systems without those components. Furthermore, combining these two modeling ingredients attains the state-of-the-art performance.", "text": "recent progress automatic generation image captions shown possible describe salient information conveyed images accurate meaningful sentences. paper propose image caption system exploits parallel structures images sentences. model process generating next word given previously generated ones aligned visual perception experience attention shifting among visual regions imposes thread visual ordering. alignment characterizes abstract meaning encoding semantically shared visual scene text description. system also makes another novel modeling contribution introducing scene-speciﬁc contexts capture higher-level semantic information encoded image. contexts adapt language models word generation speciﬁc scene types. benchmark system contrast published results several popular datasets. show using either region-based attention scene-speciﬁc contexts improves systems without components. furthermore combining modeling ingredients attains state-of-the-art performance. recent progress automatic generation image captions greatly disrupted well-known adage picture worth thousand words. granted still reasonable assert image contains vast amount visually discernible information diﬃcult completely characterized natural languages. nonetheless many image captioning systems shown possible describe salient information conveyed images accurate meaningful sentences systems attained promising results leveraging several crucial advances computer vision machine learning optimizing curated datasets large number images corresponding human-annotated captions representing images rich visual features designed related tasks object recognition localization learning highly complex models capable generating human-readable sentences despite progress image caption remains challenging task. abstract form caption algorithm needs infer likely sentence form sequence words given image figure architectural diagram image caption system. image ﬁrst analyzed represented multiple visual regions visual features extracted visual feature vectors recurrent neural network architecture predicts sequence focusing diﬀerent regions sequence generating words based transition visual attentions neural network model also governed scene vector global visual context extracted whole image. intuitively selects scene-speciﬁc language model generating texts arguably starting point image caption system understand image instance recognizing objects reasoning relationship among objects focusing salient parts image. identiﬁed objects correspond nouns caption generated relationship corresponds linguistic constituents determines sequentially order words sentence. finally desiderata keep salient information tunes secondary information generated caption. paper propose image caption system follows modeling idea exploits parallel structures images sentences. fig. shows conceptual diagram system. speciﬁcally assume close correspondence visual concepts detected objectlike regions textual realization words sentences. moreover process generating next word given previously generated ones aligned visual perception experience attention shifting among regions imposes thread visual ordering. alignment characterizes latent variable abstract meaning encoding semantically shared visual scene text description modeled recurrent neural network. hidden states network thus used predict next visual focus next word caption work also introduces another novel modeling contribution scene-speciﬁc contexts. contexts capture higher-level semantic information encoded image example places image taken possible activities involving people image. adapts language models generating words speciﬁc scene types. instance unlikely caption image mary asleep scene kitchen. rather likely caption would mary lies ﬂoor. scene contexts extracted visual feature vectors whole image aﬀect word generation biasing parameters recurrent neural network. systems diﬀer others following detailed comparisons deferred describing ours. identify localized regions multiple scales contain visually salient objects represent images. regions ground concepts sentences. systems extracts visual features images whole thus unable provide ﬁne-grained modeling interdependencies diﬀerent visual elements. models images collection patches. however two-stage systems focus learning mapping visual patches words. detected words test images used language models generate sentences. system resolves correspondence regions words also models parallel transitioning dynamics visual focus sentences. benchmark system contrast published results others several popular datasets. show using either region-based attention scene-speciﬁc contexts improves systems without components. furthermore combining modeling ingredients attains state-of-the-art performance outperforming competing methods noticeable margin. image caption generation long challenging problem computer vision. traditional approach pre-deﬁned templates generate sentences ﬁlling detected visual elements objects retrieval based models ﬁrst similar image training compose sentence based retrieved images’ sentences. methods’ generated sentences ﬁxed limited cannot describe speciﬁc contents test image. proposed multi-modal log-bilinear model generate sentences ﬁxed context window. proposed multimodal recurrent neural network architecture fuse text information visual features used deep extract whole image feature feature extracted whole image. input initial start word. ﬁrst used multiple instance learning train word detector. given novel test image used detector detect words patches extracted image. used maximum entropy language model generate sentence detected words last re-rank generated sentences minimum error rate training. aligns words sentence patches image images represented features computed patches words embedded using bidirectional rnn. testing whole image used extract visual context supply language models. closest spirit. represents images features computed tiles ﬁx-sized regions. learn neural network predict changing locations attention generate word based located tile. proposed multiple layers lstms investigate best feed visual text information diﬀerent lstms. system combines many features existing systems localized regions multiple scales represent images multiple layers lstms encode evolvement abstract meaning shared visual textual information learn end-to-end adjust system components generate desired sentences. system image caption composed following components visual features representation image localized regions multiple scales lstm-based neural network models attention dynamics focusing regions well generating sequentially words visual scene model adjusts lstm speciﬁc scenes describe detail components followed describing numerical optimization procedures details comparison related work. figure image representation localized regions multiple scales. image hierarchically segmented regions containing salient visual information selected. regions analyzed convolutional neural trained object recognition. resulting visual features along spatial geometric information concatenated feature vectors. texts details. stark contrast many existing work represent image global feature vector system represents image collection feature vectors computed localized regions multiple scales. representation provides explicit grounding concepts visual elements image. also enables ﬁne-grained modeling concepts pieced together attention model described next section. fig. illustrates main steps. generate candidate regions/patches technique selective search construct hierarchical segmentation image. technique ﬁrst uses color texture features over-segment image merge neighboring regions form hierarchy segmentations whole image merge single region identiﬁed region tight bounding used delineate boundaries. select good visual elements among vast collection regions diﬀerent levels select good ones ﬁrst means focusing salient relevant visual elements captioned. deﬁne goodness following desiderata semantically meaningful regions contain high-level concepts described natural language phrases. primitive non-compositional region small enough contain single concept captured words short phrases etc. contextually rich region large enough contains neighboring contextual information visual features extracted region indicative inter-dependency visual elements image. note goals naturally conﬂict need carefully balanced. train classiﬁer learn whether region good details constructing classiﬁer appendix. image select regions according outputs classiﬁer constraint union cover whole image sizes figure parallel processes generating word shifting visual attentions among visual regions image. based history previously generated word hidden states encode abstract meaning previous visual attention neural network predicts visual region focused now. based focus hidden states updated. hidden states together previous word current visual features predicts word generated next. bounding boxes regions diverse. diverse sizes preferred objects image size. moreover abstract concepts verbs tend strongly associated heterogeneous scales example ﬂying might needs large patch recognized color decided much smaller scales. achieve selection diverse sizes randomly permuting ranking order scores. comparison systems using regions detecting objects image focus deriving latent alignment detected regions words training sentences. purpose alignments train recurrent neural network generator word sequences training data become aligned regions corresponding words. however captioning image trained generator takes feature vector computed whole test image similar systems contrast need explicitly learn alignments image regions test images. grids represents images ﬁner scale collection feature vectors extracted regions. note pre-determined size regions could either correspond partial view visual element conglomeration several concepts system leverage intuition diﬀerent parts sentences ought correspond diﬀerent regions image. systems need model captioning moves regions using attention model characterize dynamics. describe model next. another. process used drive generation words yields textual form abstract meanings encoded image. process used analyze visual elements gives rise trajectory visual attention directing visual perception system attend overview model sets variables hidden states {ht} characterizing transition abstract meanings output variables {wt} words generated input variables {vt} describing visual context image example visual element focused. simplicity subscript indices time expanding length sentence. model sequence model predicting value state output variables time based values past well values input variables time particular importance dynamics modeled lstm unit. explain predictions made following. fig. gives overview predictive models. predict visual focus time system presented image already analyzed represented localized patches multiple scales denote collection feature vectors computed regions rr}. context. one-hidden-layer neural network softmax output variables mapping function note inputs neural network include extracted features i-th visual element histories previous time step including generated word abstract meaning visual context vt−. parameters neural network learnt data. adopt simpler weighted though mechanisms studied note outputs softmax layer highly peaked around particular visual element weighted approximates well feature element visual element focused. hidden units flickrk flickrk mscoco. update meaning trajectory given newly predicted visual context update abstract meaning crucial component model used lstm units stacked model complex dynamics illustrated fig. input bottom lstm tuple pwwt− composed histories visual context. output bottom lstm lstm input. output lstm equations describing lstms appendix. layers memory cells size hidden layers. details comparison decoder methods initialized averaged region features. lstms’ initialized using independently trained mlps take input hidden layer size approach directly inspired attention-based decoder though modiﬁed slightly stacked lstm layers well incorporating previous visual context order predict next visual element focused. imagine photo person walks along dog. photo taken park likely caption could person walking exercising. hand photo taken store likely caption could person takes groomed. intuitively scene category invaluable context aﬀect selection words signiﬁcantly word groom appear park scene word exercise unlikely typical activity store thus selected high conﬁdence. exploit global contexts better image captioning? describe ancontribution work scene-factored lstm. describe ﬁrst extract scene-related global contexts followed inject scene contexts lstms. scene-speciﬁc contexts goal obtain scene vector image. purpose using vector better captioning scene vector informative textual descriptions also needs inferable visual appearance. achieve goals steps unsupervised clustering captions scene categories supervised learning classiﬁer predict scene categories visual appearance. topic vector computed image’s visual feature vector. note predictive model allows extract topic vectors images without captions. call topic vectors scene vectors. details suppl. material. adapt lstms scene-speciﬁc lstms encodes language model words sequentially selected vocabulary. inject scene vectors thus adapt sentence generation process scene-speciﬁc factorize parameters lstms. speciﬁcally given image associated scene vector personalized lstms image generate caption. concretely gates aﬃne transformations reparameterized fig. avoid notation cluttering assume linear transformation matrix applied given note design scene-speciﬁc lstms represent tradeoﬀ specialization number parameters learn. another option categorize image hard scene assignment scene learn lstms parameters share among drawback option number parameters learned increase linearly respect number scenes. additionally language models learned diﬀerent lstms share commonness might desirable certainly many scene-independent language components. evaluate image caption system several datasets compare several state-of-the-art systems task captioning image/text retrieval tasks. also evaluate system qualitatively validating modeling assumptions region-based attention scene-factorization language models. describe setup empirical studies ﬁrst followed discussing quantitative qualitative results. datasets evaluation metrics experimented datasets mscoco flickrk flickrk followed standard protocols split data training validation evaluation. details appendix. evaluate captions test images following metrics bleu- bleu- bleu- bleu- meteor rouge-l cider-d given mscoco server essence measure agreements ground-truth captions outputs automatic systems. public python evaluation provided mscoco server. alternative methods compare several recently proposed image captioning systems deepvs mrnn google lrcn systems published publicly available evaluation results databases experimented. work inspired system’s evaluation procedures diﬀerent systems. implementation details generate caption needs sample word output time sound strategy form beam search pre-determined number bestby-now sentences computed kept expanded words future. experiments beam size also experimented greedy search beam size optimization details provided appendix. main results table compares method several systems task image caption. consider several systematic variants method our-base similar google represent images features computed whole image stripping away aspects region-based attention scene-factorization approach. our-sf adds scene-factorized lstms our-base. our-ra adds region-based attention our-base. our-ra+sf adds scene-factorized lstms region-based attention our-base. terms ‘-greedy’ ‘-beam’ denote search strategies used. mscoco dataset using search strategy ‘-greedy’ adding either region-based attention scene-factorized lstms improves base system noticeably metrics. moreover beneﬁts region-base attention scene-factorization additive evidenced improvement our-ra+sf-greedy. using beam search our-ra+sf-beam attains best performance metrics outperforming competing systems’s previously published results. results appendix show approach our-ra+sf-beam outperforms competing methods flickrk flickrk datasets. also demonstrate state-of-the-art performance approaches tasks image/caption retrieval. also evaluate system qualitatively aspects transition dynamics abstract meaning correspond changes concepts caption scene visual contexts inﬂuence generation caption. trajectory visual attentions illustrate transition attentions compute weighted pixel values image. weights determined amount attentions predicted regions pixels belong fig. gives example. note ﬁrst weighted pixel values clearly shows distinction foreground background weighted smallest also observe focused regions correspond well concepts caption. instance standing highlighted region contains legs standing grass highlighted region contains figure alignment attention shifted among visual regions generation words caption. many concepts sentence correspond well visual elements image. figure image mscoco baby holds toothbrush. caption given model properly describes image content using correct scene vector bias language generation. inﬂuence scene vectors replace origin scene vector four -hot topic vectors. topic indexes baby hold diﬀerent objects given diﬀerent scene vectors. example caption given model using universal language model found http//cs.stanford.edu/people/karpathy/deepimagesent/. eﬀect scene factors caption generation hypothesize global visual contexts scene categories signiﬁcantly aﬀect captions generated. verify distorting predicted scene vector test image another one. distorted scene vector generate caption contrast caption obtained undistorted scene vector. fig. exempliﬁes eﬀect using diﬀerent scene vectors. image portrays baby holding toothbrush model identiﬁes scene topic distorting prediction topics leads drastically diﬀerent captions. clearly topic corresponds scenes food caption changed regarding baby holding slice pizza. similarly topic related sports scene caption changed baby holding baseball bat. examples demonstrate scene categories signiﬁcant impact caption generation biasing language model words common targeted scene vector. propose image caption system exploits parallel structures images sentences. contribution system aligning process generating captions attention shifting among visual regions. another introducing scene-factorization lstm adapts language models word generation speciﬁc scene types. system benchmarked contrasted published results several popular dataset including mscoco flickrk flickrk. either region-based attention scene-speciﬁc contexts improves performance. combining modeling ingredients provides improvement attaining state-of-the-art performance. partially supported intelligence advanced research projects activity department defense u.s. army research laboratory contract number wnf--c- additionally awards iis- iis- google research award alfred. sloan research fellowship award u.s. government authorized reproduce distribute reprints governmental purposes notwithstanding copyright annotation thereon. views conclusions contained herein authors interpreted necessarily representing oﬃcial policies endorsements either expressed implied iarpa dod/arl u.s. government. partially supported program nsfc", "year": 2015}