{"title": "Neural Associative Memory for Dual-Sequence Modeling", "tag": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "abstract": "Many important NLP problems can be posed as dual-sequence or sequence-to-sequence modeling tasks. Recent advances in building end-to-end neural architectures have been highly successful in solving such tasks. In this work we propose a new architecture for dual-sequence modeling that is based on associative memory. We derive AM-RNNs, a recurrent associative memory (AM) which augments generic recurrent neural networks (RNN). This architecture is extended to the Dual AM-RNN which operates on two AMs at once. Our models achieve very competitive results on textual entailment. A qualitative analysis demonstrates that long range dependencies between source and target-sequence can be bridged effectively using Dual AM-RNNs. However, an initial experiment on auto-encoding reveals that these benefits are not exploited by the system when learning to solve sequence-to-sequence tasks which indicates that additional supervision or regularization is needed.", "text": "many important problems posed dual-sequence sequence-tosequence modeling tasks. recent advances building end-to-end neural architectures highly successful solving tasks. work propose architecture dual-sequence modeling based associative memory. derive am-rnns recurrent associative memory augments generic recurrent neural networks architecture extended dual am-rnn operates once. models achieve competitive results textual entailment. qualitative analysis demonstrates long range dependencies source target-sequence bridged effectively using dual am-rnns. however initial experiment autoencoding reveals beneﬁts exploited system learning solve sequence-to-sequence tasks indicates additional supervision regularization needed. dual-sequence modeling sequence-tosequence modeling important paradigms used many applications involving natural language including machine translation recognizing textual entailment auto-encoding syntactical parsing document-level question answering might even argue most problems modeled paradigm models operate distinct sequences source target sequence. tasks require generation target based source e.g. machine translation whereas tasks involve making predictions given source target sequence e.g. recognizing textual entailment. existing state-of-the-art end-to-end differentiable models tasks exploit architectural ideas. ability models carry information long distances enabling factor performance. typically achieved employing recurrent neural networks convey information time internal memory state. famous lstm accumulates information every time step additively memory state avoids problem vanishing gradients hindered previous architectures learning long range dependencies. example sutskever connected lstms conditionally machine translation memory state processing source used initialization memory state target lstm. simple architecture achieved competitive results compared existing elaborate feature-rich models. however learning inherent long range dependencies between source target requires extensive training large datasets. bahdanau proposed architecture resolved issue allowing model attend positions source sentence predicting target sentence enabled model automatically learn alignments words phrases source target sentence. important difference previous long range dependencies could bridged directly attention. however archiwork introduce novel architecture dual-sequence modeling based associative memories ﬁxed sized memory arrays used read write content associated keys. holographic reduced representations enable robust efﬁcient retrieval previously written content redundant memory arrays. approach inspired works danihelka recently demonstrated beneﬁts exchanging memory cell lstm associative memory various sequence modeling tasks. contrast architecture directly adapts lstm architecture propose augmentation generic rnns similar spirit neural turing machines decouple restrict interaction read write operations believe important. based architecture derive dual am-rnn operates associative memories simultaneously dual-sequence modeling. conduct experiments task recognizing textual entailment results qualitative analysis demonstrate used bridge long range dependencies similar attention mechanism preserving computational beneﬁts conveying information single ﬁxed-size memory state. finally initial inspection sequenceto-sequence modeling dual am-rnns shows open problems need resolved make approach applicable kinds tasks. augmenting rnns memory novel. graves introduced neural turing machines augment rnns external memory written read from. contains predeﬁned number slots write content form memory addressable content position shifts. neural turing machines inspired subsequent work using different kinds external memory like queues stacks operations memories calculated recurrent controller decoupled memory whereas am-rnns apply cell-function directly upon content associative memory. danihelka introduced associative lstms extends standard lstms directly reading writing operations associative memory. architecture closely related ours. however crucial differences fact decouple associative array original cell-function. danihelka directly include operations deﬁnition associative lstm. might cause problems since operations e.g. forget directly applied entire memory array although affect elements stored memory. believe reading writing operations respect calculated performed associative memory. operations therefore applied stored elements. neural attention another important mechanism realizes form content addressable memory. famously applied machine translation attention models automatically learn soft word alignments source translation attention requires memory stores states individual entries separately e.g. states every word source sentence textual entailment entire sentence states sukhbaatar end-to-end memory network question answering. attention weights computed based provided input stored elements. thereby weighted memory states summed result retrieved used input down-stream neural network. architectures based attention require larger amount memory larger number operations scales usually dynamically growing memory. contrast attention dual am-rnns utilize ﬁxed size memories constant number operations. am-rnns also interesting connection lstm-networks recently demonstrated impressive results various text modeling tasks. lstm-networks select previous hidden state attention memory tape past states opreduce noise danihelka introduce permuted redundant copies results uncorrelated retrieval noises effectively reduces overall retrieval noise computing mean. consider permutations represented permutation matrices retrieval equation becomes following. recurrent neural network deﬁned parametrized cell-function recurrently applied input sequence time step emits output state used additional input following time step posed using hidden state previous time step. idea implicitly present architecture retrieving previous state computed associative memory main difference lies used memory architecture. ﬁxed size memory array contrast dynamically growing memory tape requires growing computational memory resources. drawback approach however potential loss explicit memories retrieval noise overwriting. following terminology danihelka introduce redundant associative memories holographic reduced representations hrrs provide mechanism encode item written ﬁxed size memory array retrieved keys values refer complex vectors consist real imaginary part imaginary unit. represent complex vectors concatenations respective real imaginary parts e.g. encodingretrieval-operation proposed plate utilized danihelka complex multiplication value complex conjugate memory respectively. note requires modulus equal i.e. consider single memory array containing elements respective keys remainder might additionally used e.g. output lstm. brevity neglect following thus ﬁrst compute given previous output current input turn used read associative memory array retrieve memory state speciﬁed next apply original cell-function retrieved memory state concatenation current input last output serves input internal rnn. update associative memory array updated state using conjugate retrieval important tasks machine translation detecting textual entailment involve distinct sequences input sourcetarget sequence. system predicts target sequence based source whereas source target given entailment-class predicted. recently tasks successfully modelled using attention mechanism attend positions source sentence time step target sentence models able learn important task speciﬁc correlations words phrases sentences like word/phrase translation word-/phrase-level entailment contradiction. success models mainly fact long range dependencies bridged directly attention instead keeping information long distances memory state overwritten. achieved associative memory. given correct state written time step source sentence retrieved minor noise efﬁciently reduced redundancy. therefore bridge long range dependencies therefore used alternative attention. trade-off using memorized states cannot used retrieval. however retrieval operation constant time memory whereas computational memory complexity attention based architectures grow linearly length source sequence. propose different architectures solving dual sequence problems. approaches least am-rnn processing source another target sequence. ﬁrst approach reads source sequence uses ﬁnal associative memory array note basically conditional encoding architecture rocktäschel second approach uses ﬁnal array source sequence addition independent target array time step dual am-rnn computes another used read feeds retrieved value additional input inner target am-rnn. changes reﬂected equation illustrated figure setup dataset conducted experiments stanford natural language inference corpus consists roughly sentence pairs annotated textual entailment labels. task predict whether premise entails contradicts neutral given hypothesis. training perform mini-batch stochastic gradient descent using adam initial learning rate small models large model. learning rate halved whenever accuracy dropped period epoch. performance development checked every mini-batches best model used testing. employ dropout probability small large models respectively. following cheng word embeddings initialized glove randomly unknown words. glove initialized embeddings tuned initial epoch training set. associative memory implemented redundant memory copies. dual am-gru deﬁne i.e. interacting premise hypothesis associative memory array processing hypothesis. rationale behind want retrieve text passages premise similar text passages target sequence. models consist layers top-layer intended summarize outputs bottom layer. bottom layer corresponds different architectures. concatenate ﬁnal output premise hypothesis together absolute difference form ﬁnal representation used input two-layer perceptron rectiﬁer-activations classiﬁcation. results presented table long range h=-dimensional dual am-gru conditional am-gru outperform baseline system signiﬁcantly. especially dual amgru well task achieving accuracy shows important utilize associative memory premise separately reading only. notably achieves even better results comparable lstm architecture two-way attention premise hypothesis words indicates dual am-gru architecture table accuracies different rnn-based architectures snli dataset. also report respective hidden dimension number parameters |θ−e| architecture without taking word embeddings account. investigated ﬁnding qualitatively sampled examples plotting heatmaps cosine similarities content written memory every time step premise retrieved dual amgru processes hypothesis. random examples shown figure dual am-gru indeed able retrieve content premise memory related respective hypothesis words thus allowing bridge important long-range dependencies solving task similar attention. observe content related words phrases retrieved premise memory processing hypothesis e.g. play\" video game\" artist\" sculptor\". increasing size hidden dimension improves accuracy another percentage point. recently proposed lstm network achieves slightly better results. however number operations scales square summed source target sequence even larger traditional attention. sequence-to-sequence modeling end-to-end differentiable sequence-to-sequence models consist encoder encodes source sequence decoder produces target sequence based encoded source. preliminary experiment applied dual am-gru without shared parameters task auto-encoding sourcetarget sequence same. intuitively would like amgru write phrase-level information different keys associative memory. however found encoder am-gru learned quickly write everything memory makes work similar standard based encoder-decoder architecture encoder state simply used initialize decoder state. ﬁnding illustrated figure presented heatmap shows similarities content retrieved predicting target sequence written encoder memory. observe similarities between retrieved content written content horizontally slightly increasing i.e. towards encoded source sentence. indicates encoder overwrites associative memory processing source key. discussion experiments entailment show idea using associative memory bridge long term dependencies dual-sequence modeling work well. however architecture naively transferable task sequence-to-sequence modeling. believe main difﬁculty lies computation appropriate every time step target sequence retrieve related content. furthermore encoder enforced always key. example keys could based syntactical semantical cues might ultimately result capturing form frame semantics could facilitate decoding significantly. believe might achieved regularization curriculum learning", "year": 2016}