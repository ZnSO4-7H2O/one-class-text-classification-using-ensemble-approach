{"title": "Alpha-expansion is Exact on Stable Instances", "tag": ["stat.ML", "cs.AI", "cs.DS", "cs.LG"], "abstract": "Approximate algorithms for structured prediction problems---such as the popular alpha-expansion algorithm (Boykov et al. 2001) in computer vision---typically far exceed their theoretical performance guarantees on real-world instances. These algorithms often find solutions that are very close to optimal. The goal of this paper is to partially explain the performance of alpha-expansion on MAP inference in Ferromagnetic Potts models (FPMs). Our main results use the connection between energy minimization in FPMs and the Uniform Metric Labeling problem to give a stability condition under which the alpha-expansion algorithm provably recovers the optimal MAP solution. This theoretical result complements the numerous empirical observations of alpha-expansion's performance. Additionally, we give a different stability condition under which an LP-based algorithm recovers the optimal solution.", "text": "approximate algorithms structured prediction problems—such popular αexpansion algorithm computer vision—typically exceed theoretical performance guarantees realworld instances. algorithms often solutions close optimal. goal paper partially explain performance α-expansion inference ferromagnetic potts models main results connection energy minimization fpms uniform metric labeling problem give stability condition α-expansion algorithm provably recovers optimal solution. theoretical result complements numerous empirical observations α-expansion’s performance. additionally give diﬀerent stability condition lp-based algorithm recovers optimal solution. many problems machine learning large theoretical guarantees oﬀered best algorithms empirical performance algorithms real data. instance many inference problems reduce well-studied combinatorial optimization problems computationally hard worst-case; practice however heuristic approaches often obtain solutions surpass worst-case guarantees. worstcase analysis method choice theoretical computer science reason algorithms beyond-worst-case paradigms like average-case analysis smoothed analysis implicit assumptions like stability become increasingly popular recent years reconciling large theory practice important challenge machine learning. many tasks modern machine learning—especially computer vision natural language processing—are framed solved structured prediction problems local structure instance used inform global decisions. stereo vision problem given input images task output disparity value pixel left image tells much pixel moved neighboring pixels similar intensities output give similar disparities. undirected graphical models also known markov random fields provide powerful framework performing type structured prediction. solving inference problem markov random field gives maximum-probability conﬁguration variables taking account interaction eﬀects nearby variables. represented using graph vertex represents random variable take values discrete edges represent direct dependencies diﬀerent random variables. consider pairwise mrfs dependencies along edges. labeling maps write inference task pairwise energy minimization form follows interpret node cost assigning label vertex edge cost simultaneously assigning label label computing assignment corresponding known np-hard many classes mrfs well-studied special case successful practice ferromagnetic potts model here edge cost function nonnegative weight assigned diﬀerent labels otherwise. assume without loss question heuristics inference perform much better practice worstcase theoretical guarantees suggest? identify properties real-world instances make tractable? real world problem instances must structure worst-case ones not. reconcile large theory practice study structural property instances called stability think understanding tractability. many real-world instances ground-truth corresponds assignment stands out—the optimal solution unique robust small changes errors instance speciﬁcation. edge costs involved objective often imprecise rough estimates similarity endpoints. hence interested ﬁnding optimal solution instance stable errors perturbations edge costs. bilu linial introduced formal deﬁnition stability context graph partitioning problems capture instances clear ground-truth solution change small multiplicative perturbations input. uniform metric labeling instance edge weights said -stable optimal solution unique remains unchanged -perturbation edge weights— unique optimal solution instance edge costs note instance needs stable multiplicative perturbations edge weights perturbations node costs. sometimes strong assume optimal solution perturbed instance remains completely unchanged. practice edge costs perturbed optimal solution change bit; however often stable region instance assignment remains optimal. capture instances introducing weaker assumption called -weak stability roughly speaking weakly stable instances region good solutions always agree assignment even edge weight perturbations objective labeling problem known theoretical computer science uniform metric labeling known np-hard worst-case polynomial-time inference algorithms exist mrfs simple structure—like treewidth submodularity—most graphical models arise real-world applications simple structure. recent years though much work gone ﬁnding tractable model classes eﬃcient approximation algorithms inference. linear programming relaxations give class algorithms. algorithms relax problem linear program round relaxed solutions back integral assignments. fact linear programming relaxations often turn mostly integral instances arise practice applications like stereo vision stands stark contrast theoretical understanding linear programming relaxations worst-case instances. introduced boykov α-expansion algorithm simple popular combinatorial algorithm approximate inference. works iteratively improving initial labeling time trying optimal expansion label local search algorithm stuck local energy minima. empirically however α-expansion seems avoid local minima. boykov apply algorithm stereo vision—they construct ferromagnetic potts model images α-expansion approximate solution gives disparity value pixel. surprisingly solutions returned α-expansion strikingly similar solutions output algorithm depends little initial input labeling even though algorithm -approximation worst case. good performance wide adoption α-expansion algorithm practice. results. give provable guarantees natural relaxation α-expansion algorithm input uniform metric labeling instance suﬃciently stable. ﬁrst result shows standard relaxation uniform metric labeling exact suﬃciently stable instances informal theorem. given uniform metric labeling instance -weakly stable α-expansion algorithm boykov recovers optimal solution entire instance -stable α-expansion ﬁnds optimal solution. implications theorem two-fold. firstly shows recovering assignment stable region tractable polynomial time. implies recovery entire solution theorem gives global convergence guarantees stable instances α-expansion iterative hill-climbing heuristic could stuck local optima arbitrary instances. secondly theorem guarantees expansion algorithm recovers solution stable region instance matter size region. finally results require stability multiplicative perturbations factor stability conditions -stability stability) seem qualitatively diﬀerent. show section optimality algorithms breaks stability conditions theorems switched. instance stability studied context graph partitioning problems like max-cut minimum multiway clustering problems like kmeans k-median traveling salesman problem angelidakis makarychev developed general framework analyze stable instances graph partitioning problems showing exists convex relaxation rounding scheme problem satisfying certain properties convex relaxation exact suﬃciently stable instances problem. also designed polynomial time iterative algorithm weakly stable instances problem optimal solution change slightly perturbations weights. amount stability required depends guarantees rounding scheme. makarychev applied framework minimum multiway problem special case uniform metric labeling. give polynomial-time algorithm stable instances minimum multiway cut. angelidakis also studied minimum multiway cut; designed better rounding scheme improved framework makarychev give provable guarantees /k-stable instances. framework makarychev angelidakis prove integrality relaxation uniform metric labeling. however several technical challenges need address prove results brieﬂy describe below. unlike minimum multiway problem diﬀerent costs uniform metric labeling edge weights node costs. notion stability assumes optimal solution change perturbations edge weights; make assumptions perturbations node costs. simple reduction instance uniform metric labeling instance minimum multiway converts node costs edge weights. using reduction would eﬀectively force assume stability respect perturbations node costs well. further reduction creates edges large weight stability condition required becomes stringent. address challenges ﬁrst show existence rounding scheme standard relaxation delicately trades loss solution node costs loss edge weights. contrast rounding scheme incur much loss node costs relative node costs irrelevant minimum multiway cut. rounding guarantees suﬃce theorems. linear programming relaxations also commonly used approximate solutions. make extensive following relaxation tool analyze α-expansion algorithm itself integer labeling also feasible point corresponds {¯uf otherwise. case distance otherwise relaxation tight optimal solution integer solution ferromagnetic potts models relaxation equivalent local polytope relaxation commonly studied inference appendix contains proof equivalence. deﬁnition -stable). instance uniform metric labeling weights node costs optimal integer solution called -stable -perturbation labeling objective costs weights unique optimal solution perturbation. note node costs perturbed deﬁnition. requiring stability under perturbations costs would lead stronger condition input instance; perturbations suﬃcient theorems. obtain provable optimality guarantees αexpansion algorithm relate improvement step α-expansion performance linear program modiﬁed instance. carefully perturb weights given instance argue current solution optimal stable portion instance α-expansion step ﬁnds solution smaller cost. proves α-expansion algorithm recovers optimal solution restricted stable region. solving np-hard several eﬃcient approximation algorithms exist. work focus simple combinatorial algorithm known α-expansion works iteratively improving initial labeling expansion moves. full procedure described algorithm boykov show optimal α-expansion move given labeling label found solving minimum problem auxiliary graph finally show α-expansion approximation appendix contains proof guarantees. note since rounding works ε-close solutions cannot turn properties approximation algorithm. prove main theorem section proof. assume contradiction optimal solution {¯ulp fractional. construct stability-violating labeling algorithm fractional labeling {¯u} constructed {¯ulp optimal integer solution lemma show expectation output must better optimal integer solution particular -perturbation contradicts stability. restrictions objective costs weights call stable stable region instance. weak stability property says perturbation edge weights solution disagrees optimal solution stable must worse objective value. note -weakly-stable instance -stable. section prove relaxation mentioned equivalent local consistency relaxation tight -stable instances. proof follows framework introduced makarychev angelidakis assume contradiction fractional node probabilistic method show must labeling violates stability instance. construct violating labeling build randomized rounding algorithm provides certain probabilistic guarantees. show carefully constructed fractional input rounding algorithm outputs solution violates stability expectation. thus must labeling violates stability therefore optimal solution must take integer values every node. rounding algorithm deﬁned proof techniques section also used section analyze α-expansion. begin describe rounding procedure. angelidakis algorithm works inputs close integer solutions; show construct so-called ε-close inputs shortly. show construct expansion move decreases objective probabilistic method. actually construct expansion move using relaxation rounding algorithm previous section. indeed solution modiﬁed instance construct input {¯u} rounding algorithm show long current labeling diﬀers optimal stable labeling support decreases objective. following lemma shows every labeling support expansion move. proof. algorithm makes random choice label every vertex assigns either label label vertex. clearly vertices labeled decrease label assigned output labeling iexpansion theorem -weakly-stable instance optimal solution solution output algorithm α-expansion recovers optimal solution stable set. α-expansion recovers full optimal solution. remark algorithm known polynomial time long costs polynomially bounded. veksler shows converges polynomial number iterations costs weights constant iteration performs maximum computations. practice algorithm typically takes iterations converge ﬁrst term clearly zero. further {¯u} fractional guarantees lemma imply -stability instance labeling must satisfy stability fractionality imply section study broader stability condition given deﬁnition instance stable respect region prove -weakly-stable instances α-expansion algorithm recovers optimal solution stable words given input α-expansion guaranteed recover optimal solution stable portion instance irrespective size input instance -stable theorem implies recovery entire optimal solution -stable instances. weak stability instance case must labeling support rounding algorithm whose objective less input rounding algorithm ε-close lemma every labeling support expansion move long expansion move decreases objective. boykov show α-expansion terminates expansion move decreases objective hence algorithm terminates. algorithms analyzed sections give guarantees diﬀerent stability settings stability αexpansion. show algorithm provably recover optimal solution stability setting. relaxation tight -stable instances α-expansion always optimal solution -stable instances. stability tested checking possible figure shows -stable instance three nodes three labels. optimal integer solution optimal fractional solution shown table fractional solution strictly lower objective value. since edges adversarial perturbation change edge weights. optimal solution unique instance -stable. note α-expansion exact instance—no matter starting labeling expanding label gives optimal solution. table shows optimal integer solution solution returned α-expansion labels initially relaxation tight instance tree suppose α-expansion starts every node assigned label ﬁrst iteration ﬁnds optimal expansion label assigns label leaves labeled move expansions decrease objective algorithm terminates. instance -stable adversarial perturbation weights weight optimal solution still label gave conditions popular algorithms inference ferromagnetic potts models exact. results section provide possible avenue explaining relaxations often tight practice. weakly stable instances results section provide possible explanation observed phenomena solutions output α-expansion often visually indistinguishable optimal solution output heavily depend choice initial labeling since prove algorithm always recovers optimal solution stable regardless initialization. note -stable instances α-expansion exact relaxation tight. α-expansion algorithm local search algorithm essentially hill-climbing particular moves results show avoids local optima stable instances. implies energy landscape stable instances particular properties make inference tractable gives many directions future work understanding relationship stability optimization. feedback drafts paper. work supported aitf awards ccf- ccf-. opinions ﬁndings conclusions recommendations expressed material authors necessarily reﬂect views nsf. additionally supported grant ccf-.", "year": 2017}