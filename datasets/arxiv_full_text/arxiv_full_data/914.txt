{"title": "Disentangling Factors of Variation via Generative Entangling", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Here we propose a novel model family with the objective of learning to disentangle the factors of variation in data. Our approach is based on the spike-and-slab restricted Boltzmann machine which we generalize to include higher-order interactions among multiple latent variables. Seen from a generative perspective, the multiplicative interactions emulates the entangling of factors of variation. Inference in the model can be seen as disentangling these generative factors. Unlike previous attempts at disentangling latent factors, the proposed model is trained using no supervised information regarding the latent factors. We apply our model to the task of facial expression classification.", "text": "propose novel model family objective learning disentangle factors variation data. approach based spike-and-slab restricted boltzmann machine generalize include higher-order interactions among multiple latent variables. seen generative perspective multiplicative interactions emulates entangling factors variation. inference model seen disentangling generative factors. unlike previous attempts disentangling latent factors proposed model trained using supervised information regarding latent factors. apply model task facial expression classiﬁcation. many machine learning tasks data originates generative process involving complex interaction multiple factors. alone factor accounts source variability data. together interaction gives rise rich structure characteristic many challenging domains application. consider example task facial expression recognition. images diﬀerent individuals facial expression result images well separated pixel space. hand images individuals showing diﬀerent expressions well positioned close together pixel space. simpliﬁed scenario factors play identity individual facial expression. factors identity irrelevant task facial expression recognition factors could well dominate representation image pixel space. result pixel space-based facial expression recognition systems seem likely suﬀer poor performance variation appearance individual importantly interacting factors frequently combine simple superpositions easily separated choosing appropriate aﬃne projection data. rather factors often appear tightly entangled data. challenge construct representations data cope reality entangled factors variation provide features appropriate wide variety possible tasks. context face data example representation capable disentangling identity expression would eﬀective representation either facial recognition facial expression classiﬁcation. eﬀort cope factors variation broad-based movement machine learning application domains computer vision toward hand-engineering feature sets invariant common sources variation data. motivation behind inclusion feature pooling stages convolutional network architecture recent trend toward representations based large scale pooling low-level features approaches stem powerful idea invariant features data induced pooling together simple ﬁlter responses. potentially even powerful notion actually learn ﬁlters pooled together purely unsupervised data thereby extract directions variance pooling features become invariant however situations multiple relevant entangled factors variation give rise data require means feature extraction disentangles factors data rather simply learn represent factors expense propose novel model family objective learning disentangle factors variation evident data. approach based spike-and-slab restricted boltzmann machine recently shown promising model natural image data. generalize ssrbm include higher-order interactions among multiple binary latent variables. seen generative perspective multiplicative interactions binary latent variables emulates entangling factors give rise data. conversely inference model seen attempt assign credit various interacting factors combined account data eﬀect disentangle generative factors. approach relies unsupervised approximate maximum likelihood learning model parameters require label information deﬁning factors disentangled. believe research direction critical importance almost never case label information exists factors responsible variations data distribution. principle invariant features actually emerge using unsupervised learning organization features subspaces ﬁrst established assom model since then basic strategy reappeared number diﬀerent models learning paradigms including topological independent component analysis invariant predictive sparse decomposition well boltzmann machine-based approaches case basic strategy group ﬁlters together example using variable gates activation elements group. gated activation mechanism causes ﬁlters within group share common window dataset turn leads ﬁlter groups composed mutually complementary ﬁlters. span ﬁlter vectors deﬁnes subspace speciﬁes directions pooling feature invariant. somewhat surprisingly basic strategy repeatedly demonstrated useful invariant features learned strictly unsupervised fashion using statistical structure inherent data. remarkable important problem using learning strategy invariant representation formed pooling features oﬀers somewhat incomplete view data detailed representation lower-level features abstracted away pooling procedure. would like higher level features abstract exhibit greater invariance little control information lost feature subspace pooling. invariant features deﬁnition reduced sensitivity direction invariance. goal building invariant features fully desirable directions invariance reﬂect sources variance data uninformative task hand. however often case goal feature extraction disentangling separation many distinct informative factors data. situation methods generating invariant features namely feature subspace method inadequate. returning facial expression classiﬁcation example introduction consider pooling feature made invariant expression subject forming subspace low-level ﬁlters represent subject various facial expressions pooling feature associated appearance subject facial expression information lost model representation formed pooling features. illustrated hypothetical facial expression classiﬁcation task loss information becomes problem information lost necessary successfully complete task hand. obviously really would like particular feature invariant irrelevant features disentangle relevant features. unfortunately often diﬃcult determine priori features ultimately relevant task hand. further often case context deep learning methods feature trained destined used multiple tasks distinct subsets relevant features. considerations lead conclusion robust approach feature learning disentangle many factors possible discarding little information data practical. motivation behind proposed higher-order spike-and-slab boltzmann machine. figure energy function higher-order spike slab used disentangle factors variation data. groups latent spike variables interact explain data weight tensor ssrbm instantiates slab variable hidden unit higher-order model employs slab pair spike variables respectively mean precision parameters sij. additional spike variables used gate groups latent variables serve promote group sparsity. parameters thus indexed extra subscript finally standard bias terms variables diagonal precision matrix visible vector. section introduce model makes progress toward ambitious goal disentangling factors variation. model based boltzmann machine undirected graphical model. particular build spike-and-slab restricted boltzmann machine model family previously shown promise means learning invariant features subspace pooling. original ssrbm model possessed limited form higher-order interaction latent random variables spike slab extension adds higher-order interactions four distinct latent random variables. include slab variables three interacting binary spike variables. unlike ssrbm interactions latent variables violate conditional independence constraint restricted boltzmann machine therefore belong class models. consequence exact inference model tractable resort mean-ﬁeld approximation. strategy promoting model intend disentangle factors variation inference generative model. context generative models inference roughly thought running generative process reverse. thus wish inference process disentangle factors variation generative process describe means factor entangling. generative model propose represents possible means factor entangling. parameters deﬁned follows. rd×m×n×k weight -tensor connecting visible units interacting latent variables interpreted forming basis image space; ases matrices vector respectively. energy function fully speciﬁes joint probability distribution variables partition function ensures joint distribution normalized. speciﬁed above energy function similar ssrbm energy function includes factored representation standard ssrbm spike variable. clearly properties model highly dependent topology interactions real-valued slab variables sijk three binary spike variables hjk. adopt strategy permits local interactions within small groups block-like organizational pattern speciﬁed fig. local block structure allows model work incrementally towards disentangling features focusing manageable subparts problem. given block mediated weight tensor w···k corresponding slab variables s··k. contrary standard probabilistic factor models whose factors simply give rise visible vector individual contributions elements easily isolated another. think generative process entangling local block factor activations. encoding perspective interested using posterior distribution latent variables representation encoding data. unlike rbms case proposed model higher-order interactions latent variables posterior latent variables factorize cleanly. marginalizing slab variables recover conditionals describing binary latent variables interact. four-way multiplicative interaction latent variables real-valued slab variable sijk acts scale contribution weight vector w·ijk. consequence marginalizing factors also seen contributing conditional mean condiimportant property spike-and-slab framework also shared latent variable models real-valued data meancovariance restricted boltzmann machine mean product t-distributions model generative perspective model thought consisting factor blocks whose activity gated variables. within block variables thought local latent factors whose interaction gives rise active block’s contribution visible vector. crucially multiplicative interaction overlapping pooling regions type promising direction future inquiry. potentially interesting direction consider overlapping blocks overlap deﬁne topology features share lower-level features topology thus deﬁned could potentially exploited build higher-level data representations possess local receptive ﬁelds. kind local receptive ﬁelds shown useful building large deep models perform well object classiﬁcation tasks natural images multiplicative interaction latent variables computation intractable. slab variables also interact multiplicatively able analytically marginalize them. consequently resort variational approximation joint conditional standard mean-ﬁeld structure. i.e. choose qvqvqv divergence minimized equivalently variational lower bound likelihood data maximized conditionals distributions deﬁnes form local bilinear interaction interpret values within block acting basis indicators dimensions linear subspace visible space deﬁned w·ijksijk. relative interaction w·ijksijk. concrete example imagine structure weight tensor that along dimension indexed weight vectors w·ijk form oriented gabor-like edge detectors diﬀerent orientations. along dimension indexed weight vectors w·ijk form oriented gabor-like edge detectors diﬀerent colors. hypothetical example encodes orientation information invariance color edge encodes color information invariant orientation. hence could disentangled latent factors. alluded above interpretation role distinct complementary sum-pooled feature sets. returning fig. that block pool across columns block along pool across rows along column. variables also interpretable pooling across elements block. interpret complementary pooling structures multi-way pooling strategy. particular pooling structure chosen study potential learning kind bilinear interaction exists within block. present promote block cohesion gating interaction visible vector higher-order structure course choice many possible higher-order interaction architectures. easily imagine deﬁning arbitrary overlapping pooling regions number overlapping pooling regions specifying order latent variable interaction. believe explorations motivates block-wise organization interactions variables. block structure allows interactions remain local interacting relatively interacting relatively local neighborhood structure allows inference learning procedures better manage complexities teasing apart latent variable interactions adapting model parameters maximize likelihood. using many blocks local interactions leverage known tractable learning properties models rbm. speciﬁcally consider block kind super hidden unit gated interactions across blocks model assumes form rbm. chosen interaction structure allows higher-order model able learn consequence model capable disentangling relatively local factors appear within single block. suggest promising avenue accomplish extensive disentangling consider stacking multiple version proposed model consider layer-by-layer disentangling factors variation present data. idea start local disentangling move gradually toward disentangling local abstract factors. model proposed strongly inﬂuenced previous attempts disentangle factors variation data using latent variable models. earlier eﬀorts direction also used higher-order interactions latent variables speciﬁcally bilinear multilinear models. critical diﬀerence previous attempts disentangle factors variation method unlike previous methods attempting learn disentangle entirely unsupervised information. interpret approach attempt extend subspace feature pooling approach problem disentangling factors variation. spect model parameters contains terms positive phase depends data negative phase derived partition function joint not. adopt training strategy similar combine variational approximation positive phase gradient block gibbs sampling-based stochastic approximation negative phase. gibbs sampler alternately samples parallel random variables sampling brieﬂy outline procedure training unsupervised learning. interactions latent random variables particularly makes unsupervised learning model parameters particularly challenging learning problem. diﬃcultly learning tensor thought generalization typical weight matrix found unsupervised models considered above. developed em-based algorithm learn model parameters demonstrated using images letters distinct fonts model could disentangle style content later developed bilinear sparse coding model similar form described included additional terms objective function render elements sparse. also require observation factors order train model used model develop transformation invariant features natural images. multilinear models simply generalization bilinear model number factors composed together more. develop multilinear model model images faces disentangle factors variation illumination views identities people. hinton also propose disentangle factors variation learning extract features associated pose parameters changes pose parameters known training time. proposed model also closely related recent work higher-order boltzmann machines used models spatial transformations images. number diﬀerences model ours signiﬁcant diﬀerence multiplicative interactions latent variables. included higher-order interactions within boltzmann energy function used exclusively observed variables dramatically simplifying inference learning procedures. another major point departure instead relying low-rank approximations weight tensor approach employs highly structured sparse connections latent variables reminiscent recent work structured sparse coding structured l-norms discussed above sparse connection structure allows isolate groups interacting latent variables. keeping interactions local component ability successfully learn using unsupervised data. figure samples synthetic dataset image ﬁgure appear diﬀerent positions eight basic colors. objects given image must color. filters learnt bilinear ssrbm succesfully show disentangling color information position basic object varying color appear diﬀerent positions. constraint objects given image must color. additive gaussian noise super-imposed resulting images facilitate mixing negative phase. bilinear ssrbm theory capacity disentangle factors variation present data possible colors conﬁgurations object placement. resulting ﬁlters shown fig. model succesfully learnt binary encoding color along g-units positions along note would extremely diﬃcult perform without multiplicative interactions latent variables hidden units technically capacity learn similar ﬁlters however would incapable enforcing mutual exclusivity between hidden units diﬀerent color. bilinear ssrbm model hand generates near-perfect samples factoring representation deeper layers. well suited study problem disentangling models successfully separate identity emotion perform well supervised learning task involves classifying images parts large unlabeled smaller labeled set. note emotions appear much prominently latter since acted thus prone exaggeration. contrast unlabeled contains natural expressions wider range individuals. natural ﬁlters unit-norm. maximum likelihood gradient update thus followed projection ﬁlters onto unit-norm ball. similarly exists over-parametrization direction w·ijk sign µijk parameter controlling mean sijk. thus constrain µijk positive case greater similar constraints applied ensure variances visible slab variables remain bounded. previous work used expected value spike variables input classiﬁers higher-layers deep networks found re-parametrization consistently lead better results using product expectations pooled models simply take product binary spike norm associated slab vector. weighted combination labeled unlabeled training sets. allows greater interpretability results emotion prominent factor variation labeled set). results shown figure clearly show global cohesion within blocks pooled column structure correlating variances appearance/identity emotions. task emotion recognition. main objective evaluate usefulness disentangling traditional approaches pooling well larger unpooled models. thus consider ssrbms features either pooling pooling along single dimension disentangled higher-order ssrbm representation data useful classiﬁcation. reference linear classiﬁer pixels achieves first four models contain approximately ﬁlters bottom four contain cases compare eﬀect using factored representation unfactored representation. presented higher-order extension spike-and-slab restricted boltzmann machine factors standard binary spike variable three interacting factors. generative perspective interactions entangle factors represented latent binary variables. inference interpreted process disentangling factors variation data. previously mentioned believe important direction future research exploration methods gradually disentangle factors variation stacking multiple instantiations proposed model deep architecture.", "year": 2012}