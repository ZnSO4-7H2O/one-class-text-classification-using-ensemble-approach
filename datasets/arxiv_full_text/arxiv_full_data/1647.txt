{"title": "Representation Learning for Grounded Spatial Reasoning", "tag": ["cs.CL", "cs.AI", "cs.LG"], "abstract": "The interpretation of spatial references is highly contextual, requiring joint inference over both language and the environment. We consider the task of spatial reasoning in a simulated environment, where an agent can act and receive rewards. The proposed model learns a representation of the world steered by instruction text. This design allows for precise alignment of local neighborhoods with corresponding verbalizations, while also handling global references in the instructions. We train our model with reinforcement learning using a variant of generalized value iteration. The model outperforms state-of-the-art approaches on several metrics, yielding a 45% reduction in goal localization error.", "text": "figure sample worlds instruction describing goal location. optimal path common start position denoted white dashed line varies considerably changes layout. paper explore problem spatial reasoning context interactive worlds. speciﬁcally assume access simulated environment agent take actions interact world rewarded reaching location speciﬁed language instruction. feedback source supervision model uses interpreting spatial references. modeling task induce representation closely ties environment observations linguistic expressions. prior work issue addressed learning representations modality combining them instance concatenation approach captures high-level correspondences between instructions maps encode deinterpretation spatial references highly contextual requiring joint inference language environment. consider task spatial reasoning simulated environment agent receive rewards. proposed model learns representation world steered instruction text. design allows precise alignment local neighborhoods corresponding verbalizations also handling global references instructions. train model reinforcement learning using variant generalized value iteration. model outperforms state-of-the-art approaches several metrics yielding reduction goal localization error. understanding spatial references natural language essential successful human-robot communication autonomous navigation. problem challenging interpretation spatial references highly context-dependent. instance instruction reach cell westernmost rock translates different goal locations environments shown figure therefore enable generalization unseen worlds model must jointly reason instruction text environment conﬁguration. moreover richness ﬂexibility verbalizing spatial references complicates interpretation instructions. tailed lower-level mappings speciﬁc positions descriptions. experiments demonstrate combining language environment representations spatially localized manner yields signiﬁcant performance gains task. model uses instruction text drive learning environment representation. start converting instruction text realvalued vector using recurrent neural network lstm cells using vector kernel convolution operation obtain instruction-conditioned representation state. allows model reason immediate local neighborhoods references cells left triangle. augment design handle global references involve information concerning entire achieved predicting global value using additional component instruction representation. entire model trained reinforcement learning using environmental reward signal feedback. conducted experiments using virtual world shown figure overall created tasks across maps instructions sourced mechanical turk. compare model state-of-the-art systems adapted task ﬁndings experiments threefold. first model precisely interpret instructions baseline models goal location yielding reduction manhattan distance error closest competitor. second model robustly generalize across unseen layouts. finally demonstrate factorizing instruction representation enables model sustain high performance handling local global references. spatial reasoning text topic attracted theoretical practical interest. linguistic cognitive perspectives research focused wide range mechanisms speakers express spatial relations spatial templates geometrically deﬁned mappings recent work robotics integrated text containing position information spatial models environment obtain accurate maps navigation approaches typically assume access detailed geometry forms domain knowledge. contrast knowledge-rich approaches learning spatial reference interaction environment acquiring knowledge environment process. instruction following spatial reasoning common element many papers instruction following source supervision methods assume access demonstrations specify path corresponding provided instructions. setup agent driven ﬁnal rewards goal achieved. weaker source supervision motivates development techniques considered prior work. recently misra proposed neural architecture jointly mapping instructions visual observations actions environment. model separately induces text environment representations concatenated single vector used output action policy. representation captures coarse correspondences modalities doesn’t encode mappings level local neighborhoods negatively impacting performance task. presents clever trick factorizing value function states goals using singular value decomposition learning regression model predict low-rank vectors. results quick effective generalization goals state space. however work stops short exploring generalization layouts model designed handle. furthermore setup also involves specifying goals using natural language instructions different coordinate-style speciﬁcation used work. task setup model task markov decision process autonomous agent placed interactive environment capability choose actions affect world. goal described text rewards available agent correspondingly. represented tuple possible state conﬁgurations actions available agent goal speciﬁcations natural language transition distribution reward function. state includes information locations different entities along agent’s position. work deterministic environments considered; however methods also apply stochastic case. text instructions prior work investigated human usage different types referring expressions describe spatial relations order build robust instruction following system examine several categories spatial expressions exhibit wide range natural language goal descriptions. speciﬁcally consider instructions utilize objects/entities present environment describe goal location. instructions categorized three groups category local global references objects. local references require understanding spatial prepositional phrases ‘above’ between’ ‘next order determine precise goal location. comprehension invariant global position object landmark provided instruction. global reference hand contains superlatives ‘easternmost’ ‘topmost’ require reasoning entire map. example case above local reference would describe unique object whereas global reference might require comparing positions objects speciﬁc type generalized value iteration learning reach goal maximizing cumulative reward done using value function represents agent’s notion expected future reward state popular algorithm learn optimal value function value iteration uses technique dynamic programming. standard bellman equation value function dependent solely state. schaul proposed value function describing expected reward state given goal capturing state values goal-dependent single environment offer many goals. also make generalized value function although goals observed directly figure schematic depiction model. text instructions represented vector states embeddings portion text representation used convolutional kernel giving text-conditioned local state representation remaining components used coefﬁcients linear combination gradient functions give global map-level representation concatenated input convolutional neural network predict ﬁnal value map. model combines textual instructions spatially localized manner opposed prior work joins goal representations environment observations simpler functions like inner product approach effectively learn local relations speciﬁed language cannot naturally capture descriptions global environment level. address problem also language representation predict coefﬁcients basis gradient functions combined encode global spatial relations. formally inputs model consist environment observation textual description goal simplicity assume matrix although model easily extended input representations. ﬁrst convert tensor projecting cell low-dimensional embedding function objects contained cell. parallel text instruction passed lstm recurrent neural network obtain continuous vector representation vector split local global components local component reshaped kernel perform convolution operation state embedding generalization environment conﬁgurations text instructions requires model meets desiderata. first must ﬂexible representation goals encode local structure global spatial attributes inherent natural language instructions. second must compositional order learn generalizable representation language even though unique instruction observed single training. namely learned representation given instruction still useful even objects rearranged layout changed entirely. state sampled discount factor parameters entire model parameters target network copied periodically model. complete training procedure shown algorithm experimental setup puddle world navigation data order study generalization across wide variety environmental conditions linguistic inputs develop extension puddle world reinforcement learning benchmark states grid ﬁrst ﬁlled either grass water cells grass forms connected component. populate grass region unique objects appear four non-unique objects appear number times given map. figure example visualization. goal positions chosen uniformly random grass cells encouraging spatial references describe goal locations contain unique object. used mechanical turk crowdsourcing platform collect natural language descriptions goals. human annotators asked describe positions used form coefﬁcients vertical horizontal gradient along corresponding bias term. gradients denoted figure matrices dimensionality state observation values increasing rows along columns respectively. axis-aligned gradients weighted elements summed give ﬁnal global gradient spanning entire space analogous steerable ﬁlters constructed orientation using small basis ﬁlters +h·g +h·j finally local global information maps concatenated single tensor processed convolutional neural network parameters approximate generalized value function reinforcement learning given model’s predictions resulting policy enacted giving continuous trajectory states single associated rewards timestep goals using surrounding objects. trial asked participants provide goal locations given text instructions. helped ﬁlter majority instructions ambiguous ill-speciﬁed. table provides statistics data figure shows example annotations. total collected instructions ranging words length describing maps. unique words annotated instructions. perform preprocessing annotations. plausible model designed handle local references could handle global ones clearer interpretation results evaluate model modes trained tested local global data separately combined dataset. local instructions obtained easily global instructions collected designing task nonunique objects presented annotators. precluded simple instructions like left object would always object type. therefore obtained text global properties sufﬁciently pinpoint object. average collected unique local instructions unique global instructions map. quantify diversity dataset nearest instructions training every instruction test measured edit distance normalized test instruction length. pairs also measure manhattan distance corresponding goal locations. figure visufigure heatmap showing normalized instruction edit distance goal manhattan distance corresponding similar instructions between train test set. instruction test similar instructions training set. even instructions similar goal locations describe apart. alizes analysis underscores difﬁculty task; even instructions highly similar might correspond entirely different target locations. case example figure distance four references goals. uvfa variant model described adapted task. original model made mlps learn dimensional embeddings states goals combined product give value estimates. goals represented either coordinates states themselves. goals observed directly described text replace goal lstm model. state identical architecture uvfa hidden layers dimension relu activations. consistency uvfa represent states binary vectors denoting presence type object every position. figure reward achieved model baselines training environments reinforcement learning local global instructions. epoch corresponds simulation goals goal simulation terminating either agent reaches goal state taken actions. model implementation uses lstm learnable -dimensional embedding layer hidden units -dimensional embeddings kernel applied embeddings giving dimension ﬁnal layers channels kernels padding length output value prediction equal size input observation. reward given reaching correct goal speciﬁed human annotation reward given falling puddle cell. terminal state agent goal. rewards discounted factor adam optimization training models. puddle world navigation comparison state-of-the-art ﬁrst investigate ability model learn solely environment simulation. figure shows discounted reward achieved model well baselines instruction types. experiments model uvfa original uvfa model evaluate modiﬁed puddle worlds determine difﬁculty environment generalization independently instruction interpretation. additional reinforcement learning experiments train models supervised setting isolate effects architecture choices concerns inherent reinforcement learning algorithms. purpose constructed dataset ground-truth value maps humanannotated goals using value iteration. models predict value maps entire grid minimize mean squared error compared ground truth values table performance models trained reinforcement learning held-out environments instructions. policy quality true expected normalized reward distance denotes manhattan distance goal location prediction true goal position. show results training local global instructions separately jointly. table performance test environments instructions supervised training. lower better manhattan distance; higher better policy quality. gradient basis signiﬁcantly improves reconstruction error goal localization model global instructions expectedly affect performance local instructions. following schaul also evaluated model using metric policy quality. deﬁned expected discounted reward achieved following softmax policy value predictions. policy quality normalized optimal policy score uniform random policy score intuitively policy quality true normalized expectation score maps dataset instructions start states map-instruction pair. model outperforms baselines metric well test maps also note perforfinally given nature environments predicted value maps infer goal location taking position maximum value. manhattan distance predicted position actual goal location third metric. accuracy model’s goal predictions twice baselines local references roughly better global references. figure value predictions environments paired instructions each. despite difference instructions global local nature sharing objects descriptions refer goal location environment however descriptions correspond different locations map. vertical axis considers variance goal location instruction depending conﬁguration. unseen test split maps. table shows results study. expected model withglobal gradient performs differently full model local references higher average distances true goal full model global references. also note uvfa performs much worse cnn+lstm model showing difﬁculty environment generalization even goals observed directly. demonstrated effective generalization goal states within single environment.) surprisingly model trained reinforcement learning precise goal location predictions trained true state values supervised manner. however value predictions much higher setting shows despite comparative stability supervised setting minimization value prediction error necgeneralization criteria laid model ability construct language representations produce accurate value maps independent layouts linguistic variation. figure provides examples layouts different instructions. ﬁrst instructions referring location. model able mimic optimal value accurately baselines precise either producing large ﬁeld possible goal locations completely missing goal vertical axis observe generalization across different maps instructions. model able precisely identify goals scenario spite signiﬁcant variation figure effect training size held-out predictions. curves show mean training runs shaded regions show standard deviation. model’s policy quality greater training goal annotations. also evaluate model language grounding dataset contains human-annotated instructions describing arrange blocks identiﬁed numbers logos. although contain variable environment maps dataset larger action space vocabulary. caveat task posed original dataset compatible model. policy derived value dimension state observation implicitly assumed single configure examples failure cases model. multiple levels indirection long instruction ﬁlled redundant information make instruction difﬁcult interpret. intended goal locations outlined clarity. although model compositional sense transfers knowledge spatial references between different environments types instructions prove challenging. identify poorest predictions figure multiple levels indirection unnecessarily long instructions still challenge. learning curve manual effort comes constructing dataset human annotations also important consider sampleefﬁciency model. figure shows quality policy prediction error local instructions function training size. observe model reaches policy quality figure visualizations tasks language grounding dataset model’s value predictions. agentive block goal location outlined visibility. value prediction function subgoal’s ordering overall task. model performs better subgoals later task despite subgoals treated completely independently training testing. trollable agent whereas allows multiple blocks moved. therefore modify setup using oracle determine block given agency step. allows retain linguistic variability dataset overcoming mismatch task setup. states discretized instructions lemmatized. performance modiﬁed dataset reported table representative visualizations shown figure model outperforms baselines greater margin policy quality dataset. misra also dataset report results part determining minimum distance agent goal evaluation lasting steps. evaluation metric therefore dependent timeout parameter discretized state space able represent grid embeddings notion single step changed direct comparison limited understanding grounding evaluation interesting ﬁnding analysis difﬁculty language interpretation task function stage task execution language grounding individual instruction subgoal larger task value maps predicted subgoals occurring later task accurate occurring early task. likely language plays less crucial role specifying subgoal position when model available states overwhelmingly high-dimensional policy quality useful metric independent type parameter. such default metric here. however estimating policy quality environments substantially larger investigated challenge itself. ﬁnal steps task. shown figure possible narrow candidate subgoal positions looking nearly-constructed highlevel shape. contrast would possible early task blocks randomly positioned. ﬁnding consistent result branavan reported strategy game manuals useful early game became less essential play. appears part larger trend marginal beneﬁt language grounding tasks vary predictably individual instructions. described novel approach grounded spatial reasoning. combining language representation spatially localized manner allows increased precision goal identiﬁcation improved performance unseen environment conﬁgurations. alongside models present puddle world navigation grounding dataset testing generalization capacity instructionfollowing algorithms varied environments.", "year": 2017}