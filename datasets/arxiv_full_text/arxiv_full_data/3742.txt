{"title": "Scaling-up Split-Merge MCMC with Locality Sensitive Sampling (LSS)", "tag": ["cs.LG", "cs.AI", "cs.DS", "stat.ME", "stat.ML"], "abstract": "Split-Merge MCMC (Monte Carlo Markov Chain) is one of the essential and popular variants of MCMC for problems when an MCMC state consists of an unknown number of components. It is well known that state-of-the-art methods for split-merge MCMC do not scale well. Strategies for rapid mixing requires smart and informative proposals to reduce the rejection rate. However, all known smart proposals involve expensive operations to suggest informative transitions. As a result, the cost of each iteration is prohibitive for massive scale datasets. It is further known that uninformative but computationally efficient proposals, such as random split-merge, leads to extremely slow convergence. This tradeoff between mixing time and per update cost seems hard to get around. In this paper, we get around this tradeoff by utilizing simple similarity information, such as cosine similarity, between the entity vectors to design a proposal distribution. Such information is readily available in almost all applications. We show that the recent use of locality sensitive hashing for efficient adaptive sampling can be leveraged to obtain a computationally efficient pseudo-marginal MCMC. The new split-merge MCMC has cheap proposal which is also informative and needs significantly fewer iterations than random split-merge. Overall, we obtain a sweet tradeoff between convergence and per update cost. As a direct consequence, our proposal, named LSHSM, is around 5x faster than the state-of-the-art sampling methods on both synthetic datasets and two large real datasets KDDCUP and PubMed with several millions of entities and thousands of clusters.", "text": "split-merge mcmc essential popular variants mcmc problems mcmc state consists unknown number components. well known state-of-the-art methods split-merge mcmc scale well. strategies rapid mixing requires smart informative proposals reduce rejection rate. however known smart proposals involve expensive operations suggest informative transitions. result cost iteration prohibitive massive scale datasets. known uninformative computationally efﬁcient proposals random split-merge leads extremely slow convergence. tradeoff mixing time update cost seems hard around. paper around tradeutilizing simple similarity information cosine similarity entity vectors design proposal distribution. information readily available almost applications. show recent locality sensitive hashing efﬁcient adaptive sampling leveraged obtain computationally efﬁcient pseudo-marginal mcmc. split-merge mcmc cheap proposal also informative needs signiﬁcantly fewer iterations random split-merge. overall obtain sweet tradeoff convergence update cost. direct consequence proposal named lshsm around faster state-of-the-art sampling methods synthetic datasets large real datasets kddcup pubmed several millions entities thousands clusters. bayesian mixture models great interest ﬂexibility ﬁtting countably inﬁnite number components grow data growth model complexity data also agreement modern progress machine learning massive datasets. however appealing properties bayesian modeling come hard computational problems. even simple mixture models mathematical problems associated training inference intractable. result recent research focuses developing tractable computational techniques. particular markov chain monte carlo methods sample posterior distribution widely prevalent. practical utility methods illustrated several applications including haplotype reconstruction nucleotide substitutions gene expression etc. metropolis-hastings favorite class mcmc methods includes several state-of-the-art algorithms proven useful practice. associated transition kernel provides proposal step. step followed appropriate stochastic acceptance process ensures detailed balance. notable example split-merge mcmc algorithm particularly useful problems mcmc state thought consisting number components name suggests proposal step comprises either split merge. split move partitions existing mixture component merge move combines mixture components one. seminal work split-merge mcmc procedure proposed. illustrate process authors ﬁrst introduce random split-merge mcmc split merge decision taken uniformly random. however also pointed paper random nature proposal unlikely lead state higher likelihood leading acceptance. mitigate slow progress authors propose restricted gibbs split-merge rgsm instead random proposal idea restricted gibbs sampling generate proposals higher likelihood acceptance. thus less number mcmc iterations sufﬁcient convergence fewer rejections. however cost restricted gibbs prohibitive. result even though iterations less iteration costly making overall algorithm slow especially large datasets. experiments conﬁrm slow convergence rgsm. essential surprising observation space asymmetry smart proposals split-merge mcmc made authors show necessity smart dumb proposals faster progress. proposed smart-dumb/dumb-smart algorithm alternative rgsm. instead relying gibbs sampling sdds algorithm instead uses likelihood model guiding strategy smart proposals. words sdds method evaluates large number possible proposals based likelihood choose best ones. strategy expected ensures higher chance improving state every proposal. however computational perspective difﬁcult smart proposal obtained evaluation large number proposal states based likelihood equivalent evaluating states acceptance/rejection part result reduction number iteration helpful obtaining efﬁcient algorithm. experiments show sdds also poor convergence. unfortunately mcmc methodologies ignore tradeoff number iteration computations associated iteration. instead focus reducing number rejections often achieved informative proposals increased iteration cost. paper interested efﬁcient split-merge mcmc algorithm leads overall fast convergence. thus reducing work. parallelization complementary signiﬁcance problem several works scale mcmc using parallelism. parallelism often achieved running parallel mcmc chains subsets data later merging since proposal reduces overall cost split-merge mcmc algorithm general reduce cost parallel chains thereby increasing effectiveness parallelisms mcmc. thus existing advances parallelizing mcmc complementary proposal. contributions work leverage several complementary ideas design computationally efﬁcient split-merge mcmc algorithm. ﬁrst leverage simple observation clusters entities similar vector representation favored. standard notions vector similarity cosine similarity. however observation sufﬁcient itself designing proposals favoring similar entities cluster requires computing pairwise similarities prohibitive quadratic time operation. leverage recent advances sampler perform similarity sampling linear cost. efﬁcient sampling guide proposal design. leverage surprising observation made merge dumb smart proposal fast progress. finally reduce likelihood computation time sampler proposal design produce unbiased estimator likelihood leading valid pseudo-marginal split-merge mcmc. unbiased estimators superior favorite random sampling based estimation. name method lshsm mcmc. overall lshsm obtains sweet tradeoff number iteration computational cost iteration. result reduce overall computational cost. several simulations well large public datasets lshsm signiﬁcantly outperforms stateof-the-art split-merge mcmc algorithms convergence speed measured wall clock time machine. lshsm around faster second best baseline real datasets without loss accuracy. locality-sensitive hashing popular technique efﬁcient approximate nearest-neighbor search. family functions function uniformly sampled hash family property that hash mapping similar points high probability hash value. precisely consider family hash functions mapping discrete well studied topic computer science theory database literature. many well-known families literature. please refer details. popular signed random projections signed random projections cosine similarity measure originates concept randomized rounding given vector utilizes random vector component generated i.i.d. normal i.e. stores sign projection. formally family given considered black-box algorithm similarity search dimensionality reduction. recently found used something subtler useful. data structure used efﬁcient dynamically adaptive sampling. ﬁrst describe sampling algorithm later comment properties crucial proposal. algorithm uses parameters construct independent hash tables collection hash table meta-hash function formed concatenating random independent hash functions appropriate locality sensitive hash family candidate sampling algorithm works phases pre-processing phase construct hash tables data storing elements store pointers vector hash tables storing whole data vectors memory inefﬁcient. one-time linear cost. difﬁcult show item returned candidate -parameterized algorithm sampled probability exactly collision probability function. family deﬁnes precise form used build hash tables. sampling view ﬁrst utilized perform adaptive sparsiﬁcation deep networks near-constant time leading efﬁcient backpropagation algorithm year later demonstrated ﬁrst theory using samples unbiased estimation partition functions log-linear models. speciﬁcally authors showed since know precise probability sampled elements could design provably unbiased estimators using importance sampling type idea. ﬁrst demonstration random sampling could beaten roughly computational cost vanilla sampling. used approach unbiased estimation anomaly scoring function. used sampling different context connected component estimation unique entity counts. showed improvements sample complexity kernel density estimation problems. important observation made expression monotonically increasing function turn monotonic function cosine similarity hashing scheme. thus given query points higher cosine similarity likely sampled. similarity points dissimilar less likely. noted querying cost involved hash computations followed couple memory lookups similar random sampling. capitalizing unique efﬁciency proposed ﬁrst gradient descent algorithm beat popular variants running time breaking call chicken-and-egg loop adaptive sampling. proposal heavily rely unusual probability expression design informative proposal distribution. will addition probability expression also utilize hashing obtain unbiased estimate likelihood leading pseudo-marginal mcmc algorithm. clusters components known advance. split-merge mcmc metropolis-hastings algorithm main transitions split merge. split cluster partitioned components. contrary merge takes components makes one. mcmc inference process split merge moves simultaneously change number entities change assignments entities different clusters. proposes ﬁrst non-trivial restricted gibbs splitmerge algorithm later utilized efﬁcient topic modeling large datasets authors presented surprising argument information asymmetry. shown informative split merge leads poor acceptance ratio. author proposed combination smart split dumb merge dumb split smart merge remedy. algorithm named smart-dumb/dumbsmart split merge algorithm superior rgsm. obtain non-trivial smart split authors propose evaluate large number dumb proposals based likelihood select best. search process made proposal expensive. difﬁcult ﬁnding smart split computationally different running chain several sequences dumb splits. utilizing similarity information paper make argument similarity information cosine similarity different entities almost always available. example clustering task alalways vector representation data computing likelihood. even application deal complex entities trees uncommon approximate embeddings natural believe similar entities terms cosine similarity underlying vector representation likely cluster non-similar ones. thus designing proposals favor similar entities cluster dissimilar entities different clusters likely lead acceptation random proposals. however problem solved. similarity based sampling requires computing pairwise similarity prerequisite quadratic operation quadratic operations prohibitive large datasets. critical observation section discusses informative proposal compute transition probabilities q|x) important component acceptance ratio α|x) here denote state split/merge denote state split/merge. although likelihood terms α|x) approximated without changing equilibrium distribution mcmc q|x) still cannot approximated. thus imperative q|x) easy calculate well proposed state informative. note cheap approximation like sampling cannot used proposing likely result intractable expression q|x). thus designing right q|x) speed computation. following intuition described section introduce based proposal design rest section. ﬁrst create hash tables data structure sampling sign random projection function thus notion similarity cosine gives collision probability. one-time linear cost preprocessing. note need signiﬁcantly less compared required nearneighbor queries sampling. sampling informative values informative proposal need capabilities similarity sampling well dissimilarity sampling merge split respectively. similarity sampling usual sampling algorithm discussed section ensures given query points similar cosine similarity likely sampled. analogously also need sample points likely dissimilar. cosine similarity ﬂipping sign query i.e. changing automatically dissimilarity sampling. inspired also leverage information asymmetry smart dumb moves better convergence. however time proposals super efﬁcient. iteration mcmc start choosing randomly smart-split/dumb-merge smart-merge/dumb-split operation. operations deﬁned below derivations formula shown supplementary material. above data points returned query using denotes number elements symbols meaning before. collision probability noticing that calculate probability need possible components could expensive cluster size large. computational bottleneck probability calculation k)l). however value k)l) independent inference process practical solution cache calculated values k)l) inference process. step split/merge ﬁrst whether value k)l) already calculated directly value instead re-calculate value. experiment strategy caching speed computation. introduced before proposed lshsm algorithm belongs general framework metroplis-hastings algorithm split/merge move need calculate acceptance rate α|x) move given sampler) sample points likely dissimilar thus query data structure query another element likely away belong cluster split cluster. split create clusters assign every element randomly assign either since ensure dissimilar points split informative smart split. already different cluster dumb merge randomly select components merge components component. above number data point. data points returned querying using |s−u| denotes number elements s−u. denotes number clusters state denotes original component components split elements them. number bits used hashing number hash tables probed. collision probability calculated provide derivations formula supplementary material. based merge begins randomly selecting element dataset. sample hash tables another element similar then mixture component different merge operation corresponding mixture component. components dumb split randomly select cluster split component separate components. provide probability merge move q|x) superior variance plain random sampling estimators. please refer details. vital point note since estimators unbiased guarantees desired equilibrium distribution mcmc. superior unbiased estimation liklihood fortunately replacing computation α|x) unbiased estimator sufﬁces guarantee equilibrium distribution. method popularly knows pseudo-marginal mcmc random sampling based unbiased estimator default choice speed likelihood computation. turns existing structure better unbiased estimator. better unbiased estimation likelihood leverage insights instead calculating likelihood random sample dataset instead sample small fraction data cluster means query. small sampled data approximate likelihood function cluster means better sampling consider equation note value higher closer cluster mean thus query small sample mean query favors sampling heavier entries therefore superior estimator random sampling. also since also know probability sampling design unbiased sampler. brieﬂy review gaussian mixture model. gaussian mixture density weighted component densities. m-class clustering task could gmms associated cluster. d-dimensional feature vector denoted mixi= wipi mixture weights satisfy mixture density weighted linear combination component unimodel gaussian density functions gaussian mixture density parameterized mixture weights mean vectors covariance vectors components densities. gmm-based clustering task goal model training estimate parameters gaussian mixture density best match distribution training feature vectors. estimating parameters using expectation-maximization algorithm popular. however real world applications number clusters known required algorithm. hand split-merge based mcmc algorithms used inference unknown also focus paper. therefore compare proposal lshsm state-of-the-art split-merge algorithms clustering require prior knowledge number clusters. compare following three split-merge mcmc sampling algorithm unknown number clusters rgsm restricted gibbs split-merge mcmc algorithm considered state-of-the-art sampling algorithm. sdds smart-dumb/dumb-smart split merge algorithm sdds combines smart split/merge move proposes plausible splits heterogeneous clusters dumb merge move proposes merging random pairs clusters. lshsm based split merge algorithm proposed method paper. lshsm method ﬁxed dataset. hashing scheme signed random projection. synthetic data standard testing models paper ﬁrst synthetic datasets sanity check evaluate performance different methods. process generating synthetic dataset follow randomly generate different gaussian distributions experiment. based randomly generated gaussian distributions generate data points gaussian distribution. dimensionality data point experiment generate three sythntic dataset different size name three synthetic dataset also evaluate performance methods real word datasets kddcup dataset used data mining competition. contains data point. dimensionality dataset ground truth cluster labels dataset. pubmed pubmed abstraction dataset contains abstractions extracted pubmed documents represented bag-of-words representation. data different words. data ideal document clustering topic modeling. dataset available machine learning dataset repository. ﬁrst plot evolution likelihood function iterations well time three competing methods. evolution likelihood iterations synthetic dataset real-world data shown fig. fig. respectively. fig. fig. plots evolution time instead iteration. figure time wise comparison likelihood difference methods synthetic dataset. lshsm outperforms baselines large margin. also clear requiring less iteration mean faster convergence. poorly requires iterations well time. need combining smart dumb moves faster convergence made seems necessary. rgsm hence leads poor even iteration wise convergence. sdds seems quite well compared proposed lshsm look iteration wise convergence. however look time picture completely changed. lshsm signiﬁcantly faster sdds even convergence slower iteration wise. surprising per-iteration cost lshsm orders magnitude less sdds. sdds hides computations inside iteration evaluating every possible state iteration based likelihood equivalent several random iterations combined. costly evaluation iteration give false impressing less iteration. clear plots merely comparing iterations acceptance ratio give false impression superiority. time wise comparison legitimate comparison overall computational efﬁciency. clearly lshsm outperforms baselines large margin. experiment large datasets that data lshsm converge less seconds rgsm sdds need nearly hour converge. pubmed dataset lshsm method converge less hours sdds need hours converge rgsm requires hours converge. demonstrates proposed lshsm algorithm times faster state algorithms large dataset. figure time iteration wise comparison likelihood difference methods real dataset. obviously proposed lshsm algorithm least times faster state algorithms real large dataset. clustering accuracy comparison evaluate clustering performance different algorithms widely used measures brieﬂy review deﬁnition measures below normalized mutual information widely used measuring performance clustering alcalculated gorithms. marginal entropies mutual information accuracy accuracy measure calculated percentage target objects going correct cluster deﬁned accuracy number data objects clustered corresponding true cluster number cluster number data objects dataset. split-merge mcmc essential popular variants mcmc problems unknown number components. well known inference process splitmerge mcmc computational expensive applicable large-scale dataset. existing approaches speed split-merge mcmc stuck computational chicken-and-egg loop problem. paper proposed lshsm accelerating split merge mcmc probabilistic hashing. splitmerge mcmc constant time update time proposal informative needs signiﬁcantly fewer iterations random split-merge. overall obtain sweet tradeoff convergence update cost. experiments gaussian mixture model", "year": 2018}