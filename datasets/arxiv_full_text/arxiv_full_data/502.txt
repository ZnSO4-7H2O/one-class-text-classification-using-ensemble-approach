{"title": "Reusing Weights in Subword-aware Neural Language Models", "tag": ["cs.CL", "cs.NE", "stat.ML", "68T50", "I.2.7"], "abstract": "We propose several ways of reusing subword embeddings and other weights in subword-aware neural language models. The proposed techniques do not benefit a competitive character-aware model, but some of them improve the performance of syllable- and morpheme-aware models while showing significant reductions in model sizes. We discover a simple hands-on principle: in a multi-layer input embedding model, layers should be tied consecutively bottom-up if reused at output. Our best morpheme-aware model with properly reused weights beats the competitive word-level model by a large margin across multiple languages and has 20%-87% fewer parameters.", "text": "propose several ways reusing subword embeddings weights subwordaware neural proposed techniques beneﬁt competitive character-aware model improve performance syllablemorpheme-aware models showing signiﬁcant reductions model sizes. discover simple hands-on principle multilayer input embedding model layers tied consecutively bottom-up reused output. best morpheme-aware model properly reused weights beats competitive word-level model large margin across multiple languages fewer parameters. statistical language model model assigns probability sequence words. used speech recognition machine translation part-of-speech tagging information retrieval applications. data sparsity major problem building traditional n-gram language models assume probability word depends previous words. deal potentially severe problems confronted n-grams explicitly seen before form smoothing necessary. recent progress statistical language modeling connected neural language models tackle data sparsity problem representing words vectors. typically done twice input output especially successful models architecture neural network between input output recurrent widely used relatively recently empirical evidence well theoretical justiﬁcation simple trick improves language modeling quality decreasing total number trainable parameters almost two-fold since parameters embedding matrices. unfortunately regularization technique directly applicable subword-aware neural language models receive subwords input return words output. raises following questions possible reuse embeddings parameters subword-aware neural language models? would beneﬁt language modeling quality? experimented different subword units embedding models ways reusing parameters answer questions follows several ways reuse weights subword-aware neural language models none improve competitive character-aware model beneﬁt syllablemorphemeaware models giving signiﬁcant reductions model sizes. simple morpheme-aware model sums morpheme embeddings word beneﬁts appropriate weight tying showing signiﬁcant gain competitive word-level baseline across different languages data sizes. another contribution paper discovery hands-on principle multi-layer input embedding model layers tied consecutively bottom-up reused output. subword-aware large number publications last years subword-level subword-aware nlms especially cases subwords characters morphemes less work done syllable-level syllable-aware nlms thorough up-to-date review previous work subword-aware neural language modeling refer reader paper vania lopez authors systematically compare different subword units different representation models languages various morphological typology. tying weights reusing embeddings word-level neural language models technique used earlier studied details recently however much work done reusing parameters subword-aware subwordlevel language models. jozefowicz reused charcnn architecture dynamically generate softmax word embeddings without sharing parameters input word-embedding sub-network. managed signiﬁcantly reduce total number parameters large models trained huge dataset english large vocabulary expense deteriorated performance. labeau allauzen used similar approach augment output word representations subword-based embeddings. experimented characters morphological decompositions tried different compositional models czech dataset consisting tokens. tying weights input output representations since preliminary experiments tied weights gave worse results. subword-level rely subword-level inputs make predictions level subwords; subword-aware also rely subword-level inputs make predictions level words. focus ways reuse weights output seek model size reduction performance improvement subword-aware language models different subword units make evaluation small medium data sets across multiple languages. recurrent neural language model ﬁnite vocabulary words. assume words already converted indices. einw r|w|×dw input embedding matrix words i.e. matrix corresponds embedding word eoutw rdlm×|w| output embedding matrix r|w| bias term state size rnn. subword-based word embeddings recent advancements neural language modeling segmenting words input subword units composing word’s embedding embeddings subwords. formally ﬁnite vocabulary subwords eins r|s|×ds input embedding matrix subwords. word sequence subwords hence represented sequence corresponding subword vectors rnnlm instead plain embedding additional parameters correspond embedding model constructs word vector instance charcnn model weights convolutional highway layers. reusing word embeddings another recent technique word-level neural language modeling tying input output word embeddings assumption dlm. although useful word-level language modeling regularization technique directly applicable subword-aware language models receive subword embeddings input return word embeddings output. next section describe simple technique allow reusing subword embeddings eins well parameters subword-aware rnnlm. eouts output embedding matrix subwords modify softmax layer utilizes eouts instead word embedding matrix eoutw idea fairly straightforward reuse embedding model construct ˆeoutw instead eoutw softmax layer modiﬁcation softmax layer referred subword-based softmax. overarchitecture subword-aware rnnlm subword-based softmax given figure model allows several options reusing embeddings weights discussed below. reusing neither subword embeddings embedding model weights shown jozefowicz signiﬁcantly reduce total number parameters large models trained huge datasets large vocabularies however expect signiﬁcant reductions smaller data sets smaller vocabularies main experiments. reusing subword embeddings done setting eouts eins give signiﬁcant reduction model size models |eins| |θin| morphemeaware model botha blunsom reusing weights embedding model done setting θout θin. unlike previous option signiﬁcantly reduce sizes models |eins| |θin| character-aware model reusing subword embeddings weights embedding model done setting eouts eins θout simultaneously signiﬁcantly reduce number trainable parameters subword-aware model. exactly word representations input output option corresponds reusing plain word embeddings pure word-level language models. data sets models trained evaluated wikitext data sets. utilize standard training validation test splits along preprocessing mikolov wikitext- alternative approximately times large size three times large subword-based embedding models experiment existing representational models previously proven effective language modeling. charcnn characteraware convolutional model performs state-of-the-art wordlevel lstm model despite fewer parameters. morphsum summation morpheme embeddings similar approach botha blunsom important difference embedding word included sum. since models utilize word embeddings. subword-aware language models inject stack highway layers right word-level rnnlm done non-linear activation highway layers relu. highway layer size denoted dhw. word-level rnnlm large variety cells choose make results directly comparable previous work inan press wolf reusing word embeddings select rather conventional architecture stack lstm cells hyperparameters experiment conﬁgurations state size word-level rnnlm follows values outside brackets correspond small models values within brackets correspond medium models. charcnn hyperparameters work large model stands call medium-sized model. sylconcat optimizaton method guided previous works word-level language modeling lstms. appendix details. syllabiﬁcation morphological segmentation true syllabiﬁcation word requires grapheme-to-phoneme conversion splitting syllables based rules. true morphological segmentation requires rather expensive morphological analysis disambiguation tools. since always available under-resourced languages decided utilize liang’s widely-used hyphenation algorithm unsupervised morphological segmentation tool morfessor approximations syllabiﬁcation morphological segmentation respectively. default conﬁguration morfessor syllable morpheme vocabulary sizes wikitext- reported table order investigate extent proposed options beneﬁts language modeling task evaluate four modiﬁcations subwordaware model original versions word-level baselines. results evaluation given table negative positive ﬁndings summarized below. negative results reusing’ options never applied subword-aware language models deteriorate performance. word word reusing word emb’s charcnn charcnn charcnn charcnn charcnn sylconcat sylconcat sylconcat sylconcat sylconcat morphsum morphsum morphsum morphsum morphsum table results. pure word-level models original versions subword-aware models serve baselines. reusing input embedding architecture output charcnn leads prohibitively slow models trained wikitext- therefore abandoned evaluation conﬁgurations. charcnn biased towards surface form hypothesize reason charcnn beneﬁt tied weights character embeddings excessively ﬂexible model learns adapt surface form semantics. validate hypothesis pick several words english vocabulary consider nearest neighbors cosine similarity produced medium-sized models input examples charcnn model somewhat biased towards surface forms input sylconcat morphsum. charcnn reused generate softmax embedding matrix bias propagated output embeddings well table notice tying weights without tying subword embeddings always results worse performance tying weights embeddings recall subword embedding lookup done weights subword-aware embedding model used leads following eouts θout parameters consecutive layers subwordaware embedding model used generate output projection matrix subwordaware neural language model ﬁrst layers input output embedding subtable nearest neighbors based cosine similarity. underline character ngrams words close given word orthographically rather semantically. pyphen syllabiﬁer used sylconcat failed segment word ‘looooook’ syllables therefore neighbors available. test conjecture empirically conduct following experiments three embedding models reuse different combinations layers. embedding model layers ways reuse them layer either tied untied input output. however particular conﬁgurations embedding models interest neither layers reused ﬁrst embedding layer reused. hence model need check conﬁgurations. faster experimentation evaluate small-sized models ptb. results reported table experiments general reject conjecture sylconcat leaving untied ﬁrst highway layer tied embedding second highway layers turned slightly better tying three layers recall highway weighted average nonlinear identity transformations incom= transform gate trainable parameters element-wise multiplication operator. leaving untied highway tied beneﬁcial sylconcat compare distributions transform gate values ﬁrst highway layers conﬁgurations hw+emb hw+hw+emb sylconcat morphsum sylconcat heavily relies nonlinearity ﬁrst highway layer morphsum utilize much means morphsum highway close identity operator transform morpheme vectors much either input output. therefore tying ﬁrst highway layer natural morh-sum. sylconcat hand applies non-linear transformations concatenation syllable vectors hence makes additional preparations word vector needs rnnlm input softmax prediction output. needs differ sylconcat beneﬁts additional degree freedom ﬁrst highway left untied. figure kernel density estimations transform gate values ﬁrst highway layers sylconcat morphsum. values corresponding ‘input’ ‘output’ curves come hw+emb conﬁgurations corresponding ‘tied’ curves come hw+hw+emb conﬁgurations. i.e. leave untied layer tied one. keep mind rule guarantee performance increase layers tied. says leaving untied weights tied ones likely worse notice results experiments untied second highway layer ﬁrst always leads better performance tied. means beneﬁt letting word embeddings slightly differ input output i.e. specializing needs rnnlm input softmax output. specialization quite natural input output representations words different purposes input representations send signal rnnlm current word sequence output representations needed predict next word given preceding words. difference between input output word representations discussed greater detail garten press wolf decided verify difference indirectly test whether intrinsic dimensionality word embeddings signiﬁcantly differs input output. this apply principal component analysis word embeddings produced models reusing mode. results given figure dimensionalities input output embeddings differ word-level model charcnn sylconcat models difference less signiﬁcant morphsum model. interestingly word-level morphsum models output embeddings principal components input ones. charcnn sylconcat however results around. defer study phenomenon future work. figure applied input word embeddings produced different models. horizontal axis corresponds number principal components vertical axis corresponds percentage total variance retain. left right word-level model charcnn sylconcat morphsum. according table morphsum+re+rw comfortably outperforms strong baseline word+re interesting whether advantage extends non-english languages richer morphology. purpose conduct evaluation models small medium data languages hardware constraints train small models medium-sized data. used architectures languages perform language-speciﬁc tuning hyperparameters speciﬁed appendix results provided table advantage morpheme-aware model word-level even pronounced non-english data. also notice gain larger small data sets. hypothesize advantage morphsum+re+rw word+re diminishes decrease typetoken ratio scatterplot change versus supports hypothesis. moreover strong correlation quantities i.e. predict mean decrease text simple linear regression single best reuse parameters subword-aware neural language models reusing method tailored type subword unit embedding model. however instead testing exponential number conﬁgurations sufﬁcient check weights tied consecutively bottom-up. despite similar input output embeddings solve different tasks. thus fully tying input output embedding sub-networks subwordaware neural language models worse letting slightly different. raises question whether true pure wordlevel models defer study future work. best conﬁgurations simple morpheme-aware model sums morpheme embeddings fully reuses embedding subnetwork outperforms competitive word-level language model signiﬁcantly reducing number trainable parameters. however performance gain diminishes increase training size. references zhenisbek assylbekov rustem takhanov bagdat myrzakhmetov jonathan washington. syllable-aware neural language models failure beat character-aware ones. proc. emnlp. yoshua bengio r´ejean ducharme pascal vincent christian jauvin. neural probabilistic language model http//www.iro.umontreal. ca/œlisa/pointeurs/nips_lm.ps. wang ling chris dyer alan black isabel trancoso ramon fermandez silvio amir luis marujo tiago luis. finding function form compositional character models open vocabulary word representation. proc. emnlp. truncated bptt backpropagate time steps using stochastic gradient descent learning rate initially small word-level models small medium charcnn medium models start decaying constant rate certain epoch. small word-level networks respectively except charcnn decay rate initial values learning rates tuned follows model start decrease convergence ﬁrst epoch. batch size train epochs. parameters models randomly initialized uniformly small medium networks except forget bias word-level lstm initialized transform bias highway layer initialized values around regularization variant variational dropout proposed inan dropout rates small medium models. wikitext- dropout rates small medium models. clip norm gradients non-english small-sized data sets hyperparameters ptb. speed training non-english medium-sized data batch size sampled softmax number samples equal vocabulary size", "year": 2018}