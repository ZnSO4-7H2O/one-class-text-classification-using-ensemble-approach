{"title": "Network Trimming: A Data-Driven Neuron Pruning Approach towards  Efficient Deep Architectures", "tag": ["cs.NE", "cs.CV", "cs.LG"], "abstract": "State-of-the-art neural networks are getting deeper and wider. While their performance increases with the increasing number of layers and neurons, it is crucial to design an efficient deep architecture in order to reduce computational and memory costs. Designing an efficient neural network, however, is labor intensive requiring many experiments, and fine-tunings. In this paper, we introduce network trimming which iteratively optimizes the network by pruning unimportant neurons based on analysis of their outputs on a large dataset. Our algorithm is inspired by an observation that the outputs of a significant portion of neurons in a large network are mostly zero, regardless of what inputs the network received. These zero activation neurons are redundant, and can be removed without affecting the overall accuracy of the network. After pruning the zero activation neurons, we retrain the network using the weights before pruning as initialization. We alternate the pruning and retraining to further reduce zero activations in a network. Our experiments on the LeNet and VGG-16 show that we can achieve high compression ratio of parameters without losing or even achieving higher accuracy than the original network.", "text": "state-of-the-art neural networks getting deeper wider. performance increases increasing number layers neurons crucial design efﬁcient deep architecture order reduce computational memory costs. designing efﬁcient neural network however labor intensive requiring many experiments ﬁne-tunings. paper introduce network trimming iteratively optimizes network pruning unimportant neurons based analysis outputs large dataset. algorithm inspired observation outputs signiﬁcant portion neurons large network mostly zero regardless inputs network received. zero activation neurons redundant removed without affecting overall accuracy network. pruning zero activation neurons retrain network using weights pruning initialization. alternate pruning retraining reduce zero activations network. experiments lenet vgg- show achieve high compression ratio parameters without losing even achieving higher accuracy original network. neural networks widely adopted many scenarios achieving state-of-the-art results numerous tasks keys improved performance increased depth width thus increased number parameters. computer vision witnessed orders magnitude increase number parameters cnns lenet less parameters handwritten digit classiﬁcation deepface parameters human face classiﬁcation although cnns elegant network architectures easy deploy real-world tasks designing hard labor-intensive involves signiﬁcant amount effort empirical experiments. terms designing network architecture crucial part determine number neurons layer. directly arrive optimal number neurons layer thus even successful network architectures empirical numbers like experienced scientists often arrive numbers deem network enough representation power speciﬁc task. however extremely sparse matrices produced layers neural networks caught attention indicating empirically designed networks heavily oversized. simple statistics many neurons activiations matter data presented. weak neurons highly likely redundant excluded without damaging overall performance. existence increase chance overﬁtting optimization difﬁculty harmful network. motivation achieving efﬁcient network architectures ﬁnding optimal number neurons layer come iterative optimization method gradually eliminates weak neurons network pruning-retraining loop. starting empirically designed network algorithm ﬁrst identiﬁes redundant weak neurons analyzing activiations large validation dataset. weak neurons pruned others kept initialize model. finally model retrained ﬁne-tuned depending performance drop. retrained model maintain achieve higher performance smaller number neurons. process carried iteratively satisfying model produced. signiﬁcant redundancy demonstrated several deep learning models redundancy mainly caused overwhelming amount parameters deep neural networks. over-parameterized model wastes memory computation also leads serious overﬁtting problem. therefore reducing number parameters studied many researchers ﬁeld. however little work directly addressing optimization number neurons. previous works improving network architectures fall main categories; concentrates high level architectural design focuses level weight pruning. high level side researchers invented layers modules substitute main bottleneck components conventional neural networks. famous examples kind global average pooling network network invented replace extremely dense paramaterized fully connected layer inception module employed googlenet avoid explosion computational complexity later stage. methods achieve state-of-the-art results several benchmarks much less memory computation consumption. recently squeezenet used fire module together strategies achieve alexnet-level accuracy less parameters. level side different methods explored reduce number connections weights neural networks. late century methods magnitude-based approach hessian matrix based approach prune weights basing numerical properties weights loss functions without external data involved. recently proposed iterative method prune connections deep architectures together external compression quantization encoding network ﬁrst pruned removing weights connections. then learned mapping similar weights ﬁxed bits used perform quantization weights pruning facilitates huffman coding compression last stage reduce bits storage. three techniques used pipeline number parameters network reduced around methods work well practice reducing number parameters directly seek answers fundamental problem lies middle levels approaches determining optimal number neurons layer given network architecture speciﬁc tasks. along direction achieve parameter savings without need seeking network architectures also evaluate redundancy layer network thus provide guidance effective ways architecture optimization large neural networks. section describe algorithm network trimming. facilitate discussions vgg- case study. vgg- network consists convolutional layers fully connected layers. layers followed relu layer non-linear mapping. vgg- recognized representative network adopted many applications limited object classiﬁcation localization tasks. true false denotes dimension output feature denotes total number validation examples. larger number validation examples accurate measurement apoz. experiment validation imagenet classiﬁcation task measure apoz. deﬁnition apoz evaluate importance neuron network. validate observation outputs neurons large network mostly zero calculate apoz neuron neurons vgg- network apoz larger better understand behavior zero activations network compute mean apoz neurons layer vgg- network. since vgg- network inverse pyramid shape redundancy occurs higher convolutional layers fully connected layers. higher mean apoz also indicates redundancy layer. detailed distributions apoz conv- neurons neurons shown figure respectively. since neural network multiplication-addition-activation computation process neuron outputs mostly zeros little contribution output subsequent layers well ﬁnal results. thus remove neurons without harming much overall accuracy network. optimal number neurons layer thus obtain better network without redesign extensive human labor. network trimming method consists three main steps illustrated figure first network trained conventional process number neurons layer empirically. next network large validation dataset obtain apoz neuron. neurons high apoz pruned according certain criteria. connections neuron removed accordingly neuron pruned neuron pruning trimmed network initialized using weights trimming. trimmed network exhibits level performance drop. thus ﬁnal step retrain network strengthen remaining neurons enhance performance trimmed network. weight initialization necessary network obtain performance trimming. trimmed network trained scratch contains larger percentage zero activation neurons counterpart weight initialization. means retrained network without weight initialization much less efﬁcient. experimented different ways prune neurons according apoz measurements. found pruning many neurons severely damaged performance performance drops unrecoverable. therefore chose iterative scheme trim network. however trivial trim network deep architecture. many layers trimmed step performance would drop large margin hard recover original performance trimming retraining. example trimming conv conv vgg- network concurrently would lead top- accuracy image classiﬁcation task original accuracy vgg- hand conv- trimmed trimmed network weight initialization retraining achieve top- accuracy. retraining trimmed network achieves accuracy even higher original accuracy trimming. empirically found starting trim layers high mean apoz progressively trim neighboring layers rapidly reduce number neurons maintaining performance original network. decide neurons prune empirically found pruning neurons whose apoz larger standard derivation mean apoz target trimming layer would produce good retraining results. using threshold would reject neurons average trimmed layers assuming apoz values roughly follow gaussian distribution. implemented algorithm using standard caffe library. obtain weights initialization retraining python pycaffe interface copy weights remaining connections trimming. tested algorithm primarily networks lenet mnist dataset vgg- imagenet classiﬁcation dataset lenet network consists convolutional layers followed fully connected layers layers outputs respectively. short hand notion denote number neurons layer network. lenet parameters connections conv layer layer. consequently easily achieve efﬁcient network trimming size conv layers. apply algorithm iteratively prune neurons conv layers shown table ﬁrst iteration pruning numbers neurons conv layers reduced respectively achieves compression number parameters ﬁrst pruning. accuracy drops pruning retraining. retraining network achieve accuracy slightly higher original accuracy. repeat processes iterations. shown table algorithm achieves compression number parameters without loss accuracy. experiment algorithm retraining without weight initialization summarized table network exhibits deterioration classiﬁcation accuracy without weight initialization whereas proper weight initialization ancestor network previous iteration trimmed network retain original even achieve higher accuracy. moreover observe weight initialization trimmed network consistently smaller mean apoz values ancestor network. means retrained network less redundancy ancestor network. contrast mean apoz values increase retrain network scratch even though trimmed network less neurons ancestor network. observation gives insight proper weight initialization necessary achieve efﬁcient trimmed network. similar objective obtain optimal number neurons layer analyzed apoz values vgg- imagenet classiﬁcation validation set. shown table conv conv layers higher mean apoz compared bottom layers exhibiting redundancy. drawing previous experience lenet focus parameter bottleneck vgg-. trim vgg- network starting conv- layers since account parameters. iteratively prune neurons conv- layers. similar case lenet trimming process effectively eliminate neurons high apoz. shown figure trimming entire distribution apoz shifts left indicating signiﬁcant drop network redundancy. meanwhile diminishing tail right side curve manifests weak neurons vanishing proof beneﬁt gained weight initialization discussed section iterations trimming reduce half total number parameters achieve compression rate trimmed network higher top-/top- accuracy original vgg- model. detailed performance intermediate models summarized table interesting observations table. first initial accuracy trimming drop much last model even though around neurons conv- pruned iteration. strong proof redundancy empirically designed neural networks. also small decrease accuracy remedied fast ﬁne-tuning instead week-long retraining. experiments takes less iterations reach original accuracy therefore trimming method allows fast optimization towards better architecture. secondly trimmed networks surprisingly surpass original vgg- accuracy less parameters. good initialization provided previous model sets promising starting point trimmed model. addition less parameters also reduces chance overﬁtting also contribute increment accuracy. vgg- differs lenet greatly much deeper architecture signiﬁcantly layers naturally gives options determine layers trim. previous experiments want investigate trimming multiple layers simultaneously achieve effectiveness. trimming conv- layers continue trim neighboring layers. experimented three sets trimming layouts {conv {conv {conv conv neurons pruned large performance drop trimmed network indicates retraining necessary. training hyperparameters experiments {base-lr gamma step-size retraining trimmed networks gradually recover loss neurons rise accuracy level equivalent network trimming no-trim baseline conv-fc-trim- conv-fc-trim- conv-fc-trim- conv-fc-trim- conv-fc-trim- conv-fc-trim- conv-fc-trim- conv-fc-trim- conv-fc-trim- reference model slightly higher. contrast trimming layer models regain capacity rather slowly taking iterations recover accuracy. empirically found iteratively trimming network starting layers achieve better performance. also found trimming last convolutional layer fully connected layers effective. shown table additional trimming layer achieve high compression rate improved accuracy. underlying reason pruned layer numerous zeros contribute high apoz value neurons layer. goal reduce network parameters sufﬁces trim {conv- layers since around parameters {conv- layers. work closest work iteratively prune network connections correspondent weights connections close zero. also prune neuron connections neuron pruned. compared work work better major aspects. first although claim achieved reduction rate parameters vgg- reduction tailored implementation neural network. implementation convolutional layer implemented ﬁrst vectorizing feature feature vector followed matrix multiplication thus neuron pruned number multiplications convolutional layers remain since vectorization performed universal manner neurons layer. also case fully connected layers number multiplications universal neurons layer. note computational costs re-vectorize feature different shape neuron connections adding conditional mask checking higher simple matrix multiplication redundancy. method contrary removes unneeded neurons consume memory involved computation all. shown section trimmed vgg- less flops ﬁrst fully connected layer. second pruning neuron ﬁrst pruning connections less efﬁcient less effective apoz measurement. number connections signiﬁcantly larger number neurons network especially fully connected layers. experiments found redundancy resides fully connected layers connections last convolutional layer ﬁrst fully connected layer. however rarely case weights connections neuron layers close zero. consequently difﬁcult prune neuron layers. hand apoz measurement easily identify zero activation neurons pruning regardless weight connections. mean apoz also used guideline evaluate effectiveness network demonstrated experiments. experiments train network using training network validation obtain apozs neuron pruning. method controversial validation glimpsed ﬁnalizing model potentially lead overﬁtting validation set. also suspicion especially experiments trimmed model higher top- accuracy original vgg- validation set. therefore consult experiments explore potential issue. ﬁrst experiment randomly sampled subset training equal number images validation set. used criteria select weak neurons pruning. weak neurons selected using sampled training overlap ratio exact neurons selected using validation set. shows neurons consistent activation performance training validation sets. another word trimmed networks learned sampled training data similar trimmed networks learned validation set. addition also tested model test ilsvrc classiﬁcation track. using single model without dense evaluation original vgg- model validation error error rate test set. trimmed network conﬁguration {conv- compression rate validation error achieved error rate test set. note test validation non-overlapping ilsvrc classiﬁcation task. telling data network trimming overall accuracy increased validation error test error also shrunk indicating trimmed network less overﬁtting. extra experiments dismiss concern overﬁtting. also suggest validation used analyzing apozs. presented network trimming prune redundant neurons based statistics neurons’ activations. method network architecture deployed handle different tasks different datasets algorithm tailor network accordingly determining many neurons layer without need intensive computational power well human labor. method iteratively remove activation neurons provide little power ﬁnal results without damaging performance model. experimented algorithm lenet vgg- achieving accuracy less parameters. vgg- trimmed models even surpass original could caused reduced optimization difﬁculty. lying middle high level network redesign level weight pruning neuron pruning applied mature architecture together weight pruning sharply reduce complexity network.", "year": 2016}