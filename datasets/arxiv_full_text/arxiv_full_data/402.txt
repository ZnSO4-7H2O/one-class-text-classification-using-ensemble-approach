{"title": "Generative Topic Embedding: a Continuous Representation of Documents  (Extended Version with Proofs)", "tag": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "stat.ML"], "abstract": "Word embedding maps words into a low-dimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document.", "text": "word embedding maps words lowdimensional continuous embedding space exploiting local word collocation patterns small context window. hand topic modeling maps documents onto low-dimensional topic space utilizing global word collocation patterns document. types patterns complementary. paper propose generative topic embedding model combine types patterns. model topics represented embedding vectors shared across documents. probability word inﬂuenced local context topic. variational inference method yields topic embeddings well topic mixing proportions document. jointly represent document low-dimensional continuous space. document classiﬁcation tasks method performs better eight existing methods fewer features. addition illustrate example method generate coherent topics even based document. representing documents ﬁxed-length feature vectors important many document processing algorithms. traditionally documents represented bag-of-words vectors. however simple representation suffers high-dimensional highly sparse loses semantic relatedness across vector dimensions. continuous vectors low-dimensional embedding space learned embedding word encodes semantic/syntactic relatedness words utilizing local word collocation patterns. method core component embedding link function predicts word’s distribution given context words parameterized embeddings. comes documents wish method encode overall semantics. given embeddings word document imagine document bag-of-vectors. related words document point similar directions forming semantic clusters. centroid semantic cluster corresponds representative embedding cluster words referred semantic centroids. could semantic centroids number words around represent document. addition documents particular domain semantic clusters appear many documents. learning collocation patterns across documents derived semantic centroids could topical less noisy. topic models represented latent dirichlet allocation able group words topics according collocation patterns across documents. corpus large enough patterns reﬂect semantic relatedness hence topic models discover coherent topics. probability word governed latent topic modeled categorical distribution lda. typically small number topics present document small number words high probability topic. intuition motivated blei regularize topic distributions dirichlet priors. semantic centroids nature topics except former exist embedding space. similarity drives seek common semantic centroids model similar lda. extend generative word embedding model psdvec incorporating topics model named topicvec. topicvec embedding link function models word distribution topic place categorical distribution lda. advantage link function semantic relatedness already encoded cosine distance embedding space. similar regularize topic distributions dirichlet priors. variational inference algorithm derived. learning process derives topic embeddings embedding space words. topic embeddings approximate underlying semantic centroids. evaluate well topicvec represents documents performed document classiﬁcation tasks eight existing topic modeling document representation methods. setups topicvec outperformed methods tasks respectively fewer features. addition demonstrate topicvec derive coherent topics based document possible topic models. proposed generative word embedding method psdvec precursor topicvec. psdvec assumes conditional distribution word given context words factorized approximately independent log-bilinear terms. addition word embeddings regression residuals regularized gaussian priors reducing chance overﬁtting. model inference approached efﬁcient eigendecomposition blockwiseregression method topicvec differs psdvec conditional distribution word inﬂuenced context words also topic embedding vector indexed latent variable drawn dirichlet-multinomial distribution. huang proposed incorporate global semantic information help learning word embeddings. global embedding simply weighted average embeddings words document. mikolov proposed paragraph vector. assumes piece text latent paragraph vector inﬂuences distributions words text latent word. viewed special case topicvec topic number typically however document consists multiple semantic centroids limitation topic lead underﬁtting. nguyen proposed latent feature topic modeling extends incorporate word embeddings latent features. topic modeled mixture conventional categorical distribution embedding link function. coupling components makes inference difﬁcult. designed gibbs sampler model inference. implementation slow infeasible applied large corpous. proposed topical word embedding combines word embedding simple effective way. train word embeddings topic model separately corpus average embeddings words topic embedding topic. topic embedding concatenated word embedding form topical word embedding word. topical word embeddings words document averaged embedding document. method performs well classiﬁcation tasks. weaknesses include combine results word embedding lacks statistical foundations; module requires large corpus derive semantically coherent topics. proposed gaussian lda. uses pre-trained word embeddings. assumes words topic random samples multivariate gaussian distribution topic embedding mean. hence probability word belongs topic determined euclidean distance word embedding topic embedding. assumption might improper euclidean distance optimal measure semantic relatedness embeddings. throughout paper uppercase bold letters denote matrix lowercase bold letters denote vector normal uppercase letter denote scalar constant normal lowercase letter denote scalar variable. table lists notations paper. document sequence words referred text window denoted wi··· wi+l wiwi+l. text window chosen size word deﬁnes context wi−c··· wi−. referred focus word. context word wi−j focus word comprise bigram wi−j assume word document semantically similar topic embedding. topic embeddings reside n-dimensional space word embeddings. clear context topic embeddings often referred topics. document candidate topics arranged matrix form referred topic matrix. speciﬁcally referring null topic. almost modern word embedding methods adopt exponentiated cosine similarity link function hence cosine similarity assumed better estimate semantic relatedness embeddings derived methods. core word embedding methods link function connects embeddings focus word context words deﬁne distribution focus word. proposed following link function referred bigram residual indicating non-linear part captured wcvwl. essentially logarithm normalizing constant softmax term. literature e.g. refers term bias term. based assumption conditional distribution factorized approximately independent logbilinear terms corresponding context word. approximation leads efﬁcient effective word embedding algorithm psdvec follow assumption propose incorporate topic like latent word. particular addition context words corresponding embedding included log-bilinear term inﬂuences distribution hence obtain following extended link function current document logarithm normalizing constant named topic residual. note topic embeddings speciﬁc simplicity notation drop document index tzc. restrict impact topics avoid overﬁtting constrain magnitudes topic embeddings always within hyperball radius generative process words documents regarded hybrid psdvec. analogous psdvec word embedding residual asisj drawn respective gaussians. sake clarity ignore generation steps focus topic embeddings. remaining generative process follows k-th topic draw topic embedding uniformly hyperball radius i.e. unif; usually referred variational free energy lower bound directly maximizing w.r.t. intractable hidden variables maximize lower bound instead. adopt mean-ﬁeld approximation true posterior variational distribution variational algorithm maximizing following convention empirical bigram probabilities embedding magnitude penalty coefﬁcients normalizing constant word embeddings. volume hyperball radius learning objective process given hyperparameters learning objective embeddings topics word-topic document-topic distributions hyperparameters kept constant make implicit distribution notations. coupling makes inefﬁcient optimize simultaneously. around difﬁculty learn word embeddings topic embeddings separately. speciﬁcally learning process divided stages ﬁrst stage considering topics relatively small impact word distributions impact might averaged across different documents simplify model ignoring topics temporarily. model falls back original psdvec. optimal solution obtained accordingly; l·max{lil} learning rate function pre-speciﬁed document length threshold initial learning rate. magnitude approximately proportional document length avoid step size becoming long document normalize sometimes especially initial several iterations excessively step size gradient descent decrease update nonetheless general direction increasing. sharing topics across documents principle could topics across whole corpus choose different topics different subsets documents. could choose best utilize cross-document information. instance document category information available could make documents category share respective topics categories correspond sets topics. learning algorithm update needs changed cater situation k-th topic relevant document update using otherwise identiﬁability problem arise topic embeddings according document split different topic groups highly subsets. similar redundant topics learned. project documents topic space portions documents topic different documents projected onto different dimensions topic space similar documents eventually projected different topic proportion vectors. situation directly using projected topic proportion vectors could cause problems unsupervised tasks clustering. simple solution problem would compute pairwise similarities topic embeddings consider similarities computing similarity projected topic proportion vectors. similar documents still receive high similarity score. investigate quality document representation topicvec model compared performance eight topic modeling document representation methods document classiﬁcation tasks. moreover show topic coherence topicvec single document present words topics learned news article. tv+meanwv topic proportions concatenated mean word embedding document compare performance methods eight methods including three topic modeling methods three continuous document representation methods conventional bag-ofwords method. count vector unweighted. gaussianlda gaussian assumes words topic random samples multivariate gaussian distribution mean topic embedding. similar topicvec derived posterior topic proportions features document; news contains newsgroup documents evenly partitioned different categories. reuters contains documents document assigned categories. evaluation document classiﬁcation documents appearing categories removed. numbers documents categories reuters highly imbalanced selected largest categories leaving documents total. https//radimrehurek.com/gensim/models/ldamodel.html http//www.cs.cmu.edu/˜chongw/slda/ https//github.com/datquocnguyen/lftm/ https//radimrehurek.com/gensim/models/docvec.html https//github.com/largelymfs/topical word embeddings/ https//github.com/rajarshd/gaussian http//qwone.com/˜jason/newsgroups/ http//www.nltk.org/book/ch.html preprocessing steps applied methods words lowercased; stop words words word embedding vocabulary removed. experimental settings topicvec used word embeddings trained using psdvec march wikipedia snapshot. contains frequent words. dimensionality word embeddings topic embeddings hyperparameters news reuters speciﬁed topics category training respectively. ﬁrst topic category always null. learned topic embeddings combined form whole topic redundant null topics different categories removed leaving topics news topics reuters. initial learning rate iterations dataset topic embeddings obtained. posterior document-topic distributions test sets derived performing e-step given topic embeddings trained training set. slda lftm used speciﬁed topics reuters optimal topic number according larger news dataset used speciﬁed topics. hyperparameters compared methods left default values. method obtaining document representations training test sets trained regularized linear one-vs-all classiﬁer training using scikit-learn library. evaluated predictive performance test set. evaluation metrics considering largest categories dominate reuters adopted macro-averaged precision recall measures evaluation metrics avoid average results dominated performance categories. evaluation results table presents performance different methods classiﬁcation tasks. highest scores highlighted boldface. seen tv+meanwv topicvec obtained best performance tasks respectively. topic proportions features topicvec performed slightly better meanwv signiﬁcantly outperformed four methods. number features used much lower meanwv gaussianlda performed considerably inferior methods. checking generated topic embeddings manually found embeddings different topics highly similar other. hence posterior topic proportions almost uniform non-discriminative. addition datasets even fastest alias sampling took hours iteration days whole iterations. contrast method ﬁnished iterations hours. topic coherence suggests derived topic embeddings approximately semantic centroids document. capacity applications document retrieval compressed representation query document helpful. paper proposed topicvec generative model combining word embedding exploiting word collocation patterns level local context global document. experiments show topicvec learn high-quality document representations even given document. classiﬁcation tasks explored topic proportions document representation. however jointly representing document topic proportions topic embeddings would accurate. efﬁcient algorithms task proposed method potential applications various scenarios document retrieval classiﬁcation clustering summarization. thank xiuchao linmei help support. thank anonymous mentor careful proofreading. research funded national research foundation prime minister’s ofﬁce singapore futures funding initiative ircsg funding initiative administered idmpo. part work conceived shaohua visiting tsinghua. supported national china youngth top-notch talent support program.", "year": 2016}