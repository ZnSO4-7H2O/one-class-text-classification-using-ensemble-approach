{"title": "Learning recurrent dynamics in spiking networks", "tag": ["q-bio.NC", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Spiking activity of neurons engaged in learning and performing a task show complex spatiotemporal dynamics. While the output of recurrent network models can learn to perform various tasks, the possible range of recurrent dynamics that emerge after learning remains unknown. Here we show that modifying the recurrent connectivity with a recursive least squares algorithm provides sufficient flexibility for synaptic and spiking rate dynamics of spiking networks to produce a wide range of spatiotemporal activity. We apply the training method to learn arbitrary firing patterns, stabilize irregular spiking activity of a balanced network, and reproduce the heterogeneous spiking rate patterns of cortical neurons engaged in motor planning and movement. We identify sufficient conditions for successful learning, characterize two types of learning errors, and assess the network capacity. Our findings show that synaptically-coupled recurrent spiking networks possess a vast computational capability that can support the diverse activity patterns in the brain.", "text": "spiking activity neurons engaged learning performing task show complex spatiotemporal dynamics. output recurrent network models learn perform various tasks possible range recurrent dynamics emerge learning remains unknown. show modifying recurrent connectivity recursive least squares algorithm provides sufﬁcient ﬂexibility synaptic spiking rate dynamics spiking networks produce wide range spatiotemporal activity. apply training method learn arbitrary ﬁring patterns stabilize irregular spiking activity balanced network reproduce heterogeneous spiking rate patterns cortical neurons engaged motor planning movement. identify sufﬁcient conditions successful learning characterize types learning errors assess network capacity. ﬁndings show synapticallycoupled recurrent spiking networks possess vast computational capability support diverse activity patterns brain. neuronal populations exhibit diverse patterns recurrent activity highly irregular wellstructured learning performing behavioral task open question whether learninginduced synaptic rewiring sufﬁcient give rise observed wide range recurrent spiking dynamics encodes processes information. shown network recurrently connected neuron models trained perform complex motor cognitive tasks. approach synaptic connections outputs rewired generate desired population-averaged signal activity individual neurons emerges selforganized way. recurrent dynamics resulting learning schemes harnessed chaotic temporally irregular activity network rate-based neurons made repeatable either direct feedback outputs training recurrent connections directly resulting irregular stable dynamics provides rich reservoir complex motor commands extracted output neurons network linearly extending idea spiking networks poses challenge difﬁcult coordinate spiking dynamics many neurons especially spike times variable balanced network. success achieved training spiking networks directly feedback loop using rate-based network intermediate step different top-down approach build networks emit spikes optimally correct discrepancy actual desired network outputs optimal coding strategy tightly balanced network learned local plasticity rule able generate arbitrary network output although studies demonstrate network outputs perform universal computations possible repertoire recurrent activity extensively explored. various network models developed explain certain important features recurrent dynamics however still lack understanding scope recurrent dynamics spiking networks capable generating. show network spiking neurons capable supporting arbitrarily complex coarsegrained recurrent dynamics provided spatiotemporal patterns recurrent activity diverse synaptic dynamics fast number neurons network large. give theoretical basis network learn show various examples include stabilizing strong chaotic rate ﬂuctuations balanced networks constructing recurrent network reproduces spiking rate patterns large number cortical neurons involved motor planning movement. study suggests individual neurons recurrent network capability support near universal dynamics. consider network quadratic integrate-and-ﬁre neurons recurrently connected spike-activated synapses weighted connectivity matrix show results depend spiking mechanism. focus measures coarse-grained time-dependent neuron activity synaptic drive neuron given -weighted low-pass ﬁltered incoming spike trains time-averaged spiking rate neuron goal weight matrix autonomously generate desired recurrent target dynamics network spiking neurons connected stimulated brieﬂy external stimulus target dynamics deﬁned functions time interval learning recurrent connectivity considered successful evoked stimulus matches target functions time interval neurons previous studies shown recurrently connected rate units learn chaotic trajectories initial network could already generate trajectories different network sequential activity derived imaging data study expands results showing recurrent dynamics spiking networks trained repertoire recurrent dynamics encoded vast. train recurrent connectivity modiﬁed recursive least squares algorithm developed rate models minimizes quadratic cost function activity measure target together quadratic regularization term ﬁrst example trained network produce synaptic drive patterns matched sine functions spiking rate match positive part sine functions. initial connectivity matrix connection probability coupling strength drawn normal distribution mean standard deviation prior training synaptic drive ﬂuctuates irregularly soon algorithm instantiated synaptic drives follow target small error; rapid changes quickly adjust recurrent dynamics towards target result population spike trains exhibit reproducible patterns across training trials. brief stimulus precedes training session reset network speciﬁc state. training successful trained response elicited whenever stimulus applied regardless network state. training spiking rate challenging training synaptic drive small changes recurrent connectivity immediately affect spiking activity effect spikethreshold. therefore spike trains follow desired spiking rate pattern early stage training population spike trains longer appear similar across training trials also reﬂected relatively small changes recurrent connectivity substantially larger number training runs required produce desired spiking patterns however applying training total input neuron suprathreshold spiking rate trained reproduce target patterns. correlation actual ﬁltered spike trains target spiking rate increases gradually training progresses. contrast training network read-out proposed initial network needs edge chaos learn successfully recurrent connectivity learn produce desired recurrent dynamics regardless initial network dynamics connectivity. even initial network synaptic connections brief stimulus preceding training session sufﬁcient build fully functioning recurrent connectivity captures target dynamics. algorithm grow synapses tune existing ones long neurons become active initial stimulus learning limited targets; network able learn multiple sets targets. trained network follow independent sets targets target function sine function random frequency. every neuron network learned activity patterns training stimulated appropriate network recapitulated speciﬁed trained pattern recurrent dynamics regardless initial activity. synaptic drive spiking rate able learn multiple target patterns figure synaptic drive spiking rate neurons recurrent network learn complex patterns. schematic network training. blue square represents external stimulus elicits desired response. black curves represent target output neuron. arrows represent recurrent connectivity trained produce desired target patterns. synaptic drive sample neurons before training. pre-training followed multiple training trials. external stimulus applied prior training synaptic drive trained follow target training successful external stimulus elicit desired response. bottom shows spike rater neurons. pearson correlation actual synaptic drive target output training trials. bottom matrix norm changes recurrent connectivity filtered spike train neurons before normalized initial connectivity training. training. external stimulus applied immediately training trials. filtered spike train learns follow target spiking rate large errors early trials. applying stimulus successfully trained network elicits desired spiking rate patterns every neuron. measures correlation ﬁltered spike trains target outputs. bottom demonstrate spiking networks encode recurrent dynamics arbitrary spatiotemporal patterns considered targets generated various families functions; examples include complex periodic functions chaotic trajectories ornstein-uhlenbeck noise. randomly selected different target patterns families create heterogeneous targets trained synaptic drive network consisting neurons learn target dynamics. show rigorously section identiﬁed sufﬁcient conditions dynamical state spatiotemporal structure target dynamics ensure wide repertoire recurrent dynamics learned. ﬁrst quasi-static condition stipulates dynamical time figure learning multiple target patterns. synaptic drive neurons learns different target outputs. blue stimulus evokes ﬁrst target outputs green stimulus evokes second target outputs spiking rate individual neurons learns different target outputs. scale target patterns must slow enough compared synaptic time scale average spiking rate. second heterogeneity condition requires spatiotemporal structure target patterns diverse enough. quantify conditions below. target patterns considered figure slow temporal dynamics comparison synaptic time constant patterns selected randomly promote diverse structure. training neuron’s synaptic drive produce respective target pattern synaptic drive every neuron network followed target. verify quasi-static condition compared actual quasi-static approximation spiking rate synaptic drive. spiking rates neurons approximated using current-torate transfer function time-dependent synaptic input synaptic drive approximated weighted presynaptic neurons’ spiking rates. elicited trained patterns multiple trials starting random initial conditions calculate trial-averaged spiking rates. quasi-static approximations synaptic drive spiking rate closely matched actual synaptic drive trial-averaged spiking rates examine heterogeneity target patterns facilitate learning created sets target patterns fraction randomly generated targets varied systematically. non-random targets used target pattern repeatedly. networks trained learn target patterns strong heterogeneity showed network able encode target patterns high accuracy large fraction random targets networks trained many repeated target patterns failed learn. beyond certain fraction random patterns including additional patterns improve performance suggesting basis functions over-complete. probed stability over-complete networks neuron loss eliminating synaptic connections fraction neurons. network ﬁrst trained learn target outputs patterns selected randomly ensure target patterns form redundant basis functions. then elicited trained patterns removing fraction neurons network entails eliminating synaptic connections lost neurons. trained network neuron loss able generate trained patterns perfectly neuron loss resulted mild degradation network response trained patterns completely disappeared neuron loss target dynamics considered figure population spiking rates within training window. examine population activity inﬂuence learning trained networks learn target patterns whose average amplitude reduced gradually across target sets. networks able learn population spiking rate target dynamics however performance deteriorated population spiking rate decreased demonstrate learning depend spiking mechanism trained synaptic drive spiking networks using different neuron models. network leaky integrate-and-ﬁre neurons well network izhikevich neurons whose neuron parameters tuned different ﬁring patterns successfully learned complex synaptic drive patterns figure quasi-static heterogeneous patterns learned. example target patterns include complex periodic functions chaotic rate units noise target patterns overlaid actual synaptic drive trained network. quasi-static prediction synaptic drive spike trains trained neurons elicited multiple trials trial-averaged spiking rate calculated average number spikes time bins predicted spiking rate performance trained network function fraction randomly selected targets. network response trained network removing synaptic connections randomly selected neurons network. random network balanced excitation inhibition canonical model cortical circuit produces asynchronous single unit activity chaotic activity balanced rate models harnessed accomplish complex tasks including feedback loop stabilizing chaotic trajectories introducing low-rank structure connectivity matrix balanced spiking networks shown possess similar capabilities unknown possible make heterogeneous ﬂuctuations spiking rate strong coupling regime here extend work laje buonomano spiking networks show strongly ﬂuctuating single neuron activities turned dynamic attractors adjusting recurrent connectivity. take network randomly connected excitatory inhibitory neurons respects dale’s law. prior training synaptic spiking activity individual neurons show large variations across trials small discrepancies initial network state lead rapid divergence network dynamics. simulated different initial conditions synaptic drive neurons deviates strongly spiking activity single neurons uncorrelated across trials trial-averaged spiking rate little temporal structure balanced network exhibited sensitivity small perturbation; microstate identically prepared networks diverged rapidly spike deleted networks previously questioned whether chaotic nature balanced state could utilized perform reliable computations laje buonomano sought tame chaotic trajectories single neuron activities coupling strength strong enough induce large irregular spiking rate ﬂuctuations time across neurons initiate untrained network random initial conditions harvest innate synaptic activity i.e. synaptic trajectories network already knows generate. then recurrent connectivity trained synaptic drive every neuron network follows innate pattern stimulated external stimulus. respect dale’s learning rule modiﬁed update synaptic connections changes signs. training synaptic drive every neuron network able track innate trajectories response external stimulus within trained window diverge target pattern outside trained window trained network stimulated evoke target patterns trial-averaged spiking rate develops temporal structure present untrained network verify reliability learned spiking patterns simulated trained network twice identical initial conditions deleted spike evoking trained response simulations. within trained window relative deviation microstate markedly small comparison deviation observed untrained network. outside trained window however networks diverge rapidly again demonstrates training recurrent connectivity creates attracting tube around used chaotic spike sequences analyzing eigenvalue spectrum recurrent connectivity reveals distribution eigenvalues shifts towards zero spectral radius decreases result training consistent stable network dynamics found trained networks. demonstrate learning innate trajectories works well balanced network satisﬁes quasi-static condition scanned coupling strength synaptic time constant wide range evaluated accuracy quasi-static approximation untrained networks. increasing either promotes strong ﬂuctuations spiking rates hence improving quasi-static approximation learning performance correlated adherence quasi-static approximation resulting better performance strong coupling long synaptic time constants. next investigated training method applies actual spike recordings large number neurons. previous study network rate units trained match sequential activity imaged posterior parietal cortex possible mechanism short-term memory here aimed construct recurrent spiking networks capture heterogeneous spiking activity cortical neurons involved motor planning movement in-vivo spiking data obtained publicly available data recorded spike trains large number neurons anterior lateral motor cortex mice engaged planning executing directed licking multiple trials. compiled trial-average spiking rate ncor cortical neurons data trained recurrent network model reproduce spiking rate patterns ncor neurons autonomously response brief external stimulus. trained recurrent connectivity alter single neuron dynamics external inputs. first tested recurrent network size ncor able generate spiking rate patterns number cortical neurons. network model assumes spiking patterns ncor cortical neurons self-generated within recurrent network. training spiking rate neuron models captured overall trend spiking rate rapid changes pertinent short term memory motor response hypothesized discrepancy attributed sources input neurons included model recurrent input neurons local population input areas brain neuron dynamics cannot captured neuron model. thus sought improve performance adding naux auxiliary neurons recurrent network mimic spiking activity unobserved neurons figure learning innate activity balanced networks. synaptic drive sample neurons starting random initial conditions response external stimulus prior training. spike raster sample neurons evoked stimulus multiple trials random initial conditions. single spike perturbation untrained network. synaptic drive multi-trial spiking response single spike perturbation trained network. average phase deviation theta neurons single spike perturbation. left distribution eigenvalues recurrent connectivity training function absolution values. right eigenvalue spectrum recurrent connectivity; gray circle unit radius. accuracy quasi-static approximation untrained networks performance trained networks function coupling strength synaptic time constant color shows pearson correlation predicted actual synaptic drive untrained networks innate actual synaptic drive trained networks local population trained recurrent connectivity network size ncor naux auxiliary neurons trained follow spiking rate patterns obtained process provided heterogeneity overall population activity patterns. naux/ncor spiking patterns neuron models accurately cortical neurons population activity ncor cortical neurons well captured network model cortical activity improved gradually function fraction auxiliary neurons network increased heterogeneity target patterns verify cortical neurons network model simply driven feedforward inputs auxiliary neurons randomly shufﬂed fraction recurrent connections cortical neurons successful training. cortical data deteriorated fraction shufﬂed synaptic connections cortical neurons increased conﬁrmed recurrent connections cortical neurons played role generating spiking patterns figure generating in-vivo spiking activity subnetwork recurrent network. network schematic showing cortical auxiliary neuron models trained follow spiking rate patterns cortical neurons target patterns derived noise respectively. multi-trial spike sequences sample cortical auxiliary neurons successfully trained network. trial-averaged spiking rate cortical neurons neuron models auxiliary neurons included. trial-averaged spiking rate cortical auxiliary neuron models naux/ncor spiking rate cortical neurons data recurrent network model trained naux/ncor cortical dynamics improves number auxiliary neurons increases. random shufﬂing synaptic connections cortical neuron models degrades cortical data. error bars show standard deviation results trials. quantify sufﬁcient conditions target patterns need satisfy order successfully encoded network. ﬁrst condition target patterns must sufﬁciently slow compared dynamical time scale neurons synapses targets considered constant short time interval. show below overly restrictive. terms network dynamics quasi-static condition implies synaptic neuron dynamics operate stationary state even though stationary values change network activity evolves time. quasi-static state mean ﬁeld description spiking dynamics derive selfconsistent equation captures time-dependent synaptic spiking activity neurons closed terms implies training synaptic drive equivalent training rate-based network. second algorithm efﬁciently optimize recurrent connectivity thanks linearity equation synaptic drive closely follows target patterns shown fig. spiking rate also provides closed description network activity described equation however nonlinearity learns total input neuron supra-threshold i.e. gradient must positive. reason learning error cannot controlled tightly synaptic drive requires additional trials successful learning shown fig. second condition requires target patterns sufﬁciently heterogeneous time across neurons. complexity allows ensemble spiking activity rich spatiotemporal structure generate desired activity patterns every neuron within network. perspective reservoir computing every neuron recurrent network considered read-out time part reservoir collectively used produce desired patterns single neurons. heterogeneity condition equivalent complete basis functions i.e. equation equation generate target patterns i.e. left hand side equations conditions necessarily independent. heterogeneous targets also foster asynchronous spiking activity support quasi-static dynamics. note that although equations describe dynamical state learning works well merely ﬁnding satisﬁes equations guarantee spiking network recurrent connectivity produce target dynamics. recurrent connectivity needs trained iteratively network dynamics unfold time ensure target dynamics generated stable manner moreover equations explain target dynamics learned address recurrent spiking networks able encode dynamic patterns. learning errors classiﬁed categories. tracking errors arise target solution true spiking network dynamics sampling errors arise encoding continuous function ﬁnite number spikes. quantiﬁed learning errors function network target time scales. intrinsic time scale spiking network dynamics synaptic decay constant time scale target dynamics decay constant noise. used target patterns generated noise since trajectories predetermined time scale spatio-temporal patterns sufﬁciently heterogeneous. systematically varied fast ampa-like slow nmda-like synaptic transmission trained synaptic drive networks synaptic time scale learn trajectories time scale parameter scan reveals learning regime networks successfully encode target patterns error-dominant regimes. tracking error prevalent synapses slow comparison target patterns sampling error dominates synapse fast network synaptic decay time fails follow rapid changes target patterns still captures overall shape target patterns faster time scale prototypical example shows synaptic dynamics fast enough encode target dynamics tracking error regime. faster synapse synaptic figure sampling tracking errors. synaptic drive trained learn long trajectories generated noise decay time performance networks size function synaptic decay time target decay time examples trained networks whose responses show sampling error tracking error successful learning. target trajectories identical inverted u-shaped curve function synaptic decay time. error bars show s.d. trained networks size inverted u-shaped curve networks sizes network performance shown function τs/τc range range network performance shown function range range drive able learn identical target trajectories high accuracy however synapse fast synaptic drive ﬂuctuates around target trajectories high frequency typical network response sampling error regime since discrete spikes narrow width large amplitude summed sample target synaptic activity. better understand network parameters determine learning errors mathematically analyzed errors assuming target dynamics encoded quasi-static condition holds mean ﬁeld description target dynamics accurate learning errors characterized deviation assumptions actual spiking network dynamics. found tracking errors \u0001track substantial quasi-static condition valid i.e. synapses fast enough spiking networks encode targets sampling errors \u0001sample occur mean ﬁeld description becomes inaccurate i.e. discrete representation targets terms spikes deviates continuous representation terms spiking rates. errors estimated scale figure capacity function network size. performance trained networks function target length networks size target patterns generated noise decay time networks ﬁxed sizes trained range target length correlations. color shows pearson correlation target actual synaptic drive. black lines show function tmax ˜tmaxτc ˜tmax ﬁtted minimize least square error linear function maximal target length tmax successfully learned learning capacity ˜tmax shown function network size. patterns sampling error controlled either increasing stretch width individual spikes increasing encode targets input spikes. error estimates reveal versatility recurrent spiking networks encode arbitrary patterns since \u0001track reduced tuning small enough \u0001sample reduced increasing large enough. examined performance trained networks verify theoretical results explain learning errors. learning curve function inverted u-shape types errors present successful learning occurs optimal range consistent error analysis performance decreases monotonically right branch tracking error increases monotonically left branch sampling error. tracking error reduced target patterns slowed hence decrease ratio τs/τc. then learning curve becomes sigmoidal performance remains high even slow nmda regime hand sampling error reduced network size increased lifts left branch learning curve note error regimes well separated changes target time scale affect \u0001sample changes network size affect \u0001sample predicted. finally condensed training results wide range target time scales tracking error regime similarly condensed training results different network sizes sampling error regime demonstrate τs/τc explain overall performance tracking sampling error regimes respectively. shown recurrent rate network’s capability encode target patterns deteriorates function length time increase network size enhance storage capacity consistent results performance recurrent spiking networks learn complex trajectories decreases target length improves network size assess storage capacity spiking networks evaluated maximal target length encoded network function network size. necessary deﬁne target length terms effective length account fact target patterns length different effective length temporal structures; instance noise short temporal correlation times structure learned constant function. target trajectories generated process decay time rescaled target length respect deﬁned effective length /τc. capacity network maximal successfully encoded network. estimate maximal trained networks ﬁxed size learn trajectories varying then maximal target length tmax learned successfully estimate maximal ﬁnding constant ˜tmax best line tmax ˜tmaxτc training results figure shows learning capacity illustrate case simple albeit non-rigorous counting argument. successful learning achieved synaptic drive equation satisﬁed. artiﬁcially discretize time quasi-static bins consider matrix satisﬁes matrix equation treat independent matrix. operate matrix equation solution artiﬁcial system possible matrix invertible possible rank full. argument shows increasing pattern heterogeneity makes rows less correlated beneﬁcial learning learning capability decline increases steep decline ascribe quasi-static fraction pattern correlation time scale length time interval. intuitively visualize temporal storage capacity. shown synaptic drive spiking rate network spiking neurons trained follow arbitrary spatiotemporal patterns. necessary ingredients learning spike train inputs neuron weakly correlated synapses fast enough network large enough demonstrated balanced network consisting excitatory inhibitory neurons learn track strongly ﬂuctuating innate synaptic trajectories recurrent spiking network learn reproduce spiking rate patterns ensemble cortical neurons involved motor planning movement. scheme works network quickly enters quasi-static state instantaneous ﬁring rate neuron ﬁxed function inputs. learning fails synaptic time scale slow compared time scale target case quasi-static condition violated tracking error becomes large. trade-off tracking error sampling noise; fast synapse decrease tracking error also increases sampling noise. increasing network size decrease sampling noise without affecting tracking error. therefore adjusting synaptic time scale network size becomes possible learn arbitrarily complex recurrent dynamics. important structural property network model synaptic inputs summed linearly allows synaptic activity trained using recursive form linear regression linear summation synaptic inputs standard assumption many spiking network models physiological evidence linear summation prevalent training spiking rate hand take full advantage linear synapse nonlinear current-to-transfer function. network capable following wide repertoire patterns even though network dynamics highly nonlinear system effectively reduces linear system learning. moreover learning capacity estimated using simple solvability condition linear system. however nonlinear dendritic processing widely observed computational consequences requires investigation whether recurrent network nonlinear synapses trained learn arbitrary recurrent dynamics. note training scheme teach precise spike timings; either trains spiking rate directly trains synaptic drive turn trains spiking rate. spike times divergent instance instance hence learning scheme supports rate coding opposed spike coding. spike trains temporally irregular structure across neurons enhance rate coding scheme providing sufﬁcient computational complexity encode target dynamics. neurons network trained follow synaptic drive patterns long sufﬁcient heterogeneity e.g. noisy external input decorrelates spike trains neurons although results conﬁrm recurrent spiking networks capability generate wide range repertoire recurrent dynamics unlikely biological network using particular learning scheme. learning rule derived recursive least squares algorithm effective nonlocal time i.e. uses activity presynaptic neurons within train time window update synaptic weights. moreover neuron network assigned target signal synaptic connections updated fast time scale error function computed supervised manner. would interest whether biologically plausible learning schemes reward-based learning lead similar performance. study provided conditions spiking network learn wide range target dynamics. previous studies investigated mathematical relationship patterns stored ﬁxed points recurrent connectivity simple network models remains open question classify possible target patterns encoded spiking network identify class connectivity matrices generates target patterns dynamically stable manner. dimensionless variable representing membrane potential applied input total synaptic drive neuron receives neurons recurrent network neuron time constant. threshold spiking zero input. negative total input neuron rest positive input inﬁnity blow ﬁnite time initial condition. neuron considered spike whereupon reset recurrent connectivity describes synaptic coupling neuron neuron real matrix many simulations random matrix connection probability coupling strength non-zero elements modeled differently different ﬁgures described figure methods below. train synaptic spiking rate dynamics individual neurons convenient divide synaptic drive equation parts; isolates spike train single neuron computes synaptic ﬁltering synaptic drive ﬁltered spike train measures spiking activity trained study. note equations generate synaptic dynamics equivalent equation training procedure. select target trajectories length recurrent network consisting neurons. train either synaptic drive spiking rate individual neuron follow target time interval external stimulus amplitude sampled uniformly applied neuron immediately preceding training situate network speciﬁc state. training recurrent connectivity updated every using learning rule described order steer network dynamics towards target dynamics. training repeated multiple times changes recurrent connectivity stabilize. training synaptic drive. modify recurrent learning rule developed train recurrent connectivity network rate units using recursive least squares algorithm learning rule extends algorithm training recurrent connectivity spiking networks. measures mean-square error targets synaptic drive time interval plus quadratic regularization term. derive learning rule equation express function view synaptic connections neuron read-out weights determine synaptic drive apply learning rule vectors keep recurrent connectivity sparse learning occurs synaptic connections non-zero prior training reduced vector consisting elements non-zero connections neuron prior training. similarly vector ﬁltered spikes presynaptic neurons non-zero connections neuron synaptic update neuron finally obtained concatenating vectors training spiking rate. train spiking rate neurons approximate spike train neuron spiking rate current-to-rate transfer function theta neuron model. constant input figure methods figure network neurons connected randomly probability coupling strength drawn normal distribution mean standard deviation addition average non-zero synaptic connections neuron subtracted connections neuron summed coupling strength precisely zero. networks balanced excitatory inhibitory connections produced highly ﬂuctuating synaptic spiking activity neurons. synaptic decay time target functions synaptic drive sine waves sin/t) amplitude initial phase period sampled uniformly respectively. generated distinct target functions length synaptic drive patterns generated. immediately training loop every neuron stimulated constant external stimulus random amplitude sampled external stimulus used across training loops. recurrent connectivity updated every training using learning rule derived algorithm learning rate training network stimulated external stimulus evoke trained patterns. performance measured calculating average pearson correlation target functions evoked network response. figure initial network target functions generated figure using parameters target functions consisted sets sine waves. learn sets target patterns training loops alternated patterns immediately training loop every neuron stimulated constant external stimuli random amplitudes using different stimulus pattern. target pattern trained loops synaptic update every learning rate evoke target patterns training network stimulated external stimulus used train target pattern. figure network consisted neurons. initial connectivity sparsely connected connection probability coupling strength sampled normal distribution mean standard deviation considered three families target functions length complex periodic functions deﬁned product sine waves sin/t) sin/t) sampled randomly intervals respectively. chaotic rate activity generated network ranmij non-zero probability drawn gaussian distribution mean zero standard deviation ornstein-ulenbeck process obtained simulating times random initial conditions different realizations white noise satisfying decay time constant amplitude target function determined training loop repeated times. figure balanced network populations excitatory population consisted neurons inhibitory population consisted neurons ratio network size neuron received excitatory connections strength inhibitory connections strength randomly selected excitatory inhibitory neurons. connection probability sparse connectivity. relative strength inhibition excitation network inhibition dominant figure a-hthe initial coupling strength synaptic decay time adjusted large enough synaptic drive spiking rate individual neurons ﬂuctuated strongly slowly prior training. running initial network started random initial conditions seconds recorded synaptic drive neurons seconds harvest target trajectories innate balanced network. then synaptic drive trained learn innate trajectories synaptic update occurred every learning rate training loop repeated times. respect dale’s training network modify synaptic connections synaptic update reversed sign original connections either excitatory inhibitory inhibitory excitatory. moreover synaptic connections attempted change signs excluded subsequent trainings. figure initial trained connectivity matrices normalized figure coupling strength scanned increments synaptic decay time scanned increments measure accuracy quasi-static approximation untrained networks simulated network dynamics pair calculated average person correlation predicted synaptic drive actual synaptic drive. measure performance trained networks repeated training times using different initial network conﬁgurations innate trajectories calculated pearson correlation innate trajectories evoked network response trainings. heat shows best performance trainings pair figure initial connectivity sparsely connected connection probability coupling strength sampled normal distribution mean standard deviation synaptic decay time total neurons network model ncor neurons called cortical neurons trained learn spiking rate patterns cortical neurons naux neurons called auxiliary neurons trained learn trajectories generated process. used trial-averaged spiking rates neurons recorded anterior lateral motor cortex mice engaged motor planning movement lasted data available website crcns.org selected ncor neurons data whose average spiking rate behavioral task greater cortical neuron network model trained learn spiking rate pattern real cortical neurons. generate target rate functions auxiliary neurons simulated process converted spiking rate φ]+) low-pass ﬁltered decay time make smooth. auxiliary neuron trained ms-long target rate function generated random initial condition. figures networks consisting neurons initial connections synaptic decay time trained learn process decay time length figure target length ﬁxed time constants varied systematically log-scale. trainings repeated times pair average performance. figure synaptic decay time ﬁxed scanned increments scanned increments scanned increments ensure network connectivity training sparse synaptic learning occurred connections randomly selected probability prior training. recurrent connectivity updated every training learning rate training loop repeated times. average pearson correlation target functions evoked synaptic activity calculated measure network performance training.", "year": 2018}