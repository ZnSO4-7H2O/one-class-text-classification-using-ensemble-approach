{"title": "A Generalized Loop Correction Method for Approximate Inference in  Graphical Models", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Belief Propagation (BP) is one of the most popular methods for inference in probabilistic graphical models. BP is guaranteed to return the correct answer for tree structures, but can be incorrect or non-convergent for loopy graphical models. Recently, several new approximate inference algorithms based on cavity distribution have been proposed. These methods can account for the effect of loops by incorporating the dependency between BP messages. Alternatively, region-based approximations (that lead to methods such as Generalized Belief Propagation) improve upon BP by considering interactions within small clusters of variables, thus taking small loops within these clusters into account. This paper introduces an approach, Generalized Loop Correction (GLC), that benefits from both of these types of loop correction. We show how GLC relates to these two families of inference methods, then provide empirical evidence that GLC works effectively in general, and can be significantly more accurate than both correction schemes.", "text": "belief propagation popular methods inference probabilistic graphical models. guaranteed return correct answer tree structures incorrect non-convergent loopy graphical models. recently several approximate inference algorithms based cavity distribution proposed. methods account eﬀect loops incorporating dependency between messages. alternatively regionbased approximations improve upon considering interactions within small clusters variables thus taking small loops within clusters account. paper introduces approach generalized loop correction beneﬁts types loop correction. show relates families inference methods provide empirical evidence works effectively general signiﬁcantly accurate correction schemes. many real-world applications require probabilistic inference known probabilistic model paper probabilistic graphical models focusing factor graphs represent markov networks bayesian networks. basic challenge inference marginalization large number variables. discrete variables computing exact solutions appearing proceedings international conference machine learning edinburgh scotland copyright author/owner. conditional dependencies variables form tree structure exact inference tractable done message passing procedure belief propagation loopy belief propagation system applies repeatedly graph structures trees however provides approximately correct solution related bethe approximation free energy basis minimization sophisticated energy approximations provably convergent methods representative class energy approximations region-graph methods deal connected variables methods subsume cluster variation method junction graph method region-based methods deal short loops graph incorporating overlapping regions perform exact inference region. note valid region-based methods exact region graph loops. diﬀerent class algorithms loop correction methods tackles problem inference loopy graphical models considering cavity distribution variables. cavity distribution deﬁned marginal distribution markov blanket single variable removing factors depend initial variables. figure illustrates cavity distribution also shows cavity variables interact. observation methods that removing variable graphical model break loops involve variable resulting simpliﬁed problem ﬁnding example absorbing short loops overlapping regions. here region includes factors around hexagon variables. factor variables appear three regions region-based methods provide perform inference overlapping regions. cavity variables shown using dotted circles. deﬁne cavity distribution even removing factors variables still higher-order interactions caused cavity region includes variables shown pale circles. variables dotted circles perimeter removing pale factors marginalizing rest network gives cavity distribution section explains notation factor graph representation preliminaries glc. section introduces simple version works regions partition variables; followed extension general algorithm. section presents empirical results comparing approaches. subset variable indices non-negative function collection indexing subsets factors term factor interchangeably function subset term variable interchangeably value index partition function. cavity distribution. marginals around recovered considering cavity distribution interaction basis loop correction schemes montanari rizzo’s pairwise dependencies binary variables also mooij kappen’s extension general factor graphs called loop corrected belief propagation paper deﬁnes algorithm probabilistic inference called generalized loop correction uses general form cavity distribution deﬁned regions also novel message passing scheme regions uses cavity distributions correct types loops result exact inference region. glc’s combination loop corrections well motivated regionbased methods deal eﬀectively short loops graph approximate cavity distribution known produce superior results dealing long inﬂuencial loops simplest form produces update equations similar lcbp’s; indeed mild assumption reduces lcbp pairwise factors. general form provided information cavity variable interactions produces results similar region-based methods. theoretically establish relation region-based approxipractice obtain estimates \\rr) true cavity distribution \\rr). however suppose multiple cavity regions collectively cover variables xn}. intersects improve estimate enforcing marginal consistency ˆprp ˆprq variables intersection. suggests iterative correction scheme similar message passing. note provide good estimates turn improved neighboring regions; e.g. gives good approximation starting initial cavity distribution perform improvement estimate higher-order interactions here estimate partition function removing factors ﬁxing possible assignment. calculation \\rr) experiments approximation partition function provided using lbp. however alternatives clamping conditioning scheme rizzo makes possible notion cavity distribution borrowed socalled cavity methods statistical physics used analysis optimization important combinatorial problems basic idea make cavity removing variable along factors around factor graph general notion regional cavity around region. using clamping purpose also means that resulting network clamping loops exact hence produces exact results every cluster removing results tree. introduce approach ﬁrst consider simpler case cavity regions form partition variables denote intersection perimeter another cavity region rqp). partition perimeter disjoint union last line follows multiplying numerator denominator current version message mq→p. convergence mq→p equals mnew consistency constraints satisﬁed. repeating update order convergence ˆprs represent approximate marginals region. theorem cavity regions partition variables factors involve variables ﬁxed point particular construction also ﬁxed point starting uniform cavity distributions proof loops size factors identical domain. thus factors maximal applied maximal factor domains lbp. hand given condition single variable partitioning shares ﬁxed points applied immediate parent. figure shows r-region regions partition rp-region graph includes regions. denote rp-region graph denote regions; denote belief region rp-region graph. top-regions apply eﬀectively need enforce marginal consistency intersection regions parents accomplished message passing downward pass region sends child marginal child’s variables inﬂuenced beliefs sub-regions downward pass. enforces marginal consistency regions noconvergence tice also equivalent update partitioning case. returning example previous text provides method update ˆpr. performs remaining regions well iterates entire process convergence i.e. change distributions less threshold. section compares diﬀerent variations method well lcbp treeep methods performs kind loop correction. double-loop algorithm slower better convergence properties. methods applied without damping. stop method maximum iterations change probability distribution less report time seconds error method average absolute error single variable marginals lcbp used uniform initial cavity initial cavity distribution estimated clamping cavity variables. experiments full uniform refer kind cavity distribution used. denote partitioning case glc+ overlapping clusters form used. example glc+ refers setting full cavity contains overlapping loop clusters length factor appear loops forms cluster. form clusters used cvm. figure time error -regular ising models local ﬁeld interactions sampled standard normal. method graph points representing ising model diﬀerent size binary variable probability distribution setting connected graph given ij∈i jijxixj controls variable interactions deﬁnes single node potential a.k.a. local ﬁeld. general smaller local ﬁelds larger variable interactions result diﬃcult problems. sampled local ﬁelds independiﬀerent values figure shows time versus error graph size nodes larger instances converge within allocated number iterations. results cases methods converged. alarm bayesian network variables factors. variables discrete binary factors variables. table compares accuracy versus run-time different methods. factor domains regions i.e. loopy clusters produces results show consistently provides accurate results lcbp although often cost computation time. also suggest achieve tradeoﬀ time accuracy simply including larger loops regions. used uniform cavity performance similar appears stable introduced inference method provide accurate inference utilizing loop correction schemes region-based recent cavitybased methods. experimental results benchmarks support claim that diﬃcult problems schemes complementary successfully exploit both. also believe scheme motivates possible variations also deal graphical models large markov blankets. thank anonymous reviewers excellent detailed comments. research partly funded nserc alberta innovates technology futures alberta advanced education technology.", "year": 2012}