{"title": "Program Induction by Rationale Generation : Learning to Solve and  Explain Algebraic Word Problems", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.", "text": "solving algebraic word problems requires executing series arithmetic operations—a program—to obtain ﬁnal answer. however since programs arbitrarily complicated inducing directly question-answer pairs formidable challenge. make task feasible solve problems generating answer rationales sequences natural language human-readable mathematical expressions derive ﬁnal answer series small steps. although rationales explicitly specify programs provide scaffolding structure intermediate milestones. evaluate approach created -sample dataset questions answers rationales. experimental results show indirect supervision program learning answer rationales promising strategy inducing arithmetic programs. behaving intelligently often requires mathematical reasoning. shopkeepers calculate change sale prices; agriculturists calculate proper amounts fertilizers pesticides water crops; managers analyze productivity. even determining whether enough money list items requires applying addition multiplication comparison. solving tasks challenging involves recognizing goals entities quantities real-world onto mathematical formalization computing solution mapping solution back onto world. proxy richness real world series papers used natural language speciﬁcations algebraic word problems solved either learning templates solved equation solvers inferring modeling operation sequences lead ﬁnal answer paper learn solve algebraic word problems inducing modeling programs generate answer answer rationale natural language explanation interspersed algebraic expressions justifying overall solution. rationales examiners require students order demonstrate understanding problem solution; play role task. natural language rationales enhance model interpretability provide coarse guide structure arithmetic programs must executed. fact learner propose fails solve task withmodeling rationales—the search space unconstrained. work thus related models explain rationalize decisions however rationales work quite different role play prior work interpretation models trained generate plausible sounding posthoc descriptions decision making process used. work rationale generated latent variable gives rise answer—it thus faithful representation steps used computing answer. paper makes three contributions. first created dataset algebraic word problems includes answers natural language answer rationales figure illustrates three representative instances problem question trains running opposite directions cross standing platform seconds seconds respectively cross seconds. ratio speeds options rationale speeds trains m/sec m/sec respectively. then length ﬁrst train meters length second train meters. correct option problem question pack cards cards drawn together random. probability cards kings? options rationale sample space. event getting kings answer correct option problem question following values optionsa) rationale solve easiest value equals not. option l.h.s r.h.s l.h.s equal r.h.s option l.h.s r.h.s l.h.s r.h.s correct answer. answerb correct option dataset. second propose sequence sequence model generates sequence instructions that executed generates rationale; answer chosen since target program given training data third contribution thus technique inferring programs generate rationale ultimately answer. even constrained text rationale search space possible programs quite large employ heuristic search plausible next steps guide search programs empirically able show state-of-the-art sequence sequence models unable perform chance task model doubles accuracy baseline built dataset problems annotations shown figure question decomposed four parts inputs outputs description problem denote question possible answer options denoted options. goal generate description rationale used reach correct answer denoted rationale correct option label. problem illustrates example algebra problem must translated expression desired quantity solved for. problem example could solved multi-step arithmetic operations proposed finally problem describes problem solved testing options addressed past. construction ﬁrst collect seed problems consist multiple option math questions covering broad range topics difﬁculty levels. examples exams problems include gmat many websites contain example math questions exams answer supported rationale. next turned crowdsourcing generate questions. create task users presented questions seed dataset. then turker choose questions write similar question. also force answers rationale differ original question order avoid paraphrases original question. again manually check subset jobs turker quality control. type questions generated using method vary. turkers propose small changes values questions problem different equality valid question long rationale options rewritten reﬂect change). designate replica problems natural language used question rationales tend minimally unaltered. others propose problems topic generated questions tend differ radically existing ones. turkers also copy math problems available deﬁne instructions allowed generate multiple copies problem dataset turkers copy resource. turkers detected checking nearest neighbours within collected datasets problems obtained online resources frequently submitted turker. using method obtained additional questions. statistics descriptive statistics dataset shown figure total collected problems removed problems heldout replicas heldout problems present training removed manually listing heldout instance closest problems training terms character-based levenstein distance. ﬁltering problems remained training set. also show average number tokens vocabulary size questions rationales. finally provide statistics exclusively tokens numeric values tokens not. step within solution must explained. instance problem equation must solved obtain answer. previous work could done feeding equation expression solver obtain however would skip intermediate steps must also generated problem. propose model jointly learns generate text rationale perform math operations required solve problem. done generating program containing instructions generate output instructions simply generate intermediate values used following instructions. particular problem given problem options wish predict rationale correct option. sequence words problem concatenated words options separated special tag. note knowledge possible options required problems solved process elimination testing options wish generate sequence words rationale. also append correct option last word interpreted chosen option. example problem whereas problem answer sentence symbol table illustrates example sequence instructions would generate excerpt problem columns denote input sequence instruction sequence values executing instruction value written example instructions indexes simply position observed output string operation simply returns parameter without applying operation. such running operation analogous generating word sampling softmax vocabulary. however instruction reads input word applies operation float converts word ﬂoating point number done instruction reads previously generated output word unlike instructions operations write external memory stores intermediate values. sophisticated instruction—which shows power model—is choose training model generate instructions manipulate existing tokens model beneﬁts additional expressiveness needed solve math problems within generation process. total deﬁne different operations frequently used operations solving math problems. subtract multiply divide power sqrt sine cosine tangent factorial choose also provide operations convert radians degrees needed sine cosine tangent operations. operations convert ﬂoating point numbers strings vice-versa. include float float operations described previously well operations convert ﬂoating point numbers fractions since many math problems answers form reason operation convert ﬂoating point number number grouped thousands also used finally deﬁne operation given input string searches list options returns string option index input value match options option contains value canapplied. instance problem correct probability generated applying check operation number order predict generate hidden state function current program context embedding current predicted operation output either placed memory output compute probability logistic sigmoid function. output appended output otherwise appended memory generate must predict argc-length sequence arguments operation requires. argument either generated softmax vocabulary copied input vector copied previously generated values output memory decision modeled using latent predictor network control method used generate governed latent variable {softmax copy-input copy-output}. similar predicting order make choice also generate hidden state argument slot denoted lstm feeding previous argument time step initializing reading predicted value output softmax generated copy-input obtained copying element input vector pointer network input words x|x| represented encoder lstm state u|x|. such compute probability distribution input words generating executing instructions model programs consist sequences instructions turn model conditional text program speciﬁcation program’s history. instruction tuple consisting operation ordered sequence arguments decision results placed result applying operation arguments formally element pre-speciﬁed operations contains example float etc. number arguments required given argc e.g. argc argc arguments aiargc. instruction generate return value upon execution either placed output hidden. decision controlled deﬁne instruction probability analogous question answering work liang query generates correct answer must found during inference training proved difﬁcult without supervision. roth problem also addressed adding prior knowledge constrain exponential space. work leverage fact generating rationales sense progression within rationale. assume rationale solves problem step step. instance problem rationale ﬁrst describes number combinations cards deck cards describes number combinations kings ﬁnally computes probability drawing kings. thus generating ﬁnal answer without rationale requires long sequence latent instructions generating tokens rationale requires less operations. formally given sequence generated possible values given network denoted wish ﬁlter denotes possible options contain least path capable generating next token index finding achieved testing combinations instructions possible level indirection keeping generate means model generate intermediate value memory decoding. decoding likely sequence instructions given performed stack-based decoder. however important refer generated instruction must executed obtain avoid generating unexecutable code—e.g. log—each hypothesis instruction executed removed error occurs. finally generated allow instructions would generate option generated guarantees options chosen. copy-output model copies either output memory equivalent ﬁnding instruction value generated. again deﬁne pointer network points output instructions deﬁne distribution previously generated instructions state generate next state qij+. arguments generated operation executed obtain then embedding ﬁnal state instruction qi|ai| previous state used generate state next timestamp hi+. kronecker delta function execution denoted generates otherwise. thus redeﬁne marginal programs marginal programs would generate deﬁned marginalizing intractable approximate marginal generating samples model. denote samples generated embeddings given argument return value obtained lookup table embedding ﬂags indicating whether string whether ﬂoat. furthermore value ﬂoat also numeric value feature. attention copy mechanisms instruction model needs compute probability distribution attendable units conditioned previous state hi−. attention model input copy mechanisms output copy mechanism operations generally involve exponential number matrix multiplications size grows. instance computation probabilities input copy mechanism equation afﬁnity function current context given input generally implemented projecting single vector followed non-linearity projected single afﬁnity value. thus possible input matrix multiplications must performed. furthermore unrolling parameters intermediate outputs operations must replicated timestamp. thus becomes larger attention copy mechanisms quickly become memory bottleneck computation graph becomes large gpu. contrast sequence-to-sequence model proposed suffer issues timestamp dependent previous state hi−. deal this training method call staged back-propagation saves memory considering slices tokens rather full sequence. train minibatch would actually train mini-batches ﬁrst batch would optimize ﬁrst second third advantage method memory intensive operations attention copy mechanism need unrolled steps adjusted computation graph memory. however unlike truncated back-propagation language modeling context outside scope ignored sequence-to-sequence models require global context. thus sequence states still built whole sequence afterwards obtain slice hjj+k compute attention vector. finally prediction instruction conditioned lstm state used two-layer lstm hidden size word embeddings size number levels graph expanded sampling decoding performed beam vocabulary softmax embeddings keep frequent word types replace rest words unknown token. training model learns predict word unknown token alternative generate word. evaluation rationales performed average sentence level perplexity bleu model cannot generate token perplexity computation predict unknown token. beneﬁts baselines less expressive. perplexity model dependent latent program generated force decode model generate rationale maximizing probability program. analogous method used obtain sample programs described section choose likely instructions timestamp instead sampling. finally correctness answer evaluated computing percentage questions chosen option matches correct one. perplexity. terms perplexity observe regular sequence sequence model fares poorly dataset model requires generation many values tend sparse. adding input copy mechanism greatly improves perplexity allows generation process values mentioned question. output copying mechanism improves perplexity slightly input copy mechanism many values repeated ﬁrst occurrence. instance problem value used twice even though model cannot generate easily ﬁrst occurrence second simply generated copying ﬁrst one. observe model yields signiﬁcant improvements baselines demonstrating ability generate values algebraic manipulation essential task. example program inferred shown figure graph generated ﬁnding likely program generates node isolates value arrows indicate operation executed outgoing nodes arguments incoming node return operation. simplicity operations copy convert values included nodes copied/converted share color. examples tokens model obtain perplexity reduction values ﬁnally answer cannot copied input output. terestingly output copy mechanism adds improvement perplexity evaluation. decoding values copied output values could generated model either softmax input copy mechanism. such adding output copying mechanism adds little expressiveness model decoding. finally model achieve highest bleu score mechanism generate intermediate ﬁnal values rationale. accuracy. terms accuracy baseline models obtain values close chance indicating completely unable solve problem. contrast model solve problems rate signiﬁcantly higher chance demonstrating value program-driven approach ability learn generate programs. general problems solve correctly correspond simple problems solved operations. examples include questions billy cake slices ended slices altogether. many cakes solved single step. case model predicts cakes. answer rationale reasonable. show model outperform models built date generating complex rationales shown figure correctly still unsolved problem additional step adds complexity problem inference decoding. ﬁrst result showing possible solve math problems manner believe modeling approach dataset drive work problem. bleu. observe regular sequence sequence model achieves bleu score. fact high perplexities model generates short rationales frequently consist segments similar answer rationales similar statements. applying copy mechanism bleu score improves substantially model deﬁne variables used rationale. inextensive efforts made domain math problem solving obtaining correct answer given math problem. work focused learning math expressions formal languages generate natural language rationales bindings variables problem solving approach mixed approach strongly tied work sequence sequence transduction using encoder-decoder paradigm inherits ideas extensive literature semantic parsing program generation namely usage external memory application different operators values memory copying stored values output sequence. providing textual explanations classiﬁcation decisions begun receive attention part increased interest creating models whose decisions interpreted. jointly modeled classiﬁcation decision selection relevant subsection document making classiﬁcation decision. hendricks generate textual explanations visual classiﬁcation problems contrast model ﬁrst generate answer then conditional answer generate explanation. effectively creates post-hoc justiﬁcation classiﬁcation decision rather program deducing answer. papers like ours jointly modeled rationales answer predictions; however ﬁrst rationales guide program induction. work addressed problem generating rationales math problems task obtain correct answer problem also generate description method used solve problem. collect question rationale pairs propose model generate natural language perform arithmetic operations decoding process. experiments show method outperforms existing neural models ﬂuency rationales generated ability solve problem. alex graves greg wayne malcolm reynolds harley danihelka agnieszka grabskabarwiska sergio gmez colmenarejo edward grefenstette tiago ramalho john agapiou adri puigdomnech badia karl moritz hermann yori zwols georg ostrovski adam cain helen king christopher summerﬁeld phil blunsom koray kavukcuoglu demis hassabis. hybrid computing using neural network dynamic external memory. nature mohammad javad hosseini hannaneh hajishirzi oren etzioni nate kushman. learning solve arithmetic word problems verb categorization. proc. emnlp. wang ling edward grefenstette karl moritz hermann tom´as kocisk´y andrew senior fumin wang phil blunsom. latent predictor networks code generation. proc. acl.", "year": 2017}