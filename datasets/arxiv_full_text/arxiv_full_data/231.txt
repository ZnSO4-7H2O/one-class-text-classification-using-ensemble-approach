{"title": "Benchmarking Decoupled Neural Interfaces with Synthetic Gradients", "tag": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "abstract": "Artifical Neural Networks are a particular class of learning systems modeled after biological neural functions with an interesting penchant for Hebbian learning, that is \"neurons that wire together, fire together\". However, unlike their natural counterparts, artificial neural networks have a close and stringent coupling between the modules of neurons in the network. This coupling or locking imposes upon the network a strict and inflexible structure that prevent layers in the network from updating their weights until a full feed-forward and backward pass has occurred. Such a constraint though may have sufficed for a while, is now no longer feasible in the era of very-large-scale machine learning, coupled with the increased desire for parallelization of the learning process across multiple computing infrastructures. To solve this problem, synthetic gradients (SG) with decoupled neural interfaces (DNI) are introduced as a viable alternative to the backpropagation algorithm. This paper performs a speed benchmark to compare the speed and accuracy capabilities of SG-DNI as opposed to a standard neural interface using multilayer perceptron MLP. SG-DNI shows good promise, in that it not only captures the learning problem, it is also over 3-fold faster due to it asynchronous learning capabilities.", "text": "artiﬁcal neural networks particular class learning systems modeled biological neural functions interesting penchant hebbian learning \"neurons together wire together\". however unlike natural counterparts artiﬁcial neural networks close stringent coupling modules neurons network. coupling locking imposes upon network strict inﬂexible structure prevent layers network updating weights full feed-forward backward pass occurred. constraint though sufﬁced while longer feasible very-large-scale machine learning coupled increased desire parallelization learning process across multiple computing infrastructures. solve problem synthetic gradients decoupled neural interfaces introduced viable alternative backpropagation algorithm. paper performs speed benchmark compare speed accuracy capabilities sg-dni opposed standard neural interface using multilayer perceptron mlp. sg-dni shows good promise captures learning problem also -fold faster asynchronous learning capabilities. decoupled neural interfaces introduced novel optimization procedure minimize cross-entropy loss cost function weights neurons module neural network novel system breaks closed coupled/ locked-in dependency various layers neural networks introducing synthetic gradients. examine synthetic gradients take closer look locking phenomenon feed-forward/ back propagation procedures. locking occurs feed-forward back-propagation techniques activation units neurons layers network gradient loss computed. results forward backward update locking. \"forward locking\" seen feed-forward pass consequent modules redundant activations preceding modules completed. update locking preceding modules frozen state gradients consequent modules computed back-propagated. backward locking modules update activations complete feed-forward backward pass executed network. although processing scheme resulted impressive results across variety complex learning tasks tight bound system stiﬂes network operate sequentially synchronously learning process. drawback manifested time complexity training largescale neural networks. moreso designing implementing distributed learning network systems becomes waste computational time/ resources modules freeze state activations gradients loss updates received antecedent consequent layers light concept decoupled neural interface. critical algorithmic change decoupled neural paradigm synthetic gradients. backpropagation removed learning process break update locking modules. synthetic gradients parallel sub-modules attached layer module neural networks. goals synthetic gradients approximate gradient loss computed backpropagation synthetic gradients ﬁxed side-module network layer trained approximate gradient loss function. this take input activation output network layer attached uses labels target function dataset approximate loss function. applies gradient descent learning rule update weight decoupled layer. synthetic gradients trained using actual labels dataset explained previously back-propagated modules attached layers higher chain paper present performance benchmark metrics synthetic gradients multi-layer perceptrons comparison regular standard neural interface. effect section present brieﬂy highlight preliminary background information help reader comfortably grasp concept synthetic gradients decoupled neural interfaces. section present methodology used experiment setup. base parameters neural networks also known \"connectionist\" architectures constructed interconnection simple blocks neurons weight captures knowledge unit activations system interest connectionist systems intensiﬁed massively parallel computing structures simulate deep representations cerebral cortex realize automatic machine intelligence. practice escapes drawbacks heretofore symbolic processing artiﬁcial neural network design system loosely categorized three layers called input hidden output layers. layer consisted neurons neurons input layer receive information external preceptor; information parameterized weight function matrix multiplication compute activation neuron. activation forward-propagated hidden layers network acted upon activation function streamlines propagated signal. function carefully chosen prevent vanishing explosion gradients learning network hidden layers consist modules neurons. computations come together fully-connected output layer approximate transformation output target function model architecture described known feedforward neural networks multilayer perceptrons name so-called information travels input output hidden layers. sophisticated type network exists cyclic feedback loop output model input. networks called recurrent neural networks process sequences information memories sharing parameters weights across several time frames rnns output seen recursive input next state time quata dynamical system. formally express rnns hidden states dynamic continuous time recurrent network system. given ﬁnite time step unfold equation recurrent computational graph representing position recursive sequence recurrent networks designed produce outputs time step maintaining cyclic connections hidden states made keep feedback loops outputs successive dependent time-steps. also designed receive entire input maintaining cyclic loops hidden states generate single output backpropagation long hailed workhorse neural networks computing gradient loss function given weight network algorithm commonly known backprop mostly responsible renewing ﬂaming interest solving problem learning non-linear representations adjusting weights network. backpropagation shown \"computationally efﬁcient\" robust heuristics boosting performance designing network architecture. earlier noted problem vanishing exploding gradients issue using learning algorithm like stochastic gradient descent backpropagate partial derivates loss function update weights neurons preceding layer howbeit problem mitigated using activation functions like relu leaku relu resolving exploding vanishing gradient problem instead relu leaku relu functions squashing values seen sigmoid activation function relu threshold leaky relu instead small negative slope relu leaky relu linear slope decoupled neural interfaces backpropagation occur synthetic gradients leave main layers network update unlocked asynchronous learning. initial setup architecture similar form base benchmark results. base made -layered fully connected network minist dataset. hidden layer neurons batch-normalization relu non-linearity. made synthetic gradient architecture structurally identical inference network. experiments iterations optimized using adaptive moment estimation kingma learning rate optimized decreased factor steps. batch size inputs. program executed nvidia geforce tensorflow cuda gpu. table standard sg-dni algorithms iterations -layers speciﬁed methodology. result decoupling model signiﬁcant increase execution time standard multi-layer perceptrons. result anticipated individual modules layers constantly asynchronously adjusting weight parameters based synthetic gradient approximation. consequent result speed-up overall learning time modules longer locked step waiting full forward backward pass updating. however observe test accuracy lower compared standard implementation. figure shows plot accuracy loss training validation datasets using decoupled neural interfaces synthetic gradients. computational graph synthetic gradients figure shows tensorflow computational graph synthetic gradients. graph synthetic gradient module attached layer network. synthetic modules using real output dataset compute loss function error gradient. table above adjusted number iterations better observe speed performance measures lower termination condition. results observe synthetic gradients decoupled neural interfaces still performs remarkably better terms speed metrics although still lags behind standard backpropagation neural interface respect performance accuracy. results seen conclude synthetic gradients hold upper-hand comes speed execution vital train very-large-scale neural networks learning systems. interesting observation make experiments difference training time synthetic gradients dnis standard neural networks grows number iterations increases. clearly puts synthetic gradients winner speed gain asynchronous update schemes decoupling network layers. backpropagation still superior respect accuracy network making better computation gradient loss function. however apparent advantage accuracy matched synthetic gradients practice humongous datasets available training large-scale networks. also -fold increase training speed allow training even prolonged iterations shorter period. increased training time large dataset compensate slight gradient error computation synthetic gradient attempts approximation exact backpropagation gradient. exploration performance benchmarks given time resources beneﬁcial train synthetic gradients cifar- dataset compare performance standard neural networks. further another important metric carried future study synthetic gradients across parallel computing infrastructures perform distributed fashion. asynchronous parallel learning major promises decoupled neural interfaces. running select convolutional recurrent neural architectures distributed setting provide context performance synthetic gradients standard backpropagation. czarnecki swirszcz jaderberg osindero vinyals kavukcuoglu understanding synthetic gradients decoupled neural interfaces. arxiv e-prints. arxiv ken-ichi funahashi yuichi nakamura approximation dynamical systems continuous time recurrent neural networks neural networks volume issue pages issn https//doi.org/./s--x. kurt hornik maxwell stinchcombe halbert white multilayer feedforward networks universal approximators neural networks volume issue pages issn https//doi.org/./--. lecun bottou g.b. müller k.r. efﬁcient backprop. g.b. müller neural networks tricks trade. lecture notes computer science springer berlin heidelberg rochester j.h. holland; l.h. habit; w.l. duda \"tests cell assembly theory action brain using large digital computer\". transactions information theory. doi./tit.. sepp hochreiter yoshua bengio paolo frasconi jürgen schmidhuber \"gradient recurrent nets difﬁculty learning long-term dependencies\". kolen john kremer stefan field guide dynamical recurrent networks. john wiley sons. isbn ---- widrow lehr m.a.. backpropagation applications proceedings inns summer workshop neural network computing electric power industry stanford pp.- august", "year": 2017}