{"title": "Learning Compact Recurrent Neural Networks", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "Recurrent neural networks (RNNs), including long short-term memory (LSTM) RNNs, have produced state-of-the-art results on a variety of speech recognition tasks. However, these models are often too large in size for deployment on mobile devices with memory and latency constraints. In this work, we study mechanisms for learning compact RNNs and LSTMs via low-rank factorizations and parameter sharing schemes. Our goal is to investigate redundancies in recurrent architectures where compression can be admitted without losing performance. A hybrid strategy of using structured matrices in the bottom layers and shared low-rank factors on the top layers is found to be particularly effective, reducing the parameters of a standard LSTM by 75%, at a small cost of 0.3% increase in WER, on a 2,000-hr English Voice Search task.", "text": "dominant attention literature reducing size fully connected convolutional architectures given importance recurrent architectures speech community goal work explore compact architectures deep rnns lstms. several open questions immediately arise context precisely redundancy recurrent architectures speech recognition tasks? compact architectural variations retain performance? size constraints vary across layers recurrent feedforward weights different gates recurrent model? contributions paper follows ﬁrst undertake systematic study various compact architectures rnns lstms. speciﬁcally compare effectiveness low-rank models various parameter sharing schemes implemented using hashing structured matrices note sparsity promoting regularizers low-precision storage formats complementary study could yield even compact models. investigation reveals following aggressive reduction parameters bottom layers toeplitz-like structured matrices outperform hashing based schemes low-rank factorizations shared low-rank factors effective parameter reduction across network particularly effective hybrid strategy building compact lstms using toeplitz-like structured matrices bottom layers projection layers involving shared low-rank factors upper layers save parameters increase word error rate compared full lstm lstms relatively insensitive whether compression applied recurrent nonrecurrent weights similarly input/output/forget gates; hand cell state critical preserve better performance. recurrent neural networks including long short-term memory rnns produced state-of-the-art results variety speech recognition tasks. however models often large size deployment mobile devices memory latency constraints. work study mechanisms learning compact rnns lstms low-rank factorizations parameter sharing schemes. goal investigate redundancies recurrent architectures compression admitted without losing performance. hybrid strategy using structured matrices bottom layers shared low-rank factors layers found particularly effective reducing parameters standard lstm small cost increase english voice search task. recurrent neural network architectures become popular automatic speech recognition tasks past years. architectures recurrent neural networks long shortterm memory networks convolutional long-short term memory networks produced state results many large vocabulary continuous speech recognition tasks order fully exploit thousands hours training data lvcsr tasks best performing neural network architectures typically large size. consequently require long training time consume signiﬁcant number ﬂoating point operations prediction deployed. characteristics further exacerbated large deep architectures unroll network sequential number time frames must compute output time step feeding next time step. situation odds need enable high-performance on-device speech recognition storage power constrained mobile phones compact small-sized models strongly preferred. numerous approaches recently proposed model compression literature build compact neural network models fast training prediction speed. popular technique low-rank matrix factorization attempts compress neural network layers representing matrices low-rank. shown reduce parameters dnns loss accuracy lvcsr tasks. techniques include inducing zeros parameter matrices sparsity regularizers storing weights ﬁxed-precision formats using speciﬁc paramm rank output equation seen linear projection original hidden layer shared across layers. refer projection compression scheme. projection model compact weight sharing. besides since projection weight shared gradient also receives error signals factors. error component coming closer output compared makes learning recurrent connection easier projection model compared full model therefore projection model regularizes full model fewer number parameters facilitates learning weight sharing. lower dimensionality projects cell similar interpretation equation projection node feed forward next layer recurrently next time step layer gates cell activations equation refer reader detailed equations regarding lstm projection model. hashednets scheme recently proposed reduce memory consumption layers computer vision tasks. here assume matrix unique parameter values instead connections randomly grouped together hashed bucket length-k parameter vector predeﬁned hashing function. shared among entries feedforward back-propagation recently proposed family parameter sharing schemes small-footprint deep learning based structured matrices characterized notion displacement operators unlike hashednets weights randomly grouped parameter sharing mechanisms structured matrices highly speciﬁc deterministic. structure exploited fast matrix-vector multiplication also gradient computations during back-propagation typically using fast fourier transform like operations. ﬂavor approach consider toeplitz matrices parameters tied along diagonals. known toeplitz matrices admit time compute matrix-vector products essentially equivalent performing linear convolutions. toeplitz matrices also property certain shift scale operations implemented speciﬁc displacement operators linearly transformed matrices rank less equal thus called displacement rank toeplitz matrices propose learning parameter matrices generalizations toeplitz structure allowing displacement rank higher. class lstm lstm alternative model used address vanishing/exploding gradient issues model longer-term temporal dependencies time step layer lstm sequence sequence mapping includes forget gate peephole connection described follows factorization full-rank matrix size full-rank matrix size thus replace matrix matrices equivalent replacing fully connected layer linear bottleneck layer. notice non-linearity matrices low-rank decomposition equation recurrent feedforward matrices layer shown equation reduce number parameters system long number parameters less low-rank matrix factorization ﬁrst explored speech found models could reduced loss accuracy. furthermore context dnns explored computing singular-value-decomposition full matrix learn smaller low-rank matrices. compared different methods introduced section focus bottom layers heavily condensed. speciﬁc compress weight matrices ﬁrst layers rank rank hashednets toeplitzlike matrices deﬁnition rank shown table note limited compared original dimensionality. million parameters baseline model roughly original size. table toeplitz-like transform efﬁcient compress bottom layers attains lowest similar number parameters. given ﬁxed budget model size different compression schemes make different assumptions compressing. rank assumption performs poorest rank constrained. hashednets imposes somewhat weaker structure parameters random grouping also performs moderately. hand toeplitz-like structured matrix rank interpreted composition convolutions deconvolutions performs best reduce parameters bottom layers. also benchmarked low-rank model shared factors described here projection nodes nodes around parameters model achieves frame accuracy word error rate matching full model one-third number parameters. projection model shares weights thus gradients across layers appears effective aggressive compression bottom layers alone using hashednets toeplitz-like matrices untied low-rank models. hypothesis perhaps toeplitz-like compression convolution interpretation better lower layers shared low-rank factorizations effective higher layers. experiment found difﬁcult rnns since experiments increasing number layers exacerbated vanishing/exploding gradient problem. hence reserve experiments next section compress deep lstm layers exhibited stable optimization behavior setting. next explore behavior toeplitz-like matrix changing displacement rank table improves compress higher ranks cost increasing parameters training time. column seconds asgd optimization step training time proportional displacement rank toeplitz-like matrix. rank gives reasonable matrices called toeplitz-like include products inverses toeplitz matrices linear combinations interpreted composition convolutions deconvolutions. matrices parameterized products circulant skew-circulant matrices. displacement rank serves knob modeling capacity. high displacement rank matrices increasingly unstructured. show mobile speech recognition problems transforms highly effective learning compact toeplitz-like layers compared fully connected dnns. summary table gives summary different methods number parameters function appropriate notion rank. simplicity assume projection number parameters averaged across layers low-rank factors shared. report sets experiments rnns medium-sized noisy training thousand english-spoken utterances lstms larger training million utterances data sets created artiﬁcially corrupting clean utterances using room simulator adding varying degrees noise reverberation overall noise sources youtube daily life noisy environmental recordings. training sets anonymized hand-transcribed representative google’s voice search trafﬁc. training sets randomly split model training heldout used evaluate frame accuracy. reported noisy test containing utterances input feature models -dimensional log-mel ﬁlterbank features computed every recurrent layers initialized uniform random weights rnns lstms unrolled time steps training truncated backpropagation time addition output state label delayed frames similar neural networks trained cross-entropy criterion using asynchronous stochastic gradient descent optimization strategy described networks phone output targets output layer would parameters could focus attention compressing layers network. exponentially decaying learning rate starts decay rate billion frames. apply gradient clipping training cell clipping lstm training. light observations experiments mainly focus lstm projection layer toeplitzlike transforms bottom layers compact lstm experiments. full lstm model hidden layer hidden unit layer. projection model introduce projection equation hidden layers numbers column compression table indicate dimensions respectively. toeplitz-like transform start projection model nodes nodes replace weight matrices toeplitz-like matrices progressively. column compression stands four weights recurrent connections layer compressed. reduce gates cell state equally. note compress time projection layer removed. column projection details projection layers model. take lstm projection model projection nodes projection nodes gates cell states toeplitz-like matrices. vary compression cell state gates toeplitz-like transforms record change wer. models models last rows table huge jump table gates column indicates input gate forget gate cell state output gate compressed. compressing cell state makes major difference lstm performance. compressing forget gate alone show much impact increase wer. reducing input gate output gate forget gate altogether fewer parameters would make worse wer. notice removing forget gate would signiﬁcantly hurt performance ﬁnds output gate least importance. identify signiﬁcant difference compressing different gates experiment probably change gates third layer input weights ﬁve-layer lstm rank toeplitzlike matrices sufﬁcient retain enough information gates. projection model toeplitz-like transform performance drops reduce number parameters. however given ﬁxed model size effective compress lower layers toeplitz-like matrices displacement rank convolutional interpretation compared projection compression layers moderate rank. quite different behavior compression shallow since afford deeper network lstm overall combination toeplitz-like transform projection able compress lstm model parameters increase next answer question lstm apply compression. compare effect compressing recurrent weight non-recurrent weight table compression lists toeplitz-like matrices gates compressed equally projection speciﬁes projection layers model. makes signiﬁcant difference whether compress recurrent non-recurrent weight long number parameter matches. work studied build compact recurrent neural networks lvcsr tasks. experiments noted toeplitz-like structured matrices outperform hashednets low-rank bottleneck layers aggressive parameter reduction bottom layers. lstm parameter reduction architecting upper layers projection nodes moderate rank bottom layers toeplitz-like transforms found particularly effective strategy. strategy able build compact model fewer parameters standard lstm model incurring increase wer. compressing recurrent non-recurrent weight make signiﬁcant difference. lstm performance sensitive cell state compression making noticeable change wer.", "year": 2016}