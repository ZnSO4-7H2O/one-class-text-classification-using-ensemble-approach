{"title": "Network Dissection: Quantifying Interpretability of Deep Visual  Representations", "tag": ["cs.CV", "cs.AI", "I.2.10"], "abstract": "We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.", "text": "propose general framework called network dissection quantifying interpretability latent representations cnns evaluating alignment individual hidden units semantic concepts. given model proposed method draws broad data visual concepts score semantics hidden units intermediate convolutional layer. units semantics given labels across range objects parts scenes textures materials colors. proposed method test hypothesis interpretability units equivalent random linear combinations units apply method compare latent representations various networks trained solve different supervised self-supervised training tasks. analyze effect training iterations compare networks trained different initializations examine impact network depth width measure effect dropout batch normalization interpretability deep visual representations. demonstrate proposed method shed light characteristics models training methods beyond measurements discriminative power. observations hidden units large deep neural networks revealed human-interpretable concepts sometimes emerge individual latent variables within networks example object detector units emerge within networks trained recognize places part detectors emerge object classiﬁers object detectors emerge generative video networks internal structure appeared situations networks constrained decompose problems interpretable way. emergence interpretable structure suggests deep networks learning disentangled representations spontaneously. commonly understood network learn efﬁcient encoding makes economical hidden variables distinguish states appearance disentangled representation well-understood. disentangled representation aligns variables meaningful factorization underlying problem structure encouraging disentangled representations signiﬁcant area research internal representation deep network partly disentangled possible path understanding mechanisms detect disentangled structure simply read separated factors. examine issues propose general analytic framework network dissection interpreting deep visual representations quantifying interpretability. using broden broadly densely labeled data framework identiﬁes hidden units’ semantics given aligns human-interpretable concepts. evaluate method various cnns trained object scene recognition show emergent interpretability axis-aligned property representation destroyed rotation without affecting discriminative power. examine interpretability affected training data sets training techniques like dropout batch normalization supervision different primary tasks. growing number techniques developed understand internal representations convolutional neural networks visualization. behavior visualized sampling image patches maximize activation hidden units using variants backpropagation identify generate salient image features discriminative power hidden layers features also understood isolating portions networks transferring limiting them testing capabilities specialized problems visualizations digest mechanisms network images must interpreted; motivates work aims match representations cnns labeled interpretations directly automatically. relevant current work explorations roles individual units inside neural networks. human evaluation used determine individual units behave object detectors network trained classify scenes. automatically generated prototypical images individual units learning feature inversion mapping; contrasts approach automatically assigning concept labels. recently suggested approach testing intermediate layers training simple linear probes analyzes information dynamics among layers effect ﬁnal prediction. quantify clarity idea? notion disentangled representation rests human perception means concept mixed therefore quantify interpretability deﬁne terms alignment human-interpretable concepts. measurement interpretability deep visual representations proceeds three steps three-step process network dissection reminiscent procedures used neuroscientists understand similar representation questions biological neurons since purpose measure level representation disentangled focus quantifying correspondence single latent variable visual concept. fully interpretable local coding one-hotencoding variable match exactly humaninterpretable concept. although expect network learn partially nonlocal representations interior layers past experience shows emergent concept often align combination several hidden units present assess well representation disentangled. therefore measure alignment single units single interpretable concepts. gauge discriminative power representation; rather quantiﬁes disentangled interpretability. show sec. possible representations perfectly equivalent discriminative power different levels interpretability. assess interpretability given draw concepts broadly densely labeled image data uniﬁes labeled visual concepts heterogeneous collection labeled data sources described sec. measure alignment hidden unit concept evaluating feature activation individual unit segmentation model concept. quantify interpretability layer whole count number distinct visual concepts aligned unit layer detailed sec. broadly densely labeled dataset uniﬁes several densely labeled image data sets opensurfaces pascal-context pascal-part describable textures dataset data sets contain examples broad range objects scenes object parts textures materials variety contexts. examples segmented pixel level except textures scenes given full-images. addition every image pixel data annotated eleven common color names according human perceptions classiﬁed weijer sample types labels broden dataset shown fig. purpose broden provide ground truth exemplars broad visual concepts. concept labels broden normalized merged original data sets every class corresponds english word. labels merged based shared synonyms disregarding positional distinctions ‘left’ ‘top’ data task random imagenet places places hybrid. imagenet places places. imagenet places places hybrid. imagenet places. context puzzle egomotion tracking moving videoorder audio crosschannelcolorization. objectcentric. avoiding blacklist overly general synonyms multiple broden labels apply pixel example black pixel pascal-part label ‘left front leg’ three labels broden uniﬁed ‘cat’ label representing cats across data sets; similar uniﬁed ‘leg’ label; color label ‘black’. labels least image samples included. table shows average number image samples label class. scoring unit interpretability proposed network dissection method evaluates every individual convolutional unit solution binary segmentation task every visual concept broden method applied using forward pass without need training backpropagation. every input image broden dataset activation every internal convolutional unit collected. distribution individual unit activations computed. unit quantile level determined every spatial location activation data set. compare low-resolution unit’s activation input-resolution annotation mask concept activation scaled mask resolution using bilinear interpolation anchoring interpolants center unit’s receptive ﬁeld. thresholded binary segmentation selecting regions activation exceeds threshold segmentations evaluated every concept data computing intersections every pair. cardinality set. data contains types labels present subsets inputs sums computed subset images least labeled concept category value ioukc accuracy unit detecting concept consider unit detector concept ioukc exceeds threshold. qualitative results insensitive threshold different thresholds denote different numbers units concept detectors across networks relative orderings remain stable. comparisons report detector ioukc note unit might detector multiple concepts; purpose analysis choose ranked label. quantify interpretability layer count number unique concepts aligned units. call number unique detectors. evaluating quality segmentation unit objective conﬁdence score interpretability comparable across networks. thus score enables compare interpretability different representations lays basis experiments below. note network dissection works well underlying data unit matches human-understandable concept absent broden score well interpretability. future versions broden expanded include kinds visual concepts. testing prepare collection models different network architectures supervision primary tasks listed table network architectures include alexnet googlenet resnet supervised training models trained scratch imagenet places places imagenet object-centric data contains million images classes. places places subsets places database scene-centric data categories kitchen living room coast. places contains million images scene categories places contains million images scene categories. hybrid refers combination imagenet places. self-supervised training tasks select several recent models trained predicting context solving puzzles predicting ego-motion learning moving predicting video frame order tracking detecting object-centric alignment colorizing images predicting cross-channel predicting ambient sound frames self-supervised models analyze comparable alexnet figure illustration network dissection measuring semantic alignment units given cnn. unit last convolutional layer given probed evaluating performance segmentation tasks. method probe convolutional layer. following experiments begin validating method using human evaluation. then random unitary rotations learned representation test whether interpretability cnns axis-independent property; conclude interpretability inevitable result discriminative power representation. next analyze convolutional layers alexnet trained imagenet trained places conﬁrm method reveals detectors higher-level concepts higher layers lower-level concepts lower layers; detectors higher-level concepts emerge scene training. then show different network architectures alexnet resnet yield different interpretability differently supervised training tasks self-supervised training tasks also yield variety levels interpretability. finally show impact different training conditions examine relationship discriminative power interpretability investigate possible improve interpretability cnns increasing width. human evaluation interpretations evaluate quality unit interpretations found method using amazon mechanical turk raters shown images highlighted patches showing highly-activating regions unit alexnet trained places asked decide whether given phrase describes image patches. table summarizes results. first determined interpretable units units raters agreed ground-truth interpretations units report portion interpretations generated method rated descriptive. within also compare portion ground-truth labels found descriptive second group raters. proposed method semantic labels units comparable descriptions written human annotators highest layer. lowest layer low-level color texture concepts available broden sufﬁcient table human evaluation network dissection approach. interpretable units raters agreed ground-truth interpretations. within report portion interpretations assigned method rated descriptive. human consistency based second evaluation ground-truth labels. match good interpretations minority units. human consistency also highest conv suggests humans better recognizing agreeing upon highlevel visual concepts objects parts rather shapes textures emerge lower layers. conduct experiment determine whether meaningful assign interpretable concept individual unit. possible hypotheses explain emergence interpretability individual hidden layer units hypothesis interpretable units emerge interpretable concepts appear directions representation space. representation localizes related concepts axis-independent projecting direction could reveal interpretable concept interpretations single units natural basis meaningful understand representation. hypothesis interpretable alignments unusual interpretable units emerge learning converges special basis aligns explanatory factors individual units. model natural basis represents meaningful decomposition learned network. hypothesis default assumption past found respect interpretability there distinction individual high level units random linear combinations high level units. using network dissection analyze compare interpretability units within convolutional layers places-alexnet imagenet-alexnet. places-alexnet trained scene classiﬁcation places imagenet-alexnet identical architecture trained object classiﬁcation imagenet results summarized fig. sample units shown together automatically inferred interpretations manually assigned interpretations taken predicted labels match human annotation well though sometimes capture different description visual concept ‘crosswalk’ predicted algorithm compared ‘horizontal lines’ given human third unit conv placesalexnet fig. conﬁrming intuition color texture concepts dominate lower layers conv conv object part detectors emerge conv. network architectures supervisions different network architectures training supervisions affect disentangled interpretability learned representations? apply network dissection evaluate range network architectures supervisions. simplicity following experiments focus last convolutional layer semantic detectors emerge most. results showing number unique detectors emerge various network architectures trained imagenet places plotted fig. examples shown fig. terms network architecture interpretability resnet googlenet alexnet. deeper architectures appear allow greater interpretability. comparing training data sets places imagenet. discussed scene composed multiple objects beneﬁcial object detectors emerge cnns trained recognize scenes. results networks trained various supervised self-supervised tasks shown fig. network architecture alexnet model observe training places creates largest number unique detectors. self-supervised models create many texture detectors relatively object detectors; apparently supervision self-taught primary task much weaker inferring interpretable concepts supervised training large annotated data set. form self-supervision makes difference example colorization model trained colorless images almost color detection units emerge. hypothesize emergent units represent concepts required solve primary task. fig. shows typical visual detectors identiﬁed self-supervised models. models audio puzzle object part detectors emerge. detectors useful cnns solve primary tasks figure interpretability changes basis representation alexnet conv trained places. vertical axis shows number unique interpretable concepts match unit representation. horizontal axis shows quantiﬁes degree rotation. learned alexnet. hypothesis overall level interpretability affected change basis even rotations cause speciﬁc represented concepts change. hypothesis overall level interpretability expected drop change basis. begin representation convolutional units alexnet conv trained places examine effect change basis. avoid issues conditioning degeneracy change basis using random orthogonal transformation rotation drawn uniformly applying gramschmidt normally-distributed positive-diagonal right-triangular described interpretability summarized number unique visual concepts aligned units deﬁned sec. also test smaller perturbations basis using fractional powers chosen form minimal geodesic gradually rotating intermediate rotations computed using schur decomposition. fig. shows interpretability decreases larger rotations applied. rotated representation exactly discriminative power original layer. writing original network note deﬁnes neural network processes rotated representation exactly original operates conclude interpretability neither inevitable result discriminative power prerequisite discriminative power. instead interpretability different quality must measured separately understood. figure comparison interpretability convolutional layers alexnet trained classiﬁcation tasks places imagenet right three examples units layer shown identiﬁed semantics. segmentation generated unit shown three broden images highest activation. top-scoring labels shown left human-annotated labels shown right. disagreement seen dominant judgment meaning. example human annotators mark ﬁrst conv unit places ‘windows’ detector algorithm matches ‘chequered’ texture. figure comparison several visual concept detectors identiﬁed network dissection resnet googlenet vgg. network trained places. highest-iou matches among convolutional units network shown. segmentation generated unit shown four maximally activating broden images. units activate concept generalizations e.g. googlenet unit horses dogs white ellipsoids jets. audio model trained associate objects sound source useful recognize people cars; puzzle model trained align different parts objects scenes image. colorization tracking recognizing textures might good enough solve primary tasks colorizing desaturated natural image; thus unsurprising texture detectors dominate. training conditions number training iterations dropout batch normalization random initialization known affect representation learning neural networks. analyze effect training conditions interpretability take placesalexnet baseline model prepare several variants using alexnet architecture. varifigure ranked concepts three categories four self-supervised networks. object part detectors emerge audio. detectors person heads also appear puzzle colorization. variety texture concepts dominate models self-supervised training. ants repeat repeat repeat randomly initialize weights train number iterations. variant nodropout remove dropout layers baseline model. variant batchnorm apply batch normalization convolutional layers baseline model. repeat repeat repeat nearly top- accuracy validation set. variant without dropout top- accuracy variant batch norm top- accuracy fig. plot interpretability snapshots baseline model different training iterations. object detectors part detectors begin emerging iterations evidence transitions across different concept categories training. example units conv turn texture material detectors becoming object part detectors. fig. shows interpretability units cnns different training conditions. several effects comparing different random initializations models converge similar levels interpretability terms unique detector number total detector number; matches observations convergent learning discussed network without dropout texture detectors emerge fewer object detectors. batch normalization seems decrease interpretability signiﬁcantly. batch normalization result serves caution discriminative power property representation measured. intuition loss interpretability batch normalization batch normalization ‘whitens’ activation layer smooths scaling issues allows network easily rotate axes intermediate representations training. whitening apparently speeds training also effect similar random rotations analyzed sec. destroy interpretability. discussed sec. however interpretability neither prerequisite obstacle discriminative power. finding ways capture beneﬁts batch normalization without destroying interpretability important area future work. figure number unique object detectors last convolutional layer compared representations classiﬁcation accuracy action data set. supervised unsupervised representations clearly form clusters. generalization ability benchmark deep features several networks trained several standard image classiﬁcation data sets discrimination ability task. trained model extract representation highest convolutional layer train linear training data action action recognition task compute classiﬁcation accuracy averaged across classes test split. fig. plots number unique object detectors representation compared representation’s classiﬁcation accuracy action test set. positive correlation them. thus supervision tasks encourage emergence concept detectors also improve discrimination ability deep features. interestingly best discriminative representation action representation resnetimagenet fewer unique object detectors compared resnet-places. hypothesize accuracy representation applied task dependent number concept detectors representation suitability represented concepts transfer task. alexnet resnet cnns visual recognition grown deeper quest higher classiﬁcation accuracy. depth shown important high discrimination ability seen sec. interpretability increase depth well. however width layers less explored. reason increasing number convolutional units layer signiﬁcantly increases computational cost yielding marginal improvements classiﬁcation accuracy. nevertheless recent work shows carefully designed wide residual network achieve classiﬁcation accuracy superior commonly used thin deep counterparts. figure comparison standard alexnet alexnetgap-wide number unique detectors number detectors widening layer brings emergence detectors. networks trained places. affects emergence interpretable detectors remove layers alexnet triple number units conv i.e. units units. finally global average pooling layer conv fully connect pooled -feature activations ﬁnal class prediction. call model alexnet-gap-wide. training places alexnet-gap-wide obtains similar classiﬁcation accuracy validation standard alexnet many emergent concept detectors terms number unique detectors number detector units conv shown fig. also increased number units conv number unique concepts signiﬁcantly increase further. indicate limit capacity alexnet separate explanatory factors; indicate limit number disentangled concepts helpful solve primary task scene classiﬁcation. paper proposed general framework network dissection quantifying interpretability cnns. applied network dissection measure whether interpretability axis-independent phenomenon found not. consistent hypothesis interpretable units indicate partially disentangled representation. applied network dissection investigate effects interpretability state-of-the training techniques. conﬁrmed representations different layers disentangle different categories meaning; different training techniques signiﬁcant effect interpretability representation learned hidden units. acknowledgements. work partly supported national science foundation grant a.t.; vannevar bush faculty fellowship program sponsored basic research ofﬁce assistant secretary defense research engineering funded ofﬁce naval research grant n--- a.o.; data initiative csail toyota research institute csail joint research center google amazon awards hardware donation nvidia corporation. b.z. supported facebook fellowship. simonyan vedaldi zisserman. deep inside convolutional networks visualising image classiﬁcation models saliency maps. international conference learning representations workshop zhou lapedriza xiao torralba oliva. learning deep features scene recognition using places database. advances neural information processing systems agrawal girshick malik. analyzing performance multilayer neural networks object recognition. proc. eccv alain bengio. understanding intermediate layers using nguyen dosovitskiy yosinski brox clune. synthesizing preferred inputs neurons neural networks deep generator networks. advances neural information processing systems russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge. int’l journal computer vision", "year": 2017}