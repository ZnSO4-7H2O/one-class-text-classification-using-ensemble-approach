{"title": "Discrete Sequential Prediction of Continuous Actions for Deep RL", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "It has long been assumed that high dimensional continuous control problems cannot be solved effectively by discretizing individual dimensions of the action space due to the exponentially large number of bins over which policies would have to be learned. In this paper, we draw inspiration from the recent success of sequence-to-sequence models for structured prediction problems to develop policies over discretized spaces. Central to this method is the realization that complex functions over high dimensional spaces can be modeled by neural networks that use next step prediction. Specifically, we show how Q-values and policies over continuous spaces can be modeled using a next step prediction model over discretized dimensions. With this parameterization, it is possible to both leverage the compositional structure of action spaces during learning, as well as compute maxima over action spaces (approximately). On a simple example task we demonstrate empirically that our method can perform global search, which effectively gets around the local optimization issues that plague DDPG and NAF. We apply the technique to off-policy (Q-learning) methods and show that our method can achieve the state-of-the-art for off-policy methods on several continuous control tasks.", "text": "long assumed high dimensional continuous control problems cannot solved effectively discretizing individual dimensions action space exponentially large number bins policies would learned. paper draw inspiration recent success sequenceto-sequence models structured prediction problems develop policies discretized spaces. central method realization complex functions high dimensional spaces modeled neural networks next step prediction. speciﬁcally show q-values policies continuous spaces modeled using next step prediction model discretized dimensions. parameterization possible leverage compositional structure action spaces learning well compute maxima action spaces simple example task demonstrate empirically method perform global search effectively gets around local optimization issues plague ddpg naf. apply technique off-policy methods show method achieve state-of-the-art off-policy methods several continuous control tasks. reinforcement learning long considered general framework applicable broad range problems. reinforcement learning algorithms categorized various ways. important distinction however arises discrete continuous action spaces. discrete domains several algorithms q-learning leverage backups bellman equations dynamic programming solve problems effectively. strategies deep neural networks learn policies value functions achieve superhuman accuracy several games actions discrete domains. success spurred development techniques deep neural networks continuous control problems gains domains however outsized discrete action domains superhuman performance seems unachievable current techniques. disparity part result inherent difﬁculty maximizing arbitrary function continuous domain even dimensional settings; further techniques beam searches exist apply directly. makes difﬁcult perform inference learning settings. additionally becomes harder apply dynamic programming methods back value function estimates successor states parent states. several recent continuous control reinforcement learning approaches attempt borrow characteristics discrete problems proposing models allow maximization backups easily continuous control avail advantages discretize dimensions continuous control action spaces. noted naively however would create exponentially large discrete space actions. example dimensions discretized bins problem would balloon discrete space possible actions. leverage recent success sequence-to-sequence type models train discretized models without falling trap requiring exponentially large number actions. method relies technique ﬁrst introduced allows escape curse dimensionality high dimensional spaces modeling complicated probability distributions using chain rule decomposition. paper similarly parameterize functions interest q-values using decomposition joint function sequence conditional approximations. families models exhibit sequential prediction properties also referred autoregressive. formulation able achieve ﬁne-grained discretization individual domains without explosion number parameters; time model arbitrarily complex distributions maintaining ability perform maximization. strategy applied function approximation settings focus off-policy settings inspired algorithm q-learning. complementary results actor-critic model based autoregressive concept presented appendix empirical results illustrative multimodal problem demonstrates model able perform global maximization avoiding exploration problems faced algorithms like ddpg also show effectiveness method solving range benchmark continuous control problems including hopper humanoid. paper introduce idea building continuous control algorithms utilizing sequential autoregressive models predict action spaces dimension time. here discrete distributions dimension apply off-policy learning. explore instantiation model body work discuss three additional variants appendix appendix appendix step agent takes action receives reward environment transitions stochastically state according dynamics episode consists sequence steps last time step. episode terminates stopping criterion true γi−ri discounted reward received agent starting step episode. standard reinforcement learning goal agent learn policy maximizes expected total reward would receive environment following policy. paper focused off-policy learning q-learning provide brief description algorithm. figure demonstration transformation environment three dimensional action space. fictitious states introduced keep action dimension transition dimensional. circle represents state mdp. transformed environment’s replicated states augmented previously selected action. three action dimensions chosen underlying environment progresses st+. traditionally represented table state action pairs linear function approximators shallow neural networks recently effort apply techniques complex domains using non-linear function approximators deep neural networks models deep q-network parameterized parameters used predict q-values i.e. parameters trained performing gradient descent error equation without taking gradient q-values successor states approach takes account). since greedy policy uses action value maximum q-value essential parametric form able maxima easily respect actions. output layer predicts q-values discrete outputs easy simply action corresponding index output highest estimated q-value. continuous action problems tricky formulate parametric form q-value easy maxima. techniques like normalized advantage function constrain function approximator easy compute analytically cost reduced ﬂexibility. another approach followed deep deterministic policy gradients algorithm uses actor-critic algorithm off-policy setting train deterministic policy model trains critic estimate q-function separate policy approximate critic. work develop similar technique modify form q-value function still retaining ability local maxima actions greedy policy. propose sequential model structures function form sequential model action dimensions. introduce idea ﬁrst show transformation done environment splits action sequence actions. shift modiﬁcation environment transformation construction function resulting ﬁnal technique. consider environment dimensional action space. perform transformation environment similar environment inserting ﬁctitious states. ﬁctitious state action space. action choice conditioned previous state well previously selected actions. action selected environment dynamics take effect using previous actions perform step. process depicted figure transformation reduces actions series actions. discretize output space directly apply q-learning. note could apply strategy continuous values without discretization choosing conditional distribution mixture gaussians maxima easily found. such approach equally applicable pure continuous domains compared discrete approximations. downside transformation increases amount steps needed solve mdp. practice transformation makes learning function considerably harder. extra steps dynamic programming coupled learned function approximators causes large overestimation stability issues. possible actions values \"partial q-value function\" estimate using q-learning deﬁned recursively input \"partial q-value functions\" correspond q-values occurring ﬁctitious states introduced previous section. parameterize neural network receives input. neural network outputs dimensional vector number bins discretized action space into. parameterization allows arbitrarily complex resolution cost increasing number bins found good tradeoff performance tractability. model similar q-learning transformed environment described previous section. before entire chain trained q-learning. practice however found values signiﬁcantly overestimated increased time dependencies operator q-learning. training approximation error value predictions. approximation error operator q-learning causes overestimation. overcome issue introduce second function inspired double produce ﬁnal estimate q-value action dimensions. note traditional double work past version function used. setting train entirely separate network original untransformed environment. this learning becomes easier less long term dependencies. several components must train model. first minimizes bellman error sample transitions replay buffer minimize three losses using training step ﬁlled transition sampled using current policy policy action selection illustrated figure training depicted figure another motivation structuring losses decouple model learns values model chooses actions policy. approach similar ddpg. value learned network policy approximate learned another figure pictorial view policy discretization bins. state network gets embedded distribution ﬁrst action dimension predicted. selected converted continuous action. action state predict distribution second action dimension figure pictorial view training procedure. numerous exploration strategies explored reinforcement learning computing actions sequential policies additionally inject noise choice select remaining actions based choice. found yields better performance compared epsilon greedy. implemented learned function model deep neural network. explored parameterizations sequential components qis. first looked recurrent lstm model model shared weights passes information hidden activations action another. input time step function current state action second looked version shared weights passing information sequential prediction model. models feed forward neural networks take input concatenation previous action selections well state. complex domains vision based control tasks example would make sense untie subset weights. practice found untied version performed better. optimizing model families still ongoing work. full detail model architectures training procedures selection appendix work motivated distinct desires learn policies exponentially large discrete action spaces approximate value functions high dimensional continuous action spaces effectively. paper used sequential parameterization policies help achieve without making assumption actual functional form model. prior work attempts handle high dimensional action spaces assuming speciﬁc decompositions. example able scale learning extremely large action sizes factoring action value function product experts learn policies. alternative strategy proposed using action embeddings applying k-nearest neighbors reduce scaling action sizes. laying actions hypercube able perform binary search actions resulting logarithmic search optimal action. method similar sdqn construct q-value q-values. approach presupposes constraints however optimizes bellman equation optimizing hyperplanes independently thus enabling optimizing linear programming. approach iterative reﬁnes action selection contrasts independent sub-plane maximization. along development discrete space algorithms researchers innovated specialized solutions learn continuous state action environments including recently novel deep approaches developed continuous state action problems. trpo uses stocastic policy parameterized diagonal covariance gaussian distributions. relies quadratic advantage function enabling closed form optimization optimal action. methods structure network convex actions non-convex respect states linear policy context reinforcement learning sequential autoregressive policies previously used describe exponentially large action spaces space neural architectures sequences words approaches rely policy gradient methods whereas explore off-policy methods. hierarchical/options based methods including perform spatial abstraction perform temporal abstraction pose another factor action spaces. methods reﬁne action selection time approaches operates timescale factors action space. vast literature constructing sequential models solve tasks exists outside models natural data generated sequential process language modeling ﬁrst effective deep learned sequence-to-sequence models language modeling proposed used encoder-decoder architecture. domains techniques nade developed compute tractable likelihood. techniques like pixel used great success image domain clear generation sequence. hierarchical softmax performs hierarchical decomposition based wordnet semantic information. second motivation work enable learning ﬂexible possibly multimodal policy landscape. existing methods stochastic neural networks construct energy models sampled stein variational gradient descent work instead sampling construct secondary network evaluate max. consider effectiveness algorithm consider deterministic environment single time step action space. thought two-armed bandit problem deterministic rewards search problem action space. chose reward function multimodal distribution shown ﬁrst column figure large suboptimal mode smaller optimal mode exist. bandit problems formulation helps isolate ability method optimal policy without confounding effect arises backing rewards bellman operator sequential problems. look behavior sdqn well ddpg task. traditional exploration learning. consider uniformly sampling well sampling data normal distribution centered current policy refer \"local.\" visualization ﬁnal surfaces well training curves found figure first considered performance ddpg. ddpg uses local optimization learn policy constantly changing estimate values predicted critic. form distribution ﬂexible closed form properties make learning policy. such resort gradient descent local optimization algorithm. local nature possible algorithm stuck sub-optimal policies local maximum current critic. hypothesize local maximum policy space exist realistic simulated environments well. traditionally deep learning methods local optimizers avoid local minima maxima working high dimensional parameter space however action space policy relatively small dimensionally thus much likely exist. example hopper environment common failure mode experienced training algorithms like ddpg learn balance instead moving forward hopping. next consider behaves environment. makes assumption function quadratic action space. training quadratic surface minimize expected loss evaluated transitions replay buffer. given restricted functional form model longer possible model entire space without error. such distribution sampled points used learning matters greatly. behavior policy uniform distribution action space quadratic approximation yields surface figure left final reward/q surface algorithm tested. final policy marked sdqn model capable performing global search thus ﬁnds global maximum. contains data collected uniformly action space. sdqn ddpg accurately reconstruct target surface. algorithms like however fail even converge local maximum. bottom actions sampled normal distribution centered policy. results sample efﬁciency yields poor approximations surface outside policy right smoothed reward achieved time. ddpg quickly converges local maximum. sdqn high variance performance initially searches space quickly converges global maximum surface estimate becomes accurate. sampling uniformly fails converge global maximum. location actually lower global maximum. maximum reward region path improve. interestingly though longer case behavior policy stochastic gaussian policy whose mean greedy policy w.r.t. previous estimate values. setting models quadratic surface around samples taken i.e. locally around maxima estimated values. non-stationary optimization results behavior quite similar ddpg performs local optimization. experiment suggests balance global quadratic model exploiting local structure ﬁtting function. finally consider sdqn. expected model capable completely representing surface suffer inﬂexibility previous methods. optimization policy done locally uniform behavior policy stochastic gaussian behavior policy converge optimal solution. much like ddpg loss surface learned stationary need shift time learn optimal solution. unlike ddpg however policy stuck local maximum. uniform behavior policy setting model slowly reaches right solution many wasted samples. behavior policy closer on-policy slowness reduced. much error occurs selecting estimated actions. sampling policy estimated data points sampled frequently converging optimal solution. unlike global algorithms like performance decrease increase sampling noise cover action space. allows balance exploration learning function easily. evaluate relative performance models perform series experiments common continuous control tasks. test hopper swimmer half cheetah walkerd humanoid environments openai suite performed wide hyper parameter search various parameters models selected best performing runs. random seeds hyper parameters evaluate consistency realistic estimate performance. this assumes models enough capacity. limited capacity setting would still want explore locally. much like sdqn models shift capacity modeling spaces sampled thus making better capacity. technical reasons simulations different numerical simulation strategy provided mujoco practice though found differences ﬁnal reward within expected variability rerunning algorithm different random seed. figure learning curves highest performing hyper parameters trained mujoco tasks. show smoothed median percentiles range random seeds run. sdqn quickly achieves good performance tasks. figure maximum reward achieved training averaged step window evaluations every steps. results averaged randomly initialized trials ﬁxed hyper parameters. sdqn models perform competitively compared ddpg. believe replication necessary many algorithms sensitive hyper parameters random seeds. quality would like algorithms replications able detect phenomena. first look learning curves environments tested figure method quickly achieves good policies much faster ddpg. next qualitative analysis best reward achieved training averaged across steps evaluations sampled every steps. perform average different random seeds. metric gives much better sense stability traditionally reported instantaneous reward achieved training. compare algorithm current state-of-the-art off-policy continuous control ddpg. careful model selection extensive hyper parameter tuning train models performance better previously published ddpg tasks. despite search however believe still space signiﬁcant performance gain models given different neural network architectures hyper parameters. results seen figure algorithm achieves better performance four environments tested. conceptually approach centers idea action selection stage factored sequentially selected using autoregressive formulation. work action spaces discretized. existing work image modeling domain suggests using mixture logistic units greatly speeds training would also satisfy need closed form max. additionally work imposes prespeciﬁed ordering actions negatively impact training certain classes problems. address this could learn factor action space sequential order continuous action spaces learn group action sets discrete action spaces. another promising direction combine approximate action gradient based optimization procedure. would relieve complexity modeling task maxing network cost increased compute sampling policy. finally work presented exclusively off-policy methods. autoregressive policy discretized actions could also used policy stochastic policy optimization algorithm trpo work present continuous control algorithm utilize discretized action spaces sequential models. technique propose off-policy algorithm utilizes sequential prediction discretization. decompose model function auxiliary network acts policy responsible computing approximate actions. effectiveness method demonstrated illustrative benchmark tasks well complex continuous control tasks. additional formulations discretized sequential prediction models presented appendix appendix would like thank nicolas heess insight exploration assistance scaling task complexity oscar ramirez assistance running experiments. would like thank eric jang sergey levine mohammad norouzi leslie phillips chase roberts vincent vanhoucke comments feedback. finally would like thank entire brain team support.", "year": 2017}