{"title": "Deep Exploration via Randomized Value Functions", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.", "text": "study randomized value functions guide deep exploration reinforcement learning. oﬀers elegant means synthesizing statistically computationally eﬃcient exploration common practical approaches value function learning. present several reinforcement learning algorithms leverage randomized value functions demonstrate eﬃcacy computational studies. also prove regret bound establishes statistical eﬃciency tabular representation. reinforcement learning holds promise provide basis artiﬁcial intelligence manage wide range systems devices better serve society’s needs. date potential primarily assessed learning simulated systems data generation relatively unconstrained algorithms routinely trained tens millions trillions episodes. real systems data collection costly constrained physical context call focus statistical eﬃciency. driver lies agent explores environment. design reinforcement learning algorithms eﬃciently explore intractably large state spaces remains important challenge. though substantial body work addresses eﬃcient exploration focusses tabular representations number parameters learned quantity data required scale number states. despite valuable insights generated design analysis tabular reinforcement learning algorithms limited practical import because curse dimensionality state spaces contexts practical interest enormous. need algorithms generalize across states exploring intelligently learn make eﬀective decisions within reasonable time frame. reinforcement learning. common value function learning approaches agent maintains point estimate function mapping state-action pairs expected cumulative future reward. estimate typically takes parameterized form linear combination features neural network parameters past observations. estimate approximates agent’s prevailing expectation true value function used guide action selection. actions applied observations gathered parameters adapted growing data set. hope process quickly converges mode agent selects near optimal actions observations reinforce prevailing value estimates. using value function estimate guide actions agent could operate according greedy policy given state applies action maximizes estimated value. however policy poorly understood actions assigned unattractive point estimates. forgo enormous potential value; worthwhile experiment action since action could optimal learning provide cumulating future beneﬁt subsequent visits state. thoughtful exploration critical eﬀective learning. available actions. dithering induces experimentation required learn actions unattractive point estimates. however approaches waste much exploratory effort write-oﬀ actions known inferior. exploratory actions selected without regard level uncertainty associated value estimates. clearly worth experimenting action expected undesirable suﬃcient uncertainty surrounding assessment. discuss section ineﬃciency result learning times grow exponentially number states. sophisticated approach might experiment action applying action reveal useful information. refer approaches myopic since account subsequent learning opportunities made possible taking action. though myopic approaches write actions dithering approaches fail discuss section myopic exploration also require learning times grow exponentially number states even entirely fail learn. reliably eﬃcient reinforcement learning calls deep exploration. mean exploration method consider immediate information gain also consequences action future learning. deep exploration method could example choose incur losses sequence actions expecting informative observations multiple time periods. dithering myopic approaches exhibit strategic pursuit information. paper develop approach deep exploration. idea apply actions greedy respect randomly drawn statistically plausible value function. roughly speaking sample proxy posterior distribution value functions. randomized value functions incentivize experimentation actions highly uncertain value since uncertainty translates variance sampled value estimate. randomness often generates positive bias therefore induces exploration. much said design algorithms leverage randomized value functions cover ground section worth mentioning here though concept abstract broadly applicable transcending speciﬁc algorithms. randomized value functions synthesized multitude useful algorithmic ideas reinforcement learning literature produce custom approaches speciﬁc contexts. provide insight eﬃcacy randomized value functions section establish strong bound bayesian regret tabular algorithm. ﬁrst result establish strong eﬃciency guarantees tabular reinforcement learning. however previous algorithms shown satisfy similar regret bounds extend contexts involving generalization parameterized value functions. regard approach present ﬁrst satisfy strong regret bound tabular representations also working eﬀectively wide variety practical value function learning methods generalize states actions. section present computational results generated several reinforcement learning algorithms randomized value functions. results simple example illustrate dramatic eﬃciency gains relative dithering approaches synthesis randomization generalization parameterized value functions. bayes optimal policy serves gold standard exploration reinforcement learning. particular beginning prior distribution markov decision processes formulate problem maximizing expected reward prescribed time frame taking action future time contingent prevailing posterior distribution. policy attaining maximum must explore judiciously. unfortunately problems practical interest computing bayes optimal policy intractable. literature heuristics approximate bayes optimal policies survey). randomized value function approaches introduce paper viewed contributing literature practical technique operates eﬀectively together value function learning methods commonly used address large scale reinforcement learning problems comes provable eﬃciency guarantees tabular settings. substantial body work provably eﬃcient exploration tabular reinforcement learning. begins seminal work kearns singh identiﬁed necessity multi-period exploration strategies adopt term deep exploration polynomial-time learning established polynomial-time learning guarantee particular tabular algorithm. subsequent papers proposed analyzed alternative tabular algorithms carry deep exploration varying degrees efﬁcacy important implication literature popular schemes \u0001-greedy boltzmann exploration require learning times grow exponentially number states and/or planning horizon discuss phenomenon section despite valuable insights generated design analysis tabular algorithms algorithms limited practical import because curse dimensionality state spaces typically enormous. practical reinforcement learning algorithms must generalize across states learn make eﬀective decisions limited data literature oﬀers rich collection algorithms references therein). though algorithms genre achieved impressive outcomes notably games backgammon atari arcade games naive exploration schemes highly ineﬃcient. possibly reason applications required enormous quantities data. case example neural networks trained hundreds billions trillions simulated games. design reinforcement learning algorithms eﬃciently explore intractably large state spaces remains important challenge. work model learning algorithms apply speciﬁc model classes become statistically computationally intractable problems practical scale. policy learning algorithms identify high-performers among policies. lines work produced several interesting results particularly space possible optimal policies small sense. however existing works either entails overly restrictive assumptions make strong eﬃciency guarantees. value function learning potential overcome computational challenges oﬀer practical means synthesizing eﬃcient exploration eﬀective generalization. relevant line work establishes eﬃcient reinforcement learning value function generalization reduces eﬃcient knows knows online regression however known whether kwik online regression problem solved eﬃciently. terms concrete algorithms optimistic constraint propagation provably eﬃcient reinforcement learning algorithm exploration value function generalization deterministic systems c-pace provably eﬃcient reinforcement learning algorithm generalizes using interpolative representations. contributions represent important developments suitable stochastic systems highly sensitive model misspeciﬁcation generalizing eﬀectively high-dimensional state spaces calls methods extrapolate. paper leverage randomized value functions explore eﬃciently generalizing parameterized value functions. prior reinforcement learning algorithms generalize manner require worst case learning times exponential number model parameters and/or planning horizon. algorithms propose refer collectively randomized least-squares value iteration overcome ineﬃciencies. operate manner similar well-known approaches least-squares value iteration sarsa fundamentally distinguishes rlsvi exploration randomly sampling statistically plausible value functions whereas alternatives lsvi sarsa typically applied conjunction action-dithering schemes boltzmann \u0001-greedy exploration lead highly ineﬃcient learning. paper aims establish randomized value functions promising approach tackling critical challenge reinforcement learning synthesizing eﬃcient exploration eﬀective generalization. work know involving exploration random sampling value functions proposes tabular algorithm. preliminary version part work also reported short paper lower bound expected regret conditioned true markov decision process learning algorithm. diﬀers upper bound factor though comparison meaningful since lower bound maximum markov decision processes hold expectation markov decision processes taken respect distribution posit. recent thread work builds count-based exploration schemes operate value function learning methods maintain density state-action space pseudo-counts represent quantity data gathered relevant state-action pair. algorithms oﬀer viable approach deep exploration generalization. however potential drawbacks. separate representation required generalize counts clear design eﬀective approach this. opposed optimal value function ﬁxed environment counts generated agent’s choices single target function learn. second count model generates reward bonuses distort data used value function value function representation needs designed capture properties true optimal value function also distorted versions. finally approaches treat uncertainties uncoupled across state-action pairs incur substantial negative impact statistical eﬃciency discussed said range problems count-based schemes prove eﬀective promising computational results reported worth noting approach inspired thompson sampling particular generating randomized value function approximately sample posterior distribution optimal value function. problems thompson sampling sense near-optimal further theory suggests well-designed upper-conﬁdence-bound-based approaches appropriately couple uncertainties across state-action pairs often computationally intractable similarly near-optimal competitive thompson sampling contexts hand problems complex information structures possible explore much eﬃciently thompson sampling such expect reinforcement learning problems value function representations randomized value function approaches forth well well-designed upper-conﬁdence-bound-based approaches leave substantial room improvement. process identiﬁed tuple here ﬁnite state space ﬁnite action space reward model transition model initial state distribution. probability episode begins state time period episode. state transition generated according termination time period observing state observing reward process terminates sequence observations made episode a∈arsas′ distribution real numbers andpsa sub-distribution states. particularpsa conditional probability state transitions state action similarlyrsas′ conditional probability diﬀerence −∑s′∈spsa represents probability process terminates upon given state-action pair reward denote random time transition iso=\u0001s deﬁne policy mapping froms probability distribution overa denote policies denote probability assigns integer indices such deﬁne substochastic matrix whoseth element is∑a∈a πpsa. make following assumption policies action sampled mdpm almost surely terminates ﬁnite time. words limt→∞ matrix whoseth element is∑a∈a πpsa. mdpm policy deﬁne value function πm∶s\u0015 prior beginning episode algorithm produces policy based state action spaces historyhl−=∶ obepisode agent enjoys cumulative reward of∑τ algorithm alg. written emalg\u0002v∗ design algorithms minimizing worst-case regret set. tends yield algorithms behave overly conservative manner faced representative mdps. alternative minimizing average representative mdps. distribution mdps thought prior captures beliefs algorithm designer. spirit deﬁne bayesian regret mulative reward however reasonable algorithms regret~l bayesregret~l converge zero. feasible apply oprandomness realized probability space. notation probability takes values written probability measurable evente written reinforcement learning calls sophisticated form exploration refer deep exploration. form exploration accounts information gained upon taking action also action position agent eﬀectively acquire information subsequent time periods. following simple example illustrate critical role deep exploration well common approaches exploration fall short front. consider mdpm= with\u0000s\u0000= states thought square cell grid illustrated figure action space isa={ indexing possibly diﬀering across states. words pair distinct states s′∈s action could represent left state right state transition action transitions cell immediately left possible below. analogously right action transitions cell immediately right possible below. agent begins every episode upper-left-most state note that given dynamics described episode lasts exactly time periods. cell along diagonal cost incurred time right right action selected cell. conditioned them reward deterministic episodes whether wants reach avoid cell. particular given knowledge action chosen. cost incurred left action. situation leads additional reward cost arises agent lower-right-most cell chest. additional reward cost optimal policy select right action every time period treasure otherwise choose left action every time period. accumulates reward treasure bomb. interesting note policy randomly explores selecting action equal probability highly unlikely reach chest. particular probability policy reaches cell given episode hence expected number episodes observing chest’s content even moderate value quintillion episodes. discuss agent’s beliefs state knowledge mdpm prior ﬁrst episode. agent knows everything aboutm except action associations. state agent know action index associated right left assigns equal probability either association. associations independent across states. deterministic agent transitions state learns action associations state agent selects right action lower-right-most state learns whether treasure bomb. note reinforcement learning problem presented example easy address. particular straightforward show minimal expected time learn optimal policy achieved agent chooses right action whenever knows action otherwise applies random action discovers content chest point knows optimal policy. algorithm identiﬁes optimal policy within episodes since episode agent learns move right least additional cell along diagonal. further expected learning specialized example extend reinforcement learning problems. purposes example serve sanity check context illustrating ﬂaws features algorithms designed general reinforcement learning problem. optimal state-action value function deﬁned q∗m= represent reward transition following application action state second q∶s×a\u0015 greedy policy respect selects action note greedy policy respect optimal mdpm. policy depends random mdpm therefore applied process learning. exploration. pure-exploitation algorithm maximizes expectation ˆql= ﬁrst reinforcement learning algorithm consider pure-exploitation algorithm aims maximize expected reward current episode ignoring beneﬁts active applying episode greedy policy respect ˆql. algorithm applicable reinforcement learning problem behavior example reveals severe ineﬃciencies. particular algorithm indiﬀerent ﬁnding chest since expected reward associated further since moving toward chest incurs cost algorithm avoids that therefore never visits chest. such algorithm unlikely ever learn optimal policy. dithering approaches explore selecting actions randomly perturb pureexploitation algorithm would example form dithering known boltzmann exploration selects actions according here represents temperature parameter. approaches zero actions become would selected pure-exploitation algorithm. increases selection becomes noisier eventually converging uniform distribution actions. example dithering algorithm biased moving toward chest associated cost. random perturbations lead agent chest. such well known dithering highly ineﬃcient even bandit learning. shortcoming dithering algorithms write-oﬀ actions. particular even observations make clear particular action worthwhile dithering approaches sample action. despite understanding dithering widely used exploration method reinforcement learning. primary reason lack computationally eﬃcient approaches adequately address complex reinforcement learning problems arise practical contexts. paper aims need. bandit learning thought special case reinforcement learning actions bear delayed consequences. bandit learning literature oﬀers sophisticated methods overcome shortcomings dithering. methods write-oﬀ actions selecting action expected generate desirable reward yield useful information both. naive applying algorithm reinforcement learning observed reward and/or transition expected provide useful information. however agent applying approach refer myopic exploration problem example would avoid moving toward chest learns action associations initial state. cost moving right action associations state learned immediate beneﬁt applying right action. such myopic exploration unlikely ever learn optimal policy. myopic exploration adequately address reinforcement learning because reinforcement learning additional motivation overlooked action desirable even expected yield value immediate information action place agent state leads subsequent learning opportunities. essence deep exploration; agent needs consider actions inﬂuence downstream learning opportunities. viewed another considering explore agent probe deep decision tree. optimism serves another guiding principle much bandit learning literature provide basis deep exploration well. example agent takes optimistic plausible view would assume chest oﬀers treasure rather bomb long hypothesis invalidated. episode agent follows greedy policy respect value function assigns state-action pair maximal expected value assumption. cell along diagonal grid policy selects right action whenever agent knows hence optimistic algorithm learns optimal policy within episodes. optimistic algorithm attains strong performance example carrying deep exploration. particular assuming treasure rather bomb agent incentivized move right whenever since obtain posited treasure. exploration strategy deep since agent seek immediate information also learning opportunity arise consecutively moving right multiple time periods. reasonably eﬀective optimistic algorithms apply reinforcement learning problems small state action spaces. however design algorithms adequately address reinforcement learning problems practical scale computationally tractable manner remains challenge. alternative approach studied bandit learning literature involves randomly sampled instead optimistic estimates. focus paper extend approach known thompson sampling accommodate deep exploration complex reinforcement learning problems. applied example randomized approach would sample conditioned observations made previous episodes approximation posterior distribution. agent’s ﬁrst visit chest assigns equal probability treasure bomb therefore sample equal chance optimistic pessimistic. agent selects actions according greedy policy respect therefore average explores half episodes manner similar optimistic algorithm. such randomized algorithm expect learn optimal policy within episodes. applied example beneﬁt using randomized rather optimistic approach. however face complex reinforcement learning problems randomized approach lead computationally tractable algorithms carry deep exploration optimistic approach not. achieved agent moves right whenever knows episodes. pure-exploitation algorithm avoids active exploration requires episodes approaches carry deep exploration such still require episodes. optimistic randomized approaches require episodes. ﬁeld reinforcement learning produced substantial body algorithmic ideas serve ingredients match customize tailoring solutions speciﬁc applications. ideas well-summarized textbooks bertsekas tsitsiklis sutton barto among others. paper contribute corpus approach exploration based randomized value functions intention additional ingredient broadly enable computationally eﬃcient deep exploration. much literature notable applications build value function learning. involves ﬁtting parameterized value function observed data order estimate optimal value function. algorithms present genre. starting point section describe least-squares value iteration perhaps simplest value function learning algorithms. section consider modifying lsvi injecting randomness manner incentivizes deep exploration. gives rise class algorithms refer randomized least-squares value iteration oﬀer computationally tractable means deep exploration. lsvi plays foundational role sense popular value function learning algorithms interpreted variations designed improve computational eﬃciency robustness mis-speciﬁcation parameterized value function. reinforcement learning literature presents many ideas address practical considerations. section discuss ideas brought bear tandem rlsvi. state-action value functions indexed identiﬁes state-action value function ˜qθ∶s×a\u0015 simple example family consider representing value functions linear combinations ﬁxed feature vectors. particular vector features designed capture salient characteristics state-action pair natural consider family functions taking form ˜qθ= live provides template reinforcement learning algorithms consider. operates endless sequence episodes accumulating observations learning value functions applying actions. addition index parametrized family value functions takes arguments three algorithms. cache algorithm maintains buﬀer observations. learn algorithm produces value function index simplest version generates greedy actions expressed greedy greedy passed learn selected action maximizes estimated state-action values. multiple actions attain maximum sampled uniformly among them. basic version cache given cache infinite simply accumulates observation. next three sections present algorithms esgiven mdpm apply value iteration algorithm compute arbitrarily close approximation algorithm takesm planning horizon input computes function current state action. computation recursive given taking expected immediate reward algorithm computes converges geometric rate. hence anym satisfying assumption weighted-maximum-norm contraction mapping such suﬃciently large greedy policy respect input output realization learn lsvi takes input data buﬀer used value function. also taken input regularization parameters interpreted prior mean variance expectation degree uncertainty interpreted noise variance degree value estimates based individual transitions diﬀer expectations. reﬂecting optimal expected rewards expanding horizon. however value iteration computes optimal values using full knowledge lsvi produces estimates based observed data. iteration observed transition similarly learn lsvi computes sequence value functions learn lsvi regresses immediate reward value estimate current maxa′∈a ˜q˜θh function mappings×a easy that positive observed history grows include increasing number transitions stateaction pair value functions q˜θh however practical contexts data ﬁnite parameterization chosen less ﬂexible order enable generalization. such q˜θh addition inducing generalization less ﬂexible parameterization critical computational tractability. particular compute time memory requirements value iteration scale linearly number states which curse dimensionality grows intractably large practical contexts. lsvi sidesteps scaling instead requiring compute time memory scale polynomially dimension previous value function parameters regularization parameter regularization parameter regularization parameter planning horizon updated value function parameters simple reinforcement learning system. system work reasonably problems require active exploration. however lack exploration typically hinders agent discovering high-value policies. boltzmann greedy epsilon greedy induce dithering exploration randomly perturbing greedy actions. takes additional parameter input control intensity randomness boltzmann greedy takes temperature parameter epsilon greedy discussed section randomly perturbing greedy actions dithering achieve deep exploration. section consider randomized value function estimates alternative. high level idea randomly sample among statistically plausible parameter vectors. approach inspired thompson sampling algorithm widely used bandit learning. context multi-armed bandit problem thompson sampling maintains belief distribution models assign mean rewards arms. observations accumulate belief distribution evolves according bayes rule. selecting algorithm samples model belief distribution selects model assigns largest mean reward. address reinforcement learning problem could principle apply thompson sampling value function learning. would involve maintaining belief distribution candidates optimal value function. episode would sample function distribution apply associated greedy policy course episode. approach could eﬀective practically viable distributions value functions complex represent exact bayesian inference would likely prove computationally intractable. randomized least-squares value iteration modeled thompson sampling approach serves computationally tractable method sampling value functions. rlsvi explicitly maintain update belief distributions optimally synthesize information coherent bayesian method would. regardless later establish computational mathematical analyses rlsvi achieve deep exploration. ﬁrst consider version rlsvi induces exploration injecting gaussian noise calculations form carried lsvi. understand role noise ﬁrst consider conventional linear regression problem. suppose wish estimate suppose ˜qθ= φ∶s×a\u0015 extracts vector features observation equation target values maxa′∈a ˜q˜θh buffer. line learn grlsvi similar uses target regularizes toward rather origin. such maxa′∈a ˜q˜θh ˜q˜θh conditioned q˜θh+ discuss simple example involving four states actionsa={up down}. leth list transitions observed partition sublistshsa=∈h∶=) containing transitions distinct state-action pair. suppose that\u0000h\u0000= state-action pair \u0000hsa\u0000 virtually inﬁnite. hence highly uncertain expected immediate rewards transition probabilities infer previous value function parameters regularization parameter regularization parameter regularization parameter planning horizon updated value function parameters null sample prior buffer noise← cache infinite for∈ buffer sample noise buffer noise← cache infinite) θh+← argmin θ∈rd ∈buffer noise ˜qnull= given uncertainty aboutm horizon index. note triangles represent possible future highly uncertain transition plausible. shade smaller triangle represents degree uncertainty value concrete take measure uncertainty variance case immediate rewards inﬂuence stepping back addition highly uncertain uncertain somewhat uncertain since pairs transition exposed uncertainty associated state-action pair. uncertain from reasonable chance never see. continuing work leftward diagram easy states represents number periods visit state planning horizon. larger triangles divided smaller ones associated actions. dotted lines indicate plausible transitions except which reasons explain tend grow shrink variance keep things simple assume exhaustive tabular r\u0000s\u0000×\u0000a\u0000 encodes value single state-action pair. parameterized value function represent mapping froms×a right-hand-side average target values. recall that any≠ ˜hsa\u0000 large sample average extremely accurate therefore ˜q˜θh+ \u0000st= distinguished case essentially equal ˜hdown\u0000= average target value therefore diﬀer substantially \u0000˜θhm]. notably noise term expectation based observation case ˜q˜θ sample ˜q˜θh+ exhibits variance least virtually equal exhibits variance least unlike case ˜q˜θ also exhibit non-negligible variance since pairs transition therefore depend noise-corrupted realization ˜q˜θ working exhibit high variance variance uncertainty concerning immediate reward transition. illustrated figure uncertainty propagates oﬀer incentives agent pursue information even require multiple time periods arrive informative observation. essence deep exploration. maximization actions line algorithm. second important learn grlsvi uses noise samples across iterations loop line suppose learn grlsvi used independent noise samples iteration. then applied example figure iterations comes random sampling buffer boot includes many elements buffer sampled uniformly buffer randomness induced sampling procedure substitutes randomness induced gaussian noise learn grlsvi. bootstrap sampling value function randomization present several beneﬁts additive gaussian noise. first bootstrap resampling schemes require noise variance terms input simpliﬁes algorithm user perspective. related point bootstrap eﬀectively induce state-dependent heteroskedastic randomization appropriate complex environments. generally consider bootstrapped rlsvi non-parameteric randomization value function estimate. overloading term bootstrap here. reinforcement learning bootstrapping commonly used refer calculation state-action value estimate based value estimates states agent transition. here refer statistical bootstrap data-based simulation. common form statistical bootstrap uses sample data approximation generating distribution previous value function parameters regularization parameter regularization parameter regularization parameter planning horizon updated value function parameters would involve generating independent identically distributed variable \u0001ˆθ− θ\u0001ï\u0017 θh+← argmin θ∈rd initiating live. alternative regularization penalty ψ=\u0001ˆθ− θ\u0001~λ section present variants rlsvi designed address important practical considerations computational eﬃciency robustness mis-speciﬁcation parameterized value function. many ideas reinforcement learning literature brought bear purposes means cover exhaustive list. rather present ideas lead particular algorithm eﬀectively addresses broad range complex problems. algorithm also serves illustrate many degrees freedom mixing matching ingredients reinforcement learning literature randomized value functions part recipe. buﬀer past observations value function sometimes referred experience replay. algorithms presented inﬁnite buﬀer thus require memory compute time grow linearly number observations. complex problems require substantial learning times requirement becomes onerous. overcome this restrict buﬀer ﬁnite size treating fifo queue. accomplished using cache finite instead cache infinite live represented helpful restrict attention relevant data regressing focusses minimizing errors relevant states actions. restricting buﬀer recent observations serve reasonable heuristic here. recent work also demonstrated beneﬁt sophisticated prioritization data storage ﬁnite buﬀer even ﬁnite buﬀer value function parameterized high-dimensional vector enormous computational resources required produce estimate contexts substantial beneﬁt using ﬁrst-order algorithms vein stochastic gradient descent. temporal-diﬀerence learning oﬀers approach regularization parameter represents noise variance learn lsvi learning rate parameter determines step size updates number minibatches sample number opposed learn lsvi learn makes previously computed parameter vector oﬀers start could reduce number updates required arrive reasonable buﬀered data previous parameter vector similar data set. combination learn cache finite neural network value function representation essence deep networks achieved notable success producing high-performing strategies atari arcade games. learn oﬀers ﬁnite-buﬀer ﬁrst-order variation lsvi. consider variation rlsvi designed carry deep exploration. approach involves learning value functions parallel diﬀerent buﬀer randomly perturbed samples. viewed ensemble value functions approximation distribution value functions induced rlsvi. episode select make cache parallel gauss represent parallel buﬀers. time observation incorporated enqueued buﬀers case reward perturbed diﬀerent independent gaussian noise term. buﬀers plays role perturbed data could produce single randomized value function. given parallel buﬀers learn parallel rlsvi produces parameter vectors random index given parallel structure value function produced taken parameterized abuse notation written ˜q˜θ= ˜q˜θ˜k cache= cache parallel gauss act= greedy learn= learn parallel rlsvi learn internal= learn sample= random choice] note random choice selects random subset elements hence regularization penalty functions play important role exploration. penalty functions reﬂect prior uncertainty guide initial exploration observations accumulated. perhaps simplest generate functions proceed manner similar learn grlsvi sample independent parameter vectors section regularization penalties produced based previously gathered synthetic data. approach could leverage example data observing operation system arbitrary policy performing task diﬀerent objective studied areas oﬀ-policy learning transfer learning note that used oﬀ-policy learn update lead unstable learning even cause value function estimate diverge instability exacerbated algorithms learn parallel rlsvi lead value estimates computed data oﬀ-policy single value estimate. context beneﬁcial replace naive learn update alternative designed oﬀ-policy learning algorithm described induces randomness value functions perturbing observations gaussian noise similarly learn grlsvi also possible design variation leverages statistical bootstrap learn brlsvi done designing alternative cache parallel gauss randomizes number copies observation placed parallel buﬀers section provides regret analysis rlsvi particularly simple special case general problem section version rlsvi consider involves application live invoked cache infinite greedy learn grlsvi bound establish applies tabular time-inhomogeneous transition kernel drawn dirichlet prior. stylized setting provides rigorous conﬁrmation rlsvi capable performing provably eﬃcient deep exploration tabular environments. addition hope analysis provides framework establishing general guarantees example applying rlsvi linearly parameterized value functions. several intermediate lemmas used analysis hold much less restrictive assumptions could useful beyond setting studied here. consider class ﬁnite-horizon time-inhomogeneous mdps. formulated special case paper’s general formulation follows. assume state space factorizes =s∪s∪s∪\u0016∪sh− state always advances state st+∈st+ process terminates probability period notational convenience assume sets ...sh− contains equal number elements. state space factorizes ass=s∪s∪s∪\u0016∪sh− where\u0000s\u0000=\u0016=\u0000sh−\u0000<∞. mdpm= state s∈st written pair ...\u0000s\u0000}. similarly policy viewed sequence notation specialized time-inhomegenous problem writing transition probabilities asptxa≡pa) reward probabilities asrtxax′≡ra. consistency also diﬀerent notation deﬁne v∗mt∶= maxπ πmt. similarly deﬁne state-action value function timestep transition reward. refer pair outcome decision. assumptions distribution rewards state-transitions compactly written assumption outcome distributions. study regret rlsvi upon choosing action algorithm observes pair consisting state following bayesian model mdpm. assumption required rewards take values cardinality outcome space is\u0000x×{ \u0000x\u0000. each∈{ }×x×a outcome distribution drawn dirichlet txa∼ dirichlet αtxa∈ r\u0000x\u0000+ drawn independently across. assume αtax= all. focus ﬁrst bound given equation parameter governs relative strength prior mean q-functions sampled rlsvi. typically think constant reﬂecting situations weak prior knowledge optimal value function grow variables case regret bound cumulative bayesian regret less value scales with\u0000x\u0000~\u0001. general least order\u0000x\u0000 samples required learn transition kernelptxa. therefore large\u0000x\u0000 prove rlsvi learns make near-optimal decisions using fewer samples interesting compare bayesian regret bound bounds established tabular reinforcement learning algorithms. results directly comparable bound established rlsvi develop bounds minimax rather bayesian regret study classes mdps satisfying recurrence assumptions rather episodic mdps. however worth noting algorithms attempt represent transition probabilityptxa accurately applying analysis problem yields regret bound larger dependence on\u0000x\u0000. \u0001\u0000x\u0000\u0000a\u0000l). ﬁrst term regret bound precisely. bound number episodes large dominant term second maxa∈a nonstandard notation value function outcomes state-action value function r\u0000x\u0000\u0000a\u0000 deﬁne corresponding value function r\u0000x\u0000 outcomes vq∶= maxa∈a x′∈x data observed episode action chosen n=\u0000d−\u0000 number past observations triple. ease notation write timestep state action denote empirical distribution outcomes dataset section introduces bellman operator underlying mdpm notion assumption gaussian noise added rlsvi iterations action value function r\u0000x\u0000\u0000a\u0000 generates random state-action value function output. shows bellman update rlsvi diﬀers empirical bellman update ways slight regularization toward prior mean importantly rlsvi adds independent gaussian noise update. state-action value functions r\u0000x\u0000\u0000a\u0000 chooses actions greedily respect sequence. interpret algorithm’s estimate value following greedy policy throughout episode starting state denotes true expected value. interpret fmqt+ error bellman’s policy throughout episode. lemma says prediction maxa∈a chosen greedily respect q–functions regret v∗m− function optimistic initial state sense maxa maxa regret episode bounded policy bellman error apply regret decomposition study rlsvi taking sequence generated rlsvi episode policy bellman error simpliﬁed plugging ftqt+. next corollary lemma episode holds historyh−. tower property conditional expectation clearly highlighted subsection f\u0016fh− q∗m= fm\u0016fmh− distributions generated fashion complicated diﬃcult study compared generated applying show optimism deﬁnition closely mirrors second order stochastic dominance widely used decision theory random payout second order stochastically dominant respect holds concave increasing function rational risk-loving agent prefers intuitively requirement means draws generate payouts larger noisier goal show rlsvi applied appropriate parameters generates iterates larger noisier implies following monotonicity property bellman operator underlying rlsvi. later enable show initial iterates rlsvi stochastically optimistic optimism preserved recursive application stochastic bellman operators f\u0016fh−. random functions r\u0000x\u0000\u0000a\u0000. suppose conditioned onh− entries drawn independently across drawn independently rlsvi noise terms a′∈a qïïïï\u0017+ convex function )x′∈x a′∈a convolved independent noise term result therefore follows lemma consider random variable dirichlet. then mean size ﬂuctuations depends concentrated around mean captured pesudocount α=∑n captured span≡ maxi minj next lemma shows gaussian random spread elements ﬁxed dirichlet αi)− span x\u0002so αivi deﬁnition deﬁnes value outcomes maxa′∈a suppose assumption holds rlsvi applied parameters satisfying episode history time pair ∈x×a ﬁxed r\u0000x\u0000\u0000a\u0000 span≤ maxx∈x mintxa θtxa. remark span≤\u0001vq\u0001∞≤\u0001q\u0001∞+ therefore suﬃces miny ¯θy≥\u0001q\u0001∞+ proof. recall bellman update true mdpm fmtq= plugging ftq\u0000h−∼ assumption miny corollary assumption holds rlsvi applied parameters satisfying= miny ¯θy≥ historyh− state-action pair∈x×a. proof. reduce notation prove episode proof follows identically general conditioning history every step. recall ff\u0016fh− q∗m= fmfm\u0016fmh−. proceeding induction suppose ﬁnal step uses lemma combined fact satisﬁes span≤ basic gaussian maximal inequality implies log. next lemma slight generalization result seen taking maxj lemma jointly distributed random variables follows ...n} random index. multivariate gaussian distribution noise terms rlsvi state action visited rlsvi second corollary e]≤\u0001 corollary rlsvi applied parameters v~λ= e≤lt<h tqt+− fmtqt+) posterior-mean bellman update underm txa\u0000h−]. e\u0000h−]= recall well r\u0000x\u0000. since prior overp cannot visited prior period episode also dirichlety). e\u0000h−] −β\u0001vq\u0001∞ ftq− e\u0000h− tqt+− fmtqt+) t)\u0000h− tqt+− e\u0004h−q βt+\u0001∞) e\u0004h−q second inequality uses independent conditioned onht. summing episodes implies t)\u0017\u0017\u0017\u0017\u0017\u0017 ≤lt≤h ≤lt<h ≤lt<h \u0000x\u0000a\u0000l \u0000x\u0000a\u0000l \u0017β\u0004\u0001¯θ\u0001∞+ \u0001vqt+\u0001∞\u0004 ≤lt≤h t≤t≤l β\u0004\u0001¯θ\u0001∞+ \u0001vqt+\u0001∞\u0004\u0004 h\u0000x\u0000\u0000a\u0000 log\u0004+ l\u0000x\u0000\u0000a\u0000\u0004 ≤lt≤h β\u0002h+ log\u0002 h\u0000x\u0000\u0000a\u0000 log\u0004+ l\u0000x\u0000\u0000a\u0000\u0004 \u0000x\u0000\u0000\u0000a\u0000\u0001 log+ log\u0004+ l\u0000x\u0000\u0000a\u0000\u0004 \u0000x\u0000a\u0000l log. \u0001\u0000x\u0000\u0000a\u0000l). \u0001\u0000x\u0000\u0000a\u0000~l log\u0004+ l\u0000x\u0000\u0000a\u0000\u0004\u0004 \u0001\u0000x\u0000a\u0000l log+\u0004 \u0001\u0000x\u0000\u0000a\u0000l log+\u0004 log+\u0004+ l\u0000x\u0000\u0000a\u0000\u0004\u0004 β\u0000x\u0000\u0000a\u0000l log+ log+\u0004+ l\u0000x\u0000\u0000a\u0000\u0004 desired bound. \u0000x\u0000\u0000a\u0000 naive bound β\u0000x\u0000\u0000a\u0000l section established formal guarantees tabular version rlsvi. result serves sanity check demonstrating rlsvi carries eﬃcient deep exploration tabular nature prior structure setting limits scope theoretical results. perhaps importantly results apply parameterized representations used generalize across states actions. section present computational results oﬀer assurances. particular discuss results series experiments designed enhance insight working rlsvi beyond scope theoretical analysis. focus experiments improve understanding rather solve challenging problems. nevertheless believe observations didactic examples prove valuable toward design practical systems require synthesis eﬃcient deep exploration eﬀective generalization. begin computational experiments empirical study deep-sea exploration problem example oﬀers simple illustration importance deep exploration. although associated states dithering schemes require number episodes grows exponentially eﬀectively explore environment. deep exploration approaches hand eﬀectively explore environment within sub-exponential number episodes. results verify eﬃcacy randomized value functions rlsvi carries deep exploration. begin investigation rlsvi tabular representation. goal study behavior rlsvi simple setting similar addressed theorem this randomly generate random deep-sea environments according example empirically evaluate performance many simulations. compared setting speciﬁed theorem rescaled constant order accelerate learning deterministic deep-sea environment. compare performance learn grlsvi well-studied reinforcement learning algorithms speciﬁcally designed explore eﬃciently tabular representations ucrl psrl similarly modify ucrl psrl accelerate learning deterministic deep-sea environment. algorithms modiﬁcations reduce learning times aﬀect rates learning times scale problem size. psrl oﬀers lowest level regret followed rlsvi ucrl. hence rlsvi competitive algorithms designed yield eﬃcient exploration tabular representations. quantity random depends realization ofm∗. algorithm regret bound regret≤√ would expect learning time results theorem suggests average scaling envio regret bound. deep-sea problem bounds therefore suggests learning times scale rlsvi suggests scaling. however recent work suggests bound loose section presents evidence eﬃcacy rlsvi tabular representation. however value rlsvi lies ability function well parameterized value functions generalize across states actions. model-based algorithms ucrl psrl accommodate form generalization. subsection continue investigation deep-sea environment using linear parameterized representations. this generate random subspace dimension speciﬁcally designed include true optimal value function deep-sea environment irrespective whether treasure bomb. generate random basis unit vectors r\u0000s\u0000\u0000a\u0000 span space. vector dimensional parameter vector encoding feature weighs ˜qθ=∑d features generate nonzero values. representation rlsvi learns sion. figure plots realized regret generated learn grlsvi again simulate problem random seeds treasure least episodes even reach chest cases treasure rather bomb bind regret learning times. reason present results associated former case save computation. figure plots learning times function diﬀerent numbers features row. would expect learning time increases number features. importantly scaling chain length graceful grows plots data log-log scale highlight sub-exponential growth. empirically slope scale approximately implying learning time scales approximately quadratically another perspective scaling figure presents plots learning times function number features several values case learning time appears grow linearly number features threshold increase much empirically seems point beyond incremental learning time incurred additional features small. intuitively might speculate reasonable equal maximum number states-action pairs observed time period. beyond point additional features must linearly dependent. figure plots cumulative regret learn grlsvi varying numbers features degrees misspeciﬁcation episodes. results average seeds value noise scale results indicate rlsvi remains robust degree misspeciﬁcation. however point model-mispeciﬁcation becomes severe value increases depending upon number basis functions power representation increases number features enables rlsvi study dependence results parameter settings. deep-sea problem considered sense degenerate problem instance deterministic. order oﬀer representative results pertaining variance parameter tuning consider also consider modiﬁed version deep problem reward noise variance largely dependent upon scale noise actual environment. underlying environment deterministic beneﬁt adding noise values perform best. however environment stochastic choosing order variance noise environment necessary fall victim unlucky observations. settings bootstrapping performs competitively ex-ante best choice need speciﬁed advance. gain intuition parameter tuning take data figure present realized regret random seed figure smaller noise problem lead premature sub-optimal convergence never opens chest choices large lead slower learning exploration lead linear regret. note rlsvi bootstrap learns right noise variance beyond that even learn vary states actions. further figure plot learning times applying learn brlsvi settings learn grlsvi applied generate figures results suggest learning times learn brlsvi scale similarly learn grlsvi. experiments section designed highlight several properties rlsvi simple setting. results demonstrate rlsvi successfully synthesize eﬃcient exploration generalization. however context example underlying system involved tractable number states. process expanding upon experimental work include investigation incremental algorithms applied high-dimensional systems. preliminary investigations presented several arcade games including tetris angry birds atari games well much applied reinforcement literature focussed simulated systems eventual performance policies learning often billions trillions episodes. assessed manner performance driven largely investment computational resources simulation time eﬀectively reinforcement learning algorithm makes decisions interprets observations. many real systems data collection costly constrained physical context calls focus statistical eﬃciency. mind appropriate example evaluate algorithms based performance ﬁxed number episodes. exploration driver statistical eﬃciency. discussed section exponentially large diﬀerence data requirements agent explores dithering commonly done past applications reinforcement learning agent carries deep exploration. paper developed randomized value functions concept enables eﬃcient deep exploration conjunction value function learning methods commonly used reinforcement learning. work generously supported research grant boeing marketing research award adobe stanford graduate fellowships courtesy paccar burt deedee mcmurty microelectronics. thank xiuyuan shuhui kuang pointing errors ambiguities earlier draft broadly students participated stanford university’s oﬀering reinforcement learning feedback stimulating discussions work.", "year": 2017}