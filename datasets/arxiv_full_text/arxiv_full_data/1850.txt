{"title": "Scaling Text with the Class Affinity Model", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "Probabilistic methods for classifying text form a rich tradition in machine learning and natural language processing. For many important problems, however, class prediction is uninteresting because the class is known, and instead the focus shifts to estimating latent quantities related to the text, such as affect or ideology. We focus on one such problem of interest, estimating the ideological positions of 55 Irish legislators in the 1991 D\\'ail confidence vote. To solve the D\\'ail scaling problem and others like it, we develop a text modeling framework that allows actors to take latent positions on a \"gray\" spectrum between \"black\" and \"white\" polar opposites. We are able to validate results from this model by measuring the influences exhibited by individual words, and we are able to quantify the uncertainty in the scaling estimates by using a sentence-level block bootstrap. Applying our method to the D\\'ail debate, we are able to scale the legislators between extreme pro-government and pro-opposition in a way that reveals nuances in their speeches not captured by their votes or party affiliations.", "text": "probabilistic methods classifying text form rich tradition machine learning natural language processing. many important problems however class prediction uninteresting class known instead focus shifts estimating latent quantities related text aﬀect ideology. focus problem interest estimating ideological positions irish legislators d´ail conﬁdence vote. solve d´ail scaling problem others like develop text modeling framework allows actors take latent positions gray spectrum black white polar opposites. able validate results model measuring inﬂuences exhibited individual words able quantify uncertainty scaling estimates using sentence-level block bootstrap. applying method d´ail debate able scale legislators extreme pro-government pro-opposition reveals nuances speeches captured votes party aﬃliations. introduction. text classiﬁcation goal infer discrete class label observed text core activity statistical machine learning natural language processing. instances problem include inferring authorship genre kessler detecting deception classifying e-mail spam detecting sentiment huge appeal methods developed applications that small training possible classify large number unlabelled documents reasonable accuracy without costly human intervention. many applications however classiﬁcation uninteresting goal since correct identiﬁcation class obvious costless. fundamentally uninteresting example attempt predict political party speaker identity supreme court justice. furthermore many social political settings observed discrete outcomes institutions ∗this research supported european research council grant erc--stg cause predicted observed class membership diverge signiﬁcant ways. parliamentary democracies party discipline enforced instance voting follow party lines even best predictions observable features indicate heterogeneous outcomes. cases trivial predict class observable covariates presence covariates text speech ancillary goal class label prediction. even observing text improve prediction performance case text uninformative. legislative debates text legislators generate ﬂoor speeches provide direct opportunity express contrary divergent preferences legal briefs take another example trivial classify opinions majority dissenting using observed text information possible place briefs spectrum extremes simply attempting predict category opinion—for instance classifying amicus curiae briefs pro-petitioner pro-respondent less direct interest since categories already known. text document reveal nuances captured sometimes disagreement class label. here focus application ill-suited text classiﬁcation text nonetheless informative. analyze irish d´ail conﬁdence debate previously studied laver benoit used debate speeches demonstrate wordscores scaling method. context country coming recession series corruption scandals surfaced involving improper property deals made government certain private companies. public backlash precipitated conﬁdence vote government legislators debated voted decide whether current government would remain forced constitutionally resign. table summarizes composition d´ail provides descriptive statistics speech texts. debate chance learn legislators’ ideological positions. irish parliamentary context characterized strict party discipline move largely symbolic legislator voted strictly party members governing parties voted support government members opposition parties voted against. despite votes entirely predictable ﬂoor speeches debate oﬃcial tally reveal nuances legislators’ positions. take example following excerpt noel davern moderate fianna f´ail party decision oppose motion conﬁdence positive assertion disapproval ordinary people actions discredited government. people watched amazement unfolding scandals tainted government. government cannot said deserve conﬁdence people. legislators express views place somewhere extremes absolute government support absolute opposition support. davern ferris participated debate ideological spectrum? essential question attack manuscript. answering question disposal speech texts along additional information. know leader government give speech extreme pro-government spectrum know heads major opposition parties extreme end. three texts reference points scale ambiguous texts whose positions unknown must estimated. solve particular problem develop text scaling method broadly applicable situations documents unlabelled examples documents extremes hypothesized ideological stylistic spectrum. instead predicting class membership objective problems scale continuous characteristic measuring text known classes based degree similarity typical texts classes. follows develop class aﬃnity model demonstrate scaling degree support opposition expressed speeches made conﬁdence debate. start outlining foundations scaling model contrasting ﬁrst similar approaches designed classiﬁcation lexicographical association methods form sentiment dictionaries section sets model comparing related methods highlighting diﬀerences statistical principles also using application. sections detail model reference distributions estimated section relates aﬃnity model related methods. section show measure inﬂuence individual words provide recommendations removing common terms might skew results. apply procedure choose tailored vocabulary application section section demonstrates estimate uncertainty class aﬃnity scaled estimates. finally summarize results results ﬁtting class aﬃnity model application oﬀer concluding remarks. scaling classiﬁcation method. stated repeatedly classiﬁcation objective problem nonetheless long tradition ﬁtting classiﬁcation methods text might applying methods here. training three leadership speeches label government opposition. supervised classiﬁcation method training make predictions legislators. using naive bayes text classiﬁcation method popularized sahami would model tokens speech text independent draws label-dependent distribution estimated reference texts. letting label denote government label denote opposition label word type vocabulary would estimate probability random token drawn text label equal typically empirical word occurrence frequencies reference documents smoothed version thereof. throughout text unless otherwise noted take vocabulary word types appear least twice leadership speeches excluding common function words modixv denotes number times word type appears text. expression arises ratio multinomial likelihoods probability vectors using naive bayes classiﬁcation two-class prediction problem would predict label government would predict label opposition quantity measures strength evidence label text government opposition quantity scale virgin texts. unfortunately naive bayes scaling method serious drawbacks. first estimated odds tend absurdly high. example median absolute odds corresponding second measuring strength evidence longer texts tend higher absolute odds. illustrate defects fig. plot absolute odds class membership function text length. related methods suﬀer versions problem. multinomial inverse regression regularizes probability vector estimates adds calibration step log-odds still suﬀers drawbacks naive bayes. discriminative methods like used joachims aﬀected degree depending choice features. logistic regression example features linear functions counts still case longer documents extreme counts hence extreme predictions. choices predictors give rise predictors less sensitive variations document length. even classiﬁcation methods suﬀer defects noted above still fundamental disconnect classiﬁcation philosophy goals scaling. classiﬁcation world document either black white; unlabelled document method tell probability label black. reality though text gray mixture black white. fundamental diﬀerence perspective precludes using classiﬁcation method task. expand metaphor below. scaling dictionaries. text scaling methods take black-and-white classiﬁcation view world. successful alternatives dictionary-based scaling simplest forms dictionary methods conceive text mixture contrasting poles positive negative. neutral words discarded vocabulary. scaling text determined average orientation tokens. many variations dictionary-based scaling concreteness focus grimmer stewart’s formulation. apply scaling problem hand—scaling debate speeches—we would need non-overlapping lists words associated government words associated opposition. given lists would assign score word type government list score labor-intensive error-prone build custom dictionary application often practitioners apply dictionary scaling methods oﬀ-the-shelf dictionaries instead building own. application lexicoder sentiment dictionary broad lexicon scored positive negative tone tailored primarily political texts would natural choice however authors note applying oﬀ-the-shelf dictionary domain often leads undesirable results. table illustrates point context application comparing word orientations determined empirical associations government opposition observed leadership speeches. rows indicate lsd-assigned orientations words; columns signiﬁcant diﬀerences usage rates classes measured keyness likelihood ratio score signiﬁcance level taking negations account recommended young soroka display number word types cell along common words. dictionary appropriate application observe positive words associated government usage negative words associated opposition usage. patterns table however show diﬀerent result. positive words high usage government leadership speech negative words high usage opposition leadership speeches. positive negative words clear association either government opposition. furthermore worrying cases dictionary orientation counter association classes. example declares word negative context debate deﬁcit refers simply ﬁscal outcome; likewise conﬁdence related question debate intended convey positive valence. despite designed detect political valence dictionary fails since tailored particular debate. terms associated type aﬀect generally used diﬀerently context no-conﬁdence debate. beyond problem domain adaptation fundamental issue dictionary methods basic premise—that word clear orientation—is inappropriate domain. words application clearly either belong category other. seen table word types statistically signiﬁcantly diﬀerent usage rates government opposition leadership speeches. vast majority words used government opposition thus mixed associations classes. dictionaries adjust giving non-binary scores words adjustments often suﬀer domain adaption problems. sequel present alternative method allows mixed word association simultaneously adapting domain. aﬃnity model. classiﬁcation methods assume text member well-deﬁned category. dictionary methods make strong assumption take unrealistic view world supposing word well-deﬁned orientation. table highlights diﬀerence makes clear room third worldview allowing texts words gray. formalize intuition statistical model refer aﬃnity model. basic conceptual model course speech speaker’s orientation switches back forth government mode opposition mode. government mode chooses words manner government leadership. likewise opposition mode chooses words manner opposition leadership. place speaker spectrum extremes pro-government pro-opposition according proportion time speaker’s underlying orientation evolves parallel text represented value denotes speaker’s underlying orientation uttering token general suppose possible orientations identiﬁed labels conceptual framework speech corresponding underlying orientation sequence realizations speaker-speciﬁc random process. deﬁne speaker’s aﬃnity toward orientation expected proportion time underlying orientation speciﬁc application orientations. debate speaker separate aﬃnity vector scale speaker estimating aﬃnities government opposition impose simplifying assumptions make inference model tractable. first suppose independent identically distributed. forces every label position underlying orientation randomly distributed second suppose independent conditional distribution depends speaker picks underlying orientation probabilities determined given underlying orientation speaker picks token according distribution fig. summarizes generative process. number times word appears text. high level generative model used topic model main diﬀerence models topic models typically unsupervised aﬃnity model uses supervision estimate elaborate connection topic models section note also aﬃnity model seen generalization naive bayes model depicted fig. naive bayes model document single underlying orientation words document share underlying orientation. parameter seen estimating aﬃnities. aﬃnity model described section lends naturally likelihood-based estimation. ﬁrst consider problem estimating aﬃnity vector particular text given reference distributions sisting vectors non-negative components satisfying equality implication equality constraint model over-parametrized makes estimating directly awkward. handle constraint reparametrize model terms observed information function positive semideﬁnite indicating likelihood function concave. estimate maximizing likelihood using newton-raphson iterative method. expensive part maximization procedure computing takes time faster count vector sparse. experience d´ail speeches method typically converges iterations. diﬃcult part optimization must restrict exchange adding small bias estimates reduce variance remove explicit inequality constraints parameter space. particular firth shows asymptotic regime tends inﬁnity adding penalty order likelihood adds term size bias estimator case choose positive scalar deﬁne penalty function then estimate aﬃnities maximizing penalized likelihood penalty ensures strictly concave maximizer unique belongs interior parameter space. analyses manuscript penalty value section provides theoretical justiﬁcation penalty value related context. estimating reference distributions. reference distributions need estimated data. framework learning step requires large volumes training data rather texts clearly polar examples reference class form benchmarks estimating texts’ aﬃnities classes. context speciﬁc application irish d´ail conﬁdence debate recall contrasting classes represent government opposition leaders government opposition respectively represent archetype texts class. taoiseach charles haughey’s speech forms government reference text estimating speeches opposition party leaders form reference texts estimating estimate particular reference distribution suppose general disposal texts drawn distribution lengths denote vectors word counts texts application estimating government reference estimating opposition reference. smoothed empirical frequencies estimate advocated lidstone choose nonnegative smoothing constant estimate probability word type many reasonable choices smoothing constant including choosing adaptively natural language processing common take maximum posteriori estimator uniform prior frequentist standpoint value .—which corresponds using jeﬀreys prior p—is slightly defensible. regime ﬁxed tends inﬁnity using results firth special case reference distributions disjoint supports—that classes word type v—aﬃnity scaling exactly equivalent dictionary scaling. make equivalence clear suppose word type reference probabilities nonzero. case partition vocabulary union disjoint sets condition ensures word type associated exactly label. disjoint support condition observe token immediately infer underlying orientation class word support. log-likelihood simpliﬁes wordscores. wordscores scaling method developed laver benoit garry turns closely related class aﬃnity scaling. method primarily used scale documents reference classes works well practice criticized theoretical foundations show however wordscores scaling closely related aﬃnity scaling gives highly correlated results texts close extremes elaborate connection below. wordscore scaling turns deeply connected aﬃnity scaling. connection note using parameterization section score observed information functions aﬃnity model evaluated right hand side expression equal ﬁrst fisher scoring iterate computed maximizing starting initial value maximizer close approximately equal ﬁrst iterate. thus text roughly balanced reference support vector machines logistic regression. shown analytically aﬃnity scaling gives similar results wordscores. turns that number reference documents small scaling methods approximately equivalent classifying support vector machine linear regression. suppose two-class case reference document class. imagine ﬁtting linear classiﬁer tries predict class using document’s word frequencies features. vocabulary size greater number training documents classes perfectly separated long reference distributions corresponding training documents identical. case since classes perfectly separated multiple predictor gives classiﬁcation performance training set; precise scaling chosen ﬁtting procedure depend regularization parameters. comparing support vector machine scaling unnormalized wordscores scaling substantive diﬀerence denominator coeﬃcient thus constant shift methods give similar results. light connection wordscores aﬃnity scaling developed sec. implies situations support vector machine results highly correlated aﬃnity scaling results. veriﬁed connection methods empirically using svmlight software default tuning parameters fig. shows support vector machine estimated odds plotted aﬃnity scaling results. scalings give similar results main distinction numerical value support vector machine odds determined completely regularization parameter thus uninterpretable. aﬃnity scaling document contrast interpreted directly. topic models. topic models share similar perspective aﬃnity model represent texts mixtures topics topic associated word distribution. framework topics correspond reference classes text-speciﬁc topic weights correspond class aﬃnities. learn class distributions labeled reference texts. approach diﬀers taken unsupervised topic models estimated topics correspond scaling quantities interest. crete continuous scale supervised models force clear associations topics scaling quantities interest assume texts discrete labels indicating class membership. fundamental assumption places methods category classiﬁcation methods like naive bayes estimating probability class membership class aﬃnity. despite philosophical diﬀerences practice supervised topic models give scalings highly correlated aﬃnity model scaling. connection supervised topic models easiest understand case mcauliﬀe blei’s supervised latent dirichlet allocation models text-speciﬁc label random quantity linked linear function text-speciﬁc topic weights. roughly speaking method works stages. ﬁrst stage slda topic model reference texts. second stage slda logistic regression model using ﬁtted topic weights predictors class label response. practice slda topics logistic regression simultaneously number topics larger number reference texts diﬀerences sequential simultaneous ﬁtting determined regularization parameters random initialization. connection slda aﬃnity model scaling closest topics reference texts. case since number topics equals number reference texts slda perfect allocating topic reference text separate classes perfectly given topic weights using linear predictor odds class membership form coeﬃcient gets determined regularization parameters. slda gets used prediction unlabelled texts ﬁtted topic weights values ﬁtted aﬃnity model slda score highly correlated diﬀerence estimated aﬃnities. case topics reference texts relationship aﬃnity scaling slda simple general intuition still holds methods still give highly correlated results. fig. illustrates model using topics correlation non-reference text scalings methods here slda method gives unreasonable results extremes. furthermore interpretation scaling value diﬀerent odds class membership slda versus degree membership aﬃnity model. unsupervised methods. approaches scaling texts including latent semantic indexing slapin proksch wordﬁsh poisson scaling method estimate latent text-speciﬁc traits using unsupervised methods. often estimated traits correlated recognizable attributes used scale ideology. letting denote count word type text slapin proksch wordﬁsh model speciﬁes poisson random variable mean unknown text-speciﬁc parameters word-speciﬁc parameters estimates shown provide valid estimates latent positions expressed speeches drawback unsupervised scaling sort however provide guarantee estimated latent trait corresponds quantity interest. demonstrate behavior fig. plot wordﬁsh scaling estimates debate speeches versus aﬃnity scaling estimates. methods give similar results also notable diﬀerences. government opposition leaders extreme examples determined wordﬁsh indicating even focused context—a debate conﬁdence motion—the primary dimension diﬀerence something governmentopposition divide. previous section used simple analytic form aﬃnity scaling model understanding connections text scaling methods. beyond this another advantage model’s form simplicity facilitates computationally eﬃcient diagnostic checking model ideally exhibit characteristics. first driven small number word types instead determined accumulation information many diﬀerent word types. second word types show inﬂuence determining ones make sense subject matter perspective. check whether scaling results satisfy properties better understand generally develop inﬂuence measure characterize impact word type determining overall strategy assessing inﬂuence stems cook context linear regression assesses inﬂuence observation measuring change results deleting observation. proceeding aﬃnity estimate ideally would computing maximizer likelihood gotten setting zero large number word types makes impractical. settle ﬁnding computationally simple closed-form approximation suppose vector token counts particular text interest aﬃnity vector estimate gotten maximizer corresponding likelihood deﬁned making dependence explicit score observed information functions rv×v diagonal matrix arbitrary word type consider eﬀect setting deﬁnes vector token counts deﬁned denote standard basis vector deﬁne ˆqev note xvev vocabulary selection. previously mentioned results presented fig. elsewhere prequel vocabulary word types appearing leadership speeches excluding words appearing words english snowball stop word list. exclude words? initially exclude words vocabulary. aﬃnity model complete vocabulary used scale nonleadership speeches. then help understand results computed inﬂuence measures deﬁned speech word count vector word type also recorded direction inﬂuence entries matrix zero since count vectors sparse words appear speech inﬂuence aﬃnity estimate. word type recorded count nonzero speech inﬂuence entries along median maximum nonzero entries. report values table grouped direction inﬂuence. example word type social exhibited inﬂuence speeches. speeches deleting word social aﬀect shifting speech’s aﬃnity estimate away government median shift speeches deleting social shifts away government; equivalently appearances social push towards government. inﬂuence word determined usage rate degree usage imbalanced across reference classes. word types show inﬂuential table appear frequently exhibit small imbalance government opposition else appear moderately exhibit large imbalance classes. holds generally inﬂuential words tend either highly imbalanced moderately imbalanced high usage rates. many words table make sense example social nation economic inﬂuence aﬃnity towards government people taoiseach inﬂuence aﬃnity towards opposition. however clearly certain function words like exerting inﬂuence function words slightly imbalanced usage rates reference texts which compounded high usage rate results large inﬂuence. sensitivity stylistic diﬀerences manifestation common critique related wordscores scaling method reduce sensitivity stylistic diﬀerences eliminated function words analysis. also table words rare words like attribute proof large inﬂuence. words meaningful discriminators substantive grounds show inﬂuential appear reference speeches. estimated probabilities words unreliable. inﬂuence determined purely speeches computing inﬂuences word types reducedvocabulary model. table shows inﬂuential government opposition words computed before. possible snowball word list could missed inﬂuential function words inspecting words table words order found case application. suspicious words said context debate makes sense words pro-opposition. word said gets used typically used quote government usually opposition member criticizing government. likewise ﬁrst glance seem suspicious cent government list fact often used cite national statistics economy using state economy explain unrest. principle possible standard errors aﬃnity estimates directly expected observed information function however likelihood-based standard errors likely narrow ignore uncertainty estimates reference distributions rely independence assumptions model. ignoring uncertainty reference distribution estimates inappropriate reference small similarly independence assumption—that word tokens diﬀerent positions text independent other—simpliﬁes analysis likely violated real-world data. accurately assess uncertainty estimates need method accounts uncertainty reference distribution estimates dependence nearby words text. estimate sampling distribution scaling estimates dependence word tokens block bootstrap respects natural linguistic structure text following lowe benoit recommendation resample texts sentence level simulate sampling variation also capture meaningful dependencies among words within natural syntactic units. properly account uncertainty reference distribution estimates also construct sentence-level bootstrapped reference speeches. full procedure follows performed procedure non-leadership speeches getting separate bootstrap standard error each. comparison computed likelihood-based standard error estimates fisher information conditional reference estimates. unsurprisingly bootstrap standard errors generally wider likelihood-based estimates. uncertainty estimates order magnitude bootstrap standard error less times large likelihood-based standard error speeches median ratio standard errors sequel bootstrap standard errors quantify uncertainty aﬃnity estimates. fig. displays estimated government aﬃnities speeches performing feature selection. ﬁgure includes conﬁdence intervals computed using sentence-level bootstrap. discuss results detail next section. results. level government versus opposition inter-party levels results entirely line expectations parties arrayed order would consistent expectations opposition parties opposition side governing parties other also speeches diﬀerent parties align extremity positions regards establishment. speeches centrist opposition party fine gael express moderate anti-government positions either left party labour far-left democratic left party. median diﬀerence emerges clearly even though considered speeches labour democratic left leaders equivalent purposes training opposition class. interesting distinctions emerge examine intra-party diﬀerences expressed position. among government ministers surprising john wilson deputy prime minister gerard collins foreign minister senior fianna f´ail minister extreme government-oriented estimated positions exceeded taoiseach charles haughey himself. interesting next minister estimated ranking albert reynolds would later become next taoiseach. extreme among opposition-oriented government minister notable examples raphael burke removed ministerial position following year mary o’rourke months later would challenge albert reynolds party leadership. back-bench members voted government generally gave speeches lukewarm ministers. correspondingly estimated estimated government aﬃnities back-benchers generally lower minsters. three exceptions members extreme estimated governmentoriented aﬃnities nolan cullimore cowan. members brian cowen became minister labour following year occupied senior positions include prime minister next decades. opposition side similar heterogeneous estimated aﬃnities. salient examples extreme estimated government-oriented aﬃnities fine gael garret fitzgerald former future prime minister peter barry fought fitzgerald party leadership. emphasized fairly standard economic concerns attacking government’s poor economic performance rather corrupt behavior. notable member highest estimated pro-opposition aﬃnity member rabbitte would later become leader labour party; speech engaged personal attacks taoiseach speciﬁcally attacking character judgment. results applying class aﬃnity scaling model conﬁdence debate speeches provides results consistent expectations previous scholarly investigations episode using texts speeches succeeded revealing diﬀerences between speakers apparent party aﬃliations. application others like correct prediction class longer relevant benchmark process producing political text expected produce heterogeneous text within class. class—here voting conﬁdence motion perfectly correlated government opposition status—is observed uninteresting heterogeneity primary interest. despite would seem obvious measurement model scaling perspective however standard approach evaluating machine learning applications political science predictive accuracy benchmarked known classes focus estimating correct classes wrongly shifts attention away substantively interesting variation latent traits also ultimately impair classiﬁcation generality encouraging over-ﬁtting reduce predictive error. proposed alternative class aﬃnity scaling based probability model similar underlying class predictive methods allows mixed class membership. shifted focus class prediction something typically uninteresting social sciences form latent parameter estimation retaining advantages supervised learning approaches analyst controls inputs anchor model. strong tradition disciplines political science adapting machine learning produce continuous scales practitioners often unaware diﬀerences modeling assumptions classiﬁcation scaling methods fully explored implications assumptions highlighted diﬀerences similarities form encourages future development. relative simplicity method makes amenable direct mathematical analysis. simplicity allowed draw connections naive bayes classiﬁcation dictionary-based scaling host methods. able exploit analytic simplicity aﬃnity scaling model develop inﬂuence measure assessing sensitivity used guide vocabulary selection validate d´ail debate. using method explore nuances speeches d´ail conﬁdence motion produced estimates speaker accord qualitative reading speech transcripts expert understanding irish politics. application hard domain problem known lexicographical exists diﬀerentiate government versus opposition speech dictionary-based scaling even dictionary derived political text gives unsatisfactory results. limited training leadership speeches class aﬃnity scaling able adapt context debate give meaningful scaling. method applications beyond political text however could used score standard sentiment problems continuous scale applied problem contrasting reference texts identiﬁed. evans mcintosh cates recounting courts? applying automated content analysis enhance empirical legal research. journal empirical legal studies fienberg holland choice flattening constants estimating multinomial probabilities. journal multivariate analysis firth bias reduction maximum likelihood estimates. biometrika grimmer bayesian hierarchical topic model political texts measuring grimmer stewart text data promise pitfalls automatic content analysis methods political texts. political analysis heckerman horvitz sahami dumais bayesian approach ﬁltering junk e-mail. proceedings aaai- workshop learning text categorization joachims text categorization support vector machines learning many relevant features. european conference machine learning springer. joachims making large-scale learning practical. advances kernel methods support vector learning press cambridge pang vaithyanathan thumbs sentiment classiﬁcation using machine learning techniques. proceedings conference empirical methods natural language processing ramage hall nallapati manning labeled supervised topic model credit attribution multi-labeled corpora. proceedings conference empirical methods natural language processing volume -volume association computational linguistics.", "year": 2017}