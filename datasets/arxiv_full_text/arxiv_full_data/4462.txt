{"title": "Image denoising and restoration with CNN-LSTM Encoder Decoder with  Direct Attention", "tag": ["stat.ML", "cs.CV"], "abstract": "Image denoising is always a challenging task in the field of computer vision and image processing. In this paper, we have proposed an encoder-decoder model with direct attention, which is capable of denoising and reconstruct highly corrupted images. Our model consists of an encoder and a decoder, where the encoder is a convolutional neural network and decoder is a multilayer Long Short-Term memory network. In the proposed model, the encoder reads an image and catches the abstraction of that image in a vector, where decoder takes that vector as well as the corrupted image to reconstruct a clean image. We have trained our model on MNIST handwritten digit database after making lower half of every image as black as well as adding noise top of that. After a massive destruction of the images where it is hard for a human to understand the content of those images, our model can retrieve that image with minimal error. Our proposed model has been compared with convolutional encoder-decoder, where our model has performed better at generating missing part of the images than convolutional autoencoder.", "text": "image denoising always challenging task ﬁeld computer vision image processing. paper proposed encoder-decoder model direct attention capable denoising reconstruct highly corrupted images. model consisted encoder decoder encoder convolutional neural network decoder multilayer long short-term memory network. proposed model encoder reads image catches abstraction image vector decoder takes vector well corrupted image reconstruct clean image. trained model mnist handwritten digit database making lower half every image black well adding noise that. massive destruction images hard human understand content images model retrieve image minimal error. proposed model compared convolutional encoder-decoder model performed better generating missing part images convolutional auto encoder. image denoising well-studied problem ﬁeld computer vision image processing task take noised images restore original images. power computation day’s convolutional neural network performing well image denoising recognition task models noised images ﬁltered deep convolutional encoder decoder convolution deconvolution steps. models concerned image denoising part part image missing models good generating missing parts convolutional neural network good sequence processing. generate missing part image model needed understand predict upcoming part image reading current portion image. model need memory capture sequence long short term memory well performed recurrent neural network good sequence learning. lstm also solve general sequence sequence problems helps improve machine translation speech recognition task ﬁeld natural language processing sequence sequence mapping lstm encoder decoder performs excellently. ﬁeld also ﬁeld machine vision performing well like image description inspired works come encoder lstm decoder technique direct attention. model convolutional neural network reads image obtain ﬁxed size vector representation image. another multilayer lstm takes vector corrupted image along last output previous timestamp produces desired image. model decoder access image encoder misses information capture distorted image. main advantage model clean image well produce lost information. trained model mnist handwritten database training purpose made lower half every image black added noise image. image becomes highly destructed even human tough understand digit image. trained model highly distorted images able remove noises along retrieval lost information convolutional encoder-decoder able remove noise unable lost information accurately. convolutional neural network good cleaning image lstm good sequence generation combined power remove noise image generate missing part image. image denoising task works till today. success deep learning many deep learning models outperformed models. image denoising task deep convolutional encoder-decoder performs well sample convolutional encoder decoder shown ﬁgure models convolutional neural network encoder encode image symmetric deconvolutional layers decoder construct clean images. image denoising task model outperformed many models headache preprocessing image learning system. common convolutional lstm together image denoising task. conv-net lstm encoder decoder well performed model dense captioning image description jobs ﬁgure structure cells stacked together process sequence every timestamp output. vanishing gradient problem hard train recurrent neural network longer sequence settings long shortterm memory known solve long term dependencies memory state carry information longer help input gate output gate forget gate model consisted encoder decoder. encoder takes corrupted image decoder outputs cleaned reconstructed image. simpliﬁed version total system expressed equation intuition behind model encoder reads corrupted image creates thought vector supposed full information image vector. encoder full privilege encode anything help model reduce loss encode clean version corrupted image vector wants. decoder takes vector produce clean image access corresponding corrupted image. overall decoder reads image information current image thought vector last output decoder part image missing reproduce memory like lower half digit image missing draw part known shape figure shows architecture whole model. encoder part ﬁgure shows convolutional neural network architecture model following maxpooling maxpooling fully connected layers decoder part shows layers lstm produces ﬁnal image row. encoder model consist convolutional neural network. output encoder fully connected layer. image represented ﬁlter size convolve diﬀerent channel image stride size weights ﬁlter shared spatially diﬀerent every channel feature map. subsampling/pooling layers shrink width height feature ﬁlter size stride size sp.the equation convolutional layer here xijk value feature position vertical index local neighborhood horizontal index local neighborhood yijk pooled sampled layer. fully connected layer achieved product ﬁnal layer weight matrix adding bias vector output passed activation function decoder model consist multilayer lstm. lstm consist input gate output gate forget gate memory cell. input gate output gate forget gate memory cell/current state generated input precious hidden state thought vector encoder previous output decoder. input gate decide passing memory forget gate decide whether keep previous memory not. output gate decide outﬂow current hidden state. decoder initial hidden state memory state zero. initial bottom lstm cells multilayer lstm takes thought vector produced encoder current corrupted image output image previous units. denotes weight denotes bias. equations lstm cell decoder stated ﬁnal fully connected layer decoder tanh activation function used. decoder ﬁnal output layer sigmoid activation function used make output near original image image normalized thorough making pixel values figure shows diﬀerent activation functions. training purpose mnist handwritten digit dataset used. made lower half image blank/ black. added salt-and-pepper noise white noise image made image highly distorted. figure demonstrate training data generation process. divided data train test train contains data data belongs test set. decoder ﬁve-layered lstm. also trained convolutional encoder-decoder convolutional layer symmetric deconvolutional layer comparison. figure shows architecture model. written code help deep learning framework tensorﬂow provided google images edited python programming language. model dataset iteration batch size dropout used deter model ﬁtting. loss model depicted figure graph shows movement loss cnncnn cnn-lstm models. observed graph losses jumping behavior throughout training period happened stochastic training process. ﬁtted trend line smooth movement loss. loss cnn-cnn unable improve certain iteration cnn-lstm loss decreasing iterations. graph inferred iterations loss might goes down. model capable removing noise image well capable retrieving lost part images minimal error. diﬃcult task draw lost shape digits cnn-lstm performed outstandingly case generated lost shape great perfection even human tough understand content distorted images. figure shows performance proposed model comparison cnn-cnn model. amazing fact notice cnn-lstm model direct attention produced cleaned ﬁned image. edge produced images rough original. model produced smoothed images. amazing result happed rmse loss. rmse loss model tried predict pixel values original value. width stroke images diﬀerent pattern learn that’s model learned smoothed shape minimize loss types strokes. figure ﬁgure shows performance cnn-cnn cnn-lstm models. input column represent corrupted images cnn-cnn column shows cnn-cnn model generated images cnn-lstm shows cnn-lstm model generated images original column represents original images. convolutional encoder-decoder performs well image denoising task generation missing part good. model outperformed convolutional encoder decoder part generating missing part images. convolutional neural network good denoising that’s used encoder lstm decoder good sequence generation. image denoising active ﬁeld research area image processing. portion image missing almost algorithms produce missing part surrounding pixels. major portion image missing missing part cannot produced observing neighborhood pixels. produce missing part image knowledge object necessary. half digit missing algorithm needs understand digit looks like produce missing part keeping relevance current portion image. algorithm need learn generic shape digit job. cnn-cnn encoder decoder outperformed many models terms image denoising good producing missing part image memory. among deep learning models recurrent neural networks good sequence processing memory module. combine power model proposed cnn-lstm encoder decoder direct attention model good denoising images well producing lost part image. model trained mnist handwritten digit dataset massive distortion. goal model learn remove noise shape digit model done minimal errors removed long training time. achieved outstanding result model researching generative adversarial networks exemplary performance trying instead rmse loss layer might helpful converge model earlier better result.", "year": 2018}