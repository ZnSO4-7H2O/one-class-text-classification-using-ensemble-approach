{"title": "First-Pass Large Vocabulary Continuous Speech Recognition using  Bi-Directional Recurrent DNNs", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "We present a method to perform first-pass large vocabulary continuous speech recognition using only a neural network and language model. Deep neural network acoustic models are now commonplace in HMM-based speech recognition systems, but building such systems is a complex, domain-specific task. Recent work demonstrated the feasibility of discarding the HMM sequence modeling framework by directly predicting transcript text from audio. This paper extends this approach in two ways. First, we demonstrate that a straightforward recurrent neural network architecture can achieve a high level of accuracy. Second, we propose and evaluate a modified prefix-search decoding algorithm. This approach to decoding enables first-pass speech recognition with a language model, completely unaided by the cumbersome infrastructure of HMM-based systems. Experiments on the Wall Street Journal corpus demonstrate fairly competitive word error rates, and the importance of bi-directional network recurrence.", "text": "present method perform ﬁrst-pass large vocabulary continuous speech recognition using neural network language model. deep neural network acoustic models commonplace hmm-based speech recognition systems building systems complex domain-speciﬁc task. recent work demonstrated feasibility discarding sequence modeling framework directly predicting transcript text audio. paper extends approach ways. first demonstrate straightforward recurrent neural network architecture achieve high level accuracy. second propose evaluate modiﬁed preﬁx-search decoding algorithm. approach decoding enables ﬁrst-pass speech recognition language model completely unaided cumbersome infrastructure hmm-based systems. experiments wall street journal corpus demonstrate fairly competitive word error rates importance bi-directional network recurrence. modern large vocabulary continuous speech recognition systems complex difﬁcult modify. much complexity stems paradigm modeling words sequences sub-phonetic states hidden markov models hmm-based systems require carefullydesigned training recipes construct consecutively complex recognizers. overdifﬁculty building understanding modifying hmm-based lvcsr systems limited progress speech recognition isolated many advances related ﬁelds. recently graves jaitly demonstrated hmm-free approach training speech recognizer uses neural network directly predict transcript characters given audio utterance. approach discards many assumptions present modern hmm-based lvcsr systems favor treating speech recognition direct sequence transduction problem. approach trains neural network using connectionist temporal classiﬁcation loss function amounts maximizing likelihood output sequence efﬁciently summing possible input-output sequence alignments. using authors able train neural network predict character sequence test utterances character error rate wall street journal lvcsr corpus. impressive right results competitive existing hmm-based systems terms word error rate good word-level performance speech recognition often depends heavily upon language model provide prior probability likely word sequences. integrate language model information decoding graves jaitly ctctrained neural network rescore lattice n-best hypothesis list generated state-of-the-art hmm-based system. introduces potentially confounding factor n-best list constrains possible transcriptions signiﬁcantly. additionally results overall system still relies speech recognition infrastructure achieve ﬁnal results. contrast present ﬁrst-pass decoding results neural network language model decode scratch rather re-ranking existing hypotheses. describe decoding algorithm directly integrates language model ctc-trained neural networks search space possible word sequences. ﬁrst-pass decoding algorithm enables ctc-trained models beneﬁt language model without relying existing hmm-based system generate word lattice. removes lingering dependence hmm-centric speech recognition toolkits enables achieve fairly competitive results neural network n-gram language model. deep neural networks widely used neural network architecture speech recognition dnns fairly generic architecture classiﬁcation regression problems. hmm-based lvcsr systems dnns acoustic models predicting hmm’s hidden state given acoustic input point time. however hmm-dnn systems temporal reasoning output sequence takes place within rather neural network. training neural networks forces network model output sequence dependencies rather reasoning single time frames independently others. better handle temporal dependencies previous work used long short term memory networks. lstm neural network architecture originally designed prevent vanishing gradient problem sigmoidal dnns temporally recurrent deep neural networks work uses rdnns instead lstms neural network architecture. rdnns simpler overall dense weight matrix connections subsequent layers. simpler architecture amenable graphics processing unit computing signiﬁcantly reduce training times. recent work shows rectiﬁer nonlinearities dnns perform well dnn-hmm systems without suffering vanishing gradient problems optimization makes hopeful rdnns rectiﬁer nonlinearities able perform comparably lstms specially engineered avoid vanishing gradients. train neural networks using loss function maximum likelihood training letter sequences given acoustic features input. consider single utterance training example consisting acoustic feature matrix word transcription objective function maximizes probability reserve full exposition loss function formulation follows exactly previous work using predict characters utterance transcription loss function ﬁxed must next deﬁne compute predicted distribution output characters given audio features time many function approximators possible task choose basic model dnn. computes distribution using series hidden layers followed output layer. given input vector ﬁrst hidden layer activations vector computed compute subgradient parameters given training example thus utilize gradient-based optimization techniques. note formulation commonly used dnn-hmm models predict distribution senones instead characters. transcription many temporal dependencies sufﬁciently capture. timestep computes output using input features ignoring previous hidden representations output distributions. enable better modeling temporal dependencies present problem rdnn. rdnn select hidden layer temporally recurrent weight matrix compute layer’s hidden activations working rdnns found important modiﬁed version rectiﬁer nonlinearity. modiﬁed function selects clips large activations prevent divergence network training. setting maximum allowed activation results clipped rectiﬁer acting normal rectiﬁer function extreme cases. aside changes computations rdnn described like compute subgradient rdnn using method sometimes called backpropagation time. experiments always compute gradient completely time rather truncating obtain approximate subgradient. forward recurrent connections reﬂect temporal nature audio input perhaps powerful sequence transduction model brdnn maintains state forwards backwards time. model integrate information entire temporal extent input features making prediction. extend rdnn form brdnn choosing temporally recurrent layer brdnn creates forward backward intermediate hidden representation call respectively. temporal weight matrices propagate forward time backward time respectively. update forward backward components equations aside change recurrent layer brdnn computes output using equations rdnn. models compute subgradient brdnn directly perform gradient-based optimization. assuming input length output neural network again distribution possible characters alphabet includes blank symbol given audio input order recover character string output neural network ﬁrst approximation take argmax time step. character sequence maxc∈σ sequence mapped transcription collapsing repeat characters removing blanks. gives sequence scored reference transcription using wer. ﬁrst approximation lacks ability include constraint either lexicon language model. propose generic algorithm capable incorporating constraints. taking acoustic input time seek transcription maximizes probability overall probability transcription modeled product factors pnet given network given language model prior. practice prior given n-gram language model constraining thus down-weight include word insertion penalty alogrithm attempts word string maximizes equation algorithm maintains separate probabilities preﬁx pnb. respectively probability preﬁx ending blank ending blank given ﬁrst time steps audio input sets aprev anext maintain list active preﬁxes previous time step proposed preﬁxes next time step respectively. note size aprev never larger beam width overall probability preﬁx product word insertion term blank non-blank ending probabilities variable ℓend last character label sequence function converts string words segments sequence space character truncates characters trailing last space. incorporate lexicon language model constraint including probability whenever algorithm proposes appending space character setting last word lexicon otherwise probability acts constraint forcing character strings consist words lexicon. furthermore represent n-gram language model considering last words evaluate approach hour wall street journal news article dictation corpus training consists hours speech utterances. basic preparation transforming ldc-released corpora algorithm preﬁx beam search algorithm initializes previous preﬁxes aprev empty string. time step every preﬁx currently aprev propose adding character alphabet preﬁx. character blank extend preﬁx. character space incorporate language model constraint. otherwise extend preﬁx incorporate output network. active preﬁxes added anext. aprev include probable preﬁxes anext. output probable transcript although easily extended return n-best list. training test subsets follows kaldi speech recognition toolkit’s recipe however apply much text normalization used prepare transcripts training system. instead simply drop unnecessary transcript notes like lexical stress keeping transcribed word fragments acronym punctuation marks. safely discard much normalization approach rely lexicon pronunciation dictionary cause problems especially word fragments. language models standard models released corpus without lexical expansion. used ‘dev’ evaluation subset development report ﬁnal test performance ‘eval’ evaluation subset. subsets word vocabulary. language model used decoding constrained word vocabulary. input audio converted log-mel ﬁlterbank features frequency bins. context window frames concatenated form ﬁnal input vector size perform additional feature preprocessing feature-space speaker adaptation. output alphabet consists classes namely blank symbol letters punctuation marks well tokens noise space. table word error rate character error rate results bdrnn trained loss function. baseline decode choosing likely label timestep performing standard collapsing done training. compare baseline modiﬁed preﬁx-search decoder using dictionary constraint bigram language model. trained brdnn hidden layers hidden units total free parameters. third hidden layer network recurrent connections. weights network initialized uniform random distribution scaled weight matrix’s input output layer size nesterov accelerated gradient optimization algorithm described sutskever initial learning rate maximum momentum full pass training divide learning rate ensure overall learning rate decreases time. train network total passes training takes hours using python implementation. decoding preﬁx search beam size cross-validate held-out good setting parameters table shows word character error rates multiple approaches decoding trained brdnn. without sort language constraint quite high despite fairly cer. consistent observation many mistakes character level occur word appears mostly correct conform highly irregular orthography english. preﬁx-search decoding using word vocabulary prior possible character sequences results substantial improvement changes relatively little. comparing cers dictionary approaches demonstrates without characters mostly correct distributed across many words increases wer. large relative drop occur decode bigram performance bigram model demonstrates ctc-trained systems attain competitive error rates without relying lattice n-best list generated existing speech system. previous experiments dnn-hmm systems found minimal beneﬁts recurrent connections acoustic models. natural wonder whether recurrence especially bi-directional recurrence essential aspect architecture. evaluate impact recurrent connections compare train test cers rdnn brdnn models roughly controlling total number free parameters model. table shows results type architecture. variants recurrent models show substantial test improvements nonrecurrent model. note report performance total parameters smaller total number parameters used rdnn brdnn models. found larger dnns performed worse test suggesting dnns prone over-ﬁtting task. although brdnn fewer parameters rdnn performs better training test sets. suggests architecture drives improved performance rather total number free parameters. conversely bi-directional recurrence single recurrence small relative non-recurrent on-line speech recognition using singly recurrent network feasible without overly damaging performance. table train test character error rate results deep neural network without recurrence recurrent deep neural network forward temporal connections bi-directional recurrent deep neural network models hidden layers. rdnn hidden units hidden layer brdnn hidden units hidden layer keep total number free parameters similar models. models choose likely character timestep apply collapsing obtain character-level transcript hypothesis. presented decoding algorithm enables ﬁrst-pass lvcsr language model ctc-trained neural networks. decoding approach removes lingering dependence hmmbased systems found previous work. furthermore ﬁrst-pass decoding demonstrates capabilities ctc-trained system without confounding factor potential effects pruning search space provided lattice. results outperform best hmm-based systems corpus demonstrate promise ctc-based speech recognition systems. experiments brdnn simplify infrastructure needed create ctc-based speech recognition systems. brdnn overall less complex architecture lstms relatively easily made gpus since large matrix multiplications dominate computation. however experiments suggest recurrent connections critical good performance. bi-directional recurrence helps beyond single direction recurrence could sacriﬁced cases require low-latency online speech recognition. taken together previous work ctcbased lvcsr believe exciting path forward high quality lvcsr without complexity hmm-based infrastructure. glorot bordes bengio deep sparse rectiﬁer networks. aistats graves jaitly towards end-to-end speech recognition recurrent neural networks. icml hinton g.e. deng dahl g.e. mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition. ieee signal processing magazine hochreiter schmidhuber long short-term memory. neural computation maas hannun rectiﬁer nonlinearities improve neural network acoustic models.", "year": 2014}