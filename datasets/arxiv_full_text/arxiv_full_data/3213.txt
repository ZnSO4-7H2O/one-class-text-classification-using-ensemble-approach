{"title": "Boosted Markov Networks for Activity Recognition", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "We explore a framework called boosted Markov networks to combine the learning capacity of boosting and the rich modeling semantics of Markov networks and applying the framework for video-based activity recognition. Importantly, we extend the framework to incorporate hidden variables. We show how the framework can be applied for both model learning and feature selection. We demonstrate that boosted Markov networks with hidden variables perform comparably with the standard maximum likelihood estimation. However, our framework is able to learn sparse models, and therefore can provide computational savings when the learned models are used for classification.", "text": "explore framework called boosted markov networks combine learning capacity boosting rich modeling semantics markov networks applying framework video-based activity recognition. importantly extend framework incorporate hidden variables. show framework applied model learning feature selection. demonstrate boosted markov networks hidden variables perform comparably standard maximum likelihood estimation. however framework able learn sparse models therefore provide computational savings learned models used classiﬁcation. recognising human activities using sensors currently major challenge research. typically information extracted directly sensors either discriminative enough noisy infer activities occurring scene. human activities complex evolve dynamically time. temporal probabilistic models hidden markov models dynamic bayesian networks dominant models used solve problem however models make strong assumption generative process data generated model. makes representation complex sensor data diﬃcult possibly results large models. markov networks oﬀer alternative approach especially form conditional random ﬁelds crfs observation modelled freedom incorporate overlapping features multiple sensor fusion long-range dependencies model. discriminative nature underlying graphical structure make especially suitable problem human activity recognition. boosting general framework gradually improve performance weak learner popular version called adaboost forces weak learner focus hard-to-learn examples examples seen far. ﬁnal strong learner weighted weak learners added ensemble iteration. work boosting involves unstructured output except occasions work motivated boosting parameter estimation markov networks recent results shown close relationship boosting maximum likelihood estimation furthermore inherent capacity boosting feature selection integrated learning. motivated studies typical log-linear models imposed markov networks easily overﬁtted little data many irrelevant features overcome explicit feature selection either independently integrated learning previous work explored bmns showing promising results. bmns integrate discriminative learning power boosting rich semantics graphical model markov networks. paper explore alternative methods approach. handle hidden variables standard extend work altun variant multiclass boosting algorithm called adaboost.mr also suggest approximation procedure case intractable output structures. proposed framework demonstrated experiments boosting provide comparable performance mle. however since framework uses sparse features potential provide computational savings recognition. novelty paper two-fold present ﬁrst work applying boosting activity recognition derive boosting procedure structured models missing variables parameter update based quadratic approximation instead loose upper bounds speed learning. organisation paper follows. next section reviews related work. section presents concept boosted markov networks detailing boosting employed learn parameters markov networks. section goes details eﬃcient computation tree-structured general networks. describe experiments results applying model video-based human activity recognition indoor environment. last section discusses remaining issues. work closely related boosting applied learn parameters crfs using gradient trees objective function log-likelihood standard setting training based ﬁtting regression trees stage-wise fashion. ﬁnal decision function form linear combination regression trees. employs functional gradients log-loss manner similar logitboost whilst original gradients exponential loss adaboost thus learns model maximising likelihood data motivated minimising errors data directly. moreover indirectly solves structured model learning problem converting problem standard unstructured learning problem regression trees. contrast solve original structured learning problem directly without structured-unstructured conversion. addition paper incorporate hidden variables work does. another work integrates message passing algorithm belief propagation variant logitboost instead using per-network loss authors employ per-label loss details losses) marginal probabilities. similar also converts structured learning problem conventional unstructured learning problem. algorithm thus alternates message passing round update local per-label log-losses boosting round update parameters. however integrated algorithm made clear apply diﬀerent inference techniques fails converge general networks. also unclear extend method deal hidden variables. number attempts exploit learning power boosting applied structured models markov networks dynamic bayesian networks bayesian network classiﬁer hmms single node network often called classiﬁcation problem. network nodes number conﬁgurations exponentially large. assume conditional exponential distribution index clique deﬁned structure network. decomposition essential obtain factorised distribution vital eﬃcient inference tree-like structures using dynamic programming. straightforward check indeed upper bound seen includes loss proposed special case variables observed i.e. essentially adapted version adaboost.mr proposed iteration. thus conditional distribution updated along starting guessed distribution example uniform distribution. second assume log-linear model leading typical boosting process many rounds selects best weak hypothesis ﬁnds weight hypothesis minimise loss. translated context boosting-based learning searches best feature coeﬃcient ensemble αtfj loss minimised. still makes sense since learning incremental thus estimated distribution closer true distribution along way. indeed captures essence boosting round weak learner selects weak hypothesis best minimises following loss weighted data distribution noted boosting generic framework boost performance weak learner. thus build complex stronger weak learners using ensemble features later boosting framework. however stick simple weak learners features make algorithm compatible mle. employ regularisation term make consistent popular gaussian prior used conjunction markov networks. also maintains convexity original loss. regularised loss becomes lnon−reg either llog lexp boosting note regularisation term boosting bayesian interpretation setting simply constraint prevent parameters growing large i.e. model training data well clearly sub-optimal noisy unrepresentative data. eﬀect regularisation numerically diﬀerent losses cannot expect boosting. straightforward implementation optimisation sequentially iteratively searching best features parameters impractical number features large. partly objective function although tractable compute using dynamic programming tree-like structures still expensive. propose eﬃcient approximation requires vectors one-step evaluation. idea exploit convexity loss function approximating convex quadratic function using second-order taylor’s expansion. change update approximated made implicit assumption computation carried eﬃciently. however case general markov networks quantities interest involve summation exponentially large number network conﬁgurations. similar show dynamic programming exists tree-structured networks. however general structures approximate inference must used. distribution ﬁrst second derivative distribution hc)) partition functions form sum-product thus computed eﬃciently using single pass tree-like structure. ﬁrst second derivatives contains clique marginals estimated eﬃciently tree-like structure using downward upward sweep. general structures loopy belief propagation provide approximate estimates. details procedure omitted space constraints. update using upper bound second derivative rather conservative clear line search needed. moreover noted change newton update −.)/ upper bound change cauchy’s inequality second derivative bound thus term cauchy’s inequality restrict attention linear-chain structure eﬃcient computation suﬃcient learn video data capture. experiments reported here train model using along limited memory quasi-newton method proposed boosting scheme help line search satisfying amijo’s conditions. regularisation used features simplicity empirically selected. training data labels randomly given data slice sequence. performance measure report per-label error average -score distinct labels. paper evaluate boosting framework video sensor data. however framework applicable diﬀerent type sensors able fuse diﬀerent sensor information. observational environment kitchen dining room cameras mounted opposite ceiling corners observations sequences noisy coordinates person walking scene extracted video using background subtraction algorithm. data collected recent work consisting video sequences spanning three scenarios short meal snack normal meal. pick slightly smaller number sequences select sequences training another sequences testing. unlike build models thus separate models scenario learned tested. labels sequences ‘activities’ person performing example ‘going-fromdoor-to-fridge’. training partial label sets given leaving portion label missing. testing labels provided labels obtained decoding task compared ground-truth. labour. time slice extract vector elements observation correspond coordinates velocities speed respectively. observation features approximately normalised comparable scale. following previous boosting applications employ simple decision rules weak hypothesis returns real value certain conditions training data points otherwise. table performance three data sets activity-persistence features. short meal snack normal meal agthm algorithm itrs number iterations ftrs number selected features ftrs portion selected features. boosting studied section beam size i.e. round picks feature update weight. tables show performance training algorithms test data three scenarios three feature sets respectively. note inﬁnite regularisation factor means regularisation. general sequential boosting appears slower updates parameter time. activity persistence features feature compact informative enough attains reasonably high performance. compactness feature selection capacity almost eliminated leading poorer results compared mle. however situation changes radically activity transition feature context feature observation context small i.e. boosting consistently outperforms whilst maintaining partial subset features feature selection capacity demonstrated clearly context-based feature less features selected boosting short meal scenario less normal meal scenario. boosting performance still reasonable despite fact compact feature used. therefore clear computational advantage learned model used classiﬁcation. demonstrate section activity transition model learned boosting. transition feature sets studied previously separate transitions data transition model correctly learned. design another feature bridge activity-persistence transition feature set. similar activity persistence divided given short meal data activity transition matrix table parameters corresponding label-label features given tables learned boosting respectively. ﬁrst sight tempting select non-zero parameters associated transition features hence corresponding transition model. however transition features non-negative model actually penalises probabilities conﬁgurations activate negative parameters exponentially since diﬀerence boosting boosting penalises improbable transitions much severely thus leading much sharper decisions high conﬁdence. note data boosting learns much correct model error rate constrast without regularisation recall beam search described section allows weak hypothesis ensemble features. parameters updated parallel essentially similar thus feature selection performed. experiments diﬀerent beam sizes starting main focus paper full parameter increases number selected features also increases. however quite inconclusive ﬁnal performance. seems large update quite poor leading slow convergence. probably diagonal matrix resulting algorithm good approximation true hessian used newton’s updates. suggests exists good rather moderate beam size performs best terms convergence rate ﬁnal performance. presented scheme exploit discriminative learning power boosting methodology semantically rich structured model markov networks integrated boosted markov network framework handle missing variables. demonstrated performance newly proposed algorithm standard maximum-likelihood framework video-based activity recognition tasks. preliminary results structure learning using boosting indicates promise. moreover built-in capacity feature selection boosting suggests interesting application area small footprint devices limited processing power batteries. plan investigate select optimal feature online hand-held devices given processor memory battery status. although empirically shown successful experiments performance guarantee framework proven possibly following large margin approach asymptotic consistency statistics literature mle. application sensor networks intend explore methods incorporate richer sensory information weak learners build expressive structures model multi-level hierarchical activities.", "year": 2014}