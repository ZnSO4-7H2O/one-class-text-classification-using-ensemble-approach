{"title": "On Measuring and Quantifying Performance: Error Rates, Surrogate Loss,  and an Example in SSL", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "In various approaches to learning, notably in domain adaptation, active learning, learning under covariate shift, semi-supervised learning, learning with concept drift, and the like, one often wants to compare a baseline classifier to one or more advanced (or at least different) strategies. In this chapter, we basically argue that if such classifiers, in their respective training phases, optimize a so-called surrogate loss that it may also be valuable to compare the behavior of this loss on the test set, next to the regular classification error rate. It can provide us with an additional view on the classifiers' relative performances that error rates cannot capture. As an example, limited but convincing empirical results demonstrates that we may be able to find semi-supervised learning strategies that can guarantee performance improvements with increasing numbers of unlabeled data in terms of log-likelihood. In contrast, the latter may be impossible to guarantee for the classification error rate.", "text": "various approaches learning notably domain adaptation active learning learning covariate shift semi-supervised learning learning concept drift like often wants compare baseline classiﬁer advanced strategies. chapter basically argue classiﬁers respective training phases optimize so-called surrogate loss also valuable compare behavior loss test next regular classiﬁcation error rate. provide additional view classiﬁers’ relative performances error rates cannot capture. example limited convincing empirical results demonstrates able semi-supervised learning strategies guarantee performance improvements increasing numbers unlabeled data terms log-likelihood. contrast latter impossible guarantee classiﬁcation error rate. semi-supervised learning improve supervised learners exploiting potentially large amounts typically easier obtain unlabeled data however semi-supervised learners reported mixed results comes improvements always case semi-supervision results lower expected error rates. contrary severely deteriorated performances observed empirical studies theory shows improvement guarantees often provided rather stringent conditions principal suggestion forward chapter that dealing semi-supervised learning want study error rates classiﬁers produce also measure classiﬁers’ performances means intrinsic loss optimizing ﬁrst place. classiﬁcation routines optimize so-called surrogate loss training time—which many machine learning bayesian decision theoretic approaches propose also investigate loss behaves test provide alternative view classiﬁer’s behavior mere error rate cannot capture. fact though main example concerned semi-supervision would like argue learning scenarios similar considerations might beneﬁcial. instance active learning rather sampling randomly ones input data provide instances labels aims sampling systematic trying keep labeling cost similarly learn labeled examples possible. also interest compare error rates diﬀerent approaches achieve also surrogate losses compare techniques using underlying classiﬁers. similar remarks made learning scenarios like domain adaptation transfer learning learning data shift data drift last settings typically want compare classiﬁer trained source domain exploits additional knowledge target domain. simple idea underlying suggestion make that unless make particular assumptions generally cannot expect minimize error rate fact optimizing surrogate loss. surrogate loss large extent chosen computational reasons course hope that increasing training size minimizing lead improvements respect surrogate loss also respect expected error rate. cannot guaranteed strict however. start with classiﬁer’s error rate already rather unpredictably. general result devroye demonstrates instance classiﬁer exists classiﬁcation problem error rate converges arbitrarily slow rate bayes error classiﬁer universal approximator even guarantee bayes error ever reached. worse even case dealing model misspeciﬁcation error rates might even increasing numbers training samples leads rather counterintuitive result that cases expected error rates might actually improved throwing arbitrary samples training set. aforementioned considerations lead speculate kind generally valid expected performance guarantees possible semi-supervised learning aforementioned learning scenarios merely obtained terms surrogate loss classiﬁer hand. overall ideas line presented could deﬁnitely imagine that still takes position mere loss matters loss quantity minimized. however taking stance extreme cannot anything else directly minimize loss face computational complications less philosophical level claim loss also loss interested might actually application-relevant loss real applications seldom loss. fact true loss interest related particular classiﬁcation problem ultimately unknown. however basic reason studying surrogate loss intrinsic classiﬁer hand. matter fact lower loss really means model better sense estimated parameters closer optimal classiﬁer would obtain data labeled. particular setting semi-supervised learning decrease expected loss adding unlabeled data really indicates eﬀect—i.e. improved model ﬁt—is achieved adding labeled data. opinion seems least could semi-supervised setting. still mean claim surrogate loss quantity study give diﬀerent perspective problem various learning scenarios. finally point connection loss surrogate losses recent years attracted quite attention. papers investigating theoretical aspects particular classes loss functions also covering design surrogate losses contributions follow earlier works chapter illustrates point means classiﬁers optimize log-likelihood model data. clearly objective maximized taking minus likelihood would turn loss particular classiﬁers consideration nearest means classiﬁer classical linear discriminant analysis next section starts general reﬂection classiﬁers semi-supervised variations introduced. section reports results experiments comparing semi-supervised learners supervised counterparts empirically. ﬁnal section discusses ﬁndings light point would like make concludes chapter. semi-supervised feel need remark regular supervised versions still capable providing state-of-the-art performance. especially relatively high-dimensional small sample problems particularly good choice. rather recent examples demonstrating found bioinformatics applications also neurology pathology found high-impact journals ﬁelds oncology neuroscience general medicine pharmacology like. handful latest examples found similar remarks made though comparison relatively data available make work competitive level. like many recent contributions large number disciplines still employ classical decision rule e.g. like classiﬁer validity cannot aside outdated not-state-ofthe-art. fact classiﬁers around years more mean superseded. respect reader might also want consult relevant works semi-supervised versions based classical expectation maximization self-learning based so-called intrinsically constrained formulation. approaches introduced subsections follow. models underlying supervised based normality assumptions class-conditional probability density functions. speciﬁcally assumed across classes class means class priors vary class class. estimating variables maximum likelihood results well-known solutions priors means overall class covariance matrix becomes prior weighted estimates individual class covariance matrices. matrix classes also constrained multiple identity matrix. moreover priors ﬁxed equal classes. solution parameter estimation problem. note model necessarily unique course various ways formulate terms optimization problem. choice. self-learning self-training rather generally applicable approach semi-supervised learning initial step classiﬁer choice trained available labeled data. using trained classiﬁer unlabeled data assigned label. then next step labeled data added training classiﬁer retrained enlarged set. given newly trained classiﬁer relabel initially unlabeled data retrain classiﬁer updated labels. process iterated convergence i.e. labeling initially unlabeled data remains unchanged. foregoing gives basic recipe self-learning. many variations alternatives possible e.g. take fraction unlabeled data account retraining labeled decide relabel data etc. another well-known arguably principled semi-supervised approach treats absence certain labels missing data problem. time formulated terms maximum likelihood objective relies classical technique expectation maximization come solution although self-learning ﬁrst glance seem diﬀerent ways tackling semi-supervised classiﬁcation problem eﬀectively shows self-learners optimize objective similar observations made major problem self-learning strategies fact often suﬀer severely deteriorated performance increasing numbers unlabeled samples. behavior extensively studied various previous works typically caused model misspeciﬁcation i.e. setting statistical model actual data distribution. note contrast supervised setting classiﬁers capable handling mismatched data assumptions rather well adding labeled data typically improves performance. deﬁnitely suﬀer model misspeciﬁcation rather rigid low-complexity nature classiﬁer. ﬂexible still able model linear decision boundaries. hence also often misspeciﬁed. novel learn semi-supervised manner introduced. conceptual level idea exploit constraints known hold deﬁne relationships class-speciﬁc parameters classiﬁers certain statistics independent particular labeling. relationships automatically fulﬁlled supervised setting typically impose constraints semi-supervised setting. speciﬁcally following constraint holds number classes overall sample mean data diﬀerent sample means classes. total number training instances number observations class additional constraint holds relates standard estimates average class-conditional covariance matrix between-class covariance matrix estimate total covariance matrix covariance matrix models spread every class lda. supervised setting constraints need assumed automatically fulﬁlled. beneﬁt becomes apparent arrival unlabeled data. semi-supervised setting label-independent estimates improved. using accurate estimates however results violation constraints. fixing constraints properly adjusting labeldependent estimates become accurate expectation lead improved classiﬁers. detailed account enforce constraints refer related approaches). constrained estimation approach less generally applicable avoid severe deteriorations self-learning displays model match data model obviously good constrained semi-supervised generally still better terms error rate supervised equivalent. still also constrained setting results turn univocal either. error rates increase increasing number unlabeled samples consider insight issue paramount deeper understanding semi-supervised learning problem general. experiments used eight data sets machine learning repository classes. data sets used together basic speciﬁcations found table experiments similar performed experiments three nmcs done diﬀerent total labeled training sizes four unlabeled training sizes considered supervised semi-supervised ldas experiments carried labeled samples unlabeled training sizes nmcs. experiments study learning curves increasing numbers unlabeled data. every combination amount unlabeled objects labeled objects repetitions randomly drawn data used obtain accurate performance estimates. order able based limited amount samples provided data sets instances drawn replacement. basically means assume empirical distribution every data true distribution therefore allows measure true error rates true log-likelihoods. enabled properly study following introductory section constructed learning curves expected error rate expected log-likelihood figure shows error rates nmcs various data sets four training samples available. figure shows error samples hand. corresponding average log-likelihood curves found figures respectively. figure reports error rates obtained training samples using supervised semisupervised ldas. figure reports corresponding log-likelihoods. supervised classiﬁcation performance displayed black self-learners yellow constrained versions blue lighter bands around learning curves give indication standard deviations averaged curves providing idea statistical signiﬁcance diﬀerences curves. start with important note look error rates behaviors indeed quite disperse. classiﬁers constrained self-learned semi-supervised approaches examples error rates higher well lower averaged error rate regular supervised learners achieve. sometimes rather erratic behavior noted like self-learned wdbc figure constrained haberman transfusion figure last also behavior self-learned seem regular. overall performance self-learners disappointing wdbc labeled training samples overall convincing improvements observed. regarding expected error rates constrained approach fares signiﬁcantly better showing clear performance improvement least experiments experiments. still least classiﬁcation errors become signiﬁcantly figure mean error rates supervised self-learned constrained eight real-world data sets various unlabeled sample sizes total four labeled training samples. things drastically change indeed look log-likelihood curves. constrained approaches looking figure lower half figure story simple error rate deteriorations improvements erraticism could observed log-likelihood improves—i.e. increases—in every single case smooth monotonic signiﬁcant way. haberman maybe transfusion constrained approach improve convincingly cases. self-learned results still mixed. many case improvements still data sets likelihood decreases. notably self-learned labeled samples log-likelihood test data figure mean error rates supervised self-learned constrained eight real-world data sets various unlabeled sample sizes total labeled training samples. improves cases. monotonic behavior constrained approach displays. still curves less erratic error rates. nonetheless seems even quantify performance terms log-likelihoods critical towards self-learning em-based approaches. behavior deﬁnitely much regular terms surrogate loss performances worse supervised approach provides still occur. nevertheless results illustrate interesting study performance terms error rates also terms surrogate loss. irrespective possibility that ultimately might interested former. encouraging observe empirically seem semi-supervised learning schemes figure mean log-likelihood supervised self-learned constrained eight real-world data sets various unlabeled sample sizes total four labeled training samples. compare respective error rates figure guarantee improvements terms intrinsic surrogate loss. really nontrivial observation similar guarantees error rates seem question although illustration terms semi-supervised learning seems rather plausible similar observations made learning settings diﬀerent estimation techniques type classiﬁer relying surrogate loss compared. worthwhile considering behavior surrogate general provides view classiﬁer’s relative performance mere error rate cannot capture. figure mean log-likelihood supervised self-learned constrained eight real-world data sets various unlabeled sample sizes total labeled training samples. compare respective error rates figure", "year": 2017}