{"title": "Algorithms for Learning Kernels Based on Centered Alignment", "tag": ["cs.LG", "cs.AI"], "abstract": "This paper presents new and effective algorithms for learning kernels. In particular, as shown by our empirical results, these algorithms consistently outperform the so-called uniform combination solution that has proven to be difficult to improve upon in the past, as well as other algorithms for learning kernels based on convex combinations of base kernels in both classification and regression. Our algorithms are based on the notion of centered alignment which is used as a similarity measure between kernels or kernel matrices. We present a number of novel algorithmic, theoretical, and empirical results for learning kernels based on our notion of centered alignment. In particular, we describe efficient algorithms for learning a maximum alignment kernel by showing that the problem can be reduced to a simple QP and discuss a one-stage algorithm for learning both a kernel and a hypothesis based on that kernel using an alignment-based regularization. Our theoretical results include a novel concentration bound for centered alignment between kernel matrices, the proof of the existence of effective predictors for kernels with high alignment, both for classification and for regression, and the proof of stability-based generalization bounds for a broad family of algorithms for learning kernels based on centered alignment. We also report the results of experiments with our centered alignment-based algorithms in both classification and regression.", "text": "paper presents eﬀective algorithms learning kernels. particular shown empirical results algorithms consistently outperform so-called uniform combination solution proven diﬃcult improve upon past well algorithms learning kernels based convex combinations base kernels classiﬁcation regression. algorithms based notion centered alignment used similarity measure kernels kernel matrices. present number novel algorithmic theoretical empirical results learning kernels based notion centered alignment. particular describe eﬃcient algorithms learning maximum alignment kernel showing problem reduced simple discuss one-stage algorithm learning kernel hypothesis based kernel using alignment-based regularization. theoretical results include novel concentration bound centered alignment kernel matrices proof existence eﬀective predictors kernels high alignment classiﬁcation regression proof stability-based generalization bounds broad family algorithms learning kernels based centered alignment. also report results experiments centered alignment-based algorithms classiﬁcation regression. steps design learning algorithms choice features. choice typically left user represents prior knowledge critical poor choice makes learning challenging better choice makes likely successful. general objective work deﬁne eﬀective methods partially relieve user requirement specifying features. kernel-based algorithms features provided intrinsically choice positive-deﬁnite symmetric kernel function limit risk poor choice kernel last decade number publications investigated idea learning kernel data reduces requirement user specifying family kernels rather speciﬁc kernel. task selecting kernel family reserved learning algorithm which standard kernel-based methods must also data choose hypothesis reproducing kernel hilbert space associated kernel selected. diﬀerent kernel families studied past widely used convex combinations ﬁnite base kernels. however diﬀerent learning kernel algorithms introduced case including lanckriet knowledge past none succeeded consistently signiﬁcantly outperforming uniform combination solution binary classiﬁcation regression tasks. uniform solution consists simply learning hypothesis rkhs associated uniform combination base kernels. disappointing performance learning kernel algorithms pointed diﬀerent instances including many participants diﬀerent nips workshops organized theme well survey talk tutorial empirical results report conﬁrm observation. kernel families considered literature including hyperkernels gaussian kernel families non-linear families however performance reported families seem consistently superior uniform combination either. contrast theoretical side favorable guarantees derived learning kernels. general kernel families learning bounds based covering numbers given srebro ben-david stronger margin-based generalization guarantees based analysis rademacher complexity square-root logarithmic dependency number base kernels given cortes convex combinations kernels constraint. dependency theses bounds well others given constraints shown optimal respect number kernels. bounds generalize presented koltchinskii yuan context ensembles kernel machines. learning guarantees suggest learning kernel algorithms even relatively large number base kernels could achieve good performance. paper presents algorithms learning kernels whose performance consistent expectations based theoretical guarantees. particular seen experimental results several algorithms describe consistently outperform uniform combination solution. also surpass performance algorithm lanckriet classiﬁcation improve upon cortes regression. thus viewed ﬁrst series algorithmic solutions learning kernels classiﬁcation regression consistent performance improvements. learning kernel algorithms based notion centered alignment similarity measure kernels kernel matrices. used measure similarity base kernel target kernel derived output labels. deﬁnition centered alignment close uncentered kernel alignment originally introduced cristianini closeness superﬁcial however shall analysis several cases experimental results contrast notion alignment uncentered kernel alignment cristianini correlate well performance thus general cannot used eﬀectively learning kernels. note kernel optimization criteria similar centered alignment without normalization used authors centering normalization critical components deﬁnition. present number novel algorithmic theoretical empirical results learning kernels based notion centered alignment. section introduce analyze properties centered alignment kernel functions kernel matrices discuss beneﬁts. particular importance centering justiﬁed theoretically validated empirically. describe several algorithms based notion centered alignment section present algorithms work subsequent stages ﬁrst stage consists learning kernel non-negative linear combination base kernels; second stage combines kernel standard kernel-based learning algorithm support vector machines classiﬁcation kernel ridge regression regression select prediction hypothesis. algorithms diﬀer centered alignment used learn simplest straightforward implement algorithm selects weight base kernel matrix independently centered alignment matrix target kernel matrix. accurate algorithm instead determines weights jointly measuring centered alignment convex combination base kernel matrices target one. show accurate algorithm eﬃcient proving base kernel weights obtained solving simple quadratic program also give closed-form expression weights case linear necessarily convex combination. note alternative two-stage technique consists ﬁrst learning prediction hypothesis using base kernel learning best linear combination hypotheses. pointed section general ensemble-based techniques make richer hypothesis space used learning kernel algorithms. addition present analyze algorithm uses centered alignment select convex combination kernel hypothesis also present extensive theoretical analysis notion centered alignment algorithms based notion. prove concentration bound notion centered alignment showing centered alignment kernel matrices sharply concentrated around centered alignment corresponding kernel functions difference bounded term samples size result simpler directly bounds diﬀerence relevant quantities unlike previous work cristianini also show existence good predictors kernels high centered alignment classiﬁcation regression result justiﬁes search good learning kernel algorithms based notion centered alignment. note proofs given similar results classiﬁcation uncentered alignments cristianini erroneous. also present stability-based generalization bounds two-stage learning kernel algorithms based centered alignment second stage kernel ridge regression study application bounds case alignment maximization algorithm initiate detailed analysis stability algorithm finally section report results experiments centered alignmentbased algorithms classiﬁcation regression compare results ll-regularized learning kernel algorithms well uniform kernel combination method. results show improvement uniform combination one-stage kernel learning algorithms. also demonstrate strong correlation centered alignment achieved performance algorithm. notion kernel alignment ﬁrst introduced cristianini deﬁnition kernel alignment diﬀerent based notion centering feature space. thus start deﬁnition centering analysis relevant properties. mapping centered subtracting expectation forming denotes expected value drawn according distribution centering positive deﬁnite symmetric kernel function consists centering feature mapping associated thus centered kernel extended version cortes much additional material including additional empirical evidence supporting importance centered alignment description discussion single-stage algorithm learning kernels based centered alignment analysis unnormalized centered alignment proof existence good predictors large values centered alignment generalization bounds two-stage learning kernel algorithms based centered alignment experimental investigation single-stage algorithm. convenience matrix notation feature vectors denote inner product feature vectors similarly outer product including case dimension feature space inﬁnite case using inﬁnite matrices. simple vertical line going origin would expect alignment however alignment calculated using deﬁnition distribution admits diﬀerent expression. using real world data sets. instances problem noticed meila pothin richard suggested various data translation methods cristianini observed issue unbalanced data sets. table well figure give series empirical results several classiﬁcation regression tasks based data sets taken machine learning repository delve data sets table ﬁgure illustrate fact quantity measured respect several diﬀerent kernels always correlate well tive correlation expected good quality measure. centered notion alignment however shows good correlation along data sets always better correlated figure detailed view splice kinematics experiments presented table centered non-centered alignment plotted function accuracy apparent plots non-centered alignment misleading evaluating quality kernel. however centering kernel values opposed centering feature values directly relevant linear predictions feature space deﬁnition alignment precisely related that. also already shown section centering feature space implies centering kernel values since ij=ij kernel kernel matrix conversely however centering kernel imply centering feature space. example consider kernel marginals equal. section discusses several learning kernel algorithms based notion centered alignment. cases family kernels considered non-negative combinations ﬁrst stage algorithms determine mixture weights second stage train standard kernel-based algorithm example svms classiﬁcation regression combination kernel matrix associated learn hypothesis describe ﬁrst section simple algorithm determines mixture weight independently then section algorithm determines weights jointly selecting maximize alignment target kernel. brieﬂy discuss section relationship two-stage learning algorithms algorithms based ensemble techniques also consist stages. finally introduce analyze single-stage alignment-based algorithm learns simple eﬃcient method consists using training sample independently compute alignment kernel matrix target kernel matrix based labels choose mixture weight proportional base kernel matrices normalized respect frobenius norm independent alignment-based algorithm also viewed solution joint maximization unnormalized alignment deﬁned follows l-norm constraint norm interval however assuming kernel function labels bounded unnormalized alignment bounded well. lemma kernel. assume output label then following bounds hold malization frobenius norm base matrix. note optimization becomes trivial solved simply placing weight largest coeﬃcient whose corresponding kernel matrix largest alignment target kernel. independent alignment-based method ignores correlation base kernel matrices. alignment maximization method takes correlations account. determines mixture weights jointly seeking maximize alignment also suggested case uncentered alignment cristianini kandola later studied lanckriet showed problem solved qcqp follows present even eﬃcient algorithms computing weights showing problem reduced simple start examining case non-convex linear combination components negative show problem admits closed-form solution case. partially solution obtain solution convex combination. similar deﬁned l-norm instead shall however direction solution change respect choice norm. thus problem solved cases subsequently scaled appropriately. note that lemma umkµum thus hard problem equivalent solving hard margin problem thus solver also used solve similar problem non-centered deﬁnition alignment treated kandola optimization solution diﬀers requires cross-validation. also note solving problem require matrix inversion fact assumption invertibility matrix necessary maximal alignment solution computed using optimization proposition non-invertible case. optimization problem strictly convex however alignment solution unique. prediction hypothesis using kernel learning best linear µkhk. ensemble-based techniques make richer hypothesis space used learning kernel algorithms lanckriet ensemble techniques hypothesis αikkk diﬀerent constraints ﬁnal hypothesis form coeﬃcients decoupled αiβk solutions seem form fact diﬀerent since general coeﬃcients must obey diﬀerent constraints furthermore combination weights required positive ensemble case. present detailed theoretical empirical comparison ensemble learning kernel techniques elsewhere ﬁxed corresponding kernel matrix denote objective function dual optimization problem minimizeα∈af solved algorithm generally algorithm convex concave function convex then general form notation equation ﬁrst focus analyzing term maxµ∈m µbα. since last constraint convex standard lagrange multiplier theory guarantees exists following optimization last problem convex problem convex variable. case kernel ridge regression maximization admits closed form solution. plugging solution yields following convex optimization problem deﬁnes linear matrix inequality thus convex set. convex optimization solved eﬃciently using simple iterative algorithm cortes practice algorithm converges within iterations. existence accurate predictors classiﬁcation regression presence kernel good alignment respect target kernel. section presents stability-based generalization bounds two-stage alignment maximization algorithm whose ﬁrst stage described section shows existence predictors high accuracy classiﬁcation regression alignment kernel high. classiﬁcation thus without normalization. denote hypothesis deﬁned version result presented cristianini shawe-taylor elisseeﬀ kandola cristianini kandola elisseeﬀ shawe-taylor so-called parzen window solution non-centered kernels. however proofs incorrect since rely implicitly classiﬁcation existence good predictor based unnormalized alignment also shown. corresponding guarantees simpler depend term however unlike normalized case loss since ρu|≤ following theorem provides strong guarantees unnormalized alignment suﬃciently large close theorem denote error binary classiﬁcation. kernel supx∈x have section presents stability-based generalization bounds two-stage learning kernel algorithms. proof stability-based learning bound hinges showing learning algorithm stable pointwise loss learned hypothesis change drastically training sample changes slightly. refer reader bousquet elisseeﬀ full introduction. present learning bounds case second stage algorithm kernel ridge regression similar results given classiﬁcation using algorithms svms second stage. thus ﬁrst stage algorithms examine select second stage select hypothesis rkhs associated several results hold general speciﬁcally interested alignment maximization algorithm presented section ﬁrst analyze stability two-stage algorithms derive stabilitybased generalization bound precisely examine pointwise diﬀerence hypothesis values obtained point algorithm s)/t -norm collection matrices diﬀerence combination vector returned ﬁrst stage algorithm training respectively theorem samples size diﬀer exactly point associated hypotheses generated two-stage learning kernel algorithm constraint then bound parenthesized term separately. ﬁrst parenthesized term measures pointwise stability change single training point ﬁxed kernel. bounded using theorem cortes since application theorem bousquet elisseeﬀ bound loss uniform stability parameter directly yields statement. inequality presented holds two-stage learning kernel algorithms. deterresult given proposition used bound thus ∆µkc. note speciﬁc case alignment maximization algorithm solution obtained constraint also alignment maximizing solution found dependence explicit case constraint µ∈m. section compares performance several learning kernel algorithms classiﬁcation regression. compare alignment-based two-stage learning kernel algorithms align alignf well single-stage algorithm presented section following algorithms regularized method used regression since shown cortes outperform alternative regularized method similar settings. here regularization parameter additional regularization parameter kernel selection. experiments error measures reported -fold cross validation where trial three folds used training used validation testing. two-stage methods training validation data used stages learning. regularization parameter chosen grid search based performance validation regularization parameters l-svm l-krr ﬁxed since ratios important. explicitly algorithm scaling vector results scaled dual soluλ i)−y. turn primal solution αikµ equivalent solution algorithm uses regularization parameter equal without scaling thus suﬃces vary regularization parameter. case svms scale hypothesis change sign thus property shown hold. parameter zero experiments. chosen base kernels suﬃciently diﬀerent alignment performance. base kernel centered normalized trace one. test algorithms several data sets taken machine learning repository delve table summarizes results. regression compare l-krr method report rmse. classiﬁcation compare l-svm method report misclassiﬁcation percentage. general performance alignment well correlated. data sets improvement uniform combination well table error measures alignment values unif -stage align alignf kernels built linear combinations gaussian base kernels. choice listed labeled total size data used listed size. results shown standard figure scatter plot comparison diﬀerent kernel combination weight values obtained optimally tuned one-stage two-stage algorithms kinematics data set. one-stage kernel learning algorithms. note although align method often increases alignment ﬁnal kernel compared uniform combination alignf method gives best alignment since directly maximizes quantity. nonetheless align provides inexpensive heuristic increases alignment performance ﬁnal combination kernel. best knowledge ﬁrst kernel combination experiments alignment general base kernels. previous experiments seem dealt exclusively rank-one base kernels built eigenvectors single kernel matrix next section also examine rank-one kernels although generated spectral decomposition. experiments sentiment analysis data version blitzer books electronics kitchen. domain examples. regression setting goal predict rating classiﬁcation goal discriminate positive negative reviews rank-one kernels based frequent bigrams. base kernel corresponds bigram count base kernel normalized demonstrated performance l-svm method table also previously observed cortes sparse weight vector generally oﬀer improvement uniform combination rank-one setting. thus focus performance align compare unif one-stage learning methods. table shows align signiﬁcantly improves alignment error percentage unif also improves somewhat one-stage l-krr algorithm. evidence statistical signiﬁcance provided appendix table note that although sparse weighting provided l-svm improves alignment certain cases improve performance. presented series novel algorithmic theoretical empirical results learning kernels based notion centered alignment. experiments show consistent improvement performance alignment-based algorithms previous learning kernel techniques well straightforward uniform kernel combination diﬃcult surpass past classiﬁcation regression. algorithms described eﬃcient easy implement. algorithms presented paper available open-source library available www.openkernel.org. used variety applications improve performance. also gave extensive theoretical analysis provides number guarantees centered alignment-based algorithms methods. several algorithmic theoretical results presented extended learning settings. particular methods based similar ideas could used design learning kernel algorithms dimensionality reduction. notion centered alignment served similarity measure achieve results. note proving good alignment necessarily needed good classiﬁer theory empirical results suggest existence accurate predictors good centered alignment. diﬀerent methods based possibly diﬀerent eﬃciently computable similarity measures could used design eﬀective learning kernel algorithms. particular notion similarity suggested balcan blum could computed ﬁnite samples could used equivalent way. lemma denote kernel matrices associated kernel functions sample size according distribution assume then following perturbation inequality holds consider minimization shown proposition provide solution alignment maximization problem convex combination. matrix vector functions training sample emphasize dependency rewrite optimization sample table signiﬁcance tests general kernel combination results presented table entry indicates algorithm listed column signiﬁcantly better accuracy algorithm listed row. tables show results paired-sample one-sided t-tests pairs algorithms compared across data sets presented section regression classiﬁcation. entry tables indicates whether mean error algorithm listed column signiﬁcantly less mean error algorithm listed signiﬁcance level entry indicates signiﬁcant diﬀerence entry indicates null hypothesis cannot rejected. table indicates alignf method oﬀers signiﬁcant improvement unif data sets exception spambase signiﬁcantly improves compared onestage method data sets apart splice. table indicates align method signiﬁcantly improves uniform one-stage combination data sets apart regression setting improvement l-krr deemed signiﬁcant.", "year": 2012}