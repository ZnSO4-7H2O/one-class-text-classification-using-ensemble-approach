{"title": "Building competitive direct acoustics-to-word models for English  conversational speech recognition", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Direct acoustics-to-word (A2W) models in the end-to-end paradigm have received increasing attention compared to conventional sub-word based automatic speech recognition models using phones, characters, or context-dependent hidden Markov model states. This is because A2W models recognize words from speech without any decoder, pronunciation lexicon, or externally-trained language model, making training and decoding with such models simple. Prior work has shown that A2W models require orders of magnitude more training data in order to perform comparably to conventional models. Our work also showed this accuracy gap when using the English Switchboard-Fisher data set. This paper describes a recipe to train an A2W model that closes this gap and is at-par with state-of-the-art sub-word based models. We achieve a word error rate of 8.8%/13.9% on the Hub5-2000 Switchboard/CallHome test sets without any decoder or language model. We find that model initialization, training data order, and regularization have the most impact on the A2W model performance. Next, we present a joint word-character A2W model that learns to first spell the word and then recognize it. This model provides a rich output to the user instead of simple word hypotheses, making it especially useful in the case of words unseen or rarely-seen during training.", "text": "decision trees ﬁnite state transducers. training requires alignments acoustics sub-word units several iterations model training re-alignment. recent end-to-end models obviated need aligning sub-word units acoustics. popular models include recurrent neural networks trained connectionist temporal classiﬁcation loss function attention-based encoder-decoder rnns .these approaches truly still sub-word units hence require decoder separatelytrained perform well. recently-proposed direct acoustics-to-word models train single directly optimize pasr. eliminates need sub-word units pronunciation model decision tree decoder externally-trained signiﬁcantly simpliﬁes training decoding process. however prior research models shown models require several orders magnitude training data compared conventional sub-word based models. because models need sufﬁcient acoustic training examples word train well. example used hours speech train model vocabulary nearly words matched performance state-of-the-art state-based model. prior work explored models well-known english switchboard task presented initialization techniques effectively train models hours data. however still observed around absolute switchboard phone models hub- evaluation set. paper improves state-of-the-art models english conversational speech recognition. present training recipe achieves .%/.% switchboard/callhome subsets hub- evaluation compared previous best result .%/.% results several state-of-the-art models sub-word units decoder quantify gains made ingredient training recipe conclude model initialization training data order regularization important factors. next turn attention issue data sparsity training models. conventional solution problem uses sub-word unit-based model needs decoder during testing. alternative propose spell recognize model learns ﬁrst spell word character sequence recognize model retain advantages direct model also provides rich hypotheses user readable especially case unseen rarely-seen words. illustrate beneﬁts model out-of-vocabulary words. next section discusses baseline model section discusses proposed training recipe analysis impact individual ingredients results. section presents direct acoustics-to-word models end-to-end paradigm received increasing attention compared conventional subword based automatic speech recognition models using phones characters context-dependent hidden markov model states. models recognize words speech without decoder pronunciation lexicon externally-trained language model making training decoding models simple. prior work shown models require orders magnitude training data order perform comparably conventional models. work also showed accuracy using english switchboard-fisher data set. paper describes recipe train model closes at-par stateof-the-art sub-word based models. achieve word error rate .%/.% hub- switchboard/callhome test sets without decoder language model. model initialization training data order regularization impact model performance. next present joint wordcharacter model learns ﬁrst spell word recognize model provides rich output user instead simple word hypotheses making especially useful case words unseen rarely-seen training. conventional sub-word based automatic speech recognition typically involves three models acoustic model pronunciation model decision tree language model computes probability acoustics given sub-word units models probability ppmt word sequence given sub-word unit sequence acts prior word sequence hence ﬁnding likely sequence words given acoustics becomes maximum aposteriori optimization problem following probability density function pasr plmp prior experience training models conclude model initialization regularization important aspects training models. reason fact models attempt solve difﬁcult problem directly recognizing words acoustics single neural network. hence previously-proposed strategy initializing blstm phone blstm ﬁnal linear layer word embeddings gave gains. work started exploring several strategies spirit. experiments conducted pytorch following changes compared included delta delta-delta coefﬁcients slightly improved wer. hence total acoustic vector size stacking+decimation appending -dimensional i-vectors. initialized matrices samples uniform distribution inverse fan-in size input vector takes dimensionality input vectors layer account improved convergence compared strategy using place new-bob annealing kept ﬁxed learning rate every epoch. training data order important consideration sequence-tosequence models systems models operate entire input output sequences. training sequences padded length longest sequence batch order tensor operations. random sequence order batch creation memory-efﬁcient batches contain larger range sequence lengths lead wasteful padding average. hence sequences sorted batch creation. compare impact sorting input acoustic sequences order ascending descending length table results show ascending order gives signiﬁcantly better sorting descending order. intuition behind result shorter sequences easier train initially enables network reach better point parameter space. regarded instance curriculum learning momentum dropout conventional losses used training neural networks crossentropy require one-to-one mapping rows input feature vector matrix length-l output label sequence connectionist temporal classiﬁcation loss relaxes requirement considering possible alignments. introduces special blank symbol expands length-l target label sequence multiple length-t label sequences containing maps removal repeating symbols loss paths at/yt denote elements sequences. forward-backward algorithm efﬁciently computes loss function gradient back-propagated neural network next section describes baseline model used standard training data sets experiments. -hour contained hours segmented speech switchboard- audio transcripts provided mississippi state university. -hour contained additional hours fisher data collection hours callhome audio. extracted -dimensional logmel ﬁlterbank features frames every input speech signal. used stacking+decimation stacked successive frames dropped every alternate frame training. resulted stream -dimensional acoustic feature vectors half frame rate original stream. baseline models also used -dimensional i-vectors speaker resulting dimensional acoustic feature vectors. baseline model consisted -layer bidirectional lstm -dimensional input dimensional hidden layers forward backward directions. picked words least occurrences training data vocabulary. resulted -word output layer -hour system -output layer -hour system. noted initialization crucial training model. thus initialized blstm blstm trained phone model ﬁnal linear layer using word embeddings trained using glove table gives wers baseline phone models reported hub- switchboard callhome test sets. performed decoding models simple peak-picking output word posterior distribution removing repetitions blank symbols. phone model used full decoding graph observe -hour model lags behind phone model .%/.% absolute swb/ch much bigger -hour models. next discuss training recipe. velocity running weighted-sum gradient loss function constant usually learning rate experiments. also experimented dropout order prevent over-ﬁtting. table shows momentum dropout improve wer. contrast phone character-based models models large output size equal size vocabulary. prior research shown decomposing output linear layer size layers sizes speeds-up model training reduced number parameters. experimented projection layer size found speeds-up training factor also slightly improves attribute reduction over-ﬁtting. ﬁnally initialized model phone blstm gave improvements previous work expected initialization lowered despite presence useful strategies. dropout place trained bigger -layer model -dimensional blstm slight gains wer. initialized -hour model best -hour model used recipe training. table shows resulting system along previous best several published results. obtained signiﬁcant improvement .%/.% absolute compared previous result. also direct hybrid state-based models utilizing decoder noted callhome test challenging switchboard speakers latter appear training set. results callhome test especially good model matches best result obtained using hybrid blstm used exactly acoustic features. table table shows current previous-best model trained -hour switchboard+fisher set. also included several published results comparison. results data augmentation -hour training set. order understand impact individual components recipe conducted ablation study best -hour model. removed component recipe keeping others ﬁxed trained model decoded test sets. figure shows results experiment. observed changing training data order ascending descending order length resulted biggest drop performance. second biggest factor dropout excluding leads over-training heldout loss rises epoch choosing smaller -layer model instead -layer next largest drop wer. finally expected excluding projection layer least impact wer. despite strong results model give meaningful output user case words simply emits tag. problem switchboard task because rates swb/ch test sets .%/.% word vocabulary. tasks might affected limited vocabulary. solution next section discusses joint word-character model aims provide user richer output especially useful unseen rarely-seen words. advantage model directly emits word hypotheses forward-passing acoustic features withneeding decoder externally-trained however vocabulary ﬁxed words cannot recognized system. furthermore words infrequently seen training data recognized well network insufﬁcient training examples. prior approaches dealing limitations completely rely sub-word units. includes work character models n-grams rnn-transducer rnn-aligner contrast approach best worlds combining ease decoding model ﬂexibility recognizing unseen/rarely-seen words character-based model. natural candidate multi-task learning model containing shared lower network output networks corresponding tasks recognizing words characters. however network suitable purpose recognized word character sequences input speech utterance guaranteed synchronized time. loss impose time-alignment output sequence. proposed spell recognize model circumvents alignment problem presenting training examples contain words characters. allows continue leverage framework without resorting complex graph-based decoding methodologies employed example word-fragment based systems consider output word sequence black model uses following target sequence black lowercase alphabets character targets b-/edenote special preﬁxes word beginning end. hence model trained ﬁrst spell word recognize contrast model model single softmax words+characters output layer. experimented character sets model. ﬁrst simple character consisting total symbols alphabets digits whitespace punctuations. second character used includes separate character variants depending position word beginning middle end. also includes special symbols repeated characters e.g. separate symbol intuition behind character symbols capture context compared simple also disambiguate legitimate character repetitions double peaks emitted model. observed performance latter character slightly better using simple characters. hence present results case. higher rate -hour also contains several rare words. trained -layer blstm joint word character targets preparing output training sequences described previous section. initialized blstm using blstm. training recipe model presented previously. model permits three decodes word word predictions similar model. characters character predictions combine words bold oovs. observe model emits cases characters preceding contain spelling word. cases spelling incorrect e.g. scholarly colarly still meaningful user tag. future research errors using data-driven methods. conventional wisdom prior research suggests direct acousticto-word models require orders magnitude data sub-word unit-based models perform competitively. paper presents recipe train model -hour switchboard+fisher data performs at-par several state-ofthe-art hybrid end-to-end models using sub-word units. conclude data order model initialization regularization crucial obtaining competitive model .%/.% switchboard/callhome subsets test set. next present spell recognize model learns ﬁrst spell word recognize proposed model gives rich readable output user maintaining training/decoding simplicity performance model. show examples illustrating model’s beneﬁt utterances containing words. hinton deng dahl mohamed jaitly senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. graves fern´andez gomez schmidhuber connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks proc. icml hannun case casper catanzaro diamos elsen prenger satheesh sengupta coates deep speech scaling end-to-end speech recognition arxiv preprint arxiv. saon kurata sercu audhkhasi thomas dimitriadis ramabhadran picheny roomi hall english conversational telephone speech recognition humans machines proc. interspeech sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets proc. icassp siohan bacchiani fast vocabulary-independent audio search using path-based graph indexing proc. european conference speech communication technology", "year": 2017}