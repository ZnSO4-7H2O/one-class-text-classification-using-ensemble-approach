{"title": "Embodied Question Answering", "tag": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "abstract": "We present a new AI task -- Embodied Question Answering (EmbodiedQA) -- where an agent is spawned at a random location in a 3D environment and asked a question (\"What color is the car?\"). In order to answer, the agent must first intelligently navigate to explore the environment, gather information through first-person (egocentric) vision, and then answer the question (\"orange\").  This challenging task requires a range of AI skills -- active perception, language understanding, goal-driven navigation, commonsense reasoning, and grounding of language into actions. In this work, we develop the environments, end-to-end-trained reinforcement learning agents, and evaluation protocols for EmbodiedQA.", "text": "figure embodied question answering embodiedqa– tasks agents navigating rich environments order answer questions. embodied agents must jointly learn language understanding visual reasoning navigation succeed. virtual environments evaluation metrics novel deep reinforcement learning model task. concretely embodiedqa task illustrated fig. agent spawned random location environment asked question agent perceives environment ﬁrst-person vision perform atomic actions move-tforward backward right leftu turn-tright leftu. goal agent intelligently navigate environment gather visual information necessary answer question. embodiedqa challenging task subsumes several fundamental problems sub-tasks. clearly agent must understand language vision successful agent must also learn perform active perception agent spawned anywhere environment immediately ‘see’ pixels containing answer visual question thus agent must move succeed controlling pixels perceive. agent must learn visual input correct action based perception world underlying physical constraints understanding question. present task embodied question answering agent spawned random location environment asked question order answer agent must ﬁrst intelligently navigate explore environment gather information ﬁrst-person vision answer question challenging task requires range skills active perception language understanding goal-driven navigation commonsense reasoning grounding language actions. work develop environments end-to-end-trained reinforcement learning agents evaluation protocols embodiedqa. long-term goal build intelligent agents perceive environment communicate addition fundamental scientiﬁc goal artiﬁcial intelligence even small advance towards intelligent systems fundamentally change lives assistive dialog agents visually impaired natural-language interaction selfdriving cars in-home robots personal assistants. step towards goal-driven agents perceive communicate execute actions present task embodied question answering along common sense reasoning agent provided ﬂoor-plan environment must navigate egocentric views alone. thus must learn common sense similar humans navigate house never visited language grounding commonly noted shortcoming modern vision-and-language models lack grounding models often fail associate entities text corresponding image pixels relying instead dataset biases respond seemingly intelligently even attending irrelevant regions embodiedqa take goal-driven view grounding agent grounds visual question pixels sequence actions credit assignment reinforcement learning perspective embodiedqa presents particularly challenging learning problem. consider question ‘how many rooms contain chairs?’. agent discover question involves exploring environment visit ‘rooms’ detecting ‘chairs’ incrementing count every time ‘chair’ view stopping ‘rooms’ found? without knowing ‘room’ ‘chair’ looks like counting succeed agent must execute somewhat precise sequence hundreds inter-dependent actions learned reward signal says right answer anything else incorrect. task complex enough random action sequences result negative reward things wrong it’s difﬁcult agent know question misunderstood? agent detect chairs? agent navigate incorrectly? counting incorrect? ﬁrst step challenging space judiciously scope problem space environments question types learning paradigm allow augment sparse rewards imitation learning reward shaping speciﬁcally approach follows recent paradigm robotics deep training environments assumed sufﬁciently instrumented i.e. provide access agent location depth semantic annotations environment allow computing obstacle-avoiding shortest paths agent target location. crucially test time agents operate entirely egocentric vision alone structured representation environments access explicit localization agent mapping environment heuristic planning pre-processing handcoded knowledge environment task kind. agent entirety vision language navigation answering modules trained completely end-to-end sensory input goal-driven multi-room indoor navigation visual question answering contributions. make following contributions propose task embodiedqa agent spawned environment must intelligently navigate egocentric view gather necessary information answer visual questions environment. introduce novel adaptive computation time navigator decomposes navigation ‘planner’ selects actions ‘controller’ executes primitive actions variable number times returning control planner. agent decides seen required visual information answer question stops navigating outputs answer. initialize agents imitation learning show agents answer questions accurately ﬁne-tuning reinforcement learning allowed control navigation express purpose answering questions accurately. unlike prior work explicitly test demonstrate generalization agents unseen environments. evaluate agents housed rich interactive environment based human-designed indoor scenes suncg dataset diverse virtual environments enable test generalization agent across ﬂoor-plans objects room conﬁgurations without concerns safety privacy expense inherent real robotic platforms. introduce dataset visual questions answers grounded housed. different question types test range agent abilities scene recognition spatial reasoning color recognition embodiedqa task deﬁnition supports free-fom natural language questions represent question functional program programmatically generated executed environment determine answer. gives ability control distribution question-types answers dataset deter algorithms exploiting dataset bias provide ﬁne-grained breakdown performance skill. integrated housed renderer amazon mechanical turk allowing subjects remotely operate agent collected expert demonstrations questionbased navigation serve benchmark compare proposed future algorithms. code data made publicly available. place work context arranging prior work along axes vision language action viewed perspective embodiedqa presents novel problem conﬁguration single-shot videos captured goal-driven active agents. next contrast various slices space. vision language. like embodiedqa image video question answering tasks require reasoning natural language questions posed visual content. crucial difference lack control tasks present answering agents ﬁxed view environment agent must answer question never allowing agents actively perceive. contrast embodiedqa agents control trajectory fate good ill. task signiﬁcantly harder agent ﬂexibility avoid confusing viewpoints seek visual input maximize answer conﬁdence. visual navigation vision action. problem navigating environment based visual perception long studied vision robotics extensive survey). classical techniques divide navigation distinct phases mapping planning recent developments deep proposed fused architectures directly egocentric visual observations navigational actions model agents similar pixel-to-action navigators. distinction embodiedqa goals speciﬁed. visual navigation typically speciﬁes agent goals either implicitly reward function explicitly conditioning goal state representations including images target objects contrast embodiedqa speciﬁes agent goals language inherently compositional renders training separate policy every task infeasible. situated language learning language action. inspired classical work winograd number recent works revisited grounded language learning situating agents simple globally-perceived environments tasking goals speciﬁed natural language. form structure goal speciﬁcations range declarative programs simple templated commands free-form natural language instructions distinction embodiedqa course visual sensing environment partially observable i.e. agent access ﬂoor plan object labels attributes etc. must extract information purely ﬁrst-person visual sensing. embodiment vision language action. closest embodiedqa recent works extend situated language learning paradigm settings agents’ perceptions local purely visual change based actions setting refer embodied language learning. concurrent unpublished work hermann chaplot develop embodied agents simple game-like environments consisting rooms handful objects variable color shape. settings agents able learn understand simple x’/‘pick style commands would specify object similarly present embodied agents simple mazeworld task complete series instructions. contrast approaches embodiedqa environments consist multi-room homes densely populated variety objects furthermore instructions commands works low-level closely relate actions questions presented embodiedqa. interactive environments. number interactive environments commonly used community ranging simple grid-worlds game-like environments limited realism doom complex realistic environments stanford d-d). realistic environments provide rich representations world consist handful environments high difﬁculty creation. hand large sets synthetic environments programmatically generated; however typically lack realism work housed environment strikes useful middle-ground simple synthetic realistic environments. sec. details. hierarchical agents. model embodiedqa agents deep hierarchical agents decompose overall control problem higher-level planner invokes lowerlevel controls issue primitive actions. hierarchical modeling recently shown promise deep reinforcement learning setting model also draws inspiration work adaptive computation time models graves placed embodiedqa context dive deeper outlining environments agents embodied questions must answer. publicly release environments curated dataset code research nascent area. instantiate embodiedqa housed recently introduced rich interactive environment based indoor scenes suncg dataset concretely suncg consists synthetic scenes realistic room furniture layouts manually designed using online interior design interface scenes also further ‘veriﬁed’ realistic majority vote three human annotators. total suncg contains environments valid ﬂoors rooms containing million object instances unique objects different categories. housed converts suncg static dataset virtual environments agent navigate simple physical constraints fig. shows top-down views sample environments. full details found build dataset pruned subset environments housed. first consider environments three suncg annotators consider scene layout realistic. next ﬁlter atypical environments lacking ground small large finally exclude non-home environments requiring least kitchen living room dining room bedroom. would like pose questions agents test abilities ground language common sense reason visually navigate environments. example answering question ‘what color car?’ ostensibly requires grounding symbol ‘car’ reasoning cars typically outside navigating outside exploring found visually inspecting color. draw inspiration clevr dataset programmatically generate dataset grounded questions answers. gives ability control distribution question-types answers dataset deter algorithms exploiting dataset bias. queryable rooms objects. figs. show queryable rooms objects eqa. exclude objects rooms suncg obscure difﬁcult resolve visually merge semantically similar object categories singular plural forms object type reduce ambiguity. questions functional programs. question represented functional program executed environment yielding answer. functional programs composed small elementary operations operate sets room object annotations. number order evaluation elementary operations deﬁnes question type template. instance question type location template location ‘what room <obj> located in?’ ﬁrst object names second uniquepobjectsq retains objects single instance entire house. third queryplocationq generates question object. operation uniquepobjectsq particularly important generate unambiguous questions. instance conditioners house question ‘what room conditioner loget queried object enables shortest paths agent’s spawn location target expert demonstrations imitation learning stress static dataset rather curriculum capabilities would like achieve embodied communicating agents. question-answer generation dataset bias. principle ability automatically generate valid questions associated answers environment executing functional programs environment’s annotations provided suncg. however careful consideration needed make sure developed dataset balanced question types answers. ﬁlled question template execute functional form associated environments dataset compute answer distribution question. exclude questions normalized entropy answer distribution e.g. agent simply memorize refrigerators almost always kitchens question would discarded. also exclude questions occurring fewer four environments normalized entropy estimates unreliable. finally order benchmark performance agents human performance important questions tedious frustrating humans answer. count questions objects high counts distance questions object triplets withclear differences distance. thresholds room object blacklists manually based experience performing tasks. complete discussion question templates functional programs elementary operations various checks-andbalances found supplement. statistics. dataset consists question across environments referring total unique objects unique room types. dataset split train test overlap environments across splits. fig. shows dataset splits question type distribution. approximately questions asked environment average most fewest. relatively cated in?’ ambiguous potentially different answers depending instance referred question types. associated question type template generating question rooms objects attributes relationships. deﬁne nine question types associated templates <room> <obj> tags ﬁlled valid room object listed fig. fig. respectively. given question templates possible answers room names object names yes/no color names numbers questions test range agent abilities including object detection scene recognition counting spatial reasoning color recognition logical operators moreover many questions require multiple capabilities e.g. answering distance question requires recognizing room objects well reasoning spatial relation. furthermore agent must navigating environment room looking around room objects possibly remembering positions time different question types also require different degrees navigation memory. instance ‘how many bedrooms house?’ requires signiﬁcant navigation long-term memory question like ‘what color chair living room?’ requires ﬁnding single room living room looking chair. easily extensible include elementary operations question types templates needed increase difﬁculty task match development models. ﬁrst step challenging space experiments focus consists question types location color color_room preposition. virtue questions single tarfigure adaptive computation time navigator splits navigation task planner controller module. planner selects actions controller decides continue performing action variable number time steps resulting decoupling direction velocity strengthening long-term gradient ﬂows planner module. preposition questions many frequently occurring spatial relations easy resolve without exploration fail entropy thresholding. make entire generation engine publicly available. introduce proposed neural architecture embodiedqa agent. recall agent spawned random location environment receives question perceives single egocentric camera. importantly unlike prior work embodiedqa agent receive global structured representation environment task overview agent. agent natural modules vision language navigation answering trained sensory input goal-driven multi-room indoor navigation visual question answering. modules built largely conventional neural building blocks convolutional neural networks recurrent neural networks technical novelty model adaptive computation time rnns graves elegant approach allowing rnns learn many computational steps take receiving input emitting output back-propagating ‘halting’ layer. make idea navigation module cleanly separate decision direction velocity fig. illustrates different modules agent describe next. vision. agent takes egocentric images housed renderer input process consisting conv relu batchnorm maxpoolu blocks producing ﬁxed-size representation. strong visual system embodiedqa encode information object attributes semantics environmental geometry such pretrain multi-task pixel-to-pixel prediction framework treating encoder network train multiple network heads decode original values semantic class depth pixel supplementary material full model training details. language. agents also receive questions encode -layer lstms -dim hidden states. note learn separate question encoders navigation answering modules need focus different parts question. instance question ‘what color chair kitchen?’ ‘color’ irrelevant navigation ‘kitchen’ matters little question answering navigation. introduce novel adaptive computation time navigator decomposes navigation ‘planner’ selects actions ‘controller’ executes primitive actions variable number times returning control back planner. intuitively structure separates intention agent series primitive actions required achieve directive reminiscent hierarchical approaches division also allows planner variable time steps decisions strengthening long-term gradient ﬂows. formally denote planner timestamps nptq denote variable number controller steps. denote encoding image observed t-th planner-time n-th controller-step. instantiate planner lstm. thus planner maintains hidden state samples action tforward turn-left turn-right stop-navigationu question encoding. taking action planner passes control controller considers planner’s state current frame decide continue performing return control planner i.e. next frame. else steps reached control returned planner. instantiate controller feed-forward multi-layer perceptron hidden layer. intuitively planner encodes ‘intent’ state encoding chosen action controller keeps going visual input aligns intent planner. question answering. agent decides stop question answering module executed provide answer based agent observed sequence frames throughout trajectory. answering module attends last frame computes attention pooled visual encoding based image-question similarity combines lstm encoding question outputs softmax space possible answers. employ two-stage training process. first navigation answering modules independently trained using imitation/supervised learning automatically generated expert demonstrations navigation. second entire architecture jointly ﬁne-tuned using policy gradients. independent pretraining imitation learning. questions could asked embodiedqa natural ‘correct’ navigation required answer them. mentioned section virtue questions contain single target queried object allows shortest path agent’s spawn location target expert demonstration. navigation module trained mimic shortest path actions teacher forcing setting i.e. given history encoding question encoding current frame model trained predict action would keep shortest path. cross-entropy loss train model epochs. even imitation learning case essential train navigator distance-based curriculum. ﬁrst epoch backtrack steps target along shortest path initialize agent point full history trajectory spawned location. step back additional steps successive training epoch. train epochs total batch size ranging questions question answering module trained predict correct answer based question frames seen shortest path. apply standard cross-entropy training epochs batch size target-aware navigational fine-tuning. navigation answering modules result imitation learning perform well independent tasks poorly suited dealing other. speciﬁcally modules used following provided shortest path control navigator generalize poorly provide question answerer unhelpful views target rather force answering agent provide correct answers noisy absent views freeze ﬁne-tune navigator. provide types reward signals navigator question answering accuracy achieved navigation reward shaping term gives intermediate rewards getting closer target. speciﬁcally answering reward agent chooses stop answers correctly otherwise. navigational reward forward actions times change distance target object train agent reinforce policy gradients running average baseline answer reward. imitation learning setting follow curriculum increasing distance spawn target locations. training details. lstms -layered hidden state. adam learning rate clamp gradients incrementally load environments memory batch size imitation learning reinforce ﬁnetuning stages. forward step corresponds metres takes turns turn i.e. right left turn action leads change viewing angle. backward strafe motions allowed. snap continuous renderer space grid check obstacles. entire codebase publicly available. experiments results ultimate goal embodiedqa agent answer questions accurately. however important disentangle success/failure intermediate task navigation ultimate downstream task question answering. question answering accuracy. agent produce probability distribution possible answers report mean rank ground-truth answer answer list sorted agent’s beliefs mean computed test questions environments. navigation accuracy. evaluate navigation performance reporting distance target object navigation termination pdtq change distance table quantitative evaluation embodiedqa agents navigation answering metrics test set. ill-deﬁned cells marked reactive models don’t stopping action humans pick single answer drop-down list mean rank deﬁned distance metrics trivially deﬁned shortest paths since always target object design. resentation lstm encoding question predict next action. similar approach difference goal speciﬁed question encoding instead target image. note action space reactive baselines tforward turn-left turn-rightu stop token. test time model actions lstm+question. memoryless navigators. lstm navigator takes input encodings question current frame previous action predicts next action. note identical inputs/outputs navigator. purpose comparing ablation approach establish beneﬁt proposed planner-controller architecture. also compare ablated version baseline without question encoding input navigation oracles. compare oracles humannav* denotes goal-driven navigations workers remotely operating agent shortestpaths+vqa denotes question answering performance achieved answering module shortest path test time. table shows results baselines compared approach trained imitation learning approach ﬁne-tuned make observations baselines poor navigators. baselines methods negative i.e. farther target start. conﬁrms intuition embodiedqa indeed difﬁcult problem. figure sample trajectories act+q-rl agent projected ﬂoor plan on-path egocentric views. agent moves closer already visible objects potentially improving perception objects. note ﬂoor plan shown illustration available agents. target initial ﬁnal position pd∆q smallest distance target point episode pdminq. distances measured meters along shortest path target. also record percentage questions agent either terminates p%rtq ever enters p%rêq room containing target object. finally also report percent episodes agents choose terminate navigation answer reaching maximum episode length p%stopq. sweep difﬁculty task test time spawn agent actions away target report metric settings. navigation baselines. compare navigator number sophisticated baselines ablations. reactive cnn. feedforward network uses last-n frames predict next action. tried report worked best. note target-agnostic baseline purpose baseline check whether simply memorizing frames training environments generalizes test part career awards awards grant n--- grant n--- allen distinguished investigator award paul allen family foundation google faculty research awards amazon academic research awards education research grant nvidia donations views conclusions contained herein authors interpreted necessarily representing ofﬁcial policies endorsements either expressed implied u.s. government sponsor. navigators performs best. proposed navigator achieves smallest distance target navigation rl-ﬁnetuned navigator achieves highest answering accuracy. interestingly observe rl-ﬁnetuned agent gets closest target trajectory enters target room often closest target statistics qualitative analysis suggests rl-ﬁnetuned agents learn explore lower stopping rate often overshoot target. consistent observations literature embodiedqa overshooting behavior hurt question answering accuracy answering module attend frames along trajectory. behavior corrected including small negative reward action. shortest paths optimal vqa. number methods outperform shortestpath+vqa terms answering accuracy. shortest path clearly takes agent target object provide best vantage answer question. future work improved tracing methods appropriately frame target object termination. present embodied question answering task agent spawned random location environment asked question. order answer agent must ﬁrst intelligently navigate explore environment gather information ﬁrstperson vision answer question. develop novel neural hierarchical model decomposes navigation ‘planner’ selects actions direction ‘controller’ selects velocity executes primitive actions variable number times returning control planner. initialize agent imitation learning ﬁne-tune using reinforcement learning goal answering questions. develop evaluation protocols embodiedqa evaluate agent housed virtual environment. additionally collect human demonstrations connecting workers amazon mechanical turk environment remotely control embodied agents. code data infrastructure made publicly available. sec. presents question-answer generation engine detail including functional programs associated questions checks balances place avoid ambiguities biases redundancies. sec. describes models serve vision module embodiedqa model. describe model architecture along training details quantitative well qualitative results. recall question represented functional program executed environment yielding answer. section describe process detail. descriptions follow ‘entity’ refer either queryable room queryable object housed environment. functional forms questions. functional programs composed elementary operations described below selectpentityq fetches list entities environment. operation similar ‘select’ query relational databases. uniquepentityq performs ﬁltering retain entities unique example calling uniqueproomsq rooms r‘living_room’ ‘bedroom’ ‘bedroom’s given house return r‘living_room’s. blacklistptemplateq function operates list object entities ﬁlters pre-deﬁned list objects blacklisted given template. questions given template type corresponding blacklisted objects. example blacklist contains objects t‘column’ ‘range_hood’ ‘toy’u objects list function receives t‘piano’ ‘bed’ ‘column’u output blacklistpq function t‘piano’ ‘bed’u queryptemplateq used generate questions strings given template basis entities receives. example queryplocationq receives following object entities input r‘piano’ ‘bed’ ‘television’s produces question strings form room <obj> located <obj> t‘piano’ ‘bed’ ‘television’u. relatepq elementary operation used functional form preposition questions. given list object entities returns subset pairs objects t‘on’ ‘above’ ‘under’ ‘below’ ‘next to’u spatial relationship them. distancepq elementary operation used functional form distance comparison questions. given list object entities returns triplets objects ﬁrst object closer/farther anchor object second object. described elementary operations make functional forms explanations functional forms question template given below. categorize question types categories based objects rooms refer unambiguous object certain question types inquire object must unique unambiguous throughout environment. examples question types location color. example ‘what room piano located in?’ single instance ‘piano’ environment. hence functional forms location color following structure selectpobjectsq gets list objects house uniquepobjectsq retains objects unique thereby ensuring unambiguity. queryptemplateq function prepares question string ﬁlling slots template string. continuation above another question types talk objects rooms addition objects unique rooms also unambiguous. examples question types include color_room preposition distance. additional unambiguity constraint room question ‘what next bathtub bathroom?’ would become ambiguous bathrooms house. functional forms types given following structure functional form selectpobjectsq uniquepobjectsq blacklistplocationq queryplocationq selectpobjectsq uniquepobjectsq blacklistpcolorq querypcolorq selectproomq uniqueproomq selectpobjectsq uniquepobjectsq blacklistpcolorq querypcolor_roomq selectproomq uniqueproomq selectpobjectsq uniquepobjectsq blacklistpprepositionq relatepq querypprepositionq selectproomq uniqueproomq selectpobjectsq blacklistpexistenceq querypexistq selectproomq uniqueproomq selectpobjectsq blacklistpexistenceq queryplogicalq selectproomq uniqueproomq selectpobjectsq blacklistpcountq querypcountq selectproomq queryproom_countq selectproomq uniqueproomq selectpobjectsq uniquepobjectsq blacklistpdistanceq distancepq querypdistanceq ﬁrst operations sequence result list unambiguous rooms whereas next result list unambiguous objects rooms. note selectp¨q appears ﬁrst operation sequence used fetch rooms objects across entire house. however case selectpobjectq operates rooms returns objects found speciﬁc rooms unambiguous room ﬁnal question types ones rooms need unambiguous objects rooms referred not. examples question types existence logical count. evident asking existence objects counting them require object single instance. television living room?’ perfectly valid question even multiple televisions living room template structure simpliﬁed version selectproomsq uniqueproomsq selectpobjectsq table complete list question functional forms. checks balances. since goals benchmark performance agents human performance want avoid questions cumbersome human navigate answer. additionally would also like balanced distribution answers agent able simply exploit dataset biases answer questions effectively without exploration. section describes detail various checks place ensure properties. entropy+frequency-based filtering important ensure distribution answers question ‘peaky’ otherwise mode guess distribution unreasonably well baseline. thus compute normalized entropy distribution answers question. drop questions normalized entropy further also drop questions occur less environments entropy counts low-frequency questions reliable. object distance threshold consider triplets objects within room consisting anchor object difference distances object-anchor pairs least metres. avoid ambiguity ‘closer’/‘farther’ object distance comparison questions. collapsing object labels object types visually similar semantically hierarchical relation introduce unwanted ambiguity. cases collapse object labels manually selected labels ‘kettle’ ‘food’). rooms question types dataset room names question string generate questions rooms obscure esoteric names ‘loggia’ ‘freight elevator’ ‘aeration’ etc. names room referred might immediately obvious e.g. ‘boiler room’ ‘entryway’ etc. objects question template maintain list objects included questions. either tiny objects whose name descriptions vague e.g. ‘switch’ ‘decoration’ ‘glass’ ‘household appliance’ blacklists manually deﬁned based experiences performing tasks. table quantitative results autoencoder depth estimation semantic segmentation heads multi-task perception network. metrics reported held validation set. question answering module predicts agents’ beliefs answer given agent’s navigation. ﬁrst encodes question lstm last frames navigation computes product attention frames pick relevant ones. next combines attention-weighted image features question encoding predict softmax distribution answers. fig. order contrast human shortest-path navigations respect question answering evaluate model last frames human navigations collected amazon mechanical turk interface. mean rank ground truth answer setting attribute difference primarily mismatch system training shortest paths testing human navigations. shortest paths typically object interest ﬁlling majority view humans tend stop earlier soon correct answer discerned. such human views tend cluttered pose difﬁcult task module. fig. highlights difference contrasting last frames human shortest-path navigations across three questions environments. comprising visual system embodiedqa agents trained multi-task pixel-to-pixel prediction framework. encoder network transforms egocentric image housed renderer ﬁxed-size representation. decoding heads predict original values semantic class depth pixel. information regarding semantic class depth every pixel available renderer. range depth values every pixel lies range segmentation done classes. architecture. encoder network conv blocks comprising conv ﬁlters relu batchnorm maxpool. decoder branches network upsample encoder output spatial size original input image. number channels output decoder depends task head semantic segmentation depth autoencoder branches respectively. upsampling done using bilinear interpolation. architecture also skip connections conv layers. training details. cross-entropy loss train segmentation branch hybrid network. depth autoencoder branches trained using smooth- loss. total loss linear combination losses given overall_loss seg_loss ˆdepth_loss ˆreconstruction_loss. adam optimizer learning rate batch size hybrid network trained total epochs dataset training images renderer. quantitative results. table shows quantitative results. different decoding heads multi-task report results validation settings network trained tasks task independently segmentation report overall pixel accuracy mean pixel accuracy mean depth autoencoder report smooth- validation set. qualitative results. qualitative results images validation segmentation depth prediction autoencoder reconstruction shown figure figure qualitative results hybrid cnn. represents input image. every input image show reconstruction autoencoder head ground truth depth predicted depth well ground truth segmentation predicted segmentation maps. figure conditioned navigation frames question question answering module computes product attention last frames combines attention-weighted combination image features question encoding predict answer. figure answer distribution location template questions. represents question form ‘what room <obj> located in?’ shows distribution answers across different environments. blank spaces represent questions pruned result entropy+count based ﬁltering. figure answer distribution preposition template questions. represents question form ‘what next <obj>?’ shows distribution answers across different environments. blank spaces represent questions pruned result entropy+count based ﬁltering. figure answer distribution color template questions. represents question form ‘what color <obj>?’ shows distribution answers across different environments blank spaces represent questions pruned result entropy+count based ﬁltering.", "year": 2017}