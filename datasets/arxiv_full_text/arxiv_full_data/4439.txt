{"title": "Multi-level Residual Networks from Dynamical Systems View", "tag": ["stat.ML", "cs.CV"], "abstract": "Deep residual networks (ResNets) and their variants are widely used in many computer vision applications and natural language processing tasks. However, the theoretical principles for designing and training ResNets are still not fully understood. Recently, several points of view have emerged to try to interpret ResNet theoretically, such as unraveled view, unrolled iterative estimation and dynamical systems view. In this paper, we adopt the dynamical systems point of view, and analyze the lesioning properties of ResNet both theoretically and experimentally. Based on these analyses, we additionally propose a novel method for accelerating ResNet training. We apply the proposed method to train ResNets and Wide ResNets for three image classification benchmarks, reducing training time by more than 40% with superior or on-par accuracy.", "text": "chang∗ lili meng∗& eldad haber university british columbia xtract technologies inc. vancouver canada {bchangstat menglilics habermath}.ubc.ca deep residual networks variants widely used many computer vision applications natural language processing tasks. however theoretical principles designing training resnets still fully understood. recently several points view emerged interpret resnet theoretically unraveled view unrolled iterative estimation dynamical systems view. paper adopt dynamical systems point view analyze lesioning properties resnet theoretically experimentally. based analyses additionally propose novel method accelerating resnet training. apply proposed method train resnets wide resnets three image classiﬁcation benchmarks reducing training time superior on-par accuracy. deep neural networks powered many research areas computer vision natural language processing biology e-commerce deep residual networks variants wide resnets densenets among successful architectures. resnets authors employ identity skipconnections bypass residual layers allowing data previous layers directly subsequent layers. success resnet variants various applications several views unraveled view unrolled iterative estimation view dynamical systems view emerged interpret resnets theoretical analysis empirical results. views provide preliminary interpretations however deep understanding resnets still active on-going research topic dynamical systems view interprets resnets ordinary differential equations special kind dynamical systems opening possibilities exploiting computational theoretical success dynamical systems resnets. point view stable reversible architectures developed. however empirical analysis view done many phenomena removing layers leading performance drop explained dynamical systems view. work take steps forward complement dynamical systems view empirical analysis properties lesion studies. challenge deep resnets long training time. extremely time-consuming train large datasets imagenet deep resnets -layer networks take several days even weeks high-performance hardware acceleration. recently reversible residual networks consumes computational time reducing memory usage reconstructing activations exposes training time severe problem. inspired dynamical systems interpretation additionally propose simple effective multi-level method accelerating resnets training. demonstrate proposed multi-level training method resnets wide resnets across three widely used datasets achieving training time reduction superior on-par accuracy. resnets deep neural networks stacking simple residual blocks contain identity skip-connections bypass residual layers. residual block shown fig. written feature layer represents layer’s network parameters. referred residual module consisting convolutional layers. shown fig. network divided several stages; consists number residual blocks. ﬁrst block stage feature size halved number ﬁlters doubled. feature remains dimensionality subsequent blocks stage. success resnets popular competitions imagenet pascal microsoft coco emerged many successors instance densenet connects layers feature-map size. resnxt introduces homogeneous multi-branch architecture increase accuracy. unraveled view veit resnets interpreted unraveled view resnets viewed collection many paths data along input output. residual block consists residual module identity skip-connection; path deﬁned conﬁguration residual module enter skip. resnet residual blocks unique paths. lesion studies veit demonstrate paths resnets strongly depend behave like ensemble. residual block removed number paths reduced leaving half paths still valid explains resnets resilient dropping blocks. besides explicit ensemble view training resnets stochastic depth viewed ensemble networks varying depths implicitly. unrolled iterative estimation view resnets interpreted unrolled iterative estimation greff view level representation stays within stage. residual blocks stage work together estimate iteratively reﬁne single level representation ﬁrst layer stage provides rough estimate representation subsequent layers reﬁne estimate. implication view processing block incremental removing blocks mild effect ﬁnal results. based view jastrzebski provide analytical empirical results. dynamical systems view resnets interpreted discretization dynamical systems basic dynamics step linear transformation followed component-wise nonlinear activation function. behavior large dynamical systems often notoriously difﬁcult problem mathematics particularly discrete dynamical systems. similar gradient exploding/vanishing problem deep neural networks recurrent neural networks. imposing structural constraints dynamical systems hamiltonian systems conserve energy explored haber ruthotto chang however interpretation phenomenon deleting layers studied point view. major challenge deep resnets long training time. alleviate issue several attempts made. stochastic depth randomly drops entire residual blocks training bypassing transformations identity skip-connections; testing blocks use. block bypassed speciﬁc iteration need perform forward-backward computation. stochastic depth approximately training time could saved. figurnov reduce inference time residual networks learning predict early halting scores based image content. huang investigate image classiﬁcation computational resource limits test time. sparsenets simple feature aggregation structure shortcut paths bypassing exponentially growing number layers. mollifying networks start optimization easier objective function evolve training eventually goes back original difﬁcult optimize objective function. also interpreted form adaptive noise injection depends single hyperparameter. section ﬁrst provide brief introduction dynamical systems view resnets considered odes. based view provide empirical analysis explain intriguing properties phenomena resnets. pre-activation resnets residual module consists sets batch normalization relu convolutional layers. without loss generality conceptually parameter rewrite residual module residual block becomes time corresponds direction input output input feature initial convolution output feature softmax classiﬁer. thus problem learning network parameters equivalent solving parameter estimation problem optimal control problem involving parameter called step size discretization. original formulation resnets exist implicitly absorbed residual module call implicit step size. step size also explicitly expressed model output residual module multiplied added identity mapping. case hyper-parameter name explicit step size. section consider implicit step size. assume correspond input output feature maps network respectively time length ﬁxed. illustrated fig. resnets equally discretize using time points number blocks. thus time step thus obtain underlying depend words inversely proportional norm residual modules empirical analysis verify statement experiments resnets varying theory correct norm residual module inversely depths. proportional number residual blocks. take resnet- residual blocks total j=g. figure example. calculate average l-norm residual modules shows different resnet models. curve resembles reciprocal function consistent dynamical system point view. figure l-norm input output residual module resnet- models trained cifar- cifar-. norms evaluated test time. shows within residual block identity mapping contributes much residual module. words relatively small residual blocks. lesion studies resnets veit remove single multiple residual blocks shufﬂe residual blocks test time. surprisingly removing downsampling blocks modest impact performance block removal leads noticeable effect. according dynamical systems view removing residual block equivalent skipping time step squeezing adjacent steps one. operation change dynamical system. however show following effect negligible output residual module small enough. three consecutive time points suppose removed block corresponds time point removal time point discretization output residual module therefore effect removing block negligible small. empirical analysis empirically verify small train resnet- model cifar-/ plot l-norm input output residual module test time. shown figure except ﬁrst block stage later blocks tiny residual module outputs compared inputs provides explanation removing block notably impact performance. effect shufﬂing residual blocks analyzed similar way. outputs residual modules small block slightly modiﬁes feature map. therefore expect effect shufﬂing moderate especially later stages. network deep outputs residual modules close zero. residual module regarded feature reﬁnement. magnitude change large ﬁrst block; subsequent blocks slightly reﬁne features consistent unrolled iterative estimation view. figure illustration interpolation operation stage. insert residual block right existing block stage. model parameters including convolutional weights batch normalization parameters copied adjacent block interpolated blocks. that explicit step size halved. example interpolation stage three residual blocks numbered interpolation block become block respectively. three blocks inserted block whose parameters copied previous block respectively. table illustration multi-level method cycles. resnet model stages; residual blocks column represents number blocks stage. cycle training starts model using training steps ﬁrst interpolation happens model becomes step size halved similarly training steps later second interpolation doubles number blocks halves cycle lasts training steps. given connection resnets odes existing theories numerical techniques odes applied resnets. numerical analysis multi-grid methods algorithms solving differential equations using hierarchy discretizations varying step sizes. inspired multi-grid methods propose multi-level training method. idea multi-level method training start shallow network using large explicit step size training steps switch deeper network doubling number residual blocks halving step size operation called interpolation applies stages time. fig. illustrates interpolation operation stage. interpolation operation inserts residual block right every existing block copies convolutional weights batch normalization parameters adjacent block block. multi-level training process interpolations happen several times thus dividing training steps cycles. table gives example illustrate process. according dynamical systems view interpolating residual blocks halving step size solve exactly differential equation. therefore interpolation operation beginning cycle gives good initialization parameters. cycle regarded training process thus need reset learning rate large value beginning training cycle anneal learning rate cycle. adopt cosine annealing learning rate schedule within table number interpolations theoretical time saved relative full model. theoretically time saved monotonically increasing number interpolation increases marginal beneﬁt diminishing. ηmin ηmax represent minimum maximum learning rate respectively tcur accounts many training steps performed current cycle denotes total number training steps cycle. learning rate starts ηmax beginning cycle decreases ηmin cycle. since number residual blocks cycle half cycle theoretically running time cycle also half cycle take multi-level method cycles example trains shallow model steps switches deep model another steps. compared deep model trained steps multi-level method reduces training time generally uses multi-level method interpolations equally dividing training steps theoretically saves training time compared full model trained number total steps. table shows theoretical time saved. time saved monotonically increasing number interpolation increases marginal time saved diminishing. furthermore number interpolations large cycle might enough training steps. therefore trade-off efﬁciency accuracy. empirical results dynamical systems view presented sec. section evaluate efﬁcacy efﬁciency proposed multi-level method state-ofart deep learning architectures image classiﬁcation resnet wide resnet across three standard benchmarks. table main multi-level method results resnets different depths. model name corresponds multi-level method. multi-level training method achieves superior on-par accuracy last cycle model saving training time. unit training time minute. datasets three widely used datasets used evaluation cifar- cifar- details datasets data augmentation methods found appendix networks resnets wide resnets datasets. networks three stages number ﬁlters equal resnets wide resnets. based analysis table interpolations three cycles multi-level method order optimize trade-off efﬁciency accuracy. experiment multi-level model three cycles. comparison models trained number steps model architecture ﬁrst cycle model architecture last cycle. call ﬁrst cycle model last cycle model respectively. also cyclic learning rate schedule ﬁrst cycle model last cycle model fair comparison. models trained epochs. multi-level method models interpolated epochs. baseline models learning rate cycle also restarts epoch maximum minimum learning rates ηmin ηmax respectively. cifar- cifar- experiments mini-batch size stl- experiments mini-batch size weight decay momentum experiments evaluated machines single nvidia geforce gpu. networks implemented using tensorflow library present main results analysis section. experimental results found appendix theoretical time saved interpolations consistent experiment results. main results shown table resnets wide resnets respectively. report test error rate training time. compared ﬁrst cycle model multi-level method achieves much lower test error. compared last cycle model test error competitive slightly lower training time reduction result applies resnets wresnets across three datasets. interpolation resnet--i resnet--i show multi-level training method effective different network depths. train test curves resnets wide resnets shown fig. although training test accuracy temporarily drops start cycle performance eventually surpasses previous cycles. resnets wide resnets similar train test curves indicating multi-level training method effective different network widths. work study resnets dynamical systems view explain lesion studies view theoretical empirical analyses. based analyses develop simple effective multi-level method accelerating training resnets. proposed multi-level training method evaluated state-of-the-art residual network architectures across three widely used classiﬁcation benchmarks reducing training time similar accuracy. future work would like explore dynamical systems view resnets variants densenets. table main multi-level method results wide resnets different depths. model name corresponds multi-level method. multi-level training method achieves superior on-par accuracy last cycle model saving training time. unit training time minute. figure train test curves using multi-level method resnet--i wresnet-i cifar-/. models interpolated epoch dividing training steps three cycles. although training test accuracy temporarily drops start cycle performance eventually surpasses previous cycles. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. chang lili meng eldad haber lars ruthotto david begert elliot holtham. reversible architectures arbitrarily deep residual neural networks. arxiv preprint arxiv. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. adam coates andrew honglak lee. analysis single-layer networks unsupervised feature learning. proceedings fourteenth international conference artiﬁcial intelligence statistics andre esteva brett kuprel roberto novoa justin susan swetter helen blau sebastian thrun. dermatologist-level classiﬁcation skin cancer deep neural networks. nature michael figurnov maxwell collins yukun zhang jonathan huang dmitry vetrov ruslan salakhutdinov. spatially adaptive computation time residual networks. cvpr aaron oord sander dieleman heiga karen simonyan oriol vinyals alex graves kalchbrenner andrew senior koray kavukcuoglu. wavenet generative model audio. arxiv preprint arxiv. olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. ijcv yonghui mike schuster zhifeng chen quoc mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey google’s neural machine transarxiv preprint lation system bridging human machine translation. arxiv. cifar- cifar- cifar- dataset consists training images testing images classes image resolution. cifar dataset number training testing images cifar- classes. common data augmentation techniques including padding four zeros around image random cropping random horizontal ﬂipping image standardization stl- stl- dataset image classiﬁcation dataset classes image resolutions contains training images testing images. compared cifar-/ class fewer training samples higher image resolution. data augmentation cifar-/ except padding zeros around images. number parameters list number parameters model table interpolation figure gives conceptual illustration interpolation operation. implementing operation ﬁrst block stage requires special treatment. ﬁrst block changes size feature number channels thus shapes convolution batch normalization parameters different subsequent blocks. result cannot simply copy parameters block block instead parameters block copied block instead using implicit step size sec. also explicitly express model. residual block represents function instead becomes hyperparameter model. words residual block output residual module multiplied added identity mapping. explicit step size control. enables verify claim depth network affect underlying differential equation. empirical analysis following experiments cifar- cifar- according theory three models different discretizations differential equation result function values roughly across time interval discretized time steps respectively. figure plot l-norm residual blocks scale block number corresponding conceptual time points. seen ﬁgure three curves follow trend represents underlying function train test curves stl- show train test curves using multi-level training method resnet--i wide resnet--i fig. effect learning rate study effect ηmax ηmin cyclic learning rate plot effect test accuracy fig. empirically ηmax ηmin achieve best accuracy. increase maximum learning rate ηmax test accuracy increases ﬁrst decreases. potential explanation small learning rate figure train test curves using multi-level method resnet--i wresnet-i stl. although training testing accuracy temporarily drops start cycle performance eventually surpasses previous cycles. system learns slowly high learning rate system contains much kinetic energy unable reach deeper narrower parts loss function. effect resetting learning rate study effect resetting learning rate beginning cycle also multi-level training method without resetting learning rate throughout cycles. experiment setting described section train resnet--i cifar- cifar- stl-. setting repeated times obtain conﬁdence intervals. fig. shows results resetting learning rate beginning cycle gives better validation accuracy.", "year": 2017}