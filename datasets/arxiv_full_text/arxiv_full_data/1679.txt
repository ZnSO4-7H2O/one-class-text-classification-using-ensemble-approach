{"title": "The Expressive Power of Word Embeddings", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "We seek to better understand the difference in quality of the several publicly released embeddings. We propose several tasks that help to distinguish the characteristics of different embeddings. Our evaluation of sentiment polarity and synonym/antonym relations shows that embeddings are able to capture surprisingly nuanced semantics even in the absence of sentence structure. Moreover, benchmarking the embeddings shows great variance in quality and characteristics of the semantics captured by the tested embeddings. Finally, we show the impact of varying the number of dimensions and the resolution of each dimension on the effective useful features captured by the embedding space. Our contributions highlight the importance of embeddings for NLP tasks and the effect of their quality on the final results.", "text": "seek better understand information encoded word embeddings. propose several tasks help distinguish characteristics diﬀerent publicly released embeddings. evaluation shows embeddings able capture surprisingly nuanced semantics even absence sentence structure. moreover benchmarking embeddings shows great variance quality characteristics semantics captured tested embeddings. finally show impact varying number dimensions resolution dimension eﬀective useful features captured embedding space. contributions highlight importance embeddings tasks eﬀect quality ﬁnal results. distributed word representations capture semantic syntactic features words text corpus without human intervention language dependent processing. features embedding capture task independent make ideal language modeling. however embeddings hard interpret understand. despite eﬀorts visualizing word embeddings points high dimensional spaces carry information hard quantify. additionally publicly available embeddings generated multiple research groups different data training procedures understanding best learn paper investigate four public released word embeddings hlbl senna turian’s huang’s. context-free classiﬁcation tasks rather sequence labeling tasks isolate eﬀects context making decisions eliminate complexity learning methods. speciﬁcally work makes following contributions explore impact number dimensions resolution dimension quality information encoded embeddings space. shows minimum eﬀective space needed capture useful information embeddings. demonstrate importance word pair orientation encoding useful linguistic information. pair classiﬁcation tasks provide example pair performance greatly exceeds individual words. rest work proceeds follows first describe word embeddings consider. next discuss classiﬁcation experiments present results. finally discuss eﬀects scaling size embeddings space. signiﬁcant interest speeding generation process original language models evaluated using perplexity. argue perplexity good metric language modeling insightful well embeddings capture diverse types information. senna’s embeddings generated using model discriminating nonprobabilistic. training update read n-gram corpus concatenating learned embeddings words. corrupted ngram used replacing word middle random vocabulary. phrases model learns scoring function scores original phrases lower corrupted one. loss function used training hinge loss. shows embeddings able perform well several tasks absence features. tasks considered senna consist sequence labeling imply model might learn sequence dependencies. work enriches discussion focusing term classiﬁcation problems. turian duplicated senna embeddings diﬀerences; corrupt last word n-gram instead word middle. also show using embeddings conjunction typical features improves performance named entity recognition task. additional result shows embeddings similar eﬀect added existing task. gives wrong impression. work illustrates embeddings created equal signiﬁcant diﬀerences information captured publicly released model exist. mnih hinton proposed log-bilinear loss function model language. given n-gram model concatenates embeddings ﬁrst words learns linear model predict embedding last word. mnih hinton later proposed hierarchical log-bilinear model embeddings speed model evaluation training testing using hierarchical approach prune search space next word dividing prediction series predictions ﬁlter region space. language model eventually evaluate using perplexity. recent work mikolov investigates linguistic regularities captured relative positions points embedding space. results regarding pair classiﬁcation complementary. sentiment polarity lydia’s sentiment lexicon create sets words positive negative connotations construct -class sentiment polarity test. data size words. noun gender bergsma’s dataset compile list masculine feminine proper nouns. names corefer frequently she/he respectively considered feminine/masculine. strings corefer appear less times corpus consist multiple words ignored. total size words. synonyms antonyms wordnet extract synonym antonym pairs check whether part kind others. relation symmetric thus word pair together order-reversedcounterparts. diﬀerent word pairs. regional spellings collect words diﬀer spelling english american counterpart online source make task pair classiﬁcation task emphasize relative distances embeddings. pairs task. random classiﬁer frequent label classiﬁer. either give accuracy table shows examples -class evaluation tasks. classiﬁer asked identify classes term pair belongs classiﬁcation used logistic regression rbf-kernel linear non-linear classiﬁers. model-selection procedure running grid-search parameter space help development data. experiments written using python package scikit-learn term classiﬁcation tasks oﬀered classiﬁer embedding word input. pairwise experiments input consists embeddings words concatenated. average four folds cross validation used evaluate performance classiﬁer task. data used training development testing datasets respectively evaluation model selection. figure shows results -class term classiﬁcation tasks using logistic regression rbfkernel svm. surprising embeddings considered much better baseline even seemingly hard tests like sentiment detection. what’s more strong performance senna huang embeddings. senna embeddings seem capture plurality relationship better emphasis senna embeddings place shallow syntactic features. huang’s embeddings covers words dimensions. huang’s embeddings require context disambiguate prototype word. tasks context free average multiple prototypes single point space. emphasized models induced substantially diﬀerent training parameters. model vocabulary used diﬀerent context size trained diﬀerent number epochs training set. control variables outside scope study hope mitigate challenges running experiments vocabulary shared embeddings. size shared vocabulary words. previous plurality test senna embeddings signiﬁcantly outperformed huang’s. however regional spelling task huang’s embeddings outperform senna term pair classiﬁcation setups. believe huang’s approach building word prototypes signiﬁcant diﬀerences context provide signiﬁcant advantage task. note surprising neural language models capture relation synonym antonym. language modeling hlbl senna/turian corrupted examples favor words syntactically replace other; e.g. replace good easily excellent can. result syntactic interchangeability excellent close good embedding space. distributed word representation exist continuous space quite diﬀerent common language modeling techniques. beside powerful expressiveness demonstrated previously another advantage distributed representations size require less memory disk storage techniques. section seek understand exactly much space word embeddings need order serve useful features. also investigate whether powerful representation embeddings oﬀer result real value coordinates exponential number regions described using multiple independent dimensions. table shows examples words test datasets classifying using logistic regression senna embeddings. bottom rows show words classiﬁer conﬁdent classifying rows middle show words close decision boundary. example resilient could positive negative connotations text therefore close region words neutral polarized. senna best performing task plurality task. explains obvious contrast probabilities given words. words given almost probability bottom ones given almost results regional spelling task shown term-wise setup. despite performing well pair-wise spelling classiﬁer shows meaningful results. clearly notice british spellings words favor usage hyphens british kick-oﬀ hauliers re-exported bullet-proof initialled paralysed italicized exorcise fusing lacklustre subsidizing signaling hemorrhagic tumor homologue localize american positive world-famous award-winning high-quality achievement athletic resilient ragged discriminating stout lose bored bloodshed burglary robbery panic stone-throwing negative sometimes however choice pair classiﬁcation make quite diﬀerence results. figure shows classifying individual words according regional usage performs poorly. redeﬁne problem classiﬁer asked decide ﬁrst word pair words american spelling not. figure shows performance improves lot. hints words criteria separable hyper-plane subspace original embeddings space. instead draw similar conclusion pairs’ positions relative encodes information absolute coordinates relationship words often indicate relative diﬀerence vector corresponding points. reduce resolution real numbers make embeddings matrix. first scale integer values divide values number bits wish remove. finally scale values back preprocessing give values features classiﬁers. extreme case truncate bits values either figure shows remove bits performance embedding dataset drops reduced resolution equivalent regions encoded space. still huge resolution surprisingly seems suﬃcient solving tasks proposed. na¨ıve approximation trick interest simply take sign embedding values representation distributed word representations show promise improve supervised learning semi-supervised learning. practical advantages dense representations make ideal industrial applications software development. previous work mainly focused speeding training process metric evaluation perplexity. show metric able provide nuanced view quality. develop suite linguistic orifigure results pair-based tests. figure shows diﬀerence treating uk/us spellings single word problem using pair embeddings. figure shows results -class pair tests shaded areas represent improvements using kernel svm. bitwise truncation experiment indicates number dimensions could factor performance embeddings. experiment further embeddings datasets evaluate task performance reduced number dimensions. figure shows reducing dimensions drops accuracy classiﬁers signiﬁcantly across embedding datasets. another diﬀerence truncation experiment experiment truncation experiment preserve relationships captured non-linearities embedding space. linear ented tasks might serve part comprehensive benchmark word embedding evaluation. tasks focus words pairs isolation actual text. goal build useful classiﬁer much understand much supervised learning beneﬁt features encoded embeddings. succeed showing publicly available datasets diﬀer quality usefulness results consistent across tasks classiﬁers. future work address factors lead diverse quality. eﬀect training corpus size choice objective functions main areas better understanding needed. tasks simple diﬀerences among task performance shed light features encoded embeddings. showed addition shallow syntactic features like plural gender agreement signiﬁcant semantic partitions regarding sentiment synonym/antonym meaning. current tasks focus nouns adjectives suite tasks extended include tasks address verbs parts speech.", "year": 2013}