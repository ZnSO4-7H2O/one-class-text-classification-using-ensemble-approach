{"title": "Diverse Landmark Sampling from Determinantal Point Processes for  Scalable Manifold Learning", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "High computational costs of manifold learning prohibit its application for large point sets. A common strategy to overcome this problem is to perform dimensionality reduction on selected landmarks and to successively embed the entire dataset with the Nystr\\\"om method. The two main challenges that arise are: (i) the landmarks selected in non-Euclidean geometries must result in a low reconstruction error, (ii) the graph constructed from sparsely sampled landmarks must approximate the manifold well. We propose the sampling of landmarks from determinantal distributions on non-Euclidean spaces. Since current determinantal sampling algorithms have the same complexity as those for manifold learning, we present an efficient approximation running in linear time. Further, we recover the local geometry after the sparsification by assigning each landmark a local covariance matrix, estimated from the original point set. The resulting neighborhood selection based on the Bhattacharyya distance improves the embedding of sparsely sampled manifolds. Our experiments show a significant performance improvement compared to state-of-the-art landmark selection techniques.", "text": "high computational costs manifold learning prohibit application large point sets. common strategy overcome problem perform dimensionality reduction selected landmarks successively embed entire dataset nystr¨om method. main challenges arise landmarks selected non-euclidean geometries must result reconstruction error graph constructed sparsely sampled landmarks must approximate manifold well. propose sampling landmarks determinantal distributions non-euclidean spaces. since current determinantal sampling algorithms complexity manifold learning present efﬁcient approximation running linear time. further recover local geometry sparsiﬁcation assigning landmark local covariance matrix estimated original point set. resulting neighborhood selection based bhattacharyya distance improves embedding sparsely sampled manifolds. experiments show signiﬁcant performance improvement compared state-of-the-art landmark selection techniques. spectral methods central multitude applications machine learning statistics computer vision dimensionality reduction classiﬁcation segmentation limiting factor spectral analysis computational cost eigen decomposition. overcome limitation nystr¨om method commonly applied approximate spectral decomposition gramian matrix. subset rows/columns selected based eigen decomposition resulting small sub-matrix spectrum original matrix approximated. nystr¨om extension standard method matrix reconstruction crucial part subset selection. early work uniform sampling without replacement proposed. followed numerous alternatives including k-means clustering greedy approaches volume sampling recent comparison several approaches presented particular interest subset selection volume sampling equivalent determinantal sampling reconstruction error bounds exist. method however used practice high computational complexity sampling underlying distributions independently determinantal point processes proposed recently tracking pose estimation originally designed model repulsive interaction particles. dpps well suited modeling diversity point set. sampling algorithm dpps presented paper focus nonlinear dimensionality reduction large datasets manifold learning. popular manifold learning techniques include kernel isomap laplacian eigenmaps methods based kernel matrix pairwise relationships input points. spectral decomposition kernel matrix leads low-dimensional embedding points. large interested avoiding explicit calculation storage. contrast general rank-k matrix approximation possible taking nature non-linear dimensionality reduction account relating entries kernel matrix consequently propose perform sampling original point extract diverse landmarks. since input points non-euclidean space ignoring underlying geometry leads poor results. account non-euclidean geometry input space replace euclidean distance geodesic distance along manifold approximated graph shortest path distance. high complexity sampling deinput dimensionality subset cardinality algorithm restricts updates local enables sampling complex geometries. this together computational complexity makes algorithm well suited subset selection large scale manifold learning. consequence landmark selection manifold less densely sampled before making approximation neighborhood graphs difﬁcult. noted critical response approximation manifolds graphs topologically unstable. order improve graph construction retain local geometry around landmark locally estimating covariance matrix original point set. allows compare multivariate gaussian distributions bhattacharyya distance neighborhood selection yielding improved embeddings. shorter version work published techniques necessary compute leading eigenvectors. problem therefore also considered ﬁnding best rank-k approximation matrix optimal solution i-th largest eigen˜k matrix estimated nystr¨om method nystr¨om extension leads approximation matrix inverse replaced moore-penrose generalized inverse case rank deﬁciency. nystr¨om method leads minimal kernel completion conditioned selected landmarks reported perform well numerous applications challenge lies ﬁnding landmarks minimize reconstruction error large variety methods proposed selecting subset general matrix approximation step referred row/column selection matrix equivalent selecting subset points property important avoids explicit computation volume sampling subset selection theoretical advantages employ factorization kj×j exists positive semideﬁnite. based factorization volume simplex spanned origin selected points calculated equivalent volume parallelepiped spanned subset sampled proportional squared volume. directly related calculation determinant vol. ideas generalized based annealed determinantal distributions following ﬁrst analyze sampling determinantal distributions non-euclidean geometries. subsequently introduce eﬃcient algorithm approximate sampling manifolds. finally present approach robust graph construction sparsely sampled manifolds. described section sampling determinantal distributions used row/column selection. independently determinantal point processes introduced modeling probabilistic mutual exclusion present attractive scheme ensuring diversity selected subset. interesting construction dpps based l-ensembles date applications using determinantal point processes assumed euclidean geometry non-linear dimensionality reduction assume data points non-euclidean spaces swiss roll fig. evaluate performance dpps manifolds sample swiss roll. since know construction rule case invert display sampled points underlying space. result fig. shows inner part roll almost entirely neglected consequence taking manifold structure account. common solution geodesic distances approximated graph shortest path algorithm. consequently tion kernel matrix geodesic distance geo/σ). result shown fig. observe clear improvement diversity sampling also including points interior part swiss roll. seen last sections possible adapt determinantal sampling non-euclidean geometries error characterizations subset selection exist. however missing eﬃcient sampling algorithm dealing large point sets. approximative sampling based markov chain monte carlo method proposed circumvent combinatorial problem sampling proportional diagonal elements squared version leading additive error bounds algorithm proposed yields approximation volume sampling worsening approximation exact sampling algorithm dpps presented requires eigen decomposition state equivalent reformulation sampling algorithm algorithm first eigenvectors selected proportional magnitude eigenvalues stored columns figure sampling points lying manifold. show results standard sampling geodesic sampling eﬃcient sampling. note sampling performed plot underlying manifold reversing construction swiss roll. geodesic eﬃcient sampling yields diverse subset manifold. thogonal space since proj⊥bibi point almost surely selected twice. update formulation diﬀers orthonormal basis eigenvectors i-th basis vector lent provide diﬀerent point view algorithm. modiﬁcation however essential justiﬁcation proposed eﬃcient sampling procedure. following proposition characterizes behavior update algorithm. sampling determinantal distribution advantageous presented error bounds also makes intuitive sense selection diverse points yields accurate matrix reconstruction. computational complexity algorithm however similar even higher manifold learning spectral decomposition dense graph required whereas laplacian eigenmaps operate sparse matrices. approach efﬁcient sampling proposed works dual representation obtain hopefully smaller matrix considering work gaussian kernel matrix factorization corresponds inner product feadpp sampling algorithm. algorithm proceeds sampling points. note cardinality subset cannot original sampling algorithm introduction k-dpps iteration select point probabilgives rise initially fairly uniform distribution because points selected. update step proposition implies update probabilities selecting point changes sin. angle correlates strongly distance since initially same. selection eigenvectors algorithm likely choose ones largest eigenvalues. therefore draw analogy multidimensional scaling gaussian kernel selects eigenvectors. consequently vectors correspond low-dimensional embeddings produced multidimensional scaling original points characteristic property preserve pairwise distances original space embedding space permitting weights kernel matrix commonly used machine learning. illustrate similarity functions fig. focus welsch function experiments. subsequent iterations algorithm assumption similar norm vectors violated projection figure landmarks selected previous steps candidates current step k-means++ selects candidates equally likely since distance landmark proposed algorithm likely selects green point also black cross inﬂuences landmark selection yielding increase diversity. orthogonal space changes lengths. note however change locally restricted around currently selected point. since region less likely sampled subsequent iterations assumption still holds parts space contain probability. remark proposed algorithm bears similarities k-means++ replaces initialization uniform sampling k-means seeding algorithm. k-means++ seeding heuristic samples points based minimal distance previously selected landmarks. initially landmark selected proposed algorithm nearly identical update rule maximal neighborcause k-means++ bases selection distance nearest landmark landmarks inﬂuence probability space algorithm. consequently approach potentially yields subsets higher diversity illustrated fig. selecting landmarks next step spectral analysis consists building graph approximates manifold. common techniques graph construction include selection nearest neighbors ε-balls around node. approaches require setting parameter either number neighbors size ball crucial performance. setting parameter leads large number disconnected components many applications interested points connected obtain consistent embedding figure nearest neighbor selection bhattacharyya distance. covariance matrices estimated landmarks based original point set. points marked equal euclidean distance easily leading short cuts neighborhood graph. bhattacharyya distance considers local geometry smaller better approximating geodesic distance. nearest neighbors around selected point sampling result shown fig. obtain point high diversity covering entire manifold. illustrates proposed algorithm preserves characteristic complex geometries therefore appropriate subset selection context non-linear dimensionality reduction. second experiment quantify reconstruction error matrix completion formulated equ. compare eﬃcient sampling result uniform sampling k-means clustering uniform seeding performed best several studies including recent moreover compare selecting subset k-means++ seeding k-means++ algorithm seen landmark selection. construct gaussian kernel matrix points swiss roll bowl bowl dataset punctured sphere proposed sparsely sampled bottom densely shown supplementary material. select subsets varying size parameters swiss roll bowl. note improvement achieved adapting parameters size address issue propose technique graph construction takes initial distribution points account. landmark estimate covariance matrix around point ci+cj distance less likely produce short cuts across manifold points follow local geometry appear much closer points geometry. consequently replace euclidean distance neighborhood selection manifold learning bhattacharyya distance schematically illustrated fig. space requirements ﬁrst experiment show proposed eﬃcient sampling algorithm well suited subset selection non-euclidean spaces. beneﬁt scenario restrict update sampoints. choosing high values parameters leads short cuts yielding approximation manifold. appropriate selection parameters challenging sparsely sampled manifolds. problematic subset selection consecutive nystr¨om reconstruction dramatically reduce sampling rate limit computational complexity. table average reconstruction errors runs several sampling schemes uniform k-means uniform k-means++ seeding k-means++ eﬃcient dpp. subset sizes vary best results highlighted bold face. gorithm. compare graph construction euclidean bhattacharyya neighborhood selection. graph construction yields weight matrix wij. weight matrix sparse version kernel matrix kj×j sparsity controlled number nearest neighbors graph. generalized eigenvalue problem solved laplacian eigenmaps eigenvalues eigenvectors eigenvectors corresponding smallest non-zero eigenvalues constitute embedding l-dimensional space fig. shows low-dimensional embedding euclidean bhattacharyya neighborhood selection vary number nearest neighbors graph results show bhattacharyya based neighborhood selection much robust respect number neighbors. moreover notice euclidean neighborhood selection points seem cluster along stripes. observe eﬀect also bhattacharyya based embeddings much less pronounced. evaluated steps proposed approach separately present results scalable manifold learning image data. work datasets consisting handwritten digsecond consisting patches extracted medical images. dataset large apply manifold learning directly. consequently select landmarks discussed method perform manifold learning landmarks bhattacharyya distance nystr¨om method figure fish bowl dataset used experiments. punctured sphere sparsely sampled bottom densely top. uniform sampling likely pick points neglecting bottom part. subset. smaller subsets larger lead improvements. show average reconstruction error diﬀerent methods datasets table calculated diﬀerent runs. generally performance uniform sampling worst. k-means++ seeding yields better results. k-means improves results initializations k-means++ beneﬁts better seeding. diverse landmarks selected eﬃcient sampling leads lowest average reconstruction errors almost settings. third experiment perform manifold learning laplacian eigenmaps point consisting million points lying swiss roll. dataset large apply manifold learning directly. select landmark points eﬃcient sampling algorithm estimate local covariance matrices feed manifold learning alfigure selection landmarks million points. embedding landmarks laplacian eigenmaps. results euclidean bhattacharyya neighborhood selection shown. varying number nearest neighbors observe bhattacharyya based embeddings robust parameter setting higher quality. since diﬃcult quantify quality embedding labels associated image data perform nearest neighbor classiﬁcation dimensional space. expect advantages landmark selection scheme diverse landmarks spreads entire point embedding space helps classiﬁcation. reason also expect good performance kmeans++ seeding algorithm. note abstain pre-processing data applying advanced classiﬁers interested absolute classiﬁcation performance relative performances across diﬀerent landmark selection methods. pixels. neighborhood size embed images dimensional space laplacian eigenmaps. fig. shows statistical analysis repetitions several landmark selection schemes across diﬀerent numbers landmarks well bhattacharyya based graph construction. results show k-means++ seeding outperforms uniform initialization k-means++ cannot improve initialization. moreover observe signiﬁcant improvement classiﬁcation performance approximate sampling compared k-means++ seeding. finally bhattacharyya based graph construction improves results. acquired radiation therapy patients head neck tumors. fig. shows cross sectional slice segmentations three structures risk left parotid right parotid brainstem segmentation structures treatment planning high clinical importance ensure obtain radiation dose. exfigure classiﬁcation results mnist head neck data. comparison diﬀerent subset selection schemes varying numbers selected landmarks. bars indicate mean classiﬁcation performance error bars correspond standard deviation. indicate signiﬁcance levels respectively. means++ seeding outperforms uniform sampling. kmeans clearly improves uniform initialization. kmeans++ shows similar performance initial seeding. similar experiments mnist eﬃcient approximation leads signiﬁcantly better classiﬁcation results additional gain bhattacharyya based graph construction. addition signiﬁcant improvement runtime measurements showed unoptimized matlab code eﬃcient sampling runs approximately faster optimized version k-means. presented contributions crucial issues scalable manifold learning eﬃcient sampling diverse subsets manifolds robust graph construction sparsely sampled manifolds. precisely analyzed sampling determinantal distributions non-euclidean spaces proposed eﬃcient approximation sampling. algorithm well suited landmark selection manifolds probability updates locally restricted. further proposed local covariance estimation around landmarks capture local characteristics space. enabled robust graph construction bhattacharyya distance yielded dimensional embeddings higher quality. compared state-of-the-art subset selection procedures obtained signiﬁcantly better results proposed algorithm. tract image patches left right parotid glands brainstem surrounding region. interested classifying patches four groups outcome readily serve segmentation algorithms. work patches size patches extracted three scans. patches used training remaining ones testing. extract landmarks training patches embed dimensional space laplacian eigenmaps. fig. shows statistical analysis classiﬁcation performance repetitions various numbers selected landmarks selection schemes.", "year": 2015}