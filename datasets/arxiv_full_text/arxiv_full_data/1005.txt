{"title": "Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample  Guarantees for Oja's Algorithm", "tag": ["cs.LG", "cs.DS", "cs.NE", "stat.ML"], "abstract": "This work provides improved guarantees for streaming principle component analysis (PCA). Given $A_1, \\ldots, A_n\\in \\mathbb{R}^{d\\times d}$ sampled independently from distributions satisfying $\\mathbb{E}[A_i] = \\Sigma$ for $\\Sigma \\succeq \\mathbf{0}$, this work provides an $O(d)$-space linear-time single-pass streaming algorithm for estimating the top eigenvector of $\\Sigma$. The algorithm nearly matches (and in certain cases improves upon) the accuracy obtained by the standard batch method that computes top eigenvector of the empirical covariance $\\frac{1}{n} \\sum_{i \\in [n]} A_i$ as analyzed by the matrix Bernstein inequality. Moreover, to achieve constant accuracy, our algorithm improves upon the best previous known sample complexities of streaming algorithms by either a multiplicative factor of $O(d)$ or $1/\\mathrm{gap}$ where $\\mathrm{gap}$ is the relative distance between the top two eigenvalues of $\\Sigma$.  These results are achieved through a novel analysis of the classic Oja's algorithm, one of the oldest and most popular algorithms for streaming PCA. In particular, this work shows that simply picking a random initial point $w_0$ and applying the update rule $w_{i + 1} = w_i + \\eta_i A_i w_i$ suffices to accurately estimate the top eigenvector, with a suitable choice of $\\eta_i$. We believe our result sheds light on how to efficiently perform streaming PCA both in theory and in practice and we hope that our analysis may serve as the basis for analyzing many variants and extensions of streaming PCA.", "text": "work provides improved guarantees streaming principle component analysis given rd×d sampled independently distributions satisfying work provides o-space linear-time single-pass streaming algorithm estimating eigenvector algorithm nearly matches accuracy obtained standard batch method computes eigenvector empirical covariance analyzed matrix bernstein inequality. moreover achieve constant accuracy algorithm improves upon best previous known sample complexities streaming algorithms either multiplicative factor /gap relative distance eigenvalues results achieved novel analysis classic oja’s algorithm oldest popular algorithms streaming pca. particular work shows simply picking random initial point applying update rule ηiaiwi sufﬁces accurately estimate eigenvector suitable choice believe result sheds light efﬁciently perform streaming theory practice hope analysis serve basis analyzing many variants extensions streaming pca. ∗microsoft research india. email prajainmicrosoft.com berkeley. email chijincs.berkeley.edu ‡university washington. email shamcs.washington.edu §microsoft research england. email praneethmicrosoft.com ¶microsoft research england. email asidmicrosoft.com principal component analysis fundamental problems machine learning numerical linear algebra data analysis. commonly used data compression image processing visualization etc. desire perform large data sets case cannot afford single pass data alleviate issue popular line research past several decades consider streaming algorithms assumption data reasonable statistical properties signiﬁcant breakthroughs getting near-optimal streaming algorithms fairly specialized models e.g. spiked covariance work considers natural variants estimating eigenvector symmetric matrix mild assumptions concentration measure applies particular setting follows theorem essentially previous best sample complexity known estimating eigenvector unfortunately purely statistical claim algorithmically least concerns. second computing eigenvector empirical covariance matrix general require super linear time many attempts produce streaming algorithms space solve streaming problem knowledge previous methods either lose multiplicaanalysis order achieve constant accuracy applied tive factor either setting work answers question afﬁrmative showing succeed constant probability matching sample complexity theorem logarithmic terms small additive factors. interestingly achieved providing novel analysis classical oja’s algorithm perhaps popular algorithm streaming oja’s algorithm simplest algorithms would imagine streaming problem fact simplicity proposed neurally plausible algorithm. case comes distribution corresponds simply performing projected stochastic gradient descent objective function maximizing rayleigh quotient distribution ea∼dw⊤aw. well known mild conditions stepsize sequence oja’s maxkwk= algorithm asymptotically converges eigenvector covariance matrix however obtaining optimal rates convergence alone ﬁnite sample guarantees streaming quite challenging. best known results theorem factor known results streaming answer question afﬁrmative. particular that ﬁrst part depending exactly appears theorem second depending additional factor ﬁrst order term irrelevant once notably third part depending appear theorem arises entirely computational reasons setting allows single linear-time pass matrices theorem makes assumption. instance consider case means matrix bernstein tells sample sufﬁcient compute however evident compute using single pass note however rate lower order note theorems guarantee success probability boost probability copies algorithm success probability output geometric median solutions done nearly linear time detailes omitted here. beyond improved sample complexities believe analysis sheds light type step sizes oja’s algorithm converges quickly therefore illuminates efﬁciently perform streaming pca. note essentially assumed oracle sets step size sequence important question step size robust data data driven manner. moreover believe analysis fairly general hope extended make progress analyzing many variants occur theory practice. constant probability ignoring constant factors. recall error deﬁned analysis provides optimal error decay rate compared alecton block power method obtain tighter bound block power method assumptions made alecton different assumption; optimized bounds optimized setting. section concrete example analysis provides improvements consider three popular methods used computing ﬁrst batch method computes largest eigenvector empirical covariance uses wedin’s theorem matrix bernstein inequality second method alecton similar oja’s algorithm finally consider block-power method divides samples different blocks applies power iteration empirical estimate block. table comparison. stress results compare make different assumptions deﬁnition bounds stated best attempt adapt bounds setting deﬁnition next paragraph provides simple example demonstrates improvement result compared existing work. existing results computing largest eigenvector data covariance matrix using streaming samples divided three broad settings stochastic data arbitrary sequence data regret bounds arbitrary sequence data. stochastic data here data assumed sampled i.i.d. ﬁxed distribution. analysis oja’s algorithm well block power method alecton mentioned earlier setting. also obtained result restricted spiked covariance model. provides analysis modiﬁcation oja’s algorithm extra multiplicative factor compared ours. provides algorithm based shift invert framework obtains asymptotic error ours. however algorithm requires warm start vector already constant close eigenvector hard problem. arbitrary data setting data matrix provided arbitrary order. existing methods ﬁrst compute sketch matrix compute estimate eigenvector however direct application techniques stochastic setting leads sample complexity bounds larger multiplicative factor finally also provide methods eigenvector computation require multiple passes data hence apply streaming setting. regret bounds here step algorithm output estimate reward goal minimize regret w.r.t. algorithms regime mostly based online convex optimization applying setting would result loss multiplicative moreover typical algorithms setting memory efﬁcient bold lowercase letters used denote vectors bold uppercase letters denote matrices. symmetric matrices denotes condition x⊤ax x⊤bx deﬁne analogously. symmetric matrix positive semideﬁnite rest paper organized follows. section introduces basic mathematical facts used throughout paper also provides proof error bound standard batch method section provides overview approach analyzing oja’s algorithm provides main technical result paper. technical result used section prove running time oja’s algorithm justify choice step size. section presents proof main technical result. section concludes mentions interesting future directions. describe approach analyze oja’s algorithm. provide main theorem regarding convergence rate oja’s algorithm discuss proved. details proof deferred section theorem choose step sizes section primary difﬁculties analyzing oja’s algorithm broadly algorithm streaming choosing subtle potential function analyze method. analyze progress oja’s algorithm every iteration measuring quality risk ﬁrst iterations oja’s algorithm step actually yield orthogonal happens even typical best case future samples itself would still fail converge. short account randomness potential function difﬁcult show rapidly convergent algorithm catastrophically fail. another interpretation oja’s algorithm simply approximates performing step power method matrix fortunately analyzing step power method succeeds fairly straightforward show below v⊤bb⊤ev absolute constants. follows g⊤b⊤bg cev⊤bb⊤ev second inequality follows fact thatev⊤bg gaussian random variable varianceb⊤ev ilarly follows fact thatg⊤b⊤bg random variable trb⊤bconstant probability bnb⊤n relatively large trv⊥bnb⊤n relatively small matrix whose columns form orthonormal basis subspace orthogonal immediately alleviates issues catastrophic failure plagued analyzing long pick sufﬁciently small i.e. ηiai invertible. case bnb⊤n invertible bnb⊤n short long pick sufﬁciently small quantity wish bound trv⊥bnb⊤n bnb⊤n always ﬁnite. bnb⊤n split analysis several parts secactually bound bnb⊤n trv⊤ bnb⊤n small implies markov’s inequality tion first show etrv⊤ bnb⊤n small constant probability. then show bnb⊤n large trv⊤ varv⊤ bnb⊤n small. chebyshev’s inequality implies bnb⊤n large constant probability. putting together achieve main technical result regarding analysis oja’s method. devise roadmap proof fairly straightforward. theorem proved section theorem serves basis results regarding oja’s algorithm. next section show theorem choose step sizes achieve main results paper. theorem previous section leads main results provided here. theorem proof essentially consist choosing appropriate parameters efﬁciently apply theorem theorem theorems follow choosing respectively. ﬁrst provide several technical lemmas bounding expected behavior ultimately lemmas prove theorem begin straightforward lemma bounding rate increase w⊤tt−vv⊤ wtt−. order bound quantity ﬁrst bound expression arbitrary take expectation ﬁnally take expectation gt−. arbitrary ﬁxed symmetric matrix have work presented ﬁnite sample complexity asymptotic convergence rates classic oja’s algorithm top- component streaming match well known matrix concentration perturbation results computing eigenvector. fact asymptotically bound improves upon standard matrix bernstein bounds factor results tighter existing streaming results factor either analysis relied novel view algorithm technically fairly simple. hope analysis opens make progress many variants occur theory practice. particular believe following directions wide interest rayleigh quotient another standard metric measure optimality rayleigh quotient wn⊤σwn. converting bounds rayleigh quotient loses multiplicative factor compared optimal rate. direct analysis lose factor interesting open problem. results rayleigh quotient also help obtaining sample complexity guarantees independent eigenvalue gap. high probability work focused obtaining tight bounds error. however dependence results success probability quite suboptimal. many copies algorithm success probability output geometric median solutions done nearly linear time however conjecture tighter analysis using techniques might directly lead improved dependency success probability possibly help solve problems mentioned above.", "year": 2016}