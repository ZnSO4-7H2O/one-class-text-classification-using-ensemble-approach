{"title": "Improving the Performance of Neural Machine Translation Involving  Morphologically Rich Languages", "tag": ["cs.CL", "cs.LG", "cs.NE"], "abstract": "The advent of the attention mechanism in neural machine translation models has improved the performance of machine translation systems by enabling selective lookup into the source sentence. In this paper, the efficiencies of translation using bidirectional encoder attention decoder models were studied with respect to translation involving morphologically rich languages. The English - Tamil language pair was selected for this analysis. First, the use of Word2Vec embedding for both the English and Tamil words improved the translation results by 0.73 BLEU points over the baseline RNNSearch model with 4.84 BLEU score. The use of morphological segmentation before word vectorization to split the morphologically rich Tamil words into their respective morphemes before the translation, caused a reduction in the target vocabulary size by a factor of 8. Also, this model (RNNMorph) improved the performance of neural machine translation by 7.05 BLEU points over the RNNSearch model used over the same corpus. Since the BLEU evaluation of the RNNMorph model might be unreliable due to an increase in the number of matching tokens per sentence, the performances of the translations were also compared by means of human evaluation metrics of adequacy, fluency and relative ranking. Further, the use of morphological segmentation also improved the efficacy of the attention mechanism.", "text": "advent attention mechanism neural machine translation models improved performance machine translation systems enabling selective lookup source sentence. paper efﬁciencies translation using bidirectional encoder attention decoder models studied respect translation involving morphologically rich languages. english–tamil language pair selected analysis. first wordvec embedding english tamil words improved translation results bleu points baseline rnnsearch model bleu score. morphological segmentation word vectorization split morphologically rich tamil words respective morphemes translation caused reduction target vocabulary size factor also model improved performance neural machine translation bleu points rnnsearch model used corpus. since bleu evaluation rnnmorph model might unreliable increase number matching tokens sentence performances translations also compared means human evaluation metrics adequacy ﬂuency relative ranking. further morphological segmentation also improved efﬁcacy attention mechanism. rnns ﬁeld statistical machine translation revolutionised approaches automated translation. opposed traditional shallow models require memory neural translation models require small fraction memory used also neural translation models optimized every module trained jointly improve translation quality. said main downsides neural translation models heavy corpus requirement order ensure learning deeper contexts. application encoder decoder architectures translation and/or morphologically rich languages takes severe hit. language pair efﬁciency system depends major factors availability size parallel corpus used training syntactic divergence languages morphological richness word order differences grammatical structure etc. main differences languages stem fact languages similar english predominantly fusional languages whereas many morphologically rich languages agglutinative nature. nature morphologically rich languages structurally semantically morphologically rich languages sufﬁx added verb noun simply mean speciﬁc thing particular word sufﬁx commonly represents means exists inﬂectional forms noun verb base words conveying similar notions. example tamil least inﬂectional forms given verb forms inﬂectional forms noun. merged words carry information part speech tags tense plurality forth important analyzing text machine translation hidden meanings captured corresponding root words trained different units thereby increasing complexity developing systems complexities morphologically rich language several factors unique tamil make translation difﬁcult. availability parallel corpus tamil scarce. models ﬁeld english– tamil made translation corpora manually created purposes research. corpora available online use. another issue speciﬁc tamil addition sufﬁx characters included words language smoothness pronunciation. characters many different types; unique sufﬁx every consonant language. sufﬁxes degrade performance words different pronounciation-based sufﬁxes taken different words training. also take consideration existence different forms language used. traditionally deﬁned tamil pronunciations aren’t acoustically pleasing use. there’s linguistic syllables usage verbal communication time consuming. therefore exists forms language written form rigid structure syntax spoken form pace language given priority syntax correctness spelling. divide leads corpus different versions language increase vocabulary even words. evidently seen corpus sentences used bible traditional tamil sentences movie subtitles spoken tamil format. corpus selected experiment combination different corpora various domains. major part corpus made entam corpus corpus contained sentences taken parallel news articles english tamil bible corpus movie subtitles. also comprised tourism corpus obtained tdil corpus created tamil novels short stories aukbc anna university. complete corpus consisted sentences. fig. shows skinny shift heatmap representations relativity sentences terms sentence lengths. extra monolingual tamil corpus collated various online sources used wordvec embedding tamil target language enhance richness context word vectors. also used create language model phrasebased model. corpus contained sentences self-collected combining hundreds ancient tamil scriptures novels poems accessing websites popular online ebook libraries python using urllib package. since sources tamil text different encodings encoding scheme standardized utf- entirety monolingual parallel corpora using chardet package. corpora cleaned stray special characters unnecessary html tags website urls. word embeddings source target language sentences used initial vectors model improve contextualization. skip gram model wordvec algorithm optimizes vectors accounting average probability context words given source word. context window taken vectorization refers word corpus size training corpus terms number words. here probabily computed hierarchical softmax product transpose output vector input vector every pair entire vocabulary. processes negative sampling subsampling frequent words used original model aren’t used experiment model used translation implemented bahdanau bidirectional lstm encoder ﬁrst takes source sentence encodes context vector acts input decoder. decoder attention-based hidden states decoder input weighted hidden layer outputs encoder alongwith output previous hidden layer previously decoded word. provides contextual reference source language sentence neural machine translation models directly compute probability target language sentence given source language sentence word word every time step. model basic decoder without attention module computes probability target sentence given source sentence probabilities every word given every word that. attention-based model hand calculates weighted hidden layer outputs encoder every time step. encoder’s output context vector weighted help improve quality translation enabling selective source sentence lookup. probability computed function decoder’s output previous time step hidden layer vector decoder current timestep context vector attention mechanism context vector time step computed weighted output entire sentence using weight parameter number tokens source sentence refers value hidden layer encoder time step alignment parameter. parameter calculated means feed forward neural network ensure alignment model free difﬁculties contextualization long sentences single vector. feed forward network trained along neural translation model jointly improve performance translation. mathematically softmax output result feedforward network hidden state value decoder timestep encoder’s hidden layer annotation timestep concatenation forward reverse hidden layer parameters encoder used step compute weights attention mechanism. done enable overall context sentence opposed context previous words sentence every word consideration. fig. general architecture neural translation model without bidirectional lstm encoder. global attention mechanism preferred local attention differences structures languages cannot mapped efﬁciently enable lookup right parts source sentence. using local attention mechanism monotonic context lookup region around source word looked prediction target word impractical structural discordance english tamil sentences gaussian distributions facilitate local attention would also inefﬁcient existence various forms translations source sentence involving morphological structural variations don’t stay uniform entire corpus lstm cell. speciﬁed means gated mechanism designed ensure vanishing gradient problem prevented. lstm maintains hidden layer components cell vector actual hidden layer output vector cell vector ensured never reach zero means weighted previous layer’s cell vector regulated forget gate activation weighted input current timestep previous timestep’s hidden layer output vector hn−. combination similarly regulated input gate hidden layer output determined activation cell gate regulated output gate interplay vectors every timestep ensures problem vanishing gradients doesn’t occur. three gates also formed sigmoid weighted previous hidden layer output input current timestep output generated lstm’s hidden layer speciﬁed weighted softmax hidden layer output learnable parameters lstm cell weights biases lstm speciﬁed equations used decoder model. encoder uses bidirectional lstm cell hidden layer components −→en ←−en contribute output time step components sets lstm equations −→en every timestep computed ﬁrst timestep till token reached ←−en computed timestep backwards ﬁrst token reached. vectors components exactly lstm equations speciﬁed variation computation result. morphological segmentation used semi-supervised extension generative probabilistic model maximizing probability preﬁxrootpostﬁx recursive split words based exhaustive combination possible morphemes. details model speciﬁed extensively studied kohonen model parameters include morph type count morph token count training data morph strings counts. model trained maximizing maximum extension viterbi algorithm used decoding step based exhaustive mapping morphemes. account over-segmentation undersegmentation issues associated unsupervised morphological segmentation extra parameters used cost function likelihood cost function describes likelihood contribution annotated dataset cost function cw→a likelihood labeled data. decrease value cause smaller segments vice versa. takes care size discrepancies reduced availability annotated corpus compared training corpus complexities neural machine translation morphologically rich languages studied respect english tamil machine translation using lstm bidirectional encoder attention decoder architecture. compare baseline system phrase based system implemented using corpus. factored model source-side preprocessing kumar used reference translation language pairs. also additional monolingual tamil sentences used language model system. model used could split various modules expanded fig. process creating semantically meaningful word embeddings monolingual corpus tamil sentences used. gave vectors contextual richness increased size corpus opposed using bilingual corpus’ target side sentences experiment wordvec model trained using vector size ensure bulk limited memory used neural attention translation model. shown size used word vectorization gives similar results size performs close model -sized word vectors standard size used window size model trained worker threads simultaneously. batch size words used training. negative sampling nature morphologically rich languages important words don’t occur corpus. gensim wordvec toolkit used implement word embedding process input source target language sentences used training taken divided bucketed pairs sentences ﬁxed number sizes. relationship determined examining distribution words corpus primarily minimize number tokens sentence. heat number words english–tamil sentence pairs corpus revealed distribution centered around words region. therefore buckets region applied would enough number examples bucket pairs model learn sentences every bucket. exact scheme used rnnsearch models speciﬁed fig. bucketing scheme rnnmorph model involving morphs instead words simple shifted scheme used fig. every target sentence bucket count increased uniformly python extension morphological segmentation tool morfessor used experiment perform segmentation. annotation data tamil language collated released anoop kunchukkutan indic library used semi-supervised input model various computational constraints lack availability comprehensive corpora vocabularies english tamil languages rnnsearch model restricted respectively. vocabulary languages rnnmorph didn’t restricted actual number words corpus i.e. words english words tamil could accommodated training. words vocabulary test input output replaced universal token symbolizing unknown word. lstm hidden layer size training batch size vocabulary sizes languages together acted bottleneck. model nvidia geforce card cores memory allotment constrained limits gpu. therefore repeated experimentation determined batch size maximum hidden layer size possible size used. attempts reduce batch size resulted poor convergence parameters center around batch size models used layers lstm hidden units bidirectional encoder attention decoder. model used stochastic gradient descent optimization algorithm sampled softmax loss sample handle large vocabulary size target language model trained learning rate decay rate enforced manually. gradient clipping based global norm carried prevent gradients exploding going unrecoverable values tending towards inﬁnity. model described used tensorﬂow seqseq library. bleu metric computes bleu unigram bigram trigram bleu- modiﬁed precision values micro-averaged test sentences observed expected performance phrase-based model inferior rnnsearch model. baseline rnnsearch system reﬁned using wordvec vectors embed semantic understanding observed slight increase bleu scores. fig. plots bleu scores line graph visualization improvement performance. also -gram bleu scores various models plotted graph fig. agglutinative morphologically rich nature target language i.e. tamil morphological segmentation split words morphemes improved bleu precision values rnnmorph model. reasons large extent increase bleu score could attributed overall increase number word units sentence. since bleu score computes micro-average precision scores increase numerator denominator precision scores apparent increase number tokens morphological segmentation target language. thus numeric extent increase accuracy might efﬁciently describe improvement performance translation. ensure increase bleu score correlated actual increase performance translation human evaluation metrics like adequacy precision ranking values estimated table group native people well-versed english tamil languages acted annotators evaluation. collection samples sentences taken test results comparison. included randomized selection translation results ensure objectivity evaluation. fluency adequacy results rnnmorph results tabulated. adequacy rating calculated -point scale much meaning conveyed translation ﬂuency rating calculated based grammatical correctness -point scale comparison process rnnmorph rnnsearch wordvec modelsâ sentence level translations individually ranked other permitting translations ties ranking. intra-annotator values computed metrics scores shown table observed ranking kappa co-efﬁcient intra-annotator ranking rnnmorph model higher rnnsearch+wordvec model implying annotators found rnnmorph model produce better results compared rnnsearch wordvec model. learning rate decay training process rnnmorph model showcased graph fig. process done manually learning rate decayed speciﬁc epochs based observed stagnation perplexity.the rnnmorph model achieved saturation perplexities much earlier epochs rnnsearch wordvec model. conforms expected outcome morphological segmentation reduced vocabulary size target language words mere morphs. error function used sampled softmax loss ensure large target vocabulary could accommodated zoomed inset graph used visualize values error function rnnsearch wordvec rnnmorph models hidden layers. seen rnnmorph model consistently better terms perplexity values time steps. order demonstrate quality rnnmorph model attention vectors rnnsearch wordvec embedding rnnmorph models compared several good translations figs. observed reduction vocabulary size improved source sentence lookup quite extent. cell heatmap displays magnitude attention layer weight tamil word english word respective sentences. intensity black corresponds magnitude cell also attention vectors rnnsearch model wordvec embeddings tend attend token middle sentence leading incomplete translations. could fact tamil vocabulary english vocabulary taken training model opposed english tamil words rnnmorph model. large target vocabulary inadvertent consequence morphological richness tamil language. creates potential restriction accuracy model many inﬂectional forms word trained independent units. advantages morphological segmentation tamil text target vocabulary size decreased mere reduction helps improve performance translation occurrence unknown tokens reduced compared rnnsearch model. morphologically segmented vocabulary divided collection morphological roots inﬂections individual units. translations rnnmorph model repetitions phrases whereas repetitions occur much less frequently rnnsearch predictions. translations would make good results repetitions weren’t present parts sentence occur once. repetitions might increase general sequence length target sentences morphological segmentation. true target vocabulary size decreased morphological segmentation rnnmorph input units sentence makes demanding lstm’s memory units feed forward network attention model. additionally behavior could also attributed errors semi-supervised morphological segmentation complexities tamil language extent corpus. translation outputs rnnsearch wordvec morphvec models input sentences test demonstrate effectiveness using morphological segmentation tool morphemes changed sentence grammatically sound. also observed professors krishnan sobha developed machine-aided-translation system similar anusaakara english hindi system using small corpus transfer rules available au-kbc website balajapally developed example based machine translation system sentences english tamil kannada hindi transliterated text renganathan developed rule based system english tamil using grammar rules language pair. vetrivel used hmms align translate english tamil parallel sentences build system. irvine tried combine parallel similar corpora improve performance english tamil amongst languages. kasthuri used rule based system using transfer lexicon morphological analysis tools. anglabharathi developed kanpur system translating english collection indian languages including tamil using like structures create pseudo target convert indian languages variety hybrid approaches also used english–tamil combinations rule based interlingua representations statistical machine translation took english–tamil system research desirable properties language independence better generalization features reduced requirement linguistic expertise various enhancement techniques external system also proposed improve performance translation using morphological post processing techniques encoder decoder models machine translation shown good results languages similar grammatical structure. deep systems performing better shallow models recently availability computational resources hardware making feasible train models. ﬁrst models came model used lstm encoder decoder model context vector output encoder every decoder unit along previous word output reached. model used score translation results another system. sutskever created similar encoder decoder model decoder getting context vector ﬁrst word target language sentence. that decoded target outputs inputs various time steps decoder. major drawback models size context vector encoder static nature. sized vector expected represent sentences arbitrary length impractical came long sentences. next breakthrough came bahdanau variable length word vectors used instead context vector weighted inputs given decoder. enabled selective lookup source sentence decoding known attention mechanism attention mechanism analysed luong made distinction global local attention means scores attention vectors. gaussian distribution monotonic lookup used facilitate corresponding local source sentence look-up. thus seen morphological segmentation morphologically rich language translation helps performance translation multiple ways. thus machine translation involving morphologically rich languages ideally carried morphological segmentation. translation carried morphologically rich languages languages’ sentences individually segmented based morphology. true morphologically rich languages schemes languages process agglutination might different case mapping units would difﬁcult without segmentation. drawback morphological segmentation increase complexity model increase average sentence lengths. cannot avoided essential enable correspondence sentences languages simple fusional language. even increase average sentence length attention models developed ensure correctness translation long sequences good involving morphologically rich languages. another point note morphologically rich languages like tamil generally lesser number words sentence languages like english inherent property agglutination. model implemented paper includes source-side morphological segmentation include target side morphological agglutination give back output words rather morphemes. order implement end-to-end translation system morphologically rich languages morphological generator essential output units translation cannot morphemes. model implemented enhanced means better corpus generalize domain speciﬁc source sentences. also better would result better allocation hidden layer sizes batch sizes thereby possibly increasing scope accuracy learning translation model. although directly related machine translation novel encoder– decoder architecture proposed rocktaschel natural language inference used same. model fuses inferences every individual word summarizing information step thereby linking hidden state encoder decoder means weighted trained optimization. would like thank anand kumar assistant professor amrita vishwa vidyapeetham continuous support guidance. would also like thank arvindan professor college engineering inputs suggestions. kumar dhanalakshmi soman rajendran factored statistical machine translation system english tamil language. pertanika journal social science humanities abadi martín ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin sanjay ghemawat goodfellow andrew harp geoffrey irving michael isard yangqing rafal jozefowicz lukasz kaiser manjunath kudlur josh levenberg mané rajat monga sherry moore derek murray chris olah mike schuster jonathon shlens benoit steiner ilya sutskever kunal talwar paul tucker vincent vanhoucke vijay vasudevan fernanda viégas oriol vinyals pete warden martin wattenberg martin wicke yuan xiaoqiang zheng. tensorflow large-scale machine learning heterogeneous systems. software available tensorﬂow.org. phanindra pydimarri madhavi ganapathiraju balakrishnan reddy. multilingual book reader transliteration word-to-word translation full-text translation. vava merrienboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv.. dzmitry bahdanau kyunghyun yoshua bengio. end-to-end continuous speech recognition using attention-based recurrent {nn} first results. corr abs/.. michael alon lavie. choosing right evaluation machine translation examination annotator automatic metric performance human judgment tasks. amta. chris callison-burch. combining bilingual comparable corpora resource machine translation. proceedings eighth workshop statistical machine translation pages kasthuri britto ramesh kumar. rule based machine translation system english tamil. computing communication technologies world congress pages ieee. kohonen oskar sami virpioja krista lagus. semi-supervised learning concatenative morphology. proceedings meeting special interest group computational morphology phonology pages association computational linguistics. kumar anand dhanalakshmi soman sharmiladevi. improving performance english-tamil statistical machine translation system using source-side pre-processing. arxiv preprint arxiv.. lakshmana pandian kumanan kadhirvelu others. machine translation english tamil using hybrid technique. international journal computer applications roukos todd ward wei-jing zhu. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics pages association computational linguistics. dhanalakshmi anand soman. rule based sentence simpliﬁcation english tamil machine translation system. international journal computer applications ramasamy loganathan ondˇrej bojar zdenˇek žabokrtský. morphological processing english-tamil statistical machine translation. proceedings workshop machine translation parsing indian languages pages petr sojka. software framework topic modelling large corpora. proceedings lrec workshop challenges frameworks pages valletta malta may. elra. http//is.muni.cz/publication//en. renganathan vasu. interactive approach development english-tamil machine translation system web. tamil internet conference papers chennai asian printers pages agrawal jain srivastava jain. anglabharti multilingual machine aided translation project translation english indian languages. systems cybernetics intelligent systems century. ieee international conference volume pages ieee. smit peter sami virpioja stig-arne grönroos mikko kurimo others. morfessor toolkit statistical morphological segmentation. conference european chapter association computational linguistics gothenburg sweden april aalto university. sridhar rajeswari pavithra sethuraman kashyap krishnakumar. english tamil machine translation system using universal networking language. s¯adhan¯a baby. english tamil statistical machine translation alignment using hmm. proceedings international conference networking vlsi signal processing pages kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show attend tell neural image caption generation visual attention. arxiv preprint arxiv.", "year": 2016}