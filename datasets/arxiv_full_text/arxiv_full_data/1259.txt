{"title": "Generic 3D Representation via Pose Estimation and Matching", "tag": ["cs.CV", "cs.LG", "cs.NE", "cs.RO"], "abstract": "Though a large body of computer vision research has investigated developing generic semantic representations, efforts towards developing a similar representation for 3D has been limited. In this paper, we learn a generic 3D representation through solving a set of foundational proxy 3D tasks: object-centric camera pose estimation and wide baseline feature matching. Our method is based upon the premise that by providing supervision over a set of carefully selected foundational tasks, generalization to novel tasks and abstraction capabilities can be achieved. We empirically show that the internal representation of a multi-task ConvNet trained to solve the above core problems generalizes to novel 3D tasks (e.g., scene layout estimation, object pose estimation, surface normal estimation) without the need for fine-tuning and shows traits of abstraction abilities (e.g., cross-modality pose estimation). In the context of the core supervised tasks, we demonstrate our representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectification (unlike SIFT and the majority of learned features). We also show 6DOF camera pose estimation given a pair local image patches. The accuracy of both supervised tasks come comparable to humans. Finally, we contribute a large-scale dataset composed of object-centric street view scenes along with point correspondences and camera pose information, and conclude with a discussion on the learned representation and open research questions.", "text": "abstract. though large body computer vision research investigated developing generic semantic representations eﬀorts towards developing similar representation limited. paper learn generic representation solving foundational proxy tasks object-centric camera pose estimation wide baseline feature matching. method based upon premise providing supervision carefully selected foundational tasks generalization novel tasks abstraction capabilities achieved. empirically show internal representation multi-task convnet trained solve core problems generalizes novel tasks without need ﬁne-tuning shows traits abstraction abilities context core supervised tasks demonstrate representation achieves state-of-the-art wide baseline feature matching results without requiring apriori rectiﬁcation also show camera pose estimation given pair local image patches. accuracy supervised tasks come comparable humans. finally contribute large-scale dataset composed object-centric street view scenes along point correspondences camera pose information conclude discussion learned representation open research questions. supposed image given interested extracting information scene layout pose visible objects. potential approach would annotate dataset every single desired problem train fully supervised system undesirable annotated dataset problem would needed well fact problems would treated independently. addition unlike semantic annotations object labels certain annotations cumbersome collect often require special sensors alternative approach develop system rather generic perception conveniently generalize novel tasks. paper take step towards developing generic perception system solve novel problems without ﬁnetuning capable certain abstract generalizations context could learn generalizable system? cognitive studies suggest living organisms perform cognitive tasks received supervision supervised learning foundational tasks learning relationship visual appearance changing vantage point among ﬁrst visual skills developed infants play fundamental role developing skills e.g. depth perception. classic experiment showed kitten deprived self-motion experienced fundamental issues perception failing understand depth placed visual cliﬀ later works argued ﬁnding least fully motion intentionality supervision signal self-motion indeed crucial elements learning basic visual skills. studies essentially suggest receiving supervision certain proxy task tasks solved suﬃciently without requiring explicit supervision vision tasks foundational others fig. learning generic representation develop supervised joint framework camera pose estimation wide baseline matching. show internal representation framework used representation generalizable various prediction tasks. inspired discussion develop supervised framework convnet trained perform camera pose estimation. basic task allows learning relationship arbitrary change viewpoint appearance object/scene-point. property approach performing camera pose estimation object/scene-centric manner training data formed image bundles show point object/scene camera moves around diﬀerent existing video+metadata datasets problem visual odometry recent works ego-motion estimation training data camera moves independent scene. object/scene-centric approach equivalent allowing learner focus physical point moving around observing appearance particular point transforms according viewpoint change. therefore learner receives additional piece information observed pixels indeed showing object giving information element looks diﬀerent viewpoints providing better grounds learning visual encoding observation. infants also explore object-motion relationships similar hold object hand observe diﬀerent views. dataset also provides supervision task wide baseline matching deﬁned identifying images/patches showing point regardless magnitude viewpoint change. wide baseline matching also important problem closely related object/scene-centric camera pose estimation identify whether images could showing point despite drastic changes appearance agent could learnt viewpoint change impacts appearance. therefore perform supervised training multi-task manner simultaneously solve wide baseline matching pose estimation. advantage learning single representation encodes problems. experiments section show possible single representation solving problems without performance drop compared dedicate representations. provides practical computational storage advantages. also training convnets using multiple tasks/losses desirable shown better regularized train convnet patch pairs extracted training data last vector siamese tower generic representation empirically investigate representation used solving novel problems whether perform abstraction dataset developed object-centric dataset street view scenes cities washington francisco paris amsterdam vegas chicago augmented camera pose information point correspondences. includes million images million matching image pairs camera metadata models cities. release dataset trained models online demo http//drepresentation.stanford.edu/. novelty supervised tasks independent providing generic representation approach solving supervised tasks novel aspects. large amount previous work detecting though visual matching/tracking also early developed cognitive skills unware studies investigating foundational role developing visual perception. therefore presume generality representation mostly attributed camera pose estimation component. scribing matching image features either handcrafting feature learning unlike majority features utilize pre-rectiﬁcation argue rectiﬁcation prior descriptor matching required; representation learn impact viewpoint change rather canceling therefore need apriori rectiﬁcation capable performing wide baseline matching descriptor level. report state-of-the-art results feature matching. wide baseline matching also topic many papers majority focused leveraging various geometric constraints ruling incorrect ‘already-established’ correspondences well number methods operate based generating exhaustive warps assuming information scene given contrast learn descriptor supervised internally handle wide baseline ﬁrst place. context pose estimation show estimating camera pose given pair local image patches without need several point correspondences feasible. diﬀerent many previous works visual odometery literature perform estimation step process consisting ﬁnding point correspondences images followed pose estimation. koser koch also demonstrate pose estimation local region though plane region lies assumed given. recent works supervise convnet camera pose image batches provide results matching pose estimation. report human-level accuracy task. existing unsupervised learning convnet initialization works majority previous unsupervised learning transfer learning representation learning works targeted towards semantics practically well observed representation convnet trained imagenet generalize other mostly semantic tasks. number methods investigated initialization techniques convnet training based unsupervised/weakly supervised data alleviate need large training dataset various tasks recently methods explored using motion metadata associated videos form supervision training convnet. however either investigate developing representation intent provide initialization strategies meant ﬁne-tuned supervised data desired task. contrast investigate developing generalizable representation perform learning object-centric manner evaluate unsupervised performance various tasks without ﬁne-tuning representation. experimentally compare related recent works made models available primary contributions paper summarized generic representation empirically validated abstraction generalization abilities. learned joint descriptor wide baseline matching camera pose estimationat level local image patches. iii) large-scale object-centric dataset street view scenes including camera pose correspondence information. dataset formulated task needs provide large amount training data also show rich camera pose variety scale aimed learning problem invalidates manual procedure. present procedure allows acquiring large amount training data automated manner based sources information google street view almost inexhaustible source geo-referenced calibrated images city models cover thousands cities around world. dataset including million images million matching image pairs camera pose models cities available here. core idea approach form correspondences georeferenced street view camera physical points given models. speciﬁcally given street view location densely shoot rays space order intersections nearby buildings. back projects image pixel space shown ﬁgure projecting resulting intersection points onto adjacent street view panoramas form image image correspondences image associated camera ﬁxates physical target point building placing optical center. make intersection procedure scalable perform occlusion reasoning models pre-identify locations arbitrary target would visible perform intersection points only. fig. illustration object-centric data collection process. large-scale georegistered building models register pixels street view images world coordinates system ﬁnding correspondences relative pose across multiple street view images represents pixel-d world coordinate correspondence. green blue colors represent street view location. shows sample collected image bundle. center pixel expected correspond physical point. pixel alignment pruning system requires integration multiple resources including elevation maps street view models. though quality output exceeded expectation slight inaccuracy metadata models cause pixel misalignment collected images also undocumented objects trees moving objects cause occlusions. thus content-based post alignment pruning necessary. used metadata alignment procedure able handle image bundles arbitrarily wide baselines interest space describe procedure supplementary material process forms dataset composed matching non-matching patches well relative camera pose matching pairs. stopped collecting data reached coverage cities mentioned section collection procedure currently done google street view performed using geo-referenced calibrated imagery. experimentally show trained representation data manifest clear bias towards street view scenes outperforms existing feature learning methods non-street view benchmarks. noise statistics performed user study amazon mechanical turk quantify amount noise ﬁnal dataset. please supplementary material complete discussion results. brieﬂy patch pairs found least overlap content. patch width) pixels respectively. perform ﬁltering geo-fencing collected data amount noise appeared within robustness tolerance convnet trainings converged. joint feature descriptor learnt supervising convolutional neural network perform camera pose estimation wide baseline matching pairs image patches. purpose training image patches depicting physical target point street view dataset labelled matching pairs images labelled non-matching. training camera pose estimation performed using matching patches. patches always cropped center collected street view image keep optical center target point. camera pose pair matching patches represented vector; ﬁrst three dimensions tait-bryan angles last three dimensions cartesian translation coordinates expressed meters. purpose training pose vectors preprocessed zero mean unit standard deviation ground-truth predicted pose vectors example denoted respectively. pose estimation loss lpose robust regression loss described equation convnet training performed optimize joint matching pose estimation loss described equation relative weighting pose matching losses controlled convnet model siamese architecture containing identical streams identical weights used computing relative pose matching score input patches. standard convnet architecture used stream c-relu-p-c-relu-p-c-relu-p-c-relu-p-f-relu-frelu. naming convention follows convolutional layer stride relu rectiﬁed linear unit. fully connected linear layer output units. feature descriptors streams concatenated fully connected layer units pose matching losses. convnet conﬁguration size image representation architecture admittedly pretty common standard. allows evaluate good performance attributed hypothesis learning foundational tasks dataset rather novel architecture. using momentum gradient clipping batch size found gradient clipping essential training even robust regression losses produce unstable gradients starting training. network converged iterations. training using euler angles performed better quaternions robust loss outperformed non-robust loss additional details evaluations street view dataset. test pose estimation composed pairs matching patches dataset. test matching includes matching non-matching pairs. made sure data areas vicinity used training. patch pair test sets veriﬁed three amazon methanical turkers verify ground truth indeed correct. matching pairs turkers fig. sample qualitative results camera pose estimation. rows show patches. depicts estimated relative camera poses unit sphere ground-truth pose patch blue estimated pose patch rightward upward positive directions. sample wide baseline matching results. green represent ‘matching’ ‘non-matching’ respectively. three failure cases shown right. pose estimation. figure provides qualitative results pose estimation. angular evaluation metric standard overall angular error deﬁned angle predicted pose vector ground truth vector plane deﬁned cross product. translational error metric norm diﬀerence vector normalized predicted translation vector ground truth translation vector normalized enable comparing up-to-scale sfm. figure -right provides quantitative evaluations. plots illustrate distribution test respect pose estimation error method green curve shows pose estimation results human subjects. users computer vision knowledge unaware particular case asked estimated relative pitch random subset test pairs. allowed train many training sampled wished. convnet outperformed human task margin median error. pose estimation baselines compared structure-from-motion default components tuned hyper-parameters pairfig. left quantitative evaluation matching. curves method corresponding values shown right quantitative evaluation camera pose estimation. denote visual odometery structure-from-motion respectively. evaluation robustness wide baseline camera shifts shown plots. figure -right shows median angular error changes baseline test pairs increases. achieved binning test bins based baseline size. plot quantiﬁes ability evaluated methods handling wide baseline. adopt slope curves quantiﬁcation deterioration accuracy baseline increases. wide baseline matching. figure shows samples feature matching results using approach three failure cases right. figure -left provides quantitative results. standard metric descriptor matching curve acquired sorting test pairs according matching score. unsupervised methods e.g. sift matching score distance. false positive rate recall area curve standard scalar quantiﬁcations descriptor matching matching baselines compared results handcrafted features sift root-sift daisy asift matching score asift number found correspondences test pair given full images. also compared learning based features zagoruyko komodakis simonyan zisserman simo-serra well human subjects figure -left provides evaluations terms handling wide baselines similar figure -right. brown benchmark mikolajczyk’s benchmark. performed evaluations non-street view benchmarks brown mikolajczyk schmid representation performing well street view scenery wide baseline handling capability achieved expense lower performance small baselines tables provide quantitative results. include thorough description evaluation setup detailed discussions supplementary material joint feature learning. studied diﬀerent aspects joint learning representation information sharing among core supervised tasks. interest space provide quantitative results supplementary material conclusion tests that first problems wide baseline matching camera pose estimation great deal shared information. second descriptor encode problems performance drop. results evaluating representation novel tasks provided section. tasks well images used evaluations signiﬁcantly diﬀerent representation trained fact that despite diﬀerences representation achieves best results among unsupervised methods gets close supervised methods tasks empirically validates hypothesis learning foundational tasks ways evaluating probing representation unsupervised manner tsne large-scale embedding representation. allows visualizing space getting sense similarity perspective representation nearest neighbors full dimensional representation training simple classiﬁer frozen representation read desired variable. latter enables quantifying required information solving novel task encoded representation extracted using simple function. compare representations related methods made models available various layers alexnet trained imagenet number supervised techniques tasks. additional results provided supplementary material website. surface normals vanishing points ﬁgure shows tsne embedding unseen patches showing organization representation space based geometry semantics/appearance. convnet trained estimate pose matching patches embedding non-matching patches similar pose placed nearby. suggests representation generalized fig. embedding representation unseen patches using tsne. organization based manhattan pose patches seen. comparable alexnet’s embedding supplementary material’s section concept pose non-matching patches. indeed relations surface normals relative pose arbitrary frontal patch equal pose arbitrary patch; ﬁgure perceived organization patches based surface normals. better understand achieved visualized activations convnet diﬀerent layers. similar convnets ﬁrst layers formed general gradient based ﬁlters higher layers edges parallel physical world seemed persist cluster together. similar concept vanishing points theoretical perspective would intriguing explain pose estimation results since three common vanishing points theoretically enough full angular pose estimation investigate this generated inversion representation using method show patterns correlating vanishing points image. figure also illustrates tsne superset several vanishing point benchmarks showing images similar vanishing points embedded nearby. therefore speculate convnet developed representation based concept vanishing points. would also explain results shown following sections. attempted quantitatively evaluate this largest vanishing point datasets include images training testing. given descriptor feasible provide statistically signiﬁcant evidence. fig. scene layout search results lsun images synthetic concave cubes deﬁning abstract layouts. images yellow boundary show ground truth layout. scene layout estimation evaluated representation lsun layout benchmark using standard protocol table provides results layout estimation using simple classiﬁer representation along supervised baselines showing representation achieved performance close hedau al.’s supervised method novel task. table provides results layout classiﬁcation using classiﬁer representation compared alexnets pool. abstraction cubelayout evaluate abstract generalization abilities representation generated sparse images showing interior simple synthetic cube parametrized diﬀerent view angles. rendered images seen abstract cubic layout room. performed search images lsun dataset using representations several baselines. apparent ﬁgure representation retrieves meaningful baselines mostly overﬁt appearance retrieve either incorrect always suggests representation could abstract away irrelevant information encode information essential image. fig. search results epfl dataset images synthetic cube deﬁning abstract pose. supplementary material tsne embedding cubes poses joint space. note poses deﬁned cubes ◦congruent. tween convex cubes images epfl multi-view dataset includes dense sampling various viewpoints cars exhibition. picked simple cube pattern simplest geometric element deﬁnes three vanishing points. observation abstraction experiment lsun’s made meaningful baselines mostly overﬁt appearance clear geometric abstraction trait. imagenet figure shows tsne embedding several imagenet categories based representation baselines. embeddings representation geometrically meaningful baselines either perform semantic organization overﬁt aspects color. fig. tsne several imagenet categories using unsupervised representation along several baselines. representation manifests meaningful geometric organization objects. tsne categories supplementary material website. level abstraction object categories drastically diﬀerent looking. also quantitatively evaluated object pose estimation pascald. experiment trained convnet scratch ﬁne-tuned alexnet pre-trained imagenet ﬁne-tuned network; read pose using linear regressor layer. results outperform scratch network come close alexnet seen thousands images categories imagenet objects. note certain aspects object pose estimation e.g. distinguishing front back semantic task rather geometric/d. explains considerable part failures representation object/semantic agnostic. summarize developed generic representation solving supervised foundational proxy tasks. reported state-of-the-art results supervised tasks showed learned representation manifests generalization abstraction traits. however number questions remain open though inspired cognitive studies deﬁning foundational supervised tasks leading generalizable representation remains inspiration level. given ‘taxonomy’ among basic tasks developed concretely deﬁned tasks foundational ones secondary. developing taxonomy generally eﬀorts understanding task space would rewarding step towards soundly developing complete representation. also semantic aspects visual world tangled together. developed independent semantic representations investigating concrete techniques integrating worthwhile future direction research. perhaps inspirations partitions visual cortex could insightful towards developing ultimate vision complete representation. acknowledgement gratefully acknowledge support icme/nvidia award muri nissan", "year": 2017}