{"title": "Power-Law Graph Cuts", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Algorithms based on spectral graph cut objectives such as normalized cuts, ratio cuts and ratio association have become popular in recent years because they are widely applicable and simple to implement via standard eigenvector computations. Despite strong performance for a number of clustering tasks, spectral graph cut algorithms still suffer from several limitations: first, they require the number of clusters to be known in advance, but this information is often unknown a priori; second, they tend to produce clusters with uniform sizes. In some cases, the true clusters exhibit a known size distribution; in image segmentation, for instance, human-segmented images tend to yield segment sizes that follow a power-law distribution. In this paper, we propose a general framework of power-law graph cut algorithms that produce clusters whose sizes are power-law distributed, and also does not fix the number of clusters upfront. To achieve our goals, we treat the Pitman-Yor exchangeable partition probability function (EPPF) as a regularizer to graph cut objectives. Because the resulting objectives cannot be solved by relaxing via eigenvectors, we derive a simple iterative algorithm to locally optimize the objectives. Moreover, we show that our proposed algorithm can be viewed as performing MAP inference on a particular Pitman-Yor mixture model. Our experiments on various data sets show the effectiveness of our algorithms.", "text": "algorithms based spectral graph objectives normalized cuts ratio cuts ratio association become popular recent years widely applicable simple implement standard eigenvector computations. despite strong performance number clustering tasks spectral graph algorithms still suﬀer several limitations ﬁrst require number clusters known advance information often unknown priori; second tend produce clusters uniform sizes. cases true clusters exhibit known size distribution; image segmentation instance humansegmented images tend yield segment sizes follow power-law distribution. paper propose general framework power-law graph algorithms produce clusters whose sizes power-law distributed also number clusters upfront. achieve goals treat pitman-yor exchangeable partition probability function regularizer graph objectives. resulting objectives cannot solved relaxing eigenvectors derive simple iterative algorithm locally optimize objectives. moreover show proposed algorithm viewed performing inference particular pitman-yor mixture model. experiments various data sets show eﬀectiveness algorithms. fundamental problems machine learning clustering received considerable amount attention applications data mining computer vision statistics social sciences others. spectral graph algorithms normalized cuts ratio ratio association studied utilized classes clustering methods. algorithms cluster data ﬁrst constructing similarity graph based given data cutting graph groups nodes according graph-theoretic objective. normalized cuts widely used computer vision community image segmentation problems ratio applied circuit layout though graph problems shown np-hard several eﬀective algorithms proposed including eigenvector-based approaches well methods based kernel k-means despite success spectral graph algorithms suﬀer several important limitations. require number clusters known running algorithm many applications number clusters known priori. importantly many graph objectives normalized objective ratio objective favor clusters equal size degree typically leads algorithms produce clusters nearly uniform sizes. consider image segmentation canonical application normalized cuts. shown human-segmented images yield segments uniform; fact follow power-law distribution terms segment sizes. power-law distributions arise frequently number clustering applications well. instance income follows power-law distribution attempting cluster individuals income brackets using census data would likely fail applying standard clustering techniques. phenomena exhibiting power-law distributions include populations cities intensities earthquakes sizes power outages applications—and lack existing graph clustering methods speciﬁcally encourage power-law cluster size structure—motivate work. paper propose general framework power-law graph algorithms encourages cluster sizes power-law distributed number clusters upfront. achieve goals borrow ideas bayesian nonparametrics provide principled automatically infer parameters model well complexity. observe pitman-yor process bayesian nonparametric prior generalizes chinese restaurant process yields clusters whose sizes follow power-law distribution. treat pitman-yor exchangeable partition probability function regularizer graph objectives resulting objectives favor clusters small graph objective value well power-law cluster size structure. algorithmically incorporating pitman-yor eppf existing formulations results optimization problem standard spectral methods longer applicable. inspired connection spectral graph objectives weighted kernel k-means derive simple k-means-like iterative algorithm optimize several power-law graph objectives. k-means proposed algorithm guaranteed converge local optima ﬁnite number steps. demonstrate graph problem viewed precisely problem particular pitman-yor gaussian mixture model. finally demonstrate utility algorithm perform extensive experiments using power-law normalized cuts synthetic datasets real-world data power-law structure image segmentation. applications line work normalized algorithm number clusters upfront however approach cannot directly applied pitman-yor process mixture models small-variance asymptotics pitman-yor process model fail capture power-law characteristics. related work algorithm scalable power-law clustering based adapting k-means. speciﬁcally authors propose powerlaw data clustering algorithm based modifying pitman-yor process performing small-variance asymptotic analysis modiﬁed piman-yor process. however objective function guarantee generation power-law distributed cluster sizes optimal clustering solutions objective often trivial. discuss method section section idea behind graph cuts partition graph disjoint clusters edges within cluster high weight edges clusters weight. several diﬀerent graph objectives proposed among normalized cuts ratio popular. denote objective shown np-complete relaxation globally optimized using spectral methods computing ﬁrst eigenvalues normalized laplacian constructed adjacency matrix further replace original data mapped data treat entire problem kernel space expressing k-means algorithm along objective terms inner products. necessary connection graph cuts. showed connection weighted kernel k-means objective several spectral graph objectives. discuss particular connection normalized cuts. deﬁne degree matrix diagonal matrix whose entries equal degree node surprising fact established normalized cuts weighted kernel k-means mathematically equivalent following sense adjacency matrix normalized cuts objective equivalent weighted kernel k-means objective kernel matrix d−ad− chosen positive semideﬁnite matrix weights data points equal degrees nodes. thus purposes minimizing weighted kmeans objective function eﬀectively interchange objective appropriate deﬁnition kernel matrix. particular result gives algorithm monotonically minimizing normalized objective— form appropriate kernel weights degrees weighted kernel k-means kernel matrix. similar equivalences hold ratio ratio association objectives—by forming appropriate kernels weights graph objectives shown mathematically equivalent weighted kernel k-means objective. goal propose study graph objectives produce power-law distributed cluster sizes. order achieve this borrow ideas bayesian nonparametrics. speciﬁcally look pitman-yor process generalization chinese restaurant process speciﬁcally yields power-law distributed cluster sizes. simplicity focus normalized objective example. simply replace normalized objective graph objectives obtain power-law graph algorithms framework. canonical bayesian nonparametric clustering prior chinese restaurant process yields distribution clusterings number clusters ﬁxed sizes clusters decay exponentially. description follows customers enter restaurant inﬁnite number tables ﬁrst customer sits ﬁrst table. subsequent customers tables probability proportional number seated customers table probability proportional table. pitman-yor process leads extension cluster sizes instead follow power-law distribution. modiﬁed version customers tables existing table probability proportional explicitly write probability observing particular seating arrangement pitman-yor resulting formula known pitman-yor exchangeable partition probability function obtain power-law distributed cluster sizes within graph clustering setting treat pitman-yor eppf regularizer cluster indicator matrix normalized cuts. resulting objective given below indicator cluster assignment node negative piman-yor eppf tradeoﬀ original graph objective regularization term. ﬁrst term standard normalized objective. desired power-law distributed partition would give high value pitman-yor eppf thus value second term. therefore clustering result minimizes objective give partition graph similarity information preserved cluster sizes power-law distributed. objective function deﬁned previous section enforces tradeoﬀ standard normalized cuts preference power-law cluster size structure. turn optimization resulting objective. standard approach relax cluster indicator matrix continuous leading simple eigenvector problem optimized globally. applying technique power-law normalized objective would need incorporate regularization term appropriately trace maximization problem emerges spectral solution turns impossible. instead must turn main optimization strategy normalized cuts—namely equivalence weighted kernel k-means—and adapt weighted kernel k-means algorithm problem. start section derive k-means-like algorithm following regularized k-means problem means weighted means points standard weighted k-means discussed section obtained algorithm case easily extend connection normalized cuts weighted kernel k-means obtain algorithm monotonic local convergence power-law normalized objective. note treatment equally applicable ratio ratio association objectives. observe that cluster indicators ﬁxed weighted mean justiﬁed objective since best cluster representative cluster terms objective function i.e. ﬁxed choice regularizer constant simple diﬀerentiation therefore updates exactly standard weighted k-means. step update indicators standard k-means updates derived ﬁxing means minimizing k-means objective function respect yields usual k-means assignment step. pitman-yor eppf regularizer makes assignment updates someless trivial still fairly straightforward. data point consider objective function assigning point every existing cluster well cluster assign cluster results smallest objective function. regularizer eﬀectively adds correction observe distance clusters goes increases analogous property pitman-yor version chinese restaurant process likely start table number tables increases. similar computing distance existing clusters distance becomes smaller cluster gets larger goes leading rich gets richer behavior. finally whenever cluster started point immediately mean algorithm full speciﬁcation. note that analogous convergence proof k-means easily show algorithm monotonically decreases regularized k-means objective local convergence. recall section discussed equivalence graph cuts formulation weighted kernel k-means objective equivalence hand extension vector case power-law graph objectives follows easily simply replace weighted k-means term graph cuts term gives exactly objective power-law graph cuts objective constant; apply algorithm kernel space solve resulting optimization problem. finally brieﬂy consider connections proposed objective simple pitman-yor process mixture model. consider following bayesian nonparametric generative model objective function incorporate number clusters optimization require encourage cluster sizes follow power-law distribution. moreover experiments authors case objective function becomes trivial clustering result namely every data point singleton cluster minimize objective. seen fact trivial clustering result minimizes k-means objective simply data points minimizes regularization term. short objective appropriate power-law clustering applications. following experiment section also compare algorithm method empirically. conclude brief experiments demonstrating utility methods. namely show approach enjoys beneﬁts kmeans algorithm real power-law datasets vector setting beneﬁts standard normalized cuts synthetic real data graph setting. also compare method pyp-means show method achieves better clustering results. throughout experiments normalized mutual information algorithm’s clusters ground-truth clusters evaluation. figure results pitman-yor generated stochastic block model graph. left adjacency matrix graph indexed clusters. middle power-law normalized results; right normalized cuts result; stochastic block model. speciﬁcally pitman-yor ﬁrst used generate data cluster assignments standard stochastic block model uses assignments generate random graph. create dataset nodes disjoint clusters using process corresponding adjacency matrix shown left figure parameters pitman-yor process model respectively. stochastic block model stochastic block matrix sampled gaussian distributions non-diagonal entries. power-law normalized algorithm applied dataset parameters validated separate validation dataset generated process. compare normalized cuts ground-truth. results shown figure normalized cuts splits clusters algorithm nearly produces ground-truth clusters. real world power-law data sets. next consider comparing algorithm k-means pyp-means real world benchmark data sets demonstrate algorithm performs best clustering vector data cluster sizes power-law distributed. selected classiﬁcation datasets whose class labels power-law distributed class labels ground-truth clusters. dataset randomly split validation/clustering. normalize datasets values features validation validate parameters algorithm parameters pyp-means yield cluster numbers close ground-truth clustering validated parameter settings algorithm pyp-means ground-truth k-means perform clustering. computed ground-truth computed clusters results averaged runs shown table algorithm performs better k-means datasets terms nmi. also better pyp-means datasets except hypothyroid. note pyp-means better k-means datasets worse k-means high variance results power-law datasets make doubt pyp-means really able achieve power-law clustering. figure show resulting clusterings ecoli dataset given algorithm pyp-means k-means whole dataset clustering validated parameters. clear k-means produces uniform clusters pyp-means also splits largest cluster dataset. real world power-law graph data sets. part convert vector datasets used preceding experiment form power-law graphs perform power-law normalized cuts graphs. also normalized cuts algorithm graphs compare method. adjacency matrix form kernel matrix weights dsiccused section randomly split data validation/clustering ratio parameters selected validation cluster numbers close ground-truth. number clusters normalized cuts true number clusters. fair comparison normalized cuts. finally apply power-law normalized cuts normalized cuts clustering dataset. averaged runs shown table image segmentation. finally brieﬂy demonstrate qualitative results image segmentation berkeley segmentation data adopt approach similar approach compute aﬃnity matrix. perform power-law normalized cuts aﬃnity matrix. compare standard normalized cuts proposed method proposed general framework power-law graph algorithms produce clusters whose sizes power-law distributed also number clusters upfront. pitman-yor exchangeable partition probability function incorporated power-law graph objectives regularizer promote power-law cluster size distributions. simple iterative algorithm proposed locally optimize several objectives. proposed algorithm viewed performing inference particular pitman-yor mixture model. finally conducted experiments various data sets showed eﬀectiveness algorithms competing baselines.", "year": 2014}