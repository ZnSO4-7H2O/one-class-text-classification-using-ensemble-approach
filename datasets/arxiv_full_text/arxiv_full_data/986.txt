{"title": "Hessian-free Optimization for Learning Deep Multidimensional Recurrent  Neural Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.", "text": "multidimensional recurrent neural networks shown remarkable performance area speech handwriting recognition. performance mdrnn improved increasing depth difﬁculty learning deeper network overcome using hessian-free optimization. given connectionist temporal classiﬁcation utilized objective learning mdrnn sequence labeling non-convexity poses problem applying network. solution convex approximation formulated relationship algorithm fisher information matrix discussed. mdrnn depth layers successfully trained using resulting improved performance sequence labeling. multidimensional recurrent neural networks constitute efﬁcient architecture building multidimensional context recurrent neural networks end-to-end training mdrnns conjunction connectionist temporal classiﬁcation shown achieve state-of-the-art performance on/off-line handwriting speech recognition previous approaches performance mdrnns depth layers limited compared recent progress feedforward networks demonstrated. effectiveness mdrnns deeper layers thus unknown. training deep architecture always challenging topic machine learning. notable breakthrough achieved deep feedforward neural networks initialized using layerwise pre-training recently approaches proposed supervision added intermediate layers train deep networks best knowledge pre-training bootstrapping method developed mdrnns. alternatively hesssian-free optimization appealing approach training deep neural networks ability overcome pathological curvature objective function furthermore applied connectionist model provided objective function differentiable. recent success deep feedforward recurrent neural networks supports application mdrnns. paper claim mdrnn beneﬁt deeper architecture application second order optimization allows successful learning. first offer details development optimization mdrnns. then apply optimization sequence labeling tasks address problem non-convexity formulate convex approximation. addition relationship algorithm fisher information matrix discussed. experimental results ofﬂine handwriting phoneme recognition show mdrnn optimization performs better depth network increases layers. mdrnns constitute generalization rnns process multidimensional data replacing single recurrent connection many connections dimensions data network access contextual information directions allowing collective decision made based rich context information. enhance ability exploit context information long shortterm memory cells usually utilized hidden units. addition stacking mdrnns construct deeper networks improves performance depth increases achieving state-of-the-art performance phoneme recognition sequence labeling applied loss function mdrnn. important advantage using pre-segmented sequences required entire transcription input sample sufﬁcient. d-dimensional mdrnn inputs outputs regarded mapping input sequence rm×t×···×td output sequence length input data input neurons given vectorization d-dimensional data length sequence dimension. learnable weights biases concatenated obtain parameter vector learning phase ﬁxed training data mdrnn formalized mapping parameters output sequence i.e. scalar loss function deﬁned output sequence learning mdrnn viewed optimization objective respect jacobian function matrix element partial derivative element output respect element input. hessian scalar function matrix second-order partial derivatives output respect inputs. throughout paper vector sequence denoted boldface vector time denoted k-th element denoted application optimization mdrnn straightforward matching loss function output layer adopted. however case necessarily adopted sequence labeling. developing appropriate approximation compatible optimization discuss considerations related approximation. ﬁrst obtaining quadratic approximation loss function second efﬁcient calculation matrix-vector product used iteration conjugate gradient method. optimization minimizes objective constructing local quadratic approximation objective function minimizing approximate function instead original one. loss function needs approximated point n-th iteration search direction i.e. parameters optimization local approximation curvature typically obtained generalized gauss-newton matrix approximation hessian. optimization uses method subroutine minimize quadratic objective utilizing complete curvature information achieving computational efﬁciency. requires computation arbitrary vector explicit evaluation neural networks efﬁcient compute proposed extending study section provide details efﬁcient computation mdrnns. denotes i-th component vector indeﬁnite hessian matrix problematic second-order optimization deﬁnes unbounded local quadratic approximation nonlinear systems hessian necessarily positive semideﬁnite thus matrix used approximation hessian matrix obtained ignoring second term given sufﬁcient condition approximation exact network makes perfect prediction every given sample stays linear region gl◦n less rank positive semideﬁnite provided thus chosen convex function positive semideﬁnite. principle best deﬁne performs much computation possible positive semideﬁniteness minimum requirement practice nonlinear output layer together matching loss function softmax function cross-entropy loss widely used. product arbitrary vector matrix hljn amounts sequential multiplication three matrices. first product jacobian times vector therefore equal directional derivative along direction thus written using differential operator properties operator utilized efﬁcient computation. mdrnn composition differentiable components computation throughout whole network accomplished repeatedly applying product chain rules starting input layer. detailed derivation operator lstm normally used hidden unit mdrnns provided appendix next multiplication performed direct computation. dimension could ﬁrst appear problematic since dimension output vector used loss function high particular adopted objective mdrnn. loss function expressed individual loss functions domain restricted time computation reduced signiﬁcantly. example commonly used crossentropy loss function matrix transformed block diagonal matrix blocks hessian matrix. t-th block then matrix written connectioninst temporal classiﬁcation provides objective function learning mdrnn sequence labeling. section derive convex approximation inspired approximation according following steps. first non-convex part original objective separated reformulating softmax part. next remaining convex part approximated without altering hessian making well matched non-convex part. finally convex approximation obtained reuniting convex non-convex parts. label observed time along path path length mapped label sequence length operator removes repeated labels blanks. several mutually exclusive paths label sequence. containing every possible sequence mapped {s|s image denote cardinality set. description above composed product softmax components. corresponding softmax cross-entropy loss convex function log. result objective convex general contains softmax components reformulation above objective regarded cross-entropy loss softmax output deﬁned possible label sequences. cross-entropy loss function matches softmax output layer objective convex except part computes label sequences. point obvious candidate convex approximation matrix separating convex non-convex parts. approximation immediately gives convex approximation hessian glc◦nc nchlc jnc. although form diagonal matrix plus rank- matrix i.e. diag dimension becomes exponentially large length sequence increases. makes practical calculation difﬁcult. hand removing linear team alter hessian. hlc◦nc positive semideﬁnite. glc◦nc glp◦nc positive semideﬁnite computationally tractable. hlp◦nc positive semideﬁnite computationally tractable. corresponds case linear mapping. contains log-sum-exp function mapping paths label sequence. label sequence corresponding then probability path sufﬁciently large ignore paths {b−\\π′} follows bπ′. linear mapping results identity fisher information matrix shown network using softmax cross-entropy loss thus follows matrix identical fisher information matrix. show proposed matrix -dimensional output network assumes output probabilities timestep independent timesteps therefore fisher information matrix given every timestep. follows section present experimental results different sequence labeling tasks ofﬂine handwriting recognition phoneme recognition. performance hessian-free optimization mdrnns proposed matrix compared stochastic gradient descent optimization settings. ifn/enit database database handwritten arabic words consists images. entire dataset subsets images corresponding subsets used training. validation consisted images corresponding ﬁrst half sorted list alphabetical order remaining images amounting used test. intensity pixels centered scaled using mean standard deviation calculated training set. timit corpus benchmark database evaluating speech recognition performance. standard training validation core datasets used. contains sentences sentences sentences respectively. spectrum coefﬁcients used feature vector pre-emphasis ﬁlter window size shift size. input feature centered scaled using mean standard deviation training set. handwriting recognition basic architecture adopted proposed deeper networks constructed replacing layer layers. number lstm cells augmented layer chosen total number weights different networks similar. detailed architectures described table together results. phoneme recognition deep bidirectional lstm adopted basic architecture. addition memory cell block cells share gates applied efﬁcient information sharing. lstm block constrained memory cells. according results using large value bias input/output gates beneﬁcial training deep mdrnns. possible explanation activation neurons exponentially decayed input/output gates propagation. thus setting large bias values gates facilitate transmission information many layers beginning learning. reason biases input output gates initialized whereas forget gates memory cells initialized weight parameters mdrnn initialized randomly uniform distribution range label error rate used metric performance evaluation together average loss deﬁned edit distance sums total number insertions deletions substitutions required match given sequences. ﬁnal performance shown tables evaluated using weight parameters gave best label error rate validation set. output probabilities label sequence best path decoding used handwriting recognition beam search decoding beam width used phoneme recognition. phoneme recognition phoneme labels used training decoding then mapped classes calculating phoneme error rate phoneme recognition regularization method suggested used. applied gaussian weight noise standard deviation together regularization strength network ﬁrst trained without noise then initialized weights gave lowest loss validation set. then network retrained gaussian weight noise table presents best result different values optimization followed basic setup described different parameters utilized. tikhonov damping used together levenberg-marquardt heuristics. value damping parameter initialized adjusted according reduction ratio initial search direction direction found previous optimization iteration decayed ensure followed descent direction continued perform minimum maximum additional iterations found ﬁrst descent direction. terminated iteration reaching maximum iteration following condition satisﬁed φ)/φ quadratic objective withoffset. training data divided mini-batches handwriting phoneme recognition experiments respectively used gradient matrix-vector product calculation. learning stopped criteria improve epochs epochs handwriting phoneme recognition respectively. optimization learning rate chosen momentum handwriting recognition best performance obtained using possible combinations parameters presented table phoneme recognition best parameters nine candidates network selected training without weight noise based loss. additionally backpropagated error lstm layer clipped remain range stable learning learning stopped epochs processed ﬁnal performance evaluated using weight parameters showed best label error rate validation set. noted order guarantee convergence selected conservative criterion compared study network converged epochs handwriting recognition epochs phoneme recognition table presents label error rate test handwriting recognition. cases networks trained using optimization outperformed using sgd. advantage using pronounced depth increases. improvements resulting deeper architecture seen error rate dropping depth increases table shows phoneme error rate core phoneme recognition. improved performance according depth observed optimization methods. best optimization layers layers comparable reported reported results network layers million weights network layers million weights. beneﬁt deeper network obvious terms number weight parameters although intended deﬁnitive performance comparison different preprocessing. advantage optimization prominent result experiments using timit database. explanation networks tend overﬁt relatively small number training data samples removes advantage using advanced optimization techniques. table experimental results arabic ofﬂine handwriting recognition. label error rate presented different network depths. denotes stack layers hidden lstm cells layer. epochs number epochs required network using optimization stopping criteria fulﬁlled. learning rate momentum. table experimental results phoneme recognition using timit corpus. presented different mdrnn architectures standard deviation gaussian weight noise. remaining parameters table hessian-free optimization approach successful learning deep mdrnns conjunction presented. apply optimization convex approximation objective function explored. experiments improvements performance seen depth network increased sgd. optimization showed signiﬁcantly better performance handwriting recognition comparable performance speech recognition. references alex graves. supervised sequence labelling recurrent neural networks volume springer alex graves marcus liwicki horst bunke j¨urgen schmidhuber santiago fern´andez. unconstrained on-line handwriting recognition recurrent neural networks. advances neural information processing systems pages christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision pattern recognition pages alex graves santiago fern´andez faustino gomez j¨urgen schmidhuber. connectionist temporal classiﬁcation labelling unsegmented sequence data recurrent neural networks. proceedings international conference machine learning pages christopher bishop editor. pattern recognition machine learning. springer mario pechwitz snoussi maddouri volker m¨argner noureddine ellouze hamid amiri. denotes element-wise vector product logistic sigmoid function input hidden cell activation vector respectively input output forget gates respectively. gates cells size hidden vector denote forward backward variables respectively {u|lu positions label occurs compact notation denote column matrix containing k-th element denote column matrix containing unfortunately cannot analytically calculated general. apply sufﬁcient condition proposed approximation exact section assumption domj] dominant path visits time inant path label sequence el[vt", "year": 2015}