{"title": "Deep Learning with Low Precision by Half-wave Gaussian Quantization", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "The problem of quantizing the activations of a deep neural network is considered. An examination of the popular binary quantization approach shows that this consists of approximating a classical non-linearity, the hyperbolic tangent, by two functions: a piecewise constant sign function, which is used in feedforward network computations, and a piecewise linear hard tanh function, used in the backpropagation step during network learning. The problem of approximating the ReLU non-linearity, widely used in the recent deep learning literature, is then considered. An half-wave Gaussian quantizer (HWGQ) is proposed for forward approximation and shown to have efficient implementation, by exploiting the statistics of of network activations and batch normalization operations commonly used in the literature. To overcome the problem of gradient mismatch, due to the use of different forward and backward approximations, several piece-wise backward approximators are then investigated. The implementation of the resulting quantized network, denoted as HWGQ-Net, is shown to achieve much closer performance to full precision networks, such as AlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision networks, with 1-bit binary weights and 2-bit quantized activations.", "text": "problem quantizing activations deep neural network considered. examination popular binary quantization approach shows consists approximating classical non-linearity hyperbolic tangent functions piecewise constant sign function used feedforward network computations piecewise linear hard tanh function used backpropagation step network learning. problem approximating relu non-linearity widely used recent deep learning literature considered. halfwave gaussian quantizer proposed forward approximation shown efﬁcient implementation exploiting statistics network activations batch normalization operations commonly used literature. overcome problem gradient mismatch different forward backward approximations several piece-wise backward approximators investigated. implementation resulting quantized network denoted hwgq-net shown achieve much closer performance full precision networks alexnet resnet googlenet vgg-net previously available low-precision networks -bit binary weights -bit quantized activations. deep neural networks achieved state-of-the-art performance computer vision problems classiﬁcation detection etc. however complexity impediment widespread deployment many applications real world interest either memory computational resource limited. main issues large model sizes resnet- alexnet vgg-net large computational cost typically requiring gpu-based implementations. generated interest compressed models smaller memory footprints computation. size quantization low-rank matrix factorization pruning architecture design etc. recently shown weight compression quantization achieve large savings memory reducing weight little marginal cost classiﬁcation accuracy however less effective along computational dimension because core network operation implemented units dot-product weight activation vector. hand complementing binary quantized weights quantized activations allows replacement expensive dot-products logical bitcounting operations. hence substantial speed possible addition weights inputs unit binarized quantized low-bit. appears however quantization activations difﬁcult weights. example shown that possible binarize weights marginal cost model accuracy additional quantization activations incurs nontrivial losses large-scale classiﬁcation tasks object recognition imagenet difﬁculty binarization quantization activations requires processing non-differentiable operators. creates problems gradient descent procedure backpropagation algorithm commonly used learn deep networks. algorithm iterates feedforward step computes network outputs backpropagation step computes gradients required learning. difﬁculty binarization quantization operators step-wise responses produce weak gradient signals backpropagation compromising learning efﬁciency. problem addressed using continuous approximations operator used feedforward step implement backpropagation step. this however creates mismatch model implements forward computations derivatives used learn result model learned backpropagation procedure tends sub-optimal. approximate activation function network unit. refer forward backward approximation activation function. start considering binary quantizer used functions seen discrete continuous approximation non-linear activation function hyperbolic tangent frequently used classical neural networks. activation however commonly used recent deep learning literature relu nonlinearity achieved much greater preponderance. exactly produces much stronger gradient magnitudes. hyperbolic tangent sigmoid nonlinearities squashing non-linearities mostly relu half-wave rectiﬁer linear response positive inputs. hence derivatives hyperbolic tangent close zero almost everywhere relu unit derivative along entire positive range axis. improve learning efﬁciency quantized networks consider design forward backward approximation functions relu. discretize linear component propose optimal quantizer. exploiting statistics network activations batch normalization operations commonly used literature show done half-wave gaussian quantizer requires learning efﬁcient compute. recent works attempted similar ideas design quantizer sufﬁcient guarantee good deep learning performance. address problem complementing design study suitable backward approximation functions account mismatch forward model back propagated derivatives. study suggests operations linearization gradient clipping gradient suppression implementation backward approximation. show combination forward hwgq backward operations produces efﬁcient low-precision networks denoted hwgq-net much closer performance continuous models alexnet resnet googlenet vgg-net available low-precision networks literature. best knowledge ﬁrst time single low-precision algorithm could achieve successes many popular networks. according theoretically hwgq-net memory convolutional computation savings. suggest hwgq-net useful deployment state-of-the-art neural networks real world applications. tions. strategy exploit widely known redundancy neural network weights example proposed low-rank matrix factorization decompose large weight matrix several separable small matrices. approach shown successful fully connected layers. alternative procedure known connection pruning consists removing unimportant connections pre-trained model retraining. shown reduce number model parameters order magnitude without considerable loss classiﬁcation accuracy. another model compression strategy constrain model architecture itself e.g. removing fully connected layers using convolutional ﬁlters small size etc. many state-of-the-art deep networks googlenet resnet rely design choices. example squeezenet shown achieve parameter reduction times accuracy comparable alexnet. moreover hash functions also used compress model size another branch approaches model compression weight binarization quantization used ﬁxed-point representation quantize weights pre-trained neural networks speed testing cpus. explored several alternative quantization methods decrease model size showing procedures vector quantization k-means enable times compression minimal accuracy loss. proposed method ﬁxed-point quantization based identiﬁcation optimal bit-width allocations across network layers. shown ternary weight quantization levels achieve model compression slight accuracy loss even large-scale classiﬁcation tasks. finally shown ﬁlter weights quantized without noticeable loss classiﬁcation accuracy datasets cifar- beyond weight binarization quantization activations additional beneﬁts speed-ups replacement expensive inner-products core network computations logical bit-counting operations; training memory savings avoiding large amounts memory required cache full-precision activations. these activation quantization attracted attention recently activations quantized bits achieve speedups cpus. performing quantization network training work avoided issues nondifferentiable optimization. developed optimal algorithm bit-width allocation across layers propose learning procedure quantized neural networks. recently tried tackle optimization networks nondifferentiable quantization units using continuous approximation quantizer function backpropagation step. proposed several potential solutions problem gradient mismatch showed gradients quantized small number bits backpropagation step. methods produced good results datasets cifar- none produced precision networks competitive full-precision models large-scale classiﬁcation tasks imagenet deep neural networks composed layers processing units roughly model computations neurons found mammalian brain. unit computes activation function form rc·w·h weight vector rc·w·h input vector non-linear function. convolutional network implements layers units weights usually represented tensor rc×w×h. dimensions deﬁned number ﬁlter channels width height respectively. since basic computation repeated throughout network modern networks large numbers units structure main factor complexity overall model. complexity problem applications along dimensions. ﬁrst large memory footprint required store weights second computational complexity required compute large numbers dot-products difﬁculties compounded requirement ﬂoating point storage weights ﬂoating point arithmetic compute dot-products practical many applications. motivated interest low-precision networks effective strategy binarize weights convolutional ﬁlters adopt work proposed consists approximating full precision weight matrix used compute activations units product binary matrix −}c×w×h scaling factor convolutional operation input approximated binary weights tremendously reduce memory footprint model fully solve problem computational complexity. since consists either activations previous layer transformation image classify usually represented full precision. hence requires ﬂoating point arithmetic produces ﬂoating point result. substantial reductions complexity obtained binarization enables implementation products logical bit-counting operations adoption greatly simpliﬁes feedforward computations deep model severely increases difﬁculty learning. follows fact derivative sign function zero almost everywhere. neural networks learned minimizing cost respect weights done backpropagation algorithm decomposes derivatives series simple computations. consider unit derivative respect derivative zero almost everywhere gradient magnitudes tend small. result gradient descent algorithm converge minima cost. overcome problem proposed squashing non-linearity replicates saturating behavior neural ﬁring rates. reason widely used classical neural literature. however squashing non-linearities close abandoned recent deep learning literature saturating behavior emphasizes problem vanishing derivatives compromising effectiveness backpropagation. second problem discrepancy approximation constant quantization step. quantization levels reconstruction values constraint reduced precision. since sufﬁces store quantization index recover quantization level non-uniform quantizer requires bits storage activation however usually requires bits represent arithmetic operations used instead index uniform quantizer universal scaling factor placed evidence intuitive store bits without storing indexes. holds arithmetic computation. probability density function hence optimal quantizer dot-products depends statistics. optimal solution usually non-uniform uniform solution available adding uniform constraint given product samples optimal solution obtained lloyd’s algorithm similar k-means algorithm. this however iterative algorithm. since different quantizer must designed network unit quantizer changes backpropagation iteration straightforward application procedure computationally intractable. difﬁculty avoided exploiting statistical structure activations deep networks. example noted dot-products tend symmetric non-sparse distribution close gaussian. taking account fact relu half-wave rectiﬁer suggests half-wave gaussian quantizer optimal quantization parameters gaussian distribution. adoption hwgq guarantees parameters depend mean variance dot-product distribution. however vary across units eliminate need repeated application lloyd’s algorithm across network. problem alleviated resorting batch normalization widely used normalization technique forces responses network layer zero mean unit variance. apply normalization dot-products result illustrated figure number alexnet units different layers. although distributions perfectly gaussian minor differences them close gaussian zero mean unit variance. follows optimal quantization parameters approximately identical across units layers backpropagation iterations. hence lloyd’s algorithm applied once data entire network. fact distributions approximately gaussian zero mean unit variance quantizer even designed samples distribution. implementation drew samples standard gaussian distribution zero mean unit variance obtained optimal quantization parameters lloyd’s algorithm. resulting parameters used parametrize single hwgq used layers batch normalization dot-products. since hwgq step-wise constant function zero derivative almost everywhere. hence approximation leads problem vanishing derivatives. section piecewise linear function used backpropagation step avoid weak convergence. summary seek piece-wise function provides good approximation relu hwgq. next consider three possibilities. since relu already piece-wise linear function seems sensible relu itself denoted vanilla relu backward approximation function. corresponds using derivative backward approximation exact equal forward approximation. hence gradient mismatch. mismatch particularly large large values note that approximation relu error usually upper bounded unbounded since values tail distribution relu said large mismatch tail. this relu used approximate originate inaccurate gradients dot-products tail dot-product distribution. experience inaccurate gradients make learning algorithm unstable. classical problem robust estimation literature outliers unduly inﬂuence performance learning algorithm quantization assumes values beyond probability large dot-products effectively outliers. classical solution mitigate inﬂuence outliers limit growth rate error function. since problem monotonicity relu beyond address investigated alternative backwards approximation functions slower growth rate. imagenet experiments training images resized crop randomly sampled image horizontal ﬂip. batch normalization applied quantization layer discussed section since weight binarization provides regularization constraints ratio dropout networks binary weights full activations dropout used networks quantized activations regardless weight precision. networks learned scratch. data augmentation used standard random image ﬂipping cropping. used parameter learning. bias term used binarized weights. similarly networks quantized activations used maxpooling batch normalization denoted layer re-ordering. ﬁrst last network layers full precision. evaluation based solely central crop. alexnet experiments mini-batch size weight decay learning rate started resnet parameters variant vgg-net denoted vgg-variant smaller version model-a convolutional layers used input size layer removed. mini-batch size learning rate started googlenet branches side losses removed inception layers maxpooling removed channel number reduce convolutional layers increased following convolutional layers. weight decay learning strategy similar resnet networks tested momentum batch normalization used mini-batch size learning rate divided every iterations total. alexnet resnet- vgg-variant explored following ablation studies. tables ﬁgures indicates full-precision weights binary weights full full-precision weights activations. approximate guarantees mismatch tail. gradients non-zero dot-products interval refer relu clipping. illustrated figure clipped relu better match hwgq vanilla relu. idea similar gradient clipping clipping gradients enable stable optimization especially deep networks e.g. recurrent neural network. experiments relu clipping also proved useful guarantee stable neural network optimization. ideally network quantized activations approach performance full-precision network number quantization levels increases. sensitivity vanilla relu approximation outliers limits performance precision networks clipped relu alleviates problem impair network performance loss information clipped interval intermediate solution interval function whose growth rate clipped relu relu possibility enforce logarithmic growth tail according used approximate log-tailed relu identical vanilla relu products amplitude smaller gives decreasing weight amplitudes larger this. behaves like vanilla relu converges clipped relu grows inﬁnity. proposed hwgq-net evaluated imagenet training images categories validation images. evaluation metrics top- top- classiﬁcation accuracy. several popular networks tested alexnet resnet variant vgg-net googlenet implementation based caffe source code available https//github.com/zhaoweicai/hwgq. combined recognition performance dropped even further. alexnet drop much drastic bw+sign bw+q results support hypotheses section well ﬁndings table training errors bw+sign bw+q alexnet shown figure note much lower training error suggesting enables much better approximation full precision activations sign. nevertheless gradient mismatch forward vanilla relu backward approximators made optimization somewhat instable. example error curve bw+q bumpy training. problem becomes serious deeper networks. fact resnet- vgg-variant bw+q performed worse bw+sign. explained fact sign smaller gradient mismatch problem vanilla relu. shown following section substantial improvements possible correcting mismatch forward quantizer backward approximator. next considered impact backward approximators section table shows performance achieved three networks different approximations. cases weights binarized hwgq used forward approximator noopt refers quantization activations pre-trained networks. requires nondifferentiable approximation fails account quantization error. attempted minimize impact cumulative errors across network recomputing means variances batch normalization layers. even this no-opt signiﬁcantly lower accuracy full-precision activation networks. substantial gains obtained training activation quantized networks scratch. although vanilla relu reasonable performance backwards approximator alexnet much better results achieved clipped relu log-tailed relu figure shows larger gradient mismatch vanilla relu created instabilities optimization networks. however instabilities serious deeper networks resnet- vggfw networks. fact results upper bound performance achievable quantization included suggests sign good choice quantization function. hand fairly reasonable upper bound. next compared performance achieved adding sign hwgq quantizers set-up previous section. results alexnet resnet- vgg-variant summarized table ﬁrst notice worse bw+q alexnet table impact layer reordering introduced section next comparing fw+q former binarizes weights latter quantizes activations only observed weight binarization causes minor degradation accuracy. consistent ﬁndings hand activation quantization leads nontrivial loss. suggests latter difﬁcult problem former. observation surprising since unlike weight binarization learning activation-quantized networks needs propagate gradients every nondifferentiable quantization layer. variant. explains sharper drop performance vanilla relu networks table note figure clipped relu log-tailed relu enabled stable learning reached much better optimum networks. among them log-tailed relu performed slightly better clipped relu alexnet slightly worse resnet- vggvariant. consistent clipped relu used following sections. next experiments studied bit-width impact activation quantization. cases weights binarized. table summarizes performance alexnet resnet- function number quantization levels. former improved latter saturation effect. default hwgq conﬁguration also used previous experiments consisted nonuniform positive quantization levels plus denoted table. alexnet low-bit quantization sufﬁced achieve recognition rates close full-precision activations. network quantization seven non-uniform levels sufﬁcient reproduce performance full-precision activations. resnet- however noticeable low-bit full-precision activations. example outperformed points resnet- points alexnet. results suggest increasing number quantization levels beneﬁcial resnet- alexnet. shows results obtained uniform quantization superscript interestingly number quantization levels performance uniform quantizer slightly worse non-uniform counterpart. width uniform quantizer noticeably superior non-uniform quantizer comparing table presents comparison full precision low-precision hwgq-net many popular network architectures. completeness consider googlenet resnet- resnet- section. cases hwgq-net used -bit binary weights -bit uniform hwgq forward approximator clipped relu backwards approximator. comparing previous ablation experiments numbers training iterations doubled polynomial learning rate annealing used hwgq-net gave slight improvement step-wise annealing. table shows hwgq-net approximates well popular networks independently complexity depth. top- accuracy drops fulllow-precision similar networks suggesting low-precision hwgq-net achieve improved performance better full-precision networks become available. training network binary weights lowprecision activations scratch challenging problem addressed previous works table compares hwgq-net recent xnor-net dorefa-net imagenet classiﬁcation task. dorefa-net result model binary weights -bit activation full precision gradient pre-training. alexnet hwgq-net outperformed xnor-net dorefa-net large margin. similar improvements xnor-net observed resnet- dorefa-net results available. worth noting gaps between full-precision networks hwgq-net much smaller xnor-net dorefa-net strong evidence hwgq better activation quantizer. note that contrast experimentation networks hwgq-net shown perform well various network architectures. best knowledge ﬁrst time single low-precision network shown successfully approximate many popular networks. addition conducted experiments cifar- dataset network structure used denoted vgg-small similar relied cross-entropy loss without fully connected layers. learning strategy used vgg-variant section mini-batch size learning rate divided every iterations data augmentation strategy used. section polynomial learning rate decay used low-precision vgg-small. hwgq-net results compared state-of-the-art table seen hwgq-net achieved better performance various popular full-precision networks e.g. maxout fitnet various precision networks. low-precision vgg-small drops points compared full-precision counterpart. ﬁndings consistent imagenet. work considered problem training high performance deep networks low-precision. achieved designing approximators relu non-linearity. ﬁrst half-wave gaussian quantizer applicable feedforward network computations. second piece-wise continuous function used backpropagation step learning. design overcomes learning inefﬁciency popular binary quantization procedure produces similar approximation less effective hyperbolic tangent nonlinearity. minimize problem gradient mismatch studied several backwards approximation functions. shown mismatch affected activation outliers. insights robust estimation literature used propose clipped relu tailed relu approximators. network results combination hwgq denoted hwgq-net shown signiﬁcantly outperform previous efforts deep learning precision substantially reducing low-precision full-precision various stateof-the-art networks. promising experimental results suggest hwgq-net useful deployment state-of-the-art neural networks real world applications.", "year": 2017}