{"title": "ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural  Projections", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Deep neural networks have become ubiquitous for applications related to visual recognition and language understanding tasks. However, it is often prohibitive to use typical neural networks on devices like mobile phones or smart watches since the model sizes are huge and cannot fit in the limited memory available on such devices. While these devices could make use of machine learning models running on high-performance data centers with CPUs or GPUs, this is not feasible for many applications because data can be privacy sensitive and inference needs to be performed directly \"on\" device.  We introduce a new architecture for training compact neural networks using a joint optimization framework. At its core lies a novel objective that jointly trains using two different types of networks--a full trainer neural network (using existing architectures like Feed-forward NNs or LSTM RNNs) combined with a simpler \"projection\" network that leverages random projections to transform inputs or intermediate representations into bits. The simpler network encodes lightweight and efficient-to-compute operations in bit space with a low memory footprint. The two networks are trained jointly using backpropagation, where the projection network learns from the full network similar to apprenticeship learning. Once trained, the smaller network can be used directly for inference at low memory and computation cost. We demonstrate the effectiveness of the new approach at significantly shrinking the memory requirements of different types of neural networks while preserving good accuracy on visual recognition and text classification tasks. We also study the question \"how many neural bits are required to solve a given task?\" using the new framework and show empirical results contrasting model predictive capacity (in bits) versus accuracy on several datasets.", "text": "deep neural networks become ubiquitous applications related visual recognition language understanding tasks. however often prohibitive typical neural network models devices like mobile phones smart watches since model sizes huge cannot limited memory available devices. devices could make machine learning models running high-performance data centers cpus gpus feasible many applications data privacy sensitive inference needs performed directly device. introduce architecture training compact neural networks using joint optimization framework. core lies novel objective jointly trains using different types networks–a full trainer neural network combined simpler projection network leverages random projections transform inputs intermediate representations bits. simpler network encodes lightweight efﬁcient-to-compute operations space memory footprint. networks trained jointly using backpropagation projection network learns full network similar apprenticeship learning. trained smaller network used directly inference memory computation cost. demonstrate effectiveness approach significantly shrinking memory requirements different types neural networks preserving good accuracy several visual recognition text classiﬁcation tasks. also study question many neural bits required solve given task? using framework show empirical results contrasting model predictive capacity versus accuracy several datasets. finally show approach extended learning settings derive projection models optimized using graph structured loss functions. recent advances deep neural networks resulted powerful models demonstrate high predictive capabilities wide variety tasks image classiﬁcation speech recognition sequence-to-sequence learning natural language applications like language translation semantic conversational understanding tasks. networks typically large comprising multiple layers involving many parameters trained large amounts data learn useful representations used predict outputs inference time. efﬁciency reasons training networks often performed high-performance distributed computing involving several cores graphics processing units similar vein applications running devices mobile phones smart watches devices rise. increasingly machine learning models used perform real-time inference directly devices—e.g. speech recognition mobile phones medical devices provide real-time diagnoses smart reply watches among others. however unlike high-performance clusters running cloud devices operate low-power consumption modes signiﬁcant memory limitations. result running state-of-the-art deep learning models inference devices challenging often prohibitive high computation cost large model size requirements exceed device memory capacity. delegating computation-intensive operations device cloud feasible strategy many real-world scenarios connectivity issues privacy reasons scenarios solution take existing trained neural network model apply compression techniques like quantization reduce model size. however techniques useful cases applying post-learning complex neural network tends dilute network’s predictive quality yield sufﬁciently high performance. alternate strategy train small models on-device prediction tasks lead signiﬁcant drop accuracies limits usability models practical applications. particular feature vocabulary pruning techniques commonly applied limit parameters models like recurrent neural networks yielding lower memory footprint affect predicitive capacity network language applications. motivates learning efﬁcient on-device machine learning models memory footprint directly device inference computation cost. neural projection framework leverage existing deep network like feed-forward recursive neural network teach lightweight projected model joint optimization setup trained end-to-end using backpropagation. projections based locality sensitive hashing represent hidden units lightweight network encodes operations extremely efﬁcient compute inference framework permits efﬁcient distributed training optimized produce neural network model memory footprint devices computation cost. model size parameterized conﬁgurable based task device capacity. demonstrate effectiveness approach achieving signiﬁcant reduction model sizes providing competitive performance multiple visual language classiﬁcation tasks proposed framework used study characterize predictive capacity existing deep networks terms number neural projection bits required represent compactly number related works literature attempt learn efﬁcient models limited size memory constraints. works employ techniques ranging simple dictionary lookups feature pruning hashing neural network compression. past researchers proposed different methods achieve compact representations neural networks using reduced numerical precision vector quantization binarization strategies networks weight sharing methods exploit redundancy network weights grouping connections using low-rank decomposition hashing tricks. contrast work proposes learn simple projection-based network efﬁciently encodes intermediate network representations operations involved rather weights. also introduce training paradigm on-device models simple network coupled jointly trained mimic existing deep network ﬂexible customized architecture task. show section speciﬁcs training process also include choice optimize towards soft targets model distillation approaches dropouts similar variants commonly used practice deep learning attempt reduce parameters neural network training dropping unimportant neurons. however serve different purpose namely better regularization. offers survey binary hashing literature relevant projection functions used work. coupled network training architecture proposed paper also resembles conceptually high level generative adversarial networks used unsupervised learning settings reconstruct synthesize data photorealistic images. section present neural projection networks joint optimization framework training neural networks reduced model sizes. ﬁrst introduce objective function using coupled full+projection network architecture describe projection mechanism used work namely locality sensitive hashing applied here. neural networks class non-linear models learn mapping inputs outputs represents input feature vector sequence output category classiﬁcation tasks predicted sequence. typically networks consist multiple layers hidden units neurons connections pair layers. example fully-connected feed-forward neural network number weighted connections network parameters trained number hidden units layer. prediction projection network. p...pt denote projection funcfull network tions transform input d-bit vectors function. represent weights/bias parameters trainer network projection network respectively. training objective optimizes combination loss projection loss biases projection network mimic learn full trainer network. objective also propose objective joint optimization framework training compact on-device models inference. architecture uses trainer network coupled projection network trains jointly. figure illustrates neural projection network architecture using feedforward trainer network. coupled networks jointly trained optimize combined loss function used network training. hθxi) represents parameterized representation hidden units trainer network transforms output prediction similarly hpxi) represents projection network parameters transforms input corresponding predictions apply softmax activation last layer networks compute predictions denotes distance function measures prediction error used loss functions. decomposed three parts—trainer prediction error projection simulation error projection prediction error. reducing ﬁrst leads better trainer network decreasing latter turn learns better projection network simpler approximately equivalent predictive practice cross-entropy experiments. projection capacity. equation follow distillation approach optimize since shown hyperparameters affect trade-off different types errors. tuned small heldout development experiments trainer network trainer model full neural network whose choice ﬂexible depends task. figure shows trainer using feed-forward network swapped lstm rnns deep neural networks. network shown ﬁgure activations layer computed follows relu activation function applied layer except last indicates computed activation values hidden units. number weights/bias parameters network arbitrarily large since used training stage effectively done using high-performance distribtuted computing cpus gpus. projection network projection model simple network encodes efﬁcientto-compute operations performed directly device inference. model deﬁnes efﬁcient projection functions pxi) project input instance different space performs learning space corresponding outputs simpliﬁed projection network operations illustrated figure inputs transformed using series projection functions followed single layer activations. projection transformations pre-computed parameterized functions i.e. trained learning process outputs concatenated form hidden units subsequent operations. training simpler projection network learns choose apply speciﬁc projection operations predictive given task. possible stack additional layers connected bit-layer network achieve non-linear combinations projections. projection model jointly trained trainer learns mimic predictions made full trainer network parameters hence predictive capacity. learning completed transform functions corresponding trained weights projection network extracted create lightweight model pushed device. inference time lightweight model corresponding operations applied given input generate predictions choice type projection matrix well representation projected space setup direct effect computation cost model size. propose leverage efﬁcient randomized projection method using modiﬁed version locality sensitive hashing deﬁne conjunction representation i.e. network’s hidden units represented using projected vectors. yields drastically lower memory footprint compared full network terms number size parameters. highlight properties approach below requirement committing preset vocabulary feature space unlike typical machine learning methods resort smaller vocabulary sizes scaling mechanism. example lstm models typically apply pruning smaller ﬁxed-size vocabularies input encoding step reduce model complexity. proposed learning method scales efﬁciently large data sizes high dimensional spaces. especially useful natural language applications involving sparse high dimensional feature spaces. dense feature spaces existing operations like fully-connected layers efﬁciently approximated prediction without relying large number parameters. operations also applied conjunction projection functions yield complex projection networks constraining memory requirements. locality sensitive projection network projection network described earlier relies transformation functions project input hidden unit representations projection operations outlined equation performed using different types functions. possibility feature embedding matrices pre-trained using wordvec similar techniques model embedding lookup features followed aggregation operation vector averaging. however requires storing embedding matrices incurs additional memory complexity. instead employ efﬁcient randomized projection method step. locality sensitive hashing model underlying projection operations. typically used dimensionality reduction technique applications like clustering motivation using within projection nets allows project similar inputs intermediate network layers hidden unit vectors nearby metric space. allows transform inputs learn efﬁcient compact network representation dependent inherent dimensionality data rather number instances dimensionality actual data vector achieve binary hash functions theorem vectors drawn spherically symmetric distribution relation signs inner products angle vectors expressed follows property holds simple geometry i.e. whenever vector projection matrix falls inside angle unit vectors directions result opposite signs. projection vector orthogonal plane containing xixj effect. since inner products used determine parameter representations nearby ||xi||·||xj||· cosxi therefore model store network hidden activation unit vectors efﬁcient manner using signature vector terms signs. computing projections. following property binary hashing repeatedly apply projection vectors transform input binary hash representation denoted pkxi) xi)] sgnxi pk]. results d-bit vector representation corresponding projection pk=...d. projection matrix ﬁxed prior training inference. note never need explicitly store random projection vector since compute using hash functions rather invoking random number generator. addition also permits perform projection operations linear observed feature size rather overall feature size prohibitively large high-dimensional data thereby saving memory computation cost. binary representation signﬁcant since results signiﬁcantly compact representation projection network parameters turn reduces model size considerably compared trainer network. note techniques like quantization weight sharing stacked method provide small gains terms memory reduction. projection parameters. practice employ different projection functions pj=...t shown figure resulting d-bit vector concatenated form projected activation units equation vary depending projection network parameter conﬁguration speci tuned trade-off prediction quality model size. training inference compact units represent projection network described earlier. training network learns move gradients points nearby projected space direction. direction magnitude gradient determined trainer network access larger parameters complex architecture. networks trained jointly using backpropagation. despite joint optimization objective training progress efﬁciently stochastic gradient descent distributed computing high-performance cpus gpus. trained networks de-coupled serve different purposes. trainer model deployed anywhere standard neural network used. simpler projection network model weights along transform functions extracted create lightweight model pushed device. model used directly device inference time applying operations equations input generate predictions complexity. overall complexity inference observed feature size linear input size number bits speciﬁed projection vector number projection functions used model size memory storage required projector inference model setup extensions. possible extend proposed framework handle type trainer projection network even simultaneously train several models multiple resolutions using architecture leave future work. alternative vector representation projection matrix instead used generate sparse representation hidden units projection network. d-bit block encoded integer instead vector. results larger parameter space overall still beneﬁcial applications actual number learned parameters tiny inference performed efﬁcient sparse lookup operations. section demonstrate effectiveness proposed approach several experiments different benchmark datasets classiﬁcation tasks involving visual recognition language understanding. experiments done using tensorflow implementation baselines method. compare projectionnets different model sizes full-sized deep neural network couterparts task. deep network architecture varies depending task type–feed-forward network employed visual tasks whereas recursive neural networks used language understanding task. case full neural network model used baseline also employed trainer network learn small-sized projectionnets. evaluation. task compute performance model terms precision different ranks i.e. accuracy within predicted output classes. models trained using multiple runs experiment ﬁxed number time steps batch size visual tasks text classiﬁcation task. observed variance across runs accuracy small around ±.%. also compute compression ratio parameters baseline deep network models compared baseline deep network model. model size ratios reported based number free parameters actual model size stored disk. note slightly unfair projectionnet models overlooks advantage since internal representations efﬁcient store compactly disk standard neural network representations. compare performance different approaches using original mnist handwritten digit dataset dataset contains instances training instances testing. hold instances training split tuning system parameters. baseline trainer network feed-forward architecture l-regularization. table shows results baseline comparison projectionnet models varying sizes results demonstrate tiny projectionnet remarkably high compression ratio able achieve high accuracy compared baseline signiﬁcantly larger memory footprint. moreover projectionnet models able achieve even reduction model size yielding around precision top- precision among top- predictions. going deeper projections furthermore going deeper projection network architecture improves prediction performance even though adds parameters overall size still remains signiﬁcantly small compared baseline model. example single-layer projectionnet produces accuracy whereas -layer deep projectionnet projection layer followed fully connected layer improves accuracy considerably yielding gain +.%. stacked projectionnet slightly parameters improves accuracy close baseline performance reduction size. different training objectives training objective varied removing speciﬁc loss components equation observe using joint architecture helps signiﬁcantly results best performance. projectionnet table achieves projectionnet trained without full joint objective worse trained using alone using also trained smaller baseline neural networks fewer layers parameters comparison observed projectionnets achieve higher compression ratios similar performance levels. trend follows comparing simpler regularized linear baseline methods perform worse produce accuracies comparable worse projection networks trained isolation without joint architecture. notice complex problems involving large output space pre-training trainer network performing joint training helps projection converge faster good performance. ages. image tagged ﬁne-grained class belongs test dataset comprises separate images images class. task much harder mnist digit classiﬁcation. goal setup attempt build image recognition system outperform existing state-of-the-art would require complex task-speciﬁc architecture customizations convolution operations. instead train standard deep neural network task contrast various projectionnets determine much compression achieved quality trade-off compared baseline. feed-forward architecture baseline trainer network. table shows results comaprisons projectionnets baseline. mentioned task complex mnist hence precision numbers lower. however observe projectionnet model able achieve model compression relative reduction precision compared baseline. model size reduced upto incurring overall relative reduction precision. hand deeper projection network signiﬁcantly reduces performance achieving model compression rate. compare performance neural projection approach training sequence models semantic intent classiﬁcation task described recent work smartreply automatically generating short email responses. underlying tasks smartreply discover short response messages semantic intent clusters. choose intent classes created dataset comprised samples sample instance corresponds short response message text paired semantic intent category manually veriﬁed human annotators. example that sounds awesome sounds fabulous belong sounds good intent cluster. sequence model multilayer lstm architecture baseline trainer network. lstm model projectionnet variant also compared baseline systems—random baseline ranks intent categories randomly frequency baseline ranks order frequency training corpus. show table projectionnet trained using lstm rnns achieves precision relative drop compared baseline lstm signiﬁcant reduction memory footprint computation higher ranks projectionnets achieve high accuracies close baseline lstm performance. study notion predictability deep neural networks context neural projections. speciﬁcally using projectionnet framework described section formulate questions many neural bits required solve given task? many bits required capture predictive power given deep neural network? motivate series studies different datasets. show empirical results answer computing number bits encoded projection network used task. since projection neural network architecture represented bits single layer projections experiments compute total number bits required represent speciﬁc projectionnet model. compare accuracy achieved given task applying projection network alone inference helps answers ﬁrst question. visual recognition task mnist neural projection bits enough solve task accuracy increasing bits achieves precision improved using deeper projection network. language task semantic classiﬁcation involving sequence input neural projection bits required achieve top- accuracy answer second question given deep neural network speciﬁed conﬁguration model full trainer network framework described section train corresponding neural projection network hidden representations. finally compute number neural projection bits used second network simulates trainer plot value predictive quality ratio deﬁned ratio accuracies achieved separately performing inference simple versus full network given task. figure shows plot mnist cifar- tasks. plot shows predictive power -layer feed-forward network parameters succinctly captured high degree simple -bit projectionnet mnist classiﬁcation bits required recover base deep network quality. complex image recognition tasks involve larger output classes higher number bits required represent trainer deep network architecture. example cifar- task observe -layer feed-forward network projected onto neural bits predictive ratio however also notice steep increase predictive ratio moving neural bits. expect similar trend even higher sizes complex tasks. introduced neural projection approach train lightweight neural network models performing efﬁcient inference device computation memory cost. demonstrated ﬂexibility approach variations model sizes deep network architectures. experimental results different visual language classiﬁcation tasks show effectiveness method achieving signiﬁcant model size reductions efﬁcient inference providing competitive performance. also revisited question predictability deep networks study context neural projections. illustration projection graph trained using graph learning algorithms. figure notation trainer graph represents input node represent neighborhood original graph refers edges nodes graph indicates labeled nodes unlabeled nodes indicates ground-truth corresponding labeled node green colgraph trained supervised manner whereas yields semi-supervised graph learning formulation. notation projection graph node trainer graph connected edge corresponding projection node projection nodes discrete representations trainer node obtained applying projection functions p...pt feature vector associated trainer node. addition also contain intra-projection edges computed using similarity metric applied projection vector representations. example hamming distance deﬁne similarity metric projection nodes represented d−bit vectors. training objective optimizes combination graph loss projection loss biases projection graph mimic learn full trainer graph. optimizes possible future extensions framework discussed section going beyond deep learning also possible apply framework train lightweight models types learning scenarios. example training paradigm changed semi-supervised unsupervised setting. trainer model modiﬁed incorporate structured loss functions deﬁned graph probabilistic graphical model instead deep neural network. figure illustrates end-to-end projection graph approach learning lightweight models using graph optimized loss function trained efﬁciently using large-scale distributed graph algorithms even neural graph approaches projection model training also extended scenarios involving distributed devices using complementary techniques leave explorations future work.", "year": 2017}