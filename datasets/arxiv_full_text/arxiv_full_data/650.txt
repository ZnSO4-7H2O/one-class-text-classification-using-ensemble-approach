{"title": "Regularized Evolution for Image Classifier Architecture Search", "tag": ["cs.NE", "cs.AI", "cs.CV", "cs.DC", "I.2.6; I.5.1; I.5.2"], "abstract": "The effort devoted to hand-crafting image classifiers has motivated the use of architecture search to discover them automatically. Reinforcement learning and evolution have both shown promise for this purpose. This study employs a regularized version of a popular asynchronous evolutionary algorithm. We rigorously compare it to the non-regularized form and to a highly-successful reinforcement learning baseline. Using the same hardware, compute effort and neural network training code, we conduct repeated experiments side-by-side, exploring different datasets, search spaces and scales. We show regularized evolution consistently produces models with similar or higher accuracy, across a variety of contexts without need for re-tuning parameters. In addition, evolution exhibits considerably better performance than reinforcement learning at early search stages, suggesting it may be the better choice when fewer compute resources are available. This constitutes the first controlled comparison of the two search algorithms in this context. Finally, we present new architectures discovered with evolution that we nickname AmoebaNets. These models achieve state-of-the-art results for CIFAR-10 (mean test error = 2.13%), mobile-size ImageNet (top-1 accuracy = 75.1% with 5.1 M parameters) and ImageNet (top-1 accuracy = 83.1%). This is the first time evolutionary algorithms produce state-of-the-art image classifiers.", "text": "automatically ﬁeld known architecture search. traditional approach architecture search neuroevolution topologies improved hardware allows evolving scale producing image classiﬁcation models competitive hand-designs parallel newer alternative approach based reinforcement learning used zoph baker zoph zhong reached state-of-the-art results zoph approaches seem suitable prior work guide researchers approach given context lacking complete theoretical solution empirical ﬁrst step direction would know compare context robust context perturbations. comparison difﬁcult every study uses novel search space preventing direct attribution results algorithm. example search space small instead algorithm fast. picture blurred different training techniques affect model accuracy different deﬁnitions flops affect model size different hardware platforms affect algorithm runtime. accounting factors study presents ﬁrst controlled comparison evolution context image classiﬁer architecture search. achieve statistical signiﬁcance undertake task running experiments repeatedly without sampling bias. experiments present regularized evolutionary algorithm. natural variant standard tournament selection strategy like tournament selection every evolutionary cycle select best random sample individuals reproduce. difference also remove oldest individual. similar happens nature individuals die. show regularized version generally performs better recently used form robust variety image classiﬁcation contexts. therefore used comparaeffort devoted hand-crafting image classiﬁers motivated architecture search discover automatically. reinforcement learning evolution shown promise purpose. study employs regularized version popular asynchronous evolutionary algorithm. rigorously compare non-regularized form highly successful reinforcement learning baseline. using hardware compute effort training code conduct repeated experiments side-by-side exploring different datasets search spaces scales. show regularized evolution consistently produces models similar higher accuracy across variety contexts without need re-tuning parameters. addition evolution exhibits considerably better performance reinforcement learning early search stages suggesting better choice fewer compute resources available. constitutes ﬁrst controlled comparison search algorithms context. finally present architectures discovered evolution nickname amoebanets. models achieve state-of-the-art results cifar- mobile-size imagenet imagenet ﬁrst time evolutionary algorithms produce state-of-theart image classiﬁers. recent neural network successes encouraged proliferation model architectures huang szegedy chen among many others). turn fueled decades-old effort discover tive work representative algorithm evolution. used algorithm zoph refer paper baseline study. chose baseline because began obtained accurate results cifar- popular dataset image classiﬁer architecture search also adopted baseline study’s search space avoid disrupting original tuning algorithm. show evolution match surpass also show holds true switch datasets perturb search space. experiments alternative contexts carried smaller compute scale attaining compromise variety resource use. larger compute scale baseline study evolved models identical conditions achieve better accuracy smaller size. finally evolution longer duration workers obtaining state-of-the-art results cifar- imagenet. pertinent work mentioned section want highlight studies stand efﬁcient search methods zhong suganuma efﬁciency however entirely algorithm also zhong close state actually reaching might require much compute power example). diminishing accuracy returns high-resource regime would surprising. architecture search speed improved variety techniques progressive-complexity search stages hypernets accuracy prediction warm-starting ensembling parallelization reward shaping early stopping netnet transformations methods could principle applied evolution too. miikkulainen took orthogonal strategy splitting search different model scales co-evolving populations could approached angle well. model population undergoing tournament selection. precedent generational evolutionary algorithms discard models regular intervals avoided generational algorithms synchronous nature. tournament selection asynchronous. makes resource efﬁcient recently used non-regularized form large-scale evolution introduced decades regularizing element—albeit complex sometimes random individual selected individuals removed. removal desirable garbage-collection purposes. version real removes worst individual regularizing. version regularized natural permits garbage collection. evolution architecture search also explored cascade-correlation boosting hillclimbing mcts smbo random search grid search even forewent idea individual architectures used evolution train single architecture weights much architecture search work beyond image classiﬁcation could justice here. searched spaces neural network classiﬁers using different algorithms following baseline study best models found augmented larger sizes produce high quality image classiﬁers. executed search process different compute scales addition studied evolutionary algorithms non-neural network simulations evolution experiments used search space design baseline study zoph consists ﬁnding architectures inception-like modules called normal cell reduction cell preserve reduce input size respectively. cells stacked feed-forward patterns form image classiﬁers. resulting models hyper-parameters control size impact accuracy convolution channel depth cell stacking depth used parameters trade accuracy size. refer reader baseline study details since knowledge search phase structure cells altered. cells look like graph vertices combinations. single combination takes inputs applies operation adds generate output. unused outputs concatenated form ﬁnal output cell. within design deﬁne three concrete search spaces differ value number allowed. order increasing size refer sp-i sp-ii sp-iii sp-i exact variant used baseline study sp-ii allowed sp-iii allows larger tree structures within cells evolution used either tournament selection regularized variant standard tournament selection method implemented real population trained models improved cycles. cycle sample models selected random. best model sample mutated produce child altered architecture trained added population. worst model sample removed population. refer approach non-regularized evolution variant regularized evolution natural modiﬁcation instead removing worst model sample remove oldest model population populations initialized random architectures. mutations modify either randomly substituting randomly reconnecting combination’s input. random distributions uniform. experiments employed algorithm baseline study without changes. evolution experiments comparison purposes different compute scales always ensuring approaches competed identical conditions. particular evolution used code network construction training evaluation. scales follows small-scale experiments. experiments could cpu. employed sp-i sp-ii spiii search spaces used g-cifar mnist g-imagenet classiﬁcation datasets gcifar g-imagenet grayscaled versions cifar imagenet respectively grayscale datasets allowed running experiments workers lasting days. unstated sp-i g-cifar used. large-scale experiments. used baseline study’s setup. particular experiments always used sp-i search space cifar- dataset gpus approximately days achieve number trained models baseline. augmentation refer process taking architecture discovered evolution turning full-size accurate model. involves enlarging increasing well training long time— much longer architecture search phase. augmented models trained cifar- imagenet classiﬁcation datasets consistency followed procedure baseline study knew addition experiments searching neural network architectures carried simulations evolve solutions simple single-optimum d-dimensional noisy optimization problem signal-to-noise ratio matching architecture evolution experiments. section describes setup thoroughly details necessary follow results. ﬁrst show beneﬁts regularization simulations small-compute-scale large-compute-scale experiments compare regularized evolution small scale various contexts large scale context baseline study different compute scales deﬁned section simulations section finally evolve models even larger scale started applying evolutionary search noisy simulations optimized perform similarly low-dimensional problems easier. dimensionality increases becomes relatively better results suggest regularization help navigate noise figure regularized non-regularized evolution comparison non-regularized regularized evolution different meta-parameters small-scale experiments g-cifar dataset. marker represents choice meta-parameters namely population size sample size best indicated legend remaining ones labelled other. combination plot quality models obtained experiment along horizontal vertical axes respectively. quality measured mean testing accuracy models found fact points line suggests regularization improves model quality generic meta-parameters. moreover reaches highest accuracy —the meta-parameters experiment selected throughout rest ﬁgure. analogously meta-parameters produced highest accuracy selected nre. comparison different contexts spanning different datasets search spaces g-cifar/sp-i g-cifar/sp-ii g-cifar/sp-iii mnist/sp-i g-imagenet/sp-i shown left right. context show ﬁnal experiments adjacent columns. superpose error bars denotes standard error mean. ﬁrst context contains many repeats identical meta-parameters values seem normally distributed normality assumption error bars represent conﬁdence intervals. experiments meta-parameters optimized simulation results. graph summarizes thousands evolutionary search simulations vertical axis measures simulated accuracy horizontal axis dimensionality problem measure difﬁculty. optimized meta-parameters independently. this carried simulations meta-parameter combination averaged outcomes. plot optima found together error bars. graph shows elementary simulated scenario never worse signiﬁcantly better larger three large-scale experiments cifar- dataset. bottom experiment best meta-parameters analogous experiment experiment meta-parameters used previous study accuracy values meaningful absolute terms models need augmented larger size reach maximum accuracy better far. generally achieved higher accuracy arbitrary choice meta-parameters. robustness desirable computationally demanding experiments below cannot afford many runs optimize meta-parameters. addition robust also achieved best accuracy overall. experiments used sp-i search space g-cifar second test robustness swapped dataset search space produce different contexts. each several repeats evolutionary search using contexts resulted statistically-signiﬁcant higher accuracy runs average. exception g-imagenet search space experiments extremely short compute demands training much data using cpus. interestingly contexts search space bigger runs better runs. verify ﬁndings hold scale three experiments using baseline study’s conditions. figure shows performed better too. taken together simulations architecture evolution experiments provide figure evolution small-compute scale different contexts plots show repeated evolution experiments side-by-side. summary hyper-parameter optimization experiments g-cifar. swept learning rate population size sample size evolution experiments scenario. vertical axis measures mean validation accuracy models experiment. superposed data error bars. results selected best meta-parameters remainder ﬁgure. assessed robustness running experiments different contexts spanning different datasets search spaces g-cifar/sp-i g-cifar/sp-ii g-cifar/sp-iii mnist/sp-i g-imagenet/sp-i shown left right. experiments models. vertical axis measures mean testing accuracy models show detailed view progress experiments g-cifar/sp-ii g-cifar/sp-iii contexts respectively. horizontal axes indicate number models produced experiment progresses. resource-constrained settings require stopping experiments early. models evolution performs better contexts. show stack normal cells best model found g-cifar sp-i sp-iii search spaces respectively labels hidden states. listed full form section data ﬂows left right. baseline study detailed description diagrams. cell replicated three times; i.e. left two-thirds diagram constrained mirror right third. contrast vastly larger sp-iii search space bigger unconstrained construct without replication explored. ﬁrst optimized meta-parameters evolution running small-scale experiments algorithm repeatedly condition. figure shows neither approach sensitive. still necessary step ensure evolution treated fairly. compared algorithms different contexts swapping dataset search space evolution either better equal statistical signiﬁcance. best contexts evolution shown detail figures respectively. show progress repeats algorithm. initial speed evolution striking especially largest search space figures illustrate architectures sp-i sp-iii respectively. regardless context figure indicates accuracy evolution increases signiﬁcantly faster initial stage. stage accelerated higher learning rates. performed large-scale architecture search experiments compare evolution side-by-side. above started optimizing approach. -hyper-parameter space evolution large explore detail tried handful trial-and-error runs informed smaller-scale results above. chose best conditions found thorough took parameters baseline study ﬁne-tuned learning rate. done sweeping accuracy decline extremes optimum consistent found small scale. hyperparameters thus obtained evolution random search experiments. repeated experiment exactly times present results figures a–c. show baseline study’s conditions evolution equally well accuracy; signiﬁcantly better evolution faster above. took models experiment augmented figure shows testing accuracy flops resulting full-size models. augmentation adds noise relative accuracy evolution roughly preserved. random search still worst high conﬁdence. evolved models exhibit slight increase accuracy variance much lower flops obtained selected best model evolution runs section nickname amoebanet-a. avoid overﬁtting model selected validation accuracy within across experiments—and model selected. adjusting trade parameters lower testing error experimental conditions baseline study obtained nasnet-a. table indicates cifar- amoebanet-a exhibits lower error matching parameters fewer parameters matching error. also reaches current state imagenet completed controlled comparison concentrated resources dedicated evolution experiments exploring larger sp-ii search space tpuv chips augmentation procedure selected model validation accuracy show results evolution random search experiments. except horizontal axes measure experiment progress terms number models generated approximately thought experiment time. vertical axes show various measures model quality. curve shows improvement accuracy models generated throughout experiment. show progress identical experiments three algorithms. evolution experiments used best meta-parameters found vertical axis measures validation accuracy model mean testing accuracy models seen model selected validation accuracy testing accuracy hidden algorithm researchers plotting model accuracy complexity equally augmented models showing true potential architecture search experiments presented. marker corresponds average within experiment. error bars sem. across experiments). nickname amoebanet-b model sets state cifar- dataset note training time constraint even largest conﬁguration parameters took less hours train completion tpu. forewent training larger models observed diminishing returns. table cifar- results. compare hand-designs† architecture search results† best evolved model +c/o indicates cutout params number free parameters. report model’s test error sem. nasnets pnasnets amoebanets reported xxnet evolution-based methods marked table imagenet classiﬁcation results. compare handdesigns† architecture search results† model params number free parameters. means number multiply-adds. /-acc refers top- top- test accuracy. nasnets pnasnets amoebanets reported xxnet evolution-based methods marked model densenet-bc resnext- densenet-bc shake-shake pyramidnet shakedrop evolving dnn∗ metaqnn cgp-cnn large scale evolution∗ smashv hierarchical block-qnn-a pnasnet- nasnet-a nasnet-a nasnet-a amoebanet-a amoebanet-b amoebanet-a amoebanet-b amoebanet-b amoebanet-b amoebanet-b c/o∗ amoebanet-b c/o∗ amoebanet-b c/o∗ amoebanet-b c/o∗ model inception xception inception resnet resnext- polynet dual-path-net- squeeze-excite-net genet-∗ block-qnn-b hierarchical pnasnet- nasnet-a amoebanet-b amoebanet-a amoebanet-a amoebanet-c employed different metrics assess experiment progress outcome. validation accuracy †table references huang gastaldi miikkulainen baker suganuma real brock zhong yamada szegedy chollet szegedy zhang chen yuille zhong zoph howard zhang sandler actual reward seen algorithms therefore natural choice assess algorithm performance. however suffers signiﬁcant uncertainty neural network training noise. averaging models yields robust quantity mean validation accuracy still drawback deceiving search-space regions models prone large generalization error. regions look good algorithm less conducive ﬁnding high-quality classiﬁers. search space issue search algorithm issue. still practice want know well models generalize. instead analyze mean testing accuracy large-scale experiment progress plots suggest evolution approaching common accuracy asymptote. raises question algorithm gets faster. plots indicate reaches half-maximum accuracy roughly twice time. abstain nevertheless quantifying effect since depends strongly speed measured note variance ﬁgures relative speed factor would also become noisy measured non-averaged curves even noisier experiment repeats performed. algorithm speed important exploring larger spaces reaching optimum requires compute available regime evolution shine. size search space deserves consideration. large spaces advantage requiring less expert input small spaces reach better results sooner constructed exclude models. consequently smaller spaces likely harder distinguish search algorithms also overlapping error bars well-crafted space zoph provided appropriate compromise work. different regimes could important elsewhere depending goals resources expertise. architecture search phase complete resulting models augmented equally accuracy augmented models mirror perfectly search-phase counterparts. introduces randomness making accuracy comparison figure less indicative algorithm performance figure flops however intrinsic property architecture figure demonstrates evolved models leaner. speculate regularized asynchronous evolution reducing flops because indirectly optimizing speed—fast models well reproduce quickly even lack high accuracy slower peers. verifying speculation beyond scope paper. regularization advantageous simulations neural-network architecture evolution experiments simulations constructed simple possible still modeling noisy evaluation present neural networks. therefore speculate regularization help navigate noise follows. regularized evolution models short lifespan. populations improve longer timescales requires surviving lineages remain good generations. this turn demands inherited architectures retrain well hand non-regularized tournament selection allows models live inﬁnitely long population improve simply accumulating high-accuracy models. unfortunately models reached high accuracy luck noisy training process. summary regularized form requires architectures remain good retrained. whether mechanism responsible observed superiority regularization conjecture. leave veriﬁcation future work. shown ﬁrst time neural-network architecture evolution produce state-of-the-art image classiﬁers. employed regularized evolutionary algorithm demonstrated controlled comparisons regularization directly responsible signiﬁcant performance improvement recently used tournament selection variant. utilizing search space existing baseline study performed ﬁrst rigorous comparison evolution image classiﬁer search. found regularized evolution faster convergence speed obtained equal better accuracy across variety contexts without need re-tuning parameters. finally concentrated compute resources explore larger search space using tpus. regularized evolution experiments yielded amoebanets novel architectures achieve state-of-the-art results cifar- mobile-sized imagenet imagenet. study ﬁrst empirical step illuminating relationship evolution particular context. hope future work generalize comparison expose merits approaches. also hope future work reduce compute cost architecture search. hand economy scale repeated efﬁcient models discovered vastly exceed compute cost discovery thus justifying regardless. undoubtedly maximization ﬁnal accuracy optimization search process worth exploring. wish thank megan kacholia vincent vanhoucke xiaoqiang zheng especially jeff dean support valuable input; barret zoph vijay vasudevan help code experiments used zoph well jianwei jacques pienaar derek murray gabriel bender golnaz ghiasi saurabh saxena coding contributions; jacques pienaar luke metz chris ying andrew selle manuscript comments patrick nguyen samy bengio geoffrey hinton risto miikkulainen yifeng david dohan david david vishy tirumalashetty yoram singer chris ying ruoming pang helpful discussions; larger google brain team. references baker bowen gupta otkrist naik nikhil raskar ramesh. designing neural network architectures using reinforcement learning. arxiv preprint arxiv. cortes corinna gonzalvo xavi kuznetsov vitaly mohri mehryar yang scott. adanet adaptive structural learning artiﬁcial neural networks. arxiv preprint arxiv. fernando chrisantha banarse dylan blundell charles zwols yori david rusu andrei pritzel alexander wierstra daan. pathnet evolution channels gradient descent super neural networks. arxiv preprint arxiv. feurer matthias klein aaron eggensperger katharina springenberg jost blum manuel hutter frank. efﬁcient robust automated machine learning. nips howard andrew menglong chen kalenichenko dmitry wang weijun weyand tobias andreetto marco adam hartwig. mobilenets efﬁcient convolutional neural networks mobile vision applications. arxiv preprint arxiv. jaderberg dalibard valentin osindero simon czarnecki wojciech donahue jeff razavi vinyals oriol green dunning iain simonyan karen population based training neural networks. arxiv preprint arxiv. chenxi zoph barret shlens jonathon li-jia fei-fei yuille alan huang jonathan murphy kevin. progressive neural architecture search. arxiv preprint arxiv. hanxiao simonyan karen vinyals oriol fernando chrisantha kavukcuoglu koray. hierarchical representations efﬁcient architecture search. arxiv preprint arxiv. mendoza hector klein aaron feurer matthias springenberg jost tobias hutter frank. towards automatically-tuned neural networks. workshop automatic machine learning miikkulainen risto liang jason meyerson elliot rawal aditya fink francon olivier raju bala navruzyan arshak duffy nigel hodjat babak. evolving deep neural networks. arxiv preprint arxiv. real esteban moore sherry selle andrew saxena saurabh suematsu yutaka leon quoc kurakin alex. large-scale evolution image classiﬁers. arxiv preprint arxiv. srivastava nitish hinton geoffrey krizhevsky alex sutskever ilya salakhutdinov ruslan. dropout simple prevent neural networks overﬁtting. journal machine learning research suganuma masanori shirakawa shinichi nagao tomoharu. genetic programming approach designing convolutional neural network architectures. arxiv preprint arxiv. szegedy christian ioffe sergey vanhoucke vincent alemi alexander inception-v inception-resnet impact residual connections learning. aaai volume section introduced search spaces sp-i sp-ii sp-iii described outline. language presented section deﬁned sp-i uses possible convolutions average sp-ii uses possible used following datasets cifar- dataset naturalistic images labeled common object classes. training examples test examples color images. training examples held validation set. remaining examples used training. g-cifar grayscaled version cifar-. original images averaged across channels. training mnist handwritten black-and-white digit classiﬁcation dataset. testing examples. held imagenet large naturalistic images labeled objects among classes. contains total examples. these held validation testing. rest constituted training set. g-imagenet grayscaled subset imagenet original images averaged across channels re-sized generated training images validation images standard training set. also generated testing images standard validation set. section introduced different compute scales. following completes descriptions. small-scale experiments. experiment ended models trained model trained epochs either g-cifar mnist g-imagenet datasets respectively. cases settings chosen close possible large-scale experiments running reasonably fast cpu. large-scale experiments. experiment also ended models trained. search space sp-i dataset cifar- model trained epochs. section introduced setup exploring larger sp-ii search space. following completes description. dedicated evolution experiments. like large-scale experiments except search space expanded sp-ii models larger training longer training larger models epochs search phase validation accuracy representative true validation accuracy model scaled order avoid selection bias experiment repeats plotted figures include actual runs optimization stage meta-parameters found. decision made priori. compare augmented models side-by-side selected evolution experiment models validation accuracy. augmented equally setting done experiment produced nasnet-a baseline study. finally trained cifar- compare best model baseline study’s best model nasnet-a matching experiment resources augmented models evolution selected best validation ﬁtness. retrained model times various sizes measure mean testing error. presented conﬁgurations matched either accuracy number parameters nasnet-a. section table selected experiment models. this binned models number parameters cover range using bins. took models validation accuracy. augmented models picked validation accuracy. re-augmented model values table trained resulting size times cifar- measure mean testing accuracy. train augmented models cifar- proceeded baseline study except setting batch size initial learning rate cosine decay zero epochs drop-connect probability measuring validation accuracy held validation included training set. measuring testing accuracy used full training set. stress testing accuracy never used evaluation ﬁnal models given dataset presented text/table. train augmented models imagenet followed training data augmentation evaluation procedures input image size mobile size models large models. used distributed synchronous workers. employed rmsprop optimizer decay regularization weight decay label smoothing value auxiliary head weighted applied dropout ﬁnal softmax layer probability learning rate started decayed every epochs rate search space used vertices d-dimensional unit cube. speciﬁc vertex analogous neural network architecture real experiment. training evaluating neural network yields noisy accuracy. likewise simulations assign noisy simulated accuracy cube vertex. fraction coordinates zero plus small amount gaussian noise thus goal close optimum origin. sample complexity used simulations helpful complete milliseconds. optimization problem simpliﬁcation evolutionary search minimum multi-dimensional integervalued paraboloid bounded support mutations treat values along coordinate categorically. restrict domain along direction reduced unit cube described above. paraboloid’s value cubes corners number coordinates zero i.e. scenario above. mention connection searching minimum paraboloid seems like natural choice trivial problem simpler unit cube version however chosen permits faster computation. stress simulations intended truly mimic architecture search experiments space neural networks. used testing ground evolving solutions presence noisy evaluations. figure shows normal reduction cells amoebanet models. labels hidden states. listed full form section data ﬂows bottom top. zoph detailed description diagrams stacked form full models. figure basic amoebanet building blocks. amoebanet-a normal reduction cells. amoebanet-b normal reduction cells. amoebanet-c normal reduction cells.", "year": 2018}