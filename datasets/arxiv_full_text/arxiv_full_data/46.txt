{"title": "Transfer Learning for Named-Entity Recognition with Neural Networks", "tag": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown promising results for named-entity recognition (NER). In order to achieve high performances, ANNs need to be trained on a large labeled dataset. However, labels might be difficult to obtain for the dataset on which the user wants to perform NER: label scarcity is particularly pronounced for patient note de-identification, which is an instance of NER. In this work, we analyze to what extent transfer learning may address this issue. In particular, we demonstrate that transferring an ANN model trained on a large labeled dataset to another dataset with a limited number of labels improves upon the state-of-the-art results on two different datasets for patient note de-identification.", "text": "recent approaches based artiﬁcial neural networks shown promising results named-entity recognition order achieve high performances anns need trained large labeled dataset. however labels might difﬁcult obtain dataset user wants perform label scarcity particularly pronounced patient note de-identiﬁcation instance ner. work analyze extent transfer learning address issue. particular demonstrate transferring model trained large labeled dataset another dataset limited number labels improves upon state-of-the-art results different datasets patient note de-identiﬁcation. electronic health records widely adopted countries united states represent gold mines information medical research. majority data exist unstructured form patient notes applying natural language processing patient notes improve phenotyping patients many downstream applications understanding diseases united states patient conﬁdentiality. health insurance portability accountability deﬁnes different types ranging patient names numbers addresses phone numbers. task removing patient note referred de-identiﬁcation. essence de-identiﬁcation recognizing patient notes form named-entity recognition existing de-identiﬁcation systems often rule-based approaches feature-based machine learning approaches. however techniques require additional lead time developing ﬁne-tuning rules features speciﬁc dataset. meanwhile recent work using anns yielded state-of-the-art performances withusing manual features compared previous systems anns competitive advantage model ﬁne-tuned dataset without overhead manual feature development long labels dataset available. however still inefﬁcient mass deploy ann-based de-identiﬁcation system practical settings since creating annotations patient notes especially difﬁcult. fact restricted individuals authorized access original patient notes; annotation task cannot crowd-sourced making slow expensive obtain large annotated corpus. medical professionals therefore wary explore patient notes deidentiﬁcation barrier considerably hampers medical research. paper analyze extent transfer learning improve de-identiﬁcation performances datasets limited number labels. training model large dataset transferring smaller datasets transfer learning studied long time. standard deﬁnition transfer learning literature follow deﬁnition transfer learning aims performing task target dataset using knowledge learned source dataset. idea applied many ﬁelds speech recognition ﬁnance successes anns many applications last years escalated interest studying transfer learning anns. particular much work done computer vision studies parameters learned source dataset used initialize corresponding parameters anns target dataset. fewer studies performed transfer learning ann-based models ﬁeld natural language processing. example focused transfer learning convolutional neural networks sentence classiﬁcation. best knowledge study analyzed transfer learning ann-based models context ner. model transfer learning experiments based type recurrent neural networks called long short-term memory utilizes token embeddings character embeddings. comprises major components token embedding layer maps token sequence optimization layer takes sequence vectors output fully connected layer outputs likely sequence predicted labels optimizing unigram label scores well bigram label transition scores. figure shows components interconnected form model. layers learned jointly using stochastic gradient descent. regularization dropout applied token lstm layer early stopping used development patience epochs. figure model ner. transfer learning experiments train parameters model source dataset transfer parameters initialize model training target dataset. datasets three de-identiﬁcation datasets transfer learning experiments mimic mimic de-identiﬁcation dataset introduced mimic-iii dataset datasets released part ib/uthealth shared task track cegs n-grid shared task respectively. table presents datasets’ sizes. experiment analyzing importance parameter transfer learning. instead transferring parameters experiment transferring different combinations parameters. goal understand components important transfer. lowest layers tend represent task-independent features whereas topmost layers task-speciﬁc. result transferring parameters starting bottommost layer toplayer adding layer time. transfer learning goal transfer learning leverage information present source dataset improve performance algorithm target dataset. setting apply transfer learning training parameters model source dataset using retrain target dataset ﬁne-tuning. mimic source dataset since dataset labels. perform sets experiments gain insights effective transfer learning parameters important transfer. experiment quantifying impact transfer learning various train sizes target dataset. primary purpose experiment assess extent transfer learning improves performances target dataset. experiment different train sizes understand many labels needed target dataset experiment figure compares f-scores trained target dataset trained source dataset followed target dataset. transfer learning improves f-scores training target dataset though improvement diminishes number training samples used target dataset increases. implies representations learned source dataset efﬁciently transferred exploited target dataset. therefore transfer learning adopted fewer annotations needed achieve level performance source dataset unused. example dataset performing transfer learning using train leads similar performance using transfer learning using train set. transfer learning thus allows half number labels needed target dataset case. datasets performance gains transfer learning greater train size target dataset small. largest improvement observed using dataset train transfer learning increases f-score around percent point even train used f-score improves using transfer learning albeit percent point figure impact transfer learning f-scores. baseline corresponds training model target dataset transfer learning corresponds training source dataset followed training target dataset. target train size percentage train whole dataset corresponds full ofﬁcial train set. experiment figure shows importance layer transfer learning. observe transferring lower layers alefﬁcient transferring layers. transferring token lstm shows great improvements layer less improvement added layer beyond that. larger improvements observed character lstm less beyond layer. parameters lower layers therefore seems contain information relevant de-identiﬁcation task general supports common hypothesis higher layers architectures contain parameters speciﬁc task well dataset used training. despite observation transferring lower layers sufﬁcient efﬁcient transfer learning interesting adding topmost layers transfer learning hurt performance. retraining model target dataset able adapt target dataset quite well despite higher layers initialized parameters likely speciﬁc source dataset. work studied transfer learning anns speciﬁcally patient note de-identiﬁcation transferring parameters trained large labeled dataset another dataset limited human annotations. demonstrated transfer learning improves performance state-of-the-art results datasets. transfer learning especially beneﬁcial target dataset small number labels. references ashwin ananthakrishnan tianxi guergana savova su-chun cheng chen raul guzman perez vivian gainer shawn murphy peter szolovits zongqi improving case deﬁnition crohn’s disease ulcerative colitis electronic medical records using natural language inﬂamprocessing novel informatics approach. matory bowel diseases franck dernoncourt young ozlem uzuner peter szolovits. de-identiﬁcation patient journal notes recurrent neural networks. american medical informatics association page ocw. maxime oquab leon bottou ivan laptev josef sivic. learning transferring mid-level image representations using convolutional neural netproceedings ieee conference works. computer vision pattern recognition. pages goldberger luis amaral leon glass jeffrey hausdorff plamen ivanov roger mark joseph mietus george moody chungkang peng eugene stanley. physiobank physiotoolkit physionet components research resource complex physiologic signals. circulation e–e. mohammed saeed mauricio villarroel andrew reisner gari clifford li-wei lehman george moody thomas heldt kyaw benjamin moody roger mark. multiparameter intelligent monitoring intensive care public-access intensive care unit database. critical care medicine yoni halpern steven horng youngduck choi david sontag. electronic medical record phenotyping using anchor learn framework. journal american medical informatics association page ocw. alistair johnson pollard shen lehman mengling feng mohammad ghassemi benjamin moody peter szolovits anthony celi roger mark. mimic-iii freely accessible critical care database. scientiﬁc data amber stubbs christopher kotﬁla ¨ozlem uzuner. automated systems de-identiﬁcation longitudinal clinical narratives overview journal ib/uthealth shared task track biomedical informatics s–s. dong wang thomas fang zheng. transfer learning speech language processing. signal information processing association annual summit conference asiapaciﬁc. ieee pages literature survey domain adaptation algorithms natural language processing. department computer science graduate center city university york pages jason yosinski jeff clune yoshua bengio lipson. transferable features deep neural networks? advances neural information processing systems. pages katherine liao tianxi guergana savova shawn murphy elizabeth karlson ashwin ananthakrishnan vivian gainer stanley shaw zongqi peter szolovits development phenotype algorithms using electronic medical records incorporating natural language processing.", "year": 2017}