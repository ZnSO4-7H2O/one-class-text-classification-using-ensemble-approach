{"title": "Adversarial Defense based on Structure-to-Signal Autoencoders", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "Adversarial attack methods have demonstrated the fragility of deep neural networks. Their imperceptible perturbations are frequently able fool classifiers into potentially dangerous misclassifications. We propose a novel way to interpret adversarial perturbations in terms of the effective input signal that classifiers actually use. Based on this, we apply specially trained autoencoders, referred to as S2SNets, as defense mechanism. They follow a two-stage training scheme: first unsupervised, followed by a fine-tuning of the decoder, using gradients from an existing classifier. S2SNets induce a shift in the distribution of gradients propagated through them, stripping them from class-dependent signal. We analyze their robustness against several white-box and gray-box scenarios on the large ImageNet dataset. Our approach reaches comparable resilience in white-box attack scenarios as other state-of-the-art defenses in gray-box scenarios. We further analyze the relationships of AlexNet, VGG 16, ResNet 50 and Inception v3 in adversarial space, and found that VGG 16 is the easiest to fool, while perturbations from ResNet 50 are the most transferable.", "text": "abstract. adversarial attack methods demonstrated fragility deep neural networks. imperceptible perturbations frequently able fool classiﬁers potentially dangerous misclassiﬁcations. propose novel interpret adversarial perturbations terms eﬀective input signal classiﬁers actually use. based this apply specially trained autoencoders referred ssnets defense mechanism. follow two-stage training scheme ﬁrst unsupervised followed ﬁne-tuning decoder using gradients existing classiﬁer. ssnets induce shift distribution gradients propagated them stripping class-dependent signal. analyze robustness several white-box gray-box scenarios large imagenet dataset. approach reaches comparable resilience white-box attack scenarios state-of-the-art defenses gray-box scenarios. analyze relationships alexnet resnet inception adversarial space found easiest fool perturbations resnet transferable. nowadays increasing adoption deep learning techniques production systems partially even safety-relevant ones hence surprising discovery adversarial spaces neural networks sparked interest. growing community quickly focused different ways reach spaces understand properties protect vulnerable models malicious nature prevalent methods exploiting adversarial spaces gradients main starting point search perturbations surrounding clean sample. intractability transformations modeled neural networks limited amount change allowed perturbations considered adversarial gradients classiﬁer expose enough information parts input correlate highly label model associates mind defenses adversarial attacks devised upon gradients fundamental ways complement traditional optimization schemes two-fold objective minimizes overall prediction cost maximizing perturbation space around clean images classiﬁers withstand gradients blocked obfuscated attacking algorithms longer eﬀective adversarial perturbations type methods enjoy mathematical rigor hence provide formal guarantees respect kind perturbations robust however note also disadvantageous since networks become attack-dependent. strategy ﬁnding perturbations could circumvent defense mechanism eﬀective small-scale problems mnist cifar found empirical evidence methods scale larger problems imagenet. even shown defenses adversarial attacks tested small datasets scale well applied bigger problems currently large scale state-of-the-art defenses rely second gradients suppression blockage deﬁned athalye gradient obfuscation result instabilities vanishing exploding gradients stochastic non-diﬀerentiable preprocessing steps. alternatives modeled lossy identities original signal contained input preserved adversarial perturbation destroyed. usefulness principle well ﬁndings recent study showing image classiﬁers small fraction entire signal within original input. therefore portion information indeed dropped without aﬀecting performance paper propose alternative defense aﬀects information contained gradients reforming class-related signal structural one. intuitively learn identity function encodes structure decodes structural parts input necessary classiﬁcation dropping everything else. autoencoder trained approximate identity function preserves part signal useful target classiﬁer. structural information preserved training encoder decoder unsupervised ﬁne-tuning decoder gradients coming existing classiﬁer. using function looks structure gradients devoid class-related information therefore invalidating fundamental assumptions gradients attackers rely call defense structure-to-signal network diﬀerent space leading attackers p˜xf words train |∇xf ∩∇xf| minimized. turn cause intersection |pxf◦g∩p˜xf| smaller resulting perturbations non-adversarial details architecture zero compromise safe use-cases drop performance defense deployed network attack. transformation done ssnet preserving required signal using either impact classiﬁer clean images used. removing ssnet defense strategy novel gray-box defense works giving away gradient information ssnet attacker removing applying adversarial attacks original classiﬁer instead. attack agnostic ssnets rely information used adversarial attacks attacks themselves. therefore ssnets require assumptions respect speciﬁc attack works. strictly speaking adversarial perturbations reached domains depend gradients instances strong adversarial attack methods base entire strategy information gradients. post-hoc implementation defense uses gradients trained network used defend models already production. likewise special considerations need made training classiﬁer scratch. test ssnets high-performing image classiﬁers inception-v three attack methods basic iterative method carlini-wager large scale imagenet dataset. experiments conducted classiﬁers white-box gray-box conditions. evaluation eﬀectiveness ssnets respect regular presented section empirically proving ssnets better approximation signal used classiﬁer. furthermore evaluation gradient main contributions paper threefold first propose novel interpret adversarial perturbations namely terms eﬀective input signal classiﬁers second introduce robust ﬂexible defense large-scale adversarial attacks based ssnets. third provide comprehensive baseline evaluation adversarial attacks several state-ofthe-art models large dataset. fast growing interest phenomenon adversarial attacks gained momentum since discovery three main areas focus. ﬁrst area seeks eﬀective ways reaching adversarial spaces. ﬁrst comprehensive analysis extent adversarial spaces explored proposing fast method compute perturbations based sign gradients. iterative version method later introduced shown work signiﬁcantly better even applied images physically printed digitized again. prominent exception attacks based gradients succeeded using evolutionary algorithms nevertheless practical wide spread method mostly costly compute. papernot showed eﬀective adversarial attacks could even assumptions attacked model. finally methods beyond greedy iteration gradient space perform diﬀerent kinds optimization maximize misclassiﬁcation minimizing norm perturbation second area focuses understanding properties adversarial perturbations. work goodfellow already pointing linear nature neural networks main enabler adversarial attacks. went opposition initially theorized claim non-linearities main vulnerability. possible perturb natural looking images look like something completely diﬀerent also possible models issuing predictions high conﬁdence using either noise images highly artiﬁcial patterns transferability adversarial perturbations shown possible crafting attacks network using fool second classiﬁer however transferable attacks limited simpler methods iterative ones tend exploit particularities model hence lose power used diﬀerent architectures turns adversarial noise transferable models also possible transfer single universal adversarial perturbation samples dataset achieve high misclassiﬁcation rates said individual perturbations even applied physical objects bias model towards speciﬁc class third arguably popular area research focused networks protected attacks. strategies include changing optimization objective account possible adversarial spaces detection dataset augmentation includes adversarial examples suppressing perturbations obfuscating gradients prevent attackers estimating eﬀective perturbation work build idea using compressed representation input tailored towards speciﬁc characteristic adversarial perturbations using notion useful input signal furthermore explore nature adversarial perturbations relationship network capacity terms used signal. section explains detail architecture ssnet particular signal-preserving training scheme followed empirical evaluation gradients provides. start testing robustness ssnets white-box setting compare simple baseline using regular aes. recreate realistic attack conditions test ensemble network simulating re-parametrization technique similar aimed circumventing defense explore strategies cope attack. next examine performance ssnets gray-box scenario. finally provide evaluation transferability single step attacks diﬀerent models correlate overlap terms input signal perspective adversarial perturbations. suﬃcient train underlying segnet architecture convergence. network referred able reproduce input signals required diverse classiﬁers top- accuracy within percentage model eﬀective input signal used trained classiﬁer palacio propose ﬁne-tune decoder using gradients itself. allows learn decode input retains signal required ﬁne-tuned variant called reconstructs images original top- accuracy preserved amount information reconstructed image decreases respect original sample. note that since encoder trained unsupervised intermediate representations produced entirely class-agnostic. means that backpropagation ssnet gradients read shallowest layer correspond information structure. intuitively gradients point parts image changed inﬂuence reconstruction error. following measure extent gradients shift images forwarded networks. furthermore explore quantify emerging resilience adversarial attacks section verify structure-to-signal training scheme produces large shifts distribution gradients forward images classiﬁer pre-trained segnet ﬁne-tuned counterpart compare gradients. magnitude gradients instead values stress diﬀerences spatial distribution. large change position gradients originally occur within image good indicator information conveyed gradients changed. defend resnet plain segnet coupled resnet ﬁne-tuned ssnet without classiﬁer plain segnet alone ﬁrst three models require gradients computed respect class label last produced measuring reconstruction error. also note true reconstruction cannot directly obtained indirectly deﬁned classiﬁer ﬁne-tuned case reconstruction gradients computed comparing output original input. table reports mean ssim gradients imagenet’s validation combinations network pairs. note dissimilarity indicates structure-to-signal training scheme indeed changed reconstruction process i.e. identity function computed based input. similarly comparing ssim values reveals evidence class-agnostic nature gradients propagated found comparing ssim gradient magnitudes diﬀerent target labels used compute gradients. input image figure visualizes phenomenon. here gradient magnitudes observed predominantly highlight edges source error. expected since diﬃcult accurately reproduce high frequencies required sharp edges compared lower frequencies blobs. extracting unavoidable since edges also important classiﬁcation however classiﬁer diﬀers considerably patterns. overall ssim least twice high variants classiﬁer conﬁgurations. table pairwise mean ssim input gradient magnitudes resnet imagenet validation without passed ssim values w.r.t. variant show least similarity. section presents experiments quantifying robustness ssnets used defense adversarial attacks experimental setup closely follows conditions order facilitate comparability image classiﬁer attack resnet inception pre-trained imagenet target models. classiﬁers trained clean conditions i.e. special considerations respect adversarial attacks made training. defense train ssnet classiﬁers attack following scheme described section defenses denoted respectively. perturbation magnitude normalized norm clean sample adversary deﬁned attack methods protected models tested single step method iterative variant optimization-based alternative. note replicate realistic threat conditions resulting adversarial samples cast discrete range defense. includes reading access predictions classiﬁer intermediate activations backpropagated gradients. attacker forced forward valid images defended network setting input images ﬁrst ssnet reaching original classiﬁer. similarly gradients read shallowest layer ssnet. comparison attacks also unprotected versions resnet inception results summarized figure contrast ssnets provide high levels protection. optimization attack generally capable comparison expected eﬀective breaking ssnet defense. however introducing large perturbations even small values fact none attacking conﬁgurations able entirely fool defended clasresults surprise already comparable state-of-the-art graybox defenses despite less favorable conditions white-box defense ssnets already match alternative state-of-the-art protections tested permissive assumptions allowed gray-box settings. completeness also evaluate defense using pre-trained segnet instead ssnet resnet similar defense proposed meng conﬁrm suﬃce guard classiﬁer adversarial attacks performance consistently lower ssnets. visual analysis attacks reconstructions ssnets refer reader supplementary material. bypassing ssnets reparametrization order push limssnets simulate hostile scenario attacker tries actively circumvent defense mechanism. based work athalye reparametrization input space implemented defenses original input function hidden state note main motivation behind reparametrization alleviate instabilities gradients caused defenses rely said instability. although ssnets deep architectures resilience defense lays directed change induced within information contained gradients. addition trivial come suitable inducing transformation gradients done ssnets. despite concerns assume experiment found attacking indeed circumvent ssnets altogether. simulate potential strength attack using gradients original classiﬁer applying input directly. resulting adversarial attack passed hardened classiﬁer. refer hypothetical scenario white-box+. results shown figure conditions observe hardened models revert back behavior shown corresponding unprotected versions fgsm bim. general condition expected conﬁrms ssnets trained preserve information useful classiﬁer. gradients collected directly vulnerable model deﬁnition information useful classiﬁcation hence perturbations based gradients preserved ssnets. interestingly successful previous white-box experiment highly optimized perturbations less eﬀective here. believe overﬁtting strongly adversarial signal original model ssnets useful preserve. point exploit beneﬁts ssnets enjoy defense adding another layer protection showing aﬀects eﬀectiveness attack. shown palacio following structure-to-signal training scheme exhibit strong resilience random noise opposed traditionally trained aes. demonstrate straightforward layer protection based random noise. said stochastic strategy added adversarial image computed passes ssnet defense. experiment three sources noise increases resilience adversarial attacks improves. initial degradation zero adversarial attacks dependent amount noise tuned trade-oﬀ maximum accuracy adversarial robustness. contrast white-box scenarios gray-box attacks assume limited access information attack target. speciﬁcally conditions gray-box deﬁne attacker knowledge network defense strategy. given compositional nature ssnets novel defend classiﬁcation network gray-box conditions consists giving access gradients ssnets. attack ready ssnet removed perturbed image processed classiﬁer only. words classiﬁer corresponding defense attack crafted based combined network attacks experimental settings described section enforce defense policy following gray-box− conditions. results presented figure overall scenario consistently robust adversarial attack. fooling perturbations clearly visible cannot considered adversarial anymore. bim-based attacks classiﬁers gain back roughly half much accuracy white-box case. again comparable fgsm i.e. mostly ineﬀective consistent observations made white-box+ case. combining results gray-box− white-box+ reason relationship pxf◦g. first white-box+ experiment tells attacks crafted valid attacks following deﬁnition spaces holds pxf◦g. furthermore looking gray-box− experiments taking elements create attack produces elements pxf◦g. attack conclude elements ﬁnally conclude tested attacks cannot reachable classiﬁer relationships adversarial space given high robustness ssnets gray-box− scenario expect insights traditional black-box attacks. instead explore relationships signal diﬀerent image classiﬁers reported intuitively signal used classiﬁer also encompasses signal another classiﬁer uses adversarial attack crafted also fool note relation directional hold opposite way. test this construct adversarial samples using fgsm four reference classiﬁers alexnet resnet inception classiﬁer black-box attacks perturbations computed three models. resulting accuracies shown figure expected adversarial examples higher compatible architectures result higher fooling rations overall adversarial samples created alexnet least compatible among four mainly comparably lower accuracy limited amount input signal used. hand resnet produces compatible perturbations model generating attacks eﬀective tried architectures. either criterion results clearly show easiest fool coming ﬁrst place followed resnet alexnet ﬁnally inception relationship terms useful signal aligns results furthermore similar experiments based universal perturbations indicate selection similar architectures maintain relationship. proposed ssnets method defend neural networks adversarial examples. model defense strategy transformation domain used attackers namely gradients coming attacked classiﬁer. instead focusing gradient obfuscation non-diﬀerentiable methods instabilities purposely induce transformation gradients strip semantic information. ssnets work masking classiﬁer function composition. information inputs preserved classiﬁcation gradients point structural changes reconstructing original sample. defense possible using novel two-stage structure-to-signal training scheme deep aes. ﬁrst stage trained unsupervised traditional way. second part decoder gets ﬁne-tuned gradients model defended. evaluate proposed defense white-box gray-box settings using large scale dataset three diﬀerent attack methods highly performing deep image classiﬁers. baseline comparison shows twostaged training scheme performs better using regular aes. interestingly show resiliency adversarial noise white-box conditions exhibit comparable performance state-of-the-art favorable gray-box settings. furthermore show properties ssnets exploited defense mechanisms maintain robustness even harshest albeit currently hypothetical conditions protection ssnets circumvented. gray-box scenario also tested defense consists removal ssnets showing high levels robustness attacks. finally comparison resiliency four well-known deep cnns presented providing evidence relation order exists classiﬁers terms amount signal use; time terms eﬀectiveness adversarial noise. ssnets transformation occur gradient space. would like explore ways transformation occur even intersection gradients yielding successful adversarial perturbations eﬀectively zero. signal-preserving nature ssnets make defense potential mechanism explore understand nature attacks. comparing classiﬁcation consistency clean sample passed ssnet potential implications detection adversarial attacks learning abnormal distribution ﬂuctuations.", "year": 2018}