{"title": "Deep Neural Network Compression with Single and Multiple Level  Quantization", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Network quantization is an effective solution to compress deep neural networks for practical usage. Existing network quantization methods cannot sufficiently exploit the depth information to generate low-bit compressed network. In this paper, we propose two novel network quantization approaches, single-level network quantization (SLQ) for high-bit quantization and multi-level network quantization (MLQ) for extremely low-bit quantization (ternary).We are the first to consider the network quantization from both width and depth level. In the width level, parameters are divided into two parts: one for quantization and the other for re-training to eliminate the quantization loss. SLQ leverages the distribution of the parameters to improve the width level. In the depth level, we introduce incremental layer compensation to quantize layers iteratively which decreases the quantization loss in each iteration. The proposed approaches are validated with extensive experiments based on the state-of-the-art neural networks including AlexNet, VGG-16, GoogleNet and ResNet-18. Both SLQ and MLQ achieve impressive results.", "text": "yuhui yongzhuang wang aojun zhou weiyao hongkai xiong school electronic information electrical engineering shanghai jiao tong university china figure comparison single-level quantization multilevel quantization. blue parts indicate full-precision weights layers network. orange parts quantized weights layers. major challenge network compression tradeoff complexity accuracy. however recent network compression methods degrade accuracy network less recently propose incremental network quantization re-trains un-quantized parameters compensate quantization loss achieve high compression rate maintaining performance. however attention distribution parameters treat layers equally paper argue width level depth level important network quantization width level quantization distribution weights directly affects accuracy network. vector quantization quantization method fully considers distribution makes quantization loss easy controlled. furthermore weights special type special extend approach using norm constrain clustering process. depth level layers important elements networks. interacted make joint contributions networks. thus quantization loss layer eliminated re-training layers. ternary quantization huge quantization loss compensated re-training considering width network quantization effective solution compress deep neural networks practical usage. existing network quantization methods cannot sufﬁciently exploit depth information generate low-bit compressed network. paper propose novel network quantization approaches single-level network quantization high-bit quantization multi-level network quantization extremely low-bit quantization ﬁrst consider network quantization width depth level. width level parameters divided parts quantization re-training eliminate quantization loss. leverages distribution parameters improve width level. depth level introduce incremental layer compensation quantize layers iteratively decreases quantization loss iteration. proposed approaches validated extensive experiments based state-of-the-art neural networks including alexnet vgg- googlenet resnet-. achieve impressive results. recent years deep convolutional neural networks playing important role variety computer vision tasks including image classiﬁcation object detection semantic segmentation face recognition promising results dnns contributed many factors. regardless training resources powerful computational hardware large number learnable parameters important one. achieve high accuracy deeper wider networks designed turn poses heavy burden storage computational resources. becomes difﬁcult deploy typical model resource constrained mobile devices mobile phones drones. thus network compression critical become effective solution reduce storage computation costs models. ∗corresponding authors weiyao hongkai xiong {wylin xionghongkai}sjtu.edu.cn. copyright association advancement artiﬁcial intelligence rights reserved. level. thus introduce incremental layer compensation quantize layers partially retrain layers compensate quantization loss. considering width level depth level accuracy recovered iteratively ternary quantization. summary contributions network compression folds propose single-level quantization approach high-bit quantization. extremely lowbit quantization propose multi-level network quantization. rest paper ﬁrst introduce related works propose single-level quantization approach. next introduce multi-level approach. finally give experiment results conclusion paper. compression low-rank decomposition. reducing parameter dimensions using techniques like singular value decomposition works well fully-connected layers achieve compression rate. introduce idea convolutional layers noting weight ﬁlters usually share smooth components low-rank subspace also remember important information represented weights sparsely scattered outside low-rank subspace. although kind method achieve relatively good compression rate accuracy neural network models hurt. compression pruning. pruning straightforward method compress networks removing unimportant parameters convolutional ﬁlters. present effective unstructured method prune parameters values threshold reduce model size alexnet vgg-. filter level pruning greatly reduce computation cost. prune ﬁlters small effect accuracy model reduce computation cost vgg- resnet- compression quantization. quantization manyto-few mapping large input smaller output. groups weights similar values reduce number free parameters. hash-net constrains weights hashed different groups before training. within group weights shared shared weights hash indices need stored. compress network vector quantization techniques. present deep compression combines pruning vector quantization huffman coding reduces model size alexnet vgg-. however quantization methods takes time less hurt performance network. recently present incremental network quantization method. method partitions weights different parts part used quantize another part used retrain compensate quantization loss. weights network quantized incrementally ﬁnally accuracy quantized model even higher original one. method basically solves problem accuracy loss network compression. however paper values codebook pre-determined quantization group handcrafted. thus kind quantization data based quantization loss controlled. besides partition weights refer width level achieve great result extremely low-bit quantization. compression strategies. people trying design dnns precision weights gradients activations. propose xnor-net network binary weights even binary inputs. discuss basic elements training high accuracy binary network. design ternary weight network. propose hwqn activations. knowledge transfer another method train small network. knowledge distilling proposed distill knowledge ensemble models single model imitate soft output them. neuron selectivity transfer method explores kind knowledge neuron selectivity transfer knowledge teacher model student model achieves better performance. speciﬁc architectures designed mobile devices. propose mobilenets apply depth-wise separable convolution factorize standard convolution depthwise convolution convolution show effectiveness architecture across wide range applications. present shufﬂenet. apply group convolutions pointwise convolutions introduce shufﬂe operations maintain connections groups achieves speed alexnet. framework approach shown figure either single-level quantization multi-level quantization composed four steps clustering loss based partition weightsharing re-training. clustering uses k-means clustering cluster weights clusters layer-wise. loss based partition divides clusters layer disjoint groups based quantization loss. weights group quantized centroids corresponding clusters weight-sharing step. weights group re-trained. furthermore four steps iteratively conducted weights quantized. mainly difference loss based partition step. partition clusters. partition clusters layers. actually particular case mlq. technique details discussed next sections. group containing clusters quan group containing clusters tized re-trained. covers weights layer. quantization loss cluster. minimum clusters bigger maximum clusters weight-sharing quantize weights group weightsharing. weights group quantized centroids corresponding clusters. weight-sharing layer described equation centroid re-training weight-sharing brings error network need retrain model recover accuracy. thus quantized weights re-train weights group. re-training shown figure come back beginning approach quantize left weights iteratively weights quantized. taking layer example denote quantized weights layer. simplify problem deﬁne mask matrix size weight matrix acts indicator function indicate weights quantized. deﬁned solve optimization problem re-train network using stochastic gradient decent update un-quantized weights. quantized weights indicator function mask gradient weights control gradient propagation figure shows quantization different clusters generated k-means alexnet. means value weights quantized divided centroid cluster c.the accuracy network changes change shows test accuracy clusters alexnet quantized respectively. clusters sorted descending order. clustering using handcrafted mapping rules adopt k-means clustering datadriven easily control quantization loss. choose clusters generated k-means quantize weights different values including centroids them. figure shows quantization loss quantize weights centroids clusters. loss based partition layer-wise clustering layer holds code book denotes centroid code book layer. partition weights groups weights group quantized weights re-trained. pruning inspired strategy weights bigger values important need quantized prior. however strategy suitable approach accuracy network affected many factors quantization including value quantized number weights quantized. test quantization loss different clusters alexnet generated k-means. result shown figure exist clusters pruning inspired strategy beneﬁt clustering weights roughly partitioned need partition clusters. besides fact number clusters relatively small propose loss based partition. test quantization loss cluster sort clusters quantization loss. cluster bigger quantization loss quantized prior. figure schematic diagram single-level quantization approach small rectangle codebook. blue green orange points indicates full precision re-trained quantized weights. clustering conducted pre-trained full-precision network; performing loss based partition clusters; group clusters weights quantized centroids clusters; fixing quantized weights clusters re-trained; re-trained weights clustered k-means clustering; several iterations weights quantized centroids. reset base learning rate learning policy apply k-means clustering layer-wise perform loss based partition layer-wise equation quantize weights group equation re-train network described re-training section proposed approach suitable low-bit quantization because number clusters small quantization loss iteration step huge eliminated. introduce incremental layer compensation partition layers network depth level network. motivated intuition different layers different impact performance network quantization e.g. convolutional layers fully connected layers. layers network partitioned groups group containing layers quantization loss quantized prior another group containing remaining layers re-trained introduce multi-level quantization partitions layers parameters within layers lowers huge quantization loss low-bit quantization taking layer example layer clustered clusters oblater extend single-level quantization approach. approach quantize weights layer-wise centroids clusters. however sometimes need weights special type. instance weights power model convenient deployed fpga devices. main difference extended single-level quantization original extend traditional clustering constrain cluster centroid close equal number oriented type thus weight-sharing quantize weights values oriented type. instance want constrain centroid close equal speciﬁc type t-centroid incorporate norm regulation tradici tional k-means loss function eslq ﬁrst conduct traditional clustering loss based partition. determine t-centroids cluster quantized. subsequently re-cluster weights extended clustering. weight-sharing re-training steps slq. several iterations network quantized oriented type. figure quantization process multi-level ternary quantization. blue green orange parts indicates full precision re-trained quantized layers. ﬁrst quantize boundaries quantize hearts network. results cifar- light offered caffe resnet conduct classiﬁcation cifar-. light trained scratch iterations trained full-precision light model quantized -bit low-precision model quantization loss iteration decreasing. quantization results networks shown table networks enjoy accuracy increase quantization slq. results imagenet apply proposed approach various popular models imagenet including alexnet googlenet resinet- fullprecision networks quantized -bit precision ones. setting parameters shown table cluster partition ways four networks means approach easier implement robust different architectures. results shown table -bit models quantized better performance imagenet large scale classiﬁcation task accuracy full-precision references. also compare results approach achieves improvement accuracy accuracy. shows considering distribution weights quantization important loss based partition also contributes increase. tain three centroids affect performance networks more. call boundaries. holding smaller effect called heart. ﬁrst quantize boundaries network. different quantizes boundaries time quantizes boundaries iteratively ilc. boundaries different layers partitioned groups group quantized remaining weights network re-trained. boundaries quantized quantize hearts iteratively too. several iterations boundaries hearts quantized bit-width parameter represents space used store quantized weight. fairly compared methods bits code centroids store zero bits code non-zero centroids means bit-width centroid number layer cifar- dataset consists colour images classes images class. training images test images. results low-bit setting experiment test approach different bitwidth settings. vgg- test model. except original -bit quantization result present -bit -bit results shown table -bit compressed model -bit compressed model also good performance top- top-. however bit-width means centroid number accuracy compressed model drops little. loss based partition step related number centroids. centroid number enough iterations quantization. centroid number small less iterative quantization steps quantization loss last quantization step big. accuracy -bit compressed model slightly lower reference full-precision vgg- model. thus ways conduct extremely low-bit quantization. partition ways experiments described bellow results centroid initialization conduct experiments show effect centroid initialization approach. choose kinds centroid initialization ways. linear non-linear choose vgg- test model. results shown table accuracy model quantized non-linear initialization higher accuracy linear initialization. centroid clusters quantized last iteration smaller number weights also smaller. leads smaller quantization loss last iteration. thus adopt nonlinear initialization experiments. results eslq experiment test eslq approach. highlights eslq approach quantize weights oriented type t-centroids. test choose types scientiﬁc notation signiﬁcant ﬁgures either power zero. experiment results shown table table eslq indicates scientiﬁc notation eslq indicates power model quantized eslq situations accuracy increase shows effectiveness eslq. results quantize light resnet ternary networks cifar-. experiments train networks cifar- without using data augmentation. results shown accuracy ternary light ternary resnet decrease little compared fullprecision ones. alexnet model quantized ternary imagenet mlq. compare approach bach normalization layers baseline alexnet reach moreover batch normalization layers also contribute convergency network training. quantize ﬁrst convolutional layer last fully connected layer another reason high performance. different them approach robust change architecture network quantize layers alexnet still achieve comparable results. another method conducts ternary quantization without additional training. method outperforms though training time cost. compression ratio acceleration compression ratio easily computed bitwidth networks. compression ratio compressed alexnet besides proposed approach combined pruning strategy compress network. -bit pruned alexnet compressed without accuracy loss. since current blas libraries support indirect look-up relative indexing accelerators designed quantized models adopted. paper propose single-level quantization multi-level quantization considering network quantization width depth. taking distribution parameters account obtains accuracy gain high-bit quantization state-of-the-art networks datasets. besides achieves impressive results extremely low-bit quantization without changing architecture networks. work supported part nsfc program shanghai academic research leader tencent research grant. would like thank haoyang shenzhen tencent computer system co.ltd. valuable discussions paper.", "year": 2018}