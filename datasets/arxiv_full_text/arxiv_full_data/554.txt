{"title": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models", "tag": ["cs.LG", "cs.CL", "cs.NE"], "abstract": "We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.", "text": "formation network input hidden output layer typically trained keeping rest network parameters ﬁxed. motivations approaches stem concept relatively parameters could learned adaptation therefore preferable training entire network adaptation limited. linear hidden network layer approaches last hidden layer usually designed bottleneck ensure affordable parameter size however adapting parameters cd-dnn-hmm much challenging earlier connectionist adaptation schemes huge parameter size large number network branches connected large tied states often referred senones furthermore parameters adapted every sample frame regardless senone class. therefore posterior probabilities unobserved scarcely seen senones often pushed towards zero adaptation. phenomenon commonly referred catastrophic forgetting conservative ad-hoc solutions anns proposed force senone distribution estimated adapted model close unadapted model. example kullback-leibler divergence based objective criterion used adaptation devised order alleviate catastrophic forgetting problem. variation standard method assigning target values instead discussed nonetheless bayesian solutions adopted cd-gmm-hmms address issue fully exploited. study attempt cast adaptation within bayesian framework spirit maximum posteriori adaptation goal re-estimate parameters representing available information augmented linear hidden network added last non-linear hidden layer. experimental results word open vocabulary wall street journal task demonstrates feasibility proposed approach. supervised adaptation proposed adaptation scheme provide relative word error rate reduction already-strong speaker independent cd-dnn-hmm baseline consistently outperform conventional transformation based adaptation schemes. also compares favorably feature space maximum posteriori linear regression approach speaker adaptation proposed also present initial attempt generate hierarchical priors improving adaptation efﬁciency small amounts adaptation data exploiting similarities among senones. present bayesian approach adapting parameters well-trained context-dependent deep-neural-network hidden markov model improve automatic speech recognition performance. given abundance parameters limited amount data effectiveness adapted model often compromised. formulate maximum posteriori adaptation parameters specially designed cd-dnn-hmm augmented linear hidden networks connected output tied states senones compare feature space linear regression previously proposed. experimental evidences -word open vocabulary wall street journal task demonstrate feasibility proposed framework. supervised adaptation proposed adaptation approach provides relative error reduction consistently outperforms conventional transformation based methods. furthermore present initial attempt generate hierarchical priors improve adaptation efﬁciency effectiveness limited adaptation data exploiting similarities among senones. index terms deep neural networks hidden markov model bayesian adaptation automatic speech recognition. despite recent outstanding results demonstrated contextdependent deep-neural-network based hidden markov models various automatic speech recognition tasks data sets acoustic models similarly conventional context-dependent gaussian-mixturemodel based hmms still suffer performance degradation potential mismatched conditions training testing conditions. standard hybrid system using artiﬁcial neural networks hmms cd-dnn-hmm special case exist many adaptation techniques. simplest approach modiﬁes weights connectionist architecture using adaptation materials. unfortunately leads over-ﬁtting adaptation material amount adaptation patterns limited recent approaches regularization based subspace based transformation based i-vector based native neural network based factorization based fast adaptation schemes based discriminant speaker codes proposed circumvent problem. figure basic neural architecture adapting hmm/ann parameters weights associated links dashed rectangles estimated weights remain unchanged. activation function units linear function. using layer might harm ability data representation hidden layers. hand approach issue usually can’t reduce number neurons output layer want directly model senones means huge augmented layer even parameters adapted. deem hidden layers feature extractor output layer discriminative model. model parameters weights output layer’s afﬁne transform matrix output expressed activation last hidden layer used feature representation extracted hidden layers. adding augmented last hidden layer equivalent applying transformation matrix wlhn model parameters obtain adapted model parameter adaptation structure shown figure formulation quite similar maximum likelihood linear regression difference mllr model parameters gaussian mean variance model parameters log-linear model’s transformation matrix weights. although conventional adaptation approaches alleviate over-ﬁtting issues reducing number parameters adapted number could still cases. inspired adaptation address problem effectively gmm-hmm systems section explain apply approach adaptation. note though choose demonstration proposed approach easily applied adaptation frameworks like well. weight matrices bias vectors input frame time total number hidden layers sigmoid softmax functions element-wise operations. vector corresponds pre-nonlinearity activations vectors neuron outputs hidden layer output layer respectively. softmax outputs considered estimate senone posterior probability trained maximizing posterior probability training frames. equivalent minimizing cross-entropy objective function. whole training contains frames i.e. loss respect given deﬁned target probability frame real practices systems target probability often obtained forced alignment existing system resulting target entry equal mini-batch stochastic gradient descent reasonable size mini-batches make matrices memory used update neural parameters training. pre-training methods used initialisation parameters adaptation researchers choose afﬁne transformation network last hidden layer output layer weights matrix i.e. adapt parameters keeping ﬁxed parameters order reduce amount parameters adapt usually last hidden layer designed bottleneck superior results obtained kind formulation transformation based adaptation schemes linear input network linear output network approach performs adaptation adding augmented linear input layer adapts parameters. follow common idea hidden layers actually learning suitable data representation extracting better feature output layer serving log-linear model transforming input speakers sentences speaker) used perform adaptation afﬁne transformation added speakerindependent dnn. standard open vocabulary -word read senneheiser microphone data used evaluation. standard trigram language model adopted decoding. performance given terms word error rate hidden layers. ﬁrst hidden layers units whereas last hidden layer units. output layer softmax units. architecture follows conventional conﬁgurations used speech community except last hidden layer acts bottleneck layer. conﬁguration chosen large dimension last non-linear hidden layer might harmful adaptation. bottleneck based rank methods widely used achieve compact models equivalent performance number units equal chosen simulate sort three-state phone layer thereby obtaining kind hierarchical structure mono-phones hidden layer senones output layer. input feature vector dimension mean-normalized log-ﬁlter bank feature second-order derivatives context window frames forming vector -dimension input. trained initial learning rate using crossentropy objective function. initialised stacked restricted boltzmann machines using layer layer generative pre-training. word error rate attained different adaptation techniques reported table available adaptation material used performing adaptation namely sentences speaker. term baseline refers speaker independent cd-dnn-hmm system. lin-kld refer adaptation technique based standard linear input network approach regularisation technique combination maximum posteriori transformation based adaptation prior deﬁned parameters respectively. terms lon-kld used denote little abuse terminology direct adaptation output layer weights matrix without respectively. adaptation results also reported along corresponding version adaptation approach proposed paper. since inserted last hidden layer output layer weights matrix dimension initialised identity matrix zero bias gives starting point equivalent unadapted model. supervised adaptation performed updating parameters. performed described section sake comparison lhn-kld denotes standard combined also evaluated. indeed outperforms attains worst performance improvement. always improves afﬁne transformation based adaptation techniques expected. furthermore proposed outperforms techniques given task attains best recognition results relative improvement baseline. finally would like remark compares favourably conﬁrms density utilized training data baseline dnn. adopted empirical bayes approach treated speaker training sample speaker supervised adaptation performed. that particular speaker. observed histograms weights adapted speakers quite like gaussian assume distribution weights wlhn joint gaussian expressing weights transformation matrix wlhn vector entry representing particular weight prior density following form diagonal entries covariance matrix non-zero close look prior density standard gaussian learning degenerate conventional l-regularized training. gradient respect expressed study concerned problem speaker adaptation experiments reported k-word open vocabulary wall street journal task using kaldi toolkit baseline cd-dnn-hmm system trained using material standard adaptation similar ideas recently explored learning enhancing classiﬁcation performance classes examples hierarchical priors devised output layer weights matrix using tree data structure either ﬁxed learnable training. top-level weights hybrid acoustic model regarded senone embeddings hierarchical priors deﬁned organising embedding tree data structure. denote output layer weights matrix line corresponds senone embedding. speciﬁcally senone embedding denoted w×l. tree structure used generate hierarchical priors either learnt training given. here used ﬁxed two-layer tree shown figure leaf nodes leaf corresponding senone embedding parent nodes clustering together similar leaf nodes. parent node clusters senone embeddings sharing central phone-state; therefore equal work. hierarchical priors established associating vector leaf node vector parent node imposing gaussian probability density distribution vectors follows focus attention experimental results small adaptation data amounts shown table limited adaptation data namely utterances small performance improvements observed using priors adaptation carried hierarchical priors. although current improvement still quite small believe sophisticated trees adopted better performance future studies. investigated maximum posteriori adaptation approach linear hidden networks. idea treat parameters augmented afﬁne transformation random gaussian variables incorporate prior information obtained training data. speaker adaptation results show proposed approaches lead consistent performance improvement conventional adaptation. furthermore outperforms regularisation schemes. ﬁrst attempt hierarchical-based priors ﬁxed two-layer tree structure also studied small improvements observed preliminary experiments using limited amount adaptation sentences. better results might still hindered current ﬁxed tree hierarchy structure employed preliminary work. indeed demonstrated learning tree hierarchy training improves classiﬁcation performance finally objective function perspective still relying cross-entropy. forms frame-level sequence-level discriminative objectives also applied. sainath kingsbury ramabhadran fousek novak mohamed making deep belief networks effective large vocabulary continuous speech recognition proc. asru dahl deng acero context-dependent pre-trained deep neural networks large-vocabulary speech recognition ieee trans. audio speech language processing vol. seide kl-divergence regularized deep neural network adaptation improved large vocabulary speech recognition proc. icassp chen deng factorized deep neural networks adaptive speech recognition proc. int. workshop statistical machine learning speech processing siniscalchi c.-h. hermitian polynomial speaker adaptation connectionist speech recognition systems ieee trans. audio speech language processing vol. abdel-hamid jiang fast speaker adaptation hybrid nn/hmm model speech recognition based discriminative learning speaker code proc. icassp abdel-hamid jiang direct adaptation hybrid dnn/hmm model fast speaker adaptation lvcsr based speaker code proc. icassp abdel-hamid jiang fast adaptation deep neural network based discriminant codes speech recognition ieee/acm trans. audio speech lang. proc. vol. sainath kingsbury sindhwani arisoy ramabhadran low-rank matrix factorization deep neural network training high-dimensional output targets acoustics speech signal processing ieee international conference gauvain c.-h. maximum posteriori estimation multivariate gaussian mixture observations markov chains ieee trans. speech audio processing vol. dekel gilad-bachrach shamir xiao optimal distributed online prediction proc. icml hinton salakhutdinov reducing dimensionality data neural networks science vol. leggetter woodland maximum likelihood linear regression speaker adaptation continuous density hidden markov models computer speech language vol. povey ghoshal boulianne burget glembek goel hannemann motlicek qian schwarz silovsk`y stemmer vesel`y kaldi speech recognition toolkit proc. asru huang weng c.-h. beyond cross-entropy towards better frame-level objective functions deep neural network training automatic speech recognition proc. interspeech", "year": 2015}