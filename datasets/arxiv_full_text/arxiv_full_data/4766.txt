{"title": "Efficient Algorithms for Bayesian Network Parameter Learning from  Incomplete Data", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose an efficient family of algorithms to learn the parameters of a Bayesian network from incomplete data. In contrast to textbook approaches such as EM and the gradient method, our approach is non-iterative, yields closed form parameter estimates, and eliminates the need for inference in a Bayesian network. Our approach provides consistent parameter estimates for missing data problems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approach is orders of magnitude faster than EM (as our approach requires no inference). Given sufficient data, we learn parameters that can be orders of magnitude more accurate.", "text": "propose efﬁcient family algorithms learn parameters bayesian network incomplete data. contrast textbook approaches gradient method approach non-iterative yields closed form parameter estimates eliminates need inference bayesian network. approach provides consistent parameter estimates missing data problems mcar cases mnar. empirically approach orders magnitude faster given sufﬁcient data learn parameters orders magnitude accurate. learning parameters bayesian network data missing values conventional wisdom among machine learning practitioners options either expectation maximization likelihood optimization gradient method; e.g. approaches known consistently estimate parameters values data missing random however standard approaches suffer following disadvantages. first iterative hence require many passes potentially large dataset. next require inference bayesian network already intractable finally algorithms stuck local optima means that practice must algorithms multiple times different initial seeds keep parameter estimates obtained best likelihood. recently mohan pearl tian showed joint distribution bayesian network recovered consistently incomplete data mcar problems well major subset mnar problems given access missingness graph. graph formal representation causal mechanisms responsible missingness incomplete dataset. using ∗both authors contributed equally work. gvdb also representation able decide whether exists consistent estimator given query answer afﬁrmative identify closed-form expression estimate terms observed data asymptotically consistent. based framework contribute practical family parameter learning algorithms bayesian networks. insight work following. exists most-general least-committed missingness graph captures mcar assumption invokes additional independencies. although mitechnical observation far-reaching consequences. enables techniques mohan pearl tian applied directly mcar data without requiring user provide speciﬁc missingness graph. hence enables algorithms serve drop-in replacements already inﬂuential algorithm existing applications. results practical algorithms learning parameters bayesian network incomplete dataset following advantages parameter estimates efﬁciently computable closed-form requiring single pass data data missing advantages signiﬁcant computational advantages particular dataset size large bayesian networks intractable exact inference. moreover advantage iterative optimization estimates suffer local optima. note advantages already available learning bayesian networks complete datasets properties certainly contributed popularity bayesian networks today probabilistic models. secondary contributions show factorize estimates extract information data additional information missingness mechanism improve convergence algorithms. moreover present initial experimental evaluation paper upper case letters denote variables lower case letters denote values. variable sets denoted bold-face upper case letters instantiations bold-face lower case letters generally denote variable bayesian network denote parents. network parameter therefore general form θx|u representing probability illustrative example consider figure depicting dataset directed acyclic graph bayesian network variables here value variable always observed data value variable missing. graph denote variable always observed double-circle. happen know mechanism causes value become missing data include model depicted figure here missingness variable reported variable depends value another variable graph called missingness graph serve useful tool analyzing missing data problems example augmented dataset graph variables. variable represents causal mechanism dictates missingness value mechanism active denote unob. otherwise mechanism passive denote variable acts proxy value data observed value special value value missing. value thus depends functionally variables corresponding unob otherwise proxy assumes observed value variable using missingness graphs analyze distribution observed missing values relate underlying distribution seek estimate data. mohan pearl tian show exploit conditional independencies graphs encode order extract consistent estimates missing data problems including mnar ones whose underlying assumptions would scope existing techniques. formally deﬁne missing data problems consider paper. learning bayesian network incomplete dataset underlying unknown distribution induced network want learn. variables partitioned sets fully-observed variables partially-observed variables missing values data. take account mechanisms cause values variables missing example above introducing variables representing causal mechanisms themselves variables proxies variables augmented bayesian network refer missingness graph variables fully-observed variables partially-observed. moreover network induces distribution embeds original distribution network marginal distribution. recently mohan pearl tian identiﬁed conditions missingness graph allow original partially-observed distribution identiﬁed howfully-observed distribution ever practice access dataset corresponding data distribution induces paper show leverage results mohan pearl tian even access complete missingness graph speciﬁes direct causes parents missingness mechanisms identify practical efﬁcient algorithms consistent estimation bayesian network parameters. first assume general conditions hold broad classes missingness graphs characterize commonlyused assumptions missing data. subsequently show exploit speciﬁc knowledge underlying missingness graph available obtain improved parameter estimates. missingness categories incomplete dataset categorized missing completely random mechanisms cause values variables missing marginally independent i.e. note data distribution well-deﬁned variables fully-observed augmented dataset represented compactly space linear need explicitly represent instantiations observed data. estimate simply using subset data every variable observed because data distribution tends true distribution implies consistent estimate marginals contrast technique listwise deletion corresponds estimate m|rxm= technique pairwise deletion corresponds above contains variables. facilitate comparisons interesting estimation algorithms shall subsequently consider refer general estimation approach direct deletion. direct deletion case data cannot simple deletion techniques described mcar data—the resulting estimates would consistent. however show next possible obtain consistent estimates data using technique simple efﬁcient direct deletion. roughly view technique deleting certain instances dataset re-weighting remaining ones consistent estimate obtained. contribution paper provides algorithm desirable properties described introduction. shall subsequently show obtain even better estimates later. again estimate network parameters θx|u sufﬁces show estimate family marginals denote fullymar assumption. observed variables outside family variables hence reduced problem estimating sets probabilities. estimating probabilities straightforward variables fully observed data. conditional probabilities contain partially observed variables conditioned fully observed variables deﬁnition implies subset data ﬁxes value locally mcar. analogous mcar case estimate conditional probability incomplete dataset categorized missing random missingness mechanisms conditionally independent partially-observed variables given fully-observed variables i.e. corresponds missingness graph variables allowed parents long none partiallyobserved. example missingness graph section adding edge results graph yields data. stronger variable-level deﬁnition previously used machine learning literature contrast event-level deﬁnition prevalent statistics literature present algorithms learn parameters θx|u bayesian network data distribution fully-observed variables augmented dataset. different missing data assumptions without knowing missingness graph generated data. estimate conditional probabilities θx|u parameterize bayesian network estimate joint distributions subsequently normalized. hence sufﬁces discussion estimate marginal distributions families here denote observed variables y∩xm denote partially-observed variables. further denote missingness mechanisms partially-observed variables appendix illustrates learning algorithms concrete dataset. statistical technique listwise deletion perhaps simplest technique performing estimation mcar data simply delete instances dataset contain missing values estimate parameters remaining dataset complete. course technique potentially ignore large parts dataset. next simplest technique perhaps pairwise deletion available-case analysis estimating quantity pair variables delete instances variable variable missing. consider following deletion technique expressed terms causal missingness mechanisms reviewed previous section. particular estimate marginals variables speciﬁcally factored deletion algorithm works follows. first estimate conditional probabilities edges lattice estimate using subset data variables observed. second propagate estimates bottom-up. node several alternative estimates available incoming edges. various ways aggregating estimates mean median propagating lowest-variance estimate. whereas direct deletion uses instances data variables observed factored deletion uses instance data least variable observed. learning missingness graph made general assumptions structure missingness graph capturing mcar assumptions. section show exploit additional knowledge missingness graph improve quality estimates. informed deletion suppose in-depth knowledge missing data mechanisms problem namely know subset observed variables sufﬁce separate missing values causal mechanisms i.e. exploit knowledge direct deletion algorithm obtain improved parameter estimates. particular reduce scope summation direct deletion algorithm variables variables yielding algorithm refer algorithm informed direct deletion. reducing scope summation need estimate fewer sub-expressions rym= ob). results efﬁcient computation further individual sub-expression estimated data. factored deletion propose class deletion algorithms exploit data direct deletion. ﬁrst step generate multiple consistent estimands query estimand utilizes different parts dataset estimate query. second step aggregate estimates compute ﬁnal estimate thus almost tuples dataset. since method exploits data direct deletion obtains better estimate query. discussed consistent estimate computed given factorization. shall detail estimates factorization aggregated compute accurate estimates number variables family number possible factorizations however different factorizations share sub-factors estimate once reuse across factorizations. organize computations using lattice figure nodes edges. algorithm compute many estimates edges lattice order number moreover estimates remain consistent. similarly replace factored deletion algorithm yielding informed factored deletion algorithm. appendix presents empirical evaluation informed deletion exempliﬁes cases knowing missingness graph lets consistently learn mnar data beyond capabilities maximum-likelihood learners. evaluate proposed learning algorithms simulate partially observed datasets bayesian networks relearn parameters data. consider following algorithms d-mcar f-mcar direct/factored deletion mcar remember d-mcar f-mcar consistent mcar data only d-mar f-mar consistent general data. consistent data converges maximum-likelihood estimates. evaluate learned parameters terms likelihood independently generated fully-observed test data kullback-leibler divergence original learned bayesian networks. report perinstance log-likelihoods evaluate learned models unseen data learning algorithms assume symmetric dirichlet prior network parameters concentration parameter appendix provides empirical results simpler case learning mcar data algorithms consistent. expected produce increasingly accurate estimates data becomes available. compared complexity inference prevents scaling large datasets d-mcar f-mcar obtain accurate estimates orders-of-magnitude faster. section investigate challenging problem learning data generated follows select m-fraction variables partially observed introduce missingness mechanism variable partially observed variable assign parents randomly selected observed variables giving preference neighbors network sample parameters missingness mechanism cpts beta distribution sample complete dataset values hide values accordingly. ﬁrst experiment work small network tractable enough scale large dataset sizes. figure shows fire alarm network variables missing data mechanisms generated beta distribution shape parameters numbers averaged repetitions different random learning problems. signiﬁcant difference without restarts indicating likelihood landscape challenging optimize em--jt performs well small dataset sizes stops converging around instances. could restarts getting stuck local optima. f-mar starts em--jt em--jt small sizes quickly outperforms largest dataset sizes learns networks whose several orders magnitude smaller improves further f-mar estimates seed although scale larger intractable networks. d-mcar f-mcar consistent data indeed converge biased estimate around factorized algorithms outperform direct counterparts. second experiment work classical alarm network variables. missing data mechanisms generated beta distribution shape parameters reported numbers averaged repetitions number reported minute time limit exceeded. figures show test likelihood function dataset size learning time. em--jt performs well small dataset sizes outperforms em-jt. however inference time non-negligible em-jt fails scale beyond instances whereas em--jt scales closed-form learners dominate versions function time scale dataset sizes orders magnitude larger. seeded fmar achieves similar quality em--jt signiﬁcantly faster learners. third experiment table reports results four larger networks exact inference challenging. method given time limit minutes data generated above. appendix provides results settings. consider following algorithms. em-jt em--jt algorithm used anytime fashion returns given time limit best parameters found restart even converge. em-jt performs exact inference scale well networks. problem mitigated em-bp performs approximate inference also difﬁculties scaling contrast f-mar particularly d-mar scale much larger datasets. accuracy f-mar method typically obtains best likelihoods larger datasets although em-bp perform better small datasets. evaluated d-mcar f-mcar although general consistent data scale even further also produce relatively good estimates estimating parameters bayesian networks maximum likelihood estimation typical approach used incomplete data common wisdom among machine learning practitioners needs expectation-maximization gradient methods discussed methods scale well large datasets complex bayesian networks iterative require inference bayesian network suffer local optima. considerable effort expended improving across dimensions order example accelerate convergence intelligently sample subsets dataset e.g. thiesson meek heckerman approximate inference algorithms lieu exact ones inference intractable e.g. ghahramani jordan; caffo jank jones escape local optima e.g. elidan suitable data exceptions recent work recommender systems explicitly incorporate missing data mechanisms case complete data parameter estimation task simpliﬁes considerably case bayesian networks maximum likelihood estimates obtained inferencefree closed-form using single pass data θx|u fact estimation algorithms proposed paper also obtain parameter estimates case complete data although concerned maximum likelihood estimation here— simply want obtain estimates consistent summary proposed inference-free closed-form method consistently learning bayesian network parameters mcar datasets empirically demonstrate practicality method showing orders-of-magnitude efﬁcient allowing scale much larger datasets. further given access enough data show method learn much accurate bayesian networks well. inference-free estimators proposed classes probabilistic graphical models. example abbeel koller identiﬁed method closed-form inference-free parameter estimation factor graphs bounded degree complete data. recently halpern sontag proposed efﬁcient inference-free method consistently estimating parameters noisy-or networks latent variables under certain structural assumptions. note inferencefree learning parameters bayesian networks under data factor graphs bounded degree complete data structured noisy-or bayesian networks latent variables surprising results. perspective maximum likelihood learning evaluating likelihood seems unavoidable ability consistently estimate murphy machine learning probabilistic perspective. press. pearl evidential reasoning using stochastic simulation causal models. artiﬁcial intelligence rubin inference missing data. biometrika thiesson meek heckerman accelerating large databases. machine learning tsamardinos aliferis statnikov statnikov algorithms large scale markov blanket discovery. proceedings flairs volume yaramakala margaritis speculative markov blanket discovery optimal feature selection. proceedings icdm. note knowing parents mechanism variable effectively equivalent purposes informed deletion knowing markov blanket variables learned data sufﬁcient domain knowledge expert able specify parents mechanism variables. sufﬁces even identify subset observed variables contains markov blanket; knowledge still exploited parameters without need inference greatly extends accessibility potential models. example opens door practical structure learning algorithms incomplete data notoriously difﬁcult problem practice", "year": 2014}