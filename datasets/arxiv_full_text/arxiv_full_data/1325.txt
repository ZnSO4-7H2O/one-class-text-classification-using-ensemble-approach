{"title": "Object Recognition Using Deep Neural Networks: A Survey", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Recognition of objects using Deep Neural Networks is an active area of research and many breakthroughs have been made in the last few years. The paper attempts to indicate how far this field has progressed. The paper briefly describes the history of research in Neural Networks and describe several of the recent advances in this field. The performances of recently developed Neural Network Algorithm over benchmark datasets have been tabulated. Finally, some the applications of this field have been provided.", "text": "recognition objects using deep neural networks active area research many breakthroughs made last years. paper attempts indicate ﬁeld progressed. paper brieﬂy describes history research neural networks describe several recent advances ﬁeld. performances recently developed neural network algorithm benchmark datasets tabulated. finally applications ﬁeld provided. recognition objects challenges ﬁeld artiﬁcial intelligence. many systems developed recognize classify images. recent years huge strides made making systems accurate. deep neural network class algorithms shown good results benchmark datasets prior using neural networks popular approach recognizing objects design algorithms would look predetermined features image. programmer required deep knowledge data would laboriously engineer feature detection algorithms. expert systems created still vulnerable small ambiguities image. neural networks effort decding engineering feature detector dispensed with. advantage neural network lies following theoretical aspects. first neural networks data driven self-adaptive algorithms; require prior knowledge data underlying properties. second approximate function arbitrary accuracy classiﬁcation task essentially task determining underlying function property important. thirdly neural networks estimate posterior probabilities provides basis establishing classiﬁcation rule performing statistical analysis vast research topics extensive literature makes impossible review cover work ﬁled. review aims provide summary recent improvements made deep neural network architecture record breaking performances object recognition. overall organization paper follows. introduction brief history research ﬁeld given section section describes innovations done parts neural network. section lists commonly used datasets benchmark image classiﬁcation algorithm. finally section tabulates state-of-the-art performance benchmark data sets. literature compiled deeplearning.net hosted university montreal. earliest experiments neural networks began neurophysiologist warren mcculloch mathematician walter pitts modeled simple neural network using electrical circuits neuron took inputs depending weighted would give binary output. advent fast computers became possible simulate neural networks bigger scale. organized group study pattern recognition information theory switching circuit theory headed nathanial rochester among projects group simulated behavior abstract neural networks computer. bernard widrow marcian hoff stanford developed models called adaline madaline. adaline similar todays perceptron. developed recognize binary patterns reading streaming bits phone line could predict next bit. madaline extension adaline similar today’s single layer neural netowrk. ﬁrst neural network applied real world problem using adaptive ﬁlter eliminates echoes phone lines. developed learning procedure could change weight values depending error prediction. alongside research artiﬁcial neural networks basic research layout neurons inside brain also conducted.the idea convoluted neural networks traced hubel wiesels work cats primary visual cortex. identiﬁed orientation-selective simple cells local receptive ﬁelds whose role similar feature extractors complex cells whose role similar pooling units. ﬁrst model simulated computer fukushimas neocognitron used layer-wise unsupervised competitive learning algorithm feature extractors separately-trained supervised linear classiﬁer output layer. even decades research artiﬁcial neural networks little networks could perform owing mainly requirement fast computations operation lack good technique train them. recent developments yann proposed algorithm train neural networks. innovation simplify architecture back-propagation algorithm train entire system. approach successful tasks handwriting recognition. operational bank check reading system built around convolutional neural networks developed early ﬁrst deployed commercially check-reading machines europe late reading checks motivated microsoft number handwriting recognition systems including arabic chinese characters. supervised convolutional neural networks also used object detection images including faces record accuracy real-time performance. google recently deployed convolutional neural networks detect faces license plate streetview images protect privacy. supervised convnets also used vision-based obstacle avoidance off-road mobile robots. participants recent darpa-sponsored lagr program vision-based navigation off-road robots used convnets long-range obstacle detection recently development occurred ﬁeld leading number improvements performances accuracy. ilsvrc- task assign labels image. winning algorithm produced result shown fig.. accuracy task described image caption years since then ilsvrc- winning team google accuracy fig. eight ilsvrc- test images labels considered probable winning algorithm. correct label written image probability assigned correct label also shown convolutional layer fully connected layer neural comes large number parameters. leads over-ﬁtting reduced generality. simple solution comes imitating visual cortex work living organisms. hubel’s research know visual cortex hierarchy exists neuron upper layer connected small region lower layer. first neural nets based models neo-cognitron lecun’s net- architecture lower layer divided number small regions called receptive fields receptive ﬁeld mapped neuron upper layer. connection called feature extractor. named connection extracts features receptive field. many feature extractors applied receptive fields generate feature vector ﬁeld. advantages using architecture sparse connectivity instead connecting whole lower layer upper layer section lower layer connected single neuron upper layer. drastically cuts number connections hence parameters. makes training easier. determine parameters feature extractors usually back propagation error used. methods developed. create function maps input vector feature vector features. many small patches images sampled supervised unsupervised techniques used model function. sparse auto-encoders auto-encoder hidden nodes trained using back-propagation minimize squared reconstruction error additional penalty term encourages units maintain average activation algorithm outputs weights rk×n biases feature mapping deﬁned logistic sigmoid function applied componentwise vector sparse restricted boltzmann machine restricted boltzmann machine undirected graphical model binary hidden variables. sparse rbms trained using contrastive divergence approximation type sparsity penalty autoencoders. training also produces weights biases feature mapping autoencoder. thus algorithms differ primarily training method. k-means clustering data points clustered around centroids distances data used generate k-dimensional vector. choices commonly used creating kdimensional vector. ﬁrst -of-k hardassignment coding scheme k-dimensional vector representing distances centroids. scheme gives centroid closest remaining zero. noted however terse second choice feature mapping non-linear mapping attempts softer encoding also keeping sparsity mean elements activation function outputs feature distance centroid average. practice means roughly half features thought simple form competition features. methods referred k-means k-means respectively. gaussian mixtures gaussian mixture models represent density input data mixture gaussian distributions widely used clustering. gmms trained using expectation-maximization algorithms single iteration k-means initialize mixture model. feature mapping maps mlpconv units conventional convolutional layer uses linear ﬁlters followed nonlinear activation function scan input. micro neural networks used convolve input. convolution unit called mlpconv unit contains multi layer perceptron. mlpconv unit contains layers rectiﬁed linear units activation function. pooling feature created input image pooling performed. spatial pooling outputs several nearby feature detectors combined local global features preserves task-related information removing irrelevant details. pooling used achieve invariance image transformations compact representations better robustness noise clutter pooling layer thought grid pooling units spaced pixels apart summarizing neighborhood size called pooling window centered location pooling unit. typically stride taken equal window size taken pooling units overlapping pooling windows feature map. overlapping architecture shown better difﬁcult overﬁt. functions commonly used pooling units stochastic pooling pooling average pooling strongly affected largest activation pooling window. however additional activations pooling window taken account passing information network stochastic pooling ensures non-maximal activations utilized. feature pooling region assigned probability crafted. example feature split equal sized quadrants pooling performed regions. contrast propose algorithm generate learnable pooling regions. allows richer possible pooling regions depend task data. activation functions every neuron neural network gives output determined activation function acting inputs. often non-linear activation functions used network able approximate non-linear functions. commonly used function sigmoid function tanh function. however running gradient descent train networks saturating functions require time converge compared non-saturating functions. shown rectiﬁed linear units max) train several times faster equivalent tanh units. adaptable activation function maxout also proposed. single maxout unit interpreted making piecewise linear approximation arbitrary convex function. maxout units learn relationship hidden units also activation function hidden unit. given input maxout hidden layer implements function input vector layer weight parameters layer size used calculate output vector layer learnt function. dropout presentation training case output hidden unit randomly omitted network probability therefore output fully connected layer modiﬁed binary mask element wise product operator. hypothesized prevents complex co-adaptations neuron helpful context several speciﬁc neurons. instead neuron learns detect feature generally helpful producing correct answer given combinatorially large variety internal contexts must operate. dropconnect technique instead masking outputs inputs neurons randomly switched off. makes generalization dropout technique. output training given binary mask where average mean squared error parameter modiﬁed error containing penalty term correct labels predicted labels total number instances regularization coefﬁcient error increases weights become high. larger error back propagated bigger weights forced become smaller again. dropout generalization dropconnect attempt regularize network novel equivalent training ensemble networks averaging predictions. consider following notation microsoft coco microsoft common objects context dataset. contains common object categories images containing instances. spatial location object given precise pixel level segmentation. additionally critical distinction dataset number labeled instances image. learning contextual information. tiny image data largest image data available. million images stored resolution image labeled non-abstract nouns english listed wordnet lexical database. noted many labels reliable dataset offers possiblity using wordnet cinjuction nearest-neighbor methods perform object classiﬁcation range semantic levels minimizing effects labeling noise. cifar- cifar- subsets derived tiny image dataset images labelled accurately. cifar- examples classes cifar- examples classes. image resolution imagenet imagenet image dataset organized according wordnet hierarchy. meaningful concept called synonym synset. imagenet average images illustrate synset. images concept quality-controlled human-annotated. imagenet expected label tens millions images. present slightly million labeled images. images come various sizes. generally resolution around compared images tiny image data set. also images object object annotated bounding box. street view house numbers svhn realworld image dataset minimal requirement data preprocessing formatting. seen similar ﬂavor mnist labeled data comes signiﬁcantly harder unsolved real world problem svhn obtained house numbers google street view images. resolution images mnist mnist database handwritten digits training examples test equal dimension equation holds true case dropout also difference mask constrained fact input weights chosen neurons either turned together. inference output networks averaged give refers number binary masks. computation unfeasible masks. instead massive computation dropout technique mean network created contains hidden units outgoing weights halved compensate fact twice many active. mean network essentially approximation equation above mathematically )v). although shows good performance approximation mathematically justiﬁed. dropconnect different approach used consider single unit activation function a;ui jmij. since sampled froma bernoulli’s distribution mean variance calculated ariance constructing gaussian using parameters values sampled passed activation function averaging presenting next layer. data augmentation increasing size dataset reduces overﬁtting improves generalization machine learning algorithm. dataset consists images simple distortion translations rotations skewing generated applying afﬁne displacement ﬁelds. works because intuitively identity object invariant afﬁne transformations. data augmentation techniques used. ﬁrst form data augmentation consists generating image translations horizontal reﬂections. done extracting random patches images training network extracted patches. increases size training factor test time network makes prediction extracting patches well horizontal reﬂections averaging predictions made networks softmax layer patches. second form data augmentation uses property identity object invariant change intensity color illumination. difﬁculties faced early experiments machine learning limited availability labeled data sets. many image datasets created growing rapidly meet demand larger data sets norb database intended experiments object recognition shape. contains images toys belonging generic categories four-legged animals human ﬁgures airplanes trucks cars. objects imaged cameras lighting conditions elevations azimuths training composed instances category test remaining instances making total number image pairs algorithm estimated human performance network network regularization neural networks using dropconnect maxout networks network network discriminative transfer learning tree-based priors improving deep neural networks probabilistic maxout units multi-task bayesian optimization unsupervised feature learning rgb-d based object recognition discriminative learning sumproduct networks regularization neural networks using dropconnect multi-column deep neural networks image classication maxout network regularization neural networks using dropconnect human performance multi-digit number recognition street view imagery using deep neural networks network network speech recognition current speech recognition systems hidden markov models deal temporal variability speech gaussian mixture models determine well state frame short window frames coefﬁcients represents acoustic input. deep neural networks many hidden layers trained using methods shown outperform gmms variety speech recognition benchmarks sometimes large margin. image compression neural networks property creating lower dimensional internal representation input. tapped create algorithms image compression. techniques fall three main categories direct development neural learning algorithms image compression neural network implementation traditional image compression algorithms indirect applications neural networks assist existing image compression techniques medical diagnosis vast amounts medical data store today form medical images doctors’ notes structured tests. convoluted neural networks used analyze data. example medical image analysis common design group speciﬁc features high-level task classiﬁcation segmentation. detailed annotation medical images often ambiguous challenging task. shown deep neural networks effectively used perform tasks. datasets need made reliable. also datasets grow larger annotating gets difﬁcult. crowd sourcing used create datasets like tinyimage dataset ms-coco imagenet still many ambiguities removed manually. better crowd sourcing strategies developed. training neural networks requires huge amount computational resource. efforts made make code efﬁcient compatible upcoming high performance computational platforms. investigation needs done image stored neural network. gain intuitive understanding features organized high level. ever neural network trained knowledge cannot added without retraining entirely understanding high level feature representation seem adding knowledge neural networks. kunihiko fukushima neocognitron self-organizing neural network model mechanism pattern recognition unaffected shift positionbiological cybernetics olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei imagenet large scale visual recognition challenge arxiv. p.y. simard steinkraus j.c. platt best practices convolutional neural networks applied visual document analysis proceedings seventh international conference document analysis recognition volume pages coates honglak analysis singlelayer networks unsupervised feature learning international conference artiﬁcial intelligence statistics volume jmlr. malinowski fritz learning smooth pooling regions visual recognition scalable learning perception planck institute informatics saarbrcken germany g.e. hinton srivastava krizhevsky sutskever r.r. salakhutdinov improving neural networks preventing coadaptation feature detectors arxiv preprint arxiv. torralba fergus w.t. freeman million tiny images large dataset non-parametric object scene recognition ieee transactions pattern analysis machine intelligence vol. yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning nips workshop deep learning unsupervised feature learning. nitish srivastava ruslan salakhutdinov discriminative transfer learning tree-based priors advances neural information processing systems pages goodfellow yaroslav bulatov julian ibarz sacha arnoud vinay shet multi-digit number recognition street view imagery using deep convolutional neural networks iclr hinton deng dong dahl mohamed jaitly andrew senior vanhoucke nguyen sainath kingsbury deep neural networks acoustic modeling speech recognition google dean corrado monga chen devin marcaurelio ranzato andrew senior tucker yang large scale distributed deep networks jeffrey nips jiang image compression neural networks survey qiwei feng peilin zhong maode eric i-chao chang deep learning feature representation multiple instance learning medical image analysis ieee international conference acoustic speech signal processing", "year": 2014}