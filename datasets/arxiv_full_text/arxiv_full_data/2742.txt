{"title": "Towards Scalable Spectral Clustering via Spectrum-Preserving  Sparsification", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "The eigendeomposition of nearest-neighbor (NN) graph Laplacian matrices is the main computational bottleneck in spectral clustering. In this work, we introduce a highly-scalable, spectrum-preserving graph sparsification algorithm that enables to build ultra-sparse NN (u-NN) graphs with guaranteed preservation of the original graph spectrums, such as the first few eigenvectors of the original graph Laplacian. Our approach can immediately lead to scalable spectral clustering of large data networks without sacrificing solution quality. The proposed method starts from constructing low-stretch spanning trees (LSSTs) from the original graphs, which is followed by iteratively recovering small portions of \"spectrally critical\" off-tree edges to the LSSTs by leveraging a spectral off-tree embedding scheme. To determine the suitable amount of off-tree edges to be recovered to the LSSTs, an eigenvalue stability checking scheme is proposed, which enables to robustly preserve the first few Laplacian eigenvectors within the sparsified graph. Additionally, an incremental graph densification scheme is proposed for identifying extra edges that have been missing in the original NN graphs but can still play important roles in spectral clustering tasks. Our experimental results for a variety of well-known data sets show that the proposed method can dramatically reduce the complexity of NN graphs, leading to significant speedups in spectral clustering.", "text": "data clustering graph partitioning playing increasingly important roles many compute-intensive applications related scientiﬁc computing data mining machine learning image processing etc. among existing data clustering graph partitioning methods spectral methods gained great attention recent years typically involve solving eigenvalue decomposition problems associated graph laplacians. example classical spectral clustering partitioning algorithms leverage ﬁrst nontrivial eigenvectors corresponding smallest nonzero eigenvalues graph laplacians dimensional embedding followed k-means clustering procedure usually leads high-quality data clustering results. although spectral methods many advantages easy implementation good solution quality rigorous theoretical foundations high computational cost eigenvalue decomposition procedure immediately hinder applications emerging data analytics tasks address computational bottleneck data clustering graph partitioning methods recent research efforts reduce complexity original data network various kinds approximations k-nearest neighbor graphs maintain nearest neighbors node whereas ǫ-neighborhood graphs keep neighbors within range distance sampling-based approach afﬁnity matrix approximation using nystr¨om method introduced error analysis introduced landmarkbased method representing original data points introduced proposed general framework fast approximate spectral clustering collapsing original data points small number centroids using kmeans random-projection trees; introduced method compressing original graph sparse bipartite graph generating small number supernodes; graph sparsiﬁcation method using similarity-based heuristic proposed scalable clustering however none existing approximation methods efﬁciently robustly preserve spectrums original graphs thus lead degraded misleading result. recent spectral graph sparsiﬁcation research enables compute nearly-linear-sized sparsiﬁers well preserve spectrum original graph immediately leads series theoretically nearly-linear numerical graph algorithms solving sparse matrices graph-based semisupervised learning well spectral graph partitioning max-ﬂow problems instances sparsiﬁed transportation networks allow developing scalable navigation algorithms large transportation systems; sparsiﬁed social networks spectral graph sparsiﬁcation. graph sparsiﬁcation aims graph proxy vertices original graph much fewer edges. general types graph sparsiﬁcation methods. sparsiﬁcation methods preserve cuts random sampling edges whereas spectral sparsiﬁcation methods preserve graph spectral properties eigenvalues eigenvectors laplacian thus powerful sparsiﬁcation methods. since preserving ﬁrst eigenvectors graph laplacians within graph sparsiﬁer spectral clustering tasks work focus spectral sparsiﬁcation graphs. laplacian quadratic form deﬁned able effectively understand predict information propagation phenomenons large social networks; sparsiﬁed data networks enable efﬁciently store partition analyze data networks; sparsiﬁed matrices leveraged accelerate solving large linear system equations. inspired recent progress development efﬁcient spectral graph sparsiﬁcation methods propose novel spectrum-preserving graph sparsiﬁcation method constructing ultra-sparse nearest-neighbor graphs immediately lead highly-scalable spectral clustering without loss accuracy. contributions work include contrast existing graph approximation approaches proposed method enables directly preserve spectral properties original graph within nearly-linear-sized u-nn graphs scalable spectral clustering. novel spectral edge embedding scheme well robust eigenvalue stability checking procedure introduced iteratively recovering small portions off-tree edges low-stretch spanning tree dramatically improve spectral similarity sparsiﬁed graph. incremental graph densiﬁcation procedure based efﬁcient spectral graph embedding scheme proposed adding extra edges missing original k-nn graph still critical high-quality spectral clustering tasks. spectral clustering algorithms. spectral clustering methods often outperform traditional clustering algorithms k-means algorithms consider data graph denote graph vertex edge sets respectively denotes weight function assigns positive weights edges. symmetric diagonally dominant laplacian matrix graph constructed follows spectral clustering methods typically include following three steps construct laplacian matrix according entire data set; embed data points k-dimensional space using ﬁrst nontrivial eigenvectors graph laplacian; perform k-means algorithm partition embedded data points different clusters. existing spectral clustering algorithms computationally expensive recent theoretical computer science research showed every undirected graph lsst total stretch bounded also shown many large generalized eigenvalues eigenvalues greater consequently good spectral sparsiﬁer obtained recovering off-tree edges spanning tree dramatically reducing largest generalized eigenvalues. spectral embedding off-tree edges. practically efﬁcient nearly-linear time algorithm constructing σsimilar spectral sparsiﬁers off-tree edges proposed identify spectrally critical off-tree edges adding lsst following generalized eigenvalue perturbation analysis adopted perturbation includes off-tree edges applied resulting perturbed generalized eigenvalues eigenvectors respectively. effective spectral sparsiﬁcation identify off-tree edges result greatest reduction following spectral embedding steps step start initial random vector orthogonal vector written recent nearly-linear time spectral graph sparsiﬁcation algorithm proposed dramatically reduce following steps also shown extract low-stretch spanning tree original graph; recover small portion spectrally critical off-tree edges lsst form spectral sparsiﬁer. shown σ-similar spectral sparsiﬁer edges obtained almost linear time based lsst overview work. overview proposed method shown given input data lsst ﬁrst extracted based original graph. next spectral embedding ranking off-tree edges performed leveraging generalized eigenvalue perturbation analysis framework small portions spectrally critical off-tree edges iteratively selected recovered lsst dramatically improve approximation spectral sparsiﬁer. determine suitable amount offtree edges needed high-quality spectral clustering tasks propose effective scheme stability checking ﬁrst eigenvalues eigenvectors assures good preservation original graph spectrums. additional edges missing original graph added sparsiﬁer performing efﬁcient spectral graph embedding procedure latest spectral sparsiﬁer. spanning-tree spectral sparsiﬁer. assume original graph weighted undirected connected graph whereas graph sparsiﬁer. start analyzing spectral similarity given graph spanning-tree sparsiﬁer indicates consequently spectrally critical off-tree edges identiﬁed largest stretch values therefore immediately impact largest eigenvalues fact regarded randomized version trace off-tree edge scaled factor denote descending eigenvalues corresponding unit-length mutually-orthogonal eigenvectors similarly denote eigenvalues eigenvectors ˜ωn. following spectral decompositions according spectrally critical off-tree edges identiﬁed impact largest eigenvalues well bottom eigenvalues since smallest values directly contribute largest components trace fact enables recover small portions spectrally critical off-tree edges lsst preserving spectral graph properties within sparsiﬁed graph. criteria selecting off-tree edges. propose iteratively recover small portions off-tree edges checking stabilities bottom eigenvalues sparsiﬁed laplacian. stop adding off-tree edges bottom eigenvalues become sufﬁciently stable output ﬁnal spectral sparsiﬁer spectral clustering purpose. δlpmax includes off-tree edges vector p-th element q-th element others reﬂects spectral similarity graphs greater qδlpmax values indicate larger thus lower spectral similarity. importantly allows embedding generalized eigenvalues laplacian quadratic form off-tree edge subsequently ranking off-tree edges according spectral criticality levels. recovering off-tree edges largest |vpq| values highly likely signiﬁcantly impact largest generalized eigenvalues. noted required number generalized power iterations rather small practice achieving good spectral embedding result. preservation bottom eigenvalues important assure recovering spectrally critical off-tree edges identiﬁed always effectively improve preservation eigenvalues eigenvectors within sparsiﬁed laplacians. following theoretical analysis proposed. spectrally unique off-tree edge deﬁned edge connects vertices impacts single large generalized eigenvalue though off-tree edge usually inﬂuence eigenvalue eigenvector according following truncated expansion laplacian quadratic form dominant spectrally unique off-tree edges ﬁxing largest eigenvalues proposed graph densiﬁcation procedure achieved leveraging efﬁcient graph embedding scheme approximately preserve distances nodes similar effective resistance metric used sampling edges. graph embedding scheme motivated one-dimensional graph embedding using fielder vector corresponds smallest nonzero eigenvalue graph laplacian. however expensive compute exact fielder vector practice since many inverse power iterations required. propose perform small number inverse power iterations using multiple random vectors also lead decent graph embedding results. embedding scheme requires solve sparsiﬁed laplacian random right-hand-side vectors solution vectors subsequently used embedding sparsiﬁed graph r-dimensional space. result extra spectrally-critical edges connect remote nodes sparsiﬁer effectively identiﬁed added sparsiﬁer stretch values large enough. extra edges missing original k-nn graph included latest sparsiﬁed graph signiﬁcantly improve spectral clustering accuracy. following show one-step inverse power iteration used graph embedding distance nodes similar effective-resistance distance laplacian edge given eigen-stability checking. propose novel method checking stability bottom eigenvalues sparsiﬁed laplacian. approach proceeds follows iteration recovering off-tree edges compute record several smallest eigenvalues latest sparsiﬁed laplacian example bottom eigenvalues critical spectral clustering tasks; determine whether off-tree edges recovered looking stability comparing eigenvalues computed previous iteration change eigenvalues signiﬁcant off-tree edges added current sparsiﬁer. speciﬁcally store bottom eigenvalues computed previous iteration vector calculate eigenvalue variation ratio greater eigenvalue variation ratio indicates less stable eigenvalues within latest sparsiﬁed graph laplacian thus justiﬁes another iteration allow adding spectrally-critical off-tree edges sparsiﬁer. inexact fast eigenvalue computation. software package arpack become standard solver solving practical large-scale eigenvalue problems shows arpack employs implicitly iterative restarted arnoldi process contains steps arnoldi length empirically sparse matrices. since cost iteration sparse matrix-vector product overall runtime cost arpack solver proportional o)×o memory cost number data points arnoldi length number nearest neighbors number desired eigenvalues. algorithms sparsiﬁed laplacian dramatically reduce time memory cost arpack solver dramatically reduced gain high efﬁciency propose quickly compute eigenvalues stability checking based inexact implicitly restarted arnoldi method shown relaxing tolerance total inner iteration counts signiﬁcantly reduced inner iteration cost reduced using subspace recycling iterative linear solver. incremental graph densiﬁcation. using spectral sparsiﬁer computed proposed spectral sparsiﬁcation method achieve similar spectral clustering quality original k-nn graph. improve clustering accuracy incremental graph densiﬁcation scheme introduced work identifying extra edges missing original k-nn graph construct initial lsst original graph embed off-tree edges spectral perturbation analysis rank off-tree edges spectral criticality levels iteratively small portions off-tree edges lsst bottom eigenvalues stable different objects normalized grayscale images. pendigits dataset consists images handwritten digits writers using sampled coordination information. usps dataset scanned hand-written digits envelops u.s. postal service. mnist dataset consists images handwritten digits. consists documents newswire stories categories statistics data sets shown table algorithms compare method following clustering algorithms standard spectral clustering algorithm nystr¨om method landmark-based spectral clustering algorithm parameters selection. number nearest neighbors data sets except data following gaussian kernel similarity function converting original distance matrix afﬁnity matrix observed distance using either graph embedding method mainly inﬂuenced smallest nonzero laplacian eigenvalue slight difference effective-resistance distance upper-bound distance obtained proposed embedding scheme random factor replaced using multiple random vectors steps inverse power iterations effectively reduce impact random factors thus achieve decent graph embedding results identifying extra spectrally-critical edges. noted observed extensive experiments amount extra edges added incremental graph densiﬁcation procedure usually less thus signiﬁcantly increase complexity sparsiﬁer algorithm complexity analysis. complete algorithm spectrum-preserving graph sparsiﬁcation shown algorithm complexity proposed method proposed also analyzed follows step constructing low-stretch spanning tree takes nearly-linear time according step also ﬁnished almost linear time ﬁxed power iteration number since factorization spanning-tree laplacian done within linear time cost associated steps eigen-stability checking procedure analyzed section cost associated steps incremental graph densiﬁcation procedure done efﬁciently since sparsiﬁed laplacian solved quickly either using preconditioned iterative direct methods. based discussion expect overall algorithm highly scalable even handling largescale data sets. perform extensive experiments demonstrate effectiveness proposed method. experiments performed using matlab running linux machine memory. reported results averaged runs. also adopt self-tuning method determine scaling parameter sparsiﬁcation threshold spart recovering off-tree edges deﬁned spart added −tree edges good indicator density sparsiﬁed graph. data sets spart experiments less graph densiﬁcation procedure. evaluation metric. measure quality clustering metrics clustering accuracy normalized mutual information clustering results generated clustering algorithms groundtruth labels provided data sets. higher value indicates better clustering quality. value range higher value indicates better matching algorithm-generated result ground-truth result. experimental results. table fig. shows impact adding off-tree edges stability bottom eigenvalues computed which veriﬁes theoretical foundation proposed method. adding extra off-tree edges immediately reduce variation ratio bottom eigenvalues indicating gradually improved eigenvalue stability. also observed adding off-tree edges spanning tree good spectral clustering results obtained. clustering quality results provided table table since impossible standard spectral clustering algorithm original data extremely large size results clustering accuracy obtained represent best results generated distributed computing systems. improve runtime efﬁciency spectral clustering sparsiﬁed mnist data faster clustering original data set; original data handled using original k-nn graph server memory minutes required clustering sparsiﬁed data set. show effectiveness spectral sparsiﬁcation reducing graph complexity also list numbers nonzero elements afﬁnity matrices table observed nearly-linear-sized laplacians much smaller original ones leading dramatically improved memory/storage efﬁciency spectral clustering tasks. expected proposed method enabler storing processing much bigger data sets energy-efﬁcient computing platforms fpgas even hand-held devices. also visualize original graph initial spanning tree spectral sparsiﬁer according afﬁnity matrices usps data fig. fig. fig. respectively. observed initial spanning tree seems poor approximation original graph adding off-tree edges already leads good approximation original k-nn graph. work introduce spectrum-preserving graph sparsiﬁcation algorithm enables build ultra-sparse graph sparsiﬁers well preserve ﬁrst eigenvectors original graph laplacian immediately enables highly scalable spectral clustering without loss accuracy. method starts constructing spectrally-critical low-stretch spanning tree followed novel spectral off-tree edge embedding scheme identifying recovering small portion off-tree edges critical preserving bottom eigenvalues eigenvectors original laplacian. last extra edges added sparsiﬁer incremental graph densiﬁcation procedure form almost-linear-sized spectral graph sparsiﬁers immediately lead highly scalable accurate spectral clustering. extensive experimental results variety well-known data sets demonstrate signiﬁcant speedups traditional spectral clustering method without sacriﬁcing clustering quality.", "year": 2017}