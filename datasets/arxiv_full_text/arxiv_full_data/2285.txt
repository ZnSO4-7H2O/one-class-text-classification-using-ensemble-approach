{"title": "PRNN: Recurrent Neural Network with Persistent Memory", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Although Recurrent Neural Network (RNN) has been a powerful tool for modeling sequential data, its performance is inadequate when processing sequences with multiple patterns. In this paper, we address this challenge by introducing an external memory and constructing a novel persistent memory augmented RNN (termed as PRNN). The PRNN captures the principle patterns in training sequences and stores them in an external memory. By leveraging the persistent memory, the proposed method can adaptively update states according to the similarities between encoded inputs and memory slots, leading to a stronger capacity in assimilating sequences with multiple patterns. Content-based addressing is suggested in memory accessing, and gradient descent is utilized for implicitly updating the memory. Our approach can be further extended by combining the prior knowledge of data. Experiments on several datasets demonstrate the effectiveness of the proposed method.", "text": "although recurrent neural network powerful tool modeling sequential data performance inadequate processing sequences multiple patterns. paper address challenge introducing external memory constructing novel persistent memory augmented prnn captures principle patterns training sequences stores external memory. leveraging persistent memory proposed method adaptively update states according similarities encoded inputs memory slots leading stronger capacity assimilating sequences multiple patterns. contentbased addressing suggested memory accessing gradient descent utilized implicitly updating memory. approach further extended combining prior knowledge data. experiments several datasets demonstrate effectiveness proposed method. introduction recent years witnessed great success deep learning models. owing increasing computation resources strong model capacity neural network models applied numerous applications. among neural network models recurrent neural networks shown notable potential sequence modeling tasks e.g. speech recognition machine translation therefore receive particular attention. increasing explorations several variants long short-term memory gated recurrent unit proposed successively. advantages come recurrent structures carry transition time steps eventually contribute satisfactory performance. merit validate assumption sequences follow pattern. conventional inappropriate processing sequences multiple patterns. mentioned difﬁcult optimize network using parameters time steps multiple pattern scenarios. adaptive networks required. recently extended mechanisms proposed augment model adaptability. ﬁrst attention mechanism popular technique machine translation. attention mechanism suggests aligning data prediction time step encoded contexts likely capture different patterns translating different words. another attractive mechanism memory mechanism basic idea memory mechanism setup external memory sequence. however memory based approaches build temporal memory memory reset sequence arrives. thus temporal memory probably cannot capture principle patterns training sequences. instance people read documents comprehension based context current document also knowledge accumulated previous reading life experiences. therefore besides temporal memory persistent memory immensely needed capture historical principle patterns. paper novel persistent memory augmented proposed. different external memories existing works p-memory holds principle patterns along training testing phases. memory accessed content-based addressing updated gradient descent. slot p-memory denotes principle pattern. relationship memory accessing mixture model also illustrated. introducing p-memory prnn presents stronger capacity processing sequences multiple patterns conventional rnns. moreover prnn model ﬂexibly extended prior knowledge data provided. contributions work summarized follows paper external persistent memory employed memorize principal patterns training sequences persistent memory augmented ﬂexibly process sequences multiple patterns. structure prnn shown figure hidden states prnn updated denotes memory accessing content-based addressing hidden states memory matrix serving inputs. note pmemory introduced function remains same means cell need change inner structure. manner persistent memory equip cell thus generally applicable. memory accessing memory matrix mm×n contains different slots slot represents certain pattern training sequences. given input sequence hidden state able represent subsequence following content-based addressing similarity slot p-memory denoting easily calculated retrieving column memory matrix. similarity used produce weight vector elements computed according softmax evaluate proposed prnn extensive experiments including time series prediction task language modeling task. experimental results demonstrate signiﬁcant advantages prnn. remainder paper organized follows. section reviews related work. section persistent memory introduced detail well extension lstm combination prior knowledge. experimental evaluations included section ﬁnal conclusion looking forward comes section related work research traced back past decades number variants model appear. skip connections introduced allow units access values distant past bidirectional suggests predict output based entire input sequence gated rnns belong another family variants popular long shortterm memory lstm networks introduce memory cells utilize gating mechanism control memory accessing. another classic gated variant gated recurrent unit simpliﬁes lstm single update gate controls forgetting factor updating factor simultaneously recently advanced mechanisms attention memory appear modify structure. attention mechanism particularly useful machine translation requires extra data alignment similarity encoded source sentence output word calculated. beyond machine translation attention mechanism gains notable popularity areas including video captioning cascade prediction comprehensive study attention mechanism found terms memory mechanism basic idea borrow external memory sequence popular addressing approaches i.e. locationbased addressing context-based addressing utilized memory based models. existing works reading weights writing weights usage weights memory accessing update memory explicit manner memory mechanism ﬁrst introduced few-shot learning tasks extended applications question answering detection programming mixture model perspective p-memory slots hidden states training sequences partitioned clusters given hidden state probability belongs i-th cluster matter fact mixture model perspective pmemory accessing seen variant algorithm. given hidden state current memory matrix e-step assigns responsibility cluster memory addressing. m-step optimizes parameters memory slot based rather explicit parameter updating mechanism appears conventional procedure gaussian mixture model p-memory updating implicit derived gradient descent. cosine similarity widely applied related works considering robustness computational efﬁciency. memory updating memory acts temporal memory explicitly updated training testing phases. however p-memory paper updated entirely implicit manner. updating mechanism prnn completely based gradient descent regardless read weights write weights well usage weights utilized previous works. memory matrix updated straightforwardly training phase leading simple training procedure memory augmented networks. speciﬁcally procedure updating p-memory based gradient loss function i-th slot p-memory given gradient consists terms. ﬁrst assigns similarity weight common derivative ∇pl. thus ﬁrst term becomes smooth less similar memory slot second term considers similarity measure affects gradient. taking measure illustration seen difference hidden state memory slot thus second part combination difference memory slot equations easily found gradient simply quadratic given computed memory accessing beforehand. therefore memory updating reasonably expected fast during training process. power consumption dataset contains measurements electric power consumption household period weeks. global-active-power aggregated hourly averaged time series prediction target globalactive-power every hour next day. entire dataset divided parts training part testing part sales forecast dataset collected largest e-commerce platforms world contains four features million items categories weeks. target predict total sales next week item. category information provided utilized prior knowledge. training includes million samples size testing thousand setup select lstm baseline model. lstm ﬁrst compared classical arima model p-memory added lstm construct approach. hyper-parameters tuned datasets follows power consumption hour input sequence contains power consumption last days. every step sequence considers values three adjacent hours centering target hour. goal predict global-active-power every hour next day. lstm single layer dimension hidden units equals terms approach p-memory augmented lstm constructed adding p-memory lstm. models trained epochs. sales forecast item input sequence four features recent days target total sales next week. lstm also single layer dimension hidden units similarly plstm model pmemory constructed. addition since category information prior knowledge construct prior knowledge based memory model plstm assign p-memory category. models trained epochs. normalization constant also memory accessing represented mixture model. lstm persistent memory general external memory mechanism persistent memory able equip almost models. section take lstm example illustrate pmemory works practice. since persistent memory added gates cells forget gate input gate revised persistent memory prior knowledge many practical situations prior domain knowledge either sample distribution data pattern known beforehand. matter fact prior knowledge provide much information help build efﬁcient persistent memories substantially beneﬁts training process. subsection select example text modeling illustration. category information often provided advance text modeling. leverage prior knowledge multiple persistent memories rnn. assume buckets exist training data independent p-memory allocated bucket. therefore memory accessing extended experiments performance prnn evaluated different tasks time series prediction task language modeling task. experiments implemented tensorﬂow different measures calculating similarity suggested section considering popularity cosine similarity select measure experiments consistency. claimed baseline model name persistent memory augmented model preﬁx name prior knowledge based memory model preﬁx tuning hyper-parameters independent validation randomly drawn training model trained remaining samples. hyperparameters determined model trained entire training set. experiments several times average results reported. table evident plstm model signiﬁcantly outperforms lstm arima datasets. therefore advantages p-memory fully demonstrated. particularly dataset utilizing prior knowledge pplstm could enhance prediction accuracy indicates strong adaptability approach well. penn treebank popular dataset language modeling. experiment select version provided dataset consists million words thousand words vocabulary. -newsgroup dataset originally benchmark text categorization thousand news documents evenly categorized groups. word-level prediction group information considered prior knowledge. preprocessed data found consists million words thousand words included vocabulary. simple setting. setting word embedded -dimensional representation. size hidden units baseline lstm model. dimension p-memory plstm dataset since group information used prior knowledge construct pplstm allocate p-memory group. settings parameter initialization optimization method learning rate remain time series prediction tasks. model trained epochs. complex setting. large network architecture provides strong baseline language modeling tasks open source implementation. contains many extensions including multiple layers stacking dropout gradient clipping learning rate decay setting select large network baseline model. terms approach p-memory added large construct persistent memory augmented large model. similarly prior knowledge based plarge established introducing particular p-memory group dataset. sophisticated model ensemble multiple models lower perplexities word-level prediction tasks achieved. simply focus assessing impact p-memory added existing architecture rather absolute stateof-the-art performance. results results measured perplexity popular metric evaluate language models results summarized table simple setting beneﬁt p-memory signiﬁcant datasets especially dataset. possible explanation content semantically richer model could easily distinguish various semantic patterns dataset. similarly assist prior knowledge efﬁciency p-memory improved. advantages approach remain complex setting. according results table large model already achieved high accuracy datasets approach able enhance model performance. thus general superiority approach proved simple setting complex setting. addition also noted another advantage approach appears model convergence. introducing p-memory plarge model able provide better convergence rate. taking dataset example training procedure word-level perplexity validation shown figure clearly plarge model requires less epochs converges. hence beyond superiority high accuracy advantage convergence performance also makes approach appealing. conclusion conventional limited adaptively process sequences multiple patterns. paper novel prnn approach proposed external p-memory introduced memorize principal patterns training sequences. content-based accessing prnn applies adaptive transition time step. p-memory updated gradient descent entire memory accessing interpreted mixture model. moreover proposed approach easily combine prior knowledge data. experiments time series task language modeling task demonstrate superiority effectiveness prnn method. proposed p-memory universal block look forward applying types neural networks feed-forward networks convolutional neural networks. another interesting topic studies memory updating mechanism. gradient descent based updating mechanism works well practice simpliﬁes end-to-end training. however still necessary explore appropriate updating mechanisms compare existing mechanisms comprehensively. references mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. amodei dario ananthanarayanan sundaram anubhai rishita deep speech end-to-end speech recognition english mandarin. proceedings international conference machine learning pages song jingkuan lianli zhao zhang dongxiang shen heng. hierarchical lstm proadjusted temporal attention video captioning. ceedings international joint conference artiﬁcial intelligence pages slava katz. estimation probabilities sparse data language model component speech recognizer. ieee transactions acoustics speech signal processing kyung-min min-oh choi seong-ho zhang byoung-tak. deepstory video story proceedings deep embedded memory networks. international joint conference artiﬁcial intelligence pages tomas mikolov martin karaﬁ´at lukas burget cernock`y sanjeev khudanpur. recurrent neural network based language model. interspeech volume page anton milan seyed hamid rezatoﬁghi anthony dick reid konrad schindler. online multitarget tracking using recurrent neural networks. proceedings aaai conference artiﬁcial intelligence pages choi min-je jeong sehun hakjoo choo jaegul. end-to-end prediction buffer overruns source code neural memory networks. proceedings international joint conference artiﬁcial intelligence pages adam santoro sergey bartunov matthew botvinick daan wierstra timothy lillicrap. meta-learning memory-augmented neural networks. international conference machine learning pages ashish vaswani noam shazeer niki parmar jakob uszkoreit llion jones aidan gomez attention need. proceedings advances neural information processing systems pages wang yongqing shen huawei shenghua jinhua cheng xueqi. cascade dynamics modeling attention-based recurrent neural network. proceedings international joint conference artiﬁcial intelligence pages", "year": 2018}