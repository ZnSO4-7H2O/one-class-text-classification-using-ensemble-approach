{"title": "Loss Functions for Multiset Prediction", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "We study the problem of multiset prediction. The goal of multiset prediction is to train a predictor that maps an input to a multiset consisting of multiple items. Unlike existing problems in supervised learning, such as classification, ranking and sequence generation, there is no known order among items in a target multiset, and each item in the multiset may appear more than once, making this problem extremely challenging. In this paper, we propose a novel multiset loss function by viewing this problem from the perspective of sequential decision making. The proposed multiset loss function is empirically evaluated on two families of datasets, one synthetic and the other real, with varying levels of difficulty, against various baseline loss functions including reinforcement learning, sequence, and aggregated distribution matching loss functions. The experiments reveal the effectiveness of the proposed loss function over the others.", "text": "study problem multiset prediction. goal multiset prediction train predictor maps input multiset consisting multiple items. unlike existing problems supervised learning classiﬁcation ranking sequence generation known order among items target multiset item multiset appear once making problem extremely challenging. paper propose novel multiset loss function viewing problem perspective sequential decision making. proposed multiset loss function empirically evaluated families datasets synthetic real varying levels difﬁculty various baseline loss functions including reinforcement learning sequence aggregated distribution matching loss functions. experiments reveal effectiveness proposed loss function others. relatively less studied problem machine learning particularly supervised learning problem multiset prediction. goal problem learn mapping arbitrary input multiset items. problem appears variety contexts. instance context high-energy physics important problems particle physics data analysis count many physics objects electrons muons photons taus jets collision event computer vision automatic alt-text available facebook representative example multiset prediction multiset prediction learner presented arbitrary input associated multiset items. assumed predeﬁned order among items annotations containing information relationship input items multiset. properties make problem multiset prediction unique wellstudied problems. different sequence prediction known order among items. ranking problem since item appear once. cannot transformed classiﬁcation number possible multisets grows exponentially respect maximum multiset size. paper view multiset prediction sequential decision making process. view problem reduces ﬁnding policy sequentially predicts item time outcome still evaluated based aggregate multiset predicted items. ﬁrst propose oracle policy assigns non-zero probabilities prediction sequences result exactly target ground-truth multiset given input. oracle optimal sense prediction never decreases precision recall regardless previous predictions. decision optimal state propose novel multiset loss minimizes divergence oracle policy parametrized policy every point decision trajectory parametrized policy. compare proposed multiset loss extensive baselines. include sequential loss arbitrary rank function sequential loss input-dependent rank function aggregated distribution matching loss one-step variant. also test policy gradient done welleck recently multiset prediction. evaluation conducted sets datasets varying difﬁculties properties. according experiments proposed multiset loss outperforms loss functions. variants multiset prediction problem extensively studied. however differ deﬁnition problem. here variant discuss differs deﬁnition multiset prediction. power multiset classiﬁcation perhaps naive approach multiset prediction transform class possible multisets. transformation size well deﬁned unless constraints place. maximum size target multiset number possible multisets class transformed train multi-class classiﬁer maps input elements however infeasible practice generally intractable. instance coco medium dataset used later experiments roughly thousand elements dataset contains roughly thousand training examples. full coco dataset order making infeasible learn classiﬁer using method. ranking ranking problem considered learning mapping pair input items score items class sorted according score sorted order determines rank item. taking top-k items sorted list turn problem ranking prediction. similarly multiset prediction input arbitrary target without prespeciﬁc order. however ranking differs multiset prediction unable handle multiple occurrences single item target set. indicator function. then simply minimize divergence distribution predictive distribution model. loss function works conditional distribution substantially differs marginal distribution since model would resort trivial solution predicting marginal distribution regardless input prediction include machine translation automatic speech recognition tagging problems part-of-speech tagging natural language processing. similarly multiset prediction input arbitrary item class appear target sequence. however different multiset prediction clear predetermined order items target sequence. detail sequence prediction approach later sec. paper propose novel loss function called multiset loss problem multiset prediction. loss function best motivated treating multiset prediction problem sequential decision making process model considered policy policy takes input input previously predicted classes ˆy<t time outputs distribution next class predicted. policy parametrized parameters. prediction made policy time free label multiset contains items remain predicted predictions policy. construct oracle policy oracle policy takes input sequence predicted labels ˆy<t input outputs distribution whose entire probability evenly distributed items free label multiset words deﬁnition interesting important property oracle optimal given preﬁx ˆy<t respect precision recall. intuitively clear noticing oracle policy allows correct item selected. call property optimality oracle. remark given arbitrary preﬁx ˆy<t trivial show sampling oracle policy would never result incorrect prediction. oracle policy assigns zero probability sequence predictions permutation target multiset. remark short oracle policy tells time step items class must selected. optimality allows consider step-wise loss parametrized policy oracle policy oracle policy provides optimal decision regardless quality preﬁx generated far. thus propose minimize divergence oracle policy parametrized policy step separately. divergence deﬁned call per-step loss function. note indeed possible another divergence place divergence. intractable minimize per-step loss every possible state since size state space grows exponentially respect size target multiset. thus propose minimize per-step loss state deﬁned pair input preﬁx ˆy<t visited parametrized policy generate entire trajectory executing parametrized policy either items target multiset predicted predeﬁned maximum number steps passed. then compute loss function time based ﬁnal loss function per-step loss functions. deﬁnition shown ross parametrized policy instead oracle policy allows upper bound learned policy’s error linear respect size target multiset. oracle policy used upper bound would grown quadratically respect size target multiset. conﬁrm empirically test following three alternative strategies executing parametrized policy experiments variable-sized target multiset deﬁned proposed loss function multiset prediction assuming size target multiset known. however major limitation relax constraint introducing additional termination policy. termination policy outputs stop distribution given predicted sequence items ˆy<t input because size target multiset known training simply train termination policy supervised using binary cross-entropy loss. evaluation time simply threshold predicted stop probability predeﬁned threshold aggregated distribution matching case distribution matching consider target multiset samples single underlying distribution class underlying distribution empirically estimated counting number occurrences item indicator function before. similarly construct aggregated distribution computed parametrized policy proposed multiset loss def. ﬁrst execute predict multiset converted aggregated distribution turned target multiset oracle aggregate distribution. major issue approach minimizing divergence aggregated distributions necessarily result optimal policy policy minimizes loss function assign non-zero probability incorrect sequence predictions unlike oracle policy. invariance aggregated distribution order predictions. later analyzing loss function empirically notice learned policy often different behaviour oracle policy instance reﬂected increasing entropy action distribution time. one-step variant train one-step predictor aggregate distribution matching criterion instead learning policy predictor outputs point |c|dimensional simplex size target multiset. then unique item number occurrences predicted multiset major weakness one-step variant compared approaches based sequential decision making lack modelling dependencies among items predicted multiset. test approach experiments later observe lack output dependency modelling results substantially worse prediction accuracy. loss functions deﬁned relied availability existing order items target multiset. however turning problem multiset prediction sequential decision making minimizing loss function equivalent capturing order items target multiset implicitly. here instead describe approach based explicitly deﬁning order advance. serve baseline later experiments. ﬁrst deﬁne rank function maps unique items class unique integer. function assigns rank item used order items target multiset results sequence target sequence created using rank function deﬁne sequence loss function minimizing loss function equivalent maximizing conditional log-probability sequence given sequence loss function clear disadvantages. first take account actual behaviour policy makes learned policy potentially vulnerable cascading error test time. second importantly loss function requires pre-speciﬁed rank function multiset prediction come rank function deﬁnition must design arbitrary rank function ﬁnal performance varies signiﬁcantly based choice. demonstrate variation section input-dependent rank function input well-known structure object within input item target multiset annotated possible devise rank function input. representative example image input bounding annotations. here present input-dependent rank functions case. first spatial rank function rspatial assigns integer rank item given target multiset objects corresponding items second area rank function rarea decides rank label target multiset according size corresponding object inside input image approach based reinforcement learning proposed multiset prediction. instead assuming existence oracle policy approach solely relies reward function designed speciﬁcally multiset prediction. reward function deﬁned reinforce stochastically minimize loss function respect loss function optimal return i.e. step-wise rewards maximized precision recall maximal words oracle policy deﬁned def. maximizes expected return. however approach reinforcement learning known difﬁcult high variance especially true here size state space grows exponentially respect size target multiset action space step large number unique items class set. section extensively evaluate proposed multiset loss function various baseline loss functions presented throughout paper. speciﬁcally focus applicability performance image-based multiset prediction. mnist multi mnist multi class synthetic datasets. dataset consists multiple images contains varying number digits original mnist vary size digit also clutters. experiments consider following variants mnist multi coco real-world dataset microsoft coco includes natural images multiple objects. compared mnist multi image coco objects varying sizes shapes large variation number object instances image spans problem made even challenging many overlapping occluded objects. variants include images whose objects large common classes. object deﬁned large object’s area percentile across train coco. reducing dataset large objects image remove images containing objects rare classes. class considered rare frequency less class set. stages ensure images proper number large objects kept. ﬁne-grained annotation except creating input-dependent rank functions sec. variant hold randomly sampled training examples validation set. form separate test sets applying ﬁlters coco validation set. test sizes coco easy coco medium. mnist multi three convolutional layers channel sizes followed convolutional long short-term memory layer step feature convolutional lstm layer average-pooled spatially softmax classiﬁer. case one-step variant aggregate distribution matching lstm layer skipped. coco resnet- pretrained imagenet feature extractor. ﬁnal feature resnet- convolutional lstm layer described mnist multi above. ﬁnetune resnet- based feature extractor. training evaluation loss model trained epochs epoch exact match computed validation set. model state highest validation exact match used evaluation test set. evaluating trained policy greedy decoding auxiliary stop prediction determining size predicted multiset. predicted multiset compared ground-truth target multiset report accuracy based exact match score first investigate sequence loss function lseq sec. varying rank function. test three alternatives random rank function input-dependent rank functions rspatial rarea. compare rank functions mnist multi coco easy validation sets. present results table clear results performance sequence prediction loss function dependent choice rank function. case mnist multi area-based rank function worse choices. however true coco easy spatial rank function worst among three. cases observed random rank function performed best random rank function remaining experiments. experiments ﬁrmly suggests need order-invariant multiset loss function multiset loss function proposed paper. experiments compare three execution strategies proposed multiset loss function illustrated sec. greedy decoding stochastic sampling oracle sampling. test mnist multi coco easy. shown table greedy decoding stochastic sampling consider states likely visited parametrized policy outperform oracle sampling. consistent theory ross although ﬁrst strategies perform comparably other across datasets evaluation metrics greedy decoding tends outperform stochastic sampling. conjecture better matching training compare proposed multiset loss function baseline loss functions reinforcement learning aggregate distribution matching–l one-step variant l-step sequence prediction lseq. mnist multi present results mnist multi variants table three variants according metrics proposed multiset loss function outperforms others. reinforcement learning based approach closely follows behind. performance however drops number items target multiset increases. understandable variance policy gradient grows length episode grows. similar behaviour observed sequence prediction well aggregate distribution matching. able train decent models one-step variant aggregate distribution matching. true especially terms exact match attribute one-step variant capable modelling dependencies among predicted items. coco similarly results variants mnist multi proposed multiset loss function matches outperforms others variants coco presented table coco easy objects predict example aggregated distribution matching sequence loss functions competitive proposed multiset loss. loss functions signiﬁcantly underperform three loss functions mnist multi. performance proposed loss others however grows substantially challenging coco medium objects example. proposed multiset loss outperforms aggregated distribution matching divergence percentage points exact match analogous experiments mnist multi variants performance increased moving four digits. property oracle policy deﬁned sec. entropy predictive distribution strictly decreases time i.e. natural consequence fact pre-speciﬁed rank function oracle policy cannot prefer item others free label multiset. hence examine policy learned based shown fig. policy trained mnist multi using proposed multiset loss closely follows oracle policy. entropy decreases predictions made. decreases interpreted concentrating probability mass progressively smaller free labels sets. variance quite small indicating strategy uniformly applied input. policy trained reinforcement learning retains relatively entropy across steps decreasing trend second half. carefully suspect entropy earlier steps greedy nature policy gradient. policy receives high reward easily choosing many possible choices earlier step later step. effectively discourages policy exploring possible trajectories training. hand policy found aggregated distribution matching opposite behaviour. entropy general grows predictions made. sub-optimal consider ﬁnal step. assuming ﬁrst nine predictions correct correct class left ﬁnal prediction high entropy however indicates model placing signiﬁcant amount probability incorrect sequences. believe policy found minimizing aggregated distribution matching loss function cannot properly distinguish policies increasing decreasing entropies. increasing entropy also indicates policy learned rank function implicitly fully relying given unknown free label multiset inferred input policy uses implicitly learned rank function choose item set. conjecture reliance inferred rank function deﬁnition sub-optimal resulted lower performance aggregate distribution matching. extensively investigated problem multiset prediction paper. rigorously deﬁned problem proposed approach perspective sequential decision making. oracle policy deﬁned shown optimal loss function called multiset loss introduced means train parametrized policy multiset prediction. experiments families datasets mnist multi variants coco variants revealed effectiveness proposed loss function loss functions including reinforcement learning sequence aggregated distribution matching loss functions. success proposed multiset loss brings opportunities applying machine learning various domains including high-energy physics. samy bengio oriol vinyals navdeep jaitly noam shazeer. scheduled sampling sequence prediction recurrent neural networks. advances neural information processing systems daum´e daniel marcu. learning search optimization approximate large margin methods structured prediction. proceedings international conference machine learning deng dong richard socher li-jia fei-fei. imagenet large-scale hierarchical image database. computer vision pattern recognition cvpr ieee conference ieee ehrenfeld buckingham cranshaw cuhadar donszelmann doherty gallas hrivnac malon nowak slater viegas vinek zhang atlas collaboration. using tags speed atlas analysis process. journal physics conference series http//stacks.iop.org/-//i=/a=. kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition st´ephane ross geoffrey gordon drew bagnell. reduction imitation learning structured prediction no-regret online learning. international conference artiﬁcial intelligence statistics xingjian zhourong chen wang dit-yan yeung wai-kin wong wang-chun woo. convolutional lstm network machine learning approach precipitation nowcasting. advances neural information processing systems model input ﬁrst processed tower convolutional layers resulting feature volume size feature maps i.e. rw×h×d. time step resize previous prediction’s embedding tensor concatenate resulting rw×h×. feature volume stack convolutional lstm layers. output ﬁnal convolutional lstm layer rw×h×q cij· feature vector spatially average-pooled i.e. turned conditional distribution next item afﬁne transformation followed softmax function. one-step variant aggregated distribution matching used skip convolutional lstm layers i.e. preprocessing mnist multi preprocess input all. case coco input images different sizes. image ﬁrst resized larger dimension pixels along dimension zero-padded pixels centered resulting image. training model trained end-to-end except resnet- remains ﬁxed pretrained imagenet. experiments train neural network using adam ﬁxed learning rate learning rate selected based validation performance preliminary experiments parameters default values. mnist multi batch size coco", "year": 2017}