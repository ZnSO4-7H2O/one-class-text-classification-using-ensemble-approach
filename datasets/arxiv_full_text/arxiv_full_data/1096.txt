{"title": "Neural Expectation Maximization", "tag": ["cs.LG", "cs.NE", "stat.ML", "I.2.6"], "abstract": "Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network. Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities. We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects. We demonstrate that the learned representations are useful for next-step prediction.", "text": "many real world tasks reasoning physical interaction require identiﬁcation manipulation conceptual entities. ﬁrst step towards solving tasks automated discovery distributed symbol-like representations. paper explicitly formalize problem inference spatial mixture model component parametrized neural network. based expectation maximization framework derive differentiable clustering method simultaneously learns group represent individual entities. evaluate method perceptual grouping task able accurately recover constituent objects. demonstrate learned representations useful next-step prediction. learning useful representations important aspect unsupervised learning main open problems machine learning. argued representations distributed disentangled latter recently received increasing amount attention producing representations disentangle features like rotation lighting methods mostly focused single object case whereas real world tasks reasoning physical interaction often necessary identify manipulate multiple entities relationships. current systems difﬁcult since superimposing multiple distributed disentangled representations lead ambiguities. known binding problem extensively discussed neuroscience solution problem involves learning separate representation object. order allow representations processed identically must described terms features. would avoid binding problem facilitate wide range tasks require knowledge individual objects. solution requires process known perceptual grouping dynamically splitting input constituent conceptual entities. work tackle problem learning group efﬁciently represent individual entities unsupervised manner based solely statistical structure data. work follows similar approach recently proposed tagger aims develop understanding well build theoretical framework problem symbol-like representation learning. formalize problem inference spatial mixture model component parametrized neural network. based expectation maximization framework derive differentiable clustering method call neural expectation maximization trained unsupervised manner perform perceptual grouping order learn efﬁcient representation group naturally extends sequential data. goal training system produces separate representations individual conceptual entities contained given input depends notion entity use. since interested case unsupervised learning notion rely statistical properties data. therefore adopt intuitive notion conceptual entity common cause multiple observations common cause induces dependency-structure among affected pixels pixels correspond different entities remain independent. intuitively means knowledge pixels object helps predicting remainder whereas improve predictions pixels objects. especially obvious sequential data pixels belonging certain object share common fate makes setting particularly appealing. interested representing entity vector captures structure affected pixels carries information remainder image. modularity powerful invariant since allows representation reused different contexts enables generalization novel combinations known objects. further possible objects represented format makes easier work representations. finally separate object allows distributed disentangled without suffering binding problem. treat image composition objects pixel determined exactly object. objects present well corresponding assignment pixels varies input input. assuming access family distributions corresponds object level representation described above model image mixture model. expectation maximization used simultaneously compute maximum likelihood estimate individual θk-s grouping interested central problem consider work therefore learn completely unsupervised fashion. accomplish parametrizing family distributions differentiable function show case corresponding procedure becomes fully differentiable allows backpropagate appropriate outer loss weights neural network. remainder section formalize derive method call neural expectation maximization parametrized spatial mixture model model image spatial mixture components parametrized vectors differentiable non-linear function used transform representations parameters separate pixel-wise distributions. distributions typically bernoulli gaussian case would single probability mean variance respectively. parametrization assumes given representation pixels independent identically distributed binary latent variables encodes unknown true pixel assignments graphical representation model seen figure mixing coefﬁcients full likelihood given given expectation maximization directly optimizing respect difﬁcult marginalization many distributions optimizing much easier. expectation maximization takes advantage instead optimizes lower bound given expected likelihood iterative optimization bound alternates steps e-step compute estimate posterior probability distribution latent variables given θold previous iteration yielding soft-assignment pixels components m-step conﬁguration would maximize expected loglikelihood using posteriors computed e-step. non-linearity exists analytical solution maxθ however since differentiable improve taking gradient ascent step model information statistical regularities required clustering pixels objects encoded neural network weights considered ﬁxed shown compute alongside appropriate clustering. observe unrolling iterations presented generalized obtain end-to-end differentiable clustering procedure based statistical model implemented therefore gradient descent statistical model capture regularities corresponding objects given dataset. implemented back-propagating appropriate loss time weights refer trainable procedure neural expectation maximization overview seen figure upon inspection structure n-em resembles copies recurrent neural network hidden states that timestep receive input. copy generates used e-step re-estimate soft-assignments order accurately mimic m-step must impose several restrictions weights structure encoder must correspond jacobian ∂ψk/∂θk recurrent update must linearly combine output encoder previous timestep. instead introduce algorithm named rnn-em substituting part computational graph n-em actual although rnn-em longer guarantee convergence data likelihood recurrent weights increase ﬂexibility clustering procedure. moreover using fully parametrized recurrent weight matrix rnn-em naturally extends sequential data. figure presents computational graph single rnn-em timestep. n-em differentiable clustering procedure whose outcome relies statistical model interested particular unsupervised clustering corresponds grouping entities based statistical regularities data. train system therefore require loss function teaches representations parameters correspond pixelwise distributions objects. accomplish two-term loss function guides networks model structure single object independently information image intra-cluster loss corresponds expected data log-likelihood optimized n-em. analogous standard reconstruction loss used training autoencoders weighted cluster assignment. similar autoencoders objective prone trivial solutions case overcapacity prevent network modelling statistical regularities interested standard techniques used overcome problem making bottleneck using noisy version compute inputs network. furthermore rnn-em used sequential data next-step prediction loss. weighing loss pixelwise crucial since allows network specialize predictions individual object. however also introduces problem loss out-of-cluster pixels vanishes. leaves network free predict anything yield specialized representations. therefore second term penalizes divergence out-of-cluster predictions pixelwise prior data. intuitively tells representation contain information regarding non-assigned pixels disadvantage interaction yield conﬂicting gradients. loss given pixel reduced better predicting decreasing realized worse predicting practical solution problem obtained stopping gradients i.e. setting ∂l/∂γ backpropagation. method closely related approach tagger similarly learns perceptual grouping unsupervised fashion using copies neural network work together reconstructing different parts input. unlike case n-em copies additionally learn output grouping gives tagger direct control segmentation supports complex texture segmentation tasks. work maintains close connection relies posterior inference e-step grouping mechanism. facilitates theoretical analysis simpliﬁes task resulting networks markedly smaller tagger. furthermore tagger include recurrent connections level hidden states precluding next step prediction sequential tasks. binding problem ﬁrst considered context neuroscience sparked early work oscillatory neural networks synchronization grouping mechanism later complex valued activations used replace explicit simulation oscillation virtue general computers principle learn suitable mechanism. practice however seems hard learn adding suitable mechanism like competition fast weights perceptual grouping n-em seems necessary. figure groupings rnn-em input images methods recover individual shapes accurately separated even confronted shape rnn-em able handle occlusion sometimes fails exact assignments permutation invariant depend initialization; compare unsupervised segmentation studied several different contexts random vectors texture segmentation images early work unsupervised video segmentation used generalized expectation maximization infer split frames moving sprites. recently optical used train convolutional networks ﬁgure/ground segmentation related line work term multi-causal modelling formalized perceptual grouping inference generative compositional model images. masked rbms example extend restricted boltzmann machines latent mask inferred block-gibbs sampling. gradient backpropagation inference updates previously addressed context sparse coding iterative shrinkage/tresholding algorithms ista; unrolled graph ﬁxed number ista iterations replaced recurrent neural network parametrizes gradient computations trained predict sparse codes directly derive rnn-em n-em similar fashion likewise obtain trainable procedure structure iterative pursuit built architecture leaving tunable degrees freedom improve modeling capabilities alternative empower network untying weights across iterations considered ﬂexibility reasons. evaluate approach perceptual grouping task generated static images video. composing images simple shapes control statistical structure data well access ground-truth clustering. allows verify proposed method indeed recovers intended grouping learns representations corresponding objects. particular interested studying role next-step prediction unsupervised objective perceptual grouping effect hyperparameter usefulness learned representations. experiments train networks using adam default parameters batch size train validation test inputs. consistent earlier work evaluate quality learned groupings respect ground truth ignoring background overlap regions. comparison done using adjusted mutual information score provides measure clustering similarity early stopping validation loss improved epochs. detailed overview experimental setup found appendix reported results averages computed runs. validate approach yields intended behavior consider simple perceptual grouping task involves grouping three randomly chosen regular shapes located random positions binary images simple setup serves test-bed comparing n-em rnn-em moving complex scenarios. implement means single layer fully connected neural network sigmoid output pixel corresponds mean bernoulli distribution. representation figure sequence shapes ﬂying along random trajectories next-step prediction copy network soft-assignment pixels copies observe network learns separate individual shapes means efﬁciently solve next-step prediction. even many shapes overlapping seen time-steps network still able disentangle individual shapes clutter. real-valued -dimensional vector squashed range sigmoid function network. similarly rnn-em recurrent neural network sigmoidal hidden units equivalent output layer. networks trained unrolled steps. shown figure observe approaches able recover individual shapes long separated even confronted identical shapes. n-em performs worse image contains occlusion rnn-em general stable produces considerably better groupings. observation line ﬁndings sparse coding similarly conclude tunable degrees freedom rnn-em help speed-up optimization process resulting powerful approach requires fewer iterations. beneﬁt reﬂected large score difference compared n-em. comparison tagger achieves score using twenty times parameters flying shapes consider sequential extension static shapes dataset shapes ﬂoating along random trajectories bounce walls. example sequence shapes seen bottom figure convolutional encoder decoder inspired discriminator generator networks infogan recurrent neural network sigmoidal units timestep network receives input current frame corrupted additional bitﬂip noise next-step prediction objective implemented replacing evaluated time-step. table summarizes results ﬂying shapes example sequence shapes using seen figure shapes observe produced groupings close perfect even cluttered case shapes network able separate individual objects almost cases results demonstrate adequacy next step prediction task perceptual grouping. however converse also holds corresponding representations useful prediction task. figure compare next-step prediction error rnn-em rnn-em task. evaluate rnn-em next-step prediction computed loss using opposed γikp avoid including information next timestep. reported loss rnn-em therefore upperbound true loss. ﬁgure observe rnn-em produces signiﬁcantly lower errors especially number objects increases. figure binomial cross entropy error obtained rnn-em recurrent autoencoder denoising next-step prediction task. rnn-em produces signiﬁcantly lower across different numbers objects. figure average score measured rnn-em across ﬂying mnist test-set corresponding quartiles computed time-steps. learned grouping dynamics generalize longer sequences even further improve score. finally table also provide insight impact choosing hyper-parameter unknown many real-world scenarios. surprisingly observe training large fact favourable network learns leave excess groups empty. training components network still learns individual shapes observe slight drop score correctly setting number components test time. conclude rnn-em robust towards different choices speciﬁcally choosing high detrimental. order incorporate greater variability among objects consider sequential extension mnist. sequence consists gray-scale images containing down-sampled mnist digits start random positions ﬂoat along randomly sampled trajectories within image timesteps. example sequence seen bottom figure deploy slightly deeper version architecture used ﬂying shapes. details found appendix since images gray-scale gaussian distribution pixel ﬁxed computed copy network. training procedure identical ﬂying shapes except replace bitﬂip noise masked uniform noise ﬁrst sample binary mask multi-variate bernoulli distribution mask interpolate original image samples uniform distribution minimum maximum values data train ﬂying mnist digits obtain score test measured across runs. early experiments observed that given large variability among unique digits boost model performance training stages using digits. exploit generalization capabilities rnn-em quickly transfer knowledge less varying figure sequence mnist digits ﬂying across random trajectories image next-step prediction copy network soft-assignment pixels copies although network trained sequences digits accurately able separate three digits. mnist digits unseen variations. used hyper-parameter conﬁguration obtain score test measured across runs. study generalization capabilities robustness trained rnn-em networks means three experiments. ﬁrst experiment evaluate ﬂying mnist three digits likewise even without training able maintain high score test-set. test example seen figure second experiment interested whether grouping mechanism learned transferred static images. using rnn-em steps able transfer large part learned grouping dynamics obtain score static digits. ﬁnal experiment evaluate directly trained network dataset larger number timesteps. figure displays average score across test well range upper lower quartile timestep. results experiments conﬁrm earlier observations ﬂying shapes learned grouping dynamics robust generalize across wide range variations. moreover score improves test time increasing sequence length. experimental results indicate proposed neural expectation maximization framework indeed learn group pixels according constituent objects. network learns useful localized representation individual entities encodes information relevant entity represented separately space avoids binding problem makes representations usable efﬁcient symbols arbitrary entities dataset. believe useful reasoning particular potentially wide range tasks depend interaction multiple entities. empirically learned representations already beneﬁcial next-step prediction multiple objects task overlapping objects problematic standard approaches handled efﬁciently learning separate representation object. typical clustering methods n-em preferred assignment objects groups grouping numbering arbitrary depends initialization. property renders results permutation invariant naturally allows instance segmentation opposed semantic segmentation groups correspond pre-deﬁned categories. rnn-em learns segment unsupervised fashion makes applicable settings little labeled data. downside lack supervision means resulting segmentation always match intended outcome. problem inherent task since real world images notion object ill-deﬁned task dependent. envision future work alleviate extending unsupervised segmentation hierarchical groupings dynamically conditioning task hand using top-down feedback attention. argued importance separately representing conceptual entities contained input suggested clustering based statistical regularities appropriate unsupervised approach separating them. formalized notion derived novel framework combines neural networks generalized trainable clustering algorithm. shown method trained fully unsupervised fashion segment inputs entities represent individually. using synthetic images video empirically veriﬁed method recover objects underlying data represent useful way. believe work help develop theoretical foundation understanding important problem unsupervised learning well providing ﬁrst step towards building practical solutions make symbol-like representations. authors wish thank paulo rauber anonymous reviewers constructive feedback. research supported swiss national science foundation grant project input grateful nvidia corporation donating dgx- part pioneers research award donating minsky machine. amir beck marc teboulle. fast iterative shrinkage-thresholding algorithm application wavelet-based image deblurring. acoustics speech signal processing icassp ieee international conference pages ieee chen duan rein houthooft john schulman ilya sutskever pieter abbeel. infogan interpretable representation learning information maximizing generative adversarial nets. arxiv. june ingrid daubechies michel defrise christine mol. iterative thresholding algorithm linear inverse problems sparsity constraint. communications pure applied mathematics klaus greff antti rasmus mathias berglund tele hotloo jürgen schmidhuber harri valpola. tagger deep unsupervised perceptual grouping. arxiv. june klaus greff rupesh kumar srivastava jürgen schmidhuber. binding reconstruction karol gregor yann lecun. learning fast approximations sparse coding. proceedings international conference machine learning pages jose guerrero-colón eero simoncelli javier portilla. image denoising using mixtures gaussian scale mixtures. image processing icip ieee international conference pages ieee irina higgins loic matthey arka christopher burgess xavier glorot matthew botvinick shakir mohamed alexander lerchner. beta-vae learning basic visual concepts constrained variational framework. proceedings international conference learning representations nebojsa jojic brendan frey. learning ﬂexible sprites video layers. computer vision pattern recognition cvpr proceedings ieee computer society conference volume pages i–i. ieee milner. model visual shape recognition. psychological review augustus odena vincent dumoulin chris olah. deconvolution checkerboard artifacts. cecchi. objective function utilizing complex sparsity efﬁcient segmentation multi-layer oscillatory networks. international journal intelligent computing cybernetics christopher rozell johnson richard baraniuk bruno olshausen. sparse coding thresholding local competition neural circuits. neural computation sudheendra vijayanarasimhan susanna ricco cordelia schmid rahul sukthankar katerina fragkiadaki. sfm-net learning structure motion video. arxiv. april ronald williams. complexity exact gradient computation algorithms recurrent neural networks. technical report technical report technical report nu-ccs-- boston northeastern university college computer science following subsections provide detailed information experimental setup empirical evaluation. experiments train networks using adam default parameters batch size train validation test inputs. quality learned groupings evaluated computing adjusted mutual information respect ground truth ignoring background overlap regions early stopping validation loss improved epochs. experiments static shapes input consists binary image containing three regular shapes located random positions n-em implement means single layer fully connected neural network sigmoid activation function. receives real-valued -dimensional vector input outputs pixel value parameterizes bernoulli distribution. squash sigmoid passing network train additional weight implement learning rate used combine gradient ascent updates current parameter estimate. similarly rnn-em recurrent neural network sigmoidal hidden units fully-connected output-layer sigmoid activation function parametrizes bernoulli distribution pixel fashion. train networks steps bitﬂip noise probability pixels. prior pixel data bernoulli distribution outer-loss injected ﬁnal em-step. experiments flying shapes input consists sequence binary images containing ﬁxed number shapes start random positions ﬂoat along randomly sampled trajectories within image steps. convolutional encoder-decoder architecture inspired recent gans recurrent neural network bottleneck fully connected. relu. layer norm fully connected. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. sigmoid instead using transposed convolutions ﬁrst reshape image using default nearest-neighbour interpolation followed normal convolution order avoid frequency artifacts note layer norm recurrent connection. timestep feed input network input added bitﬂip noise rnn-em trained next-step prediction objective implemented replacing evaluate time-step. single rnn-em step used timestep. prior pixel data bernoulli distribution prevent conﬂicting gradient updates back-propagating gradients experiments flying mnist input consists sequence gray-scale images containing ﬁxed number downsampled mnist digits start random positions across randomly sampled trajectories within image timesteps. slightly deeper version architecture used ﬂying shapes conv. elu. stride layer norm conv. elu. stride layer norm conv. elu. stride layer norm fully connected. elu. layer norm recurrent. sigmoid. layer norm output fully connected. relu. layer norm fully connected. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. relu. layer norm reshape nearest-neighbour conv. linear training procedure largely identical described ﬂying shapes except replace bitﬂip noise masked uniform noise ﬁrst sample binary mask multi-variate bernoulli distribution mask interpolate original image samples uniform distribution minimum maximum values data. learning rate scale second-loss term factor beneﬁcial normalize masked differences prediction image passing network.", "year": 2017}