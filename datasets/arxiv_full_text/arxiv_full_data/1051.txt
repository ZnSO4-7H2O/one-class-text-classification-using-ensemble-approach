{"title": "Full-Capacity Unitary Recurrent Neural Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Recurrent neural networks are powerful models for processing sequential data, but they are generally plagued by vanishing and exploding gradient problems. Unitary recurrent neural networks (uRNNs), which use unitary recurrence matrices, have recently been proposed as a means to avoid these issues. However, in previous experiments, the recurrence matrices were restricted to be a product of parameterized unitary matrices, and an open question remains: when does such a parameterization fail to represent all unitary matrices, and how does this restricted representational capacity limit what can be learned? To address this question, we propose full-capacity uRNNs that optimize their recurrence matrix over all unitary matrices, leading to significantly improved performance over uRNNs that use a restricted-capacity recurrence matrix. Our contribution consists of two main components. First, we provide a theoretical argument to determine if a unitary parameterization has restricted capacity. Using this argument, we show that a recently proposed unitary parameterization has restricted capacity for hidden state dimension greater than 7. Second, we show how a complete, full-capacity unitary recurrence matrix can be optimized over the differentiable manifold of unitary matrices. The resulting multiplicative gradient step is very simple and does not require gradient clipping or learning rate adaptation. We confirm the utility of our claims by empirically evaluating our new full-capacity uRNNs on both synthetic and natural data, achieving superior performance compared to both LSTMs and the original restricted-capacity uRNNs.", "text": "recurrent neural networks powerful models processing sequential data generally plagued vanishing exploding gradient problems. unitary recurrent neural networks unitary recurrence matrices recently proposed means avoid issues. however previous experiments recurrence matrices restricted product parameterized unitary matrices open question remains parameterization fail represent unitary matrices restricted representational capacity limit learned? address question propose full-capacity urnns optimize recurrence matrix unitary matrices leading signiﬁcantly improved performance urnns restricted-capacity recurrence matrix. contribution consists main components. first provide theoretical argument determine unitary parameterization restricted capacity. using argument show recently proposed unitary parameterization restricted capacity hidden state dimension greater second show complete full-capacity unitary recurrence matrix optimized differentiable manifold unitary matrices. resulting multiplicative gradient step simple require gradient clipping learning rate adaptation. conﬁrm utility claims empirically evaluating full-capacity urnns synthetic natural data achieving superior performance compared lstms original restricted-capacity urnns. deep feed-forward recurrent neural networks shown remarkably effective wide variety problems. primary difﬁculty training using gradient-based methods so-called vanishing exploding gradient problem instability gradients multiple layers impede learning problem particularly keen recurrent networks since repeated recurrent weight matrix magnify instability. problem addressed past various means including gradient clipping using orthogonal matrices initialization recurrence matrix using pioneering architectures long short-term memory recurrent networks gated recurrent units recently several innovative architectures introduced improve information network residual networks directly pass information previous layers feed-forward network attention networks allow recurrent network access past activations idea using unitary recurrent weight matrix introduced gradients inherently stable vanish explode resulting unitary recurrent neural network complex-valued uses complex form rectiﬁed linear activation function. however idea investigated using show potentially restricted form unitary matrices. main components contribution summarized follows provide theoretical argument determine smallest dimension parameterization unitary recurrence matrix cover entire unitary matrices. argument relies counting real-valued parameters using sard’s theorem show smooth parameters unitary manifold onto. thus show previously proposed parameterization cannot represent unitary matrices larger thus parameterization results refer restricted-capacity unitary recurrence matrix. overcome limitations restricted-capacity parameterizations propose method stochastic gradient descent training unitary recurrence matrix constrains gradient differentiable manifold unitary matrices. approach allows directly optimize complete full-capacity unitary matrix. neither restricted-capacity full-capacity unitary matrix optimization require gradient clipping. furthermore full-capacity optimization still achieves good results without adaptation learning rate training. test limitations restricted-capacity representation conﬁrm full-capacity urnn practical implications test restricted-capacity full-capacity urnns synthetic natural data tasks. tasks include synthetic system identiﬁcation long-term memorization frame-to-frame prediction speech spectra pixel-by-pixel classiﬁcation handwritten digits. proposed full-capacity urnns generally achieve equivalent superior performance synthetic natural data compared lstms original restrictedcapacity urnns next section give overview unitary recurrent neural networks. section presents ﬁrst contribution theoretical argument determine unitary parameterization restricted-capacity. section describes second contribution show optimize full-capacity unitary matrix. conﬁrm results simulated natural data section present conclusions section urnn proposed arjovsky consists following nonlinear dynamical system realcomplex-valued inputs dimension complex-valued hidden states dimension realcomplex-valued outputs dimension note non-linearity consists soft-thresholding magnitude using bias vector hard-thresholding would output |zi| parameters urnn follows unitary hidden state transition matrix; cn×m input-to-hidden transformation; nonlinearity bias; cl×n hidden-to-output transformation; output bias. arjovsky propose following parameterization unitary matrix diagonal unitary matrices householder reﬂection matrices discrete fourier transform matrix permutation matrix. resulting matrix unitary component matrices unitary. decomposition efﬁcient diagonal reﬂection permutation matrices compute dfts computed efﬁciently time using fast fourier transform parameter vector consists real-valued parameters parameters diagonal matrices ejθi parameters householder reﬂection matrices real imaginary values complex reﬂection vectors uiuh section state prove theorem used determine particular unitary parameterization capacity represent unitary matrices. application theorem show parameterization capacity cover unitary matrices first establish upper bound number real-valued parameters required represent unitary matrix. then state prove theorem. proof unitary matrices well-known unitary group group identiﬁes group elements points differentiable manifold dimension manifold equal dimension algebra vector space tangent space identity element algebra consists skewhermitian matrices skew-hermitian matrix cn×n conjugate transpose. determine dimension determine dimension skew-hermitian constraint diagonal elements purely imaginary corresponds real-valued parameters. also since upper lower triangular parts parameterized complex numbers corresponds additional real parameters. thus manifold dimension theorem family unitary matrices parameterized real-valued parameters cannot contain unitary matrices. proof consider family unitary matrices parameterized real-valued parameters smooth space parameters space unitary matrices space parameters considered -dimensional manifold space unitary matrices -dimensional manifold according lemma then sard’s theorem implies image measure zero particular onto. since onto must exist unitary matrix corresponding input thus manifold cannot represent unitary matrices apply theorem parameterization note parameterization real-valued parameters. solve thus parameterization cannot represent unitary matrices dimension section show around limitations restricted-capacity parameterizations directly optimize full-capacity unitary matrix. consider stiefel manifold complex-valued matrices whose columns orthonormal vectors mathematically stiefel manifold deﬁned matrix tangent space twvn stiefel manifold satisﬁes stiefel manifold becomes riemannian manifold tangent space equipped inner product. tagare suggests using canonical inner product given matrix usual gradient loss function respect matrix using facts tagare suggests descent curve along stiefel manifold training iteration given matrix product cayley transformation current solution learning rate gradient descent proceeds performing updates tagare suggests armijo-wolfe search along curve adapt procedure would expensive neural network optimization since requires multiple evaluations forward model gradients. found simply using ﬁxed learning rate often works well. also rmsprop-style scaling gradient running average previous gradients’ norms applying multiplicative step improve convergence. additional substantial computation required beyond forward backward passes network matrix inverse models implemented theano based implementation restricted-capacity urnns available https//github.com/amarshah/complex_rnn. code replicate results available https//github.com/stwisdom/urnn. models rmsprop optimization except full-capacity urnns optimize recurrence matrices ﬁxed learning rate using update step optional rmsprop-style gradient normalization. first compare performance full-capacity urnns restricted-capacity urnns lstms tasks synthetic data. ﬁrst task synthetic system identiﬁcation urnn must learn dynamics target urnn given samples target urnn’s inputs outputs. second task copy memory problem network must recall sequence data long period time. task system identiﬁcation consider problem learning dynamics nonlinear dynamical system form given dataset inputs outputs system. draw true system wsys randomly either constrained restricted-capacity unitary matrices using parameterization wider restricted-capacity unitary matrices guaranteed outside sample taking matrix product unitary matrices drawn sequence length input dimension output dimension equal hidden state dimension input-to-hidden transformation output-tohidden transformation identity output bias initial state hidden bias drawn uniform distribution range hidden bias mean ensure stability system outputs. inputs generated sampling -length i.i.d. sequences zero-mean diagonal unit covariance circular complexvalued gaussians dimension outputs created running system forward inputs. compare restricted-capacity urnn using parameterization full-capacity urnn using stiefel manifold optimization gradient normalization described section choose hidden state dimensions test critical points predicted arguments section dimensions chosen test below critical dimension experiments number training validation test sequences respectively. mean-squared error used loss function. learning rate batch size experiments. models matrix drawn initialization. isolate effect unitary recurrence matrix capacity optimize setting parameters true oracle values. method report best test loss epochs random initializations optimization. results shown table wsys init. refers initialization true system unitary matrix wsys sampled either restricted-capacity wider table results system identiﬁcation terms best normalized mse. restricted-capacity unitary matrices wider unitary matrices. notice restricted-capacity urnn achieves comparable better performance full-capacity urnn. restricted-capacity full-capacity urnns achieve relatively comparable performance full-capacity urnn achieving slightly lower error. full-capacity urnn always achieves better performance versus restricted-capacity urnn. result conﬁrms theoretical arguments restricted-capacity parameterization lacks capacity model matrices unitary group indicates advantage using full-capacity unitary recurrence matrix. experimental setup follows copy memory problem based experiment consider alternative hidden state dimensions extend sequence lengths longer maximum length considered previous literature. task data vector length consists elements categories. vector begins sequence symbols sampled uniformly categories next elements vector ninth ’blank’ category followed element tenth category ‘delimiter’. remaining elements ‘blank’. task output blank characters followed sequence beginning vector. average cross entropy training loss function. baseline solution outputs blank category time steps guesses random symbol uniformly ﬁrst eight categories. baseline expected average cross entropy figure results copy memory problem sequence lengths full-capacity urnn converges quickly perfect solution lstm restrictedcapacity urnn approximately number parameters unable improve past baseline naive solution. full-capacity urnn uses hidden state size gradient normalization. match number parameters restricted-capacity urnn lstm. training size test size results experiment found left half figure full-capacity urnn converges solution zero average cross entropy training iterations whereas restricted-capacity urnn settles baseline solution results experiment found right half figure full-capacity urnn hovers around baseline solution training iterations drops zero average cross entropy. restricted-capacity settles baseline solution results demonstrate full-capacity urnn effective problems requiring long memory. apply restricted-capacity full-capacity urnns real-world speech data compare performance lstms. main task consider predicting log-magnitude future frames short-time fourier transform stft commonly used feature domain speech enhancement deﬁned fourier transform short windowed frames time series. stft domain real-valued audio signal represented complex-valued matrix composed frames composed nwin/ frequency bins nwin duration time-domain frame. speech processing algorithms log-magnitude complex stft values reconstruct processed audio signal using phase original observations. frame prediction task follows given log-magnitudes stft frames time predict log-magnitude stft frame time timit dataset according common practice training utterances speakers validation utterances evaluation utterances. training validation evaluation sets distinct speakers. results reported evaluation using network parameters perform best validation terms loss function three training trials. timit audio resampled khz. stft uses hann analysis window samples window samples lstm requires gradient clipping optimization restricted-capacity fullcapacity urnns not. hidden state dimensions lstm chosen match number parameters full-capacity urnn. restricted-capacity urnn models match either number parameters. lstm restricted-capacity urnns rmsprop learning rate momentum averaging parameter full-capacity urnn also rmsprop optimize network parameters except recurrence matrix stochastic gradient descent along stiefel manifold using update ﬁxed learning rate gradient normalization. lstm restricted-capacity urnn restricted-capacity urnn full-capacity urnn lstm restricted-capacity urnn restricted-capacity urnn full-capacity urnn lstm restricted-capacity urnn full-capacity urnn results shown table figure shows example predictions three types networks. results table given terms mean-squared error loss function several metrics computed time-domain signals reconstructed predicted log-magnitude figure ground truth one-frame-ahead predictions spectrogram example utterance. model hidden state dimension chosen best validation mse. notice full-capacity urnn achieves best detail predictions. original phase stft. time-domain metrics segmental signal-to-noise ratio short-time objective intelligibility perceptual evaluation speech quality segsnr computed using uses voice activity detector avoid measuring silent frames. stoi designed correlate well human intelligibility speech takes values higher score indicating higher intelligibility pesq itu-t standard telephone voice quality testing popular perceptual quality metric speech enhancement pesq ranges note full-capacity urnns generally perform better restricted-capacity urnns number parameters types urnn signiﬁcantly outperform lstms. another challenging long-term memory task natural data test performance lstms urnns pixel-by-pixel mnist permuted pixel-by-pixel mnist ﬁrst proposed used test restricted-capacity urnns. permuted pixel-by-pixel mnist pixels shufﬂed thereby creating non-local dependencies pixels image. since mnist images pixels resulting pixel-by-pixel sequences elements long. training examples validation perform early stopping patience loss function cross-entropy. weights best validation loss used process evaluation set. full-capacity urnn uses rmsprop-style gradient normalization. table results unpermuted permuted pixel-by-pixel mnist. classiﬁcation accuracies reported trained model weights achieve best validation loss. lstm lstm restricted-capacity urnn full-capacity urnn full-capacity urnn lstm lstm restricted-capacity urnn full-capacity urnn full-capacity urnn learning curves shown figure summary classiﬁcation accuracies shown table unpermuted task lstm achieves best evaluation accuracy permuted task full-capacity urnn achieves best evaluation accuracy state-of-the-art task. urnns outperform lstms permuted case achieving best performance fewer traing epochs using equal lesser number trainable parameters. performance difference suggests lstms able model local dependencies urnns superior long-term memory capabilities. despite representing unitary matrices restricted-capacity urnn still achieves impressive test accuracy trainable parameters outperforming full-capacity urnn matches number parameters. result suggests exploration potential trade-off hidden state dimension capacity unitary parameterizations necessary. unitary recurrent matrices prove effective means addressing vanishing exploding gradient problems. provided theoretical argument quantify capacity constrained unitary matrices. also described method directly optimizing full-capacity unitary matrix constraining gradient differentiable manifold unitary matrices. effect restricting capacity unitary weight matrix tested system identiﬁcation memory tasks full-capacity unitary recurrent neural networks outperformed restrictedcapacity urnns well lstms. full-capacity urnns also outperformed restrictedcapacity urnns log-magnitude stft prediction natural speech signals classiﬁcation permuted pixel-by-pixel images handwritten digits types urnn signiﬁcantly outperformed lstms. future work plan explore general forms restricted-capacity unitary matrices including constructions based products elementary unitary matrices householder operators givens operators. acknowledgments thank anonymous reviewer suggesting improvements proof section vamsi potluru helpful discussions. scott wisdom thomas powers funded u.s. contract number n--g- delivery orders atlas funded u.s. grant wnf---.", "year": 2016}