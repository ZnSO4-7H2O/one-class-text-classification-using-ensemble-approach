{"title": "Understanding Black-box Predictions via Influence Functions", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "How can we explain the predictions of a black-box model? In this paper, we use influence functions -- a classic technique from robust statistics -- to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.", "text": "explain predictions blackbox model? paper inﬂuence functions classic technique robust statistics trace model’s prediction learning algorithm back training data thereby identifying training points responsible given prediction. scale inﬂuence functions modern machine learning settings develop simple efﬁcient implementation requires oracle access gradients hessian-vector products. show even non-convex non-differentiable models theory breaks down approximations inﬂuence functions still provide valuable information. linear models convolutional neural networks demonstrate inﬂuence functions useful multiple purposes understanding model behavior debugging models detecting dataset errors even creating visuallyindistinguishable training-set attacks. question often asked machine learning systems system make prediction? want models high-performing also explainable. understanding model does hope improve model discover science provide end-users explanations actions impact however best-performing models many domains e.g. deep neural networks image speech recognition complicated blackbox models whose predictions seem hard explain. work interpreting black-box models focused understanding ﬁxed model leads particular predictions e.g. locally ﬁtting simpler model around test paper tackle question tracing model’s predictions learning algorithm back training data model parameters ultimately derive from. formalize impact training point prediction counterfactual would happen training point values training point changed slightly? answering question perturbing data retraining model prohibitively expensive. overcome problem inﬂuence functions classic technique robust statistics tells model parameters change upweight training point inﬁnitesimal amount. allows differentiate training estimate closedform effect variety training perturbations. despite rich history statistics inﬂuence functions seen widespread machine learning; best knowledge work closest wojnowicz introduced method approximating quantity related inﬂuence generalized linear models. obstacle adoption inﬂuence functions require expensive second derivative calculations assume model differentiability convexity limits applicability modern contexts models often non-differentiable non-convex highdimensional. address challenges showing efﬁciently approximate inﬂuence functions using second-order optimization techniques remain accurate even underlying assumptions differentiability convexity degrade. inﬂuence functions capture core idea studying models lens training data. show versatile tool applied wide variety seemingly disparate tasks understanding model behavior debugging models detecting dataset errors creating visually-indistinguishable adversarial training examples neural network test predictions training analogue goodfellow def= training point deﬁne consider perturbation ˆθzδ−z empirical risk minimizer training points place approximate effects deﬁne parameters def= resulting moving mass onto ˆθ\u0001zδ−z minθ∈θ analogous calculation yields before make linear approximation ˆθzδ−z iupparams) giving closedθ form estimate effect model. analogous equations also apply changes inﬂuence functions might appear work inﬁnitesimal perturbations important note approximation holds arbitrary \u0001upweighting scheme allows smoothly interpolate between particularly useful working discrete data discrete label changes. ∇x∇θl. ipertlossδ tells approximate effect loss ztest. setting direction ipertloss construct local perturbations maximally increase loss ztest. section construct training-set attacks. finally note ipertloss help identify features responsible prediction ztest. approach consider prediction problem input space output space given training points loss pirical risk. empirical risk minimizer given def= minθ∈θ assume empirical risk twice-differentiable strictly convex section explore relaxing assumptions. goal understand effect training points model’s predictions. formalize goal asking counterfactual would model’s predictions change training point? fortunately inﬂuence functions give efﬁcient approximation. idea compute parameter change upweighted small giving parameters ˆθ\u0001z classic result tells inﬂuence upweighting parameters given hessian positive deﬁnite assumption. essence form quadratic approximation empirical risk around take single newton step; appendix derivation. since removing point upweighting linearly approximate parameter change removing computing ˆθ−z next apply chain rule measure upweighting changes functions particular inﬂuence upweighting loss test point ztest closed-form expression terms iuploss? here plot iuploss figure components inﬂuence. effect training loss variants missing terms show necessary picking truly inﬂuential training points. calculations logistic regression distinguish mnist picking arbitrary test point ztest; similar trends hold across test points. green dots train images label test image dots left without train loss term overestimate inﬂuence many training points points near line iuploss close instead high inﬂuence remove train loss term. without green training points helpful points harmful xtest incorrect many harmful training points actually share label ztest. panel right without training loss testx fails accurately capture inﬂuence; scatter plot deviates quite diagonal. test image harmful training image label. model look different presence training image makes model think test image less likely euclidean inner product pick less intuitive important harmful inﬂuences. space points norm equivalent choosing largest xtest. intuition compare iuploss logistic regression model show inﬂuence much accurate accounting effect training. +exp. seek maximize probability training set. training point logx)) −σx)yx iuploss highlight differences xtest. first gives points high training loss inﬂuence revealing outliers dominate model parameters. second weighted covariance matrix measures resistance training points removal points direction little variation inﬂuence higher since moving direction signiﬁcantly increase loss training points. show differences mean inﬂuence functions capture effect model training much accurately nearest neighbors. expensive models like deep neural networks millions parameters. second often want calculate iuploss across training points ﬁrst problem well-studied second-order optimization. idea avoid explicitly computing instead implicit hessian-vector products efﬁciently approximate stest compute iuploss −stest ∇θl. also solves second problem test point interest precompute stest efﬁciently compute −stest training point discuss techniques approximating stest relying fact single term computed arbitrary time would take typically conjugate gradients ﬁrst technique standard transformation matrix inversion optimization problem. since assumption mint{ thˆθt vt}. solve approaches require evaluation hˆθt takes time without explicitly forming hˆθ. exact solution takes iterations practice good approximation fewer iterations; martens details. stochastic estimation. large datasets standard slow; iteration still goes training points. method developed agarwal estimator samples single point iteration results signiﬁcant speedups. ﬁrst terms taylor expansion rewrite recursively validity taylor expansion iteration substitute full draw unbiased estimator form ˜hj. since particular uniformly sample unbiased estimator gives following procedure uniformly sample points ﬁnal unbiased estimate h−v. pick large enough stabilizes reduce variance repeat procedure times average results. empirically found signiﬁcantly faster note original method agarwal dealt generalized linear models efﬁciently computed time. case rely pearlmutter general algorithm fast hvps described above achieve time complexity. techniques compute iuploss training points time; show section empirically choosing gives accurate results. similarly compute ipertloss n∇θlh− matrix-vector products ﬁrst compute stest test∇x∇θl trick. computations easy implement auto-grad systems like tensorflow theano users need specify rest automatically handled. recall inﬂuence functions asymptotic approximations leave-one-out retraining assumptions model parameters minimize empirical risk empirical risk twice-differentiable true scale loss without affecting parameters. cases upper bound makes easy. otherwise treat scaling separate hyperparameter tune taylor expansion converges. figure inﬂuence matches leave-one-out retraining. arbitrarily picked wrongly-classiﬁed test point ztest trend held broadly. results -class mnist. left plotted iuploss actual change test loss removing point retraining. inverse solved exactly same stochastic approximation. right plot computed inﬂuential points actual difference loss removed point retrained steps. inﬂuence functions assume weight training point changed inﬁnitesimally small investigate accuracy using inﬂuence functions approximate effect removing training point retraining compared niuploss logistic regression model -class mnist predicted actual changes matched closely stochastic approximation agarwal also accurate repeats iterations since iteration requires result could negative eigenvalues. show inﬂuence functions still give meaningful results practice. figure smooth approximations hinge loss. varying approximate hinge loss arbitrary accuracy green blue lines overlaid other. using random wrongly-classiﬁed test point compared predicted actual differences loss leave-one-out retraining inﬂuential training points. similar trend held test points. objective minimize hingexi). left inﬂuence functions unable accurately predict change overestimating magnitude considerably. using smoothhinge accurately predict change hinge loss retraining. right correlation remained high wide range though degrades large. pearson’s pearson’s here damping term negative eigenvalues; corresponds adding regularization calculate iuploss using close local minimum correlated result taking newton step removing weight checked behavior iuploss non-convergent non-convex setting training convolutional neural network iterations. model converged added damping term even difﬁcult setting predicted actual changes loss highly correlated non-differentiable losses happens derivatives loss section show inﬂuence functions computed smooth approximations non-differentiable losses predict behavior original non-differentiable loss leave-one-out retraining. robustness approximation suggests train non-differentiable models swap non-differentiable components smoothed versions purposes calculating inﬂuence. this trained linear mnist task section involves minthe network sets convolutional layers tanh non-linearities modeled all-convolutional network speed used mnist training parameters since repeatedly retraining network expensive. training done mini-batches examples adam optimizer model converged iterations; training another iterations using full training pass iteration reduced train loss imizing hinge max; simple piecewise linear function similar relus cause nondifferentiability neural networks. derivatives hinge calculated iuploss. might expect inaccurate second derivative carries information close support vector hinge quadratic approximation linear leads iuploss overestimating inﬂuence purposes calculating inﬂuence approximated hinge smoothhinge log) approaches hinge loss using weights before found calculating iuploss using smoothhinge closely matched actual change retraining original hinge remained accurate wide range telling training points responsible given prediction inﬂuence functions reveal insights models rely extrapolate training data. section show models make correct predictions different ways. compared state-of-the-art inception network layer frozen kernel image classiﬁcation dataset extracted imagenet training examples class. freezing neural networks uncommon computer vision equivalent training logistic regression model bottleneck features picked test image models correct used smoothhinge compute inﬂuence svm. expected iuploss varied inversely pixel distance training images test image pixel space almost inﬂuence. inception inﬂuences much less correlated distance pixel space looking helpful images model -right inception network picked distinctive characteristics clownﬁsh whereas pattern-matched training images superﬁcially. moreover close test image mostly helpful dogs mostly harmful acting soft nearest neighbor function contrast inception network dogs could helpful harmful correctly classifying test image ﬁsh; fact helpful training images dogs that model looked different test svm. bottom left figure inception −iuploss green dots dots dogs. bottom right helpful training images model test. right image training helped inception model correctly classify test image ﬁsh. section show models place inﬂuence small number points vulnerable training input perturbations posing serious security risk real-world systems attackers inﬂuence training data recent work generated adversarial test images visually indistinguishable real test images completely fool classiﬁer demonstrate inﬂuence functions used craft adversarial training images similarly visuallyindistinguishable model’s prediction separate test image. best knowledge ﬁrst proof-of-concept visually-indistinguishable training attacks executed otherwise highly-accurate neural networks. idea ipertloss tells modify training point increase loss ztest. concretely target test image ztest construct adversarial version training image ini˜ tializing step size projects onto valid images share representation iteration retrain model. iterated training-set analogue methods used e.g. goodfellow moosavidezfooli test-set attacks. tested training attacks inception network dogs section choosing pair animals provide stark contrast classes. attack iterations test image. before froze layer training; note computing ipertloss still involves differentiating entire network. originally model correctly classiﬁed test images. test images considered separately tried visually-indistinguishable perturbation single training image total training images would model’s prediction. able test images. perturbing training images test image could predictions test images; perturbed training images could results attacking test image separately i.e. using different training attack test image. also tried attack multiple test images simultaneously increasing average loss found single training image perturbations could simultaneously multiple test predictions well make three observations attacks. first though change pixel values small change ﬁnal inception feature layer signiﬁcantly larger using distance pixel space training values change less mean distance training point class centroid whereas inception feature space change order mean distance. leaves open possibility attacks visuallyimperceptible detected examining feature space. second attack tries perturb training exfigure training-set attacks. targeted test images featuring ﬁrst author’s variety poses backgrounds. maximizing average loss images created visuallyimperceptible change particular training image ﬂipped predictions test images. ample direction variance causing model overﬁt direction consequently incorrectly classify test images; expect attacking harder number training examples grows. third ambiguous mislabeled training images effective points attack model conﬁdence thus high loss them making highly inﬂuential example image contains highly ambiguous; result training example model least conﬁdent attack mathematically equivalent gradientbased training attacks explored biggio others context different models. biggio constructed dataset poisoning attack linear two-class mnist task modify training points obviously distinguishable effective. measuring magnitude ipertloss gives model developers quantifying vulnerable models training-set attacks. domain mismatch training distribution match test distribution cause models high training accuracy poorly test data show inﬂuence functions identify training examples responsible errors helping model developers identify domain mismatch. case study predicted whether patient would readmitted hospital. domain mismatches common biomedical data e.g. different hospitals serve different populations models trained population poorly another used logistic regression predict readmission balanced training dataset diabetic patients hospitals represented features children dataset re-admitted. induce domain mismatch ﬁltered children re-admitted leaving readmitted. caused model wrongly classify many children test set. identify children training responsible errors. baseline tried common practice looking learned parameters indicator variable child obviously different. however work features larger coefﬁcient. picking random child ztest model wrong calculated −iuploss training point clearly highlighted training children times inﬂuential next inﬂuential examples. child training readmitted positive inﬂuence negative inﬂuences. moreover calculating ipertloss children showed ‘child’ indicator variable contributed signiﬁcantly magnitude iuploss. labels real world often noisy especially crowdsourced even adversarially corrupted. even human expert could recognize wrongly labeled examples impossible many applications manually review training data. show inﬂuence functions help human experts prioritize attention allowing inspect examples actually matter. user-provided labels also vulnerable adversarial attack ﬂipped labels random training data simulated manually inspecting fraction training points correcting ﬂipped. using inﬂuence functions prioritize training points inspect allowed repair dataset without checking many points outperforming baselines checking points highest train loss random method access test data. figure fixing mislabeled examples. plots test accuracy fraction ﬂipped data detected change fraction train data checked using different algorithms picking points check. error bars show std. dev. across repeats experiment different subset labels ﬂipped each; error bars right small seen. results enron spam dataset training test examples; trained logistic regression bag-of-words representation emails. inﬂuence-based diagnostics originated statistics driven seminal papers cook others though similar ideas appeared even earlier forms e.g. inﬁnitesimal jackknife earlier work focused removing training points linear models later work extending general models wider variety perturbations prior work focused experiments small datasets e.g. cook weisberg special attention therefore paid exact solutions possible characterizations error terms. inﬂuence functions used much literature exceptions. christmann steinwart debruyne inﬂuence functions study model robustness fast cross-validation kernel methods. wojnowicz uses matrix sketching estimate cook’s distance closely related inﬂuence; focus prioritizing training points human attention derive methnoted section training-set attack mathematically equivalent approach ﬁrst explored biggio context svms follow-up work extending framework applying linear logistic regression topic modeling collaborative ﬁltering papers derived attack directly conditions without considering inﬂuence though continuous data result equivalent. inﬂuence functions additionally consider attacks discrete data tested empirically. work connects literature trainingset attacks work adversarial examples visuallyimperceptible perturbations test inputs. contrast training-set attacks cadamuro consider task taking incorrect test prediction ﬁnding small subset training data changing labels subset makes prediction correct. provide solution gaussian process models labels continuous. work inﬂuence functions allow solve problem much larger range models datasets discrete labels. discussed variety applications creating training-set attacks debugging models ﬁxing datasets. underlying applications common tool inﬂuence function based simple idea better understand model behavior looking derived training data. core inﬂuence functions measure effect local changes happens upweight point inﬁnitesimally-small locality allows derive efﬁcient closed-form estimates show surprisingly effective. however might want global changes e.g. subpopulation patients hospital affect model? since inﬂuence functions depend model changing much tackle open question. seems inevitable high-performing complex blackbox models become increasingly prevalent important. hope approach presented looking model lens training data become standard part toolkit developing understanding diagnosing machine learning. consider training point model parameters close local minimum iupparams approximately equal constant plus change parameters upweighting taking single newton step high-level idea even though gradient empirical risk newton step decomposed component following existing gradient second component responding upweighted tracks). gradient emlet def= pirical risk since local minimum upweighting gradient goes \u0001∇θl empirical hessian goes newton step therefore changes parameters thank jacob steinhardt zhenghao chen hongseok namkoong helpful discussions comments. work supported future life research award microsoft research faculty fellowship. completeness provide standard derivation inﬂuence function iupparams context loss minimization derivation based asymptotic arguments fully rigorous; vaart statistics textbooks thorough treatment. recall minimizes empirical risk isard j´ozefowicz kaiser kudlur levenberg man´e monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan vi´egas vinyals warden wattenberg wicke zheng tensorﬂow large-scale machine learning heterogeneous distributed systems. arxiv preprint arxiv. amershi chickering drucker simard modeltracker redesigning performance analysis tools machine learning. conference human factors computing systems donahue vinyals hoffman zhang tzeng darrell decaf deep convolutional activation feature generic visual recognition. international conference machine learning volume jiang liao efﬁcient approximation cross-validation kernel methods using bouligand inﬂuence function. international conference machine learning strack deshazo gennings olmo ventura cios clore impact hbac measurement hospital readmission rates analysis clinical database patient records. biomed research international russakovsky deng krause satheesh huang karpathy khosla bernstein imagenet large scale visual recognition challenge. international journal computer vision", "year": 2017}