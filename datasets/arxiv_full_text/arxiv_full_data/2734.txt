{"title": "Summable Reparameterizations of Wasserstein Critics in the  One-Dimensional Setting", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Generative adversarial networks (GANs) are an exciting alternative to algorithms for solving density estimation problems---using data to assess how likely samples are to be drawn from the same distribution. Instead of explicitly computing these probabilities, GANs learn a generator that can match the given probabilistic source. This paper looks particularly at this matching capability in the context of problems with one-dimensional outputs. We identify a class of function decompositions with properties that make them well suited to the critic role in a leading approach to GANs known as Wasserstein GANs. We show that Taylor and Fourier series decompositions belong to our class, provide examples of these critics outperforming standard GAN approaches, and suggest how they can be scaled to higher dimensional problems in the future.", "text": "maintain estimate earth-mover’s distance between generator’s distribution target distribution functional form used guide improving generator. informally earth-mover’s distance probability distributions thought amount work would transporting probability mass within distribution make indistinguishable. particularly nice property earth-mover’s distance that mild constraints deﬁned gradient almosteverywhere making ideal gradient-based optimization. optimize space critics optimizer must ensure functions produces k-lipschitz—that norm gradients less scalar domain. popular approach toward enforcing constraint involves assigning penalty functions violate subset domain. approach shown produce visually appealing results variety popular image benchmarks guarantee critic network converge optimal critic. failure critic achieve optimality result generators diverge suffer modecollapse. work introduce reparameterization critic network one-dimensional setting guarantees convergence. show reparameterized critic performs better standard gradient-penalty wgan approaches one-dimensional simulated domains. generative adversarial networks generative adversarial networks traditionally introduced setting target data source samples drawn. deﬁned terms distinct network components generator discriminator generator takes randomly sampled noise input produces generated samples distributed according output. discriminator takes real generated data points input returns generative adversarial networks exciting alternative algorithms solving density estimation problems—using data assess likely samples drawn distribution. instead explicitly computing probabilities gans learn generator match given probabilistic source. paper looks particularly matching capability context problems one-dimensional outputs. identify class function decompositions properties make well suited critic role leading approach gans known wasserstein gans. show taylor fourier series decompositions belong class provide examples critics outperforming standard approaches suggest scaled higher dimensional problems future. generative adversarial networks introduced goodfellow quickly become leading technique learning generate data points matching samples distribution. gans produce samples without directly modeling target probability distribution. jointly training neural networks generator attempts produce synthetic data points consistent source distribution discriminator seeks determine whether given data point drawn source distribution generated one. joint training procedure difﬁcult stabilize many conceptual variants framework proposed improve results. focus speciﬁcally variant wasserstein wgan standard framework derived minimax game agents wgan framework reformulates problem terms minimizing distance metric probability distributions. particularly wgan formulated using dual form earth-mover’s distance reasonably approximated neural network. construction results similar two-network setup network acting generator another acting critic—its role copyright association advancement artiﬁcial intelligence rights reserved. bearing constraint mind success gradientbased optimization relies heavily parameter space optimized function nice topological sense. particularly highly concave disconnected parameter space provides much harder optimization problem increases likelihood optimizer settling local optimum. imposing -lipschitz constraint optimization procedure certainly complicates topology parameter space. denotes derivative denotes factorial arbitrary point domain important note approximation centered moves away central point approximation become less accurate. fourier series another method approximating functions functions time sinusoidal functions decreasing periodicity. one-dimensional setting expressed deﬁne properties critic representation show leads improvements wgan framework. sequence functions functions derivatives bounded maxx |ξnm| maxx bounding constant depend optimize objective generator discriminator networks take turns updating parameters network’s parameters held ﬁxed. collectively objective thought certainty discriminator. generator aims minimize certainty produce generated samples distributionally indistinguishable drawn real data-generating source. conversely discriminator aims maximize certainty learning discern real samples generated ones providing pressure generator closely match real distribution. wasserstein generative adversarial networks alternative game-theoretic approach wasserstein gans seek minimize earth-mover’s distance probability distributions denotes space -lipschitz functions. optimized similarly setup described above. form generator network used produce generated data samples. however place discriminator critic network used represent function collectively resulting optimization procedure takes following form setting critic updates parameters successively generator network held ﬁxed maximize expression. sufﬁciently maximized critic’s parameters frozen generator network takes step minimize important note procedure special care must taken ensure critic function belongs class -lipschitz functions. successful method ensuring property applying gradient penalty additional term loss function last inequality satisfying modiﬁed -lipschitz constraint hence critic linearly interpolated critics satisfying also satisﬁes constraint. thus space critics satisﬁes constraint convex summable parameterization. next observe wasserstein distance summable critic parameterization linear parameters critic. collectively procedure maximizing wasserstein distance respect critic parameters convex objective convex constraint. settings critic’s parameters. next suppose satisfy constraint local maxima without loss generality. exist then convexity constraint interpolations also satisfy constraint. thus consider sequence parameter settings given clearly limt→∞ satisﬁes constraint. moreover write last approximation follows property emphasize parameterization critic’s parameters coefﬁcients structure parameters constraint objective function give useful properties. speciﬁcally approximation -lipschitz constraint convex optimization objective respect critic linear. hence following theorem. theorem optimization summable critic network setting parameters local maximum also global maximum. work gretton explore using reproducing kernel hilbert spaces function class perform maximum mean discrepancy tests. kernel-based approach adopted swersky zemel work generative moment matching networks. work offers method competes directly gans. rather mini-max game generator discriminator generative moment matching networks boast needing generator network trained minimize maximum mean discrepancy real generated sources. authors note kernels approximates matching moments sampled generated random variables. moment matching moment matching also known method-of-moments process ﬁtting model distribution sampling distribution setting model’s parameters distribution’s sampled moments. general moments refer functions characterize behavior random variable commonly represented random variable raised different powers. denote moment random variable domains evaluated method three different synthetic data sources real-world data source. synthetic data sources consisted sawtooth distribution discrete distribution three possible values mixture gaussian distributions. distribution sampled times construct dataset used across experiments models. distributions correpond following random variables deﬁned below thus representing class critic functions taylor fourier expansions obtain clean enforce -lipschitz constraint entire domain ensuring gradient-based optimization schemes globally optimal critic. note enforcing upper bound -lipschitz constraint optimizing smaller functions. however noticed additional constriction affect performance empirically. minimizing expresions above slight modiﬁcations must made computational tractability. first must choose remaining terms outer sum. second must enforce constraint penalty term loss function. fortunately neither practical considerations change theoretical guarantees proved above. particularly limiting number terms outer affect convexity-based arguments embedding constraint loss function penalty still results convex optimization problem. maximum mean discrepancy recent work gretton explores maximum mean discrepancy technique distinguish samples drawn different data sources. maximum mean discrepancy data sources deﬁned real-world data source collection city populations free world cities database pre-processed data applying logarithmic scaling population numbers normalizing resulting log-populations denote random variable associated data source xcities. network architectures maintain consistency experiments used generator network architecture wgan-gp experiments method. generator network architecture consists batch-normalized fully connected layers neurons leaky relu activation followed single fully connected output layer neuron tanh activation. wgan-gp experiment used discriminator fully connected layers neurons leaky relu activations followed single fully connected output layer neuron linear activation. following gulrajani used enforce constraints across experiments. additionally used adamoptimizer learning rate reparameterized critic models clipped inﬁnite sums expansions additionally batch normalization used generator networks experiments. evaluation procedure comparison algorithms attempt learn representation target -dimensional probability distribution. model measure accuracy computing sample earth-mover’s distance. quantity computed sampling model times constructing histogram sampled outputs. entries histograms normalized values similar histogram constructed using true data source earth-mover’s distance computed between them. computing earth-mover’s distance used publicly available python library pyemd. methods training conducted iterations estimate earth-mover’s distance computed training data every iterations. training lowest estimate course training reported model’s earth-mover’s distance results present results running trials gan-based models. denote runs reparameterized critics taylor critic fourier critic fourier series taylor series reparameterizations respectively. best obtained earth-mover’s distances model reported tables additionally report average earth-mover’s distances across trials compare numbers performance kernel density estimator nonparametric baseline. results posted table observe models reparameterized critics signiﬁcantly outperform wgan-gp frequently competitive kernel density estimation. tables observe reparameterized critic models’ worst runs generally better wgan-gp model’s best runs reparameterized critic models significantly lower variance across runs wgan-gp. since gan-based methods paper network architecture generators reasonable attribute difference forms critics. showed theorem process optimizing critic respect given generator cannot stuck locally maximal region space critics. thus long critics satisfying sufﬁciently close critics satisfying generator always clean gradient follow optimization shown lemma gulrajani preclude possibility generator could stuck optimization critic difference consistency across runs reparameterized critic models wgangp models evidence additional guarantees reparameterized critics helps empirically. note made every effort hyperparameters gp-wgan reduce eliminate instability. possible would perform better parameter setting able setting. said performance reparameterized critic models relatively unchanged across parameter settings explored. work illustrated alternate parameterization critic networks ideal theoretical properties gradient-based optimization. demonstrated that one-dimensional setting summable critic models categorically outperform wasserstein gradient penalty competitive kernel density estimation variety synthetic real-world domains. work paper focuses onedimensional setting considerable room explore extending approach higher dimensions. taylor fourier series expansions highdimensional analogues. higher-dimensional decompositions generally require exponentially many terms number input dimensions. possible alleviate computational cost exploiting recent techniques learn sparse polynomials fourier series particularly exponentially many terms series necessary model arbitrarily messy functions unlikely even required reasonably approximate space -lipschitz functions.", "year": 2017}