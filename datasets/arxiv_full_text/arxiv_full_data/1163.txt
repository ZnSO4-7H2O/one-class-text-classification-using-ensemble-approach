{"title": "Unsupervised Learning of Video Representations using LSTMs", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "We use multilayer Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (\"percepts\") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We try to visualize and interpret the learned features. We stress test the model by running it on longer time scales and on out-of-domain data. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only a few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.", "text": "caption generation images also applied videos recognizing actions generating natural language descriptions general sequence sequence learning framework described sutskever recurrent network used encode sequence ﬁxed length representation anrecurrent network used decode sequence representation. work apply extend framework learn representations sequences images. choose work unsupervised setting access dataset unlabelled videos. videos abundant rich source visual information seen window physics world live showing examples constitutes objects objects move backgrounds happens cameras move things occluded. able learn representation disentangles factors would help making intelligent machines understand environment. additionally learning good video representations essential number useful tasks recognizing actions gestures. supervised learning extremely successful learning good visual representations produce good results task trained also transfer well tasks datasets. therefore natural extend approach learning video representations. research convolutional nets different temporal fusion strategies exploring different ways presenting visual information convolutional nets however videos much higher dimensional entities compared single images. therefore becomes increasingly difﬁcult credit assignment learn long range structure unless collect much labelled data feature engineering keep multilayer long short term memory networks learn representations video sequences. model uses encoder lstm input sequence ﬁxed length representation. representation decoded using single multiple decoder lstms perform different tasks reconstructing input sequence predicting future sequence. experiment kinds input sequences patches image pixels high-level representations video frames extracted using pretrained convolutional net. explore different design choices whether decoder lstms condition generated output. analyze outputs model qualitatively well model extrapolate learned video representation future past. visualize interpret learned features. stress test model running longer time scales out-of-domain data. evaluate representations ﬁnetuning supervised learning problem human action recognition ucf- hmdb- datasets. show representations help improve classiﬁcation accuracy especially training examples. even models pretrained unrelated datasets help action recognition performance. understanding temporal sequences important solving many problems ai-set. recently recurrent neural networks using long short term memory architecture used successfully perform various supervised sequence learning tasks speech recognition machine translation makes particularly well suited domain building unsupervised learning models. designing unsupervised learning model crucial right inductive biases choose right objective function learning signal points model towards learning useful features. paper lstm encoder-decoder framework learn video representations. inductive bias operation must applied time step propagate information next step. enforces fact physics world remains same irrespective input. physics acting state time must produce next state. model works follows. encoder lstm runs sequence frames come representation. representation decoded another lstm produce target sequence. consider different choices target sequence. choice predict sequence input. motivation similar autoencoders wish capture needed reproduce input time inductive biases imposed model. another option predict future frames. motivation learn representation extracts needed extrapolate motion appearance beyond observed. natural choices also combined. case decoder lstms decodes representation input sequence another decodes representation predict future. inputs model principle representation individual video frames. however purposes work limit attention kinds inputs. ﬁrst image patches. natural image patches well dataset moving mnist digits. second high-level percepts extracted applying convolutional trained imagenet. percepts states last layers rectiﬁed linear hidden states convolutional neural model. order evaluate learned representations qualitatively analyze reconstructions predictions made model. quantitative evaluation lstms initializations supervised task action recognition. unsupervised learning model comes useful representations classiﬁer able perform better especially labelled examples. indeed case. ﬁrst approaches learning representations videos unsupervised based approached problem using multiple layers independent subspace analysis modules. generative models understanding transformations pairs consecutive images also well studied work extended recently michalski model longer sequences. recently ranzato proposed generative model videos. model uses recurrent neural network predict next frame interpolate frames. work authors highlight importance choosing right loss function. argued squared loss input space right objective respond well small distortions input space. proposed solution quantize image patches large dictionary train model predict identity target patch. solve problems squared loss introduces arbitrary dictionary size picture altogether removes idea patches similar dissimilar other. designing appropriate loss function respects notion visual similarity hard problem therefore paper simple squared loss objective function starting point focus designing encoder-decoder architecture used loss function. section describe several variants lstm encoder-decoder model. basic unit network lstm cell block. implementation lstms follows closely discussed graves vised learning. model consists rnns encoder lstm decoder lstm shown fig. input model sequence vectors encoder lstm reads sequence. last input read decoder lstm takes outputs prediction target sequence. target sequence input sequence reverse order. reversing target sequence makes optimization easier model ground looking range correlations. also inspired lists represented lisp. encoder seen creating list applying cons function previously constructed list input. decoder essentially unrolls list hidden output weights extracting element list hidden hidden weights extracting rest list therefore ﬁrst element last element decoder kinds conditional unconditioned. conditional decoder receives last generated output frame input i.e. dotted input fig. present. unconditioned decoder receive input. discussed detail sec. fig. shows single layer lstm autoencoder. architecture extend multiple layers stacking lstms other. learn good features? state encoder lstm last input read representation input video. decoder lstm asked reconstruct back input sequence representation. order representation must retain information appearance objects background well motion contained video. however important question autoencoder-style model prevents learning identity mapping effectively copying input output. case information input would still present representation better input. factors control behaviour. first fact ﬁxed number hidden units makes unlikely model output gate lstm unit operates follows. time step receives inputs external sources four terminals ﬁrst source current frame second source previous hidden states lstm units layer ht−. additionally gate internal source cell state cell block. links cell gates called peephole connections. inputs coming different sources added along bias. gates activated passing total input logistic function. total input input terminal passed tanh non-linearity. resulting activation multiplied activation input gate. added cell state multiplying cell state forget gate’s activation ﬁnal output lstm unit computed multiplying output gate’s activation updated cell state passed tanh non-linearity. updates summarized layer lstm units follows ftct− tanh tanh. note matrices diagonal whereas rest dense. advantage using lstm unit traditional neuron cell state lstm unit sums activities time. since derivatives distribute sums error derivatives don’t vanish quickly sent back time. makes easy credit assignment long sequences discover longrange features. learn trivial mappings arbitrary length input sequences. second lstm operation used decode representation recursively. means dynamics must applied representation stage decoding. prevents model learning identity mapping. another natural unsupervised learning task sequences predicting future. approach used language models modeling sequences words. design future predictor model autoencoder model except decoder lstm case predicts frames video come input sequence ranzato similar model predict next frame time step. model hand predicts long sequence future. consider variants decoder conditional unconditioned. learn good features? order predict next frames correctly model needs information objects background present moving motion extrapolated. hidden state coming encoder capture information. therefore state seen representation input sequence. models consider possibilities decoder lstm conditioned last generated frame not. experimental section explore choices quantitatively. brieﬂy discuss arguments conditional decoder. strong argument favour using conditional decoder allows decoder model multiple modes target sequence distribution. withthat would averaging multiple modes low-level input space. however issue expect multiple modes target sequence distribution. lstm autoencoder correct target hence unimodal target distribution. lstm future predictor possibility multiple targets given input even assume deterministic universe everything needed predict future necessarily observed input. also argument using conditional decoder optimization point-of-view. strong short-range correlations video data example content frame previous one. decoder given access last frames generating particular frame training time would easy pick correlations. would small gradient tries extremely subtle errors require long term knowledge input sequence. unconditioned decoder input removed model forced look information deep inside encoder. tasks reconstructing input predicting future combined create composite model shown fig. encoder lstm asked come state predict next frames well reconstruct input. composite model tries overcome shortcomings model suffers own. high-capacity autoencoder would suffer tendency learn trivial representations memorize inputs. however memorization useful predicting future. therefore composite model cannot memorize information. hand future predictor suffers form tendency store information last frames since important predicting future i.e. order predict frames {vt− vt−k} much important small value therefore representation encoder forgotten large part input. model also predict input sequence cannot attention last frames. ucf- hmdb- datasets supervised tasks. ucf- dataset contains videos average length seconds belonging different action categories. dataset standard train/test splits training containing around videos split hmdb- dataset contains videos belonging different action categories. mean length videos seconds. also train/test splits videos training rest test. train unsupervised models used subset sports-m dataset contains million youtube clips. even though dataset labelled actions supervised experiments logistical constraints working huge dataset. instead collected hours video randomly sampling second clips dataset. possible collect better samples instead choosing randomly extracted videos motion happening shot boundaries. however spirit unsupervised learning want introduce unnatural bias samples. also used supervised datasets unsupervised training. however found using give signiﬁcant advantage using youtube videos. extracted percepts using convolutional neural model simonyan zisserman videos resolution sampled frames second. took central patch frame convnet. gave percepts. additionally ucf- computed percepts extracting ﬂows using brox method training temporal stream convolutional network described simonyan zisserman found features worked better single frame classiﬁcation using percepts. therefore used -dimensional layer input representation data. besides percepts also trained proposed models patches pixels. models trained using backprop single nvidia titan gpu. layer unit composite model predicts frames reconstructs frames took hours converge hours percepts. initialized weights sampling uniform distribution whose scale /sqrt. biases gates initialized zero. peep-hole connections initialized zero. supervised classiﬁers trained frames took minutes converge. code found https//github.com/emansim/ unsupervised-videos. experiments mnist ﬁrst trained models dataset moving mnist digits. dataset video frames long consisted digits moving inside patch. digits chosen randomly training placed initially random locations inside patch. digit assigned velocity whose direction chosen uniformly randomly unit circle whose magnitude also chosen uniformly random ﬁxed range. digits bounced-off edges frame overlapped location. reason working dataset inﬁnite size generated quickly makes possible explore model without expensive disk accesses overﬁtting issues. also interesting behaviours occlusions dynamics bouncing walls. ﬁrst trained single layer composite model. lstm units. encoder took frames input. decoder tried reconstruct frames future predictor attempted predict next frames. used logistic output units cross entropy loss function. fig. shows examples running model. true sequences shown ﬁrst rows. next rows show reconstruction future prediction layer composite model. interesting note model ﬁgures separate superimposed digits model even pass other. shows evidence disentangling independent factors variation sequence. model also correctly predict motion bouncing walls. order adding depth helps trained layer composite model layer units. adding depth helps model make better predictions. next changed future predictor making conditional. model makes sharper predictions. experiments natural image patches next tried models also work natural image patches. this trained models sequences natural image patches extracted ucf- dataset. case used linear output units squared error loss function. input frames model asked reconstruct frames predict future frames. fig. shows results obtained layer composite model generalization time scales next experiment test model work time scales different trained take hidden layer unconditioned composite model trained moving mnist digits. model lstm units looks input. trained input sequences frames reconstruct frames well predict frames future. order test future predictor able generalize beyond frames model steps future. fig. shows pattern activity lstm units future predictor pathway randomly chosen test input. shows activity three sigmoidal gates input cell state ﬁnal output even though units ordered randomly along vertical axis dynamics periodic quality model able generate persistent motion long periods time. terms reconstruction model outputs blobs ﬁrst frames motion relatively well preserved. results including long range future predictions hundreds time steps figure reconstruction future prediction obtained composite model dataset natural image patches. ﬁrst rows show ground truth sequences. model takes frames inputs. last frames input sequence shown here. next frames ground truth future. rows follow show reconstructed predicted frames instances model. http//www.cs.toronto.edu/˜nitish/ unsupervised_video. show setting periodic behaviour trivial fig. shows activity randomly initialized future predictor. here lstm state quickly converges outputs blur completely. out-of-domain inputs next test model’s ability deal out-ofdomain inputs. this test model sequences three moving digits. model trained sequences moving digits never seen inputs digit three digits. fig. shows reconstruction future prediction results. moving digit model good really tries hallucinate second digit overlapping ﬁrst one. second digit shows towards future reconstruction. three digits model merges digits blobs. however well getting overall motion right. highlights drawback modeling entire frames input single pass. order model videos variable number objects perhaps need models attention mechanism place also learn execute variable number times variable amounts computation. visualizing features next visualize features learned model. fig. shows weights connect input frame encoder lstm. four sets weights. weights connects frame input units. three sets corresponding three gates weight size features look like thin strips. others fig. shows output features lstm decoders composite model. correspond weights connecting lstm output units output layer. appear somewhat qualitatively different input features shown fig. many output features local blobs whereas rare input features. output features ones look like strips much shorter input features. interpret following. model needs know motion input. requires precise information location velocity generating output model wants hedge bets suffer huge loss predicting things sharply wrong place. could explain output features somewhat bigger blobs. relative shortness strips output features explained fact inputs hurt longer feature needed detect location information coarse-coded multiple features. output model want feature bigger digit units conspire correct figure pattern activity randomly chosen lstm units future predictor layer composite model trained moving mnist digits. vertical axis corresponds different lstm units. horizontal axis time. model trained predict next frames predict next frames. dynamics periodic quality out. bottom pattern activity trained weights future predictor replaced random weights. dynamics quickly dies out. trained layer composite model hidden units conditioning either decoders. model trained percepts extracted hours youtube data. model trained autoencode frames predict next frames. initialize lstm classiﬁer weights learned encoder lstm model. classiﬁer shown fig. output lstm second layer goes softmax classiﬁer makes prediction action performed time step. since action performed video datasets consider target time step. test time predictions made time step averaged. prediction entire video average predictions frame blocks video stride frames. using smaller stride improve results. baseline comparing models identical lstm classiﬁer randomly initialized weights. classiﬁers used dropout regularization dropped activations communicated across layers time within lstm proposed zaremba emphasize strong baseline signiﬁcantly better using single frames. using dropout crucial order train good baseline models especially training examples. figure input features composite model trained moving mnist digits. lstm input frame connected four sets units input input gate forget gate output gate. ﬁgures show top- features ordered norm input features. features corresponding locations belong lstm unit. fig. compares three models single frame classiﬁer baseline lstm classiﬁer lstm classiﬁer initialized weights composite model number labelled videos class varied. note labelled video means many labelled frame blocks. case training examples unsupervised learning gives substantial improvement. example ucf- performance improves training labelled video. size labelled dataset grows improvement becomes smaller. even full ucf- dataset still considerable improvement hmdb- improvement full dataset video class. although improvement classiﬁcation using unsupervised learning expected still managed yield additional improvement strong baseline. discuss avenues improvements later. similar experiments optical percepts extracted ucf- dataset. temporal stream convolutional similar proposed simonyan zisserman trained single frame optical ﬂows well stacks optical ﬂows. gave accuracy respectively. again models took frames input reconstructed predicted frames future. lstms hidden units improved accuracy single frame case. bigger lstms improve results. pretraining lstm able improve classiﬁcation stacks frames improved slightly results summarized table experiments compare different variants model proposed paper. since always possible lower reconstruction error copying inputs cannot input reconstruction error measure good model doing. however error predicting future reasonable measure good model doing. besides performance supervised tasks proxy good unsupervised model doing. section future prediction results summarized table mnist compute cross entropy predictions respect ground truth patches. natural image patches compute squared loss. composite model always better predicting future compared future predictor. indicates autoencoder along future predictor force model remember inputs actually helps predict future better. next compare model conditional variant. here conditional models perform better also noted fig. next compare models using performance supervised task. table shows performance action recognition achieved ﬁnetuning different unsupervised learning models. besides running experiments full ucf- hmdb- datasets also experiments small subsets better highlight case training examples. unsupervised models improve baseline lstm well-regularized using dropout. autoencoder model seems perform consistently better future predictor. composite model combines better either alone. conditioning generated inputs seem give clear advantage composite model conditional future predictor works best although performance almost composite model. finally compare models state-of-the-art action recognition results. performance summarized table table divided three sets. ﬁrst compares models data second compares models explicitly computed features only. models third both. method baseline lstm autoencoder future predictor conditional autoencoder conditional future predictor composite model composite model conditional future predictor models. performs better lrcn model also used lstms convnet features. model performs better features convolutional net. however features concatenated percepts slightly better model. improvement features using randomly initialized lstm network quite small. believe atleast partly fact percepts already capture motion information lstm would otherwise discover. combine predictions models obtain accuracy ucf-. believe improvements made running model different patch locations mirroring patches. also model applied deeper inside convnet instead top-level. potentially lead improvements. paper focus showing unsupervised training helps consistently across datasets across different sized training sets. table comparison state-of-the-art action recognition models. conclusions proposed models based lstms learn good video representations. compared analyzed properties visualizations. moreover managed improvement supervised tasks. best performing model composite model combined autoencoder future predictor. conditioning generated outputs signiﬁcant impact performance supervised tasks however made future predictions look slightly better. model able persistently generate motion well beyond time scales trained for. however lost precise object features rapidly training time scale. features input output layers found interesting properties. improvements supervised tasks believe model extended applying convolutionally across patches video stacking multiple layers models. applying model lower layers convolutional could help extract motion information would otherwise lost across max-pooling layers. future work plan build models based autoencoders bottom instead applying percepts. acknowledgments acknowledge support samsung raytheon technologies nvidia corporation donation used research. authors would like thank geoffrey hinton ilya sutskever helpful discussions comments. references kyunghyun merrienboer bart g¨ulc¸ehre aglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using proencoder-decoder statistical machine translation. ceedings conference empirical methods natural language processing emnlp donahue jeff hendricks lisa anne guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate long-term recurrent convolutional darrell trevor. corr networks visual recognition description. abs/. graves alex jaitly navdeep. towards end-to-end speech proceedings recognition recurrent neural networks. international conference machine learning karpathy andrej toderici george shetty sanketh leung thomas sukthankar rahul fei-fei large-scale video cvpr classiﬁcation convolutional neural networks. memisevic roland hinton geoffrey learning represent spatial transformations factored higher-order boltzmann machines. neural computation june michalski vincent memisevic roland konda kishore. modeling deep temporal dependencies recurrent grammar cells. advances neural information processing systems curran associates inc. ranzato marc’aurelio szlam arthur bruna joan mathieu micha¨el collobert ronan chopra sumit. video modeling baseline generative models natural videos. corr abs/. susskind memisevic hinton pollefeys modeling joint density images variety transformations. proceedings ieee conference computer vision pattern recognition hateren ruderman independent component analysis natural image sequences yields spatio-temporal ﬁlters similar simple cells primary visual cortex. proceedings. biological sciences royal society", "year": 2015}