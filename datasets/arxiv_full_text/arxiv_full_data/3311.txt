{"title": "Large-scale subspace clustering using sketching and validation", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The nowadays massive amounts of generated and communicated data present major challenges in their processing. While capable of successfully classifying nonlinearly separable objects in various settings, subspace clustering (SC) methods incur prohibitively high computational complexity when processing large-scale data. Inspired by the random sampling and consensus (RANSAC) approach to robust regression, the present paper introduces a randomized scheme for SC, termed sketching and validation (SkeVa-)SC, tailored for large-scale data. At the heart of SkeVa-SC lies a randomized scheme for approximating the underlying probability density function of the observed data by kernel smoothing arguments. Sparsity in data representations is also exploited to reduce the computational burden of SC, while achieving high clustering accuracy. Performance analysis as well as extensive numerical tests on synthetic and real data corroborate the potential of SkeVa-SC and its competitive performance relative to state-of-the-art scalable SC approaches. Keywords: Subspace clustering, big data, kernel smoothing, randomization, sketching, validation, sparsity.", "text": "nowadays massive amounts generated communicated data present major challenges processing. capable successfully classifying nonlinearly separable objects various settings subspace clustering methods incur prohibitively high computational complexity processing large-scale data. inspired random sampling consensus approach robust regression present paper introduces randomized scheme termed sketching validation tailored large-scale data. heart skeva-sc lies randomized scheme approximating underlying probability density function observed data kernel smoothing arguments. sparsity data representations also exploited reduce computational burden achieving high clustering accuracy. performance analysis well extensive numerical tests synthetic real data corroborate potential skeva-sc competitive performance relative state-of-theart scalable approaches. turn decade trademarked society computing research data deluge number smart internet-capable devices increases amount data generated collected. desirable mine information data sheer amount dimensionality introduces numerous challenges processing pattern analysis since available statistical inference machine learning approaches necessarily scale well number data dimensionality. addition cost cloud computing rapidly declining need redesigning traditional approaches take advantage ﬂexibility emerged distributing required computations multiple nodes well reducing per-node computational burden. digital technology center university minnesota minneapolis usa. konstantinos slavakis dept. electrical engineering university buﬀalo state university york buﬀalo usa. work supported grants eager afosr grant muri-fa--. e-mails tragaumn.edu kslavakibuﬀalo.edu georgiosumn.edu successful clustering algorithms simplicity however k-means well kernel-based variants provides meaningful clustering results data mapped appropriate feature space form tight groups separated hyperplanes subspace clustering popular method group non-linearly separable data generated union subspaces high-dimensional euclidean space well-documented impact various applications diverse image video segmentation identiﬁcation switching linear systems controls recent advances advocate algorithms high clustering performance price high computational complexity goal paper introduce randomized framework reducing computational burden algorithms number available data becomes prohibitively large maintaining high levels clustering accuracy. starting point sketching validation approach skeva oﬀers low-computational complexity randomized scheme multimodal probability density function estimation since draws random computationally aﬀordable sub-populations data obtain crude sketch underlying massive data population. validation step based divergences pdfs follows assess quality crude sketch. sketching validation phases repeated independently preﬁxed number times draw achieving best score ﬁnally utilized cluster whole data population. skeva inspired random sampling consensus method although ransac’s principal ﬁeld application parametric regression problem presence outliers also employed achieve goal devising scheme computational complexity footprint present paper broadens scope skeva context. moreover support stateof-the-art performance skeva real-data contribution provides rigorous performance analysis lower bound number independent draws skeva identify draw represents well underlying data high probability. analysis facilitated non-parametric estimation framework kernel smoothing models underlying data mixture gaussians known oﬀer universal approximations assess proposed skeva-sc extensive numerical tests synthetic real-data presented underline competitive performance skeva-sc relative state-of-the-art scalable approaches. rest paper organized follows. section provides preliminaries along notation tools kernel density estimation. section introduces proposed algorithm large-scale section provides performance bounds skeva-sc. section presents numerical tests conducted evaluate performance skeva-sc comparison state-ofthe-art algorithms. finally concluding remarks future research directions given section notation unless otherwise noted lowercase bold letters denote vectors uppercase bold letters represent matrices calligraphic uppercase letters stand sets. entry matrix denoted stands d-dimensional real euclidean space positive real numbers expectation l-norm. dimensionality matrix whose columns form basis low-dimensional representation respect centroid intercept denotes noise vector capturing unmodeled eﬀects. linear using described datum lies subspace belong multiple clusters latter case thought probability datum belongs given data matrix rd×n number subspaces involves ﬁnding data-to-subspace assignment vectors {πi}n dimensions {dk}k well centroids {mk}k unknown. bases subspaces recovered using singular value decomposition data associated subspace. indeed given basis obtained ﬁrst singular vectors given assignment matrix recovered case hard clustering ﬁnding closest subspace kernel smoothing kernel density estimation non-parametric approach estimating pdfs. kernel density estimators similar histogram unlike parametric estimators make minimal assumptions unknown employ general kernel functions rather rectangular bins kernel smoothers ﬂexible attain faster convergence rates histogram estimator number observed data tends inﬁnity however convergence rate denotes pre-deﬁned kernel function centered positive-deﬁnite bandwidth matrix rd×d. bandwidth matrix controls amount smoothing across dimensions. typically chosen density also pdf. role bandwidth understood clearly corresponds covariance matrix besides k-subspaces solver outlined sec. various algorithms developed machine learning data-mining community solve probabilistic counterpart k-subspaces mixture probabilistic assumes data drawn mixture degenerate gaussians. building assumption agglomerative lossy compression utilizes ideas rate-distortion theory minimize required number bits encode cluster certain distortion level. algebraic schemes costeira-kanade algorithm generalized tackle linear algebra point view generally performance guaranteed independent noise-less subspaces. methods recover subspaces ﬁnding local linear subspace approximations also online clustering approaches handling streaming data. eigenvectors indicator vectors connected components. sake completeness alg. summarizes procedure trailing eigenvectors used obtain cluster assignments spectral clustering. column sparse contains coeﬃcients repreij=ij. matrix used create aﬃnity matrix |ij| |ji|. finally spectral clustering e.g. found taking sample means cluster {uk}k future summarized alg. low-rank representation algorithm similar replaces ℓ-norm stands rank singular value high clustering accuracy achieved comes price high complexity. solving scales quadratically number data performing spectral clustering across clusters renders computationally prohibitive current state-of-the-art approach scalable sparse subspace clustering involves drawing randomly data performing them expressing rest data according clusters identiﬁed random draw samples. sssc clearly reduces complexity performance potentially suﬀer random sample representative entire regarding kernel smoothing available algorithms address important issue bandwidth selection achieve desirable convergence rate properties approximation unknown present paper however pioneers framework randomly choose subset kernel functions yielding small error estimating multi-modal pdf. skeva-sc draws prescribed number rmax realizations phases performed sketching validation phase. structure philosophy skeva-sc based notion divergences pdfs. realization sketching stage proceeds follows sub-population data randomly drawn estimated based sample using kernel smoothing. clusters assumed suﬃciently separated expected multimodal. conﬁrm this compared unimodal discrepancy. pre-selected threshold skeva-sc proceeds validation stage; otherwise skeva-sc deems draw uninformative advances next realization without performing validation step. validation stage skeva-sc another random sample data diﬀerent drawn forming rd×n′ purpose stage evaluate well represents whole dataset. estimated compared using score assigned using non-increasing scoring function grows realizations come closer. finally rmax realizations data received highest score maxr selected performed denotes column estimated generally unknown prudent choice bandwidth matrix provides isotropic smoothing across dimensions greatly simpliﬁes analysis. since ﬁnding gaussian dissimilarities d-dimensional data incurs complexity complexity alg. iteration gaussian kernel utilized. algorithm involves kernel smoothing choice bandwidth matrices aﬀects critically performance alg. hold. class bregman divergences generalizations squared euclidean distance include kullback-leibler well itakura-saito among others. furthermore generalized symmetric bregman divergences jensen-bregman satisfy triangle inequality although dise distance since satisfy triangle denotes estimate data stands reference fig. depicts points statistical manifold namely space probability distributions. letting triangle inequality suggests crux alg. rmax number independent random draws identify subset data represents well whole data population. since skeva-sc aims large-scale clustering natural whether rmax iterations suﬃce mine subset data whose approximates well unknown therefore crucial prior implementation skeva-sc estimate minimum number rmax ensures informative draw high probability. concerns taken account sssc single draw performed prior applying section provides analysis establish lower-bound rmax underlying obeys gaussian mixture model universal approximation properties generic assumption also employed mixture probabilistic well {st} denotes event statement true. naturally complement holds true deemed good estimate underlying given triangle inequality dictates implies ﬁxed smaller larger becomes. arbitrarily ﬁxed implies i.e. occurs here chosen unimodal gaussian centered around contrast multimodal nature true number well-separated clusters. according estimate close unimodal give rise event remark section centered sample mean entire dataset alg. realization centered around sample mean sampled data. avoid step incurs complexity skeva-sc. dataset mean available unimodal iteration replaced induced mean entire dataset respectively. using thm. results shown fig. intuitively pleasing number sampled points increases required number random draws decreases draw required. dataset fig. depicts accuracy alg. using k-means clustering instead rmax increases. here number sampled data ﬁxed number data validation phase alg. compared simple scheme taking single random draw data performing k-means. vertical line fig. indicates value overestimate provided thm. figure accuracy versus rmax number sampled points data used fig. alg. compared simple scheme taking single random draw data followed clustering. vertical line represents value given theorem averaging takes place across iterations. speciﬁcally replaced denotes running sample average realization moreover since number data used validation phase larger equal number data drawn sketching phase alg. provides potentially better estimates does. result distance employed surrogate further √dise using well replacing ensemble sample averages prescribed absolute minimum number iterations. fig. shows values iterations alg. dataset used figs. approaches theoretical value given thm. values distances averaged across iterations. figure estimates versus iteration index dataset clusters generated cluster means variances number points cluster horizontal line represents value given theorem random variable taking values probabilities nk/n datum belong cluster number data cluster value provided algorithms tested. quantity random variable identical probabilities derived ground-truth labels mutual information random variables figs. respectively. subspaces dimensions number data subspace proportional subspace dimension data subspace generated according randomly generated subspace angle sisj given least projections xi’s onto subspaces also denotes uniform distribution. finally awgn variance added data number points validation stage alg. maximum number iterations rmax number sampled data increases clustering accuracy well normalized mutual information cluster assignments ground truth ones. metrics proposed method larger sssc corroborating fact single random sketch representative data ensure reliable clustering. additionally required processing times sssc skeva-sc comparable. real datasets tested pendigits extended yale face pokerhand databases. pendigits dataset includes data dimension separated clusters datum representing handwritten digit. clusters group digits cluster contains data. results pendigits dataset shown fig. /)id rmax similar synthetic tests number data increases accuracy alg. sssc alg. showing higher accuracy levels cost higher computational time. accuracy diﬀerence sssc alg. pronounced synthetic datasets possibly pendigits dataset uniform clusters number data. dimension dimensionality data reduced using extracting important features alg. sssc tested dimensionality reduced normalized data. fig. shows results dataset /)id rmax alg. exhibits higher accuracy one-shot random sampling counterpart sssc basically comparable clustering time. furthermore figs. show accuracy time results yale face database maximum number iterations estimated on-the-ﬂy described section using replacing ensemble averages sample averages across iterations. fig. shows values rmax versus number sampled data. /)id similarly prescribed rmax case alg. exhibits higher clustering accuracy requiring comparable running time sssc. pokerhand database contains data belonging clusters. datum -card hand drawn deck cards card described suit rank. cluster represents valid poker hand. fig. compares skeva-sc sssc dataset selecting /)id rmax similar previous datasets alg. enjoys higher accuracy sssc retaining clustering time corroborating fact alg. handle large datasets. however suprisingly levels present paper introduced novel iterative data-reduction scheme skeva-sc enables grouping data drawn union subspaces based random sketching validation approach fast yet-accurate subspace clustering part proposed algorithm builds sparse algorithm also utilize algorithm. analytical bounds derived number required iterations performance algorithm evaluated synthetic real datasets. future research directions focus development online skeva-sc able handle also fast-streaming data. formed mixing coeﬃcients denotes determinant distances random variables since depend turn depends randomly drawn data expectation w.r.t. true data expressed closed form. data drawn independently iteration", "year": 2015}