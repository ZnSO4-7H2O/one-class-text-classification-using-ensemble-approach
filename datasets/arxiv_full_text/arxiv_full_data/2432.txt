{"title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and  Optimization", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Integration is affected by the curse of dimensionality and quickly becomes intractable as the dimensionality of the problem grows. We propose a randomized algorithm that, with high probability, gives a constant-factor approximation of a general discrete integral defined over an exponentially large set. This algorithm relies on solving only a small number of instances of a discrete combinatorial optimization problem subject to randomly generated parity constraints used as a hash function. As an application, we demonstrate that with a small number of MAP queries we can efficiently approximate the partition function of discrete graphical models, which can in turn be used, for instance, for marginal computation or model selection.", "text": "integration aﬀected curse dimensionality quickly becomes intractable dimensionality problem grows. propose randomized algorithm that high probability gives constant-factor approximation general discrete integral deﬁned exponentially large set. algorithm relies solving small number instances discrete combinatorial optimization problem subject randomly generated parity constraints used hash function. application demonstrate small number queries eﬃciently approximate partition function discrete graphical models turn used instance marginal computation model selection. computing integrals high dimensional spaces fundamental largely unsolved problem scientiﬁc computation numerous applications ranging machine learning statistics biology physics. volume grows exponentially dimensionality problem quickly becomes computationally intractable phenomenon traditionally known curse dimensionality revisit problem approximately computing discrete integrals namely weighted sums sets items. problem encompasses several important probabilistic inference tasks computing marginals normalization constants graphical models turn cornerstones parameter structure learning although focus discrete case continuous case principle also addressed approximated numerical integration. common approaches approximate large discrete sums sampling variational methods. variational methods often inspired statistical physics fast provide guarantees quality results. since sampling counting reduced approximate techniques based sampling quite popular suﬀer similar issues number samples required obtain statistically reliable estimate often grows exponentially problem size. among sampling techniques markov chain monte carlo methods asymptotically accurate guarantees practical applications exist limited number cases therefore often used heuristic manner. practice performance crucially depends choice proposal distributions often must domain-speciﬁc expert-designed approximately correct estimate general weighted sums deﬁned exponentially large sets items possible variable assignments discrete probabilistic graphical model. computational complexity perspective counting problem consider complete complexity class problems encapsulating entire polynomial hierarchy believed signiﬁcantly harder idea reduce problem small number instances combinatorial optimization problem deﬁned space subject randomly generated parity constraints. rationale behind approach although combinatorial optimization intractable worst case witnessed great success past years ﬁelds mixed integer programming propositional satisﬁability testing problems computing maximum posteriori assignment although np-hard practice often approximated solved exactly fairly eﬃciently fact modern solvers exploit structure real-world problems prune large portions search space often dramatically reducing runtime. contrast counting problem computing marginal probability needs consider contributions exponentially large number items. algorithm called weighted-integrals-and-sums-by-hashing relies randomized hashing techniques evenly high dimensional space. hashing introduced valiant vazirani study relationship number solutions hardness combinatorial search. techniques also applied gomes obtain bounds number solutions problem. work general handle general weighted sums ones arising probabilistic inference graphical models. work also closely related recent work hazan jaakkola obtain lower bound partition function taking suitable expectations combination queries randomly perturbed models. improve upon crucial aspects namely estimate constant factor approximation true partition function provide concentration result showing bounds hold expectation high probability polynomial number queries. note consistent known complexity results regarding bppnp; remark below. demonstrate practical eﬃcacy wish algorithm context computing partition function random clique-structured ising models grid ising models known ground truth challenging combinatorial application completely reach techniques mean field belief propagation. also consider model selection problem graphical models speciﬁcally context hand-written digit recognition. show anytime highly parallelizable algorithm handle problems level accuracy scale well beyond current state art. assume given input compactly represented instance factored form product conditional probabilities tables. note however results general rely factored representation. inference graphical models consider graphical model speciﬁed factor graph discrete random variables global random vector takes value cartesian product consider probability distribution factors potentials factors {x}α conﬁgurations) index {x}α subset variables factor depends given graphical model possible conﬁgurations deﬁne weight function assigns conﬁguration score computing typically intractable involves exponential number conﬁgurations often challenging inference task many families graphical models. computing however needed many inference learning tasks evaluating likelihood data given model computing marginal probabilities parameter estimation context graphical models inference assume access optimization oracle answer maximum posteriori queries namely solve following constrained optimization problem corresponding element volume. case require compact representation access oracle able optimize discretized function subject arbitrary constraints. e.g. figure simplicity following restrict binary case i.e. general multinomial case x×x×···×xn transformed former case using binary representation requiring |xi| bits dimension deﬁnition family functions pairwise independent following conditions hold function chosen uniformly random random variable uniformly distributed random variables independent. simple construct function think family possible functions family pairwise independent fully independent functions. however function requires bits represented thus impractical typical case large. hand pairwise independent hash functions constructed represented much compact follows; appendix proof. random matrix therefore ﬁnite dimensional vector space operations deﬁned ﬁeld refer constraints form parity constraints rewritten terms xors operations ainxn figure visualization thinning eﬀect random parity constraints adding parity constraints. leftmost plot shows original function integrate. constrained optimal solution red. tail distribution weights u}|. note non-increasing step i.e. total area curve. approximate divide area either horizontal vertical slices approximate area slice suppose eﬃcient procedure estimate given hard could create enough slices dividing x-axis estimate points estimate area using quadrature. however natural degree accuracy would require number slices grows least logarithmically weight range x-axis undesirable. alternatively could split y-axis i.e. value range geometrically growing values i.e. bins sizes weights conﬁgurations split points. words i-th quantile weight distribution. unfortunately despite monotonicity area horizontal slice deﬁned diﬃcult bound could arbitrarily other. however area vertical slice deﬁned must bounded i.e. within factor thus summing lower bound slices i−bi. course don’t know could approximate within factor functions optimization oracle approximate values high probability. note method allows compute partition function estimating weights carefully chosen points only optimization problem. insight compute values follows. suppose apply conﬁgurations randomly sampled pairwise independent hash function buckets optimization oracle compute weight heaviest conﬁguration ﬁxed bucket. repeat process times consistently infer properties hashing least conﬁgurations likely weight least token fact least conﬁgurations heavier weight good chance optimization oracle would underestimate weight m-th heaviest conﬁguration. shortly process using pairwise independent hash functions keep variance allows estimate accurately samples. pseudocode wish shown algorithm parameterized weight function dimensionality correctness parameter constant notice algorithm requires solving optimization instances compute deﬁned items. following section formally prove output constant factor approximation probability least figure shows working algorithm. random parity constraints added outer loop algorithm conﬁguration space thinned optimization oracle selects heaviest surviving conﬁgurations. ﬁnal output weighted median modes obtained level. remark parity constraints change worst-case complexity np-hard optimization problem. result thus consistent fact approximated bppnp approximately count number solutions randomized algorithm polynomial number queries oracle remark although parity constraints impose simple linear equations ﬁeld make optimization harder. instance ﬁnding conﬁguration smallest hamming weight satisfying parity constraints known np-hard i.e. equivalent computing minimum distance parity code hand density parity check codes solved extremely fast practice using heuristic methods message remark optimization instances solved independently allowing natural massive parallelization. also discuss algorithm used anytime fashion implications obtaining suboptimal solutions. know true values lemma shows values computed algorithm suﬃciently close high probability. recall median values computed adding random parity constraints repeating process times. speciﬁcally follows lemma estimating tail distribution also estimate entire tail distribution weights deﬁned u}|. theorem deﬁned algorithm maximum {··· then probability -approximation interesting result right goal estimate total weight scheme section requiring total queries eﬃcient ﬁrst estimating tail distribution several values given κ-approximation algorithm algorithm design approximation algorithm following construction. log+\u0001 deﬁne conﬁgurations weight function ww··· note construction requires running algorithm enlarged problem times variables. although number optimization queries grows polynomially increasing number variables might signiﬁcantly increase runtime. output always approximate lower bound even optimization stopped early. lower bound monotonically non-decreasing time guaranteed eventually reach within constant factor thus anytime algorithm. implemented wish using open source solver toulbar solve inference problem. toulbar complete solver winning algorithms uai- inference competition. augmented toulbar ilog cplex optimizer based techniques borrowed gomes eﬃciently handle random parity constraints. speciﬁcally equations linear equations ﬁeld thus allow eﬃcient propagation domain ﬁltering using gaussian elimination. experiments wish parallel using compute cluster cores. assign optimization instance inner loop core ﬁnally process results optimization instances solved reached timeout. comparison consider tree reweighted belief propagation provides upper bound mean field provides lower bound loopy belief propagation provides estimate guarantees. implementations algorithms available libdai library ground truth around estimate predicted theorem remains accurate visually overlapping plot. actual estimation error much smaller worst-case factor guaranteed theorem practice overunder-estimation errors tend cancel out. don’t ground truth methods fall well outside provable interval provided wish reported error small compared magnitude errors made methods. within timeout hours resulting high conﬁdence tight approximations partition function. aware practical method provide guarantees counting problems size i.e. weighted deﬁned items. ground truth computed using junction tree method experimental setup hazan jaakkola also random queries derive bounds partition function. speciﬁcally binary figure reports estimation error log-partition function using timeout minutes. wish provides accurate estimates wide range weights often improving methods. slight performance drop wish coupling strengths appears occur weight range terms corresponding parity constraints mi+i. empirically optimization instances roughly parity constraints often hardest solve resulting possibly signiﬁcant underestimation value timeout occurs. directly compare work hazan jaakkola access code. however visual look plots suggests wish would provide improvement accuracy although longer runtime. computer enumeration clever exploitation properties symmetry group here attempt approximately compute number using general-purpose scheme wish. replace toulbar cryptominisat solver designed unweighted cryptographic problems natively supports parity constraints. observed wish consistently solutions adding random parity constraints constraints cannot applied timeouts larger values estimate clearly close known true count. contrast simple local reasoning done variational methods powerful enough even single solution. mean field belief propagation report many inference learning tasks require computing normalization constant graphical models. instance needed evaluate likelihood observed data given model. necessary model selection i.e. rank candidate models trigger early stopping training likelihood validation starts decrease order avoid overﬁtting figure depicts confabulations three learned models. evaluate loglikelihood data determine model best needs compute wish estimate quantity timeout minutes rank models according average loglikelihood data. scores obtain case toulbar able prove optimality instances theorem applies results. although ground truth seen ranking models consistent visually appears closer large collection hand-written digits figure note clearly good representative highly uneven distribution digit occurrences. ranking wish also consistent fact using gibbs sampling steps inference phase provide better gradient estimates introduced wish randomized algorithm that high probability gives constant-factor approximation general discrete integral deﬁned exponentially large set. wish reduces intractable counting problem small number instances combinatorial optimization problem subject parity constraints used hash function. context graphical models showed approximately compute normalization constant partition function using small number queries. using state-of-the-art combinatorial optimization tools thus able provide discrete integral partition function estimates approximation guarantees scale could till handled heuristically. finally method massively parallelizable anytime algorithm also stopped early obtain empirically accurate estimates provide lower bounds high probability.", "year": 2013}