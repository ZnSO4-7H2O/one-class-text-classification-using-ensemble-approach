{"title": "Structured learning of sum-of-submodular higher order energy functions", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Submodular functions can be exactly minimized in polynomial time, and the special case that graph cuts solve with max flow \\cite{KZ:PAMI04} has had significant impact in computer vision \\cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we address the important class of sum-of-submodular (SoS) functions \\cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via a variant of max flow called submodular flow \\cite{Edmonds:ADM77}. SoS functions can naturally express higher order priors involving, e.g., local image patches; however, it is difficult to fully exploit their expressive power because they have so many parameters. Rather than trying to formulate existing higher order priors as an SoS function, we take a discriminative learning approach, effectively searching the space of SoS functions for a higher order prior that performs well on our training set. We adopt a structural SVM approach \\cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the training problem in terms of quadratic programming; as a result we can efficiently search the space of SoS priors via an extended cutting-plane algorithm. We also show how the state-of-the-art max flow method for vision problems \\cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flow problem. Experimental comparisons are made against the OpenCV implementation of the GrabCut interactive segmentation technique \\cite{Rother:GrabCut04}, which uses hand-tuned parameters instead of machine learning. On a standard dataset \\cite{Gulshan:CVPR10} our method learns higher order priors with hundreds of parameter values, and produces significantly better segmentations. While our focus is on binary labeling problems, we show that our techniques can be naturally generalized to handle more than two labels.", "text": "paper focus important generalization functions graph cuts minimize express higher-order priors. functions called sum-of-submodular efﬁciently solved variant functions expressive power also involve large number parameters. rather addressing question existing higher order priors expressed function take discriminative learning approach effectively search space functions goal ﬁnding higher order prior gives strong results training set. main contribution introduce ﬁrst learning method training functions demonstrate effectiveness approach interactive segmentation using learned higher order priors. following structural approach show training problem cast quadratic optimization problem extended linear constraints. generalizes large-margin training pairwise submodular mrfs submodularity corresponds simple non-negativity constraint. solve training problem show extended cutting-plane algorithm efﬁciently search space functions. sum-of-submodular functions priors course submodular functions submodular could general submodular optimization minimize function. however general submodular optimization signiﬁcant impact computer vision paper address important class sum-of-submodular functions efﬁciently minimized variant called submodular functions naturally express higher order priors involving e.g. local image patches; however difﬁcult fully exploit expressive power many parameters. rather trying formulate existing higher order priors function take discriminative learning approach effectively searching space functions higher order prior performs well training set. adopt structural approach formulate training problem terms quadratic programming; result efﬁciently search space priors extended cutting-plane algorithm. also show state-of-the-art method vision problems modiﬁed efﬁciently solve submodular problem. experimental comparisons made opencv implementation grabcut interactive segmentation technique uses hand-tuned parameters instead machine learning. standard dataset method learns higher order priors hundreds parameter values produces signiﬁcantly better segmentations. focus binary labeling problems show techniques naturally generalized handle labels. discrete optimization methods graph cuts proven quite effective many computer vision problems including stereo interactive segmentation texture synthesis underlying optimization problem behind graph cuts special case submodular function optimization solved exactly using graph methods however limited reliance ﬁrst-order priors involving pairs pixels considerable interest expressing priors low-level vision problems) whereas able exploit neighborhood structure better. example cliques pairs energy function referred regular problem reduced mentioned above underlying technique used popular graph cuts approach limitation restriction pairwise cliques allow naturally express important higher order priors involving image patches common approach solving higher-order priors graph cuts involves transformation pairwise cliques practice almost always produces non-submodular functions cannot solved exactly many learning problems computer vision cast structured output prediction allows learning outputs spatial coherence. among popular generic methods structured output learning conditional random fields trained maximum conditional likelihood maximum-margin markov networks structural support vector machines advantage svmstruct crfs training require computation partition function. among large-margin approaches svm-struct follow svmstruct methodology since allows efﬁcient inference procedures training. paper learn submodular discriminant functions. prior work learning submodular functions falls three categories submodular function regression maximization submodular discriminant functions minimization submodular discriminant functions. learning submodular discriminant functions prediction computed maximization widespread information retrieval submodularity models diversity ranking search engine automatically generated abstract exact submodular maximization intractible approximate inference using simple greedy algorithm approximation guarantees generally excellent performance practice. models considered paper submodular discriminant functions prediction computed minimization. popular models regular mrfs traditionally parameters models tuned hand several learning methods exist. closely related work paper associative markov networks take approach exploit fact regular mrfs integral linear relaxation. linear programs folded quadratic program solved monolithic contrast svm-struct training using cutting planes regular mrfs allows graph inference also training show approach interesting approximation properties even multi-class case graph inference approximate. complex models learning spatially coherent priors include separate training unary pairwise potentials learning mrfs functional gradient boosting potts models success variety vision problems. note general approach learning multi-label functions described section includes potts model special case. submodular similar problem network nodes arcs want push however notion residual capacity slightly modiﬁed standard ﬂow. much complete description begin network reduction graph cuts source sink arcs every additionally clique every also associated residual capacity residual capacity arcs familiar residual capacities capacities whenever push source sink decrease residual capacity amount. interior arcs need piece information. addition residual capacities also keep track residual clique functions related values following rule whenever push units update incremental breadth first search state methods vision applications improves algorithm guarantee polynomial time complexity. show modify ibfs compute maximum submodular ibfs augmenting paths algorithm step ﬁnds path positive residual capacity pushes along additionally augmenting path found shortest path ensure paths found shortest paths keep track distances search trees containing nodes distance respectively. invariants maintained algorithm proceeds alternating forward passes reverse passes. forward pass attempt grow source tree layer grow scan vertices distance away examine out-arc positive residual capacity. distance level parent found augmenting path push operation pushing saturate arcs parent tree becomes saturated becomes orphan. augmentation perform adoption step orphan ﬁnds parent. details adoption step similar relabel operation push-relabel algorithm search potential parent arcs neighbor lowest distance label make node parent. order apply ibfs submodular problem basic datastructures still make sense graph arcs residual capacities maximum found longer augmenting path main change submodular problem increase edge instead affecting residual capacity reverse also change residual capacities arcs however following result ensures problem. corollary ensures never create shorter paths contained push operation cause edges become saturated problem normal case orpans created ﬁxed adoption step. therefore invariants ibfs algorithm maintained even submodular case. ﬁnal property ibfs algorithm involves current heuristic mechanism avoiding iterating possible potential parents performing adoption step. case submodular flows also case whenever create residual arcs maintain invariants related current heuristic speedup applies here. however affect correctness algorithm runtime defer proof supplementary material. running time. asymptotic complexity standard ibfs algorithm submodular-ﬂow case still perform number basic operations. however note ﬁnding residual capacity requires minimizing separating done time using however likely much efﬁcient naive algorithm searching values overall work basic step ibfs cliques total runtime moment assume prediction task assign binary label element base cover multi-label case section since labels binary prediction consists assigning subset input goal construct feature vector that used svm-struct algorithm section allow learn sum-of-submodular energy functions. let’s begin simplest case learning discriminant function deﬁned single clique depend input intuitively parameters correspond table values clique function feature vector chosen accomplish letting entries indexed subsets deﬁning otherwise). note that claimed parameters allowed vary r|c| arbitrary function necessarily submodular. however enforce submodularity adding number linear inequalities. recall submodular therefore submodular parameters satisfy theorem choosing feature vector adding linear constraints quadratic program learned discriminant function maximum margin function allowed vary possible submodular functions proof. adding constraints ensure optimal solution deﬁnes submodular conversely submodular function feasible deﬁned optimal solution must maximum-margin function. structured output prediction describes problem learning function space inputs space outputs given problem. learn assume training sample input-output pairs available drawn unknown distribution. goal i.i.d. function hypothesis space prediction error relative loss function function quantiﬁes error associated predicting correct output value. example image segmentation natural loss function might hamming distance true segmentation predicted labeling. mechanism structural svms ﬁnds hypothesis learn discriminant function input/output pairs. derives prediction given input minimizing write argminy∈y assume linear quantities parameter vector feature vector relating input output intuitively think cost function measures poorly output matches given input ideally would weights hypothesis always gives correct results training set. stated another example correct prediction discriminant value incorrect predictions large loss high discriminant values. write constraint linear inequality convenient deﬁne inequality becomes since possible satisfy conditions exactly also slack variable constraints example intuitively slack variable represents maximum misprediction loss example. since want minimize prediction error objective function penalizes large slack. finally also penalize discourage overﬁtting regularization parameter trade costs. -slack formulation equivalent replaces slack variables single variable loss constraints replaced constraints penalizing losses across training examples. also include submodular constraints quadratic program -slack structural tuple exponential sized set. despite large constraints solve desired precision using cutting plane algorithm. algorithm keeps track current constraints solves current regard constraints given solution ﬁnds violated constraint adds finding violated constraint consists solving example problem since features ensure long factors cliques solved submodular ibfs. note also allows arbitrary additional features learning unary potentials well. pseudocode entire ssvm learning given algorithm generalization multi-label prediction submodular functions intrinsically binary functions. order handle multi-label case expansion moves reduce multi-label optimization problem series binary subproblems pixel either switch given label keep current label. every binary subproblem computing optimal expansion move problem call original multi-label energy function expansion energy. label output space clique label deﬁne i.e. subset taking label theorem clique functions form ψdata ψdata negative function corollary feature vector ψdata adding linear constraints learned discriminant function maximum margin function allowed vary possible submodular functions. finally learn multiple clique potentials simultaneously. neighborhood structure cliques data-dependence create feature vector ψsos composed concatenating difc ferent features ψdata n-slack formulation ssvms makes intuitive sense point view minimizing misprediction error training set. however practice better -slack reformulation compared n-slack -slack solved binary denoising dataset consists black white images. image either geometric lines hand-drawn sketch unable obtain original data used created similar data adding independent gaussian noise pixel. denoising hand-tuned generic cuts algorithm posed simple unary pixels equal absolute valued distance noisy input prior clique penalizes square-root number edges different labeled endpoints within clique. single parameter tradeoff unary energy smoothness term. neighborhood structure consists patches image. learned prior includes unary terms clique structure instead square-root smoothness prior learn clique function note clique energy every other analogous graph cuts prior pairwise edge attractive potential. energy function total parameters randomly divided input images training images test images. loss function hamming distance correct un-noisy image predicted image. hand tune value picked value gave minimum pixel-wise error training set. ssvm training took minutes. numerically ssvm performed signﬁcantly better hand-tuned method average pixel-wise error training compared generic cuts. time needed inference training similar methods sec/image ssvm sec/image generic cuts. visually ssvm images signiﬁcantly cleaner looking shown figure interactive segmentation input interactive segmentation color image together sparse foreground/background annotations provided user. figure examples. small labeled foreground background pixels prediction task recover ground-truth segmentation whole image. baseline comparison grabcut algorithm solves pairwise crf. unary terms obtained ﬁtting gaussian mixture model histograms pixels labeled deﬁnitely foreground background. pairwise terms standard contrastsensitive potts potential cost pixels theorem characterizes large class expansion energies. functions generalize commonly used multi-label clique functions including potts model model pays cost pixels equal label λmax otherwise. write expansion energy letting λmax equal potts model additive constant. generalizations robust model encoded similar fashion. finally order learn functions composed copies ψdata corresponding copies constraints ﬁnal note even though individual expansion moves computed optimally α-expansion still global optimum multi-labeled energy. however practice α-expansion ﬁnds good local optima used inference structural good results order evaluate algorithms focused binary denoising interactive segmentation. binary denoising generic cuts provides natural comparison since state-of-the-art method uses priors. interactive segmentation natural comparison grabcut used opencv implementation. general ssvm method learn arbitrary function also considered special case using pairwise priors. denoising segmentation applications signiﬁcantly improve accuracy hand-tuned energy functions. special case algorithm applied pairwise-submodular energy functions solves optimization problem associative markov networks automatically learning parameters allows large number learned unary features crf. result addition smoothness parameter also learn relative weights approximately features describing color values near pixel relative distances nearest labeled foreground/background pixel. details features found supplementary material. refer method ssvm-amn. general ssvm method incorporate higherorder priors instead pairwise ones. addition unary features used ssvm-amn sumof-submodular higher-order crf. patch image learned submodular clique function. obtain beneﬁts contrast-sensitive pairwise potentials higher-order case cluster gradient responses patch clusters learn submodular potential cluster. note ssvm automatically allows learning entire energy function including clique potentials unary potenuse standard interactive segmentation dataset images annotations together pixellevel segmentations provided ground truth. images randomly sorted training validation testing sets size respectively. trained ssvm-amn ssvm training various values regularization parameter picked value gave best accuracy validation report results value test set. overall performance shown table below. training time measured seconds testing time seconds image. implementation used submodular algorithm based ibfs discussed section made freely available license. figure multi-label segmentation result data purple label represents vegetation rhino/hippo blue ground. labels input problem though present output obtain particular image.", "year": 2013}