{"title": "MAttNet: Modular Attention Network for Referring Expression  Comprehension", "tag": ["cs.CV", "cs.AI", "cs.CL"], "abstract": "In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: language-based attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks. Demo and code are provided.", "text": "figure modular attention network given expression attentionally parse three phrase embeddings input three visual modules process described visual region different ways compute individual matching scores. overall score computed weighted combination module scores. proposal/object highest likelihood/probability selected predicted region. however work uses simple concatenation features input single lstm encode/decode whole expression ignoring variance among different types referring expressions. depending distinctive target object different kinds information might mentioned referring expression. example target object ball among black balls referring expression simply ball. ball placed among balls location-based information become important e.g. ball right. balls scene ball’s relationship objects might distinguishing information e.g. ball next cat. therefore natural intuitive think compaper address referring expression comprehension localizing image region described natural language expression. recent work treats expressions single unit propose decompose three modular components related subject appearance location relationship objects. allows ﬂexibly adapt expressions containing different types information end-to-end framework. model call modular attention network types attention utilized languagebased attention learns module weights well word/phrase attention module focus visual attention allows subject relationship modules focus relevant image components. module weights combine scores three modules dynamically output overall score. experiments show mattnet outperforms previous state-of-art methods large margin bounding-box-level pixel-level comprehension tasks. demo code provided. referring expressions natural language utterances indicate particular objects within scene e.g. woman sweater right. robots intelligent agents communicating people world ability accurately comprehend expressions real-world scenarios necessary component natural interactions. referring expression comprehension typically formulated selecting best region proposals/objects {oi}n image given input expression recent work referring expressions uses cnn-lstm based frameworks model uses joint vision-language embedding framework model testmodular networks successfully applied address tasks question answering visual reasoning relationship modeling multi-task reinforcement learning best knowledge present ﬁrst modular network general referring expression comprehension task. moreover previous work typically relies off-the-shelf language parser parse query sentence/question different components dynamically assembles modules model addressing task. however external parser could raise parsing errors propagate model setup adversely effecting performance. therefore paper propose modular network referring expression comprehension modular attention network takes natural language expression input softly decomposes three phrase embeddings. embeddings used trigger three separate visual modules compute matching scores ﬁnally combined overall region score based module weights. model illustrated fig. main novelties mattnet. first mattnet designed general referring expressions. consists modules subject location relationship. referring expression could parsed attributes category name color size absolute location relative location relative object generic attribute. mattnet covers them. subject module handles category name color attributes location module handles absolute relative location relationship module handles subject-object relations. module different structure learns parameters within modular space without affecting others. second mattnet learns parse expressions automatically soft attention based mechanism instead relying external language parser show learned parser attends relevant words module outperforms off-the-shelf parser large margin. additionally model computes module weights adaptive input expression measuring much module contribute overall score. expressions like larger subject module weights smaller location relationship module weights expressions like woman left larger subject location module weights. third apply different visual attention techniques subject relationship modules allow relevant attention described image portions. subject module soft attention attends parts object mentioned expression like shirt yellow hat. call in-box attention. contrast relationship module hard attention used attend relational objects mentioned expressions like chair girl holding frisbee. attention focuses chair frisbee pinpoint target object girl. call out-of-box attention. demonstrate attentions play important roles improving comprehension accuracy. training supervision object proposal referring expression pairs automatically learned end-to-end unsupervised manner including word attention module weights soft spatial attention hard relative object attention. demonstrate mattnet signiﬁcantly superior comprehension performance state-of-art methods achieving improvements bounding-box localization almost doubling precision pixel segmentation. related work referring expression comprehension task referring expression comprehension localize region described given referring expression. address problem recent work uses cnnlstm structure model looks object maximizing probability. recent work uses joint embedding model compute directly. hybrid types approaches proposed joint speaker-listener-reinforcer model combined cnn-lstm embedding model achieve state-of-art results. treat comprehension bounding localization object segmentation referring expression also studied recent work papers fcn-style approaches perform expression-driven foreground/background classiﬁcation. demonstrate addition bounding prediction also outperform previous segmentation results. modular networks neural module networks introduced visual question answering. networks decompose question several components dynamically assemble network compute answer given question. since introduction modular networks applied several tasks visual reasoning question answering relationship modeling multitask reinforcement learning etc. early work requires external language parser decomposition recent methods propose learn decomposition end-to-end. apply idea referring expression comprehension also taking end-toend approach bypassing external parser. different relationship detection phrases always decomposed triplets referring expressions well-posed structure. example expressions like smiling contain language relevant subject module expressions like left relevant subject location modules chair relevant subject relationship modules. handle variance compute module weights expression weighting much module contributes expressionobject score. concatenate ﬁrst last hidden vectors memorizes structure semantics whole expression another fully-connected layer transform module weights previous work evaluates features region proposal/candidate object faster r-cnn backbone faster principled implementation. additionally resnet main feature extractor also provide comparisons previous methods using vggnet features given image candidates faster r-cnn extract region representations. speciﬁcally forward whole image faster r-cnn crop feature following compute feature faster r-cnn typically contains higher-level visual cues category prediction contains relatively lower-level cues including colors shapes proposal judgment making useful purposes. compute matching score given modular phrase embedding i.e. related work decomposes expression triples. however referring expressions much richer forms ﬁxed template. example expressions like left hard model using paper propose generic modular network addressing kinds referring expressions. network adaptive input expression assigning word-level attention module-level weights. model mattnet composed language attention network plus visual subject location relationship modules. given candidate object referring expression ﬁrst language attention network compute soft parse referring expression three components phrase embedding. second three visual modules compute matching scores respective embeddings. finally take weighted combination scores overall matching score measuring compatibility language attention network instead using external language parser pre-deﬁned templates parse expression propose learn attend relevant words automatically module similar language attention network shown fig. given expression {ut}t bi-directional lstm encode context word. ﬁrst embed word vector using one-hot word embedding bidirectional lstm-rnn applied encode whole expression. ﬁnal hidden representation word concatenation hidden vectors directions figure subject module composed visual subject representation phrase-guided embedding. attribute prediction branch added resnet-c stage convolution output attribute prediction used subject visual representation. subject phrase embedding attentively pools spatial region feeds pooled feature matching function. matching function measure similarity bephrase embedding qsubj using matching function qsubj). shown top-right fig. consists mlps normalization layers following input. composed fully connected layers relu activations serving transform visual phrase representation common embedding space. inner-product lnormalized representations computed similarity score. matching function used compute location score relationship score location module shown fig. location frequently used referring expressions expressions refcoco expressions refcocog containing absolute location words e.g. right indicating object location image. tasks. ﬁrst attribute prediction helping produce representation understand appearance characteristics candidate. second phrase-guided attentional pooling focus relevant regions within object bounding boxes. attribute prediction attributes frequently used referring expressions differentiate objects category e.g. woman fuzzy cat. inspired previous work attribute prediction branch subject module. preparing attribute labels training ﬁrst template parser obtain color generic attribute words low-frequency words removed. combine predicting attributes high-level visual cues important. concatenation followed convolution produce attribute feature blob. average pooling attribute representation candidate region. binary cross-entropy loss used multi-attribute classiﬁcation +log] phrase-guided attentional pooling subject description varies depending information salient object. take people example. sometimes person described accessories e.g. girl glasses; sometimes particular clothing items mentioned e.g. woman white pants. thus allow subject module localize relevant regions within bounding in-box attention. compute spatial attention ﬁrst concatenate attribute blob wsubjs wlocs wrels training given positive pair randomly sample negative pairs expression describing object object image calculate combined hinge loss referring expression datasets refcoco refcoco+ refcocog evaluation collected coco images several differences. refcoco refcoco+ collected interactive game interface refcocog collected non-interactive setting thereby producing longer expressions words average respectively. refcoco refcoco+ contain same-type objects respectively. refcoco+ forbids using absolute location words making data focused appearance differentiators. testing refcoco refcoco+ provide person object splits evaluation images containing multiple people testa containing multiple objects categories testb. overlap training validation testing images. refcocog types data partitions. ﬁrst divides dataset randomly partitioning objects training validation splits. testing split released recent work evaluates performance validation set. denote validation split refcocog’s val*. note since data split objects image could appear training validation. second partition composed randomly partitioning images training validation testing splits. denote validation testing splits refcocog’s test experiments split. additionally expressions like middle second left person imply relative positioning among objects category. encode relative location representation candidate object choosing surrounding objects category calculating offsets area ratio i.e. δlij xtl]ij ﬁnal location representation target object subject module deals in-box details target object expressions involve relationship out-of-box objects e.g. chaise lounge. relationship module used address cases. fig. given candidate object ﬁrst look surrounding objects regardless categories. average-pooled feature appearance feature supporting object. then encode offsets candidate object δmij xtl]ij visual representation surrounding object then results referring expression comprehension given test image proposals/objects {oi}n eqn. compute matching score proposal/object given input expression pick highest score. evaluation compute intersection-over-union selected region ground-truth bounding considering correct comprehension. first compare model previous methods using coco’s ground-truth object bounding boxes proposals. results shown table. previous methods used -layer vggnet feature extractor experiments using feature fair comparison. note single -dimensional feature prevents using phrase-guided attentional pooling fig. average pooling subject matching. despite this results still outperform previous state-of-art methods. switching res-based faster r-cnn representation comprehension accuracy improves another note faster r-cnn pre-trained coco’s training images excluding refcoco refcoco+ refcocog’s validation+testing. thus training images seen evaluation. full model phrase-guided second study beneﬁts module mattnet running ablation experiments res-frcn features. baseline concatenation regional visual feature location feature visual representation last hidden output lstm-encoded expression language representation feed matching function obtain similarity score compared this simple two-module mattnet using features already outperforms baseline showing advantage modular learning. line shows beneﬁt encoding location adding relationship module performance improves lines line show beneﬁts brought attribute subbranch phrase-guided attentional pooling subject module. attentional pooling greatly improves person category demonstrating advantage modular attention understanding localized details like girl hat. third tried training model using hard-coded phrases template language parser shown line table. lower end-toend model main reason drop errors made external parser tuned referring expressions. table ablation study mattnet fully-automatic comprehension task using different combination modules. features used res-frcn except last using res-mrcn. fourth show results using automatically detected objects faster r-cnn providing analysis fully automatic comprehension performance. table. shows ablation study fully-automatic mattnet. performance drops detection errors overall improvements brought module consistent table. showing robustness mattnet. results also outperform state-of-art margin. besides show performance using detector branch mask r-cnn line whose results slightly better faster r-cnn. language model able attend right words module even though learned weakly-supervised manner. also observe expressions refcoco refcoco+ describe location details target object frequently refcocog mentions relationship target object surrounding object frequently accords dataset property. note complex expressions like woman plaid jacket blue pants skis contains several relationships language model able attend portion used in-box subject module portion used out-of-box relationship module. additionally subject module also displays reasonable spatial in-box attention qualitatively explains attentional pooling outperforms average pooling comparison incorrect comprehension shown fig. errors sparsity training data ambiguous expressions detection error. segmentation referring expression model also used address referential obinstead using faster rject segmentation backbone turn res-based mask r-cnn apply procedure described sec. detected objects highest matching score prediction. feed predicted bounding mask branch obtain pixel-wise segmentation. evaluate full model mattnet compare best results reported precisionx overall intersectionover-union metrics. results shown table. model outperforming state-of-art results large margin metrics. mattnet features gains proposed model. believe decoupling localization segmentation brings large gain fcn-style foreground/background mask classiﬁcation instance-level segmentation problem end-to-end segmentation system studied future work. referential segmentation examples shown fig. conclusion modular attention network addresses variance referring expressions attending relevant words visual regions modular framework dynamically computing overall matching score. demonstrate model’s effectiveness bounding-box-level pixellevel comprehension signiﬁcantly outperforming state-ofthe-art. acknowledgements research supported awards nvidia google research microsoft research adobe research. optimize model using adam initial learning rate batch size images learning rate halved every iterations ﬁrst -iteration warm-up. word embedding size hidden state size lstm also output mlps within model -dimensional. avoid overﬁtting regularize word-embedding output layers lstm language attention network using dropout ratio also regularize inputs matching function using dropout ratio constrastive pairs ranking loss lrank. besides λattr multi-label attribute cross-entropy loss lattr subj. training full model mattnet converges around iterations takes around half using single titan-x. inference time fully automatic system goes mask r-cnn mattnet takes average seconds forward seconds spent mask r-cnn seconds mattnet. attribute prediction full model also able predict attributes testing. attribute labels extracted using template parser fetch object name color generic attribute words expression low-frequency words removed. frequently used attribute words training. histograms top- attribute words shown fig. quantitative analysis multi-attribute prediction results shown table. show examples comprehension using full model fig. fig. fig. example show input image input expression predicted module weights word attention subject attention top- attributes box-level comprehension pixelwise segmentation comparison also show incorrect comprehension fig. figure examples fully automatic comprehension refcoco. column shows input image. column shows expression word attention module weights. column shows predicted subject attention column shows top- attributes. column shows box-level comprehension dotted boxes show prediction yellow dotted boxes shows relative object green boxes ground-truth. column shows segmentation. figure examples fully automatic comprehension refcoco+.the column shows input image. column shows expression word attention module weights. column shows predicted subject attention column shows top- attributes. column shows box-level comprehension dotted boxes show prediction yellow dotted boxes shows relative object green boxes ground-truth. column shows segmentation. figure examples fully automatic comprehension refcocog. column shows input image. column shows expression word attention module weights. column shows predicted subject attention column shows top- attributes. column shows box-level comprehension dotted boxes show prediction yellow dotted boxes shows relative object green boxes ground-truth. column shows segmentation. figure examples incorrect comprehension three datasets. column shows input image. column shows expression word attention module weights. column shows predicted subject attention column shows top- attributes. column shows box-level comprehension dotted boxes show prediction yellow dotted boxes shows relative object green boxes ground-truth. column shows segmentation. shen wang dick hengel. image captioning visual question answering based ieee transactions attributes external knowledge. pattern analysis machine intelligence", "year": 2018}