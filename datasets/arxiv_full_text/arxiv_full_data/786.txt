{"title": "Dynamic Frame skip Deep Q Network", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain. One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate. It decides the granularity at which agents can control game play. A frame skip value of $k$ allows the agent to repeat a selected action $k$ number of times. The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state. In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter. This allows us to choose the number of times an action is to be repeated based on the current state. We show empirically that such a setting improves the performance on relatively harder games like Seaquest.", "text": "deep reinforcement learning methods achieved state performance learning control policies games atari domain. important parameters arcade learning environment frame skip rate. decides granularity agents control game play. frame skip value allows agent repeat selected action number times. current state architectures like deep qnetwork dueling network architectures consist framework static frame skip rate action output network repeated ﬁxed number frames regardless current state. paper propose architecture dynamic frame skip deep q-network makes frame skip rate dynamic learnable parameter. allows choose number times action repeated based current state. show empirically setting improves performance relatively harder games like seaquest. introduction video game domains mario atari served test measure performance learning algorithms artiﬁcial intelligence games present unique challenge agent since performing well requires able understand current state game well able choose actions considering long term reward recent application deep learning atari domain resulted state performance. deep q-learning neural network architecture method uses deep neural network convolutional ﬁlters learn hierarchically compositional representation state space. hyper-parameters used framework frame skip rate denotes number times action selected agent repeated. frame skip rate decision making frequency agent high. leads policies change rapidly space time. also leads slower game play since make decision often decision involves running convolutional neural networks computationally intensive. hand high frame skip rate causes infrequent decisions reduces time train episode cost losing ﬁne-grained control. policies learnt higher frame skip rate fewer action sequences exhibit super-human reﬂexes compared frame skip rate frame skip rate reduction decision making frequency gives agent advantage having learn control policies intermediate states given persistent action current state lead agent advantageous next state. skills would useful agent games good policies require level temporal abstraction. example situation seaquest agent continually shoot multiple enemies arriving together depth behind another. said that also situations agent take actions require quick reﬂexes perform well example seaquest being states multiple enemies attack placed varying depths. agent expected manoeuvre skillfully avoid enemies. therefore high static frame skip rate probably solution better game playing strategies spite temporal abstraction provides. ﬁrst step direction temporal abstractions policy space explore dynamic frame skip model built architecture. model agent choose play options macro actions corresponding different frame skip rate values available every state game. show efﬁcacy policies learnt using model learnt using model empirically establishing fact dynamic action repetition important agent’s capabilities extended. thereby make claim need explore experiment structured parametrized policies using actor critic setup atari domain. model propose agent decides whether take repetitive action sequence temporally long shorter sequence repeated actions based input image. also outline generic method wherein agent’s policy particular frame would contain action probabilities level persistence actions well. note experiments performed generic method framework proposed. related work ﬁrst efforts pushed state signiﬁcantly atari domain architecture motivated convolutional neural networks playing games atari domain. combines image-based-state representational capabilities deep convolutional neural networks robustness q-learning work direct modiﬁcation architecture. work focuses power frame skip rate parameter experiments atari domain. learning framework variant enforced sub-populations neuroevolution approach successfully trained complex control tasks controlling robots playing games. show empirically frame skip parameter important factor deciding performance agent atari domain. demonstrate example seaquest achieves best performance frame skip rate frames used. equivalent agent pausing three seconds decisions since emulator runs frames second. argue higher value frame skip rate allows agent learn action selection states skipped hence develop associations states actions temporally distant. however experiment static frame skip setup. dfdqn differs approaches make frame skip dynamic learnable parameter. idea dynamic length temporal abstractions policy space atari domain explored monte carlo tree search planner macro-actions composed action repeated times different approach differs terms using build neural network q-value approximators instead making search techniques cannot generalize. leads high scoring agent also capable playing human speeds generalization states encountered before. similar attempt design high level actions imitate human-like play mario. compose high level actions based human play like walk right small jump right high jump avoid enemy macro actions composed varying length repetitive presses. instance right high jump involves pressing jump right keys together frames right small jump requires frames. background learning control policies game environments commonly learnt reinforcement learning framework game formulated sequence transition tuples markov decision process stateaction value function used guide agent selecting optimal action state measure long-term reward obtained taking action state learn optimal policies agent estimate optimal task. q-learning off-policy temporal difference learning algorithm q-values updated iteratively bellman optimality equation rewards obtained game below deep network high dimensional state spaces infeasible update qvalue possible state-action pairs. address issue approximating parametrized function approximator thereby generalizing states actions operating higher level features able discover features without feature engineering need neural network function approximator. approximates q-value function deep neural network able predict actions states loss function used learning deep network below here represents expected error corresponding current parameter estimate represents parameters separate target network represents parameters online network. usage target network improve stability learning updates. gradient descent step shown below experiences pooled fifo fashion. transitions previous episodes sampled multiple times update current network thereby avoiding divergence issues correlated updates. paper sample transitions uniformly random however show prioritized sampling lead substantial improvement performance. dynamic frame skip deep network motivation behind dfdqn architecture observation humans play games actions tend temporally correlated almost always. certain states game play long sequences action whereas states switch actions performed quickly. example seaquest agent oxygen deep inside would want agent resurface oxygen using long sequence actions. incorporate longer repetitive actions introduce following architecture level changes {a··· a|a|} denote legal actions particular atari domain game introduce actions {a|a|+··· a|a|}. semantics associated actions action results basis action played action difference terms number times basis action repeated emulator. architecture operates static frame skip parameter equal hence selected action repeated times ale. dfdqn action repeated number times number times |a|. think representing temporally repeated action. given this objective work come model learn near-optimal options. scheme implemented doubling number neurons ﬁnal layer architecture. given state dfdqn thus outputs discrete value function approximations {q··· represents approximation return agent would taking action state following optimal policy thereafter. additional repetitive actions value functions learnt dfdqn differ drastically compared learnt ideally would want learn policies parametrized action space agent outputs action parameter denotes number times agent wants play action consecutively. framework would representative power select optimal number times action executed. would also proportionately complex result difﬁcult train. dfdqn framework seeks provide approximation richer model favors simplicity optimality game play. future work wish explore learning parametrized policies games atari domain learnt help actor-critic algorithm. experiments results perform general game-playing evaluation three atari domain games seaquest space invaders alien. deploy single algorithm architecture ﬁxed hyper-parameters learn play three games given pixel observations game rewards. challenging environment since games seaquest reasonably complex observations used high-dimensional network architecture low-level convolutional structure input preprocessing partial observable nature atari games three previous frames combined current frame given multi-channel input dfdqn. followed convolutional layers turn followed fully-connected layers. ﬁrst convolutional layer ﬁlters size stride second ﬁlters size stride third convolutional layer consists ﬁlters size stride since dfdqn double number output neurons wanted representational power able decide larger actions. therefore units pre-output hidden layer compared units used architecture hidden layers followed rectiﬁer nonlinearity. values kept three games equal respectively. similar hyperparameters dfdqn kept three games. double number actions compared dqn. thus ensure sufﬁcient exploration anneal exploration probability million steps million steps used dqn. follow using rmsprop max-norm clipping gradient updates learning. even though values larger extended options possibly giving higher rewards still max-norm clipping used dqn. leave future work best max-norm clip value well hyper-parameters like learning rate ﬁnal exploration probability replay memory size freeze interval would work well games dfdqn setup. establish fact improvement performance increase representational power double number pre-ﬁnal hidden layer neurons baselines units pre-ﬁnal layer three games. implementation https//github.com/spragunr/ deep_q_rl based theano also report scores obtained original architecture pre-ﬁnal hidden layer neurons recent usage reported baseline dudqn model training evaluation framework used training epoch consists steps followed testing epoch consists steps. score single episode deﬁned rewards received emulator episode. score testing epoch deﬁned average scores obtained complete episodes played testing epoch. scores reported testing epoch highest average episode score table presents experimental results. denotes pre-ﬁnal hidden layer size. stands frame-skip value used. value stands dynamic. arch denotes architecture used. denotes best average testing epoch score deﬁned above. following ﬁgures epoch consists steps dqn-a-b refers architecture units pre-output layer operates frame skip rate dfdqn-b dynamic frame skip deep q-network architecture units pre-ﬁnal layer. figure figure figure depict change expected cumulative reward testing episode time games seaquest space invaders alien respectively. dfdqn units pre-output layer signiﬁcantly outperforms neurons pre-output layer. figure figure figure depict evolution average q-values actions time games seaquest space invaders alien respectively. interesting characteristic graph dfdqn-’s q-value seaquest keeps increasing time even epochs. means still scope performance improve training beyond epochs. verify claim temporally extended actions lead better policies analysis percentage times longer sequence basis actions selected. using network yielded best score testing epoch decision-making steps resulted episodes seaquest episodes space invaders episodes alien. recorded actions executed decision-step. average dfdqn chooses longer actions times seaquest times space invaders times alien. shows dfdqn agent able make good longer actions times ignore shorter actions. agent learnt repeated exploration-feedback cycles prefer extended actions still exercises fast reﬂexes situation needs safely claim addition higher frame skip options important contributing factor improvement performance. strengthen claim need dynamic frame skip show results game purely high static frame skip scores poorly gameplay. videos learned policies space invaders seaquest alien available https //goo.gl/vtsen https//goo.gl/dtwzc https//goo.gl/acclb) respectively. conclusions paper introduce architecture called dynamic frame skip deep q-network introduces temporal abstractions extended actions without losing ability make quicker reﬂexes required. show empirically dfdqn leads signiﬁcant improvement performance compared results three atari domain games seaquest space invaders alien. results section illustrates importance extended actions temporally extended versions actions selected times seaquest times space invaders times alien. discussion experiments conducted frame skip rates generic framework would enable agent play wide range frame skip rates selected according current state game. outline generic framework structured policy wherein agent decide action also long action persist. this would need actor critic setup similar deep actor critic learning parametrized policies half field offense domain. policy composed continuous valued parameters like power direction addition action probabilities kick turn tackle dash. parameters restricted ﬁxed continuous interval. adapt setup learning structures policies atari domain continuous valued parameter frame skip rate setup envision network composed three parts core-policy sub-network parameter sub-network critic sub-network ncr. three share lower level convolutional ﬁlters desired depth similar architecture used know successful attempt using actor critic algorithms atari domain. output action probabilities π··· π|a|} probability choosing action a··· a|a|} basis actions chosen task. single scalar output frame skip parameter could constrain ﬁxed interval rmin could rmax could high together constitute actor part actor critic network. critic part depicted outputs single scalar value estimate value function current state. error function optimized manner gradients backpropagated closely follow actor-critic implementation given alternative learning perform repeated actions could previous action input policy similar could combined proposed actor critic setup help deciding frame skip parameter improvement performance achieved without much tuning network parameters. believe dfdqn setup improve appropriate hyper-parameter choices. future work would also like explore paradigm games atari domain recent state methods prioritized replay dueling asynchronousdqn believe combination dynamic frame skip duelling prioritized replay would lead state model games atari domain. acknowledgements thank reviewers ijcai- workshop deep reinforcement learning frontiers challenges useful comments regarding work. also thank karthik narasimhan leandro soriano marcolino suggestions discussions. references fr´ed´eric bastien pascal lamblin razvan pascanu james bergstra goodfellow arnaud bergeron nicolas bouchard yoshua bengio. theano features speed improvements. deep learning unsupervised feature learning nips workshop marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research pages june james bergstra olivier breuleux fr´ed´eric bastien pascal lamblin razvan pascanu guillaume desjardins joseph turian david warde-farley yoshua bengio. theano math expression compiler. proceedings python scientiﬁc computing conference june oral presentation. jakob foerster yannis assael nando freitas shimon whiteson. learning communicate solve riddles deep distributed recurrent q-networks. arxiv preprint arxiv. february volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature february volodymyr mnih adria puigdom enech badia mehdi mirza alex graves harley timothy lillicrap david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. february juan ortega noor shaker julian toimitating human gelius georgios yannakakis. playing styles super mario bros. entertainment computing elsevier ziyu wang schaul matteo hessel hado hasselt marc lanctot nando freitas. dueling network architectures deep reinforcement learning. arxiv preprint arxiv.", "year": 2016}