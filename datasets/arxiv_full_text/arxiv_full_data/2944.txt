{"title": "A dataset and architecture for visual reasoning with a working memory", "tag": ["cs.AI", "cs.CV", "cs.LG"], "abstract": "A vexing problem in artificial intelligence is reasoning about events that occur in complex, changing visual stimuli such as in video analysis or game play. Inspired by a rich tradition of visual reasoning and memory in cognitive psychology and neuroscience, we developed an artificial, configurable visual question and answer dataset (COG) to parallel experiments in humans and animals. COG is much simpler than the general problem of video analysis, yet it addresses many of the problems relating to visual and logical reasoning and memory -- problems that remain challenging for modern deep learning architectures. We additionally propose a deep learning architecture that performs competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as easy settings of the COG dataset. However, several settings of COG result in datasets that are progressively more challenging to learn. After training, the network can zero-shot generalize to many new tasks. Preliminary analyses of the network architectures trained on COG demonstrate that the network accomplishes the task in a manner interpretable to humans.", "text": "abstract. vexing problem artiﬁcial intelligence reasoning events occur complex changing visual stimuli video analysis game play. inspired rich tradition visual reasoning memory cognitive psychology neuroscience developed artiﬁcial conﬁgurable visual question answer dataset parallel experiments humans animals. much simpler general problem video analysis addresses many problems relating visual logical reasoning memory problems remain challenging modern deep learning architectures. additionally propose deep learning architecture performs competitively diagnostic datasets well easy settings dataset. however several settings result datasets progressively challenging learn. training network zero-shot generalize many tasks. preliminary analyses network architectures trained demonstrate network accomplishes task manner interpretable humans. major goal artiﬁcial intelligence build systems powerfully ﬂexibly reason sensory environment vision provides extremely rich highly applicable domain exercising ability build systems form logical inferences complex stimuli avenue studying visual reasoning visual question answering datasets model learns correctly answer challenging natural language questions static images advances multi-modal datasets significant datasets highlight several limitations current approaches. first uncertain degree models trained datasets merely follow fig. sample sequence images instruction dataset. tasks dataset test aspects object recognition relational understanding manipulation adaptation memory address problem. task involve objects shown current image previous images. note ﬁnal example instruction involves last instead latest former excludes current image. target pointing response image shown high-resolution image proper english used clarity. statistical cues inherent images instead reasoning logical components problem second datasets avoid complications time memory integral factors design intelligent agents analysis summarization videos address shortcomings related logical reasoning spatial relationships datasets johnson colleagues recently proposed clevr directly test models elementary visual reasoning used conjunction datasets clevr dataset provides artiﬁcial static images natural language questions images exercise ability model perform logical visual reasoning. recent work demonstrated networks achieve impressive performance near perfect accuracy work address second limitation concerning time memory visual reasoning. reasoning agent must remember relevant pieces visual history ignore irrelevant detail update manipulate memory based information exploit memory later times make decisions. approach create artiﬁcial dataset many complexities found temporally varying data also eschew much visual complexity technical diﬃculty working video particular take inspiration decades research cognitive psychology modern systems neuroscience ﬁelds long history dissecting visual reasoning core components based spatial logical reasoning memory compositionality semantic understanding. towards build artiﬁcial dataset termed exercises visual reasoning time parallel human cognitive experiments rect answers. randomly generated triplets exercise visual reasoning across large array tasks require semantic comprehension text visual perception image sequence working memory determine temporally varying answers highlight several parameters programmatic language allow researchers modulate problem diﬃculty easy challenging settings. finally introduce multi-modal recurrent architecture visual reasoning memory. network combines semantic visual modules stateful controller modulates visual attention memory order correctly perform visual task. demonstrate model achieves near state-of-the-art performance clevr dataset. addition network provides strong baseline achieves good performance dataset across array settings. ablation studies analysis network dynamics network employs human-interpretable attention mechanisms solve visual reasoning tasks. hope dataset corresponding architecture associated baseline provide helpful benchmark studying reasoning time-varying visual stimuli broadly understood community memory largely unsolved problem many eﬀorts underway understand problem e.g. studied ability sequential models compute time notably limited memory horizon memory capacity measured synthetic sequential datasets indeed large constraint training network models perform generic turing-complete operations ability train systems compute time developing computer systems comprehend time-varying sequence images prominent interest video understanding intelligent video game agents attempts used feed-forward architecture baseline model much work invested building video analysis game agents contain memory component types systems often limited ﬂexibility network memory systems clear degree systems reason based complex relationships past visual imagery. consider visual question answering datasets based single static images datasets construct natural language questions probe logical understanding network natural images. strong suggestion literature networks trained datasets focus statistical regularities prediction tasks whereby system cheat superﬁcially solve given task towards several researchers proposed build auxiliary diagnostic synthetic datasets uncover potential failure modes highlight logical comprehension further many specialized neural network architectures focused multi-task learning proposed address problem leveraging attention external memory family feature-wise transformations explicitly parsing task executable sub-tasks inferring relations pairs objects contribution takes direct inspiration previous work single images focuses aspects time memory. second source inspiration long line cognitive neuroscience literature focused developing battery sequential visual tasks exercise measure speciﬁc attributes visual working memory several lines cognitive psychology neuroscience developed multitudes visual tasks time exercise attribute identiﬁcation counting comparison multiple attention logical operations work emphasizes compositionality task generation ingredient generalizing unseen tasks importantly literature provides measurements humans animals tasks well discusses biological circuits computations underlie explain variability performance. designed large tasks requires broad range cognitive skills solve especially working memory. major goal dataset build compositional tasks include variants many cognitive tasks studied humans animals dataset contains triplets task instruction sequences synthetic images sequences target responses image consists number simple objects vary color shape location. possible colors possible shapes network needs generate verbal pointing response every image. build large tasks ﬁrst describe potential tasks using common uniﬁed framework. task dataset deﬁned abstractly constructed compositionally basic building blocks namely operators. operator performs basic computation selecting object based attributes comparing attributes operators deﬁned abstractly without specifying exact attributes involved. task formed directed acyclic graph operators finally instantiate task specifying relevant attributes graph task instance used generate verbal task instruction minimally-biased image sequences. many image sequences generated task instance. operators tasks trillion possible task instances dataset vary number images maximum memory duration maximum number distractors image explore memory capacity proposed model systematically vary task diﬃculty. explicitly stated canonical setting mmax dmax fig. generating compositional dataset. dataset based operators combined form various task graphs task instantiated specifying attributes operators graph. task instance used generate image sequence semantic task instruction. forward pass graph image sequence normal task execution. generating consistent minimally biased image sequence requires backward pass graph reverse topological order image sequence reverse chronological order. dataset many ways similar clevr dataset contain synthetic visual inputs tasks deﬁned operator graphs however diﬀers clevr important ways. first tasks dataset involve objects shown past sequential nature inputs. second dataset visual inputs minimal response bias generated operator simple function receives produces abstract data types attribute object objects spatial range boolean. operators total select getcolor getshape getloc exist equal switch using operators dataset currently contains tasks number operators task graph ranging task instruction obtained task response bias major concern designing synthetic dataset. neural networks achieve high accuracy dataset exploiting bias. rejection sampling used ensure balanced response distribution developed method dataset generate minimally-biased synthetic image sequences tailored individual tasks. short ﬁrst determine minimally-biased responses generate images would lead speciﬁed responses. images generated reversed order normal task execution generation images visited reverse chronological order task graph traversed reverse topological order visiting operator target output already speciﬁed randomly choose allowable outputs. based speciﬁed output image modiﬁed accordingly and/or supposed input passed next operator target outputs addition overall network contains four major systems visual system processes images. semantic system processes task instructions. visual short-term memory system maintains processed visual information provides outputs guide pointing response. finally control system integrates converging information systems uses several attention gating mechanisms regulate systems process inputs generate outputs provides verbal outputs. critically network allowed multiple time steps ponder image giving potential solve multi-step reasoning problems naturally iteration. visual system processes input images. visual inputs maps respectively. convolutional layer employs kernels followed max-pooling layer batch-normalization rectiﬁed-linear last layers convolutional network subject feature spatial attention. feature attention scales shifts batch normalization parameters individual feature maps activity neurons within feature multiplied added scalars. particular implementation feature attention termed conditional batch-normalization fig. diagram proposed network. sequence images provided input convolutional neural network instruction form english text provided sequential embedding network visual short-term memory network holds visual-spatial information time provides pointing output vstm module considered convolutional lstm network external gating. stateful controller provides attention gating signals directly indirectly. output network either discrete continuous feature-wise linear modulation film critical component model achieved near state-of-the-art performance clevr dataset soft spatial attention applied convolutional layer following feature attention activation function. multiplies activity neurons spatial preferences using positive scalar. semantic processing system receives task instruction generates semantic memory controller later attend conceptually produces semantic memory contextualized representation word instruction task actually performed. pondering step performing task controller attend individual parts semantic memory corresponding diﬀerent words phrases. word mapped -dimensional trainable embedding vector sequentially -unit bidirectional long short-term memory network outputs bidirectional lstm words form semantic memory size rule nword number words instruction rule -dimensional vector semantic memory forms key. semantic attention query vector dimension used retrieve semantic memory summing keys weighted similarities query. used bahdanau attention computes similarity utilize spatial information preserved visual system pointing output layer convolutional network feeds visual short-term memory module turn projects group pointing output neurons. structure also inspired posterior parietal cortex brain maintains visual-spatial information guide action visual short-term memory module extension convolutional lstm network gating mechanisms conditioned external information. vstm module consists number feature maps input output connections convolutional. currently recurrent connections within vstm module besides forget gate. state output module step indicates convolution. vstm module diﬀers convolutional lstm network mainly input forget output gates self-generated. instead provided externally controller. addition input directly network convolutional layer applied between. convolutions currently equivalently feature vstm module adds gated previous activity weighted combination post-attention activity feature maps layer visual system. finally activity vstm feature maps combined generate single spatial output synthesize information across entire network include controller receives feedforward inputs systems generates feedback attention gating signals. architecture inspired prefrontal cortex brain controller gated recurrent unit network. pondering step post-attention activity visual layer processed -unit fully connected layer concatenated retrieved semantic memory vstm module output controller. addition activity visual layer summed across space provided controller. controller generates queries semantic memory linear feedforward network. retrieved semantic memory generates feature attention another linear feedforward network. controller generates -dimensional soft spatial attention layer feedforward network -unit hidden layer rectiﬁed-linear activation function followed softmax normalization. finally controller state concatenated retrieved semantic memory generate input forget output gates used vstm module linear feedforward network followed sigmoidal activation function. verbal output single word pointing output coordinates pointing. coordinate loss function deﬁned output loss function used every task. verbal output uses cross-entropy loss. ensure pointing output loss comparable scale verbal output loss include group pointing group neurons. given target coordinates gaussian distribution centered target location target probability distribution pointing output neurons. clevr. clip controller state norm clevr. also trained initial states recurrent networks. network trained end-to-end adam combined learning rate decay schedule. demonstrate reasoning capability proposed network trained clevr dataset even though explicit need working memory clevr. network achieved overall test accuracy clevr surpassing human-level performance comparable stateof-the-art methods performing network controller used pondering steps image. feature attention applied convolutional layers. vstm module disabled since pointing output. output network human-interpretable intuitive. figure illustrate verbal output various attention signals evolved pondering steps example image-question pair. network answered long question decomposing small executable steps. even though training relies verbal outputs last pondering steps network learned produce interpretable verbal outputs reﬂect reasoning process. figure computed eﬀective feature attention diﬀerence between normalized activity maps without feature attention. post feature-attention normalized activity average activity across feature maps feature attention divide activity mean. relative spatial attention normalized subtracting time-averaged spatial attention map. example network uses pondering steps. proposed model achieved maximum overall test accuracy dataset canonical setting noticed small signiﬁcant variability ﬁnal accuracy even networks hycontaining operators tend take substantially longer learned remain lower accuracy. tried many approaches reducing variance including various curriculum learning regimes diﬀerent weight bias initializations different optimizers hyperparameters. approaches tried either signiﬁcantly reduce variance degraded performance. model proposed contains multiple attention mechanisms short-term memory module multiple pondering steps. assess contribution component overall accuracy trained versions network clevr dataset component ablated full network. also trained baseline network components ablated. baseline network still contains visual processing lstm network semantic processing network controller. give ablated network fair chance re-tuned hyperparameters total number parameters limited original network reported maximum accuracy. found baseline network performed poorly datasets surprise network relies diﬀerent combination mechanisms solve clevr dataset. network depends strongly feature attention clevr depends strongly spatial attention dataset possible explanation fig. pondering process proposed network visualized attention output single clevr example. example question image clevr validation set. eﬀective feature attention pondering step. relative spatial attention maps. semantic attention. verbal outputs. blue indicate stronger weaker respectively. simultaneous feature attention small metal spheres spatial attention behind rubber object color attended object reﬂected verbal output. later pondering process network paid feature attention large matte ball correct answer emerged verbal output. better suited select objects clevr. multiple pondering steps important datasets demonstrating beneﬁcial solve multistep reasoning problems iteration. although semantic attention rather minor impact overall accuracy datasets useful tasks operators longer task instructions explore range diﬃculty visual reasoning dataset varied maximum number distractors image maximum memory duration number images sequence setting selected best network across hyper-parameter settings involving model capacity learning rate schedules. models explored accuracy best network drops substantially distractors. large number distractors network accuracy also drops longer memory duration. results suggest network diﬃculty ﬁltering many distractors maintaining memory time. however doubling number images clear eﬀect accuracy indicates network developed solution invariant number images used sequence. harder setting dataset dmax mmax potentially serve benchmark powerful neural network models. fig. ablation studies. overall accuracies various ablation models clevr test vstm module included model clevr. breaking accuracies based output type whether spatial reasoning involved number operators last operator task graph. fig. proposed network zero-shot generalize tasks. networks trained tasks. shown maximum accuracies networks trained tasks excluded task chance levels task hallmark intelligence ﬂexibility capability generalize unseen situations. training testing image sequence generated anew therefore network able generalize unseen input images. that network generalize trillions task instances although millions used training. challenging form generalization completely tasks explicitly trained test whether network generalize tasks trained groups networks. group contains networks trained tasks. monitored accuracy tasks. task report highest accuracy across networks. found networks able immediately generalize untrained tasks average accuracy tasks excluded training substantially higher average chance level although still lower average accuracy trained tasks hence proposed model able perform zero-shot generalization across tasks success although matching performance trained task explicitly. understand network able perform tasks generalize tasks carried preliminary analyses studying activity controller. suggestion networks perform many tasks engaging clusters units cluster supports operation address question examined low-dimensional representations activation space controller labeled points based individual tasks. figure highlight clustering behavior across tasks emerges training dataset fig. clustering compositionality controller. level task involvement controller unit task task involvement measured task variance quantiﬁes variance activity across diﬀerent inputs given task. unit task variances normalized maximum units clustered according task variance vectors showing tasks accuracy higher t-sne visualization task variance vectors units colored cluster identity. example compositional representation tasks. compute state-space representation task mean controller activity vector obtained averaging across many diﬀerent inputs task. representation tasks shown ﬁrst principal components. vector direction shared direction altering task change shape color. previous work suggested humans ﬂexibly perform tasks representing learned tasks compositional manner instance analysis semantic embeddings indicates network learn shared directions concepts across word embeddings searched signs compositional behavior exploring directions activation space controller correspond common sub-problems across tasks. figure highlights direction identiﬁed corresponds axis shape color across multiple tasks. results provide ﬁrst step understanding neural networks understand task structures generalize tasks. work built synthetic compositional dataset requires system perform various tasks sequences images based english instructions. tasks included dataset test range cognitive reasoning skills particular require explicit memory past objects. dataset minimallybiased highly conﬁgurable designed produce rich array performance measures large number named tasks. also built recurrent neural network model harnesses number attention gating mechanisms solve dataset natural humaninterpretable way. model also achieves near state-of-the-art performance another visual reasoning dataset clevr. model uses recurrent controller attention diﬀerent parts images instructions produce verbal outputs iterative fashion. iterative attention signals provide multiple windows model’s step-by-step pondering process provide clues model breaks complex instructions smaller computations. finally network able generalize immediately completely untrained tasks demonstrating zero-shot learning tasks.", "year": 2018}