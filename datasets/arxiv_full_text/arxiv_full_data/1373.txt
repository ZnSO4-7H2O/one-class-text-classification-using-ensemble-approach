{"title": "A Single Model Explains both Visual and Auditory Precortical Coding", "tag": ["q-bio.NC", "cs.CV", "cs.LG", "cs.NE"], "abstract": "Precortical neural systems encode information collected by the senses, but the driving principles of the encoding used have remained a subject of debate. We present a model of retinal coding that is based on three constraints: information preservation, minimization of the neural wiring, and response equalization. The resulting novel version of sparse principal components analysis successfully captures a number of known characteristics of the retinal coding system, such as center-surround receptive fields, color opponency channels, and spatiotemporal responses that correspond to magnocellular and parvocellular pathways. Furthermore, when trained on auditory data, the same model learns receptive fields well fit by gammatone filters, commonly used to model precortical auditory coding. This suggests that efficient coding may be a unifying principle of precortical encoding across modalities.", "text": "precortical neural systems encode information collected senses driving principles encoding used remained subject debate. present model retinal coding based three constraints information preservation minimization neural wiring response equalization. resulting novel version sparse principal components analysis successfully captures number known characteristics retinal coding system center-surround receptive ﬁelds color opponency channels spatiotemporal responses correspond magnocellular parvocellular pathways. furthermore trained auditory data model learns receptive ﬁelds well gammatone ﬁlters commonly used model precortical auditory coding. suggests eﬃcient coding unifying principle precortical encoding across modalities. sensory information goes various forms processing reaches cerebral cortex. visual information transformed neural signals retina passes retinal ganglion cells characterized center-surround shaped receptive ﬁelds auditory information hand passed brain auditory nerve ﬁbers whose ﬁltering properties well described gammatone ﬁlters since peripheral processing prepares data subsequent despite intensive research still mysteries concerning functional role pre-cortical processing. example diﬀerent sensory modalities adopt computational principles pre-cortical stages? although tantalizing assume recent studies suggest otherwise. example learned gammatone ﬁlters natural sound using independent component analysis previously applied natural image patches learn edge/bar shaped ﬁlters resembling simple cells’ receptive ﬁelds since gammatones model pre-cortical auditory nerve ﬁbers region cortex gives rise puzzle would brain strategy preprocessing pre-cortical stage auditory pathway early cortical processing visual pathway questions remain even peripheral processing single modality. recently graham proposed decorrelation response equalization sparseness form minimum constraints must considered account known linear properties retinal coding hypothesis combination several previous theories. response equalization theory hypothesizes retinal coding seeks representation ensures neuron approximately average activity level animal presented natural scenes output decorrelation theory follows eﬃcient coding principle hypothesizes retinal coding represents eﬃcient coding information visual domain capturing second-order statistical structure visual inputs making signals neurons less correlated. theories derivatives whitening theory hypothesizes retina coding produces ﬂattened response spectrum natural visual inputs speciﬁc range spatial frequencies whitening theory links properties retinal coding statistics natural scenes part prevailing view retinal processing. third theory suggests system trying minimize energy usage wiring cost vincent argued systems minimize energy usage minimizing wiring give center surround receptive ﬁelds. given various objectives still clear constraints actually operating speciﬁcation retinal coding system. would desirable build retinal coding model integrates diﬀerent ideas behind theories explains origins observed center-surround receptive ﬁelds. ideally model also able explain pre-cortical processing modalities. model takes account following considerations. retina compresses approximately million photoreceptor responses million ganglion cell responses. hence ﬁrst consideration would like ganglion cells retain maximum amount information photoreceptor responses. make simplifying assumption ganglion cells respond linearly optimal linear compression technique terms reconstruction error principal components analysis neural network figure weight vectors hidden unit network correspond eigenvector covariance matrix data. standard ordering hidden units ﬁrst hidden unit high response variance last hidden unit practically variance means ﬁrst hidden unit orders magnitude work last one. second consideration then would like spread work evenly among hidden units. hence impose threshold average squared output hidden units. simulations order preserve maximum information units threshold equalizes work. third consideration proﬂigate connections every ganglion cell would non-zero connections every photoreceptor. hence also impose constraint connectivity network. latter constraint inspired earlier work proposed model retinal early cortical processing based energy minimization showed could create center-surround shaped receptive ﬁelds grayscale images. however system sometimes cells center-surround ﬁelds optimization unstable. minimizes reconstruction error maximizes information maintained encoding. sparsity weight small reconstruction error reduces term involves corresystem sensitive second order statistics. second constraint minimizes connections photoreceptors ganglion cells incorporating sparsity economy elementary features. constraint average energy ganglion cells equalizes work across ganglion cells. system fact push term threshold order maintain maximum information. thus objective function integrates three major theories retinal coding eﬃcient coding response equalization economy elementary features directly another three terms objective function determine diﬀerent aspects basis functions reconstruction error determines subspace basis functions span; output constraint speciﬁes lengths basis functions; sparsity penalty rotates basis functions within subspace determined reconstruction error. sense three terms hence three theories embody necessary fully characterize retinal coding model. observation partially agrees prediction conclude minimum three constraints must considered account known linear properties decorrelation response equalization size/sparseness. three constraints eﬃcient coding information response equalization sparseness connections. important feature objective function allows derive eﬃcient algorithm learn model parameters revised model turns particular variation sparse reducible sparse coding therefore eﬃciently estimate parameters model apply larger range data typically used past. critical insight provided mapping sparse coding model exactly sparse coding applied transpose data matrix rather data matrix itself. means also model dimensionality expansion well dimensionality reduction although expansion longer eﬃcient approximation derived supplementary materials. follows show simple objective function able account retinal ganglion cell receptive ﬁelds gammatone ﬁlters used characterize signals auditory nerve. going forward important understand distinction features ﬁlters. features rows represent connections photoreceptors hidden units figure ﬁlters hand correspond rows pseudoinverse correspond receptive ﬁelds ganglion cells would result reverse correlation. visualize computation figure network receives input photoreceptors inhibition hidden units similar receptive ﬁelds grayscale images applied model four grayscale image datasets. results qualitatively similar across sets; results describe subset hateren natural image described simulation used figure neural networks implement sparse pca. left hand panel represents network performs pca. weights hidden unit pixels represent coordinates unit-length eigenvector covariance matrix data activations projections onto eigenvectors. weights used reconstruct data right panel represents sparse pca. connections pixels sparse followed recurrent connections give center-surround shape. would reﬂected recordings hidden units hence receptive ﬁeld hidden unit represented −at. variance captured standard eigenvectors retained connection weights absolute zero; contrast none connection weights standard zero. figure plots distribution connection weights learned model versus standard pca. selected features. features gaussians plot circles figure center radius circle represent center twice standard deviation ﬁtted gaussian. visualize well gaussians features display ﬁrst feature figure highlight ﬁtted gaussian. shown ﬁgure gaussians provide mosaic coverage image patch. reduce number hidden units blobs enlarge cover image shown figure optimal ﬁlters center-surround shaped shown bottom panel figure ﬁrst ﬁlter example recovers weight assigned ﬁrst feature panel. tantalizing think ﬁlters on-centered others off-centered. however switch signs features without changing model’s objective function. hence model provide insight diﬀerence on-centered off-centered cells beyond usual explanation neurons cannot positively negatively. interesting population gaussian blob shaped features give rise center-surround shaped ﬁlters. shown figure ﬁlter weighted elementary features viewed result sequence eﬀorts recover contribution corresponding feature. feature ﬁrst applied template ﬁlter image patch estimate contribution. however estimation inaccurate feature overlaps neighboring features. accurate estimation contribution neighboring features must subtracted. potentially overcompensates even accurate estimation must back contribution features neighboring features surround ﬁrst feature. process repeats moving ever outward. however weight reduces quickly features removed ﬁrst feature makes resulting ﬁlter eﬀectively localized keeps ﬁlter center-surround shaped aspects natural scene images give rise learned features observe? answer question apply algorithm white noise images contain statistical structure pink noise images follow power natural scene images otherwise contain structure. figure displays example images learned ﬁlters. white noise images learned features one-pixel image templates; corresponding ﬁlters also contain non-zero pixel. model simply keeps pixels ignores pixels. that’s best features white noise images contain structure. pink noise images learn essentially elementary features learned natural scene images. result supports hypothesis center-surround shaped ﬁlters come power spectrum natural scene images agrees classic whitening theory sparseness level plays important role shaping learned features ﬁlters. increases model puts emphasis sparse connections cost keeping less information inputs. biological system occur system strict energy budget. check learned ﬁlters change larger values. analyzing ﬁlters fourier space plot amplitude various frequencies giving contrast sensitivity function. shown figure larger value model becomes less sensitive frequency information sensitive high frequency information. change matches psychophysical studies contrast sensitivity children chronic malnutrition. compared normal children malnourished children reported less sensitive spatial frequencies slightly figure panel displays features randomly grayscale image patches. selected features; bottom panel displays optimal ﬁlters. features gaussian blobs plot circles figure radius circle represents twice standard deviation ﬁtted gaussian blob. help visualize well gaussian blobs features display ﬁrst feature figure highlight ﬁtted gaussian. gaussian become bigger smaller number features construct image patches shown figure figure displays ﬁrst ﬁlter figure weighted features. feature plotted circle figure color circle represents weight assigned feature figure experiments white noise pink noise images. figure display images containing white noise pink noise. panel figure displays ﬁlters learned white noise images; bottom panel plays ﬁlters learned pink noise images. sensitive high spatial frequencies shift acuity towards high frequencies suggested result might eﬀort neural system capture visual information limited neural wiring budget. chromatic images applied algorithm four chromatic image datasets found learn qualitatively similar features each. here report features learned kyoto image dataset. retinal cones estimated given input model. resulting model captures variance captured optimal linear model output neurons connections absolute zero. figure displays representative features well corresponding ﬁlters learned chromatic image patches. visualize connection strength three ﬁlters l/m/s channels figure among learned features black/white blobs blue/yellow blobs red/green blobs. figure plot spatial layout learned features. result replicates segregation spatial channel color channel retina stage segregation explored previous research applied information-theoretic methods natural color spectra/images common observation studies learned visual features segregate black/white blue/yellow red/green opponent structures either shaped fourier basis functions gabor kernel functions results similar obtained although small connections inputs figure experiments increased sparseness level plot amplitudes diﬀerent frequency component ﬁlters learned shown ﬁgure increased value ﬁlter becomes less sensitive frequencies sensitive high frequencies. grayscale videos explore spatio-temporal structure natural videos collected video dataset clips nature documentaries. dimensional image patch ﬂattened vector input responses three dimensional spatiotemporal patch video also tranformed vector. vectors given model input. learned features black/white blobs whose contrast changes time. shown figure features well ﬁtted spatio-temporal gaussians. corresponding ﬁlters spatially center-surround shaped. temporal proﬁle seems provide edge detector along temporal axis similar temporal proﬁle describing retinal ganglion cells animation learned ﬁlters http//cseweb.ucsd.edu/~gary/video_w.gif. another interesting observation features segregate groups low-spatial high-temporal frequencies ﬁgure) high-spatial low-temporal frequencies ﬁgure) plotted figure suggests division ganglion cells magno-pathway parvo-pathway represents eﬃcient encoding visual environment. segregation appears reveal statistical properties natural videos instead coming speciﬁc algorithm. fact found segreimage patches figure left panel displays representative features; right panel displays corresponding ﬁlters. features belong three categories black/white blobs blue/yellow blobs red/green blobs. corresponding optimal ﬁlters center-surround shaped black/white blue/yellow red/green antagonism. figure plots connection strength ﬁlters cones. figure plot spatial layout learned features figure sound applied algorithm three sound datasets qualitatively similar results three datasets. shown figure learned ﬁlters well ﬁtted gammatone ﬁlters. gammatone ﬁlters resemble ﬁltering properties auditory nerve ﬁbers estimated using reverse correlation technique animal cats chinchillas note ﬁrst time non-ica algorithm learned gammatone-like ﬁlters sound. also since used algorithm visual auditory modalities provides answer question posed olshausen o’connor perhaps even deeper question accounts neural response properties earliest stage analysis auditory system whereas visual system accounts response properties cortical neurons many synapses removed photoreceptors. model suggests necessary obtain gammatone ﬁlters sound; rather sparse account receptive ﬁelds neurons earliest stages analysis auditory visual modalities. suggested three principles used explain precortical encoding information preservation minimization neural wiring response equalization. principles independently justiﬁed evolutionary energy minimization arguments. clearly organism extract much relevant information possible environment. organisms evolve survive enriched environments information relevant becomes diﬃcult encode genome. barlow attneave argued redundancy reduction eﬃcient coding reasonable response environmental complexity. minimizing energy usage suggests constructing minimal architecture possible minimizing wiring terms development daily energy budgets. finally equalizing work results single component crucial organism. model closely related previous theories diﬀers crucial respects. inspired vincent al.’s model also attempted derive center surround receptive ﬁelds information preservation minimizing wiring. including response normalization constraint able obtain stable algorithm none occasional double receptive ﬁelds generated model. unlike many previous models explicitly decorrelating outputs model necessary order obtain results. hence model integrates three components suggested necessary retinal coding model graham chandler field figure figure plot revcor ﬁlters estimated cat’s auditory nerve ﬁbers using linear reverse correlation technique well ﬁtted gammatone ﬁlter. figure plot ﬁlters learned timit speech dataset pittsburgh environmental sound dataset respectively. blue line plots estimated ﬁlter; line plots ﬁtted gammatone ﬁlter. response equalization sparseness replaces decorrelation minimization neural wiring. sparse model interesting link olshausen’s sparse coding model; models minimize reconstruction error response equalization model imposes sparseness dictionary sparse coding model imposes sparseness output. finally model closely related described proposed model retinal output linear transform input ignoring optical blur. objective model minimize diﬀerence input reconstruction awx. hence model seen approximation take small also include extra terms learning regularize average amplitude outputs impose sparseness version sparseness leads simple interpretation locallyconnected ganglion cells inhibitory surround. also convexity properties model lead excellent convergence properties. derived eﬃcient algorithm learn model parameters transforming sparse coding problem. approximate algorithm uses covariance matrix runs orders magnitude faster exact algorithm obtaining results less percent diﬀerent objective function learned basis functions. approximation works well assumption small fast approximation necessary part successful model speed computation allows used rich data video. applied algorithm grayscale images color images grayscale videos human speech environmental sound learned visual auditory ﬁlters resemble ﬁltering properties retinal ganglion cells auditory nerve ﬁbers. learned ﬁlters novel. example learns magno parvo segregation pathways; learns gammatone ﬁlters natural sound. finally noted above model suggests answer question gives features corresponding cortical receptive ﬁelds vision seems necessary obtain gammatone-like ﬁlters sound pre-cortical level processing. suggestion necessary obtain gammatone ﬁlters; rather algorithm sparsity response equalization constraints result gammatone ﬁlters sound also producing receptive ﬁelds similar peripheral neurons vision. vincent interpret synaptic strength input output neurons; hence interpret sparsity penalty desire minimize neural wiring cost. generative point view columns elementary features model uses construct observed inputs. thus overall objective function understood capturing information inputs using economic dictionary elementary features. however ﬁxed full rank optimal output fact given pseudoinverse apply linear reverse correlation technique model neurons recovers ﬁlters transforming inputs outputs rows model neurons’ receptive ﬁelds. columns rows describe diﬀerent aspects model. columns describe elementary features model uses construct observed inputs; forms kind visual dictionary. rows hand represent best linear ﬁlters recover weights assigned elementary features generating observed inputs. since better reﬂects properties cells’ receptive ﬁelds focus primarily throughout paper. discussion suggests another interpretation retinal coding model interpreted removing redundancy input samples sparse coding usually interpreted removing redundancy input dimensions sense architecture- applies instead perhaps ﬁrst sparse algorithm ever proposed. need pick eﬃcient sparse coding algorithms optimize model parameters. typically optimizing objective function factored sub-problems optimizing ﬁxing optimizing ﬁxing algorithm computational complexity depends number samples give approximate algorithm whose complexity relies input output dimensionalities. experiments grayscale images reduces computation time minutes seconds learned parameters close learned without approximation. derivation algorithm also helps connection retinal coding model output decorrelation theory denotes identity matrix denotes taking trace matrix. constraint since positive semideﬁnite factor using eigenvalue decomposition uvut unitary matrix containing eigenvectors columns; diagonal matrix eigenvalues diagonal. bbt. replacing also symbolically mapped replace hence objective function also optimized eﬃcient sparse coding algorithms coordinate descent algorithms appendix since sparse coding reduces redundancy inputs derivation also brings another interpretation retinal coding model seen removing redundancy eigenvectors small algorithm eﬃciently minimizes objective function. test optimization methods directly optimize objective function ﬁrst initialize optimizing optimize implement experiments single precision computer server intel core processors. reduce dimensionality takes minutes directly optimize hand takes less seconds initialize optimizing another seconds optimize shown figure approximate method eﬃciently minimizes objective function. fact initialized approximate method optimizing objective function direct method causes less change objective function average weights changed visible diﬀerence features ﬁlters observed. closeness approximation means later experiments approximation provided used without additional optimization. grayscale images four image sets used hateren natural images kyoto natural images berkeley segmentation dataset caltech- object category dataset didn’t observe qualitative diﬀerence features learned datasets. features reported obtained using hateren natural denotes pixel value selected image average pixel value equals nonlinearity. nonlinearity seem alter features learned retinal coding model might help expose higher-order figure comparison spca connection weights grayscale image patches. figure plot distribution connection weights learned model versus learned standard pca. first estimate retinal cones’ responses images. original images stored srgb color representation normalize pixel values image transform image color space estimate cone responses following ciecam color appearance model manner roughly estimate retinal cones would respond presented image content. that apply cone nonlinearity estimated cone responses. extract image patches estimate matrix apply retinal coding model using approximation reduce dimensionality resulting model captures variance captured optimal linear model output neurons connections absolute zero. grayscale videos explore spatio-temporal structure natural videos collected video dataset clips youtube videos natural history shows world wide channel order realistic sample natural videos eliminate obviously unnatural walking dinosaurs cavemen pigeon-mounted cameras. calculated power spectrum videos order eliminate interlaced-format videos ellipse-shaped power spectrum elongated horizontal direction. initially applied algorithm hateren video dataset learned features ﬁlters qualitatively similar reported below except learned features ellipse even shaped elongated along horizontal direction. features likely result fact original videos interlaced although videos original youtube videos color. transform grayscale videos using method described matlab function rgbgray normalize pixel values apply cone nonlinearity estimate correlation sound applied algorithm three sound datasets pittsburgh natural sounds dataset timit speech dataset rainforest mammal vocalization dataset pittsburgh natural sounds dataset contains recordings natural sound recorded around pittsburgh region including ambient sounds quick acoustic events timit speech dataset contains english speech speakers person speaking sentences. rainforest mammal vocalization dataset contains characteristic sounds species rainforest mammals primates anteaters bats jaguars manatees. dataset recording re-sampled khz. normalize maximum amplitude recording take segments formed using sliding window sample points estimate matrix sound segments. apply approximate method learn features ﬁlters.", "year": 2016}