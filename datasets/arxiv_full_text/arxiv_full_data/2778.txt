{"title": "Generalizing Hamiltonian Monte Carlo with Neural Networks", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. We release an open source TensorFlow implementation of the algorithm.", "text": "daniel levy∗ matthew hoffman jascha sohl-dickstein stanford university google perception google brain danilevycs.stanford.edu {mhoffmanjaschasd}google.com present general-purpose method train markov chain monte carlo kernels parameterized deep neural networks converge quickly target distribution. method generalizes hamiltonian monte carlo trained maximize expected squared jumped distance proxy mixing speed. demonstrate large empirical gains collection simple challenging distributions instance achieving improvement effective sample size case mixing standard makes measurable progress second. finally show quantitative qualitative gains real-world task latent-variable generative modeling. release open source tensorflow implementation algorithm. high-dimensional distributions analytically tractable normalizing constant ubiquitous many ﬁelds. instance arise protein folding physics simulations machine learning sampling distributions critical task learning inference however extremely hard problem general. markov chain monte carlo methods promise solution problem. operate generating sequence correlated samples converge distribution target. convergence often guaranteed detailed balance sufﬁcient condition chain target equilibrium distribution. practice proposal distribution ensure detailed balance metropolis-hastings accept/reject step. despite theoretical guarantees eventual convergence practice convergence mixing speed depend strongly choosing proposal works well task hand. what’s more often science know mcmc chain converged chain produced uncorrelated sample additionally reliance detailed balance assigns equal probability forward reverse transitions often encourages random-walk behavior thus slows exploration space densities continuous spaces hamiltonian monte carlo introduces independent auxiliary momentum variables computes state integrating hamiltonian dynamics. method traverse long distances state space single metropolis-hastings test. state-of-the-art method sampling many domains. however perform poorly number settings. mixes quickly spatially struggles mixing across energy levels volume-preserving dynamics. also work well multi-modal distributions probability sampling large enough momentum traverse low-density region negligibly small. furthermore struggles ill-conditioned energy landscapes deals poorly rapidly changing gradients recently probabilistic models parameterized deep neural networks achieved great success approximately sampling highly complex multi-modal empirical distributions building successes present method that given analytically described distribution automatically returns exact sampler good convergence mixing properties class highly expressive parametric models. proposed family samplers generalization hmc; transforms trajectory using parametric functions retaining theoretical guarantees tractable metropolis-hastings accept/reject step. sampler trained minimize variation expected squared jumped distance parameterization reduces easily standard hmc. capable emulating several common extensions withintrajectory tempering diagonal mass matrices show signiﬁcant empirical gains various distributions performs poorly. ﬁnally evaluate method real-world task training sampling latent variable generative model show improvement model’s log-likelihood greater complexity distribution posterior samples. adaptively modifying proposal distributions improve convergence mixing explored past case prior work reduced need choose step size number leapfrog steps adaptively tuning parameters. salimans proposed alternate scheme based variational inference. adopt much simpler approach pasarica gelman show choosing hyperparameters proposal distribution maximize expected squared jumped distance principled effective practice. previous work also explored applying models machine learning mcmc tasks. kernel methods used learning proposal distribution approximating gradient energy physics restricted semirestricted boltzmann machines used build approximations energy function allow rapid sampling motivate hand-designed proposals similar approach recent work song uses adversarial training volume-preserving transformation subsequently used mcmc proposal distribution. promising technique several limitations. gradient information often crucial maintaining high acceptance rates especially high dimensions. also indirectly measure quality generated sample using adversarial training notoriously unstable suffers mode collapse often requires objective modiﬁcation train practice finally since proposal transformation preserves volume suffer difﬁculties mixing across energy levels illustrate section compute metropolis-hastings acceptance probability deterministic transition operator must invertible tractable jacobian. recent work introduces rnvp invertible transformation operates layer modifying subset variables function depends solely remaining variables. exactly invertible efﬁciently computable jacobian. furthermore chaining enough layers model made arbitrarily expressive. parameterization directly motivate extension leapfrog integrator hmc. target distribution analytically known constant space markov chain monte carlo methods provide samples mcmc methods construct markov chain whose stationary distribution target distribution obtaining samples corresponds simulating markov chain i.e. given initial distribution transition kernel constructing following sequence random variables order stationary distribution chain three conditions must satisﬁed must irreducible aperiodic ﬁxed |x)pdx. condition often satisﬁed satisfying stronger detailed balance condition written given proposal distribution satisfying mild conditions easily construct transition kernel respects detailed balance using metropolis-hastings accept/reject rules. formally starting step sample probability accept next sample chain. reject retain previous state typical proposals algorithm strong asymptotic guarantees. practice must often choose acceptance probabilities cautious proposals lead slow mixing. continuous state spaces hamiltonian monte carlo tackles problem proposing updates move state space staying roughly iso-probability contours exp) state extends state space additional momentum vector distributed independently augmented state produces proposed state approximately integrating hamiltonian dynamics jointly taken potential energy kinetic energy. since hamiltonian dynamics conserve total energy system approximate integration moves along approximate iso-probability contours dynamics typically simulated using leapfrog integrator single time step consists following sohl-dickstein write action leapfrog integrator terms operator introduce momentum operator important note properties operators. first transformation involution i.e. flfl fl−v) second transformations volume-preserving shear thus easy compute. vanilla transformations i.e. variables changes amount determined leave symbolic form section metropolis-hastings one. determinant jacobian section describe proposed method lhmc given access energy function lhmc learns parametric leapfrog operator augmented state space. begin describing desiderata detail parameterize sampler. finally conclude section describing training procedure. powerful algorithm still struggle even simple problems. example two-dimensional multivariate gaussian ill-conditioned covariance matrix take arbitrarily long traverse whereas trivial sample directly another problem move energy levels random walk leads slow mixing models. finally cannot easily traverse low-density zones. example given simple gaussian mixture model cannot modes without recourse additional tricks illustrated figure observations determine list desiderata learned mcmc kernel fast mixing fast burn-in mixing across energy levels mixing modes. pursuing goals must take care ensure proposal operator retains features leapfrog operator used must invertible determinant jacobian must tractable. leapfrog operator satisﬁes properties ensuring sub-update affects subset variables sub-update depends nonlinearly variables updated. free generalize leapfrog operator preserves properties. particular free translate rescale sub-update leapfrog operator long careful ensure translation scale terms depend variables updated. begin augmenting current state continuous momentum variable drawn standard normal. also introduce binary direction variable drawn uniform distribution. denote complete augmented state probability density ppp. finally step operator assign ﬁxed random binary mask determine variables affected half entries half convenience write denotes element-wise multiplication ones vector). update steps describe details augmented leapfrog integrator single time-step direction ﬁrst update momenta update depend subset full state excludes takes form leapfrog momentum update. update hinted above make transformation expressive ﬁrst update subset coordinates followed complementary subset. ﬁrst update yields affects depends state subset conversely deﬁned below second update affects again translation rescales effect momenta rescales positions recover original leapfrog position update determinant jacobian ﬁrst transformation determinant jacobian second transformation finally update again based subset give intuition terms scaling applied momentum enable among things acceleration low-density zones facilitate mixing modes. scaling term applied gradient energy allow better conditioning energy landscape partial ignoring energy gradient rapidly oscillating energies. corresponding integrator given appendix essentially inverts updates equations experiments functions implemented using multi-layer perceptrons shared weights. encode current time step input. leapfrog operator corresponds running steps modiﬁed leapfrog operator reverses direction variable written terms modiﬁed operators proposal acceptance probability identical standard hmc. note however parameterization enables learning non-volume-preserving transformations determinant jacobian function necessarily evaluate quantity derived appendix convenience denote operator re-samples momentum direction. i.e. given sampling thus consists alternating application following steps markov transition satisﬁes detailed balance respect need criterion train parameters control functions choose loss designed reduce mixing time. speciﬁcally minimize lag-one autocorrelation. equivalent maximizing expected squared jumped distance extended state space deﬁne expected squared jumped distance thus however loss need indeed maximizing objective lead encourage mixing across entire state space. regions state space almost mixing occurs long average squared distance traversed remains high. optimize typical worst case behavior include reciprocal term loss scale parameter capturing characteristic length scale problem. second term encourages typical moves large ﬁrst term strongly penalizes sampler ever state cannot move effectively small resulting large loss value. train sampler minimizing loss target distribution initialization distribution. formally given initial distribution deﬁne minimize ﬁrst term loss encourages mixing considers operator applied draws distribution; second term rewards fast burn-in; controls strength ‘burn-in’ regularization. given loss exactly describe training procedure algorithm important note training iteration done pass network efﬁciently batched. emphasize training procedure applied learnable operator whose jacobian’s determinant tractable making general framework training mcmc proposals. input energy function gradient initial distribution augmented state space number iterations niters number leapfrogs learning rate schedule t≤niters initialize parameters sampler initialize niters present empirical evaluation trained sampler diverse energy functions. ﬁrst present results collection distributions capturing common pathologies energy landscapes followed results task machine learning maximum-likelihood training deep generative models. each compare well-tuned step length show signiﬁcant gains mixing time. code implementing algorithm available online. evaluate lhmc sampler diverse collection energy functions posing different challenges standard hmc. ill-conditioned gaussian gaussian distribution diagonal covariance spaced loglinearly demonstrates lhmc learn diagonal inertia tensor. strongly correlated gaussian rotate diagonal gaussian variances extreme version example neal problem shows that although parametric sampler applies element-wise transformations adapt structure axis-aligned. mixture gaussians mixture isotropic gaussians centroids separated distance means thus standard deviations apart making almost impossible modes. rough well similar example sohl-dickstein given small energy altered negligibly gradient perturbed high frequency noise oscillating experiments choose report effective sample size compute quantities sohl-dickstein observe samplers trained lhmc show greatly improved autocorrelation presented tasks providing improved task. addition show lhmc easily modes standard gets stuck mode unable traverse density zone. experimental details well comparison lahmc shown appendix comparison a-nice-mc addition well known challenges associated adversarial training note parameterization using volume-preserving operator dramatically fail simple energy landscapes. build experiment presented mixture isotropic gaussians separated distance variances consider setup increase ratio show figure sample chains trained lhmc variances a-nice-mc; a-nice-mc cannot effectively modes fraction volume large mode mapped small making highly improbable traverse. also issue hmc. hand lhmc traverse low-density region modes larger volume left mode smaller volume right mode. important note distance clusters less case thus good diagnostic shortcomings volume-preserving transformations. apply learned sampler task training sampling posterior latentvariable generative model. model consists latent variable choose conditional distribution generates image given family parametric ‘decoders’ samples {x}i≤n tractable conditional distribution parameters typically parameterized neural network. recently improve upon well-known pitfalls like over-pruning hoffman proposed hmc-dlgm. data sample obtaining sample approximate posterior hoffman runs mcmc algorithm energy function p|z; obtain exact posterior sample given better posterior sample algorithm maximizes p|z; show beneﬁts lhmc borrow method hoffman replace jointly training lhmc sampler improve efﬁciency posterior sampling. call model lhmc-dlgm. diagram model formal description training procedure presented appendix deﬁne subsequent sections compare method standard model kingma welling hmc-dglm hoffman important note that since sampler trained jointly performs exactly number gradient computations energy function hmc. ﬁrst show training latent variable generative model lhmc results better generative models qualitatively quantitatively. show improved sampler enables expressive non-gaussian posterior. implementation details decoder neural network fully connected layers units softplus non-linearities outputs bernoulli activation probabilities pixel. encoder architecture returning mean variance approximate posterior. model trained epochs adam learning rate experiments done dynamically binarized mnist dataset ﬁrst present samples decoders trained lhmc elbo although higher likelihood necessarily correspond better samples figure shown appendix decoder trained lhmc generates sharper samples compared methods. compare method terms log-likelihood data. previously stated marginal likelihood data point tractable requires integrating high-dimensional space. however estimate using annealed importance sampling following evaluate generative models training held-out data. figure plot data’s log-likelihood number gradient computation steps hmc-dglm lhmc-dglm. similar number gradient computations lhmc-dglm achieves higher likelihood training held-out data. strong indication lhmc provides signiﬁcantly better posterior samples. standard framework approximate posteriors often parametrized gaussian thus making strong assumption uni-modality. section show using lhmc sample posterior enables learning richer posterior landscape. block gibbs sampling highlight ability capture expressive posteriors in-paint image using block gibbs sampling using approximate posterior lhmc. formally starting image. denote bottom-half pixels xtop xbottom step sample sample xtop ˜xtop compare results obtained sampling using trained sampler. results reported figure lhmc easily mixes modes approximate posterior gets stuck reconstructed digit visualization posterior training decoder lhmc randomly choose element parallel lhmc chains metropolis-hastings steps. direction highest variance project samples along direction show histogram figure plot shows non-gaussianity latent space posterior. using improved sampler enables decoder make expressive posterior enables encoder sample non-gaussian posterior. loss section targets lag-one autocorrelation. possible extend also target lag-two higher autocorrelations. also possible extend loss reward fast decay autocorrelation statistics samples instance sample energy well sample position. additional statistics could also include learned statistics samples combining beneﬁts adversarial approach current work. learned generalization prove complementary several research directions related hmc. would interesting explore combining work minibatch setting shadow hamiltonians gradient pre-conditioning approaches similar used riemannian alternative accept-reject rules non-canonical hamiltonian dynamics variants adapted proposals extension discrete state spaces alternative hamiltonian integrators finally work also complementary methods utilizing gradient information. example could incorporate intuition behind multiple metropolis schemes several parametric operators training used. addition could draw inspiration adaptive literature component-wise strategies work presented general method train expressive mcmc kernels parameterized deep neural networks. given target distribution analytically known constant method provides fast-mixing sampler able efﬁciently explore state space. hope method utilized black-box manner domains sampling constitutes huge bottleneck protein foldings physics simulations would like thank poole aditya grover david belanger colin raffel insightful comments draft mohammad norouzi support encouragement launching project jiaming song discussions help running a-nice-mc. wei-lun chao justin solomon dominik michels sha. exponential integration hamiltonian monte carlo. proceedings international conference machine learning salimans diederik kingma welling. markov chain monte carlo variational inference bridging gap. proceedings international conference machine learning yichuan zhang zoubin ghahramani amos storkey charles sutton. continuous relaxations discrete hamiltonian monte carlo. advances neural information processing systems section neural networks hidden layers units relu non-linearities. train adam learning rate train iterations batch size rough well. tasks train sampler temperature parameter continuously anneal; evaluate trained sampler without using temperature. compare trained sampler lahmc results reported table lhmc largely outperforms lahmc task. lahmc also unable modes task. also note lhmc could easily combined lahmc replacing leapfrog integrator lahmc learned lhmc. section present training algorithm well diagram explaining lhmc-dglm. conciseness given operator denote distribution next state given sampling momentum direction metropolis-hastings step. similar lhmc training unconditional sampling share weights across addition auxiliary variable ﬁrst passed -layer neural network softplus non-linearities hidden units. input given input dataset number iterations niters number metropolis-hastings step number leapfrogs learning rate schedule t≤niters randomly initialize decoder’s parameters approximate posterior initialize parameters sampler leapfrog steps. niters", "year": 2017}