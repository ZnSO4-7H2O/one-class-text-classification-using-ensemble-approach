{"title": "Optimal Solutions for Sparse Principal Component Analysis", "tag": ["cs.AI", "cs.LG"], "abstract": "Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefficients, with total complexity O(n^3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n^3) per pattern. We discuss applications in subset selection and sparse recovery and show on artificial examples and biological data that our algorithm does provide globally optimal solutions in many cases.", "text": "given sample covariance matrix examine problem maximizing variance explained linear combination input variables constraining number nonzero coeﬃcients combination. known sparse principal component analysis wide array applications machine learning engineering. formulate semideﬁnite relaxation problem derive greedy algorithm computes full good solutions target numbers zero coeﬃcients total complexity number variables. relaxation derive suﬃcient conditions global optimality solution tested pattern. discuss applications subset selection sparse recovery show artiﬁcial examples biological data algorithm provide globally optimal solutions many cases. keywords subset selection sparse eigenvalues sparse recovery lasso. principal component analysis classic tool data analysis visualization compression wide range applications throughout science engineering. starting multivariate data ﬁnds linear combinations variables called principal components corresponding orthogonal directions maximizing variance data. numerically full involves singular value decomposition data matrix. shortcomings factors linear combinations original variables; factor coeﬃcients non-zero. means facilitates model interpretation visualization concentrating information factors factors still constructed using variables hence often hard interpret. many applications coordinate axes involved factors direct physical interpretation. ﬁnancial biological applications axis might correspond speciﬁc asset gene. problems these natural seek trade-oﬀ goals statistical ﬁdelity interpretability solutions nonzero coeﬃcients principal components usually easier interpret. moreover applications nonzero coeﬃcients direct cost hence direct trade-oﬀ statistical ﬁdelity practicality. eﬃciently derive sparse principal components sparse vectors explain maximum amount variance. belief many applications decrease statistical ﬁdelity required obtain sparse factors small relatively benign. numerically easy factor requires computing leading eigenvector done sparse hard combinatorial problem. fact moghaddam show subset selection problem ordinary least squares np-hard reduced sparse generalized eigenvalue problem sparse particular intance. sometimes rotation techniques used post-process results interpretable directions underlying particular subspace another simple solution threshold loadings small absolute value zero systematic approach problem arose recent years various researchers proposing nonconvex algorithms slra zhang d.c. based methods modiﬁed principal components zero loadings. spca algorithm based representation regression-type optimization problem allows application lasso penalization technique based norm. exception simple thresholding algorithms require solving convex problems. recently also d’aspremont derived based semideﬁnite relaxation sparse problem complexity given finally moghaddam used greedy search branch-and-bound methods solve small instances problem exactly good solutions larger ones. step greedy algorithm complexity leading total complexity full solutions. contribution twofold. ﬁrst derive greedy algorithm computing full good solutions total numerical cost based convexity largest eigenvalue symmetric matrix. derive tractable suﬃcient conditions vector global optimum means practice that given vector support test globally optimal solution problem performing binary search iterations solve dimensional convex minimization problem. fact take sparsity pattern candidate algorithm test optimality. paper builds earlier conference version providing simpler conditions optimality describing applications subset selection sparse recovery. certainly case made penalized maximum eigenvalues strictly focus formulation. however shown recently donoho tanner meinshausen among others) fact deep connection constrained extremal eigenvalues lasso type variable selection algorithms. sufﬁcient conditions based sparse eigenvalues guarantee consistent variable selection sparse recovery results derive produce upper bounds sparse extremal eigenvalues thus used prove consistency lasso estimation prove perfect recovery sparse recovery problems prove particular solution subset selection problem optimal. course conditions suﬃcient necessary duality bounds produce sparse extremal eigenvalues cannot always tight observe duality often small. paper organized follows. begin formulating sparse problem section section write eﬃcient algorithm computing full candidate solutions problem total complexity section formulate convex relaxation sparse problem section derive tractable suﬃcient conditions global optimality particular sparsity pattern. section detail applications subset selection sparse recovery variable selection. finally section test numerical performance results. variable parameter controlling sparsity. assume without loss generality positive semideﬁnite variables ordered decreasing marginal variances i.e. σnn. also assume section focus ﬁnding good solution problem using greedy methods. ﬁrst present simple preprocessing solutions complexity recall simple greedy algorithm complexity finally ﬁrst contribution section derive approximate greedy algorithm computes full solutions problem total complexity simplest ranking algorithm sort diagonal matrix rank variables variance. works intuitively diagonal rough proxy eigenvalues schur-horn theorem states diagonal matrix majorizes eigenvalues sorting costs another quick solution compute leading eigenvector form sparse vector thresholding zero coeﬃcients whose magnitude smaller certain level. done cost means formed padding zeros leading eigenvector submatrix σikik note entire algorithm written terms factorization matrix means signiﬁcant computational savings given eigenvalues eigenvectors transformed matrix i.e. eigenvector section step computes maximum eigenvalue matrices size approximate algorithm section computes full path ﬁrst cholesky decomposition complexity k-th iteration maximum eigenvalue problem computing products also matrix directly given gram matrix rq×n advantageous directly square root total complexity getting path cardinality reduced eigenvalue problems computing vector products). last expression concave function pointwise minimum aﬃne functions. relax original problem convex optimization problem simply dropping rank constraint section derive necessary suﬃcient conditions test optimality solutions relaxations obtained sections well suﬃcient condition tightness semideﬁnite relaxation derive conditions problem particular case given rank candidate solution need test optimality. necessary suﬃcient conditions optimality convex relaxation provide suﬃcient conditions global optimality non-convex problem previous lemma shows given candidate vector test optimality semideﬁnite program solving semideﬁnite feasibility problem variables solution indeed optimal semideﬁnite relaxation must also globally optimal original nonconvex combinatorial problem lemma provides suﬃcient global optimality conditions combinatorial problem based optimality conditions convex relaxation given lemma practice given sparsity pattern rather vector lemma also proof following proposition lemma matrices dual optimal solutions corresponding primal optimal solution primal solution rank semideﬁnite relaxation tight pattern optimal section shows globally optimal solution need eﬃcient algorithm theorem show below dual variables deﬁned duality convex function hence given sparsity pattern eﬃciently search best possible performing binary search iterations. minimizing convex function done eﬃciently using binary search. simple outer matrix product iteration binary search subgradient computed solving maximum eigenvalue problem cost means complexity ﬁnding optimal given interval o/ǫ)) conditions satisﬁed relaxation optimal solution rank strictly larger hence tight. point diﬀerent relaxation dspca d’aspremont better solution. also apply randomization techniques improve quality solution problem consider data points data matrix rp×n. assume given real numbers predict using linear regression estimated least squares. thus looking minimum. subset selection bound gives suﬃcient condition optimality subset selection problem instance given subset. contrasted suﬃcient conditions derived particular algorithms lasso backward greedy selection note optimality conditions often based sparse eigenvalue problems observation restricted isometry constant condition computed solving following sparse maximum eigenvalue problem deﬁned means cand`es obtained asymptotic proof random matrices satisﬁed restricted isometry condition overwhelming probability whenever satisﬁed tractable optimality conditions upper bounds obtain section sparse problems allow prove deterministically ﬁnite dimensional matrix satisﬁes restricted isometry condition note cand`es provide slightly weaker condition based restricted orthogonality conditions extending results sparse conditions would increase maximum perfect recovery holds. practice however section relaxations d’aspremont provide tight upper bounds sparse eigenvalues random matrices solving semideﬁnite programs large scale instances remains signiﬁcant challenge. section ﬁrst compare various methods detailed artiﬁcial examples test performance biological data set. pathspca matlab code reproducing results downloaded authors’ pages. form test matrix σvvt signal-to-noise ratio. ﬁrst compare relative performance algorithms section identifying correct sparsity pattern given matrix resulting curves plotted ﬁgure example computing time approximate greedy algorithm section seconds versus seconds full greedy solution section algorithms produce almost identical answers. also sorting thresholding curves dominated greedy algorithms. plot variance versus cardinality tradeoﬀ curves various values signal-to-noise ratio. ﬁgure notice magnitude error decreases signal-to-noise ratio. also structure problem kink variance cardinality curves. note examples error minimal precisely kink. search algorithms section consistency intervals ﬁgure plot variance versus cardinality tradeoﬀ curve plot greedy variances dual upper bounds section upper bounds computed using dspca present simulation experiments synthetic datasets subset selection problem. consider data sets generated sparse linear regression problem study optimality subset selection problem given exact cardinality generating vector. setting known regularization ℓ-norm procedure also known lasso asymptotically lead correct solution certain consistency condition satisﬁed results provide tractable test optimality solutions obtained various algorithms lasso forward greedy backward greedy algorithms. figure consider pairs randomly generated examples dimension lasso provably consistent isn’t. perform simulations samples varying noise compute average frequency optimal subset selection lasso greedy backward algorithm together frequency provable optimality backward greedy algorithm exhibits good performance suﬃcient optimality condition satisﬁed long much noise. figure plot average mean squared error versus cardinality replications using forward backward selection lasso exhaustive search plot left shows results lasso consistency condition satisﬁed plot figure left variance versus cardinality tradeoﬀ curves plot variance dual upper bounds section target cardinality. right variance versus cardinality tradeoﬀ curve plot greedy variances dual upper bounds section upper bounds computed using dspca optimal points bold. right shows mean squared errors consistency condition satisﬁed. sets ﬁgures show lasso consistent consistency condition satisﬁed backward greedy algorithm ﬁnds correct pattern noise small enough even lasso inconsistent case. pick normally distributed small enough computing sparse eigenvalues exhaustive search numerically feasible. plot maximum sparse eigenvalue versus cardinality obtained using exhaustive search approximate greedy fully greedy algorithms. also plot upper bounds obtained minimizing rank solution solving semideﬁnite relaxation explicitly solving dspca dual figure backward greedy algorithm lasso. plot probability achieved provable optimality versus noise greedy selection lasso subset selection problem noisy sparse vector. left lasso consistency condition satisﬁed. right consistency condition satisﬁed. figure greedy algorithm lasso. plot average mean squared error versus cardinality replications using forward backward selection lasso exhaustive search left lasso consistency condition satisﬁed. right consistency condition satisﬁed. figure upper lower bound sparse maximum eigenvalues. plot maximum sparse eigenvalue versus cardinality obtained using exhaustive search approximate greedy fully greedy algorithms. also plot upper bounds obtained minimizing rank solution solving semideﬁnite relaxation explicitly solving dspca dual left matrix gaussian. right sparse rank plus noise matrix. biological examples follow) gaussian random matrices harder. note however duality semideﬁnite relaxations optimal solution small cases bounds based greedy solutions good. means solving relaxations d’aspremont could provide tight upper bounds sparse eigenvalues random matrices. however solving semideﬁnite programs large values remains signiﬁcant challenge. algorithm section gene expression data sets colon cancer alon lymphoma alizadeh plot variance versus cardinality tradeoﬀ curve ﬁgure together dual upper bounds section cases consider genes largest variance. note many cardinalities optimal close optimal solutions. table also compare important genes selected second sparse factor colon cancer data genes selected rankgene software observe genes sparse genes rankgene genes. figure variance versus cardinality tradeoﬀ curve gene expression data sets lymphoma colon cancer together dual upper bounds section optimal points bold. eﬃcient greedy algorithms provide candidate solutions many turn optimal practice. resulting upper bounds also direct applications problems sparse recovery subset selection lasso variable selection. note extensively convex relaxation test optimality provide bounds sparse extremal eigenvalues almost never attempt solve numerically would provide optimal bounds. matrix variables appendix consider various results expansions eigenvalues order derive suﬃcient conditions. following proposition derives second order expansion eigenvectors corresponding single eigenvalue. matrix single zero eigenvalue equal eigenvector eigenvalue zero multiplicity proposition applied eigenvalues eigenvalue around remaining ones around zero. eigenvalue close equal", "year": 2007}