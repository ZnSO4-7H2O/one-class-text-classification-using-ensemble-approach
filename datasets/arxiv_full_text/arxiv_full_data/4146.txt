{"title": "Deep Neural Networks Under Stress", "tag": ["cs.CV", "cs.AI"], "abstract": "In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.", "text": "fig. overview framework. input images converted stressed feature vectors extracting descriptions using pre-trained deep network transforming/stressing feature vectors reducing precision number dimensions. saving computational resources training data. transfer learning often used great success deep models greedy terms data processing power. straightforward scheme choose pre-trained network freeze weights certain layer introduce train layers task. picking different layers original network controls degree transfer models. conceptually output frozen transferred layers image seen feature vector thus classiﬁer like used classiﬁcation target dataset. paper propose stress tests represented figure consistently interfere network selectively destroy information. explore important aspects deep architectures dimensionality numerical precision representations. dimensionality stress tests introduce smaller original dimensionality quantization stress tests introduce aggressively quantized subset real numbers also combine stresses. although recent studies reevaluate deep architectures respect size precision representations primary focus practical impacts upon original tasks. framework designed transfer learning tasks shed light general properties networks show strong degree redundancy opening opportunity create powerful compact descriptors. recent years deep architectures used transfer learning state-of-the-art performance many datasets. properties features remain however largely unstudied transfer perspective. work present extensive analysis resiliency feature vectors extracted deep models special focus trade-off performance compression rate. introducing perturbations image descriptions extracted deep convolutional neural network change precision number dimensions measuring affects ﬁnal score. show deep features robust disturbances compared classical approaches achieving compression rate losing original score pascal deep convolutional neural networks swept computer vision community state-of-the-art performance many tasks however analytical understanding models still lacking shrouding cloud procedures tricks trade without simply fail work. therefore full understanding deep representations became holy grail research machine learning computer vision explore properties deep networks measuring extent preserve discriminative information input i.e. measuring robustness feature vectors generate. indeed understand deep model ﬁrst learns extract good representation uses representation make decision challenge understanding deep models unknown nature learned features. pursuing understanding transfer learning stress tests probe networks. transfer learning consists recycling knowledge model another form model weights initialization architecture originally trained imagenet transfer scheme classiﬁcation task pascal dataset perform extensive experiments study robustness architecture detailed table different types stresses. formalize pre-trained deep model series functions layer network equal dimensionality input data output layer. stress tests choose layer freeze network ﬁrst output layer train svm. then pick stressing function retrain model using input. comparing scores infer network’s resiliency chosen stress. better highlight inherent properties deep models instead speciﬁc characteristics vgg-m also evaluate part experiments googlenet furthermore order differentiate deep models classical approaches also report comparative results recent bag-of-words model’s bossanova cases pre-process images according model’s recommended protocol. order understand redundant deep representation ﬁrst stress tests drop dimensions feature vector. number dimensions preserved step proportional initial size feature vector according expression contrast strategies selecting dimensions dropped step tdr- drops randomly; tdr- uses pca-based strategy. latter discards dimensions encoding less variance. take consideration random choice repeat experiment times. table vgg-m model. description layers groups vgg-m model matconvnet toolbox proposed chatﬁeld conv. indicates convolutional layer fully fully connected layer relu rectiﬁed linear unit layer local response normalization layer pooling pooling layer softmax activation softmax function. ﬁnal experiment applies stressors simultaneously dropping dimensions feature vector quantizing values remaining elements. goal measure cross-effects stressor diminishes numerical precision representation quantizing feature vectors. objective explore advanced quantization strategies here consider fast simple scalar quantizations analyze effect classiﬁcation task. ﬁrst dimensions quantized regular intervals using minimum maximum scalar values observed training dimensions. explained given experimental point freeze pretrained network layer discarding upper layers. pick stressing function output feature vector transfer learning classiﬁcation task. -normalize feature vectors feed linear model measuring model’s scores diffig. results dimensionality reduction standard deviation horizontal axis value indicates percentage original dimensions kept corresponding score respect initial shown vertically. right side ﬁgure show number dimensions model initial size preserved. ferent choices picking stressing functions different kinds intensities gain insight resiliency deep models stresses. experiments report classiﬁcation scores mean average precision pascal accuracy food- mit- following literature tradition datasets. although tested deep networks extensively space constraints report experiments layer vgg-m layer googlenet. results representative observations throughout networks. table shows scores vanilla experiments using setups without perturbating feature vectors simplify bossanova’s pipeline pascal disabling concatenation classic visual words using linear instead recommended kernel. results dimensionality reduction experiments pascal dataset shown figure strong redundancy representations detected since across runs small variations score observed. googlenet robust random dimensionality perturbation average drop dimensions removed. however googlenet start -times bigger cnn-m. considering direct comparison descriptions approximately size scores models equivalent. although bossanova shown similar resiliency dimensionality reduction vgg-m held better scores every test point despite feature vectors -times smaller pca-based strategy effective preserving information dropping dimensions. held dimensions removed could keep map. choosing right dimensions drop improves robustness feature vectors dimensionality perturbations. number classes target dataset also seems play important role performance resiliency seen figure correctly classifying data diverse datasets need complementary feature points lost dimensionality reduction. quantization experiments hand reduce size feature vectors initial bits aggressively limiting values. performed better indicating adaptiveness scale plays important role main results summarized table column indicates maximum desired loss respect original score experiment cells indicate minimum value satisﬁes requirement. example second line second column reveals dimensions preserved googlenet score drops less table minimum representation rate pascal column indicates requirement line represents dataset. cells reveal minimum representation needed losing indicated percentage. instance googlenet means dimensions lose initial score. finally results base setup shown figure region represents combinations parameters complementary characteristics indicating features compressed terms dimension precision time. point circle square cross markers speciﬁc combinations compression rates respectively maintaining original score. paper evaluated robustness deep representations introducing perturbations feature vectors extracted upper layers deep networks. explored depth resiliency features transferred vgg-m model pascal dataset. ﬁndings show high level redundancy deep representations thus heavily compressed. experiments achieve compression rate losing fig. results feature compression reduce number dimensions precision feature vectors time. circle square cross mark conﬁgurations compression rates respectively maintaining original score. original score pascal ensure conclusions datasetmodel-speciﬁc main approaches dimensionality reduction quantization extensively tested supplementary results mit- food- googlenet bossanova. furthermore observed despite compact deep architectures also robust perturbations compared approaches based bags visual words. ﬁndings specially useful image retrieval metric learning size feature vector crucial achieve fast response times applications involving portable devices remote classiﬁcation data must efﬁciently transferred network. research partially supported cnpq santander samsung eletrˆonica amazˆonia ltda. framework also thank cenapad-sp microsoft azure amazon services computational resources michel fornaciali valuable advices. thibaut durand nicolas thome matthieu cord weldon weakly supervised learning deep convolutional neural networks computer vision pattern recognition razavian azizpour sullivan carlsson features off-the-shelf astounding baseline recognition ieee conference computer vision pattern recognition chevalier thome cord fournier henaff dusch lr-cnn ﬁne-grained classiﬁcation varying resolution ieee international conference image processing thibaut durand nicolas thome matthieu cord mantra minimum maximum latent structural image classiﬁcation ranking international conference computer vision courbariaux bengio j.-p. david binaryconnect training deep neural networks binary weights propagations advances neural information processing systems", "year": 2016}