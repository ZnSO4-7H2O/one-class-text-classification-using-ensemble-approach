{"title": "Investigating the Impact of Data Volume and Domain Similarity on  Transfer Learning Applications", "tag": ["cs.CV", "cs.AI"], "abstract": "Transfer Learning helps to build a system to recognize and apply knowledge and experience learned in previous tasks (source task) to new tasks or new domains (target task), which share some commonality. The two important factors that impact the performance of transfer learning models are: (a) the size of the target dataset and (b) the similarity in distribution between source and target domains. Thus far there has been little investigation into just how important these factors are. In this paper, we investigated the impact of target dataset size and source/target domain similarity on model performance through a series of experiments. We found that more data is always beneficial, and that model performance improved linearly with the log of data size, until we were out of data. As source/target domains differ, more data is required and fine tuning will render better performance than feature extraction. When source/target domains are similar and data size is small, fine tuning and feature extraction renders equivalent performance. We hope that our study inspires further work in transfer learning, which continues to be a very important technique for developing practical machine learning applications in business domains.", "text": "transfer learning helps build system recognize apply knowledge experience learned previous tasks tasks domains share commonality. important factors impact performance transfer learning models size target dataset similarity distribution source target domains. thus little investigation important factors are. paper investigated impact target dataset size source/target domain similarity model performance series experiments. found data always beneﬁcial model performance improved linearly data size data. source/target domains diﬀer data required tuning render better performance feature extraction. source/target domains similar data size small tuning feature extraction renders equivalent performance. hope study inspires work transfer learning continues important technique developing practical machine learning applications business domains. many applications computer vision data short supply. convolutional neural networks trained large datasets imagenet beneﬁt massive computational power revolutionized computer vision many business applications computer vision limited relevant data. many researchers machine learning engineers great success addressing problem utilizing transfer learning transfer learning knowledge network trained large dataset imagenet transferred another problem domain variety techniques applied accomplish this however commonly ﬁnal network layers original network replaced layers suitable target domain network trained data related target domain. network beneﬁts learnings original network allowing network perform task using signiﬁcantly less training data. study motivated nearly entirely work done revisiting unreasonable eﬀectiveness data deep learning era. paper authors noted model size computational power increased previous years imagenet still used train models. paper goes examine impact increasing volume training data imagenet’s million images million images google jftm image dataset. authors observed model performance increases logarithmically based volume training data. results sparked curiosity. wanted understand impact target domain data volume source/target domain similarity model performance. understand impact data volume relationships target source data domains transfer learning problems various pre-trained neural network architectures selected tuned training data diﬀerent domains sizes series experiments. model performances subsequently compared analyzed obtain insights. datasets primarily used throughout study namely miniplace dataset imdb-wiki dataset although internally enhanced preprocessed. kaggle’s dogs cats data also used exploratory dataset. dogs/cats dataset kaggle’s dogs cats competition originally provided microsoft research. dataset consists training images cats dogs creating binary classiﬁcation problem. separate unlabeled test provided used. miniplace data originates places data scene-centric database million images comprising unique scene categories. miniplace scaled-down version places images categories resolution imdb-wiki dataset collected published rothe largest publicly available dataset face images gender labels training. dataset consists public celebrity facial images gender labels inferred associated timestamps names totals images. images contain multiple faces suﬀer quality issues entire dataset internally cleaned preprocessed well merged internal employee proﬁle image data enhance size quality. study labels used. common issues encountered business applications lack data typical data would order records subset datasets used throughout experiments investigate eﬀects data transfer learning similar data volume regime. diﬀerent neural network architectures tested study. google’s inceptionv model pre-trained imagenet tested dogs/cats dataset exploratory analysis impact data size. vgg-face model pre-trained public internet data applied imdb-wiki dataset example applications highly similar target source domains. addition vgg-face model also applied miniplace data investigate transfer learning applications highly diﬀerent target source domains. model pre-trained using imagenet data applied miniplace data example case moderately diﬀerent target source domains. series transfer learning experiments diﬀerent target/source domains varied training data sizes conducted shed lights eﬀect data size transfer learning performances. experiments categorized three groups depending diﬀerent target domain form source domain. feature extraction using convnet pre-trained source domain data remove last fully connected layer output layer. layers replaced fully connected layer target domain speciﬁc output layer. remaining layers used ﬁxed feature extractor replaced layers trained. fine-tuning accomplishing training steps speciﬁed feature extraction allow pretrained layers convnet updated back-propagation tuning weights target domain. possible ﬁne-tune layers convnet keep earlier layers ﬁxed ﬁne-tune higher-level portion network. highly similar source/target domains vgg-face model trained imdbwiki dataset increasing data size incremented predict individuals’ ages. fine-tuning feature extraction respectively applied experiments. highly diﬀerent source/target domains vgg-face model trained miniplace dataset increasing data size incremented classify diﬀerent scenes contained data. fine-tuning feature extraction respectively applied experiments. moderately diﬀerent source/target domains model trained miniplace dataset increasing data size incremented classify diﬀerent scenes contained data. fine-tuning feature extraction respectively applied experiments. training processes utilized adam optimizer number epochs ensure consistent comparisons across models. model performances evaluated holdout dataset size. ‘categorical accuracy’ metric used evaluating classiﬁcation models ‘mean absolute error’ metric used evaluating regression models. experiment model parameters ﬁxed except size training data grows increment step experiment conducted vgg-face inception models imdb-wiki miniplaces dogs/cats datasets respectively. figure shows improvement model accuracy training data size grows. surprisingly number epochs learning rate model accuracy continues increasing training data expands demonstrates eﬀectiveness data size. addition lift model performance scales logarithmically data size consistency ﬁndings figure model performance logarithmically correlated training data size. vggface model shown left model shown right. lower left shows results inceptionv model models trained epochs expanding training data since original data used vgg-face model imdb-wiki dataset consists facial images experiment serves investigation transfer learning applications highly similar target/source domains. figure shows decay loss function training epochs experiment results feature extraction illustrated left subﬁgures ﬁne-tuning shown right. seen training validation loss values using feature extraction tend stabilize plateau epochs training data become available. seemingly transition point roughly training data contains images. contrarily ﬁne-tuning leads ever-decreasing train validation loss indicating model performance improved data training time provided. observations indicate transfer learning problems highly similar target source domains would beneﬁcial perform ﬁne-tuning rather feature extraction training data becomes large enough. particular combination vgg-face model imdb-wiki dataset transition training size around however possible corresponding training size exists every transfer learning setting ﬁne-tuning gives better model performance feature extraction still produce similar results training data transition point. figure source vgg-face; target imdb-wiki. comparison feature extraction ﬁne-tuning reduction loss function values. decreasing rates training validation loss diminish training data grows feature extraction losses keep declining ﬁne-tuning. experiment ﬁne-tuning feature extraction applied training model miniplace data classify image scenes data. models trained epochs expanding training data imagenet source data trained contains thousands image categories including similar scenes miniplace data categories diﬀer signiﬁcantly miniplace images. therefore experiment serves investigation transfer learning applications target/source domains sharing limited similarities still largely diﬀerent. experimental results shown figure feature extraction plotted left ﬁne-tuning right. plots compare evolution training validation respect training epochs feature extraction ﬁne-tuning. seen sizable training validation loss also large absolute values plots. implies model suﬀers higher bias compared last experiment meaning training becomes diﬃcult target source data domains diverge. furthermore ﬁnal loss values training validation lower observed ﬁne-tuning rendered similar training validation losses compared feature extraction training size small; training size gets ﬁne-tuning starts out-perform feature extraction lower validation losses generated. additionally overﬁtting present experiments training loss increases size training data increases monotonically explained small size training data noted deliberately compared performance ﬁne-tuning feature extraction across diﬀerent sizes small large experiments. figure source vgg; target miniplace. comparison feature extraction ﬁnetuning reduction loss function values. left panel shows results feature extraction right panel shows results ﬁne-tuning. training validation loss slightly lower ﬁne-tuning feature extraction training. experiment ﬁne-tuning feature extraction applied training vggface model miniplace dataset classify image scenes data. models trained epochs expanding training data aforementioned source data used vgg-face model primarily contains facial information distinctively diﬀerent miniplace data contains general scenes airport county yard experiment hence investigation transfer learning applications target/source domains dramatically diﬀerent other. results ﬁne-tuning feature extraction summarized figure showing decay validation loss function training epochs experiment. feature extraction plotted left whereas ﬁne-tuning shown right. comparing last experiment becomes diﬃcult achieve similar accuracies number epochs training. target/source domains become diﬀerent level diﬃculty training increases. although validation loss increases training epochs obvious diﬀerences model performance observed feature extraction ﬁne-tuning seems critical training size approximately validation loss exhibits starting training epoch addition training size becomes larger increase validation loss slows eventually starts decreasing training size approaches observations indicate early stopping beneﬁcial problem setting data size limited. larger training data better model performances still large enough uncover performance diﬀerences feature extraction ﬁne-tuning. figure source vgg-face; target miniplace. comparison feature extraction ﬁnetuning reduction loss function values. validation loss starting around training epoch observed training size larger sets experiments. study investigated impact target data size typical business application transfer learning training data much limited. addition also studied similarity target source data domains could aﬀect ﬁnal model performance using diﬀerent transfer learning training approaches namely feature extraction ﬁne-tuning. series experiments general larger target data size always lead better training convergence although marginal performance decreases dataset grows. precisely apparent logarithmic relationship model performance target data sizes. level similarity source target data domains also plays vital role selecting appropriate training framework. target source domains similar feature extraction gives equivalent training results ﬁne-tuning small training data settings. however ﬁne-tuning tends give better results data size increases uncovered speciﬁc target/source combination likely unique transition data size. target source domains diverge it’s crucial collect training data. feature extraction ﬁnetuning comparable throughout experiments validation performance would increase target training data suﬃciently large. data precious asset unfortunately always suﬃciently available best quality especially real-world business problems. although detailed research surely essential fully uncover quantitative impact target data size diﬀerences feature extraction ﬁne-tuning study still provides insights appropriately apply transfer learning techniques typical business application settings also motivate business explore signiﬁcance data developing accurate statistical models.", "year": 2017}