{"title": "Kernel Truncated Regression Representation for Robust Subspace  Clustering", "tag": ["cs.CV", "cs.AI"], "abstract": "Subspace clustering aims to group data points into multiple clusters of which each corresponds to one subspace. Most existing subspace clustering methods assume that the data could be linearly represented with each other in the input space. In practice, however, this assumption is hard to be satisfied. To achieve nonlinear subspace clustering, we propose a novel method which consists of the following three steps: 1) projecting the data into a hidden space in which the data can be linearly reconstructed from each other; 2) calculating the globally linear reconstruction coefficients in the kernel space; 3) truncating the trivial coefficients to achieve robustness and block-diagonality, and then achieving clustering by solving a graph Laplacian problem. Our method has the advantages of a closed-form solution and capacity of clustering data points that lie in nonlinear subspaces. The first advantage makes our method efficient in handling large-scale data sets, and the second one enables the proposed method to address the nonlinear subspace clustering challenge. Extensive experiments on five real-world datasets demonstrate the effectiveness and the efficiency of the proposed method in comparison with ten state-of-the-art approaches regarding four evaluation metrics.", "text": "abstract—subspace clustering aims group data points multiple clusters corresponds subspace. existing subspace clustering methods assume data could linearly represented input space. practice however assumption hard satisﬁed. achieve nonlinear subspace clustering propose novel method consists following three steps projecting data hidden space data linearly reconstructed other; calculating globally linear reconstruction coefﬁcients kernel space; truncating trivial coefﬁcients achieve robustness block-diagonality achieving clustering solving graph laplacian problem. method advantages closed-form solution capacity clustering data points nonlinear subspaces. ﬁrst advantage makes method efﬁcient handling large-scale data sets second enables proposed method address nonlinear subspace clustering challenge. extensive experiments realworld datasets demonstrate effectiveness efﬁciency proposed method comparison state-of-the-art approaches regarding four evaluation metrics. subspace clustering popular techniques data analysis attracted increasing interests numerous areas computer vision image analysis signal processing assumption highdimensional data lying union low-dimensional subspaces subspace clustering aims seek subspaces given data perform clustering based identiﬁed subspaces. past decades many subspace clustering methods proposed roughly classiﬁed four categories iterative approaches statistical approaches algebraic approaches spectral clustering-based approaches recent years spectral clustering-based approaches achieved state-of-the-art subspace clustering ﬁnding blockdiagonal afﬁnity matrix element matrix denotes similarity data points blockliangli zhen machine intelligence laboratory college computer science sichuan university chengdu china cercia school computer science university birmingham birmingham obtain block-diagonal afﬁnity matrix recent spectral clustering-based approaches measure similarity using so-called self-expression i.e. representing data point linear combination whole data using representation coefﬁcients build afﬁnity matrix. major difference methods constraints enforced representation coefﬁcients. example sparse subspace clustering assumes data point linearly represented points. achieve adopts ℓ-norm constraint. low-rank representation encourages coefﬁcient matrix rank capture global structures data. obtain rankness enforces nuclearnorm constraint coefﬁcients. different truncated regression representation takes frobenius norm instead ℓnuclear-norm shown promising performance many real-world applications. like existing subspace clustering algorithms major disadvantage give satisfactory clustering result data points cannot linearly represented other. fact many real-world data sampled multiple nonlinear subspaces brings challenges towards limits applications practice. group data drawn multiple nonlinear subspaces paper propose novel nonlinear subspace clustering method termed kernel truncated regression representation basic idea based following assumption i.e. exists projection space data linearly represented. illustrate simple effective idea give example fig. proposed method consists following steps projecting input another space implicit nonlinear transformation; calculating global self-expression whole data projection space data linearly reconstructed; eliminating effect errors gaussian noise zeroing trivial coefﬁcients; constructing laplacian graph using obtained coefﬁcients; solving generalized eigen-decomposition problem obtain clustering kmeans. contributions novelty work could summarized follows propose novel method cluster data points drawn multiple nonlinear subspaces. best knowledge ﬁrst nonlinear extension ﬁrst several nonlinear clustering approaches. past decades spectral clustering-based methods proposed achieve subspace clustering many applications image clustering motion segmentation gene expression analysis methods obtain block-diagonal similarity matrix nonzero elements located connections points subspace. common strategies compute similarity matrix i.e. pairwise distance-based strategy linear representationbased strategy pairwise distance-based strategy computes similarity points according pairwise relationship e.g. original spectral clustering method adopts euclidean distance heat kernel calculate similarity i.e. linear representation-based approaches assume data point could represented linear combination points intra-subspace. based assumption linear representation coefﬁcient used measurement similarity achieved state subspace clustering since encodes global structure whole data similarity. diag avoids trivial solution uses data point represent enforcing diagonal elements zeros. denotes adopted prior structured regularization major difference among existing subspace clustering methods choice example enforces sparsity adopting ℓ-norm obtains rankness using nuclear norm kck∗. achieving robustness extended follows stands errors induced noise corruption measures impact errors. generally kekf used describe gaussian noise laplacian noise respectively. k·kf denotes frobenius norm methods failed achieve nonlinear subspaces clustering. address challenging issue recent works proposed however methods following disadvantages methods computationally inefﬁcient since involve solving ℓnuclear-norm minimization problem; like methods fig. basic idea method. projecting data another space implicit nonlinear transformation method could solve problem nonlinear subspace clustering. left right plots correspond distribution data input hidden space respectively. different existing subspace clustering methods like ktrr achieves robustness eliminating impact noises projection space instead input space. words ktrr require prior structure errors competitive handle corrupted subspaces. extensive experimental results show method signiﬁcantly outperforms state-of-the-art subspace clustering algorithms regarding accuracy robustness computational cost. rest paper organized follows. section discusses related work. section presents kernel truncated regression robust subspace clustering method. section provides experimental results illustrate effectiveness efﬁciency proposed algorithm. section concludes paper. notations paper unless speciﬁed otherwise lowercase bold letters represent column vectors upper-case bold letters represent matrices entries matrices denoted subscripts. instance column vector entry. matrix entry column denotes column moreover represents transpose denotes inverse matrix stands identity matrix. table summarizes notations used throughout paper. deﬁnition dimension input data points number input data points number underlying subspaces balance parameter data point data matrix dictionary matrix data point kernel matrix input data points representation vector mapped data point linear representation coefﬁcients matrix similarity matrix among data points normalized laplacian matrix need prior errors existed data sets correct mathematical formulation. prior inconsistent real situation methods could achieve inferior performance. solve issues propose nonlinear subspace clustering method complementary existing approaches. noticed that peng recently proposed achieve nonlinearity deep structures ﬁrst works leverage deep learning subspace clustering. however beyond scope paper. φ+λi)−. solution require explicitly computed i.e. need products. therefore employ kernel functions computing products without explicitly performing mapping choices kernel shown product kernel space induced mapping notable pseudo-inverse operation needed solving representation problems data points. computational complexity calculating optimal solutions decreased data points dimensions. proved that certain condition coefﬁcients intra-subspace data points larger inter-subspace data points representing data kernel matrix handle errors performing hard thresholding operator keeps largest entries sets entries zeros like i.e. section give details proposed method consists three steps calculating kernel truncated regression representation whole data set. eliminating effectiveness possible errors noises representation building graph laplacian. obtaining clustering performing k-means algorithm leading eigenvectors graph laplacian. moreover also give computational complexity proposed method. deﬁne matrix nonlinear mapping transforms input kernel space mapping kernel space corresponding {φ}n generally believed lying linear subspaces based basic idea propose formulate objective function ktrr follows ﬁrst term reconstruction error kernel space second term serves ℓ-norm regularization positive real number controls strength ℓ-norm regularization term. algorithm learning kernel truncated regression representation robust subspace clustering input given data rm×n tradeoff parameter thresholding parameter number subspaces construct symmetric similarity matrix calculate normalised laplacian matrix compute eigenvector matrix rn×l consists ﬁrst normalized eigenvectors corresponding smallest nonzero eigenvalues. section experimentally evaluate performance proposed method. consider results terms three aspects accuracy robustness computational cost. robustness evaluated conducting experiments using samples different types corruptions i.e. gaussian noises random pixel corruption. five popular image databases used experiments including extended yale database columbia object image library columbia object image library usps mnist give details databases follows exyaleb database contains frontal face images subjects around near frontal images different illuminations individual image manually cropped normalized size pixels coil coil databases contain objects respectively. images object taken degrees apart object rotated turntable object images. size image pixels grey levels pixel given data consists data points assume points lying union low-dimensional nonlinear subspaces. propose project data points another space mapped points linearly represented mapped points intra-subspace. representation coefﬁcients require projection function explicit form needed products. induce kernel function calculate products obtain representation coefﬁcients moreover existence errors input data leads error connections among data points different subspaces. propose remove errors hard thresholding column vector coefﬁcient matrix claimed before representation coefﬁcients seen similarities among input data points. similarity intra-subspace data points large inter-subspace data points zero close zero. build similarity matrix based obtained coefﬁcient matrix matrix positive semi-deﬁnite eigenvalue equals eigenvector next calculate ﬁrst eigenvectors corresponding ﬁrst smallest nonzero eigenvalues construct matrix rn×l. finally apply k-means clustering method matrix treating vector point clustering membership. proposed subspace clustering algorithm summarized algorithm given data matrix rm×n ktrr takes compute kernel matrix takes obtain matrix calculate solutions matrices finally requires largest coefﬁcients column representation matrix putting steps together computational complexity ktrr computational complexity popular subset contains handwritten digit images experiments images normalized size pixels. experiment select samples subject database randomly following strategy mnist handwritten digit database includes classes samples total. ﬁrst handwritten digit images training subset conduct experiments images normalized size pixels. experiment also select samples subject database randomly evaluate performance different algorithms. compare ktrr state-of-art subspace clustering algorithms including truncated regression representation kernel low-rank representation kernel sparse subspace clustering latent lowrank representation low-rank representation ℓ-norm low-rank representation ℓ-norm sparse subspace clustering sparse manifold clustering embedding local subspace analysis standard spectral clustering fair comparison spectral clustering framework different similarity matrices obtained tested algorithms. like kernel-based algorithms adopt commonly used gaussian kernel datasets default bandwidth parameter mean distances samples. four popular metrics adopted evaluate subspace clustering quality i.e. accuracy normalized mutual information adjusted rand index fscore values four metrics higher method works better. values four metrics equal indicates predict results perfectly matching ground truth whereas indicates totally mismatch. evaluating clustering performance proposed method illustrate visualization results ktrr coefﬁcients matrix obtained similarity matrix. result using ﬁrst facial images exyaleb database ﬁrst samples belong ﬁrst subject samples belong second subject. parameters representation matrix constructed similarity matrix shown fig. fig. respectively. fig. upper-left part bottom-right part illuminated upper-right part bottom-left part still exist non-zero elements upper-right part bottom-left part. connections among subject much stronger among different subjects many trivial connections among samples different subjects since samples various subjects facial images common characteristics. know ideal similarity matrix spectral clustering algorithm block diagonal matrix connections exist among data points cluster hard thresholding operation executed. result similarity matrix fig. that method reveals latent structure data though images belong subjects. exist bright spots upper-right part bottomleft part obtained similarity matrix i.e. trivial connections among samples different subjects mostly removed using thresholding processing; experiment compare ktrr method state-of-the-art approaches four differi.e. extended yale database benchmark databases image library usps mnist dataset perform algorithm runs k-means clustering step repeated times report mean standard deviation used metrics. clustering quality four databases shown table table better means database highlighted boldface. statistically sound conclusions wilcoxon’s rank test signiﬁcance level adopted test signiﬁcance differences results obtained proposed method algorithms. results obtain following conclusions. evaluation exyaleb facial database fig. visualization representation matrix similarity matrix facial images ﬁrst subjects exyaleb database. representation matrix similarity matrix obtained algorithm. experiment carried ﬁrst subjects exyaleb. rows right columns illustrate images subjects. dotted lines split matrix four parts. upper-left part similarity relationship among images ﬁrst subject. bottom-right part similarity relationship among images second subject. upper-right part bottom-left part similarity relationship among images different subjects. connections easy upper-left part bottom-right part illuminated upper-right part bottom-left part means method reﬂects correct relationship among samples different subjects. linear representation methods i.e. inferior kernel-based extensions i.e. ktrr klrr kssc means non-linear representation methods suitable model exyaleb facial images. linear representation methods i.e. still inferior kernel-based extensions i.e. ktrr klrr kssc non-linear versions obtain accuracy improvements respectively. linear representation methods i.e. inferior kernel-based extensions i.e. ktrr klrr kssc performance improvement considerable e.g. accuracy kssc higher ssc. inferior kernel-based extension klrr outperforms kernel-based extension lrr. implicit transformation usps images makes mapped data points much better represented sparse representation form. ktrr algorithm achieves best results tests. accuracy ktrr higher higher klrr higher kssc. performance indices ktrr score also greater tested methods. linear representation methods i.e. inferior kernel-based extensions i.e. ktrr klrr kssc especially results poor performance database kernel-based version klrr obtains much better clustering quality regarding accuracy score. proposed method ktrr achieves best clustering result obtains signiﬁcant improvement accuracy trr. indexes score ktrr also higher tested methods. evaluate robustness proposed method conduct experiments ﬁrst subjects coil database exyaleb database respectively. used images corrupted additive white gaussian noises random pixel corruptions. corrupted image samples different levels noises shown fig. actually additional gaussian noises noises equals random pixel corruptions adopt pepper salt noises ratios affected pixels spectral-based methods relatively robust additional gaussian noises. performance latlrr sharply deteriorated databases. main reason additional gaussian noises destroyed underlie low-rank structure representation matrix. proposed ktrr considerably robust methods additional gaussian noises. speciﬁcally ktrr obtains accuracy around much higher tested algorithms especially lrr. investigated methods perform well case white gaussian noise. result consistent widely-accepted conclusion non-additive corruptions challenging additive ones pattern recognition; test algorithms perform much better coil database exyaleb database. accuracy algorithms lower exyaleb corrupted pixels. fig. pixel values images coil database close leads corruptions useless weakens impact ﬁnal clustering results. random pixel corruptions. achieves best results ratio affected pixels equals test databases. obtains accuracy around ratio affected pixels equals coil database fig. clustering results images different levels additional gaussian noises. clustering accuracy exyaleb database. clustering accuracy coil database. fig. clustering results images different ratios pepper salt corruptions. clustering accuracy exyaleb database. clustering accuracy coil database. challenging situation fig. however accuracy ktrr drops severely increase ratios corrupted pixels lower corrupted pixels. ktrr improved handle images salt pepper corruptions. investigate efﬁciency ktrr compare computational time approaches clean images four databases. hardware conﬁguration comprises .-ghz ram. time cost building similarity graph whole time cost time cost proposed method close linear version speciﬁcally method faster ktrr exyaleb usps databases slower ktrr mnist database. similar time cost coil database. computational time comparisons different methods exyaleb coil usps mnist databases. denote time cost similarity graph construction process time cost whole clustering process proposed method obtains satisfactory performance coil database. achieves perfect clustering result gets satisfactory performance accuracy score around respectively. ktrr parameters tradeoff parameter thresholding parameter selection values parameters depends data distribution. bigger suitable highly corrupted databases corresponds dimensionality corresponding subspace mapped data points. proposed method obtain satisfactory performance exyaleb database accuracy score respectively coil database accuracy score performance ktrr sensitive parameter makes ktrr suitable real applications. clustering quality exyaleb coil databases much better cases. means thresholding process helpful improve performance ktrr dimensionality underlying subspaces exyaleb coil databases hidden space belongs scope commonly used kernel functions polynomial kernels radial basis functions sigmoid kernels. investigate performance proposed method using different kernels study different kernel functions. results wilcoxon’s rank test signiﬁcance level shows signiﬁcant difference time costs ktrr similarity graph construction whole clustering process tested four databases. ktrr algorithms much faster kssc klrr methods. results consist fact theocratical computation complexities ktrr much lower kssc klrr methods. ktrr algorithms analytical solutions pseudo-inverse operation required solving representation problems data points ktrr algorithms. subsection investigate clustering performance proposed method different number subjects coil image database. experiments carried ﬁrst classes database increases interval clustering results shown fig. fig. clustering performance proposed method exyaleb database. clustering performance proposed method versus different values clustering performance proposed method versus different values clustering performance proposed method versus different values fig. clustering performance proposed method coil database. clustering performance proposed method versus different values clustering performance proposed method versus different values clustering performance proposed method versus different values outperforms usps database difκ bradley mangasarian k-plane clustering journal global optimization vol. tron vidal motion segmentation robust subspace separation presence outlying incomplete corrupted trajectories computer vision pattern recognition cvpr ieee conference ieee derksen hong wright segmentation multivariate mixed data lossy coding compression ieee transactions pattern analysis machine intelligence vol. peng tang robust subspace clustering thresholding ridge regression twenty-ninth aaai conference artiﬁcial intelligence conference proceedings peng tang zhang xiao uniﬁed framework representation-based subspace clustering outof-sample large-scale data ieee transactions neural networks learning systems vol. tron vidal motion segmentation robust subspace separation presence outlying incomplete corrupted trajectories computer vision pattern recognition cvpr ieee conference ieee peng xiao feng deep subspace clustering sparsity prior proceedings international joint conference artiﬁcial intelligence york july available http//www.ijcai.org/abstract// pollefeys general framework motion segmentation independent articulated rigid non-rigid degenerate non-degenerate european conference computer vision. springer", "year": 2017}