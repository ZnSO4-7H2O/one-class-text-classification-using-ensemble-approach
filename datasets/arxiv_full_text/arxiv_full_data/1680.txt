{"title": "Transfer Topic Modeling with Ease and Scalability", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "The increasing volume of short texts generated on social media sites, such as Twitter or Facebook, creates a great demand for effective and efficient topic modeling approaches. While latent Dirichlet allocation (LDA) can be applied, it is not optimal due to its weakness in handling short texts with fast-changing topics and scalability concerns. In this paper, we propose a transfer learning approach that utilizes abundant labeled documents from other domains (such as Yahoo! News or Wikipedia) to improve topic modeling, with better model fitting and result interpretation. Specifically, we develop Transfer Hierarchical LDA (thLDA) model, which incorporates the label information from other domains via informative priors. In addition, we develop a parallel implementation of our model for large-scale applications. We demonstrate the effectiveness of our thLDA model on both a microblogging dataset and standard text collections including AP and RCV1 datasets.", "text": "increasing volume short texts generated social media sites twitter facebook creates great demand eﬀective eﬃcient topic modeling approaches. latent dirichlet allocation applied optimal weakness handling short texts fast-changing topics scalability concerns. paper propose transfer learning approach utilizes abundant labeled documents domains improve topic modeling better model ﬁtting result interpretation. speciﬁcally develop transfer hierarchical model incorporates label information domains informative priors. addition develop parallel implementation model large-scale applications. demonstrate eﬀectiveness thlda model microblogging dataset standard text collections including datasets. social-media websites twitter facebook become novel real-time channel people share information broad range subjects. millions messages updates posted daily obvious need eﬀective ways organize data tsunami. latent dirichlet allocation bayesian hierarchical model capture text generation process shown powerful modeling text corpus. however several major characteristics distinguish social media data traditional text corpus raise great challenges model. first post tweet limited certain number characters result abbreviated syntax often introduced. second texts noisy broad topics repetitive less meaningful content. third input data typically arrives high-volume streams. known topic outputs completely depend word distributions training documents. therefore results blog microblogging data would naturally poor cluster words co-occur many documents without actual semantic meanings intuitively generative process model guided document labels learned hidden topics meaningful. therefore discriminative training explored particular applied labeled analyze twitter data using hashtags labels. approach addresses challenges certain extent. even though supervised gives comprehensible topics general limitation framework document represented single topic distribution. makes comparing documents diﬃcult sparse noisy documents constantly changing topics. furthermore difﬁcult obtain labels continuously growing text collections like social media mention fact twitter application many hashtags refer broad topics therefore could even misleading used guide topic models. proposed hierarchical model generates topic hierarchies inﬁnite number topics represented topic tree document assigned path tree root tree leaves. hlda capability encode semantic topic hierarchies noisy sparse data user generated short tweet messages robust results lack meaningful interpretations recently hlda studied summarize multiple documents. built two-level learning model using hlda model discover similar sentences given summary sentences using hidden topic distributions document. distance dependent studied several types decay window decay exponential decay logistic decay customers’ table assignments depend external distances them. paper aims bridge short noisy texts actual generation process without additional labeling eﬀorts. time develop parallel algorithms speed inference model applicable large-scale applications. paper propose simple solution model transfer hierarhicial model. basic idea extracting human knowledge topics source domain corpus form representative words consistently meaning across contexts media encode priors learn topic hierarchies target domain corpus hlda model. extract source domain corpus thlda model utilizes related labeled documents sources encode laels modiﬁed nested chinese restaurant process guidance inferring latent topics target domains. base model hierarchical mainly hlda natural capability encode semantic topic hierarchies document clusters. addition recent study suggests hierarchical dirichlet process provides eﬀective explanation model human transfer learning latent dirichlet allocation gained popularity automatically extracting representation corpus. completely unsupervised model views documents mixture probabilistic topics represented dimensional random variable generative story document generated ﬁrst picking topic distribution dirichlet prior document’s topic distribution sample latent topic variables makes assumption word generated topic latent variable indicating hidden topic assignment word probability choosing word topic appropriate labeled corpora extended several ways incorporate supervised label learning process. ramage introduced labeled novel model multi-labeled corpora address credit assignment problem. unlike labeled constrains topics documents given label set. instead using symmetric dirichlet distribution single hyper-parameter dirichlet prior topic distribution restricted topics correspond observed labels authors proposed stochastic processes bayesian inference longer restricted ﬁnite dimensional spaces. unlike restrict given number topics allows arbitrary breadth depth topic hierarchies. topics hlda model represented topic tree document assigned path tree root tree leaves. document generated ﬁrst sampling path along topic tree sampling topic among topic nodes path word authors proposed nested nested chinese restaurant process implies ﬁrst customer sits ﬁrst table customer sits table drawn equations. path depth sampled number topic nodes along path document sample topic among topic nodes path word based distribution. experiment result hlda shows document generating story actually encode semantic topic hierarchies. transfer learning extensively studied past decade leverage data task help another summary types transfer learning problems i.e. shared label space shared feature space. shared label space main objective transfer label information observations diﬀerent distributions uncover relations multiple labels better prediction shared feature space representative works self-taught learning uses sparse coding construct higher-level features abundant unlabeled data help improve performance classiﬁcation task limited number labeled examples. unsupervised generative model possesses advantage modeling generative procedure whole dataset establishing relationships documents associated hidden topics hidden topics concrete words un-supervised way. intuitively transfer learning generative model realized ways utilizing document labels domain learned hidden topics much meaningful utilizing documents domains enrich contents learn robust shared latent space. authors proposed discriminative adds supervised information original model guides generative process documents using labels. methods clearly demonstrate advantages discriminative training generative models. however diﬀerent transfer learning since simply utilize labeled documents domain help build generative model. motivation applied transfer learning supervised information used guide generation common latent semantic space shared source domain target domain. transferring information source target domain extremely desirable social media analysis target domain example features sparse missing features. based shared common latent semantic space missing features recovered extent helpful better representing examples. content analysis social media data challenging problem unique language characteristics prevents standard text mining tools used full potential. several models developed overcome barrier aggregating many messages applying temporal attributes examining entities applying manual annotation guide topic generation main motivation work previous unsupervised approaches analyzing social media data fail achieve high accuracy noise sparseness supervised approaches require annotated data often requires signiﬁcant amount human eﬀort. even though hierarchical natural capability encode semantic topic hierarchies clusters similar documents based hierarchies still cannot provide robust result noise sparseness exchangeability assumption dirichlet process mixture exchangeability essential mixture equivalent mixture; thus customers exchangeable. however assumption reasonable applied microblogging dataset. based experiment data noisy sparse unrelated words tends cluster word co-occurrences.) consider task modeling short sparse documents based speciﬁc source domain structure. example user interested browsing documents particular categories topics might prefer clusters documents based category. clustering target domain transferring topic hierarchy category source domain could produce better document clusters topic hierarchies leveraging context information source domain. user generated categories found various source domains twitter list flickr collection del.icio.us hierarchy wikipedia news categories. transfer source domain knowledge target domain documents assigning prior path sequence assigned topic nodes root leaf node. prior paths documents used identify whether target documents similar model could group similar documents cluster together keep diﬀerent documents separate based label. label document’s prior path source domain hierarchy generate word vectors nodes source domain hierarchy. label generated measuring similarity source domain topic hierarchy document. many ways measure similarity vectors cosine similarity euclidean distance jaccard index simplicity paper label prior knowledge target document computing cosine similarity between target document node source domain hierarchy. start root node hierarchy keep assigning similar topic node level considering child nodes currently assigned topic nodes next level candidates. incorporated label hierarchies model changing prior path hlda path selection favors ones existing label hierarchies. similar original hlda thlda models document mixture topics path generates word topics. original hlda model path prior nested chinese restaurant process probability choosing topic topic layer depends number documents already assigned node layer. nodes assigned documents higher probability total number labels document total number documents weighted indicator variable controls strength prior added original nested chinese restaurant process. graphical model model shown whether customer sits speciﬁc table related many customers currently sitting table often customer chooses table also related close current customer customers table note that work diﬀerent topic simplicity; however table speciﬁc diﬀerent prior diﬀerent table applied sophisticated model. number level paths drawn modiﬁed nested crp. represents restaurant correponding topic distribution document distribution deﬁned modiﬁed nested conditioned previous assume table restaurant assigned parameter vector multinomial topic distribution vocabulary topic dirichlet prior also assume words generated mixture model speciﬁc random distribution document. document drawn ﬁrst choosing level path modiﬁed-nested drawing words topics associated restaurants along path. refers binary label presence indicators label prior label thlda able transfer source domain knowledge topic hierarchy making assignment related many documents assigned topic also close current topic documents topic based source domain knowledge unseen topics source domain knowledge transfer source domain assign probability proportional number documents already assigned topic unlike transferring knowledge labeled entities given source domain knowledge model learns unlabeled labeled data based different prior probability equations modiﬁed nested chinese restaurant process imagined following scenario. suppose inﬁnite number inﬁnite-table chinese restaurants city headquarter restaurant. note table every restaurant refers restaurants restaurant referred once. starting headquarter restaurant restaurants connected branching tree. think table door restaurants unless current restaurant leaf node restaurant tree. starting root restaurant reach ﬁnal destination table leaf node restaurant customers restaurant share path. customer arrives instead following original nested inference procedure thlda model similar hlda except modiﬁcation path prior. gibbs sampling scheme document corpus ﬁrst sample path topic tree based path sampling rest documents corpus ﬁrst term distribution levels second term word emission probability given topic assignment word. thing need change inference scheme path prior probability path sampling step. developed thlda parallel approximate inference algorithm independent processors facilitate learning eﬃciency. words split data parts implement thlda processor performing gibbs sampling partial data. however gibbs sampler requires sample step conditioned rest sampling states hence introduce tree merge stage help gibbs samplers share sampling states periodically independent inference processes. vectors topic allocations process excluding word document process excluding note separate processor need total vocabulary size number words assigned topic global state crp. merge topic assignment count table single counts gibbs iteration global sampling state shared among processes. conditional distribution prior likelihood data given particular choice computed locally. note that compute second term needs known global state documents’ path assignment number instances word assigned topic index tree. given number inﬁnite trees chinese restaurants pick tree base tree recursively merge topics remaining trees base tree top-down manner. topic node merged similar node base tree parent node. thlda model diﬀerent existing topic models. llda incorporate supervision restricting document’s label set. wordtopic assignments restricted given labels. number unique labels llda number topics lda. unlike llda thlda directly correlate label topic modifying number topics determined number labels number serves guidance inferring latent topics. proposed thlda model signiﬁcant overcome barrier unsupervised models applied noisy sparse data. transferring diﬀerent domain knowledge thlda also saves time cumbersome annotation eﬀorts required supervised models. thlda advantage model producing topic hierarchy document clusters without additionally computing similarity among topic distribution documents. furthermore thlda oﬀers major advantages supervised semisupervised models providing mixture detailed topic hierarchy certain level unsupervised providing level topic structure guided given prior knowledge. applying prior knowledge supervised unsupervised ways apply thlda learn deeper level topic hierarchy given depth source domain prior hierarchy. following experiment show performance thlda combination supervised prior knowledge upto certain level unsupervised level. experiments used source domain three target domain text data sets show eﬀectiveness transfer hierarchical model. used well known text collections associated press data reuters corpus volume data sparse noisy microblog data twitter target domains yahoo news categories source domain. used crawler fetch news titles categories yahoo news science business health sports politics world technology entertainment shown parsed stemmed news titles computed tf-idf score generate weighted word vector topic category. computed topic category score using tf-idf weighted word vector picked optimal category level label target document. text retrieval conference contains associated press news stories original data includes documents categories. sample data sampled subset trec corpus contains documents vocabulary size unique terms. divided documents observed documents held-out documents measure held-out predicrcv archive manually categorized newswire stories provided reuters ltd. distributed on-line appendices jmlr article. also includes categories associated industries regions. work subset data used. sample data documents vocabulary size unique terms. randomly divided observed documents held-out documents experiments. weeks obtained around user proﬁles tweets. twitter users structure conventions user-to-message relation type message type resources overcome character limit. capture trending topics many applications analyze twitter data group similar trending topics using structure information shows list trending topics. however according tweets contain hashtag containing also tweets include url. work instead using structure information tweet used words. removed structural information initial tweet authors stemmed word transform word base form using porter stemmer. experiment randomly sampled used documents vocabulary size unique terms. randomly divided observed documents held-out documents. labeled implemented llda using standard collapsed gibbs sampling method. compare learned topic results supervised unsupervised topic models standard topics llda topics yahoo news level categories topic freedom topic. results show main observations ﬁrst multiple topics standard mapped popular topics llda; second llda topics discovered standard hierarchical used hlda-c ﬁxed depth tree stick breaking prior depth weights. topic hierarchy generated hlda shown unsupervised model hlda gives result totally depends term co-occurance documents. hlda gives topic hierarchy easily understood human beings tweet contains small number terms co-occurance would sparse less relevant compared long documents. nodes topic hierarchy capture clusters words input documents topic column words focusing smart phones nodes column covers online multimedia resources. however level topic nodes less informative relationship child nodes level parent nodes level less semantically meaningful. topics belong parent level usually relate result. ideally work documents long length dense word distribution overlapping. however hlda gives less interpretable results noisy sparse data. transfer hierarchical implemented standard thlda modifying hlda-c. shows important advantages model ﬁrst topic nodes better interpretable transferring source domain knowledge. second child topics reﬂect strong semantic relationship parent. example topic world ﬁrst child iraq wikileak death relates iraq topic second child police kill injury drup relates criminals interestingly child topic chile rescue miner denotes recent event traped miners chile. note impose prior knowledge level level topics automatically emerged data set. science topics child topic space nasa moon astronomy child topic gulf spill environment recent gulf spill. furthermore example tweets assigned topic nodes show strong association tweet clustering. quantitatively measure performance thlda used predictive held-out likelihood. divided corpus observed held-out approximate conditional probability held-out given training set. make fair comparison applied hyper parameters exist three models applied diﬀerent hyper parameter obtain similar number topics hlda thlda models. following used outer samples taking samples iterations apart using burn-in samples. collected samples latent variables given held-out documents computed conditional probability given outer sample harmonic mean. illustrates held-out likelihood hlda thlda twitter corpus. ﬁgure applied ﬁxed topic cardinality ﬁxed depth hierarchy hlda thlda. thlda always provides better predictive performance hlda three cases interestingly thlda provides signiﬁcantly better performance twitter data hlda shows poor performance lda. blei showed eventually large numbers topics dominate hlda thlda predictive performance however thlda performs better predictive performance reasonable numbers topics. manual evaluation tweet assignments learned topics randomly selected tweets manually annotate correctness. pick highest assigned topic hlda thlda parallel-thlda third level topic node used accuracy respectively. table shows example tweets assigned topic hlda thlda. likelihood training data gibbs sampling iterations shown cases gibbs sampling converges distribution similar likelyhood. shows speedup parallel inference method. addition overhead topic tree merging stage system also suﬀers overhead state loading saving time similar since occur everytime need update global tree. overheads system performs better merging step large. merge topic trees every gibbs iterations speedup processes times faster process merge topic trees every gibbs iterations speedup times. overhead roughly seen extend lines intersect y-axes. since merging stage complexity linear number topic trees merged greater overhead process experiment. however merging algorithm complexity depend size dataset means merging overhead ignorable huge datasets. paper proposed transfer learning approach eﬀective eﬃcient topic modeling analysis social media data. speciﬁcally developed transfer hierarchical model extension hierarchical model inferred topic distributions documents incorporating knowledge domains. addition designed parallel inference framework parallel gibbs sampler synchronously multi-core machines perform topic modeling large-scale datasets. work signiﬁcant among frontier approaches explore knowledge transfer domains topic modeling largescale microblog analysis. future work interested exploring eﬀective approaches transfer domain knowledge addition topic priors examined current paper. research sponsored u.s. defense advanced research projects agency anomaly detection multiple scales program agreement number wnf--c-. views conclusions contained document author interpreted representing oﬃcial policies either expressed implied u.s. defense advanced research projects agency u.s. government. u.s. government authorized reproduce distribute reprints government purposes notwithstanding copyright notation hereon.", "year": 2013}