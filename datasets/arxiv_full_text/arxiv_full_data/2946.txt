{"title": "Pattern Recognition for Conditionally Independent Data", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "In this work we consider the task of relaxing the i.i.d assumption in pattern recognition (or classification), aiming to make existing learning algorithms applicable to a wider range of tasks. Pattern recognition is guessing a discrete label of some object based on a set of given examples (pairs of objects and labels). We consider the case of deterministically defined labels. Traditionally, this task is studied under the assumption that examples are independent and identically distributed. However, it turns out that many results of pattern recognition theory carry over a weaker assumption. Namely, under the assumption of conditional independence and identical distribution of objects, while the only assumption on the distribution of labels is that the rate of occurrence of each label should be above some positive threshold.  We find a broad class of learning algorithms for which estimations of the probability of a classification error achieved under the classical i.i.d. assumption can be generalised to the similar estimates for the case of conditionally i.i.d. examples.", "text": "work consider task relaxing i.i.d assumption pattern recognition aiming make existing learning algorithms applicable wider range tasks. pattern recognition guessing discrete label object based given examples consider case deterministically deﬁned labels. traditionally task studied assumption examples independent identically distributed. however turns many results pattern recognition theory carry weaker assumption. namely assumption conditional independence identical distribution objects assumption distribution labels rate occurrence label positive threshold. broad class learning algorithms estimations probability classiﬁcation error achieved classical i.i.d. assumption generalised similar estimates case conditionally i.i.d. examples. pattern recognition informally following task. ﬁnite number classes complex objects. predictor learning label objects according class belong based examples typical practical examples recognition hand-written text. case object hand-written letter label letter alphabet represents. examples include sequence identiﬁcation recognition illness based symptoms speech recognition many others. distribution labels given object according function space labels assumed ﬁnite task construct best predictor labels based data observed i.e. actually learn task usually considered either following settings. oﬀ-line setting examples divided ﬁnite subsets training testing set. predictor constructed based ﬁrst used classify objects second. online setting predictor starts classifying ﬁrst object zero knowledge; given correct label proceeds classifying second object correct second label given plenty algorithms developed solving pattern recognition tasks widely used methods). however i.i.d assumption central model tight many applications. turns also tight wide range methods developed assumptions model work nearly well weaker conditions. show following assumptions distribution examples suﬃcient pattern recognition. first dependence objects labels type object-label dependence change time. unknown distribution inﬁnite sequences labels. type dependence labels; moreover assume dealing combinatorial sequence labels. however sequence rate occurrence label keep positive threshold. label corresponding main diﬀerence i.i.d. model conditional model made distribution labels primal; done relax requirement independence objects conditional independence. work provide tool obtaining estimations probability error predictor conditional model estimation probability error i.i.d. model. general theorems extending results concerning performance predictor conditional model illustrated classes predictors. first extend weak consistency results concerning partitioning nearest neighbour estimates i.i.d. model conditional model. second results vapnik-chervonenkis theory estimate performance conditional model predictors minimising empirical risk also obtain strong consistency results. results obtained applications following rule. assumption predictor predictor works model well i.i.d. model call tolerance data large dataset small subset strongly changes probability error. property also hold respect permutations. assumption predictor valid i.i.d. model. thus results achieved i.i.d. model extended conditional model; concerns distribution–free results well distribution–speciﬁc results performance ﬁnite samples well asymptotic results. various approaches relaxing i.i.d. assumption learning tasks proposed literature. thus authors study nearest neighbour kernel estimators task regression estimation continuous regression function assumption labels conditionally independent given objects objects form individual sequence. another approach considered regression estimation scheme proposed consistent individual stable sequence object-label pairs assuming known upper bound variation regression function. also several approaches diﬀerent types assumptions joint distribution objects labels made; authors construct predictor class predictors work well assumptions made. thus generalisation approach markov chains ﬁnite countable state space presented. estimates probability error constructed cases assumption optimal rule generating examples belongs pre-speciﬁed class decision rules. also track research prediction assumption distribution generating examples stationary ergodic. basic diﬀerence learning task apart diﬀerent probabilistic assumption concerned object-label dependence predicting ergodic sequences label-label dependence primary interest. task references therein. another approach taken model generalised allow concepts changing time. methodology proposed track time series dependences authors classes dependences exploited learning. diﬀerence approach class problems time series dependence ignored reasonable pattern recognition method rather constructing methods speciﬁc dependences kind. consider sequence examples example consists object label measurable space called object space called label space deterministic function. simplicity case ﬁnite space notation used measurable space examples. objects drawn according probability distribution thus consider case deterministically deﬁned labels section discuss possible generalisations. notation used distributions symbol latter case denotes i.i.d. reserved distributions distribution generated correspondingly symbols expectations spaces letters used elements spaces correspondingly letters reserved random variables spaces. note ﬁrst condition means objects conditionally independent given labels conditions objects conditionally independent identically distributed assumptions conditional model also interpreted follows. assume individual sequence labels probability distributions exists sets example drawn according distribution pyn; examples drawn independently other. tolerance data means eﬀect typical large portion data small portion changes strongly probability error. property also hold respect permutations. various notions similar tolerance data studied literature. perhaps ﬁrst appeared connection deleted condensed estimates later called stability present studies diﬀerent kinds stability extensive overviews). naturally notions arise need study behaviour predictor training examples removed. notions much similar call tolerance data interested maximal deviation probability error usually average minimal deviations estimated. predictor developed work oﬀ-line setting loosely speaking tolerant small changes training sample. next theorem shows conditions property predictor utilised. proofs section found appendix theorem says know conﬁdence rate occurrence label less bounds error rate tolerance data predictor i.i.d. model obtain bounds error rate conditional model. thus tool estimating performance predictor ﬁnite step section show result applied predictors minimising empirical risk. however interested asymptotic results formulations somewhat simpliﬁed. observe generalise results concerning weak consistency nearest neighbour non-data-dependent partitioning rules. general results exist particular data-dependent rules. however generalise state-of-the-art results nonparametric classiﬁcation rather illustrate weak consistency results extended conditional model. section show estimate performance predictor minimising empirical risk using theorem estimate tolerance data predictors using results vapnik-chervonenkis theory. overviews vapnik-chervonenkis theory basic results vapnik-chervonenkis theory estimation diﬀerence probabilities error best possible function class function minimises empirical error introduced conditionally i.i.d. model pattern recognition generalises commonly used i.i.d. model. naturally question arises whether conditions distributions predictors necessary generalised direction. section discuss conditions model point view. ﬁrst question results obtained without assumptions tolerance data? following negative example shows bounds tolerance data necessary. another point requirement frequencies labels. particular assumption might appear redundant rate occurrence label tends zero ignore label without aﬀecting asymptotic? appears case following example illustrates. next construct distribution p|y∞. assume according ﬁrst label always object irrational number). next labels always follows zeros easy check exists sequence probability least another question whether results generalised case non-deterministically deﬁned labels often considered literature. noted consider task learning object-label dependence ignoring label-label dependence hand allows consider sort labellabel dependence. hand best bound probability error obtain maximum class-conditional probabilities error so-called bayes error best achievable bound i.i.d. case. observe predictors question probability error given true label decrease arbitrary portion training examples labelled ones replaced arbitrary portion size examples labelled model theorem know en+|ˆηn thus e|ˆηn e|ˆηn−η| markov inequality obtain union cells clearly ﬁxed moreover since always either change decision cell need remove least examples clearly", "year": 2005}