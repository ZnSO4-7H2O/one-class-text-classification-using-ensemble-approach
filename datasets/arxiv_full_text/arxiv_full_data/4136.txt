{"title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense  Image Annotations", "tag": ["cs.CV", "cs.AI"], "abstract": "Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked \"What vehicle is the person riding?\", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that \"the person is riding a horse-drawn carriage\".  In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.", "text": "ranjay krishna yuke oliver groth justin johnson kenji hata joshua kravitz stephanie chen yannis kalantidis li-jia david shamma michael bernstein fei-fei image. asked what vehicle person riding? computers need identify objects image well relationships riding pulling order answer correctly person riding horse-drawn carriage. paper present visual genome dataset enable modeling relationships. collect dense annotations objects attributes relationships within image learn models. speciﬁcally dataset contains images image average objects attributes pairwise relationships objects. canonicalize objects attributes relationships noun phrases region descriptions questions answer pairs wordnet synsets. together annotations represent densest largest dataset image descriptions objects attributes relationships question answers. holy grail computer vision complete understanding visual scenes model able name detect objects describe attributes recognize relationships interactions. understanding scenes would enable important applications image search question answering robotic interactions. much progress made recent years towards goal including image classiﬁcation shown objects attributes relationships shown dataset also contains image related question answer pairs object detection important contributing factor availability large amount data drives statistical models underpin today’s advances computational visual understanding. progress exciting still reaching goal comprehensive scene understanding. figure shows existing models would able detect discreet objects photo would able explain interactions relationships them. explanations tend cognitive nature integrating perceptual information conclusions relationships objects scene cognitive understanding visual world thus requires complement computers’ ability detect objects abilities describe objects increasing eﬀort together next generation datasets serve training benchmarking datasets deeper cognitive scene understanding reasoning tasks notable ms-coco ms-coco dataset consists real-world photos collected flickr. image pixel-level segmentation object classes independent user-generated sentences describing scene. adds question-answer pairs related visual contents image information ms-coco provide fertile training testing ground models aimed tasks accurate object detection segmentation summary-level image captioning well basic example stateof-the-art model provides description ms-coco image figure standing next elephant. missing understanding object person doing relationship person elephant etc. without relationships models fail diﬀerentiate image images people next elephants. understand images thoroughly believe three elements need added existing datasets grounding visual concepts language complete descriptions image based multiple image regions formalized representation components image spirit mapping complete information visual world introduce visual genome dataset. ﬁrst release visual genome dataset uses images intersection yfccm ms-coco section provides detailed description dataset. highlight motivation contributions three elements visual genome apart existing datasets. visual genome dataset regards relationships attributes ﬁrst-class citizens annotation space addition traditional focus objects. recognition relationships attributes important part complete understanding visual scene many cases elements story scene visual genome dataset among ﬁrst provide detailed labeling object interactions attributes grounding visual concepts language. image often rich scenery cannot fully described summarizing sentence. scene figure contains multiple stories taking photo elephants woman feeding elephant river background lush grounds etc. existing datasets flickr ms-coco focus high-level descriptions image. instead image visual genome dataset collect descriptions diﬀerent regions image providing much denser complete descriptions scene. addition inspired also collect average question-answer pairs based descriptions image. regionbased question answers used jointly develop vision models answer questions either description image them. dense descriptions image explicit correspondences visual pixels textual descriptors visual genome dataset poised ﬁrst image dataset capable providing structured formalized representation image form widely used knowledge base representations example figure formally express relationship holding between woman food holding). putting together objects relations scene represent image scene graph scene graph representation shown improve semantic image retrieval image captioning furthermore objects attributes relationships image visual genome dataset canonicalized corresponding wordnet mapping connects images visual genome provides eﬀective consistently query concept dataset. also potentially help train models learn contextual information multiple images. paper introduce visual genome dataset training benchmarking next generation computer models comprehensive scene understanding. paper proceeds follows section provide detailed description component dataset. section provides literature review related datasets well related recognition tasks. section discusses crowdsourcing strategies deployed ongoing eﬀort collecting dataset. section collection data analysis statistics showcasing properties visual genome dataset. last least section provides experimental results visual genome benchmark. fig. example image visual genome dataset. show region descriptions corresponding region graphs. also show connected scene graph collected combining image’s region graphs. region description woman park bench along river. contains objects woman bench river. relationships connect objects sits front sits fig. example image dataset along scene graph representation. scene graph contains objects localized image bounding boxes objects also attributes large green behind etc. finally objects connected relationships wears wears etc. fig. representation visual genome dataset. image contains region descriptions describe localized portion image. collect types question answer pairs freeform region-based qas. region converted region graph representation objects attributes pairwise relationships. finally region graphs combined form scene graph objects grounded image. best viewed color visual genome dataset consists seven main components region descriptions objects attributes relationships region graphs scene graphs questionanswer pairs. figure shows examples component image. enable research comprehensive understanding images begin collecting descriptions question answers. texts without restrictions length vocabulary. next extract objects attributes relationships descriptions. together objects attributes relationships fabricate scene graphs represent formal representation image. section break figure explain seven components. section describe detail data component collected crowdsourcing platform. real-world image simple summary sentence often insuﬃcient describe contents interactions image. instead natural extend might collection descriptions based diﬀerent regions scene. visual genome collect human-generated image region descriptions region localized bounding box. figure show three examples region descriptions. regions allowed high degree overlap descriptions diﬀer. example yellow hydrant woman shorts standing behind little overlap jumping hydrant high overlap regions. dataset contains average total region descriptions image. description phrase ranging words length describing region. image dataset consists avarege objects delineated tight bounding furthermore object canonicalized synset wordnet example person would mapped man.n. similarly person gets mapped person.n. afterwards concepts joined person.n. since hypernym man.n.. important standardization step avoid multiple fig. describe contents interactions image visual genome dataset includes multiple human-generated image regions descriptions region localized bounding box. here show three regions descriptions jumping hydrant yellow hydrant woman shorts standing beghind man. image visual genome average attributes. objects zero attributes associated them. attributes color states etc. like extract objects region descriptions also extract attributes attached objects. figure phrase yellow hydrant extract attribute yellow fire hydrant. objects canonicalize attributes wordnet example yellow mapped yellow.s. fig. descriptions also provide attributes objects. example region description yellow hydrant adds fire hydrant yellow. show attributes yellow standing. object fire hydrant. relationship canonicalized wordnet synset i.e. jumping canonicalized jump.a. average image dataset contains relationships. combining objects attributes relationships extracted region descriptions create directed graph representation regions. examples region graphs shown figure region graph structured representation part image. nodes graph represent objects attributes relationships. objects linked respective attributes relationships link object another. links connecting objects figure point subject relationship relationship object. comparative prepositional phrases example region description jumping hydrant extract relationship jumping objects fire hydrant relationships directed object called subject another called object. case subject performing relationship jumping region graphs localized representations image also combine single scene graph representing entire image scene graph union region graphs contains objects attributes relationships region description. able combine multiple levels scene information coherent way. example figure leftmost region description tells hydrant yellow middle region description tells jumping hydrant. together descriptions tell jumping yellow hydrant. types pairs associated image dataset freeform based entire image region-based based selected regions image. collect diﬀerent types questions image what where when why. figure woman standing next to?; belongings freeform image least question type listed above. regionbased collected prompting workers region descriptions. example region yellow hydrant collect region-based color hydrant?; yellow. region based allow independently study methods vision priors answer questions. discuss existing datasets released used vision community classiﬁcation object detection. also mention work improved object attribute detection models. then explore existing work utilized representations similar relationships objects. addition dive literature related cognitive tasks like image description question answering knowledge representation. datasets growing size researchers begun tackling increasingly complicated problems. caltech ﬁrst datasets hand-curated image classiﬁcation object categories examples category. biggest criticisms caltech lack variability examples. caltech increased number categories also addressing shortcomings caltech however still handful examples category images contained single object. labelme introduced dataset multiple objects category. also provided interface experts novices could annotate additional images. interface enabled images labeled polygons helping create datasets image segmentation. lotus hill dataset contains hierarchical decomposition objects along segmentations. small part dataset freely available. like labelme lotus hill curated object detection. pushing size datasets even further million tiny images created signiﬁcantly larger dataset predecessors. contains tiny synsets queries. however data million images human-veriﬁed contain numerous errors. yfccm another large database million images still largely unexplored. contains human generated machine generated tags. pascal pushed research classiﬁcation object detection dataset containing semantic categories images. imagenet took wordnet synsets crowdsourced large dataset million images. started ilsvrc challenge variety computer vision tasks. ilsvrc pascal provide test bench object detection image classiﬁcation object segmentation person layout action classiﬁcation. mscoco recently released dataset images sentence descriptions segmentations object categories. current largest dataset contains images annotated question answers. collected dataset freeform questions ground truth answers provided baseline approach answering questions using image textual question input. visual genome aims bridge datasets collecting annotations large number objects also scene graphs region descriptions question answer pairs image regions. unlike previous datasets collected single task like image classiﬁcation visual genome dataset collected general-purpose representation visual world without bias toward particular task. images contain average objects almost order magnitude dense existing vision dataset. similarly contain average attributes relationships table comparison existing datasets visual genome. show visual genome order magnitude descriptions question answers. also diverse object attribute relationship classes. additionally visual genome contains higher density annotations image. image. also order magnitude unique objects attributes relationships dataset. finally million question answer pairs also larger dataset visual question answering. core contributions visual genome descriptions multiple regions image. such mention image description datasets models subsection. work related describing images divided categories retrieval human-generated captions generation novel captions. methods ﬁrst category similarity metrics image features predeﬁned models retrieve similar sentences methods sentences images common vector space space triples among second category common theme recurrent neural networks produce novel captions recently researchers also used visual attention model drawback approaches attention describing salient aspect image. problem ampliﬁed datasets like flickr ms-coco whose sentence desriptions tend focus someredundantly salient parts. example elephant seen wandering around sunny large elephant tall grass ﬁeld large elephant standing alone brush descriptions ms-coco dataset focus salient elephant image ignore regions image. many real-world scenes complex multiple objects interactions best described using multiple descriptions dataset pushes toward complete understanding image collecting dataset capture scene-level descriptions also myriad low-level descriptions grammar scene. self-driving cars road. involves classifying object semantic category localizing object image. visual genome uses objects core component visual scene built. early datasets include face detectio pedestrian datasets pascal ilsvrc’s detection dataset pushed research object detection. images datasets iconic capture settings objects usually co-occur. remedy problem ms-coco annotated real-world scenes capture object contexts. however ms-coco unable describe objects images since annotated object categories. real world many objects ones captured existing datasets. visual genome aims collecting annotations visual elements occur images increasing number semantic categories inclusion attributes allows describe compare easily categorize objects. even haven’t seen object before attributes allow infer something example yellow brown spotted long neck likely refers giraﬀe. initial work area involved ﬁnding objects similar features using examplar svms. next textures used study objects methods learned predict colors finally study attributes explicitly demonstrated lead improvements object classiﬁcation attributes deﬁned paths shapes materials could used classify categories objects. attributes also played large role improving ﬁne-grained recognition ﬁne-grained attribute datasets like cub- visual genome generalized formulation extend attributes image-speciﬁc binaries rather object-speciﬁc object real-world scene. also extend types attributes include size pose state emotion many more. dependency tree methods deep neural networks employed extract relationships entities sentence. however computer vision little work gone learning predicting relationships. instead relationships implicitly used improve vision tasks. relative layouts objects improved scene categorization spatial geometry objects helped object detection comparative adjectives prepositions pairs objects used model visual relationships improved object localization relationships already shown utility improving cognitive tasks. meaning space relationships improved mapping images sentences relationships structured representation objects deﬁned graph structure called scene graph nodes objects attributes edges relationships objects. representation used generate indoor images sentences also improve image search similar scene graph representation image generalizes across previous works recently relationships come focus form question answering associations objects questions relationship involving generally objects true e.g. dogs cream?. believe relationships necessary higher-level cognitive tasks collect largest corpus attempt improve tasks actually understanding relationships objects. visual question answering recently proposed proxy task evaluating computer vision system’s ability understand image beyond object recognition several visual benchmarks proposed last months. daquar dataset ﬁrst toy-sized benchmark built upon indoor scene rgb-d images depth datasets collected pairs ms-coco images either generated previous datasets questions concentrated simple recognition-based questions salient objects answers often extremely short. instance daquar answers answers consist single-word object names attributes quantities. shortness limits diversity fails capture long-tail details images. given availability datasets array visual models proposed tackle tasks. proposed models range classiﬁers probabilistic inference recurrent neural networks convolutional networks visual genome aims capture details images diverse question types long answers. questions cover wide range visual tasks basic perception complex reasoning. dataset million also larger currently existing dataset. knowledge representation visual world capable tackling array vision tasks action recognition general question answering. however diﬃcult answer what minimal viable knowledge needed understand physical world? later proposed certain plurality concepts related axioms eﬀorts grown model physical processes model series actions scripts stories—both depicted single static image play roles image’s story. recently nell learns probabilistic horn clauses extracting information web. deepqa proposes probabilistic question answering architecture involving diﬀerent techniques. others used markov logic networks representation perform statistical inference knowledge base construction. work similar attempt learn common-sense relationships images. visual genome scene graphs also considered dense knowledge representation images. similar format used knowledge bases nlp. visual genome’s main goal enable study cognitive computer vision tasks. next step towards understanding images requires studying relationships objects scene graph representations images. however observed collecting scene graphs directly image leads workers annotating easy frequently-occurring relationships like wearing instead focusing salient parts image. evident previous datasets contain large number relationships. experimentation observed asked describe image using natural language crowd workers naturally start salient part image move describing parts image one. inspired ﬁnding focused attention towards collecting dataset region descriptions diverse content. image added crowdsourcing pipeline annotations sent worker asked draw three bounding boxes write three descriptions region enclosed box. next image sent another worker along previously written descriptions. workers explicitly encouraged write descriptions written before. process repeated collect region descriptions image. prevent workers skim long list previously written descriptions show seven similar descriptions. calculate similar descriptions using bleu scores pairs sentences. deﬁne bleu score description previous description visual genome collected veriﬁed entirely crowd workers amazon mechanical turk. section outline pipeline employed creating components dataset. component involved multiple task stages. mention diﬀerent strategies used make data accurate enforce diversity component. also provide background information workers helped make visual genome possible. used amazon mechanical turk primary source annotations. overall total unique workers contributed dataset. dataset collected course months months experimentation iteration data representation. approximately human intelligence tasks launched involved creating descriptions questions answers region graphs. designed workers manage earn anywhere hour work continuously line ethical research standards mechanical turk visual genome hits achieved retention rate meaning workers completed tasks went ahead more. table outlines percentage distribution locations workers. workers contributed united states. figures outline demographic distribution crowd workers. majority workers ages years old. youngest contributor years oldest years old. also near-balanced split male female workers. global image descriptions. list common written descriptions images dataset. prevents common phrases like blue dominating region descriptions. finally workers draw bounding boxes satisfy requirement coverage. bounding must cover objects mentioned description. figure shows example good covers street well mentioned description well example box. region descriptions collected image extract visual objects description. description sent crowd worker extracts objects description grounds object bounding image. example figure let’s consider description woman shorts standing behind man. worker would extract three objects woman shorts man. would draw around objects. require bounding drawn satisfy requirements coverage quality. coverage deﬁnition described section workers make sure bounding covers object completely quality requires bounding tight possible around object box’s length height decreased pixel would longer satisfy coverage requirement. since pixel error physically impossible workers relax deﬁnition quality four pixels. multiple descriptions image might refer object sometimes diﬀerent words. example description might referred person another description. thus crowdsourcing stage build co-reference chains. region description given worker process include list previously extracted objects suggestions. allows worker choose previously drawn annotated instead redrawing person. parser manages nouns sometimes misses compound nouns avoided completely depending automated method. combining parser crowdsourcing tasks able speed object extraction process without losing accuracy. objects extracted region description extract attributes relationships described region. present worker region description along extracted objects attributes objects connect pairs objects relationships based text description. description woman shorts standing behind workers extract attribute standing woman relationships behind together objects attributes relationships form region graph region description. descriptions like sunny contain objects therefore region graphs associated them. workers asked generate graphs descriptions. create scene graphs combining region graphs image combining co-referenced objects diﬀerent region graphs. scene graph union region graphs extracted region descriptions. merge nodes region graphs correspond object; example person diﬀerent region graphs might refer object image. objects diﬀerent graphs refer object bounding boxes overlap union however heuristic might contain false positives. merging objects workers conﬁrm pair objects signiﬁcant overlap indeed object. example figure might extracted diﬀerent region descriptions. boxes combined together constructing scene graph. region graphs combined together merging objects co-referenced graphs. image. ensure quality instruct workers follow three rules start questions seven avoid ambiguous speculative questions; precise unique relate question image clearly answerable image shown. collected separate types freeform region-based qas. freeform worker look image write eight pairs encourage diversity enforce workers write least three diﬀerent seven eight pairs. region-based workers write pair based given region. select regions large areas long phrases enables collect around twenty region-based pairs cost eight freeform qas. general freeform tends yield diverse pairs enrich question distribution; region-based tends produce factual pairs lower cost. visual genome data veriﬁcation stage soon annotated. stage helps eliminate incorrectly labeled objects attributes relationships. also helps remove region descriptions questions answers might correct vague subjective opinionated veriﬁcation conducted using separate strategies majority voting rapid judgments components dataset except objects veriﬁed using majority voting. majority voting involves three unique workers looking annotation votuse rapid judgments speed veriﬁcation objects dataset. meanwhile rapid judgments interface inspired rapid serial visual processing enable veriﬁcation objects order magnitude increase speed majority voting. descriptions collect freeform worker-generated texts. constrained limitations. example force workers refer image man. allow choose refer person etc. ambiguity makes instances diﬃcult collect dataset. order reduce ambiguity concepts dataset connect resources used research community objects attributes relationships noun phrases region descriptions synsets wordnet example above person would synsets person.n. male child.n. man.n. respectively. thanks wordnet hierarchy possible fuse three expressions person.n. since lowest common ancestor node aforementioned synsets. stanford tools extract noun phrases region descriptions qas. next frequent matching synset wordnet according wordnet lexeme counts. reﬁne simple heuristic hand-crafting mapping rules common failure cases. example according wordnet’s lexeme counts common semantic table table.n. however data likely pieces furniture therefore bias mapping towards table.n. objects scene graphs already noun phrases mapped wordnet way. adjectives. include hand-crafted rules address common failure cases typically occur concrete spatial sense word seen image common overall sense. example synset long.a. long.a. even though instances word long images much likely refer spatial sense. relationships ignore prepositions recognized wordnet. since meanings verbs highly dependent upon morphology syntactic placement wordnet synsets whose sentence frames match context relationship. sentence frames wordnet formalized syntactic frames certain sense word might appear; example play.v. participate games sport occurs sentence frames somebody somebody something. verb-synset pair consider root hypernym synset reduce potential noise wordnet’s ﬁne-grained sense distinctions. wordnet hierarchy verbs root verbs. example draw.v. cause move pulling traces back root hypernym move.v. cause move shift draw.v. position derive traces root get.v. come possession something concrete abstract. also include hand-mapped rules correct wordnet’s lower representation concrete spatial senses. mappings perfect still contain ambiguity. therefore send mappings along four alternative synsets term amazon mechanical turk. workers verify mapping accurate change mapping alternative better present workers concept want canonicalize along proposed corresponding synset additional options. prevent workers always defaulting proposed synset explicitly specify synsets presented proposed synset. section provides experimental precision recall scores canonicalization strategy. section provide statistical insights analysis component visual genome. speciﬁcally examine distribution images collected data region descriptions questions answers analyze region graphs scene graphs together section also break graph structures three constituent parts—objects attributes relationships —and study part individually. finally describe canonicalization pipeline results fig. example image dataset region descriptions. display localizations descriptions avoid clutter; descriptions corresponding bounding boxes. region bounding boxes visualized image. visual genome dataset consists images intersection ms-coco’s images yfcc’s million images. images real-world non-iconic images uploaded onto flickr users. images range small pixels wide large pixels wide average width fig. distribution width bounding region description normalized image width. distribution height bounding region description normalized image height. fig. distribution number words region description. average number words region description shortest descriptions word longest descriptions words. pixels. collected wordnet synsets images categorized using method imagenet visual genome images cover synsets. figure shows synsets images belong. common synset images; followed ballplayer racket three synsets referring images people playing sports. dataset somewhat biased towards images people figure shows; however quite diverse overall synsets images synsets examples. regions bounding descriptive phrase. figure shows example image dataset region descriptions. display bounding boxes descriptions ﬁgure avoid clutter. descriptions tend highly diverse focus single object like multiple objects like taking photo elephants. encompass salient parts image elephant taking food woman also capturing background small buildings surrounded trees. ms-coco dataset good generating variations single scene-level descriptor. consider three sentences ms-coco dataset similar image there person petting large elephant person touching elephant front wall white shirt petting cheek elephant. three sentences single scenelevel descriptions. comparison visual genome descriptions emphasize diﬀerent regions image thus less semantically similar. ensure diversity descriptions bleu score thresholds descriptions previously written descriptions. information crowdsourcing found section region descriptions must speciﬁc enough image describe individual objects like description must also general enough describe high-level concepts image like chased bear. qualitatively note regions cover large portions image tend general descriptions image regions cover small fraction image tend speciﬁc. figure show distribution regions width region normalized width image. majority regions tend around image width. also note large number regions covering image width. regions usually include elements like ocean snow mountains etc. cannot bounded thus span entire image width. figure show similar distribution normalized height region. similar overall pattern regions tend speciﬁc descriptions image height. unlike distribution width however increase number regions span entire height image common visual equivalents span images vertically. descriptions gathered tend global scene descriptions similar ms-coco examining distribution size regions described also valuable look semantic information captured descriptions. figure show distribution length region descriptions. average word count description words minimum word maximum words. figure plot common phrases occurring region descriptions stop words removed. common visual elements like green grass tree distance blue occur much often other nuanced elements like fresh strawberry. also study descriptions ﬁner precision figure plot common words used descriptions. again eliminate stop words study. colors like white black frequently used words describe visual concepts; conduct similar study captioning datasets including mscoco flickr similar distribution colors occursemantic diversity. also study actual semantic contents descriptions. unsupervised approach analyze semantics descriptions. speciﬁcally wordvec convert word description -dimensional vector. next remove stop words average remaining words vector representation whole region description. pipeline outlined figure hierarchical agglomerative clustering vector representations region description semantic syntactic groupings clusters. figure shows four example clusters. cluster contains descriptions related tennis like swings racquet white lines ground tennis court another cluster contains descriptions related numbers like three dogs street people inside tent. quantitatively measure diversity visual genome’s region descriptions calculate number clusters represented single image’s region descriptions. show distribution variety descriptions image figure average image contains descriptions diﬀerent clusters. image least diverse descriptions contains descriptions clusters image diverse descriptions contains descriptions clusters. finally also compare descriptions visual genome captions ms-coco. first aggregate visual genome ms-coco descriptions remove stop words. removing stop words descriptions datasets roughly length. conduct similar study vectorize descriptions image calculate dataset’s cluster diversity image. average clusters represented captions image ms-coco images clusters represented. image ms-coco contains captions fair comparison compare number clusters represented region descriptions visual genome dataset. thus randomly sample visual genome region descriptions image calculate number clusters image. visual genome descriptions come clusters. show comparison results figure diﬀerence between semantic diversity datasets fig. plot common visual concepts phrases occur region descriptions. common phrases refer universal visual concepts like blue green grass etc. plot frequently used words region descriptions. colors occur frequently followed common objects like universal visual concepts like sky. fig. example illustration showing four clusters region descriptions overall themes. clusters shown limited space. distribution images number clusters represented image’s region descriptions. take visual genome random descriptions taken image ms-coco dataset sentence descriptions image compare many clusters represented descriptions. show visual genome’s descriptions varied given image average clusters image ms-coco’s images average clusters image. comparison related datasets visual genome fares well terms object density diversity. visual genome contains approximately objects image exceeding imagenet pascal ms-coco datasets large margins. shown figure object categories represented visual genome dataset. comparison especially pertinent regards microsoft ms-coco uses images visual genome. lower count objects category result higher number categories. fairer comparison ilsvrc detection visual genome objects category categories considered comparable ilsvrc’s objects category. fairer comparison ms-coco visual genome objects category objects visual genome come variety categories. shown figure objects related wordnet categories humans animals sports scenery common; consistent general bias image subject matter dataset. common objects like person woman occur especially frequently occurrences objects also occur ms-coco also well represented around instances average. figure shows examples objects images. objects visual genome span diverse wordnet categories like food animals man-made structures. important look types objects also distribution objects images regions. figure shows expected objects region average. possible regions contain objects descriptions refer explicit objects image. example region described dark outside objects extract. regions object generally descriptions focus attributes single object. hand regions objects generally descriptions contain attributes speciﬁc objects relationships pairs objects. shown figure image contains average around unique objects. images number objects expect since images usually capture objects. moreover images extremely high number objects fig. examples objects visual genome. object localized image tightly drawn bounding box. plot frequently occurring objects images. people frequently occurring objects dataset followed common objects visual elements like building shirt sky. attributes allow detailed description disambiguation objects dataset. objects visual genome annotated least attribute; dataset contains million total attributes unique attributes. attributes include colors sizes continuous action verbs materials etc. attribute scene graphs belongs object object multiple attributes. denote attributes attribute. average image visual genome contains attributes shown figure region contains average attribute though regions contain attribute all; primarily many regions relationship-focused. figure shows distribution common attributes dataset. colors frequent attributes. also common sizes materials figure shows distribution attributes describing people common attributes describing people intransitive verbs describing states motion certain sports overrepresented bias towards sports images. attribute graphs. also qualitatively analyze attributes dataset constructing co-occurrence graphs nodes unique attributes edges connect attributes describe object. example image contained large black black) another image contained large yellow yellow) attributes would form incomplete graph edges create graphs total attributes second consider objects refer people. subgraph frequently connected personrelated attributes shown figure cliques graphs represent groups attributes least co-occurrence exists pair attributes group. previous example third image contained black yellow taxi yellow) resulting third edge would create clique attributes black large yellow. calculated across entire visual genome dataset cliques provide insight commonly perceived traits different types objects. figure selected representation three example cliques overlaps. clique attributes predict types objects usually referenced. figure cliques describe animal water body human hair cliques also uniquely identify objects. clique contains athletic young skateboarding focused teenager male skinny happy capturing common traits skateboarders set. anclique shiny small metal silver rusty parked empty likely describing subset cars. cliques thus infer distinct objects object types based solely attributes potentially allowing highly speciﬁc object identiﬁcation based selected characteristics. fig. distribution showing common attributes dataset. colors materials common. distribution showing number attributes describing people. state-of-motion verbs common certain sports also highly represented image source bias image set. fig. graph person-describing attributes co-occurrences. edge thickness represents frequency co-occurrence nodes. subgraph showing co-occurrences intersections three cliques appear describe water hair type animal edges cliques removed clarity. relationships core components link objects scene graphs. relationships directional i.e. involve objects acting subject object predicate relationship. denote relationships form relationship. example swinging write swinging. relationships spatial action compositional etc. complex relationships standing includes action spatial aspect also represented. relationships extracted region descriptions crowd workers similarly attributes objects. visual genome contains total unique relationships million total relationships. figure shows distribution relationships region description. average relationship region maximum also descriptions like tall multiple attributes associated relationships. figure distribution relationships image object. finally figure shows distribution relationships image. image average relationships minimum relationship maximum relationships. relationship distributions. display frequently occurring relationships figure common relationship dataset. primarily ﬂexibility word refer spatial conﬁguration attachment etc. common relationships involve actions like holding wearing spatial conﬁgurations like behind next under. figure shows similar distribution relationships involving people. notice human-centric relationships actions kissing chatting with talking distributions follow zipf distribution. understanding aﬀordances. relationships allow also understand aﬀordances objects. show using speciﬁc distribution subjects objects involved relationship riding figure figure shows distribution subjects figure shows similar distribution objects. comparing distributions clear patterns people-like subject entities person policeman skateboarder ride objects; distribution contains objects aﬀord riding horse bike elephant motorcycle skateboard. related work comparison. also worth mentioning section prior work relationships. concept visual relationships already explored visual phrases introduced dataset relationships next riding. however dataset limited relationships. similarly ms-coco-a dataset introduced actions humans performed ms-coco’s dataset however dataset limited actions relationships general numerous unique relationships. finally viske introduced relationships much smaller dataset images visual genome. fig. sample frequent relationships dataset. general common relationships spatial sample frequent relationships involving humans dataset. relationships involving people tend action oriented introduce paper largest dataset scene graphs date. graph representations images deeper understanding visual world. section analyze properties representations region level region graphs image level scene graphs. also fig. distribution subjects relationship riding. distribution objects relationship riding. subjects comprise people-like entities like person policeman skateboarder ride objects. hand objects like horse bike elephant motorcycle entities aﬀord riding. scene graphs asking humans write triples image however unlike them collect graphs much ﬁne-grained level region graph. obtained graphs asking workers create descriptions collected regions. therefore multiple graphs image every region description. together combine individual region graphs aggregate scene graph image. scene graph made individual region graphs. scene graph representation merge objects referenced multiple region graphs node scene graph. images distribution region graphs image average image exactly scene graph. note number region descriptions number region graphs image same. example consider description sunny day. description contains objects building blocks region graph. therefore descriptions region graphs associated them. objects attributes relationships occur normal distribution data. table shows region graph average objects attributes relationships. scene graph consequently image average objects attributes relationships. collected question answering pairs visual genome images. pair consists question correct answer regarding content image. average every image pairs. rather collecting unconstrained pairs previous work done question visual genome starts what where when how. major beneﬁts focusing types questions. first oﬀer considerable coverage question types ranging basic perceptual tasks complex common sense reasoning second categories present natural consistent stratiﬁcation task diﬃculty indicated baseline performance section instance questions involve complex reasoning lead poorest performance categories. enables obtain better understanding strengths weaknesses today’s computer vision models sheds light future directions proceed. analyze diversity quality questions answers. goal construct largescale visual question answering dataset covers diverse range question types basic cognition tasks complex reasoning tasks. demonstrate richness diversity pairs examining distributions questions answers figure question type distributions. questions naturally fall categories interrogative words. inside categories second following words categorize questions increasing granularity. inspired show distributions questions ﬁrst three words figure what common categories. notable diﬀerence question distribution vqa’s focus ensuring question categories fig. distribution question types starting words. ﬁgure shows distribution questions ﬁrst three words. angles regions proportional number pairs corresponding categories. what questions largest category nearly half pairs. adequately represented questions yes/no binary questions. result trivial model achieve reasonable performance predicting answers. encourage diﬃcult pairs ruling binary questions. question answer length distributions. also analyze question answer lengths category. figure shows average question answer lengths category. overall average question answer lengths words respectively. contrast dataset answers consist three words answers exhibit long-tail distribution answers three words respectively. avoid verbosity instructing workers write answers concisely possible. coverage long answers means many answers contain short description contains details merely object attribute. shows richness complexity visual tasks beyond object-centric recognition tasks. foresee long-tail questions mofig. question answer lengths question type. bars show average question answer lengths question type. whiskers show standard deviations. factual questions what questions usually come short answers single object number. questions disproportionately counting questions start many. questions where categories usually phrases sentences answers. order reduce ambiguity concepts dataset connect resources used research community canonicalize semantic meanings objects relationships attributes visual genome. canonicalization refer word sense disambiguation mapping components dataset respective synsets wordnet ontology mapping reduces noise concepts contained dataset also facilitates linkage visual genome data sources imagenet built wordnet ontology. figure shows example image visual genome dataset components canonicalized. example horse canonicalized horse.n. solid-hoofed herbivorous quadruped domesticated since prehistoric times. attribute clydesdale canonicalized breed clydesdale.n. heavy feathered-legged breed draft horse originally scotland. also show example fig. example image visual genome dataset region descriptions objects attributes relationships canonicalized. large text boxes wordnet synsets referenced image. example carriage mapped carriage.n. vehicle wheels drawn horses. show bounding boxes objects order allow readers image clearly. also show subset scene graph image avoid cluttering ﬁgure. related work. canonicalization used numerous applications including machine translation information retrieval information extraction english sentences sentences like scored goal goal life carry diﬀerent meanings word goal. understanding diﬀerences crucial translating languages returning correct results query. similarly visual genome ensure components canonicalized understand different objects related other; example person hypernym woman. past canonicalization models precision recall score evaluate semeval dataset current state-of-the-art performance semeval score since canonicalization setup diﬀerent semeval benchmark canonicalization method directly comparable existing methods. however achieve similar precision recall score held-out test described below. objects need extracted phrase text stanford tools extract noun phrases region description resulting recall noun phrases subset region descriptions manually annotated. obtaining noun phrases frequent matching synset resulted overall mapping accuracy recall common synsets extracted region descriptions objects shown figure attributes. canonicalize attributes crowdextracted attributes present scene graphs. attribute designation encompasses wide range grammatical parts speech. part-of-speech taggers rely high-level syntax information thus fail disjoint elements scene graphs normalize attribute based morphology alone then objects attribute phrase frequent matching wordnet synset. include hand-mapped rules address common failure cases wordnet’s frequency counts prefer abstract senses words spatial senses present visual data e.g. short.a. limited duration short.a. lacking length. veriﬁcation randomly sample attributes produce ground-truth mappings hand compare results algorithm. resulted recall mapping accuracy common attribute synsets shown figure relationships. attributes canonicalize relationships isolated scene graphs. exclude prepositions recognized wordnet leaving primarily composed verb relationships. since meanings verbs highly dependent upon morphology syntactic placement structure relationship appropriate wordnet sentence frame consider wordnet synsets matching sentence frames. verb-synset pair consider root hypernym synset reduce potential noise wordnet’s ﬁne-grained sense distinctions. also include hand-mapped rules correct wordnet’s lower representation concrete spatial senses; example concrete hold.v. hold one’s hand grip less frequent wordnet abstract hold.v. cause continue certain state. veriﬁcation randomly sample relationships compare results canonicalization ground-truth mappings. resulted recall mapping accuracy several datasets verbnet framenet include semantic restrictions frames improve classiﬁcation comprehensive method mapping restrictions frames. common relationship synsets shown figure thus presented visual genome dataset analyzed individual components. rich information provided numerous perceptual cognitive tasks tackled. section provide baseline experimental results using components visual genome extensively studied. object detection already well-studied problem similarly region graphs scene graphs shown improve semantic image retrieval therefore focus remaining components i.e. attributes relationships region descriptions question answer pairs. section present results experiments attribute prediction. ﬁrst treat attributes independently objects train classiﬁer attribute i.e. classiﬁer classiﬁer second experiment learn object attribute classiﬁers jointly predict object-attribute pairs section present experiments relationship prediction. ﬁrst predict predicate objects e.g. predicting predicate kicking wearing objects. experiment synonymous existing work action recognition another experiment study relationships classifying jointly objects predicate show diﬃcult task high variability appearance relationship experiment generalizations tasks study spatial relationships objects ones jointly reason interaction humans objects section present results region captioning. task closely related image captioning however results directly comparable region descriptions short incomplete sentences. train state-of-the-art image caption generator dataset generate region descriptions flickrk generate sentence descriptions. compare results between training approaches simple templates convert region descriptions complete sentences. robust evaluation validate descriptions generate using human judgment. section experiment visual question answering i.e. given image question attempt provide answer question. report results retrieval correct answer list existing answers. semantic cues various problems lead deeper understanding images. express wide variety properties attributes form function sentiment even intention distinguishing similar objects leads ﬁner-grained classiﬁcation describing previously unseen class attributes shared known classes enable zero-shot learning validation testing. image number examples results approximately %-%-% split attributes themselves. input data experiment cropped bounding object associated attribute. fig. example predictions attribute prediction experiment. attributes ﬁrst predicted correctly second diﬀer ground truth still correctly classify attribute image third classiﬁed incorrectly. model tends associate objects attributes example predictions joint object-attribute prediction experiment. cally ﬁne-tune -layer network experiments using attribute object-attribute pair instances respectively. modify network learning rate ﬁnal fully-connected layer times layers improves convergence time. base learning rate scale every iterations momentum weight decays respectively. ﬁne-tuned features network train individual svms predict attribute. output multiple attributes bounding input. second experiment also output object class. results. table shows results experiments. ﬁrst experiment attribute prediction converge around iterations topone accuracy top-ﬁve accuracy. thus attributes visually distinguishable other. second experiment also predict object class converge around iterations top-one accuracy top-ﬁve accuracy. predicting objects jointly attributes increases top-one accuracy implies attributes occur exclusively small number objects. additionally jointly learning attributes objects increase inter-class variance making classiﬁcation process easier task. figure shows example predictions ﬁrst attribute prediction experiment. general model good associating objects salient attributes example animal stuffed elephant grazing. however difference user-provided result correct ground truth model incorrectly classiﬁes correct predictions. example white stuﬀed animal correct evaluated incorrect. figure shows example predictions second experiment also predict object. results second might considered correct keep consistent evaluation mark incorrect. example predicted green grass might considered subjectively correct even though annotated brown grass. cases objects clearly visible abstract outlines model unable predict attributes objects accurately. example thinks ﬂying bird actually black jacket. table results attribute prediction task predict attributes given image crop. attribute-object prediction experiment predict attributes well object given crop image. attribute clique graphs section clearly show learning attributes help identify types objects. experiment strengthens insight. learn studying attributes together objects improve attribute prediction. objects core building blocks image relationships context. relationships help distinguish images contain objects diﬀerent holistic interpretations. example image riding bike falling bike contain bike relationship changes perceive situations. visual genome largest known dataset relationships total million relationships average relationships image. setup. setups experiments similar experiments performed attributes. focus frequent relationships. lowercase lemmatize strip excess whitespace relationships. around relationships subject-relationshipobject triples training validation testing. input data experiment image region containing union bounding boxes subject object ﬁne-tune -layer network learning rates mentioned section results. overall relationships visually distinct enough discriminative model learn eﬀectively. table shows results experiments. relationship classiﬁcation converge around iterations top-one accuracy top-ﬁve accuracy. unlike attribute prediction accuracy results relationships much lower high intra-class variability relationships. second experiment jointly predicting relationship object classes converge around iterations top-one accuracy top-ﬁve accuracy. notice object classiﬁcation aids relationship prediction. relationships occur objects never others; example relationship drive occurs object person never objects figure structured figure shows example predictions joint prediction relationships objects. model able predict salient features image fails distinguish diﬀerent objects generating sentence descriptions images gained popularity task computer vision however current state-of-the-art models fail describe diﬀerent events captured image instead provide high-level summary image. section test well state-of-the-art models caption details images. experiments neuraltalk model since provides state-of-the-art results also shown robust enough predicting short descriptions. train neuraltalk visual genome dataset region descriptions flickrk full sentence descriptions. model trained datasets would generate complete sentences would comparable region descriptions convert region descriptions generated model complete sentences using predeﬁned templates fig. example predictions relationship prediction experiment. relationships ﬁrst predicted correctly second diﬀer ground truth still correctly classify relationship image third classiﬁed incorrectly. model learns associate animals leaning towards ground eating drinking bikes riding. example predictions relationship-objects prediction experiment. ﬁgure organized figure model able predict salient features image fails distinguish diﬀerent objects setup. training begin preprocessing region descriptions; remove non-alphanumeric characters lowercase strip excess whitespace them. region descriptions total. region descriptions training validation testing. note ensure descriptions regions image exclusively training validation testing set. feed bounding boxes regions pretrained -layer network -dimensional feature vectors region. neuraltalk model train long short-term memory network generate descriptions regions. learning rate trained rmsprop model converges four days. testing crop ground-truth region bounding boxes images extract -dimensional -layer network features. feed vectors pretrained neuraltalk model predictions region descriptions. results. table shows results experiment. calculate bleu cider meteor scores generated descriptions ground-truth descriptions. cases model trained visualgenome performs better. moreover asked crowd workers evaluate whether generated description correct—we models trained flickrk visual genome respectively. large increase accuracy model trained data speciﬁcity dataset. region descriptions shorter cover smaller image area. comparison flickrk data generic descriptions entire images multiple events happening diﬀerent regions image. model trained data able make predictions likely concentrate speciﬁc part image looking instead generating summary description. objectively accuracy cases illustrates current models unable reason complex images. setup. split pairs training test ensure images exclusive either training test set. implement simple baseline model relies answer frequency. model counts frequent answers training predictions test questions model make diﬀerent predictions. model correct predictions matches exactly groundtruth answer. report accuracy test questions. evaluation method works well answers short especially single-word answers. however causes problems answers long phrases sentences. evaluation methods require word ontologies multiple choices human judges fig. example predictions region description generation experiment. regions ﬁrst column accurately describe region second column incorrect unrelated corresponding region. descriptions predicted descriptions also short expected; however causes model fail produce descriptive phrases regions multiple objects distinctive objects templates convert region descriptions sentences future work explore smarter approaches combine region descriptions generate paragraph connecting regions coherent description. visual genome currently largest dataset visual question answers million question answer pairs. images contains average question answer pairs. answering questions requires deeper understanding image generic image captioning. question answering involve ﬁne-grained recognition object detection activity recognition knowledge base reasoning common-sense reasoning results. table shows performance openended visual question answering task. baseline results imply long-tail distribution answers. long-tail distribution common existing datasets well frequent answers cover correct answers. comparison corresponding sets frequent answers cover test answers. where questions tend involve spatial common sense reasoning tend diverse answers hence perform poorly performances top- respectively. frequent answers cover correct answers question types respectively. table results region description generation experiment. scores ﬁrst region descriptions generated neuraltalk model trained flickrk second generated model trained visual genome data. bleu cider meteor scores compare predicted description ground truth diﬀerent ways. analyzed individual components dataset presented experiments baseline results tasks attribute classiﬁcation relationship classiﬁcation description generation question answering. however applications experiments dataset used. section note potential applications dataset enable. dense image captioning. seen numerous image captioning papers attempt describe entire image single caption. however captions exhaustively describe every part scene. natural extension application visual genome dataset enables ability create dense captioning models describe parts scene. visual question answering. visual question answering studied standalone task introduce dataset combines question answers descriptions scene graphs. future work build supervised models utilize various components visual genome tackle question answering. image understanding. seen surge image captioning question answering models little work creating comprehensive evaluation metrics measure well models performing. models usually evaluated using bleu cider meteor similar metrics effectively measure well models understand image visual genome scene graphs used measurement image understanding. generated descriptions answers matched ground truth scene graph image evaluate corresponding model. relationship extraction. relationship extraction extensively studied information retrieval natural language processing visual genome ﬁrst largescale visual relationship dataset. dataset used study extraction visual relationships images interactions objects also used study action recognition spatial orientation objects semantic image retrieval. previous work already shown scene graphs used improve semantic image search methods explored using region descriptions combined region graphs. attention-based search methods also explored area interest speciﬁed query also localized retrieved images. visual genome provides multi-layered understanding pictures. allows multi-perspective study image pixel-level information like objects relationships require inference even deeper cognitive tasks like question answering. comprehensive dataset training benchmarking next generation computer vision models. visual genome expect models develop broader understanding visual world complementing computers’ capacities detect objects abilities describe objects explain interactions relationships. visual genome large formalized knowledge representation visual understanding complete descriptions question answers grounds visual concepts language. acknowledgements would like start thanking sponsors stanford computer science department yahoo labs brown institute media innovation toyota adobe. next specially thank michael stark yutian frederic sherman leung michelle gavin contributions. thank carsten rother university dresden facilitating oliver groth’s involvement. also thank thousands crowd workers diligent contribution visual genome. finally thank members stanford vision useful comments discussions.", "year": 2016}