{"title": "Identifying and Harnessing the Building Blocks of Machine Learning  Pipelines for Sensible Initialization of a Data Science Automation Tool", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "As data science continues to grow in popularity, there will be an increasing need to make data science tools more scalable, flexible, and accessible. In particular, automated machine learning (AutoML) systems seek to automate the process of designing and optimizing machine learning pipelines. In this chapter, we present a genetic programming-based AutoML system called TPOT that optimizes a series of feature preprocessors and machine learning models with the goal of maximizing classification accuracy on a supervised classification problem. Further, we analyze a large database of pipelines that were previously used to solve various supervised classification problems and identify 100 short series of machine learning operations that appear the most frequently, which we call the building blocks of machine learning pipelines. We harness these building blocks to initialize TPOT with promising solutions, and find that this sensible initialization method significantly improves TPOT's performance on one benchmark at no cost of significantly degrading performance on the others. Thus, sensible initialization with machine learning pipeline building blocks shows promise for GP-based AutoML systems, and should be further refined in future work.", "text": "abstract data science continues grow popularity increasing need make data science tools scalable ﬂexible accessible. particular automated machine learning systems seek automate process designing optimizing machine learning pipelines. chapter present genetic programming-based automl system called tpot optimizes series feature preprocessors machine learning models goal maximizing classiﬁcation accuracy supervised classiﬁcation problem. further analyze large database pipelines previously used solve various supervised classiﬁcation problems identify short series machine learning operations appear frequently call building blocks machine learning pipelines. harness building blocks initialize tpot promising solutions sensible initialization method signiﬁcantly improves tpot’s performance benchmark cost signiﬁcantly degrading performance others. thus sensible initialization machine learning pipeline building blocks shows promise gp-based automl systems further reﬁned future work. words pipeline optimization hyperparameter optimization automated machine learning sensible initialization building blocks genetic programming pareto optimization data science python machine learning often touted ﬁeld study gives computers ability learn without explicitly programmed despite common claim well-known machine learning practitioners designing effective machine learning pipelines often tedious endeavor typically requires considerable experience machine learning algorithms expert knowledge problem domain brute force search accomplish. figure depicts typical machine learning pipeline step requires intervention machine learning practitioners. thus contrary machine learning enthusiasts would believe machine learning still requires considerable explicit programming. response challenge several automated machine learning methods developed years past year developing tree-based pipeline optimization tool automatically designs optimizes machine learning pipelines given problem domain without need human intervention. short tpot optimizes machine learning pipelines using version genetic programming well-known evolutionary computation technique automatically constructing computer programs previously demonstrated combining pareto optimization enables tpot automatically construct high-accuracy compact pipelines consistently outperform basic machine learning analyses chapter report progress toward introducing sensible initialization population tpot goal enabling tpot harness expert knowledge machine learning pipelines efﬁciently discover effective pipelines given problem domain. fig. typical machine learning pipeline. machine learning practitioners often start data must formatted missing values imputed otherwise prepared analysis. following step practitioners must often transform feature format amenable modeling example preprocessing features scaling constructing features existing features removing less useful features feature selection. next practitioners must select machine learning model data optimize parameters model feature transformation operations allow model best capture underlying signal data. process practitioners must evaluate pipeline validation data pipeline never before allows practitioners determine whether pipeline generalizes beyond initial training data. previous research initialization populations shown initialization process vitally affect performance algorithms however research focused generating diversity valid tree structures useful application domains. follow footsteps focus harnessing expert knowledge—in case expert knowledge machine learning pipelines—to initialize population. particular attempt identify building blocks machine learning pipelines harness building blocks sensible initialization population tpot. early days machine learning automation research researchers focused primarily hyperparameter optimization example commonly-used form hyperparameter optimization grid search users apply brute force search evaluate predeﬁned range model parameters model parameters allows best model recently researchers showed possible discover optimal parameter sets faster exhaustive grid search randomly sampling within predeﬁned grid search shows promise guided search hyperparameter space. bayesian optimization particular proven effective hyperparameter optimization even outperformed manual hyperparameter tuning expert practitioners another focus automl research feature construction. recent example automated feature construction data science machine automatically constructs features relational databases deep feature synthesis. work demonstrated crucial role automated feature construction machine learning pipelines entering data science machine three machine learning competitions achieving expert-level performance them. thus know automated feature construction play vital role automl systems. recently developed automl system called auto-sklearn uses bayesian optimization discover ideal combination data feature preprocessors models model hyperparameters maximize classiﬁcation accuracy particular problem domain. however auto-sklearn optimizes predeﬁned pipelines include data preprocessor feature preprocessor model precludes auto-sklearn producing arbitrarily large pipelines important automl. demonstrated automl system using genetic programming optimize machine learning pipelines signal processing found capable designing better pipelines humans signal processing task. such shows considerable promise automl domain signiﬁcantly extend work tpot. following sections provide overview tree-based pipeline optimization tool including machine learning operators used genetic programming primitives tree-based pipelines used combine primitives working machine learning pipelines algorithm used evolve said tree-based pipelines. describe process implemented provide sensible initialization algorithm tpot conclude description data sets used evaluate version tpot. tpot open source project github underlying python code found https//github.com/rhiever/tpot. core tpot wrapper python machine learning package scikitlearn thus machine learning pipeline operator tpot corresponds machine learning algorithm supervised classiﬁcation model. implementations machine learning algorithms listed scikit-learn refer scikit-learn documentation detailed explanations machine learning algorithms used tpot. supervised classiﬁcation operators. decisiontree randomforest extreme gradient boosting classiﬁer logisticregression knearestneighborclassiﬁer. classiﬁcation operators store classiﬁer’s predictions feature well classiﬁcation pipeline. feature preprocessing operators. standardscaler robustscaler minmaxscaler maxabsscaler randomizedpca binarizer polynomialfeatures. preprocessing operators modify data return modiﬁed data set. feature selection operators. variancethreshold selectkbest selectpercentile selectfwe recursive feature elimination feature selection operators reduce number features data using criteria return modiﬁed data set. also include operator combines disparate data sets demonstrated figure allows multiple modiﬁed copies data combined single data set. lastly provide integer ﬂoat terminals parameterize various operators number neighbors k-nearest neighbors classiﬁer. combine operators machine learning pipeline treat primitives construct trees them. figure shows example tree-based pipeline copies data provided pipeline modiﬁed successive manner operator combined single data ﬁnally used make classiﬁcations. operators receive data input return modiﬁed data output possible construct arbitrarily large machine learning pipelines multiple copies data set. thus trees provide inherently ﬂexible representation machine learning pipelines. order tree-based pipelines operate store three additional variables record data set. class variable indicates true label record used evaluating accuracy pipeline. guess variable indicates pipeline’s latest guess record classiﬁcations last classiﬁcation operator pipeline stored guess. finally group variable indicates whether record used part internal training testing tree-based pipelines trained training data evaluated testing data. note data provided tpot split internal stratiﬁed training/testing set. automatically generate optimize tree-based pipelines algorithm implemented python package deap tpot algorithm follows standard process settings listed table begin table genetic programming algorithm settings. parameter population size generations multi-objective selection per-individual crossover rate per-individual mutation rate crossover mutation replicate runs unique seeds algorithm generates random tree-based pipelines evaluates accuracy data set. every generation algorithm algorithm selects pipelines population according nsga-ii selection scheme pipelines selected simultaneously maximize classiﬁcation accuracy data minimizing number operators pipeline. selected pipelines produce offspring next generation’s population offspring experience crossover another offspring remaining unaffected offspring experience random mutations. every generation algorithm updates pareto front non-dominated solutions discovered point run. algorithm repeats evaluate-select-crossover-mutate process generations—adding tuning pipeline operators improve classiﬁcation accuracy pruning operators degrade classiﬁcation accuracy—at point algorithm selects highestaccuracy pipeline pareto front representative best pipeline run. next implement version tpot sensible initialization call tpot-si. tpot-si algorithm creates initial population seeding random selection building blocks identiﬁed previous tpot runs. building blocks consist tree-based pipelines operators e.g. building block identiﬁed polynomialfeatures logisticregression building block casts data polynomial feature space provides features logistic regression model make classiﬁcation. primary idea behind providing sensible initialization building blocks genetic programming algorithms typically rely heavily crossover thus want initialize population building blocks start population pipelines already effectively solve least part classiﬁcation task mixed matched build better pipelines efﬁcient manner. however note preliminary investigations showed identify building blocks replicates tpot supervised classiﬁcation benchmark data sets described section identiﬁed highest-accuracy tree-based pipeline ﬁnal pareto front performed n-gram analysis pipelines count frequent combinations pipeline operators. example figure selectkbest knearestneighborclassiﬁer would -gram knearestneighborclassiﬁer would -gram. counting n-grams tree-based pipeline summed counts across replicates determine frequent n-grams used tpot building blocks. listed frequent building blocks table randomforest xgbclassiﬁer logisticregression decisiontree knearestneighborclassiﬁer xgbclassiﬁer randomforest logisticregression randomforest polynomialfeatures logisticregression polynomialfeatures randomforest selectpercentile randomforest compiled supervised classiﬁcation benchmarks wide variety sources including machine learning repository large preexisting benchmark repository simulated genetic analysis data sets benchmark data sets range records hundreds features include binary well multi-class supervised classiﬁcation problems. selected data sets wide range application domains including genetic analysis image classiﬁcation time series analysis many more. thus benchmark represents comprehensive suite tests evaluate automated machine learning systems. provide initial evaluation tpot-si replicates tpot-si supervised classiﬁcation benchmark data sets described section compare experiments experiments version tpot without sensible initialization. cases measured pipeline accuracy balanced accuracy corrects class frequency imbalances data sets computing accuracy per-class basis averaging per-class accuracies. figure summarizes difference performance tpot-si tpot benchmark. overall tpot-si showed improvement large portion benchmarks small performance degradation benchmarks fair improvement handful benchmarks. notably largest performance degradation tutorial benchmark median accuracy decrease largest performance increase parity benchmark median accuracy increase. order provide better insight many benchmarks improvement tpot-si plotted original tpot accuracy data tpot-si accuracy corresponding data figure benchmarks fig. violin plot difference median balanced accuracy tpot-si tpot benchmarks. positive values indicate improvement accuracy tpot-si whereas negative values indicate degradation accuracy tpot-si. width violin represents relative density points value e.g. differences centered around accuracy improvement. note density estimated underlying data appears differences accuracy fig. median balanced accuracy tpot-si tpot benchmarks. point represents median balanced accuracies benchmark. line represents linear regression median accuracies whereas histograms sides show density points axes. finally show distribution balanced accuracies benchmarks largest performance differences figure surprisingly benchmark statistically signiﬁcant difference performance analcatdata lawsuit benchmark tpot-si allowed higher accuracy average. benchmarks tpot-si allowed small statistically insigniﬁcant improvements tpot. fig. plots showing distribution balanced accuracies benchmarks biggest difference median accuracy tpot-si tpot. plot represents replicates inner line shows median notches represent bootstrapped conﬁdence interval median. chapter presented preliminary results implementing sensible initialization tpot. summary tpot-si signiﬁcant improvement performance benchmark although sensible initialization method could likely improvement note tpot-si signiﬁcantly degrade performance benchmarks well. course goal chapter extends beyond implementing sensible initialization method tpot seek identify building blocks machine learning pipelines information harnessed many machine learning applications. chapter suggest machine learning building blocks small sequences machine learning operators occur frequently pipelines used solve benchmark classiﬁcation problems. although building blocks identiﬁed seem signiﬁcantly improve performance sensible initialization many benchmarks results could many reasons. example building blocks used useful benchmarks fact could detrimental benchmarks. discuss limitations overcome section furthermore tpot-si automatically optimized pipelines benchmarks discovered pipelines achieve median balanced accuracy benchmarks without prior knowledge problem domains. results show signiﬁcant promise gpbased automated machine learning systems. note however tpot considered replacement machine learning practitioners; rather tpot saves practitioners time automating tedious portions machine learning pipeline design ultimately practitioners responsible deciding ﬁnal pipeline. similarly consider tpot data science assistant idea generator discover unique ways model data sets—and export ﬁnding corresponding python code—so practitioners take tpot pipelines customize particular application. effort providing easily accessible gp-based automated machine learning system released tpot open source python application https//github.com/rhiever/tpot. sensible initialization method implemented tpot-si quite simple many reﬁnements made. example meta-learning techniques intelligently match building blocks pipeline conﬁgurations work well particular data analyzed short meta-learning uses information previous machine learning runs estimate well pipeline conﬁguration work particular data set. place data sets standard scale meta-learners compute meta-features data sets data size number features various aspects features used data meta-features corresponding pipeline conﬁgurations work well data sets meta-features. intelligent meta-learning algorithm likely improve tpot sensible initialization process. similarly harness expert knowledge machine learning building blocks bias mutation crossover operations similar done case would provide algorithm information well particular pipeline combinations perform average e.g. replacing randomforest operator decisiontree operator likely degrade accuracy. information could used bias mutation crossover operations toward producing better pipelines. further information could learned finally population-based optimization methods typically criticized maintaining large population solutions prove slow wasteful certain optimization problems. case turn gp’s purported weakness strength creating ensemble populations. explored population ensemble method previously standard showed signiﬁcant improvement natural extension create ensembles tpot’s population machine learning pipelines. conclusion automated machine learning ﬁeld research ripe systems. focus efforts reﬁning gp-based automated machine learning system particular highlight gp’s strengths compared bayesian optimization simulated annealing greedy optimization techniques. tpot represents effort toward goal continue reﬁne tpot consistently produces human-competitive machine learning pipelines.", "year": 2016}