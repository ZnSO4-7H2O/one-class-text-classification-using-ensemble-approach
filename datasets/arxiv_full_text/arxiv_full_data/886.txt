{"title": "Precise Recovery of Latent Vectors from Generative Adversarial Networks", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Generative adversarial networks (GANs) transform latent vectors into visually plausible images. It is generally thought that the original GAN formulation gives no out-of-the-box method to reverse the mapping, projecting images back into latent space. We introduce a simple, gradient-based technique called stochastic clipping. In experiments, for images generated by the GAN, we precisely recover their latent vector pre-images 100% of the time. Additional experiments demonstrate that this method is robust to noise. Finally, we show that even for unseen images, our method appears to recover unique encodings.", "text": "generative adversarial networks transform latent vectors visually plausible images. generally thought original formulation gives out-of-the-box method reverse mapping projecting images back latent space. introduce simple gradient-based technique called stochastic clipping. experiments images generated precisely recover latent vector pre-images time. additional experiments demonstrate method robust noise. finally show even unseen images method appears recover unique encodings. deep convolutional neural networks standard tools machine learning practitioners. currently outperform computer vision techniques discriminative learning problems including image classiﬁcation object detection. generative adversarial networks adapt deterministic deep neural networks task generative modeling. gans consist generator discriminator. generator maps samples lowdimensional latent space onto space images. discriminator tries distinguish images produced generator real images. training generator tries fool discriminator. training researchers typically discard discriminator. images generated drawing samples latent space passing generator. generative capacity gans well-known best perform reverse mapping remains open research problem. donahue suggests extension third model explicitly learns reverse mapping. creswell bharath suggest inverting generator difﬁcult noting that principle single image multiple latent vectors propose gradient-based approach recover latent vectors evaluate process reconstruction error image space. reconstruct latent vectors performing gradient descent components latent representations introduce technique called stochastic clipping. knowledge ﬁrst empirical demonstration dcgans inverted arbitrary precision. moreover demonstrate reconstructions robust added noise. adding small amounts gaussian noise images nonetheless recover latent vector little loss ﬁdelity. research also seek insight regarding optimizations neural network loss surfaces. seek answers questions optimization achieve globally minimal loss stuck sub-optimal critical points? optimization recover precisely input every time? experiments pre-trained dcgan network gradient descent stochastic clipping recovers true latent vector time arbitrary precision. related work several papers attempt gradient-based methods inverting deep neural networks. mahendran vedaldi invert discriminative cnns understand hidden representations. creswell bharath invert generators gans report ﬁnding faithful reconstruction latent space. note task ﬁnding pre-images non-convex mappings history computer vision dating least back bakır al.. invert mappings learned generator apply following idea. latent vector produce image initialize random vector shape vector maps corresponding image order reverse engineer input successively update components order push representation closer original image experiments minimize norm yielding following optimization problem note optimization global minima however know solution achieves global minimum unique. moreover optimization non-convex thus know theoretical reason optimization precisely recover original vector. many cases know original input comes bounded domain. dcgans latent vectors sampled uniformly hyper-cube. enforce constraint apply modiﬁed optimization standard clipping replace components large maximum allowed value components small minimum allowed value. standard clipping precisely recovers large fraction vectors failure cases noticed reconstructions components stuck either uniform) know probability component right boundary close zero. prevent reconstructions getting stuck introduce heuristic technique called stochastic clipping. using stochastic clipping instead setting components reassign clipped components uniformly random allowed range. can’t guard interior local minima helps local minima contain components stuck boundary. summarize experimental ﬁndings. experiments conducted dcgans described radford re-implemented tensorﬂow amos first visualize reconstruction process showing initialization iterations iterations reconstruction produces image indistinguishable original. figure z-space reconstruction error grows proportional added noise. stochastic clipping appears robust noise methods. pixel values scaled noise variance corresponds standard deviation pixel values. next consider ﬁdelity reconstruction updates. table show even conservative thresholds determining reconstruction success stochastic thresholding recovers percent latent vectors. evaluate numbers using examples. consider robustness reconstructions noise. apply gaussian white noise attempting reconstruct experiments show even substantial levels noise reconstruction error z-space appears grow proportionally added noise finally whether unseen images recovered vector always same. determine consistency recovered vector recover vectors image plot average pair-wise distance reconstructions. table average pairwise distance recovered vectors unseen images. baseline score average distance random vectors sampled randomly latent space. show generators practice inverted arbitrary precision. inversions robust noise inversions appear unique even unseen images. stochastic clipping accurate robust standard clipping. suspect stochastic clipping also give better robust reconstructions images discriminative reconstructions leaving experiments future work. aravindh mahendran andrea vedaldi. understanding deep image representations inverting them. proceedings ieee conference computer vision pattern recognition", "year": 2017}