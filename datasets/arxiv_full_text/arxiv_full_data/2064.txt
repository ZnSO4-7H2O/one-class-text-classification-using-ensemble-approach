{"title": "How To Grade a Test Without Knowing the Answers --- A Bayesian Graphical  Model for Adaptive Crowdsourcing and Aptitude Testing", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "We propose a new probabilistic graphical model that jointly models the difficulties of questions, the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings. We devise an active learning/adaptive testing scheme based on a greedy minimization of expected model entropy, which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses. We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario.", "text": "propose probabilistic graphical model jointly models diﬃculties questions abilities participants correct answers questions aptitude testing crowdsourcing settings. devise active learning/adaptive testing scheme based greedy minimization expected model entropy allows eﬃcient resource allocation dynamically choosing next question asked based previous responses. present experimental results conﬁrm ability model infer required parameters demonstrate adaptive testing scheme requires fewer questions obtain accuracy static test scenario. collective decision making well-studied topic social choice voting artiﬁcial intelligence. long known decisions based aggregating opinions several agents higher quality based opinions single individuals. condorcet jury theorem dating back century concerned group individuals attempting reach binary decision majority vote; assumed outcomes vote correct individual independently chooses correct response probability theorem states adding agents increases probability making correct decision probability collective decision correct approaches limit inﬁnitely many participating agents. recent technological advances make easier share opinions knowledge enabling harness collective intelligence crowds solving tasks. companies crowdsourcing carry business tasks using platforms amazon mechanical turk. services allow collect opinions many individuals leave open question aggregate collected data reach decisions. technique solving tasks using collective intelligence obtain information multiple sources aggregate single complete solution. consider crowd experts assigned many similar classiﬁcation tasks classifying many news articles according topic refer expert participant classiﬁcation task question. suppose participant expresses opinion regarding correct answer question form response chosen list possible answers question. similarly condorcet’s jury theorem make simplifying assumption questions answer correct. call domain multiple problem domain. given responses provided participants regarding various questions multiple problem domain best determine correct answer items? questions easy hard? competent participants crowd? questions best test ability participant? given correct answers items easy competent participants differentiate easy hard questions participant answered almost questions correctly likely skilled participant correct responses. typically however correct answers known advance whole point crowdsourcing classiﬁcation task determine correct classiﬁcations items. possible solution problem ﬁrst evaluate skill expert asking provide responses items correct answer known prominent example this correct answers known psychologists studied human intelligence designed tests evaluating aptitude individuals. tests shown predictive person’s performance many domains academic attainment success tests typically participants respond questionnaires composed many multiplechoice questions allow ranking participants according individual skill levels examining responses. properties responses tests widely studied psychometricians datasets serve testing ground exploring inference models multiple problem domains. contribution propose family graphical models analyzing responses multiple problem domains evaluate models data completed questionnaires standard test. proposed framework enables jointly infer correct answer question diﬃculty levels questions ability participant. show model determine probability distribution answers given question aggregating responses participants based abilities questions’ diﬃculties; test ability levels participants eﬃciently ﬁnding best next question adaptive depending previous responses; automatically calibrate aptitude tests questions responses provided participants determining relative diﬃculty levels questions ability discriminate between participants similar uneven skill levels. measuring intelligence topic psychology. psychologists showed peoples’ performance many cognitive tasks strongly correlated single statistical factor called general intelligence emerges measure performance groups people joint tasks called collective intelligence investigated approach focuses explicit collaboration interaction members crowd. although setting participants interact directly view model method inferring correct answers questions given responses crowd individuals. number correct answers inferred serve measure intelligence crowd. sense work somewhat similar approaches also aggregated responses tests measuring collective intelligence psychometricians developed body theory called test theory analyzes outcomes psychological testing ability levels participants diﬃculty questions test trying improve reliability tests paradigm designing tests mental abilities item-response theory used develop high-stakes adaptive tests graduate management admission test based idea probability participant providing correct response question function parameter question parameter item applying aptitude tests parameter person latent manifestation form participant’s responses directly observed. framework relies probabilistic graphical model using themes similar irt. many papers deal merging opinions ranging information aggregation semantic prediction markets frameworks probabilistic relational models combine logical representation probabilistic semantics allow inference aggregate information opinions. basic method collective decision making voting. voting studied social choice theory focuses participants manipulate lying preferences. assume experts’ responses true opinion focus inference problem. application model aggregating crowdsourced opinions. machine learning approach model task diﬃculty proposed technique models task diﬃculty uses approach proposed anmethod based graphical models model questions endowed features could represent concepts topics participants diﬀerent areas expertise matching topics. model focuses general domain spirit test theory rely speciﬁc features. active learning approach labeling data proposed similar adaptive testing technique. another approach akin trueskill system uses graphical model estimate relative skill levels people based past contests. present probabilistic model analyzing multiple problem domains refer diﬃculty-ability-response estimation model dare short. inputs model responses participants give multiple choice questions. additional inputs ground truth information questions. model falls framework probabilistic graphical models. models allow structurally describing generative process assumed underlie observed data terms latent observed random variables. domain interest information correct response question ability participant diﬃculty question modeled unobserved variables whereas given response question user viewed observed variable. structure model determined conditional independence assumptions made variables model. pearl introduced bayesian networks encode assumptions conditional independence graph whose vertices represent variables whose edges represent dependencies variables. general notion factor graph e.g. describe factorial structure assumed joint probability distribution among variables. deﬁning structure model factor graph setting observed variables observed values approximate message passing algorithms infer marginal probability distributions unknown variables interest correct response question tions. assume question possible answers which produce responses questions lying ability determines ability determine correct answer questions question inherent diﬃculty determines likely participants propose joint probabilistic model whose factor graph given figure model parts modeling probability participant knowing correct answer question relating true answer question response given participant depending knowing correct answer represented answer knowledge determines discriminative question integral representation emphasizes probability viewed emerging binary process resulting evaluating step function variable added gaussian noise variance response modeled mixture distributions. participant knows correct answer question constrain response match correct answer otherwise assume sampled uniformly random mixture expressed gate switches factor connecting depending state variable cpq. gates introduced powerful ﬂexible notation simpliﬁes factorgraph representations mixture models. diﬃculties abilities normal. choose gaussian prior lets specify range plausible values based parameters variable admits relatively simple approximate inference. factorization assumption reﬂects belief priori knowing diﬃculty question would informative diﬃculty another question similarly abilities participants. also assume factorizing discrete uniform priors gamma prior conveniently parameterized shape parameter scale parameter conjugate prior precision parameter normal distribution mean known. choice simpliﬁes inference approximate message passing posterior also takes functional form gamma distribution. based speciﬁcation deﬁned joint probability distribution speciﬁc pairs question participant assuming exchangeability questions participants model plates depicted figspecial case ground-truth questionanswer pairs known. traditional test scenario used aptitude tests including school tests gmat tests etc. another special case crowdsourcing domains ground-truth available obtain participantquestion-response triples depending budget time constraints. shown section providing ground-truth question-answer pairs improve accuracy inferred answers used assess abilities participants accurately leading accurate inference answers generally every observed response discrete prior distribution single point distribution concentrated observed response similarly every known ground-truth question-answer pair discrete distribution correct answers assign probability possible responses gaussian density abilities participants means variances gaussian density diﬃculties questions means variances bernoulli distribution correctness participant rediscrete distribution responses bility πpqr possible responses gamma distribution inference model done using approximate message passing used infer.net package probabilistic inference. speciﬁcally used expectation-propagation algorithm presented allows calculate marginal distributions interest given factor graph iteratively calculating messages along edges propagate information across factor graph. case provides approximation exact soluadaptive testing ground-truth answers available goal determine ability participant accurately possible using questions possible. special case parameter vector includes ability participant posterior distribution inclusion observation normal normal posterior distribution inclusion normal pm+). entropy univariate gaussian parameters thus response minimizing posterior variance preferred. given participant possible question expectation epm] calculated examining following quantities possible responses probabilities resulting posterior variances updated model. compute expected entropy reduction question empirically tested dare model discussed section using dataset responses standard intelligence test called raven’s standard progressive matrices falls within category multiple choice domains. consists sixty questions consists matrix shapes element missing eight possible answers. answer possible shape completes matrix answer correct. sample item similar shown figure popular intelligence tests used research clinical purposes. tion underlying factor graph loopy messages junction approximations messages going gate connected cpq. thus iteratively convergence running time linear input size joint probabilistic model data number advantages including ability query diﬀerent distributions interest handle missing data principled way. addition maintaining information uncertainty present model allows reason impact future observations model uncertainty. idea forms basis active learning variant known psychometrics literature adaptive testing. often considerable cost associated obtaining additional data points model data determine measurements take next improve inferred knowledge according pre-determined criterion. absence problem speciﬁc information reasonable goal reducing uncertainty estimates model parameters measured entropy posterior distributions idea forward suppose determined model parameters interest denoted vector goal criterion decide response elicit order maximally reduce posterior entropy parameters deﬁned arbitrary base measure inﬂuence outcome consider posterior distributions inclusion data point inclusion data point rpq. maximizing entropy reduction choice response elicit. since actual response unknown choice guided expected entropy reduction epm] expectation taken predictions model inclusion data point i.e. based predictive disfull generality active learning scheme guide full observation/measurement process including possible responses ground truth answers however focus case given answer correct. minimize probability error select mode distribution model’s answer question. provided responses participants dare model correctly infers correct responses questions. note number errors surprising items test diﬃcult participants answered correctly. highest score ﬁfty even scoring participant answered items incorrectly. calculate participant’s score respect true correct answers respect predicted correct answers refer latter score model score. crowdsourcing situations correct answer question unknown model scores estimate participants’ abilities. figure shows scatter plot point represents participant; position x-axis represents participant’s score position along y-axis model score. figure indicates strong correlation true scores model scores diﬀerence scores rather small across participants. think dare aggregator receives responses crowd participants outputs inferred answer question. existing work tests simpler aggregators using test data. former uses majority voting latter consider ability levels participants assumes items equal diﬃculty. another possible simplifying assumption examined earlier work participants equal ability. contrast dare model probability participant know correct answer depends diﬃculty question ability participant refer model diﬀerent question diﬃculties question model model diﬀerent participant abilities participant model. examine simpliﬁcations aﬀect model’s ability infer correct answers function amount available data. figure shows well question participant dare models perform regard. given crowd size shown x-axis randomly selected subsets participants size. crowd inferred correct answer question using model used number questions inferred answer equal true correct answer measure model’s performance. y-axis quality model averaged sampled crowds. figure shows ability models infer correct answers increases amount data. also shows dare outperforms simpler models. interestingly modeling participants ability better modeling question diﬃculty crowdsourcing data examine applicability model crowdsourcing tested model trec crowdsourcing track dataset generated crowdsourced workers classiﬁed search engine responses queries querydocument pair question workers must determine document relevant query. dataset sparse workers examined questions questions answers total includes ground-truth judgements. isolated questions answers workers answered questions analysis shows dare slightly outperforms majority voting dataset. majority voting gets questions correct dare gets correct. also tested well dare estimate participants skills similarly figure although crowdsourcing dataset resulting scatter plot quite noisy similar figure examine situations participants ﬁrst tested gold-set questions correct answer known. consider choosing questions making correct answer questions observable model. reveal corallow model better estimate ability levels participants turn allows model better infer correct answer remaining items. figure shows eﬀect dare. x-axis represents number revealed questions y-axis represents proportion remaining questions model inferred right answer. number revealed items sampled crowds participants revealed questions location y-axis average proportion remaining questions model inferred right answer sample. ﬁgure shows larger gold-set increases model’s ability infer correct response remaining items. show dare used adaptive skill testing. given budget questions goal infer participants’ ability levels. dare estimate participant’s score observing participant’s responses asked questions static approach budget choose speciﬁc heuristically choose question set. selected static question given budget choosing questions equally partition participant population terms fraction participants solved question example budget selected question roughly thirds participants solved correctly roughly third participants solved incorrectly. also implemented adaptive testing scheme section compared baseline static approach. adaptive approach next question depends participant’s response earlier questions reveal participant’s responses time. measure rmse given budget simulated adaptive process participants averaged errors across participants. figure shows rmses static adaptive approaches diﬀerent budget levels. shows adaptive approach smaller error inferred ability levels given budget. static approach essentially access information regarding diﬃculty questions normally available. analysis shows active approach beats static approach even static approach information. levels participants multiple problem domains. evaluation model shows joint inference quantities possible high level accuracy indeed possible grade test without knowing answers. showed setting modeling participants’ ability levels important questions’ diﬃculty levels including gold-set helps active learning leads eﬃcient testing. approach subject several limitations. evaluation used dataset whereas crowdsourcing tasks exhibit diﬀerent properties greater homogeneity task diﬃculty levels. also assume participants answer best ability participants selﬁsh agents varying motives. game theoretic treatment issues many questions open future research. better models aggregating responses models better tailored domains? tractably compute optimal non-adaptive test given population? similar models infer ability levels individuals performance within context group known?", "year": 2012}