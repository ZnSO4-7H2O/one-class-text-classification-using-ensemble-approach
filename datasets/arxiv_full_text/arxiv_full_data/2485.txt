{"title": "Scalable Recommendation with Poisson Factorization", "tag": ["cs.IR", "cs.AI", "cs.LG", "stat.ML"], "abstract": "We develop a Bayesian Poisson matrix factorization model for forming recommendations from sparse user behavior data. These data are large user/item matrices where each user has provided feedback on only a small subset of items, either explicitly (e.g., through star ratings) or implicitly (e.g., through views or purchases). In contrast to traditional matrix factorization approaches, Poisson factorization implicitly models each user's limited attention to consume items. Moreover, because of the mathematical form of the Poisson likelihood, the model needs only to explicitly consider the observed entries in the matrix, leading to both scalable computation and good predictive performance. We develop a variational inference algorithm for approximate posterior inference that scales up to massive data sets. This is an efficient algorithm that iterates over the observed entries and adjusts an approximate posterior over the user/item representations. We apply our method to large real-world user data containing users rating movies, users listening to songs, and users reading scientific papers. In all these settings, Bayesian Poisson factorization outperforms state-of-the-art matrix factorization methods.", "text": "netﬂix. netﬂix data contains ratings users movies organized matrix cells data extract patterns users’ interests movies associated interests. left panel illustrates patterns—the algorithm uncovered action movies independent comedies science ﬁction. panel illustrates patterns form recommendations user. user enjoys various types movies including fantasy classic science ﬁction independent comedies course seen handful available movies. ﬁrst uses movies seen infer kinds movies interested uses inferred interests suggest movies. list movies bottom ﬁgure suggested algorithm. includes comedies science ﬁction users items. associates user latent vector preferences item latent vector attributes constrains sets vectors sparse non-negative. cell observed behavior matrix assumed drawn poisson distribution—an exponential family distribution non-negative integers—whose parameter linear combination corresponding user preferences item attributes. main computational problem posterior inference given observed matrix user behavior discover latent attributes describe items latent preferences users. example components figure illustrate items speciﬁc attribute dimensions plot figure illustrates estimated preference vector given user. spike preference vector implies user tends like items corresponding latent attribute. general procedure common many variants matrix factorization. found however enjoys signiﬁcant quantitative advantages classical methods wide variety data sets including implicit feedback explicit feedback figure shows hierarchical variant perform signiﬁcantly better existing methods—including industry standard matrix factorization user item biases —for large data sets netﬂix users watching movies last.fm users develop hierarchical poisson matrix factorization recommendation. models sparse user behavior data large user/item matrices user provided feedback small subset items. handles explicit ratings number stars implicit ratings views clicks purchases. develop variational algorithm approximate posterior inference scales massive data sets demonstrate performance wide variety real-world recommendation problems–users rating movies users listening songs users reading scientiﬁc papers users reading news articles. study reveals hierarchical poisson factorization deﬁnitively outperforms previous methods including nonnegative matrix factorization topic models probabilistic matrix factorization techniques. recommendation systems vital component modern web. help readers eﬀectively navigate otherwise unwieldy archives information help websites direct users items—movies articles songs products—that like. recommendation system built user behavior data historical data items user consumed clicked viewed rated purchased. first uncover behavioral patterns characterize various types users kinds items tend like. then exploit discovered patterns recommend future items users. paper develop poisson factorization algorithms recommendation. algorithms easily scale massive data signiﬁcantly outperform existing methods. show poisson factorization recommendation tailored real-world properties user behavior data heterogenous interests users varied types items realistic distribution ﬁnite resources users consume items. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copy otherwise republish post servers redistribute lists requires prior speciﬁc permission and/or fee. copyright x-xxxxx-xx-x/xx/xx ..... figure panel shows movies components user netﬂix data set. bottom panel illustration showing subset highly rated movies user right panel shows movies recommended user algorithm. expected user’s k-vector weights inferred algorithm shown middle panel. main advantages poisson factorization traditional methods contribute superior empirical performance. first better captures real consumption data speciﬁcally users ﬁnite resources view items. this rewrite model stage process user ﬁrst decides budget movies watch spends budget watching movies interested model accurately captures distribution budgets watched items carry weight unwatched items because unwatched items partially explained lack resources. conjecture classical matrix factorization systematically overestimates users’ budgets conﬁrm hypothesis section using posterior predictive check misﬁt leads overweighting zeros explains practitioners require complex methods downweighting poisson factorization need modiﬁed way. second advantage algorithms need iterate viewed items observed matrix user behavior i.e. non-zero elements true even implicit positive only data sets. thus poisson factorization takes advantage natural sparsity user behavior data easily analyze massive real-world data. contrast classical matrix factorization based gaussian distribution must iterate positive negative examples implicit setting. thus cannot take advantage data sparsity makes computation diﬃcult even modestly sized problems. example cannot full netﬂix data without appealing stochastic optimization note algorithms also amenable stochastic optimization analyze data sets even larger studied. roots poisson factorization come nonnegative matrix factorization objective function equivalent factorized poisson likelihood. original update equations shown expectationmaximization algorithm maximum likelihood estimation poisson model data augmentation placing gamma prior user weights results model developed alternative text model latent dirichlet allocation model using expectation-maximization algorithm obtain point estimates user preferences item attributes. probabilistic factor model improves upon placing gamma prior item weights well using multiplicative update rules infer approximate maximum posteriori estimate latent factors. contrast explained below model uses hierarchical prior structure gamma priors user item weights gamma priors rate parameters weights drawn. enables accurately model skew user activity item popularity contributes good predictive performance. furthermore approximate full posterior latent factors using scalable variational inference algorithm. independently user behavior models poisson factorization studied context signal processing source separation purpose detecting community structure network data research includes variational approximations posterior though issues details around data diﬀer signiﬁcantly user data consider derivation direct. modeling implicit feedback data sets researchers proposed merging factorization techniques neighborhood models weighting techniques adjust relative importance positive examples samplingbased approaches create informative negative examples addition diﬃculty appropriately weighting sampling negative examples known selection bias provided ratings causes complication. estimate posterior expectation user’s preferences items attributes subsequently form predictions unconsumed items user like. discuss posterior inference section modeling details place highlight several statistical properties hierarchical poisson factorization. properties provide advantages classical matrix factorization. captures sparse factors. mentioned above gamma priors preferences attributes encourages sparse representations users items. speciﬁcally setting shape parameter small weights close zero large. models long-tail users items. statistical characteristic real-world user behavior data distribution user activity item popularity distributions tend long-tailed users consume handful items tail users consume thousands items. question statistical model user behavior data well captures distributions. found captures well classical matrix factorization not. check this implemented posterior predictive check technique model assessment bayesian statistics literature. idea behind simulate complete data posterior predictive distribution—the distribution data posterior induces—and compare generated data true observations. good model produce data captures important characteristics observed data. developed matrix factorization algorithms user behavior data. first formed posterior estimates user preferences item attributes classical hpf. then estimates simulated user behavior drawing values user item. finally compared matrix generated posterior predictive distribution true observations. speciﬁcally classical matrix factorization mean regularized matrix factorization bias terms users items using stochastic gradient descent without bias terms corresponds maximum aposteriori inference probabilistic matrix factorization given data users items user consumed possibly rated items. observation rating user gave item zero rating given. user behavior data purchases ratings clicks views typically sparse. values matrix zero. model data factorized poisson distributions item represented vector latent attributes user vector latent preferences observations modeled poisson parameterized inner product βi). variant probabilistic matrix factorization user item’s weights positive poisson replaces gaussian. beyond basic data generating distribution place gamma priors latent attributes latent preferences encourage model towards sparse representations users items. furthermore place additional priors user item-speciﬁc rate parameter gammas controls average size representation. hierarchical structure allows capture diversity users tending consume others diversity items popular others. literature recommendation systems suggests good model must capture heterogeneity across users items process describes statistical assumptions behind model. note also study sub-class rate parameters users items pair hyperparameters. call model bayesian poisson factorization quadratic loss seeing star wars evidence liking science ﬁction seeing empire strikes back evidence disliking poisson model however prefer bring user’s latent weights closer movies’ weights favors information user watching star wars. further movies similar increases poisson model’s predictive score user watches star wars also watch empire strikes back. fast inference sparse matrices. finally likelihood observed data depends consumed items non-zero elements user/item matrix facilates computation kind sparse matrices observe real-world data. classical enjoy property. methods especially applied massive data sets implicit feedback must iterate cells matrix. practitioners require solutions sub-sampling approximation stochastic optimization using recommendation hinges solving posterior inference problem. given observed ratings would like infer user preferences item attributes explain ratings inferences recommend content users. section discuss details practical challenges posterior inference present mean-ﬁeld variational inference algorithm practical scalable approach. algorithm easily accommodates data sets millions users hundreds thousands items single cpu. given matrix user behavior would like compute posterior distribution user preferences item attributes user activity item popularity many bayesian models however exact posterior computationally intractable. show eﬃciently approximate posterior mean-ﬁeld variational inference. variational inference optimization-based strategy approximating posterior distributions complex probabilistic models variational algorithms posit family distributions hidden variables indexed free variational parameters member family closest kullback-liebler divergence true posterior. thus variational inference turns inference problem optimization problem. variational inference tends scale better alternative samplingbased approaches like monte carlo markov chain sampling deployed solve many applied problems complex models including large-scale recommendation describe simple variational inference algorithm figure posterior predictive check distribution total ratings netﬂix data set. pink curve shows empirical count number users rated given number items green blue curves show simulated totals ﬁtted poisson traditional matrix factorization models respectively. poisson marginal closely matches empirical whereas classical matrix factorization large mean account skew distribution missing ratings. data replicated distribution data replicated classical captures truth much closely classical badly overestimates distribution user activity. indicates better represents real data measured ability capture distributions user activity item popularity. downweights eﬀect zeros. another advantage implicitly down-weights contribution items user consume. appropriate user activity model ways explaining unconsumed item either user interested would interested likely active. contrast user consumes item must interested thus model beneﬁts making consumed user/item pair similar making unconsumed user/item pair less similar. classical based gaussian likelihoods gives equal weight consumed unconsumed items. consequently faced sparse matrix implicit feedback i.e. binary consumption data matrix factorization places total emphasis unconsumed user/item pairs. address this researchers patched complex ways example including per-observation conﬁdences considering zeroes hidden variables poisson factorization naturally solves problem better capturing user activity. example consider similar science ﬁction movies star wars empire strikes back consider user seen them. gaussian model pays equal penalty making user similar items making user diﬀerent them— though variables independent ﬂexible family distributions variable governed free parameter. variational factors preferences attributes activity popularity gamma distributions freely scale rate variational parameters. variational factor free multinomial i.e. k-vector sums one. form stems bank poisson variables conditional ﬁxed property conditional poissons distributed multinomial optimize variational parameters coordinate ascent algorithm iteratively optimizing parameter holding others ﬁxed. algorithm illustrated figure denote shape superscript rate superscript rte. note algorithm eﬃcient sparse matrices. step need update variational multinomials non-zero user/item observations yui. steps sums users items need consider nonzero observations. eﬃciency thanks likelihood full matrix depending non-zero observations discussed previous section. terminate algorithm variational distribution converges. convergence measured computing prediction accuracy validation set. speciﬁcally approximate probability user consumed item using variational approximations posterior expectations compute average predictive likelihood validation ratings. algorithm stops change likelihood less algorithm largely insensitive small changes hyper-parameters. enforce sparsity shape hyperparameters provide exponentially shaped prior gamma distributions. ﬁxed hyperparameter hyperparameters ﬁxing prior mean evaluate performance hierarchical poisson factorization algorithm non-hierarchical variant variety large-scale user behavior data sets users listening music users watching movies users reading scientiﬁc articles users reading newspaper. give signiﬁcantly better recommendations competing methods. ﬁrst discuss details data competing recommendation methods. describe study noting superior performance computational eﬃciency hpf. conclude exploratory analysis preferences attributes several data sets. hpf. however ﬁrst give alternative formulation model additional layer latent variables. auxiliary variables facilitate derivation description algorithm poisson integers user/item value yui. poisson random variables poisson rate equal rates. thus latent variables preserve marginal distribution βi). variables thought contribution component total observation yui. note auxiliary variables random—the posterior distribution place mass zero vector. consequently inference procedure need consider user/item pairs latent variables place describe algorithm. first posit variational family hidden variables. show optimize parameters member close posterior interest. latent variables model user weights item weights user-item contributions zuik represent k-vector counts zui. mean-ﬁeld family considers variables independent figure predictive performance data sets. bottom plots show normalized mean precision mean recall recommendations respectively. competing method performance varies across data sets consistently outperform competing methods. scale diversity data sets enables robust evaluation algorithm. mendeley echo nest york times data sparse compared netﬂix. example observe possible user-item ratings mendeley ratings non-zero netﬂix data. partially reﬂection large number items relative number users data sets. furthermore intent signaled observed rating varies signiﬁcantly across data sets. instance netﬂix data gives direct measure stated preferences items users provide explicit star rating movies watched. contrast article click counts york times data less clear measure much user likes given article—most articles read once click weak indicator whether article fully read alone liked. ratings echo nest data presumably fall somewhere between number times user listens song likely reveals indirect information preferences. such treat data source implicit feedback observed positive rating indicates user likes particular item rating value ignored. mendeley data already simple binary form. echo nest york times data consider song play article click positive rating regardless play click count. also consider versions netﬂix data—the original explicit ratings implicit version star ratings retained observations user preferences item attributes modeled non-negative vectors low-dimensional space. latent vectors randomly initialized modiﬁed alternating multiplicative update rule minimize kullback-leibler divergence actual modeled rating matrices. probabilistic generative model user preferences represented distribution diﬀerent topics topic distribution items. interest topic distributions randomly initialized item biases. variant matrix factorization popularized netﬂix prize linear predictor—comprised constant term user activity item popularity biases low-rank interaction term—is minimize mean squared error predicted observed rating values subject regularization avoid overﬁtting. weights randomly initialized updated stochastic gradient descent using vowpal wabbit package corresponds maximum a-posteriori inference probabilistic matrix factorization note take non-zero observed ratings input traditional matrix factorization requires provide explicit zeros ratings matrix negative examples implicit feedback setting. practice amounts either treating missing ratings zeros down-weighting balance relative importance observed missing ratings generating negatives randomly sampling missing ratings training take latter approach computational convenience employing popularity-based sampling scheme sample users activity—the number items rated training set— items popularity—the number training ratings item received generate negative examples. finally note couple candidate algorithms failed scale data sets. fully bayesian treatment probabilistic matrix factorization uses mcmc algorithm inference. authors report single gibbs iteration netﬂix data latent factors requires minutes throw away ﬁrst samples. implies least days training variational inference algorithm converges within hours netﬂix data. another alternative bayesian personalized ranking optimizes ranking-based criteria using stochastic gradient descent. algorithm performs expensive bootstrap sampling step iteration generate negative examples vast unobserved. found time space constraints prohibitive attempting data sets considered here. evaluation. prior training models randomly select ratings data used heldtest comprised items user consumed. additionally aside training ratings validation determine algorithm convergence tune free parameters. used settings described section across data sets number latent components testing generate recommendations user items highest predictive score method. user compute variant precision-at-m measures fraction relevant items user’s top-m recommendations. artiﬁcially deﬂate measurement lightly active users consumed fewer items compute normalized precision-at-m adjusts denominator number items user test set. likewise compute recall-at-m captures fraction items test present recommendations. figure shows normalized mean precision recommendations method data sets. outperform methods data sets sizeable margin—as much percentage points. poisson factorization provides high-quality recommendations—a relatively high fraction items recommended found relevant many relevant items recommended. shown plots relative performance methods within data consistent vary number recommendations shown users. also note poisson factorization dominates across data sets relative quality recommendations competing methods varies substantially data next. instance performs quite well echo nest data fails beat classical matrix factorization implicit netﬂix data set. also study precision recall function user activity investigate performance varies across users diﬀerent types. particular figure shows mean diﬀerence precision recall recommendations look performance users varying activity measured percentile. example mark shows mean performance across bottom users least active; mark shows mean performance active users. poisson factorization outperforms methods users activity levels—both light users constitute majority relatively heavy users consume more—for data sets. exploratory analysis. ﬁtted model explored discover latent structure among items users conﬁrm model capturing components data reasonable way. example figure illustrate components discovered algorithm scientiﬁc articles mendeley news articles york times. data illustration shows items—items sorted decreasing order expected weight βi—from three components discovered algorithm. these learned components across diﬀerentiate conventional topics categories. instance york times data multiple business-related topics comprise separate components whereas articles appear across diﬀerent sections newspaper uniﬁed content demonstrated poisson factorization efﬁcient eﬀective means generating high quality recommendations across variety data sets ranging movie views scientiﬁc article libraries. signiﬁcantly outperforms existing recommendation methods explicit rating data implicit behavior data without need modiﬁcations. poisson factorization algorithms scale massive data diﬀer traditional methods ability capture heterogeneity amongst users items accounting wide range activity future work includes extensions provide coldstart recommendations using text data infer number latent components using bayesian nonparametric assumptions stochastic variational inference analyze data sets larger studied. given observed matrix user behavior would like compute posterior distribution user preferences item attributes user activity item popularity derivation variational algorithm complete conditionals. variational inference variational parameters minimize divergence posterior. large class conditionally conjugate models easily perform optimization coordinate-ascent algorithm iteratively optimize variational parameter holding others ﬁxed. complete conditional conditional distribution latent variable given observations latent variables model. conditionally conjugate model complete conditional exponential family. distributions stem conjugacy properties gamma poisson. user weight distribution example item weights exposure variables similarly write complete conditionals user activity item popularity expectations complete conditionals equations shape parameter expected count item multinomial yuiφuik. rate parameter expectation gamma variable shape divided rate. though variables poisson model complete conditional multinomial. reason conditional distribution poisson variables given multinomial parameter normalized rates. deriving algorithm. derive variational inference hpf. first factor mean-ﬁeld family type distribution complete conditional. complete conditionals item weights user weights gamma distributions thus variational parameters gamma parameters containing shape rate. similarly variational user activity parameters variational item popularity parameter gamma parameters containing shape rate. complete conditional auxiliary variables zuik multinomial thus variational parameter multinomial parameter point k-simplex variational distribution mult. coordinate ascent iteratively optimize variational parameter holding others ﬁxed. conditionally conjugate models amounts setting variational parameter equal expected parameter complete conditional. parameter complete conditional function latent variables mean-ﬁeld family sets variables independent. facts guarantee parameter optimizing appear expected parameter.", "year": 2013}