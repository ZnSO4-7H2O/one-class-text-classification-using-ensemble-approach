{"title": "From Machine Learning to Machine Reasoning", "tag": ["cs.AI", "cs.LG"], "abstract": "A plausible definition of \"reasoning\" could be \"algebraically manipulating previously acquired knowledge in order to answer a new question\". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labeled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.  This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated \"all-purpose\" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.", "text": "plausible definition \"reasoning\" could \"algebraically manipulating previously acquired knowledge order answer question\". definition covers first-order logical inference probabilistic inference. also includes much simpler manipulations commonly used build large learning systems. instance build optical character recognition system first training character segmenter isolated character recognizer language model using appropriate labelled training sets. adequately concatenating modules fine tuning resulting system viewed algebraic operation space models. resulting model answers question converting image text page computer readable text. observation suggests conceptual continuity algebraically rich inference systems logical probabilistic inference simple manipulations mere concatenation trainable learning systems. therefore instead trying bridge machine learning systems sophisticated \"all-purpose\" inference mechanisms instead algebraically enrich manipulations applicable training systems build reasoning capabilities ground since learning reasoning essential abilities associated intelligence machine learning machine reasoning received much attention short history computer science. statistical nature learning well understood statistical machine learning methods commonplace internet search support vector machines returns million pages. nature reasoning proven elusive. although computer algorithms logical inference share roots foundations mathematics converting ordinary data consistent logical expressions proved challenging searching discrete spaces symbolic formulas often leads combinatorial explosion computer algorithms general probabilistic inference still suffer unfavorable computational properties however practical algorithms many special cases interest. algorithms gained considerable popularity machine learning community. practicality comes price reduced expressive capabilities since probabilistic inference mathematical construction easily described using first order logic; converse true. particular expressing causality first order logic simple expressing causality probabilities challenging human reasoning displays neither limitations logical inference probabilistic inference. ability reason often confused ability make logical inferences. observe visual scene hear complex sentence able explain formal terms relation objects scene precise meaning sentence components. however evidence formal analysis necessarily takes place scene hear sentence know mean. suggests existence middle layer already form reasoning formal logical. investigating informal reasoning attractive hope avoid computational complexity issues associated combinatorial searches vast space discrete logic propositions. minsky papert shown simple cognitive tasks cannot implemented using linear threshold functions require multiple layers computation. recent advances uncovered effective strategies train deep models deep learning attracted considerable interest machine learning community. regular workshops held nips icml conferences since ability train deep machine learning models appears related appropriate definition unsupervised auxiliary tasks help discovering internal representations inner layers models deep structures trained past using supervised intermediate tasks surprise deep learning results achieved using loosely related auxiliary tasks. deep learning therefore intimately related multi-task learning document presents idea results long maturation deep learning multi-task learning show leverage auxiliary tasks help solving task interest. apparently simple idea interpreted rudimentary form reasoning. enriching algebraic structure leads higher forms reasoning. provides path build reasoning abilities machine learning systems ground auxiliary tasks frequently mentioned problem scarcity labeled data. assertion biased usually build learning machine accomplish valuable task. corresponding training labels expensive therefore scarce. conversely labels available abundance often associated tasks valuable. make abundant labels useless vicinity interesting valuable task less valuable tasks provide opportunities approach initial problem. consider task identifying persons face images. despite increasing availability collaborative image tagging schemes certainly remains expensive collect label millions training images representing face subject good variety positions contexts. however easy collect training data slightly different task telling whether faces images represent person faces picture likely belong different persons; faces successive video frames likely belong person. tasks much common image analysis primitives feature extraction part recognizers trained auxiliary task certainly help solving original task. figure outlines transfer learning strategy involving three trainable modules. preprocessor computes compact face representation image. comparator compares representations associated images. classifier produces person label associated image representation. first assemble instances preprocessor comparator train resulting model using abundant labels auxiliary task. training simply works minimizing regularized loss function using stochastic gradient descent. assemble another instance preprocessor classifier train resulting model using restrained number labeled examples original task. works preprocessor already performs useful tasks vastly simplifies classifier alternatively could simultaneously train assemblages making sure instances preprocessor share parameters. comparable transfer learning systems achieved high accuracies vision benchmarks designed structurally complex system address various natural language processing benchmark tasks word embedding module computes -dimensional representation vocabulary word. fixed length sequences words extracted large corpus incorrect sequences created randomly replacing central word. auxiliary task consists producing score whose magnitude indicates whether sequence words genuine incorrect. assemblage several word embedding module ranking module trained task. benchmark tasks trained using smaller corpora labelled sentences. sentence processed assembling word embedding components routing outputs together ancillary information classifiers produce tags word interest system reaches near state-of-the-art performance running hundreds times faster natural language processing systems comparable performance. many natural language processing systems rely considerable linguistic knowledge went manual design task specific input features. system described learns useful features using essentially unsupervised task trained large corpus. figure illustrates quality resulting word representation. although modular learning systems training algorithms researched extensively little attention paid rules describe assemble trainable modules order address particular task. fact composition rules play extremely important role. dictionary elementary trainable modules composition operators form simple algebraic system space models. composition rules describe algebraic manipulations combine previously acquired knowledge form models previously trained auxiliary tasks order create model addresses task. would like point draw bold parallel algebraic manipulation previously acquired knowledge order answer question plausible definition word reasoning. significant differences conventional reasoning operates premises conclusions; composition rules operate trainable modules. easily argue history mathematics teaches algebraic structures significant objects operate. face recognition natural language processing examples implicit composition rules derive assumption internal representations learned auxiliary task benefit task interest. internal representations play role reasoning abstractions concepts cannot observed directly assumed relevant multiple problems. composition rules described different levels sophistication. like face recognition natural language processing examples works discussing multi-task learning construct adhoc combinations justified semantic interpretation internal representations. works discussing structured learning systems often provide explicit rules. instance graph transformer networks construct specific recognition training models input image using graph transduction algorithms. specification graph transducers viewed description composition rules rich algebraic structure probability theory plays important role appeal probabilistic models machine learning tells combine conditional probability distributions interpret combinations. however order construct algebraic structure probabilistic models necessary also discuss probability distributions parametrized. graphical models describe factorization joint probability distribution elementary conditional distributions specific conditional independence assumptions. factorization suggests individually parametrize elementary conditional distribution. probabilistic inference rules induce algebraic structure space conditional probability distribution models describing relations arbitrary subsets random variables. many refinements devised make parametrization explicit. plate notation compactly represents large graphical models repeated structures usually share parameters. figure shows treating parameters like random variable makes parametrization even explicit. recent works propose considerably richer languages describe large graphical probabilistic models. probabilistic relational models relational dependency networks derive graphical probabilistic models frame-based knowledge bases. markov logic networks derive graphical probabilistic models clauses first order logic knowledge base. high order languages describing probabilistic models expressions composition rules described previous section. clearly drifting away statistical approach longer fitting simple statistical model data. fact dealing complex object composed algebraic space models composition rules establish homomorphic correspondence space models space questions interest. call object reasoning system. potentially surprising consequence definition arbitrary nature reasoning system. like statistical models reasoning systems vary expressive power predictive abilities computational requirements. salient examples illustrate diversity first order logic reasoning consider space models composed functions predict truth value first logic formula function potential values free variables. functions usually represented collections fully instantiated predicates. space functions highly constrained algebraic structure first order logic formulas know functions apply logical inference deduct constrain functions associated formulas therefore representing different tasks. first order logic high expressive power bulk mathematics formalized first order logic statements sufficient however express subtleties natural language every first order logic formula easily expressed natural language; converse true. finally first order logic typically leads computationally expensive algorithms often involve combinatorial searches vast discrete spaces. probabilistic reasoning consider space models formed conditional probability distributions associated predefined collection random variables. conditional distributions highly constrained algebraic properties probability theory know subset conditional distributions apply bayesian inference deduct constrain additional conditional distributions therefore answer different questions continuous nature probability theory provides opportunities avoid computationally expensive discrete combinatorial searches. improved computational requirements come price reduced expressive capabilities since probabilistic inference mathematical construction easily described using first order logic; converse true. despite limitation inference probabilistic models popular machine learning topic. causal reasoning causality well known expressive limitation probabilistic reasoning. instance establish correlation events raining people carry open umbrellas. correlation predictive people carry open umbrellas pretty certain raining. correlation tells little consequences intervention banning umbrellas stop rain. serious limitation causes effects play central role understanding world. pearl proposes address issue enriching probabilistic machinery construction whereas represents distribution random variable given observation construction represents distribution intervention enforces condition y=y. newtonian mechanics classical mechanics extremely successful example causal reasoning system. consider motion point masses various experimental setups. newton's laws define abstract concept force cause explaining deviation uniform motion. second third laws describe compute consequences interventions applying force transferring point mass experimental setup another. instance first setup could weighting device measures relative masses point masses second setup could involve collision point masses spatial reasoning would visual scene change changes viewpoint manipulates objects scene? questions clearly obey algebraic constraints derive bi-dimensional projection relations objects. spatial reasoning require full logic apparatus certainly benefits definition specific algebraic constructions social reasoning changes viewpoint also play important role social interactions. placing oneself somebody else's shoes allows understand beliefs intents members society therefore plays essential cognitive role certainly challenging conceivable approach social interactions terms processes. non-falsifiable reasoning history provides countless examples reasoning systems questionable predictive capabilities. mythology interprets world applying social reasoning systems abstract deities. astrology attempts interpret social phenomena reasoning motion planets. like non-falsifiable statistical models non-falsifiable reasoning systems unlikely useful predictive capabilities ways face universe reasoning systems. approach would identify single reasoning framework strictly powerful others. whether framework exists whether leads computationally feasible algorithms unknown. symbolic reasoning fulfill hopes probabilistic reasoning practical considerably less expressive. second approach embrace diversity opportunity better match reasoning models applicative domain interest when solving given problem avoid solving general problem intermediate step therefore desirable universe reasoning systems. potential algebraic structures? footprint terms expressive power suitability specific applications computational requirements predictive abilities? unfortunately cannot expect theoretical advances schedule. however nourish intuitions empirically exploring capabilities algebraic structures designed specific applicative domains. replication essential human cognitive processes scene analysis language understanding social interactions forms important class applications. processes probably include form reasoning able facts explain conclusions logical arguments. however actual processes usually happen without conscious involvement suggesting full complexity logic reasoning required. algebraic reasoning primitive suitable applications? following sections describe specific ideas investigating reasoning systems suitable natural language processing vision tasks. defines trainable modules provide means represent arbitrary hierarchical structures using fixed size representation vectors. discussion includes preliminary results natural language processing tasks potential directions vision tasks. additional modules working space representations proposed. section concludes description conceptual algorithmic issues associated learning algorithms operating space. already demonstrated possibility learn salient word embeddings using essentially supervised task learn salient embeddings meaningful segment sentence? proven create rich algebraic system define operations take inputs certain space produce outputs space. number possible concatenations potential depth becomes infinite. consider collection trainable modules. word embedding module computes continuous representation word dictionary. preliminary experiments simple lookup table specifies vector -dimensional representation space word dictionary. coordinates vectors determined training algorithm. association module trainable function takes vectors representation space produces single vector space expected represent association input vectors. given sentence segment composed words figure shows applications association module reduce sentence segment single vector representation space. would like vector representation meaning sentence. would also like intermediate result represent meaning corresponding sentence fragment. many ways sequence applications association module associated particular bracketing sentence meaningful segments. figure instance corresponds standard bracketing sentence order determine bracketing splits sentence meaningful segments introduce additional saliency scoring module takes input vector representation space produces score whose magnitude expected measure meaningful corresponding sentence segment applying saliency scoring module intermediate results summing resulting scores yields global score measuring meaningful particular bracket sentence. meaningful recursively apply association module determined maximizing global score. specific parsing algorithms described later document. challenge train modules order achieve desired function. figure illustrates non-supervised training technique inspired collobert stochastic gradient procedure. iteration short sentence segment randomly picked large text corpus bracketed described arbitrary word replaced random word vocabulary. consequence certain intermediate results representation space likely correspond meaningless sentence fragments. would like make associated scores smaller scores associated genuine sentence segments. expressed adequate ranking loss function. parameters modules adjusted using simple gradient descent step. repeating iterative procedure corresponds stochastic gradient descent optimization well defined loss function. however evidence training works much faster starts short segments limited vocabulary size. preliminary results obtained using similar procedure sentence segments length five extracted dump english wikipedia vocabulary restricted frequent words initialized collobert embeddings. initial sentence segment brackets constructed randomly. order investigate resulting system maps word sequences representation space two-word sequences common words constructed mapped representation space. figure shows closest neighbors representation space sequences. analysis restricted two-word sequences computational cost grows exponentially sequence length. socher independently trained similar system supervised manner using section annotated penn treebank corpus. although much smaller corpus obtained meaningful representations. instance report phrases decline comment would disclose terms close induced embedding space. supervised training approach also provides objective assess results since compare bracketing performance system established parsers. report bracketing performance compares statistical parsing systems much work left accomplish including robustly addressing numerical aspects training procedure seamlessly training using supervised unsupervised corpora assessing value sentence fragment representations using well known benchmarks finding better navigate sentence fragment representations. introduce module address last problem. representation space vectors single vector. input meaningful output association module output inputs association module. stacking instance association module instance dissociation module equivalent auto-encoder recursively applying dissociation module provides convenient means traversing hierarchical representations computed stack association modules recursive auto-associative memory proposed connectionist representation infinite recursive structures comparable ideas proposed hinton domain definition dissociation module obvious. given vector representation space need know whether results association elementary vectors whether considered atomic. sperduti defines compares various labelling schemes purpose. however probably simpler saliency scoring module specify domain definition dissociation module sufficiently meaningful associations dissociated. definition dissociation module implies association module injective output uniquely defines inputs. property enforced training procedures outlined previous subsection. therefore necessary simultaneously train association dissociation modules. achieved augmenting earlier loss function terms apply dissociation module presumed meaningful intermediate representation measure close outputs inputs corresponding association module. association dissociation modules similar primitives cons car/cdr elementary operations navigate lists trees lisp computer programming languages. primitives used construct navigate arbitrary propositional logic expressions. main difference nature representation space. instead discrete space implemented pointers atoms using vectors continuous representation space. hand depth structure construct limited numerical precision issues. hand numerical proximity representation space meaningful property reduces computational cost search algorithms. multilayer stochastic gradient algorithms able discover meaningful intermediate representations first place. constructed means represent arbitrary phrases using continuous representation consider training variety modules. consider instance trainable module converts representation sentence present tense sentence past tense. parse initial sentence construct representation convert representation representation sentence past tense dissociation module reconstruct converted sentence. manipulations entirely open question. association dissociation modules limited natural language processing tasks. number state-ofthe-art systems scene categorization object recognition combination strong local features sift features consolidated along pyramidal structure similar pyramidal structures long associated visual cortex convolutional neural networks exploit idea interpreting pyramidal structures recursive application association module relatively straightforward drawback pyramidal structures fixed geometry. since local features aggregated according predefined pattern upper levels pyramid represent data poor spatial orientation accuracy. pyramidal recognition systems often work poorly image segmentation tools. instance designed large convolutional neural network identify label objects city scenes labelme corpus system provides good object recognition accuracies coarse segmentations parsing mechanism described natural language processing system provides opportunity work around limitation. attach intermediate representations image regions. initially image regions small patches used computation local features. guided scoring module evaluate saliency potential association association module opportunistically aggregate representations attached neighboring regions produce representation attached union input regions. training system could achieved supervised unsupervised modes using methods explained previous subsection. algebraic constraints enrich vision system. instance could consider modules transform vectors representation space account affine transformations initial image. interestingly maybe could consider modules transform representation vectors account changes position viewer. since viewpoint changes modify occlusion patterns modules provides interpretation three-dimensional geometry scene. since viewpoint changes also reveal hide entire objects modules could conceivably provide tool constructing vision system implements object permanence finally also envision modules convert image representations sentence representations conversely. training modules would provide means associate sentences images. given image could parse image convert final image representation sentence representation apply dissociation module reconstruct sentence. conversely given sentence could produce sketch associated image similar means. much work needed specify semantic nature conversions. return problem determining meaningful apply association module tersely defined maximization scores computed ranking component intermediate results. figure illustrates maximization algorithm template whose main element short-term memory able store collection representation vectors. possible actions inserting representation vector short-term memory applying association module representation vectors taken short-term memory replacing combined representation vector. application association module scored using saliency scoring module algorithm terminates neither action possible short-term memory contains single representation vector representation vectors insert. main algorithm design choices criteria decide representation vector inserted short-term memory representation vectors taken short-term memory associated. design choices determine data structure appropriate implementing short-term memory. instance case english language nearly syntactically meaningful sentence segments contiguous sequences words. therefore attractive implement short-term memory stack construct shift/reduce parser first action consists picking next sentence word pushing representation stack; second action consists applying association module stack elements replacing resulting representation. problem reduces determining sequence actions perform order maximize saliency scores. even simple case graph possible actions grows exponentially length sentence. fortunately heuristic beam search techniques available efficiently explore graph. also handle complicated ways organize short-term memory often without dramatically increasing computational complexity. greedy parsing algorithm extreme example consists first inserting word representations short-term memory repeatedly associating representation vectors highest association saliency. simple experimentation various compromises suggest works best application. parsing algorithm template consistent cognitive psychology view short-term memory. particular miller argues human short-term memory holds seven plus-or-minus chunks information. chunks loosely defined pieces information subject recognizes entity. definition depends knowledge subject contents long-term memory. case parsing algorithm template long-term memory represented trainable parameters association module scoring module previous sections essentially discuss association dissociation modules. also briefly mention couple additional modules modules perform predefined transformations natural language sentences; modules implement specific visual reasoning primitives; modules bridge representations sentences representation images. modules enrich algebraic reasoning structure endowing space representation vectors additional semantics. natural framework enhancements case natural language processing. operator grammars provide mathematical description natural languages based transformation operators starting elementary sentence forms complex sentences described successive application sentence transformation operators. structure meaning sentence revealed side effect successive transformations. since association dissociation modules provide means navigate sentence structure necessary tools replicate sentence transformation operators described harris establish connection important body linguistic work. also natural framework enhancements case vision. modules working representation vectors model consequence various interventions. viewpoint changes causes image rotations image rescaling perspective changes occlusion changes. could also envision modules modeling representation space consequences direct interventions scene moving object. also opportunity beyond modules merely leverage structure representation space. explained earlier association dissociation modules algebraically equivalent lisp primitives cons like primitives provide means construct arbitrarily propositional logic expressions. adding variables quantifiers would provide implementation first order logic. although connectionist approaches variable binding cannot avoid computational complexity first-order logic problems. would possible instead identify capabilities necessary kind informal intuitive reasoning humans carry ease? examples anaphora resolution consists identifying components tree designate entity. amounts identifying multiple occurrences variable first-order logic expression resolving pronouns case natural language sentences. could approached constructing instantiation module takes representation vector tree applies predefined substitution occurrences designated entity tree. identifying instantiations make sense could achieved trainable module returns high score iskindof relation representation vectors. ontologies often context dependent. instance turtle kinds pets context household members different families context biological classification. restricted entailment scoring module takes representations structurally similar trees returns high score first tree valid instantiation second one. score expresses relation differing tree branches context rest tree. although previous subsection present essential modules functions operating relatively lowdimensional vectorial space modules similar algebraic properties could defined different representation spaces. choices considerable impact computational practical aspects training algorithms. investigation therefore necessary. preliminary results obtained using dense vectors relatively dimension ranging dimensions order provide sufficient capabilities trainable functions must often designed nonlinear parametrization. training algorithms simple extensions multilayer network training procedures using gradient back-propagation stochastic gradient descent. nonconvex optimization procedures inherently complex often criticized lack robustness. hand properly implemented often turn effective methods available large-scale machine learning problems. sparse vectors much higher dimensional spaces attractive provide opportunity rely trainable modules linear parametrization training algorithms exploit simpler optimization procedures. order maintain good generalization abilities good computational performance sparsity inducing terms must included optimization criteria. terms also make optimization complex potentially negating benefits sparse high-dimensional vectors first place. representation space also space probability distributions defined vector discrete random variables. learning algorithms must expressed stochastic sampling techniques gibbs sampling mcmc contrastive divergence herding demands three weeks computation standard processor. reducing training time couple days changes dynamics experimentation. research directions outlined document intended breakthrough effort towards practical conceptual understanding interplay machine learning machine reasoning. instead trying bridge machine learning systems sophisticated \"all-purpose\" inference mechanisms instead algebraically enrich manipulations applicable training systems build reasoning capabilities ground possibility gives ways work around limitations logical probabilistic inference. path artificial intelligence? would like acknowledge nearly three years discussions yoshua bengio yann lecun former labs colleagues ronan collobert david grangier jason weston.", "year": 2011}