{"title": "Averaging Weights Leads to Wider Optima and Better Generalization", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much broader optima than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.", "text": "deep neural networks typically trained optimizing loss function variant conjunction decaying learning rate convergence. show simple averaging multiple points along trajectory cyclical constant learning rate leads better generalization conventional training. also show stochastic weight averaging procedure ﬁnds much broader optima approximates recent fast geometric ensembling approach single model. using achieve notable improvement test accuracy conventional training range state-of-the-art residual networks pyramidnets densenets shakeshake networks cifar- cifar- imagenet. short extremely easy implement improves generalization almost computational overhead. better understanding loss surfaces multilayer networks accelerate convergence stability accuracy training procedures deep learning. recent work shows local optima found connected simple curves near constant loss. building upon insight garipov also developed fast geometric ensembling sample multiple nearby points weight space create high performing ensembles time required train single dnn. weights networks ensembled periphery desirable solutions. observation suggests promising average points weight space rather model space. although general idea maintaining running average weights traversed dates back ruppert procedure typically used train neural networks. sometimes applied exponentially decaying running average combination decaying learning rate smooths trajectory conventional perform differently. however show equally weighted average points traversed cyclical constant learning rate refer stochastic weight averaging many surprising promising features training deep neural networks leading better understanding geometry loss surfaces. indeed cyclical constant learning rates used dropreplacement standard training multilayer networks improved generalization essentially overhead. particular show cyclic constant learning rates traverses regions weight space corresponding high-performing networks. models moving around optimal never reach central points. show move desirable space points averaging weights proposed iterations. ensembles trained time single model test predictions ensemble models requires times computation. show interpreted approximation ensembles test-time convenience interpretability single model. figure illustrations preactivation resnet- cifar-. left test error surface three samples corresponding solution middle right test error train loss surfaces showing weights proposed starting initialization training epochs. substantially wider optima sgd. keskar conjectures width optima critically related generalization. illustrate loss train shifted respect test error show generally converges point near boundary wide region optimal points. hand able point centered region often slightly worse train loss substantially better test error. achieves notable improvement training broad range architectures several consequential benchmarks. running epochs imagenet able achieve approximately improvement resnet- densenet- improvement resnet-. achieve improvement cifar- cifar- preactivation resnet- vgg- wide resnet--. also achieve substantial improvement recent shake-shake networks pyramidnets. paper fundamentally better understanding geometry loss surfaces generalization deep learning. follow trajectory weights traversed leading geometric insights intuition lead better results standard training. empirically make discovery notably improves training many state-of-the-art deep neural networks range consequential benchmarks essentially overhead. procedures training neural networks constantly improved. methods proposed architecture design regularization optimization. approach related work optimization regularization. optimization great interest different types local optima affect generalization deep learning. keskar claim likely converge broad local optima batch gradient methods tend converge sharp optima. moreover argue broad optima found likely good test performance even training loss worse sharp optima. hand dinh argue known deﬁnitions sharpness unsatisfactory cannot explain generalization. chaudhari propose entropy-sgd method explicitly forces optimization towards wide valleys. report although optima found entropy-sgd wider found conventional generalization performance still comparable. method based averaging multiple points along trajectory cyclical constant learning rates. general idea maintaining running average weights proposed ﬁrst considered convex optimization ruppert later polyak juditsky however procedure typically used train neural networks. practitioners instead sometimes exponentially decaying running average weights found decaying learning rate smooths trajectory performs comparably. making multiple samples gathered exploration points corresponding high performing networks. enforce exploration constant cyclical learning rates. mandt show several simplifying assumptions based averaging samples proposed using learning rate schedule allows exploration region weight space corresponding high-performing networks. particular consider cyclical constant learning rate schedules. cyclical learning rate schedule adopt inspired garipov smith topin cycle linearly decrease learning rate formula learning rate iteration given base learning rates cycle length hyper-parameters method. iteration assume processing batch data. figure illustrates cyclical learning rate schedule test error corresponding points. note unlike cyclical learning rate schedule garipov smith topin propose discontinuous schedule jumps directly minimum maximum learning rates steadily increase learning rate part cycle. abrupt cycle purposes exploration important accuracy individual proposals. even greater exploration also consider constant learning rates cyclical constant learning rate schedules starting pretrained point preactivation resnet- cifar-. ﬁrst middle last point trajectories deﬁne -dimensional plane weight space containing afﬁne combinations points. figure plot loss train error test points planes. project points trajectory plane plot. note trajectories generally plane plot except ﬁrst last middle points showed black crosses ﬁgure. therefore points trajectories possible tell value train loss test error plots. running constant learning rate equivalent sampling gaussian distribution centered minimum loss covariance gaussian controlled learning rate. following explanation interpret points proposed constrained surface sphere since come high dimensional gaussian distribution. effectively allows inside sphere higher density solutions. procedure called fast geometric ensembling garipov showed using cyclical learning rate possible gather models spatially close produce diverse predictions. used gathered models train ensembles computational overhead compared training single model. recent work neklyudov also discuss efﬁcient approach model averaging bayesian neural networks. inspired following trajectories proposals order single model would approximate ensemble provide greater interpretability convenience test-time scalability. dropout extremely popular approach regularizing dnns. across minibatch used different architecture created randomly dropping neurons. authors make analogies dropout ensembling bayesian model averaging. test time ensemble approach proposed approximated similar results multiplying connection dropout rate. high level dropout regularizers training procedures motivated approximate ensemble. approach implements high level ideas quite differently show experiments combined improved performance. present stochastic weight averaging analyze properties. section consider trajectories constant cyclical learning rate helps understand geometry training neural networks motivates procedure. section present algorithm detail section derive complexity section analyze width optima found versus conventional training. section examine relationship recently proposed fast geometric ensembling finally section consider perspective stochastic convex optimization. figure cyclical learning rate function iteration. bottom test error function iteration cyclical learning rate schedule preactivationresnet- cifar-. circles indicate iterations corresponding minimum learning rates. methods exploration region space corresponding dnns high accuracy. main difference approaches individual proposals cyclical learning rate schedule general much accurate proposals ﬁxed-learning rate sgd. making large step cyclical learning rate spends several epochs ﬁne-tuning resulting point decreasing learning rate. ﬁxed learning rate hand always making steps relatively large sizes exploring efﬁciently cyclical learning rate individual proposals worse. another important insight figure train loss test error surfaces qualitatively similar perfectly aligned. shift train test suggests robust central points high-performing networks lead better generalization. indeed average several proposals optimization trajectories robust point substantially higher test performance individual proposals essentially centered shifted mode test error. discuss reasons behaviour sections following garipov start pretrained model refer number epochs required train given conventional training procedure training budget denote pretrained model trained conventional training procedure full training budget reduced number epochs latter case stop training early without modifying learning rate schedule. starting continue training using cyclical constant learning rate schedule. using cyclical learning rate capture models correspond minimum values learning rate following garipov constant learning rates capture models epoch. next average weights captured networks ﬁnal model wswa. procedure summarized algorithm note cyclical learning rate schedule algorithm related except instead averaging predictions models average weights different type learning rate cycle. section show approximate single model. time memory overhead compared conventional training negligible. training need maintain copy aggregates average weights memory. memory consumption training dominated storing activations dnn. size weights relatively small even large dnns. training complete need store model aggregates average. figure l-regularized cross-entropy train loss test error surfaces preactivation resnet cifar- plane containing ﬁrst middle last points trajectories cyclical constant learning rate schedules. requires computing weighted weights dnns. apply operation epoch computational overhead negligible. indeed similar operation performed part gradient step epoch consists hundreds gradient steps. uses batch normalization need compute running mean standard deviation activations layer network found training ﬁnished. additional pass data garipov shifted respect thus desirable converge modes broad optima stay approximately optimal small perturbations. section compare solutions found show generally leads much wider optima. follow direction vector unit sphere starting wswa wsgd respectively. figure plot train loss test error wswa wsgd function random directions drawn uniform distribution unit sphere. visualization preactivation resnet- cifar-. figure test error l-regularized cross-entropy train loss function point random starting solutions preactivation resnet- cifar-. line corresponds different random ray. figure l-regularized cross-entropy train loss test error function point line connecting solutions cifar-. left preactivation resnet-. right vgg-. siderably wider wswa wsgd suggesting indeed converges wider optimum step much away solution found wswa increase error given amount. even error curve inﬂection point present distances swa. notice figure random directions wsgd increase test error. however know direction wsgd wswa would decrease test error since wswa considerably lower test error wsgd. words path wsgd wswa qualitatively different directions shown figure because along direction wsgd optimal. therefore consider line segment connecting wsgd wswa extract several insights wswa wsgd figure first train loss test error plots indeed substantially shifted point obtained minimizing train loss optimal test. second wsgd lies near boundary wide region train loss. further loss steep near wsgd. keskar argue loss near sharp optima found large batches actually directions exist directions optima extremely steep. conjecture because sharpness generalization performance large batch optimization substantially worse solutions found small batch sgd. remarkably experiments section observe exist directions steep ascent even small batch optima provides even wider solutions better generalization. garipov proposed fast geometric ensembling procedure training ensembles time required train single model. using cyclical learning rate generates sequence points close weight space produce diverse predictions. instead averaging predictions models average weights. however predictions proposed ensembles mandt showed strong simplifying assumptions ﬁxed learning rate approximately samples gaussian distribution centered minimum loss. suppose case ﬁxed learning rate training dnn. denote dimensionality weight space neural network denote samples produced assume points concentrated around local optimum points solution given wswa samples multidimensional gaussian covariance matrix deﬁned curvature loss batch size learning rate. note samples multidimensional gaussian concentrated ellipsoid probability mass sample inside ellipsoid near negligible hand wswa guaranteed converge moreover polyak juditsky showed averaging proposals achieves best possible convergence rate among stochastic gradient algorithms. proof relies convexity underlying problem general convergence guarantees loss function non-convex loss functions known nonconvex however trajectory loss surfaces dnns approximately convex models similar properties. denote predictions neural network parametrized weights assume scalar twice continuously differentiable function respect consider points proposed fge. points close weight space design concentrated denote around average wswa ensembling networks corresponds averaging function values thus ﬁrst order smallness difference averaging predictions averaging weights second order smallness. note points proposed distances proposals relatively small design justiﬁes local analysis. analyze difference ensembling averaging weights proposals practice epochs compare predictions different models test dataset preactivation resnet- cifar-. norm difference class probabilities consecutive proposals averaged test dataset average weights proposals compute class probabilities test dataset. norm difference probabilities model ensemble substantially smaller difference probabilities consecutive proposals. further fraction objects consecutive proposals output compare conventional training cifar- cifar- imagenet ilsvrc- also compare fast geometric ensembling note ensemble whereas corresponds single model. conventional training uses standard decaying learning rate schedule convergence. found exponentially decaying average perform comparably conventional convergence. experiments cifar datasets -layer preactivation-resnet wide resnet- models. additionally experiment recent shake-shakexd cifar- pyramidnet cifar. models trained using l-regularization vgg- also uses dropout. model deﬁne budget number epochs required train model convergence conventional training improvement beyond budget. budgets preactivation resnet wide resnet models garipov shakeshake pyramidnets budgets indicated papers proposed models report results training within budgets epochs. shake-shake pyramidnet architectures report results budget. models full budget initialization procedure train cyclical learning rate schedule budgets. used long cycles small learning rates shake-shake architecture already involves many stochastic components. model also report results conventional training denote sgd. preactivation resnet wide resnet also provide results method budget reported garipov note report accuracy ensemble networks report accuracy single model. summarize experimental results table models report mean standard deviation test accuracy runs. conducted experiments substantially outperforms budget improves further allow training epochs. across different architectures consistent improvement cifar- .-.% cifar-. amazingly able achieve comparable better performance ensembles model. cifar- usually needs budget results comparable ensembles cifar- even budget outperforms fge. imagenet experimented resnet- resnet densenet- architectures used pretrained models pytorch.torchvision. models epochs cyclical learning rate schedule parameters models report mean standard deviation test error averaged runs. results shown table general aggressive constant learning rate schedule leads faster convergence swa. experiments found setting learning rate intermediate value largest smallest learning rate used annealing scheme conventional training usually gave best results. approach however universal work well different learning rate schedules tailored particular tasks. figure test error function training epoch constant decaying learning rate schedules wide resnet-- cifar-. average points along trajectory constant learning rate starting epoch section show possible train dnns scratch ﬁxed learning rate using swa. ﬁxed learning rate wide resnet-- epochs random initialization cifar-. averaged weights epoch epoch training. ﬁnal test accuracy model figure illustrates test error function number training epochs conventional training. accuracy individual models weights averaged stays level less accuracy model. results correspond intuition presented section constant learning rate oscillates around section explore learning rate schedule affects performance swa. experiments preactivation resnet- cifar-. schedules initialization model trained epochs using conventional training. baseline fully-trained model trained conventional epochs. able train ﬁxed learning rate surprising property practical purposes recommend initializing model pretrained conventional training leads faster stable convergence running scratch. presented stochastic weight averaging training neural networks. extremely easy implement architecture-agnostic improves generalization performance virtually additional cost conventional training. many exciting directions future research. require weight average correspond good solution geometry weights traversed algorithm. therefore possible develop much faster convergence standard sgd. also able combine large batch sizes preserving generalization performance since discovers much broader optima conventional training. furthermore cyclic learning rate enables explore regions high posterior density neural network weights. learning rate schedules could developed conjunction stochastic mcmc approaches encourage exploration still providing high quality samples. could also develop average whole regions good solutions using high-accuracy curves discovered garipov acknowledgements. work supported iis- samsung research samsung electronics russian science foundation grant also thank vadim bereznyuk helpful comments.", "year": 2018}