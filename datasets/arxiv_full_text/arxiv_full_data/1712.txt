{"title": "Language Modeling with Power Low Rank Ensembles", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "We present power low rank ensembles (PLRE), a flexible framework for n-gram language modeling where ensembles of low rank matrices and tensors are used to obtain smoothed probability estimates of words in context. Our method can be understood as a generalization of n-gram modeling to non-integer n, and includes standard techniques such as absolute discounting and Kneser-Ney smoothing as special cases. PLRE training is efficient and our approach outperforms state-of-the-art modified Kneser Ney baselines in terms of perplexity on large corpora as well as on BLEU score in a downstream machine translation task.", "text": "many smoothing techniques proposed address estimation challenge. reassign probability mass unseen word sequences whose probabilities estimated interpolating backing lower order n-gram models somewhat surprisingly widely used smoothing techniques differ substantially techniques coping data sparsity domains collaborative ﬁltering matrix completion areas rank approaches based matrix factorization play central role example recommender systems challenge dealing sparsity ratings single user since typical users rated items. projecting rank representation user’s preferences original space estimate ratings items obtained. methods attractive computational efﬁciency mathematical well-foundedness. paper introduce power rank ensembles rank tensors used produce smoothed estimates n-gram probabilities. ideally would like rank structures discover semantic syntactic relatedness among words n-grams used produce smoothed estimates word sequence probabilities. contrast previous rank language modeling approaches plre orthogonal n-gram models rather general framework existing n-gram smoothing methods kneser-ney smoothing special cases. insight plre compute rank approximations original present power rank ensembles ﬂexible framework n-gram language modeling ensembles rank matrices tensors used obtain smoothed probability estimates words context. method understood generalization ngram modeling non-integer includes standard techniques absolute discounting kneser-ney smoothing special cases. plre training efﬁcient approach outperforms stateof-the-art modiﬁed kneser baselines terms perplexity large corpora well bleu score downstream machine translation task. language modeling task estimating probability sequences words language important component among applications automatic speech recognition machine translation predominant approach language modeling n-gram model wherein probability word sequence decomposed using chain rule markov assumption made i−n+). assumption substantially reduces modeling complexity parameter estimation remains major challenge. power-law nature language maximum likelihood estimator massively overestimates probability rare events assigns zero probability legitimate word sequences happen observed training data joint count matrices tensors i.e. multi-way arrays instead altered quantities counts based element-wise power operation similar smoothing methods modify lower order distributions. moreover plre aspects lead easy scalability large corpora vocabularies. first since utilizes original n-grams ranks required rank matrices tensors tend remain tractable leading fast training times. differentiates approach methods leverage underlying latent space neural networks soft-class models underlying dimension required quite large obtain good performance. moreover test time probability sequence queried time κmax maximum rank rank matrices/tensors used. larger kneser ney’s virtually constant query time substantially faster conditional exponential family models neural networks require exact computation normalization constant. section detailed discussion related work. outline ﬁrst review existing n-gram smoothing methods present intuition behind components technique rank power show interpolated ensemble experimental evaluation english russian corpora plre outperforms kneser-ney smoothing variants well class-based language models. also include comparison log-bilinear neural language model evaluate performance downstream machine translation task method achieves consistent improvements bleu. discount-based smoothing ﬁrst provide background absolute discounting kneser-ney smoothing common n-gram smoothing methods. methods formulated back-off interpolated models; describe latter since basis count word similarly joint count words wi−. shorthand deﬁne denote word sequence wj}. refer maximum likelihood estimate probability word simiabsolute discounting works idea interpolating higher order n-gram models lowerorder n-gram models. however ﬁrst probability mass must subtracted higher order n-grams leftover probability allocated lower order n-grams. speciﬁcally deﬁne following discounted conditional probability smoothed conditional probability model outputs. unfortunately absolute discounting satisfy property since exclusively uses unaltered unigram model lower order model. practice lower order distribution utilized unsure higher order distribution large). therefore unigram model altered condition fact. inspiration behind kneser-ney smoothing elegant algorithm robust performance n-gram language modeling. smoothing deﬁnes alternate probabilities intuitively proportional number unique words precede thus words appear many different contexts given higher weight words consistently appear contexts. alternate distributions used absolute discounting n-gram smoothing methods bigram count zero unigram probabilities used equivalent assuming independent however situation instead backing -gram like back .-gram generally order captures coarser level dependence inspired intuition strategy construct ensemble matrices tensors consists mle-based count information also contains quantities represent levels dependence in-between various orders model. call combinations power rank ensembles thought n-gram models non-integer approach recursively formulated formulation begs answers critical questions. construct matrices represent conditional probabilities intermediate transform generalizes altered lower order distributions smoothing? combine matrices marginal constraint still holds? following propose solutions three queries rank rank gives concrete measurement dependence wi−. constructing rank approximations bigram count matrix higher-order count tensors obtain matrices represent coarser dependencies rank approximation implying variables independent. power smoothing lower order distributions original counts rather altered estimates. propose continuous generalization alteration taking element-wise power counts. creating ensemble lastly plre also deﬁnes interpolate speciﬁcally constructed intermediate n-gram matrices. unfortunately constant discount presented section general preserve lower order marginal constraint propose generalized discounting scheme ensure constraint holds. rank ﬁrst show rank utilized construct quantities n-gram -gram. general think n-gram order tensor i.e. multi-way array indices in}. computing special rank approximation slices tensor produces -gram. thus taking rank approximations fashion allows represent dependencies n-gram -gram. consider bigram count matrix note counts rank additionally considered random variable result sampling tuples agglomerating count matrix. assuming independent expected value rewritten proportional outer product unigram probability vector itself thus rank one. higher order n-grams well. order tensor furthermore matrix slice wi−n+ held ﬁxed particular sequence ˜wi−n+ ˜wi−. conditionally independent wi−n+ given i−n+ best rank approximation sums column sums equal leveraging property straightforward prove following lemma lemma best rank approximation gkl. power since smoothing alters lower order distributions instead simply using varying rank sufﬁcient order generalize suite techniques. thus plre computes rank approximations altered count matrices. consider taking elementwise power bigram count matrix denoted b·ρ. example observed bigram count matrix associated solution thought twostep procedure compute discounts weights byproduct) followed rank quantity first construct following intermediate ensemble powered full rank terms. matrix deﬁne number unique words precede thus equal kneser unigram. idea also generalizes higher order n-grams leads following lemma lemma best rank approximation gkl. ∀wi− s.t. recall overall formulation naive solution would rank approximations count matrices/tensors under varying powers interpolate constant absolute discounting. unfortunately marginal constraint generally hold strategy used. therefore propose generalized discounting scheme nonzero n-gram count associated different discount dj+). rank approximations computed discounted matrices leaving marginal constraint intact. step computing rank quantities next step compute rank approximations obtain intermediate marginal constraint preserved. constraint trivially holds intermediate ensemble ppwr discounts derived running bigram example deﬁne best rank approximation according proportional satisﬁes furthermore shown solutions form moreover interested particular subset solutions single parameter controls scaling indicated following lemma lemma assume ρj+. choose resulting discounts satisfy well inequality constraints ii−. furthermore leftover weight takes form lemma generalizes longer contexts shown algorithm note algorithm equivalent scaling counts e.g. deleted-interpolation/jelinek mercer smoothing hand algorithm equal absolute discounting used kneser-ney. thus depending method generalizes different types interpolation schemes construct ensemble marginal constraint satisﬁed. discounts row/column preserving property following lemma lemma pplre indicate plre smoothed conditional probability computed algorithms then marginal constraint holds. general algorithm general principles outlined previous sections hold higher order n-grams. assume discounts computed according algorithm parameter computed according algorithm note that shown algorithm higher order n-grams created taking rank approximations slices count tensors embellished difference replaced alternate estimate terms i−n+) enriched rank structure. since alternate estimates constructed ensemble strategy contain ﬁne-grained dependencies well coarser dependencies thus fundamentally different simply taking single matrix/tensor decomposition trigram/bigram matrices. computational considerations plre scales well even order increases. compute rank bigram rank approximation matrix required. rank trigram need compute rank approximation slice ˜wi−. seem daunting ﬁrst practice size slice usually much much smaller keeping computation tractable. similarly plre also evaluates conditional probabilities evaluation time efﬁciently. shown algorithm normalizer precomputed sparse powered matrix/tensor. ηtotal total number matrices/tensors ensemble. larger kneser ney’s practically constant complexity much faster recent methods language modeling neural networks conditional exponential family models exact computation normalizing constant costs experiments ants smoothing class-based models log-bilinear neural language model evaluated perplexity experiments also provide results evaluated bleu downstream machine translation task. made code approach publicly available build hard class-based utilized mkcls tool train word classes uses maximum likelihood criterion classing. subsequently trained trigram class language models classes using srilm kn-smoothing class transition probabilities. srilm also used baseline kn-smoothed models. evaluation built hierarchical phrase translation system using cdec kn-smoothed models experiments compiled using kenlm datasets perplexity experiments evaluated proposed approach datasets english russian. cases singletons replaced <unk> tokens training corpus word vocabulary replaced token evaluation. general dearth evaluation large-scale corpora morphologically rich languages russian thus made processed large-russian corpus available comparison small-english apnews corpus train million words test vocabulary types. large-russian monolingual data task. training million words validation test vocabulary. million types. http//www.cs.cmu.edu/∼apparikh/plre.html http//code.google.com/p/giza-pp/ http//www.statmt.org/wmt/training-monolingualmt evaluation used parallel data shared task excluding common crawl corpus data. newstest newstest evaluation sets used development test sets respectively. small corpora number classes selected could higher computationally laborious process hard clustering. kneser-ney explore four different variants back-off interpolated modiﬁed back-off modiﬁed interpolated good-turing estimates used discounts. models trained small corpora order plre used rank bigram rank trigram addition ngram estimates. powers intermediate matrices/tensors ﬁxed discounts square roots good turing estimates ranks tuned development set. smallenglish ranges rank bigram rank trigram models. small-russian ranges rank bigram rank trigram models. results shown table best classbased reported competitive baselines. plre outperforms baselines comfortably. moreover plre’s performance baselines highlighted russian. larger vocabulary sizes rank approach effective capture linguistic similarities rare common words. next discuss maximum n-gram order affects performance. figure shows relative percentage improvement approach int-mkn order increased methods. small-english dataset rather small vocabulary compared number tokens leading lower data sparsity bigram. thus plre improvement small order substantial order hand small-russian dataset vocabulary size much larger consequently bigram counts sparser. leads simcomparison mnih hinton evaluate small-english dataset obtain perplexities using contexts size respectively. preprocessing gram plre achieves perplexity. large corpora results larger corpora performing methods plre int-mkn presented table larger training size -gram models experiments. however including rank -gram tensor provided little gain therefore -gram plre additional rank bigram rank trigram matrices/tensors. above ranks tuned development set. large-english ranges rank bigram rank trigram models. smallrussian ranges rank bigram rank trigram models. statistical validity test sets size equal original test generated randomly sampling sentences replacement original test set. method outperforms intmkn gains similar smaller datasets. shown table method obtains fast training times even large datasets. table presents results task translating english russian. used mira learn feature weights. control randomness mira avoid retuning switching feature weights obtained using int-mkn same language model changes. procedure repeated times control optimizer instability unlike recent approaches additional feature weight tuned proposed model used conjunction smoothing show improvements plre provides substitute average plre outperforms baseline bleu improvement consistent plre never gets worse bleu score. related work recent attempts revisit language modeling problem largely come directions bayesian nonparametrics neural networks. goldwater discovered connection interpolated kneser hierarchical pitman-yor process. generalizations account domain effects unbounded contexts idea using neural networks language modeling recent efforts achieved impressive performance. methods quite expensive train query techniques noise contrastive estimation subsampling careful engineering approaches maximum entropy improved training models querying probability next word given still requires explicitly normalizing vocabulary expensive corpora languages large number word types. mnih vaswani propose setting normalization constant approximate thus used downstream evaluation perplexity computation. alternate technique word-classing reduce cost exact normalization contrast approach much scalable since trivially parallelized training require explicit normalization evaluation. tings generally perform comparably state-of-the-art models. roark also idea marginal constraints re-estimating back-off parameters heavilypruned language models whereas concept estimate n-gram speciﬁc discounts. conclusion presented power rank ensembles technique generalizes existing n-gram smoothing techniques non-integer using ensembles sparse well rank matrices tensors method captures ﬁne-grained coarse structures word sequences. discounting strategy preserves marginal constraint thus generalizes kneser under slight changes also extend smoothing methods deleted-interpolation/jelinekmercer smoothing. experimentally plre convincingly outperforms kneser-ney smoothing well class-based baselines. work supported army research laboratory army research ofﬁce contract/grant number wnf--- graduate research fellowship program grant grant ebay inc. references jerome bellegarda. large vocabulary speech recognition multispan statistical language models. ieee transactions speech audio processing ondˇrej bojar christian buck chris callison-burch christian federmann barry haddow philipp koehn christof monz matt post radu soricut lucia specia. findings workproshop statistical machine translation. ceedings eighth workshop statistical machine translation pages soﬁa bulgaria august. association computational linguistics. stanley chen. shrinking exponential lanproceedings human language models. guage technologies annual conference north american chapter association computational linguistics naacl pages stroudsburg usa. association computational linguistics. david chiang yuval marton philip resnik. online large-margin training syntactic structural translation features. proceedings conference empirical methods natural language processing pages association computational linguistics. jonathan clark chris dyer alon lavie noah smith. better hypothesis testing statistical machine translation controlling opproceedings antimizer instability. nual meeting association computational linguistics human language technologies short papers volume pages chris dyer jonathan weese hendra setiawan adam lopez ferhan ture vladimir eidelman juri ganitkevitch phil blunsom philip resnik. cdec decoder alignment learning framework ﬁnite-state context-free translation models. proceedings system demonstrations pages association computational linguistics. sharon goldwater thomas grifﬁths mark johnson. interpolating types tokens estimating power-law generators. advances neural information processing systems volume joshua goodman. classes fast maximum entropy training. acoustics speech signal processing proceedings.. ieee international conference volume pages ieee. michael gutmann aapo hyv¨arinen. noisecontrastive estimation unnormalized statistical models applications natural image statistics. journal machine learning research kenneth heaﬁeld. kenlm faster smaller proceedings language model queries. emnlp sixth workshop statistical machine translation pages edinburgh scotland united kingdom july. mikolov martin karaﬁt burget ernock recurrent neusanjeev khudanpur. proceedral network based language model. ings annual conference international speech communication association volume pages international speech communication association. whye teh. hierarchical bayesian language model based pitman-yor processes. proceedings international conference computational linguistics annual meeting association computational linguistics pages association computational linguistics. ashish vaswani yinggong zhao victoria fossum david chiang. decoding largescale neural language models improves translation. proceedings conference empirical methods natural language processing pages seattle washington october. association computational linguistics. wood y.w. teh. hierarchical nonparametric bayesian approach statistical language model domain adaptation. artiﬁcial intelligence statistics pages puyang asela gunawardana sanjeev khudanpur. efﬁcient subsampling training proceedings complex language models. conference empirical methods natural language processing emnlp pages stroudsburg usa. association computational linguistics. tomas mikolov stefan kombrink lukas burget cernocky sanjeev khudanpur. extensions recurrent neural network language model. acoustics speech signal processing ieee international conference pages ieee. andriy mnih geoffrey hinton. three graphical models statistical language modelling. proceedings international conference machine learning pages acm. anil kumar nelakanti cedric archambeau julien mairal francis bach guillaume bouchard. structured penalties log-linear language models. proceedings conference empirical methods natural language processing pages seattle washington october. association computational linguistics. brian roark cyril allauzen michael riley. smoothed marginal distribution constraints language modeling. proceedings annual meeting association computational linguistics pages lawrence saul fernando pereira. aggregate mixed-order markov models statistical proceedings seclanguage processing. conference empirical methods natural language processing pages somerset jersey association computational linguistics.", "year": 2013}