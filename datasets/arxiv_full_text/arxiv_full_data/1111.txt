{"title": "Biologically Inspired Feedforward Supervised Learning for Deep  Self-Organizing Map Networks", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In this study, we propose a novel deep neural network and its supervised learning method that uses a feedforward supervisory signal. The method is inspired by the human visual system and performs human-like association-based learning without any backward error propagation. The feedforward supervisory signal that produces the correct result is preceded by the target signal and associates its confirmed label with the classification result of the target signal. It effectively uses a large amount of information from the feedforward signal, and forms a continuous and rich learning representation. The method is validated using visual recognition tasks on the MNIST handwritten dataset.", "text": "abstract. study propose novel deep neural network supervised learning method uses feedforward supervisory signal. method inspired human visual system performs human-like association-based learning without backward error propagation. feedforward supervisory signal produces correct result preceded target signal associates conﬁrmed label classiﬁcation result target signal. eﬀectively uses large amount information feedforward signal forms continuous rich learning representation. method validated using visual recognition tasks mnist handwritten dataset. multilayered deep neural network powerful methods human-like recognition tasks image speech recognition previous studies demonstrated great performance supervised learning signal classiﬁcation tasks gradient-based learning rules particular back-propagation learning generally used supervised learning feedforward type networks. however amount supervisory information last layer suﬃcient supervise entire deep neural network information selected reduced layer layer. tendency serious pattern discrimination tasks amount information extremely limited discrete values discriminant label output. bengio proposed stacked auto-encoder ensure amount information error signals reconstructing input using layer-wise learning however layer-wise learning requires step-by-step learning results diﬃculties incremental learning online updating. previous studies used unsupervised learning prior information data structure reported self-organizing behavior good discrimination results deep neural network however unsupervised learning could control classiﬁcation input data resulted enlargement network eﬃciency learning. study propose novel learning method deep neural networks uses feedforward propagated supervisory signals. method eﬀectively uses large amount information feedforward propagated supervisory signal enables robust leaning deep neural network. associates classiﬁcation input pre-trained input revises internal representation entire neural network. validate propose learning method using numerical simulation visual pattern discrimination tasks. network model inspired human visual system cortex. network composed self-organizing modules module consists hundred neurons receives subset output corresponding location previous layer. connection similar receptive ﬁeld recent convolutional neural networks convolutional meaning weight sharing among modules layer. inner product l-th layer time weight matrix zl−t output vector previous layer. inner product processed using winners-share-all regularization module. variant winner-takes-all involves winning neuron also neighboring neurons. neuron prominent inner product selected winning neuron outputs neighboring neurons output distance-decayed value determined using gaussian kernel. output l-th layer described follows wltj weight vector j-th vector matrix input vector module. traditional soms learning coeﬃcient ρpre linearly decreases beginning standard deviation gaussian kernel σpre also decreases weight vector normalized l-norm every update. method generates spatially continuous feature similar generated using topographic independent component analysis pre-training performed layer-wise manner similar biological critical period. initially ﬁrst layer learned iterations. next iterations applied ﬁrst second layer. finally ﬁrst four layers processed applying iterations sequentially. last layer processed pre-training. fig. shows typical example generated feature maps ﬁrst layer. advance propagation learning supervised learning method enables feedforward supervisory signal using sparse dynamics network. based learning vector quantization requires additional advance input supervisory signal specify learning ‘location’. fig. example correct label output clearer image conducted correct propagation ‘path’. example incorrect label output diﬃcult image. diﬃcult image produced required label output guide after-eﬀect conducted clearer image. shaded region retained after-eﬀect along correct ‘path’ conducted advance propagation. processing target input advance input produces required classiﬁcation label propagated throughout entire network. then target input processed after-eﬀect advance input. after-eﬀect guides correct ‘path’ propagation speciﬁes learning ‘location’ network. point advance propagation restrict propagation ’path’ suggests merges various paths various types inputs internal learning representation. lvq-like conditional learning followed target input speciﬁes learning ‘direction’ thereby revising weight vector produce required label. learning trial processed follows first target input zl−t processed module network output label network checked. corresponds required label weights activated units updated competitive learning otherwise weights updated opposite direction learning evoked. first advance input produces required label output processed network results required label output ratio after-eﬀect advance input. vector represents direction feature vector zl−t corrected after-eﬀect advance input zl−t−. important point network highly nonlinear behavior using output equal produced linear summation inputs. following competitive learning uses combined input manner consequently full version equation multi-layer decay gaussian kernel output described follows decay coeﬃcient layer layer total number layers. weight vector normalized l-norm every update traditional competitive learning. used experiments. learning applied pre-trained network. matched output last layer pre-training result selected label. advance inputs supervisory signals dynamically determined updated trial next. input signal initially tested using label learning applied label incorrect. learning block consisted samples training dataset input learning samples validation dataset calculate error rate. calculation performed workstation using custom code openmp parallelization. fig. represents change error rate using learning. initially error rate determined using pre-training unsupervised competitive learning resulted learning improved error rate iterations entire training decay coeﬃcient decay coeﬃcient equaled zero meant learning upper layer learning stopped quite early stage resulted high error rate coeﬃcient value non-zero learning processed entire network resulted error rate. results demonstrated proposed method eﬀectively processed learning entire network time. initial scattered learning representation last layer rearranged sparse eﬃcient style upper). simultaneously optimal stimuli representative neurons modiﬁed create generalized image lower). fig. error rate discrimination layer decay parameter upper neurons last layer. brighter color represents corresponding neuron. representative neurons corresponding respective digit labels remained learning. lower optimal stimuli typical representative neuron learning. task. method focused using rich input information early layer supervisory signal layer. demonstrated proposed method could operate supervised ﬁne-tuning pre-trained multilayered network learning method formed eﬀective learning representation continuous features also drastically reorganized representation later layers proposed learning method applied entire network concurrently layer layer. correct/incorrect signal broadcast throughout network local module used broadcast signal locally propagated information quite suitable highly distributed parallel computing systems. moreover method require back propagation information decreasing usage memory drastically. critical process enormously long sequence deep recurrent networks. could good application long sequence like natural language. requirement back propagation means biologically plausible classical learning methods. error back propagation sometimes argued biological unﬁtness evidence existence physiological condition. proposed method utilizes feedforward signal association based supervised learning advance supervised signal might correspond association hebbian rule within time window spike timing dependent plasticity interesting points proposed learning method seamlessly incorporated reinforcement learning competitive learning. reinforcement learning emerges advance input traditional competitive learning emerges correct/incorrect signal. suggests learning methods share hardware implementation learning mode selected sequence input correct/incorrect signals. moreover timing correct/incorrect signal control associative layer might useful deeper recurrent networks.", "year": 2017}