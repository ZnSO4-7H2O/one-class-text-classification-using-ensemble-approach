{"title": "Learning activation functions from data using cubic spline interpolation", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Neural networks require a careful design in order to perform properly on a given task. In particular, selecting a good activation function (possibly in a data-dependent fashion) is a crucial step, which remains an open problem in the research community. Despite a large amount of investigations, most current implementations simply select one fixed function from a small set of candidates, which is not adapted during training, and is shared among all neurons throughout the different layers. However, neither two of these assumptions can be supposed optimal in practice. In this paper, we present a principled way to have data-dependent adaptation of the activation functions, which is performed independently for each neuron. This is achieved by leveraging over past and present advances on cubic spline interpolation, allowing for local adaptation of the functions around their regions of use. The resulting algorithm is relatively cheap to implement, and overfitting is counterbalanced by the inclusion of a novel damping criterion, which penalizes unwanted oscillations from a predefined shape. Experimental results validate the proposal over two well-known benchmarks.", "text": "abstract. neural networks require careful design order perform properly given task. particular selecting good activation function crucial step remains open problem research community. despite large amount investigations current implementations simply select ﬁxed function small candidates adapted training shared among neurons throughdifferent layers. however neither assumptions supposed optimal practice. paper present principled data-dependent adaptation activation functions performed independently neuron. achieved leveraging past present advances cubic spline interpolation allowing local adaptation functions around regions use. resulting algorithm relatively cheap implement overﬁtting counterbalanced inclusion novel damping criterion penalizes unwanted oscillations predeﬁned shape. preliminary experimental results validate proposal. neural networks extremely powerful tools approximating complex nonlinear functions nonlinear behavior introduced architecture elementwise application given nonlinearity called activation function every layer. since crucial dynamics computational power history last decades deeply connected example differentiable major breakthroughs leading directly back-propagation algorithm. recently progress piecewise linear functions shown facilitate backward information training deep networks time somewhat surprising vast majority small handful ﬁxed functions hand-chosen practitioner before learning process. worse principled reason believe ‘good’ nonlinearity might across layers network even across neurons layer. shown clearly recent work agostinelli every neuron deep network endowed adaptable piecewise linear function possibly different parameters concluding standard one-activation-function-ﬁts-all approach suboptimal current practice. experiments adaptation long history never wide applicability ﬁeld. simplest approach parameterize sigmoid function network ‘shape’ parameters optimized seminal paper chen chang later work trentin along similar line consider polynomial wherein coefﬁcient polynomial adapted gradient descent additional investigations found strong drawback approaches parameters involved affect globally change region function counterproductive different possibly faraway region. several years alternative approach introduced using spline interpolating functions resulting called spline splines attractive choice interpolating unknown functions since described small amount parameters parameter local effect ﬁxed number involved every time output value computed original works main drawbacks prevented wider underlying theory. first safs investigated online setting updates computed sample time. whether efﬁcient implementation possible also batch settings shown. secondly obtained safs tendency overﬁt training data resulting oscillatory behaviors hindered performance. inspired recent successes ﬁeld nonlinear adaptive ﬁltering paper two-fold. hand provide modern introduction safs neural networks particular emphasis efﬁcient implementation case batch training. treatment clearly shows major problem implementation evident discussion above design efﬁcient regularize control points. sense second contribution provide simple ‘damping’ criterion prevent unwanted oscillations testing phase penalizes deviations original points terms norm. restricted experiments shows resulting formulation able achieve lower test error standard ﬁxed time learning non-trivial activations different shapes across different neurons. rest paper organized follows. section presents basic theory safs case single neuron. section extends treatment case hidden layer deriving gradient equations safs parameters internal layer. then section goes experimental results conclude ﬁnal remarks section parameterized vector internal parameters called knots. knots sampling values representative points spanning overall function. particular suppose knots uniformly spaced i.e. ﬁxed symmetrically spaced around origin. given output computed spline interpolation closest knot rightneighbors. common choice adopt paper corresponds cubic interpolation generally good trade-off locality output interpolating precision. apart locality output safs additional interesting properties. first output extremely efﬁcient compute involving vectormatrix products small dimensionality. secondly derivatives respect internal parameters equivalently simple written closed form. particular derivative nonlinearity respect input given computing outputs inner derivatives consider elaborate case single hidden layer ddimensional input neurons hidden layer output neurons. every neuron network uses possibly different adaptive control points independently training process. easiness computation suppose sampling splines every neuron also single shared basis matrix forward phase network similar standard network. particular given input ﬁrst compute output hidden neuron slight abuse notation denote activation output extracts element input vector given derivative control points hidden neuron denote qhik currently active span corresponding reference vector. derivative respect output given note following treatment extended easily case network hidden layer. however restricting single layer allow keep discussion focused problems/advantages arising safs. leave extension future work. important aspect discussed properly initialize control points. immediate choice sample values known work well given problem e.g. hyperbolic tangent. network guaranteed work similarly standard initial phase learning. additionally found good improvements error adding gaussian noise small variance randomly chosen subset control points provides good variability beginning similarly connections close zero initialization. error function provide meaningful regularization parameters. ﬁrst terms well-known neural network literature accordingly. particularly experiments consider squared error term regularization weights derivatives computed straightforwardly formulas presented section term used avoid overﬁtted solutions control points. fact presence major difference respect previous attempts implementing safs neural networks wherein overﬁtting counterbalanced choosing large value goes outside philosophy spline interpolation itself. time choosing proper form regularization term non-trivial term cheap compute introduce much priori information needed without hindering training process. literature regularizing cannot used here corresponding formulations make sense context spline interpolation. example simply penalizing norm leads functions close zero function imposing sparsity also meaningless. represents initial values control points discussed previous section criterion makes intuitive sense follows wish penalize unwanted deviations small weights case interested penalizing changes respect ‘good’ function parameterized initial control points namely standard used training. fact setting value high essentially deactivates adaptation control points. clearly choices possible sense paper serves starting point investigations towards objective. example wish penalize ﬁrst order derivatives splines order force desired level smoothness order usable practice safs require efﬁcient implementation compute outputs derivatives concurrently entire training dataset alternatively properly chosen mini-batch begin with underline equations reference vector depend speciﬁc neuron reason easily vectorized layer-wise numerical algebra libraries obtain vectors concurrently. additionally indexes relative terms cached forward pass reused computation derivatives. sense outputs layer derivatives computed matrix-vector computation three -dimensional inner products repeated every pair input/neuron. experience cost relatively well-optimized implementation exceed twice standard network medium-sized batches onerous operation reshaping gradients single vector gradients relative global vector evaluate preliminary proposal consider simple regression benchmarks neural networks ‘chemical’ dataset ‘california housing’. respectively examples numerical features. inputs normalized range outputs normalized range. compare hidden neurons tanh number neurons nonlinearities. weight vector initialized method described initialized tanh nonlinearity control points deﬁned range good compromise locality safs overall number adaptable parameters. ﬁrst scenario kept small value experiment random dataset kept testing results averaged different splits average statistical effects. error computed normalized root mean-squared error optimization problems solved using freely available matlab implementation polack-ribiere variant nonlinear conjugate gradient optimization algorithm c.e. rasmussen. optimization process allowed maximum iterations. matlab code experiments also available web. brieﬂy remark matlab library apart repeating experiments presented here also designed handle networks single hidden layer implements adam algorithm stochastic training case larger dataset. ﬁrst example consider scenario strong underﬁtting wherein user misleadingly selected large value leading turn extremely small values elements training. results terms training test rmse provided tab. since activations tend close standard constant zero output leading rmse nonetheless networks able reach satisfactory level performance ﬁrst case almost comparable fully optimized network show reasons this plotted four representative nonlinearities training fig. easy nonlinearities adapted ‘ampliﬁers’ activations operating regime mild strong peaks around particular interest fact resulting safs need perfectly centered around even symmetrical around y-axis fact splines able efﬁciently counterbalance setting weights behaviors would hard using standard setups ﬁxed shared mild nonlinearities. second scenario consider similar comparison respect before ﬁne-tune parameters methods using grid-search -fold crossvalidation training data performance measure. searched exponential interval optimal parameters found grid-search listed table results terms training test nrmse collected table overall endowed nonlinearities able surpass large margin standard results previous scenario. minor drawback evidenced table network overﬁtting occurring ‘chemical’ dataset showing still room improvement terms spline optimal regularization. also case plot representatives safs training fig. before general safs tend provide ampliﬁcation activation around region operation. interesting observe that also case optimal shape need symmetric might even centered around resulting nonlinearities also present additional non-trivial behaviors small region insensibility around region pre-saturation actual tanh saturation paper presented principled adapt activation functions neural network training data locally independently neuron. particularly nonlinearity implemented cubic spline interpolation whose control points adapted optimization phase. overﬁtting controlled novel regularization criterion avoiding unwanted oscillations. albeit efﬁcient criterion constrain shapes resulting functions certain degree. sense design advanced regularization terms promising line research. additionally", "year": 2016}