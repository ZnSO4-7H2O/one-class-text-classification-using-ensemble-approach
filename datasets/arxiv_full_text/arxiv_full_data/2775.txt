{"title": "Transferring Agent Behaviors from Videos via Motion GANs", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "A major bottleneck for developing general reinforcement learning agents is determining rewards that will yield desirable behaviors under various circumstances. We introduce a general mechanism for automatically specifying meaningful behaviors from raw pixels. In particular, we train a generative adversarial network to produce short sub-goals represented through motion templates. We demonstrate that this approach generates visually meaningful behaviors in unknown environments with novel agents and describe how these motions can be used to train reinforcement learning agents.", "text": "major bottleneck developing general reinforcement learning agents determining rewards yield desirable behaviors various circumstances. introduce general mechanism automatically specifying meaningful behaviors pixels. particular train generative adversarial network produce short sub-goals represented motion templates. demonstrate approach generates visually meaningful behaviors unknown environments novel agents describe motions used train reinforcement learning agents. reinforcement learning shown successful approach solving complex problems games robotics tasks. still progress often hindered learned policies generalize well multiple agents environments resulting need reward function time problem changes. traditional approaches hand-crafted rewards specify goals complex tasks demonstrate desired behavior train agents imitation learning. representations successfully train behaviors known agents typically require domain knowledge generalize unexplored environments. often broad expectation objects move real-world even seen before. expect pigs sprout wings clouds ﬂoat onto earth’s surface. inanimate materials become animate rivers upstream. expectations shaped events experienced world. inspired this deep learning learn objects move artiﬁcial real-world environments information inform agents act. rather requiring task-speciﬁc engineering problem approach aims learn general representation motion videos used visually generate desired behaviors agent’s environment. develop generative model common motions training image-to-image model compute motion templates still images within wide array environments. model generate plan motion agents environments similar observed training set. visual plan learn policy imitates desired behavior. initial work focus developing framework predicting motions. demonstrate learned model generate motions environments trained well unfamiliar environments consisting novel agents. reinforcement learning problems described markov decision process consists states environment. agent takes actions receives rewards specify goals problem. transition function represents probability agent land state taking action state policy represents probability taking action state typically policies maximize total long-term expected reward. action-value q-value represents expected discounted cumulative reward agent receive taking action state following thereafter. typically interested computing optimal q-values approach represent goals motion templates spatio-temporal representations motion obtained sequence images—typically segmented frames video movement occurred recently time higher pixel intensity template earlier motion depicts motion occurred. calculating motion template iterative process. ﬁrst step obtain silhouette image motion occurred frame. silhouette computed taking absolute difference images computing binary threshold sets pixels threshold pixels threshold function computes motion template sequence images represent silhouette image time calculate motion template ﬁrst compute silhouette image consecutive images respective column pixel locations compute µtxy time words function increases intensity pixel movement occurred current iteration here parameters inﬂuence much decayed. parameter representation current time sequence increases increases. parameter represents duration motion template controls quickly pixels decay. essentially layers silhouette images weights time. image-to-image translation—a recent approach uses generative adversarial networks translate image another automatically generate motion templates. imageto-image models used convert images edges handbags images night. given input image generative model attempts generate translated output discriminative model predicts whether pair obtained training data generated gan. generative network trained fool discriminator output images match ground truth discriminative model trained make correct predictions. goals reinforcement learning traditionally deﬁned rewards indicate desired states reached. major beneﬁt good policies often learned single reward consequence sparsity learning policies slow inefﬁcient. general argue suffers reward shortage particularly tasks often require engineering rewards even environment seen before. representing goals separately rewards offers portability example using target images still requires specifying goals problem. alternative learn policies generalize across environments. transfer learning aims port learned behaviors domain another example initializing parameters policies unsolved tasks transferring skills across untrained robots training simulation transferring knowledge real-world often efﬁcient training directly sim-to-real approaches tend focus transferring across separate realizations similar domains. learning demonstration also used specifying goal difﬁcult problem challenging agent solve own. inverse aims infer reward function expert demonstrations policies learned directly demonstrations many recent works trained agents simulated real robots pixels videos. approaches often focus learning tasks speciﬁc demonstrations problems. approach aims learn general models videos. particular develop hierarchy specifying satisfying goals similar works develop temporally-extended sequences actions known options controllers selecting finally many approaches learned models world survey) methods often require access underlying mdp. approach learns general model pixels applied novel environments. approach aims learn general model motion videos. recent image-to-image architecture predict motion templates still images. call representation motion paper focuses train model also discuss generated motions construct short-term policies unknown agents within unobserved environments. mogans inspired previous work learning motion templates approach simulated robot trained comparing motion templates robot humans. motion templates allow denser reward function naturally shapes desired behavior. reward speciﬁed similarity templates utilized hand-crafted features comparability. automatically generate plans motion appropriate current environment. particular develop hierarchy meta-controller generates high-level plan action form motion controller learns actions satisfy goal. developing controller remains future work. describe train mogan model meta-controller. given dataset videos segment video sequence frames. mini-sequence compute motion template. construct frame-motion pairs taking initial frame sequence figure generated push motion templates unseen test images. represents initial frame used compute motion templates. middle represents ground truth motion template. bottom represents generated motion template. figure sample frame-motion training pairs obtained video games. left image represents initial frame right represents computed motion template. problem image-to-image translation model forced learn one-to-one mapping fact many possible outputs network. address problem develop multimodal network allows multiple outputs. expect network output visual options indicate multiple behaviors. network still train output fool discriminator generator penalized minimum distance ground truth generated outputs. figure generated motion templates screenshots unseen video games obtained pygame.org. represents initial frame bottom represents generated motion template. ﬁrst evaluate intra-domain predictions. collected videos google’s push dataset aimed predict motions within environment. dataset contains videos robotic pushing objects within bin. obtain training segmented video frames computed motion templates segment. resulting training consisted frame-motion pairs. figure shows results. clear model learns predict movement arm. additionally objects unlikely move ignored scene objects close gripper predicted move. next experiments aimed evaluate generality mogans. trained mogans generate motions videos video game play-throughs obtained archive.org obtain training segmented video frames computed motion templates segment. resulting training consisted frame-motion pairs expected could predict motions video games outside training set. tested unseen platform games pygame.org. show results figure found model able segment salient objects screen. model predicted background would remain stationary player objects would move left right standard behavior platformers. interestingly also found mogan learned detect salient objects real world even though trained video games shown figure possible reason order detect motion video games mogan must ﬁrst learn attend important regions scene predict motions another reason could domain randomization forces model robust environment changes argument sim-to-real approaches successful results demonstrate robustness mogans additionally suggest model attention problems. finally evaluate performance multimodal outputs. indeed observed model learned visual option-like behavior output predicted moving left right jumping air. paper shown behaviors generated seen unseen environments. introduced mogan meta-controller producing goal motions still images. point approach suffer common problem gans model sometimes predicts meaningless behaviors. recent techniques improving quality outputs improve model. future work entail using generated motions plan reinforcement learning. outline plan approach. describe train agent. assume agent’s environment consists visual inputs. given observation learned meta-controller generate expected behavior. particular every steps meta-controller generates goal given current observation. additionally last frames current episode compute motion template. represent agent’s state current observation generated goal current motion template. such learn plan based current goal. components reward function. first compute similarity generated computed motion templates extracting features discriminative network taking distance templates. additionally discriminator determine agent’s computed motion template looks like motion occur naturally. generated motion template acts visual plan meaningful behaviors discriminator aims discourage unnatural ones. mechanism training motivated generative adversarial imitation learning aims generate behaviors fool discriminator predicting behaviors executed demonstrator. kulkarni narasimhan saeedi tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. advances neural information processing systems pages tzeng devin hoffman finn abbeel levine saenko darrell. adapting deep visuomotor representations weak pairwise constraints. workshop algorithmic foundations robotics", "year": 2017}