{"title": "Measuring Machine Intelligence Through Visual Question Answering", "tag": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "abstract": "As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.", "text": "machines become intelligent renewed interest methods measuring intelligence. common approach propose tasks human excels machines difﬁcult. however ideal task also easy evaluate easily gameable. begin case study exploring recently popular task image captioning limitations task measuring machine intelligence. alternative promising task visual question answering tests machine’s ability reason language vision. describe dataset unprecedented size created task contains human generated questions images. using around million human generated answers machines easily evaluated. humans amazing ability understand reason world variety senses modalities. sentence mary quickly away growling bear. conjures vivid visual auditory interpretations. picture mary running opposite direction ferocious bear sound bear enough frighten anyone. interpreting sentence effortless human designing intelligent machines deep understanding anything but. would machine know mary frightened? likely happen mary doesn’t run? even simple implications sentence mary likely outside nontrivial deduce. determine machine achieved deep understanding world human? example sentence above human’s understanding rooted multiple modalities. visualize scene depicting mary running imagine sound bear even bear’s might feel touched. conversely shown picture even auditory recording woman running bear human similarly describe scene. perhaps machine intelligence could tested similar manner? machine natural language describe picture similar human? similarly could machine generate scene given written description? fact tasks goal artiﬁcial intelligence research since inception. marvin minsky famously stated studentsconnect television camera computer machine describe sees. time even today full complexities task still discovered. tasks image captioning promising candidates testing artiﬁcial intelligence? tasks advantages easy describe capable capturing imagination public unfortunately tasks image captioning proven problematic actual tests intelligence. notably evaluation image captions difﬁcult image captioning task itself observed captions judged good human observers actually contain select caption highest consensus i.e. caption similar captions set. many cases consensus caption indeed good caption. judged humans borrowed captions judged equal better written humans image speciﬁcally. despite simplicity approach competitive advance approaches using recurrent neural networks language models achieve compared human captions. even methods using recurrent neural networks commonly produce captions identical training captions even though they’re explicitly trained captions generated borrowing images algorithms clearly demonstrating deep understanding language semantics visual interpretation. odds humans repeating sentence quite rare. could make case fault algorithms data used training. dataset contains many semantically similar images. however even randomly sampled images photographer bias found. humans capture similar images other. many tastes preferences universal. demonstrated using task image captioning determining multimodal task measuring machine’s intelligence challenging. task must easy evaluate hard solve. it’s evaluation shouldn’t hard task itself must solvable using shortcuts cheats. solve problems propose task visual question answering task requires machine answer natural language question image shown figure unlike captioning task evaluating answers questions relatively easy. simplest approach pose questions multiple choice answers much like standardized tests administered students. since computers don’t tired reading long lists answers even increase length answer list. another challenging option leave answers open-ended. since answers single words blue evaluating correctness straightforward. visual question answering task challenging? task inherently multimodal since requires knowledge language vision. complexity increased fact many questions require commonsense knowledge answer. instance does vision? need commonsense knowledge vision implies don’t wear glasses. going step further might concerned signiﬁcant variance even though describe image instance figure many people would judge longer detailed captions better. however details described captions varies signiﬁcantly e.g. hands white t-shirt black curly hair label etc. evaluate caption consensus contained good caption? however shorter less detailed captions commonly written humans rough consensus achieved holding beer bottle. leads somewhat counterintuitive conclusion captions humans like aren’t necessarily human-like. task image captioning also suffers another less obvious drawback. many cases might easy consider example success recent paper image captioning figure upon ﬁrst inspection caption appears generated deep understanding image. instance figure machine must detected giraffe grass tree. understood giraffe standing thing standing grass. knows tree giraffe next other etc. interpretation machine’s depth understanding correct? judging results system important analyze output data used training. results figure obtained training microsoft common objects context dataset dataset contains independent captions written humans images examine image figure images training dataset make interesting observation. many testing images exists signiﬁcant number semantically similar training images figure images share enough semantic similarity possible single caption could describe both. commonsense knowledge that’s needed answer questions. example question what color sheep? commonsense would tell answer white. test sufﬁciency commonsense knowledge asking subjects answer questions withseeing accompanying image. case humans subjects indeed perform poorly indicating commonsense necessary sufﬁcient. similarly subjects answer question given caption describing image. case humans performed better still accurately able view image helps indicate task requires detailed information image typically provided image caption. gather diverse interesting questions images? amazon’s mechanical turk provides powerful platform crowdsourcing tasks design prompts experiments must careful chosen. instance trial experiments prompting subjects write questions would difﬁcult toddler alien smart robot answer. upon examination determined questions written smart robot interesting given increased diversity difﬁculty. comparison questions stumping toddler easy. also gathered three questions image ensured diversity displaying previously written questions stating write different question would stump smart robot. total questions gathered figure distribution questions ﬁrst four words. ordering words starts towards center radiates outwards. length proportional number questions containing word. white areas indicate words contributions small show. majority questions begin what questions include does etc. clearly type question dominates. answers questions varying diversity depending type question. since answers ambiguous e.g. what person looking collected answers question. shown figure many question types simply answered question types start what greater variety answers. interesting comparison examine distribution answers subjects asked answer questions without looking image. shown figure strong bias many questions subjects image. instance what color questions invoke answer questions answered highly favored. finally important measure difﬁculty questions. questions what color ball? many people room? seem quite simple. contrast questions does person expect company? what government document needed partake activity? require quite advanced reasoning answer. unfortunately difﬁcultly question many cases ambiguous. question’s difﬁcultly much dependent person machine answering question question itself. person machine different competencies. attempt gain insight challenging question answer asked human subjects guess person would need answer question. unlikely human subjects adequate knowledge human learning development answer question correctly. however provide effective proxy question difﬁculty. questions judged answerable year easier judged answerable teenager. note make claims questions judged answerable year actually answered correctly toddlers. would require additional experiments performed appropriate groups. since task ambiguous collected respondences question. figure show several questions majority subjects picked speciﬁed range. surprisingly perceived needed answer questions fairly well distributed across different ranges. expected questions judged answerable adult generally need specialized knowledge answerable toddler generic. visual question answering task requires variety skills. machine must able understand image interpret question reason answer. many researchers exploring interested exploring low-level tasks involved perception computer vision. many questions even impossible solve given current capabilities state-ofthe-art computer vision algorithms. instance question many cellphones image? answerable computer vision algorithms cannot accurately detect cellphones. fact even state-of-the-art algorithms many objects difﬁcult detect especially small objects enable multiple avenues researching introduce abstract scenes dataset abstract scenes cartoon images created sets clip figure scenes created human subjects using graphical user interface allows arrange wide variety objects. clip depicting humans poses expression also changed. using interface wide variety scenes created including ordinary scenes scary scenes funny scenes. since type clip it’s properties exactly known problem recognizing objects attributes greatly simpliﬁed. provides researchers opportunity directly study problems question understanding answering. computer vision algorithms catch perhaps techniques developed abstract scenes applied real images. abstract scenes useful variety tasks well learning common sense knowledge visual question answering appears promising approach measuring machine intelligence multimodal tasks prove unforseen shortcomings. we’ve explored several baseline algorithms perform poorly compared human performance. dataset explored possible solutions found don’t require true however using proper analysis hope continuously update dataset reﬂect current progress ﬁeld. certain question image types become easy answer questions images. modalities also explored audio text-based stories conclusion believe designing multimodal challenge essential accelerating measuring progress visual question answering offers approach designing challenges allows easy evaluation maintaining difﬁcultly task. ﬁeld progresses tasks challenges continuously reevaluated ensure appropriate difﬁcultly given state research. importantly tasks designed push frontiers research help ensure solutions lead towards systems truly complete.", "year": 2016}