{"title": "Gradient descent with identity initialization efficiently learns  positive definite linear transformations by deep residual networks", "tag": ["cs.LG", "cs.NE", "math.OC", "math.ST", "stat.ML", "stat.TH"], "abstract": "We analyze algorithms for approximating a function $f(x) = \\Phi x$ mapping $\\Re^d$ to $\\Re^d$ using deep linear neural networks, i.e. that learn a function $h$ parameterized by matrices $\\Theta_1,...,\\Theta_L$ and defined by $h(x) = \\Theta_L \\Theta_{L-1} ... \\Theta_1 x$. We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic.  We provide polynomial bounds on the number of iterations for gradient descent to approximate the optimum, in the case where the initial hypothesis $\\Theta_1 = ... = \\Theta_L = I$ has loss bounded by a small enough constant. On the other hand, we show that gradient descent fails to converge for $\\Phi$ whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help.  If $\\Phi$ is symmetric positive definite, we show that an algorithm that initializes $\\Theta_i = I$ learns an $\\epsilon$-approximation of $f$ using a number of updates polynomial in $L$, the condition number of $\\Phi$, and $\\log(d/\\epsilon)$. In contrast, we show that if the target $\\Phi$ is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge.  We analyze an algorithm for the case that $\\Phi$ satisfies $u^{\\top} \\Phi u > 0$ for all $u$, but may not be symmetric. This algorithm uses two regularizers: one that maintains the invariant $u^{\\top} \\Theta_L \\Theta_{L-1} ... \\Theta_1 u > 0$ for all $u$, and another that \"balances\" $\\Theta_1 ... \\Theta_L$ so that they have the same singular values.", "text": "analyze algorithms approximating function mapping using deep linear neural networks i.e. learn function parameterized matrices deﬁned θlθl−...θx. focus algorithms learn gradient descent population quadratic loss case distribution inputs isotropic. provide polynomial bounds number iterations gradient descent approximate optimum case initial hypothesis loss bounded small enough constant. hand show gradient descent fails converge whose distance identity larger constant show forms regularization toward identity layer help. symmetric positive deﬁnite show algorithm initializes learns ǫ-approximation using number updates polynomial condition number log. contrast show target symmetric negative eigenvalue members class algorithms perform gradient descent identity initialization optionally regularize toward identity layer fail converge. analyze algorithm case satisﬁes u⊤φu symmetric. algorithm uses regularizers maintains invariant u⊤θlθl−...θu another balances θ...θl singular values. residual networks deep neural networks which roughly subnetworks determine feature transformation diﬀer identity rather diﬀer zero. enabling winning entry ilsvrc classiﬁcation task become established central idea deep networks. linear transformation positive determinant bounded condition number approximated deep linear network form θlθl−...θx where large layer close identity results interesting suggest that many cases non-convex objective eﬃciently optimized gradient descent layers stay close identity possibly help regularizer. paper describes analyzes algorithms linear regression input variables response variables respect quadratic loss setting analyzed hardt abstract away sampling issues analyzing algorithm performs gradient descent respect population loss done hardt also assume hard equivalent case noise independent zero mean. finally focus case distribution input patterns isotropic. traditional analysis convex optimization algorithms provides bound terms quality initial solution together bounds eigenvalues hessian loss. non-convex problem paper show gradient descent starts identity layer loss initial solution bounded constant hessian remains well-conditioned enough throughout training successful learning. speciﬁcally constant that loss identity back-propagation initialized identity layer achieves loss time polynomial hand show constant identity loss respect backpropagation identity initialization fails learn also show target symmetric positive deﬁnite gradient descent identity initialization achieves loss number steps bounded polynomial condition number contrast symmetric negative eigenvalue show guarantee possible wide variety algorithms type loss forever bounded square negative eigenvalue. holds step-and-project algorithms also previous impossibility result proved using target positive determinant good condition number. recall proved hardt good approximation product near-identity matrices prove gradient descent cannot learn them even help regularizers reward near-identity representations. section provide convergence guarantee symmetric satisﬁes u⊤φu appears bounds. matrices margin include rotations acute angles. case consider algorithm regularizes addition near-identity initialization. gradient update algorithm performs call power projection projecting hypothesis θlθl−...θ onto matrices margin second balances that informally contribute equally θlθl−...θ. view regularizer theoretically tractable proxy regularizers promote large margin balance layers adding penalties. while practice deep networks non-linear analysis linear case provide tractable gain insight rigorous theoretical analysis might view back-propagation non-linear case approximation procedure locally modiﬁes function computed layer manner reduces loss fast possible. non-linear network obtained composing transformations chosen hilbert space functions step function space corresponds step linear space functions. related work. closely related previous work paper hardt mentioned above. saxe studied dynamics continuous-time process obtained taking step size backpropagation applied deep linear neural networks zero. kawaguchi showed deep linear neural networks suboptimal local minima. case problem studied similar structure problems arising low-rank approximation matrices especially regards algorithms approximate matrix iteratively improving approximation form interesting survey rich literature algorithms please taghvaei studied properties critical points loss learning deep linear neural networks presence weight decay regularizer; studied networks transform input output process indexed continuous variable instead discrete layers. showed that given regularity conditions random initialization gradient descent converges local minimizer almost surely; paper yields useful insights regularity condition hold problem. many papers analyzed learning neural networks non-linearities. papers closely related work analyze algorithms based gradient descent. analyze constant-depth networks. daniely showed stochastic gradient descent learns subclass functions computed log-depth networks polynomial time; class includes constant-degree polynomials polynomially bounded coeﬃcients. theoretical treatments neural network learning algorithms include arora livni janzamin safran shamir zhang nguyen hein zhang orhan pitkow although less closely related. three upper bound analyses hardt ma’s lemmas together upper bound operator norm hessian deep linear network. otherwise diﬀerent outlines. roughly bound terms loss initial solution proceeds showing distance layer identity grows slowly enough loss reduced layers stray enough harm conditioning hessian. bound symmetric positive deﬁnite matrices proceeds showing that case layers same eigenvalues converges root corresponding eigenvalue mentioned above bound matrices positive margin algorithm achieves favorable conditioning regularization. expect theoretical analysis reported inform design practical algorithms learning non-linear deep networks. potential avenue arises fact leverage provided regularizing toward identity appears already provided weaker policy promoting property composition layers positive deﬁnite. also balancing singular values layers network aided analysis; analogous balancing jacobians associated various layers improve conditioning practice non-linear case. study algorithms learn linear mappings parameterized deep networks. network layers parameters computes parameterized function θlθl− ℜd×d. notation θjθj− write θi+lθiθi−x. possibility confusion sometimes refer loss simply distribution isotropic respect target matrix produced iterative algorithm also refer loss iterate column vector constructed stacking columns denote permutation matrix mapping ℜd×d. ℜn×m ℜp×q denotes kronecker product matrix blocks block given aijb. next show that close identity gradient changing fast them rapid progress continues made. prove upper bound operator norm hessian holds uniformly members ball around identity turn obtained bound frobenius norm. proof appendix armed lemmas analyze gradient descent. roughly strategy show distance identity various layers grows slowly enough leverage lemmas enable successful learning. maxi ||θi i||. update ready analyze dynamics learning process. u⊤dlu diagonalization max{ ||φ||}. next describe sense gradient descent learns eigenvalue independently. section consider asymmetric positive deﬁnite sense u⊤φu unit length includes rotations acute angle partial reﬂections form length-preserving reﬂection motivation balanced factorization follows. want factor fraction total amount rotation fraction total amount scaling. however scaling done factor done directions take account partial rotations done factors. following property balanced factorization; proved appendix section show positive deﬁnite targets necessary several gradient descent algorithms diﬀerent kinds regularization minimize loss. family algorithms analyze parameterized function mapping number inputs number layers radius step sizes initialization parameter particular ψ-step-and-project algorithm instantiation following algorithmic template. initialize length vector particular |u⊤λ |u⊤λ implies since unitary eigenvalues since frobenius norm function eigenvalues ||u⊤u z||f since summing diagonal elements ||u⊤u proof. lemmas identical mutually diagonalizable lemma shows preserved projection step. thus real diagonal even )l)jj projection ensures elements non-negative thus )l)jj either case thank yair carmon nigel duﬀy matt feiszli frostig vineet gupta moritz hardt tomer koren antoine saliou hanie sedghi yoram singer kunal talwar valuable conversations. peter bartlett gratefully acknowledges support grant iis- australian research council australian laureate fellowship australian research council centre excellence mathematical statistical frontiers", "year": 2018}