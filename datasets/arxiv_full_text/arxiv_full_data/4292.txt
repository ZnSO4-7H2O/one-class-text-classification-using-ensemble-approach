{"title": "Cortical microcircuits as gated-recurrent neural networks", "tag": ["q-bio.NC", "cs.NE", "stat.ML"], "abstract": "Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits. Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.", "text": "cortical circuits exhibit intricate recurrent architectures remarkably similar across different brain areas. stereotyped structure suggests existence common computational principles. however principles remained largely elusive. inspired gated-memory networks namely long short-term memory networks introduce recurrent neural network information gated inhibitory cells subtractive propose natural mapping sublstms onto known canonical excitatory-inhibitory cortical microcircuits. empirical evaluation across sequential image classiﬁcation language modelling tasks shows sublstm units achieve similar performance lstm units. results suggest cortical circuits optimised solve complex contextual problems proposes novel view computational function. overall work provides step towards unifying recurrent networks used machine learning biological counterparts. last decades neuroscience research collected enormous amounts data architecture dynamics cortical circuits unveiling complex stereotypical structures across neocortex prevalent features cortical nets laminar organisation high degree recurrence even level local circuits another feature cortical circuits detailed tight balance excitation inhibition received growing support experimental theoretical level however computational processes facilitated architectures dynamics still elusive. remains fundamental disconnect underlying biophysical networks emergence intelligent complex behaviours. artiﬁcial recurrent neural networks hand crafted perform speciﬁc computations. fact rnns recently proven successful solving complex tasks language modelling speech recognition perceptual tasks tasks input data contains information across multiple timescales needs ﬁltered processed according relevance. ongoing presentation stimuli makes difﬁcult learn separate meaningful stimuli background noise rnns particular gated-rnns solve problem maintaining representation relevant input sequences needed without interference stimuli. principle protected memories conserve past inputs thus allow back-propagation errors backwards time memory properties ﬁrst successful types gated-rnns named long short-term memory networks fig. note architectural features lstms overlap closely known cortical structures important differences regard mechanistic implementation gates cortical network lstms lstms gates control memory cell multiplicative factor biological networks gates i.e. inhibitory neurons subtractively excitatory inhibitory currents cancel linearly level postsynaptic membrane potential moreover subtractive inhibitory mechanism must well balanced gate inputs ’closed’ state without perturbing activity much inhibition. previous models explored gating subtractive excitatory inhibitory balanced networks without clear computational role. hand predictive coding rnns features studied without clear match state-of-the-art machine learning networks. regarding previous neuroscientiﬁc interpretations lstms suggestions lstms models working memory different brain areas without clear interpretation individual components lstms speciﬁc mapping known circuits. propose architecture function lstms directly onto cortical circuits gating provided lateral subtractive inhibition. networks potential exhibit excitation-inhibition balance observed experiments yield simpler gradient propagation multiplicative gating. study dynamics empirical evaluation showing sublstms achieve similar performance lstms penn treebank wikitext- language modelling tasks well pixelwise sequential mnist classiﬁcation. transferring functionality lstms biologically plausible network work provides testable hypotheses recently emerging technologically advanced experiments functionality entire cortical microcircuits. architecture lstm units general feedforward structure aided additional recurrent memory controlled lateral gates remarkably similar columnar architecture cortical circuits central element lstms similar rnns memory cell hypothesise implemented local recurrent networks pyramidal cells layer-. line previous studies showing relatively high level recurrence non-random connectivity pyramidal cells layer furthermore layer- pyramidal networks display rich activity long time scales vivo slices consistent lstm-like function. strong evidence persistent neuronal activity higher cortical areas sensory areas relatively speaking sensory areas exhibit sorter timescales higher brain areas would expect given different temporal requirements brain areas have. similar behaviour expected multi-area lstms. note longer time-scales also present superﬁcial layers suggesting possibility memory cell cortical microcircuit. slow memory decay networks controlled short long-term synaptic plasticity recurrent excitatory synapses. gates protect given memory lstms mapped onto lateral inhibitory inputs cortical circuits. propose that similar lstms input gate implemented inhibitory neurons layer-/ lateral inhibition consistent canonical view microcircuits sparse sensory-evoked responses layer-/ brain inhibition believed originate basket cells providing near-exact balanced inhibitory counter signal given excitatory feedforward input excitatory inhibitory inputs thus cancel arriving signals ignored default. consequently activity within downstream memory network remains largely unperturbed unless altered targeted modulation inhibitory activity similarly memory cell affect output lstm activity unaccompanied congruent inhibition fig. i.e. lateral inhibition turned gate open. presynaptic cell ﬁres neurotransmitter released synaptic terminals. neurotransmitter subsequently bound postsynaptic receptors prompts structural change channel allow electrically charged ions postsynaptic cell. depending receptor type either increase decrease postsynaptic membrane potential. sufﬁciently depolarising excitatory input provided postsynaptic potential reach threshold stereotyped action potential behaviour formalised rc–circuit follows ohm’s laws yields standard leaky-integrate-and-ﬁre neuron model riexc riinh membrane time constant iexc iinh excitatory inhibitory synaptic input currents respectively. action potentials initiated standard model membrane potential hits hard threshold modelled momentary pulse subsequent reset resting potential. neuronal excitation inhibition opposite effects inhibitory inputs acts linearly subtractively membrane potential. leaky-integrate-and-ﬁre model approximated level ﬁring rates rate used demonstrate impact subtractive gating contrast multiplicative gating ﬁring-rate approximation forms basis gated-rnn model similar subtractive behaviour input-output function moreover rate formulation also allows cleaner comparison lstm units existing machine learning optimisation methods. could argued different form inhibition counteracts excitatory inputs decreasing membrane resistance characteristic multiplicative gating effect membrane potential. however analysed level output ﬁring rate effect becomes subtractive consistent approach model framed ﬁring-rate level figure biological artiﬁcial gated recurrent neural networks. example unit simpliﬁed cortical recurrent neural network. sensory input arrives pyramidal cells layer-/ onto memory cells memory decays decay time constant input onto layer- balanced inhibitory basket cells balance represented diagonal ‘equal’ connection. output memory cell gated basket cells layer- within area implementation following similar notation lstm units input output subtractive gates. dashed connections represent potential balance excitatory inhibitory input lstm recurrent neural network cell plots bellow illustrate different gating modes using simple current-based noisy leaky-integrate-and-ﬁre neuron subtractive inhibition; sigmoidal activation functions subtractive gating; sigmoidal activation functions multiplicative gating. output rate represents number spikes second biological circuits. lstm unit access memory cell controlled input gate time forget gate controls decay memory output gate controls whether content memory cell transmitted rest network. lstm network consists many lstm units containing memory cell input forget output gates. lstm state described unit follows dynamics given middle column below. here memory cell denotes element-wise multiplication weighted input given input vector recurrent input lstm units respectively. overall output lstm unit computed lstm networks multiple layers millions parameters typically trained using stochastic gradient descent supervised setting. above parameters multiple gates allow network adapt information depending task hand. particular enable writing memory cell adjusting timescale memory exposing memory network combined effect gates makes possible lstm units capture temporal dependencies across multiple timescales. here introduce study unit sublstm. sublstm units mapping lstms onto known canonical excitatory-inhibitory cortical microcircuits similarly sublstms deﬁned however gating subtractive rather multiplicative. sublstm deﬁned memory cell transformed input input gate model simpliﬁed notion memory forgetting balance gating controlled gates biologically plausible learned simple decay referred results ﬁx-sublstm. similarly input sublstm’s output also gated subtractive output gate evaluated different activation functions sigmoidal transformations highest performance. differences gated-rnns subtractive inhibitory gating potential balanced excitatory input detailed comparison different gating modes. difference sublstms lstms lies implementation gating mechanism. lstms typically multiplicative factor control amplitude input signal. sublstms biologically plausible interaction excitation inhibition. important consequence subtractive gating potential improved gradient backwards towards input layers. illustrate compare gradients sublstms lstms simple example. first review derivatives loss respect various components sublstm using notation based notation represents derivative loss note consider versions sublstms forget gate lstms another simple memory decay deﬁnes memory timeconstant ﬁx-sublstm). these weights could also optimised model decided keep number parameters sigmoid activation function overlined variables etc. preactivation values gate input transformation note compared lstm sublstms provide simpler gradient fewer multiplicative factors. lstms weights input transformation updated according total number temporal steps ellipsis abbreviates recurrent gradient paths time containing path backwards time simplicity analysis ignore recurrent connections lstm sublstm consider depth-wise path network; call timestep depth-only contribution derivative lstm slight abuse notation derivative tanh. notice either input output gates tanh zero corresponding contributions gradient zero. network subtractive gating depth-only derivative contribution becomes aims work two-fold. first inspired cortical circuits aimed propose biological plausible implementation lstm unit would allow better understand cortical architectures dynamics. compare performance sublstm units lstms ﬁrst compared learning dynamics subtractive multiplicative networks mathematically. second step empirically compared sublstm ﬁx-sublstm lstm networks tasks sequential mnist classiﬁcation word-level language modelling penn treebank wikitext- network weights initialised glorot initialisation lstm units initial forget gate bias selected number units ﬁx-sublstm number parameters held constant across experiments facilitate fair comparison lstms sublstms. sequential mnist digit classiﬁcation task digit image mnist dataset presented sequence pixels fig. decompose mnist images pixels sequences steps. network optimised using rmsprop momentum learning rate hidden layer hidden units. results show sublstms achieves similar results lstms results comparable previous results using task rnns. figure comparison lstm sublstm networks sequential pixel-by-pixel mnist using hidden units. samples mnist dataset. converted matrix pixels temporal sequence timesteps. classiﬁcation accuracy test set. ﬁx-sublstm ﬁxed learned forget gate. language modelling represents challenging task rnns short long-term dependencies. language models models probability text autoregressively predicting sequence words. timestep trained predict following word; words model word sequence product conditional multinoulli distributions. evaluate measuring perplexity deﬁned sequence words ﬁrst used penn treebank dataset train model word-level language modelling rnns tested hidden layers; backpropagation truncated steps batch size optimise networks used rmsprop momentum. also performed hyperparameter search validation input output update dropout rates learning rate weight decay. hyperparameter search done google vizier performs black-box optimisation using gaussian process bandits transfer learning. tables show resulting hyperparameters. table reports perplexity test understand sublstms scale network size varied number hidden units also tested wikitext- language modelling dataset based wikipedia articles. dataset twice large dataset also features larger vocabulary therefore well suited evaluate model performance longer term dependencies reduces likelihood overﬁtting. datasets results show sublstms achieve perplexity similar lstms interestingly biological plausible version sublstm achieves performance similar better sublstms. table language modelling test perplexities penn treebank wikitext-. models layers ﬁx-sublstm uses ﬁxed learned forget gate unit. number units ﬁx-sublstm chosen number parameters lstm facilitate fair comparison. size indicates number units. cortical microcircuits exhibit complex stereotypical network architectures support rich dynamics computational power dynamics properly understood. known excitatory inhibitory neuron types interact closely process sensory information great accuracy making sense interactions beyond scope contemporary experimental approaches. lstms hand well-understood powerful tool contextual tasks structure maps intriguingly well onto stereotyped connectivity cortical circuits. here analysed biologically constrained lstms could perform similarly well indeed subtractively gated excitation-inhibition recurrent neural networks show promise compared lstms benchmarks sequence classiﬁcation word-level language modelling. notable sublstms could outperform traditional counterpart hope work serve platform discuss develop ideas cortical function establish links relevant experimental work role excitatory inhibitory neurons contextual learning future work interesting study additional biological detail affect performance. next steps include dale’s principle naturally focus perplexing diversity inhibitory cell types behaviour shunting inhibition mixed subtractive divisive control overall given success multiplicative gated lstms insightful understand biological tricks cortical networks give lstms performance boost. would like thank everton agnes ça˘glar gülçehre gabor melis jake stroud helpful comments discussion. r.p.c. t.p.v. supported henry dale fellowship wellcome trust royal society y.m.a. supported epsrc research council b.s. supported clarendon fund. costa mizusaki sjostrom rossum functional consequences prepostsynaptic expression synaptic plasticity. philosophical transactions royal society london. series biological sciences glorot bengio understanding difﬁculty training deep feedforward neural networks. proceedings thirteenth international conference artiﬁcial intelligence statistics pages golovin solnik moitra kochanski karro sculley google vizier service black-box optimization. proceedings sigkdd international conference knowledge discovery data mining pages acm. huang matysiak heil könig brosch king persistent neural activity auditory cortex related auditory working memory humans nonhuman primates. elife jiang shen cadwell berens sinz ecker patel tolias principles connectivity among morphologically deﬁned cell types adult neocortex. science aac– aac. kuchibhotla gill lindsay papadoyannis field sten miller froemke parallel processing cortical inhibition enables context-dependent behavior. nature neuroscience pakan lowe dylda keemink currie coutts rochefort mrsic-flogel behavioral-state modulation inhibition context-dependent cell type speciﬁc mouse visual cortex. elife poort khan pachitariu nemri orsolic krupic bauza sahani keller mrsic-flogel hofer learning enhances sensory multiple non-sensory representations primary visual cortex. neuron thomson west wang bannister synaptic connections small circuits involving excitatory inhibitory neurons layers adult neocortex triple intracellular recordings biocytin labelling vitro. cerebral cortex figure biological properties neocortex resemble lstm features represent different components sequence steps starting input level highlight features analogous lstms. sensory input arrives layer- stellate excitatory cells propagated layer pyramidal cells output layer pyramidal cells projects layer- pyramidal cells form recurrent units stage local interneurons gate information onto memory cell; memory cell retain given input sometime finally memory sent units network another units higher cortical areas output memory cell controlled another gate balance excitation-inhibition observed across cortex represented ‘equal’ connection represents equal ﬁxed weights", "year": 2017}