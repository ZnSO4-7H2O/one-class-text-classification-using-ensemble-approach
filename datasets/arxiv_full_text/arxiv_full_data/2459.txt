{"title": "Active Model Aggregation via Stochastic Mirror Descent", "tag": ["stat.ML", "cs.AI", "cs.LG"], "abstract": "We consider the problem of learning convex aggregation of models, that is as good as the best convex aggregation, for the binary classification problem. Working in the stream based active learning setting, where the active learner has to make a decision on-the-fly, if it wants to query for the label of the point currently seen in the stream, we propose a stochastic-mirror descent algorithm, called SMD-AMA, with entropy regularization. We establish an excess risk bounds for the loss of the convex aggregate returned by SMD-AMA to be of the order of $O\\left(\\sqrt{\\frac{\\log(M)}{{T^{1-\\mu}}}}\\right)$, where $\\mu\\in [0,1)$ is an algorithm dependent parameter, that trades-off the number of labels queried, and excess risk.", "text": "consider problem learning convex aggregation models good best convex aggregation binary classiﬁcation problem. working stream based active learning setting active learner make decision on-the-ﬂy wants query label point currently seen stream propose stochastic-mirror descent algorithm called smd-ama entropy regularization. establish excess risk bounds loss convex aggregate returned smd-ama order algorithm dependent parameter trades-off number labels queried excess risk. demonstrate experimental results standard datasets show that compared passive learning smdama queries points similar accuracy passive learner. machine learning techniques become popular many ﬁelds astronomy physics biology chemistry search finance. availability large amounts data greater computational power newer machine learning algorithms applications discovered. popular subclass machine learning problems fall category supervised learning problems. classiﬁcation regression popular problems supervised learning learner provided labeled data required predict labels unseen points. case classiﬁcation problems labels discrete whereas case regression problems labels continuous. supervised learning critically relies presence labeled data. cost obtaining labels different data points depends problem domain. example astronomy easy access tons tons unlabeled data. however obtaining labeled data usually hard. problems speech recognition information extraction obtaining labeled data often tedious requires domain expertise cases natural question arises learn limited supervision? drawn unknown distribution deﬁned domain points sampled i.i.d. marginal distribution labels sampled conditional distribution |x=x. algorithms svms logistic regression choose hypothesis class appropriate loss function solve sort empirical risk minimization problem return hypothesis whose risk def= exy∼p small. given collection models collection convex aggregations models much richer class. based idea many algorithms machine learning approximation theory proposed learn aggregation basic models. ensemble methods methods combine large number simple models learn single powerful model. boosting algorithms adaboost logitboost viewed performing aggregation functional gradient descent idea minimization convex loss exponential loss case adaboost logistic loss case logitboost sequential aggregation models case boosting base models weak learners aggregating models boost learning capabilities ﬁnal aggregated model. problem model aggregation regression models ﬁrst proposed nemirovski given collection models nemirovski outlined three problems model aggregation namely model selection convex aggregation linear aggregation. paper interested actively learning convex aggregation models binary classiﬁcation problem. def= denote euclidean product. given convex margin based loss function want procedure outputs βjbj convex hull whose excess risk compared best model convex hull satisﬁes inequality small remainder term goes expectation w.r.t. random variables involved. order construct vector assume access unlabeled stream examples drawn i.i.d. underlying distribution deﬁned problem matches posed nemirovski except instead given labeled data access unlabeled data stream allowed query labels believe informative. juditsky studied original version convex aggregation problem posed nemirovski involved learning best convex aggregation given access stream labeled examples sampled i.i.d. underlying distribution. introduced online stochastic mirror descent algorithm problem learning best convex aggregation models. shall call method smd-pma showed making pass stochastic mirror descent algorithm averaging iterates obtained step aggregated number samples seen stream. essentially smd-pma slight modiﬁcation stochastic mirror descent algorithm applied stochastic optimization problem standard dimensional probability simplex. since constraint simplex entropy regularizer used smd-pma. stochastic mirror descent procedure followed averaging step allows authors obtain excess risk bounds. contributions. paper interested learning convex aggregation models help actively labeled data. consider streaming setting given unlabeled points oracle oracle provided input returns label present algorithm essentially one-pass stochastic mirror-descent based active learning algorithm called smd-ama exyθ b)]. since dealing simplex solves stochastic optimization problem minθ∈∆m constraints entropy function regularization function stochastic mirror descent algorithm. round smd-ama query label point probability allows construct unbiased stochastic subgradient objective function current iterate. length stream points smd-ama returns hypothesis def= show excess risk hypothesis algorithmic parameter. mild dependence number paper organized follows. section brieﬂy reviewing stochastic mirror descent algorithm introduce algorithm smd-ama. section present excess risk bound hypothesis returned active learning algorithms. section reviews related work section compares proposed algorithm passive learning algorithm previously proposed ensemble based active learning algorithm. mirror descent ﬁrst order optimization algorithm used solve convex optimization problems form minx∈c assumes presence gradient oracle provided point returns sub-gradient iteration mirror descent algorithm given current iterate solves minimization problem form minx∈c∇f x+dr sub-gradient bregman divergence corresponding strictly convex function shall call regularization function. power algorithm stems fact appropriate choice distance generating function could adapt geometry constraint classical mirror descent algorithm also extended stochastic optimization problems form minx∈c{f yield stochastic mirror descent algorithm assume oracle provides unbiased estimate gradient stochastic objective. standard analysis stochastic mirror descent algorithm assumes access stochastic gradient oracle provides unbiased estimate gradient objective function point domain. stochastic optimization problem trying solve naive application stochastic mirror descent method would require iteration obtain stochastic subgradient iteration current iterate stochastic subgradient given subderivative given argument. round decided query label point calculate stochastic subgradient using equation however decided query label stochastic subgradient depends unknown label cannot calculated. could case consider stochastic subgradient zero vector longer unbiased estimate subgradient. problematic classical analysis stochastic mirror descent assumes access unbiased estimates subgradient objective function. order counter problem idea importance weighting. importance weighted subgradient estimates order importance weights round upon seeing point query probability suppose random variable takes value def= ytqt. shall also make following independence queried takes value queried. assumption. assumption pt⊥⊥yt|xt collection random variables zt−. consider following importance-weighted lθt− b)ytb. easy that assumption given stochastic subgradient unbiased estimator hence importance weighted stochastic subgradient allows unbiased estimates gradient objective function equation importance weighting ﬁrst introduced stream based active learning problems beygelzimer importance weights helps construct unbiased estimators loss hypothesis hypothesis class. particularly favourable hypothesis class used obtaining actively labeled dataset goes favour. design smd-ama. ingredients algorithm place. algorithm proceeds streaming fashion looking unlabeled data point time. step smd-ama calculates probability point labeled current convex aggregate calculation used step calculate probability querying label notice probability querying point round always least value step step calculates importance weighted gradient used step calculate iterate straightforward calculus show step leads following iteration component vector step properties loss function order estimate well known standard loss functions exponential squared loss used classiﬁcation also proper losses probability estimation hence given loss function standard formulae easy estimate conditional probability instance squared loss estimate given technical point that step algorithm performed irrespective whether point received round queried not. allows evolution deterministic hence allows certain lemma juditsky crucial excess risk analysis result step current convex aggregate different even point queried. approximating inequality inequality obtained using fact obtained substituting expression replacing terms using expression mentioned statement theorem approximating desired result. madani considered problem active model aggregation given many models choose single best model collection. model problem coins problem player provided certain number ﬂips allowed coins budget runs player report coin highest probability turning heads. reinforcement learning approach problem taken kapoor greiner paper dealing complicated problem choosing best model convex hull given models. mamitsuka combine ideas query-by-committee boosting come active learning algorithm current weighted majority used decide point query next. order obtain weighted-majority hypothesis authors suggest using ensemble techniques boosting bagging. trapeznikov introduced actboost algorithm active learning algorithm boosting framework. particularly actboost works weak learning assumption freund assumes hypothesis zero error rate convex hull base classiﬁers. assumption authors suggest version space based algorithm maintains possible convex combinations base hypothesis consistent current data queries labels points hypothesis current convex hull disagree. design actboost algorithm brittle. contrast make weak learning assumptions hence avoid problems actboost might face weak learning assumption satisﬁed. active learning algorithms boosting framework also suggested iyengar admit guarantees somewhat heuristic. implemented smd-ama along smd-pma algorithm mamitsuka mentioned before ensemble based active learning algorithm builds committee adaboost algorithm. works iterative fashion. round runs adaboost algorithm currently labeled dataset αjtbj choose next point queried generates random sample points current unqueried points. suppose random sample choose next point queried look point whose margin w.r.t. smallest. query label point. process repeated condition satisﬁed experimental setup. used decision stumps along different dimensions different thresholds form basis models decision stump weak classiﬁer characterized dimension threshold classiﬁes point dimension experiments used decision stumps along dimension make symmetric adding unless otherwise mentioned smd-ama experiments. report results standard datasets loss function used smd-ama smd-pma squared loss. ﬁrst experiments compare smd-pma smd-ama. smd-ama smd-pma datasets hypothesis outputted algorithms round classify test dataset. figure plot test error rate algorithms number points seen stream. note smd-pma gets label every point smd-ama gets labels queries. alternatively could plot test error different algorithms w.r.t. number labels seen. however mentioned section hypothesis change consecutive queries hence easier plot using decision stumps yielded insigniﬁcant improvement test error increased computational complexity large amount datasets abalone statlog mnist whitewine magic redwine. test error number points seen rather number labels seen. used squared loss function training smd-ama smd-pma algorithms. finally since smd-ama randomized algorithm report results averaged iterations. table clear datasets whitewine smd-ama table comparison test error smd-ama smd-pma number queries made smd-ama different datasets. penultimate column reports number queries made algorithm last column represents fraction training dataset whose labels queried. results hypothesis returned algorithms stream. smd-pma attain almost error rate ﬁnishing single pass stream. figure shows test error changes smd-pma smd-ama number points seen stream. case abalone statlog smd-ama quickly catches smd-pma case mnist difference smd-ama smd-pma closes seen stream. case whitewine smd-pma uniformly better active learning algorithm smd-ama. difference error rates smd-pma smd-ama stream case magic redwine datasets difference performance smd-ama smd-pma negligible. number queries made abalone mnist whitewine less length stream implies well passive learning abalone mnist expense fewer labels. case statlog redwine datasets number queries made comparatively larger size dataset respectively. magic fraction queries made less number table report loss test data smd-ama smd-pma data stream. figure reports test loss algorithms number samples seen stream. note test error always test loss larger fact convex aggregation model consider chapter squared loss maximum loss large maxz∈ purpose experiments examine difference test loss suffered smd-ama smd-pma changes number points seen stream. case statlog mnist difference losses generally smaller case abalone whitewine magic redwine datasets smd-ama generally suffers slightly smaller loss smd-pma. however since scale test loss larger results seem imply smd-ama smd-pma comparable rates decay test loss number unlabeled examples seen. encouraged results believe possible derive sharper excess risk bounds smd-ama. figure shows number queries made smd-ama scale number points seen steam four datasets. scaling almost linear case statlog. expected given fact statlog query labels almost points stream. similar result holds true even redwine dataset. contrast abalone mnistwhitewine magic datasets scaling seems sublinear. µ−). hence theorem says excess risk smd-ama upper bound scales stating result convenience. lemma", "year": 2015}