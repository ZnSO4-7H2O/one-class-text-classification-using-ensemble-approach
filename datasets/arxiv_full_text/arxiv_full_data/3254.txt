{"title": "Surpassing Human-Level Face Verification Performance on LFW with  GaussianFace", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Face verification remains a challenging problem in very complex conditions with large variations such as pose, illumination, expression, and occlusions. This problem is exacerbated when we rely unrealistically on a single training data source, which is often insufficient to cover the intrinsically complex face variations. This paper proposes a principled multi-task learning approach based on Discriminative Gaussian Process Latent Variable Model, named GaussianFace, to enrich the diversity of training data. In comparison to existing methods, our model exploits additional data from multiple source-domains to improve the generalization performance of face verification in an unknown target-domain. Importantly, our model can adapt automatically to complex data distributions, and therefore can well capture complex face variations inherent in multiple sources. Extensive experiments demonstrate the effectiveness of the proposed model in learning from diverse data sources and generalize to unseen domain. Specifically, the accuracy of our algorithm achieves an impressive accuracy rate of 98.52% on the well-known and challenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the human-level performance in face verification (97.53%) on LFW is surpassed.", "text": "face veriﬁcation remains challenging problem complex conditions large variations pose illumination expression occlusions. problem exacerbated rely unrealistically single training data source often insufﬁcient cover intrinsically complex face variations. paper proposes principled multi-task learning approach based discriminative gaussian process latent variable model named gaussianface enrich diversity training data. comparison existing methods model exploits additional data multiple source-domains improve generalization performance face veriﬁcation unknown target-domain. importantly model adapt automatically complex data distributions therefore well capture complex face variations inherent multiple sources. extensive experiments demonstrate effectiveness proposed model learning diverse data sources generalize unseen domain. speciﬁcally accuracy algorithm achieves impressive accuracy rate well-known challenging labeled faces wild benchmark ﬁrst time human-level performance face veriﬁcation surpassed. face veriﬁcation task determining whether pair face images person active research topic computer vision decades many important applications including surveillance access control image retrieval automatic log-on personal computer mobile devices. however various visual complications deteriorate performance face veriﬁcation shown numerous studies real-world face images wild labeled faces wild dataset well known challenging benchmark face veriﬁcation. dataset provides large relatively unconstrained face images complex variations pose lighting expression race ethnicity gender clothing hairstyles parameters. surprisingly proven difﬁcult automatic face veriﬁcation methods although signiﬁcant work accuracy rate improved since established studies closed human-level performance face veriﬁcation. existing face veriﬁcation methods assume training data test data drawn feature space follow distribution. distribution changes methods suffer large performance drop however many practical scenarios involve cross-domain data drawn different facial appearance distributions. learning model solely single source data often leads overﬁtting dataset bias moreover difﬁcult collect sufﬁcient necessary training data rebuild model scenarios highly accurate face veriﬁcation speciﬁc target domain. cases becomes critical exploit data multiple source-domains improve generalization face veriﬁcation methods targetdomain. modern face veriﬁcation methods mainly divided categories extracting low-level features building classiﬁcation models although existing methods made great progress face veriﬁcation less ﬂexible dealing complex data distributions. methods ﬁrst category example low-level features sift gabor handcrafted. even features learned data algorithm parameters also need speciﬁed users. similarly methods second category architectures deep networks parameters models must also determined advance. since existing methods require assumptions made structures data cannot work well assumptions valid. moreover existence assumptions hard capture intrinsic structures data using methods. propose multi-task learning approach based discriminative gaussian process latent variable model named gaussianface face veriﬁcation. unlike existing studies rely single training data source order take advantage data multiple source-domains improve performance target-domain introduce multi-task learning constraint dgplvm. here investigate asymmetric multi-task learning focus performance improvement target task. perspective information theory constraint aims maximize mutual information between distributions target-domain data multiple source-domains data. moreover gaussianface model reformulation based gaussian processes non-parametric bayesian kernel method. therefore model also adapt complexity ﬂexibly complex data distributions real-world without heuristics manual tuning parameters. reformulating large-scale multi-task learning non-trivial. simplify calculations introduce efﬁcient equivalent form kernel fisher discriminant analysis dgplvm. despite gaussianface model optimized effectively using scaled conjugate gradient technique inference slow large-scale data. make approximations anchor graphs speed process inference prediction scale model largescale data. model applied face veriﬁcation different ways binary classiﬁer feature extractor. former mode given pair face images directly compute posterior likelihood class make prediction. latter mode model automatically extract high-dimensional features pair face images feed classiﬁer make ﬁnal decision. main contributions paper follows propose novel gaussianface model face veriﬁcation virtue multi-task learning constraint dgplvm. model adapt complex distributions avoid over-ﬁtting exploit discriminative introduce computationally efﬁcient equivalent form kfda dgplvm. equivalent form reformulates kfda kernel version consistent covariance function greatly simpliﬁes calculations. human computer performance face recognition compared extensively studies shown computer-based algorithms accurate humans well-controlled environments whilst still comparable humans poor condition however conclusion veriﬁed face datasets controlled variations factor changes time date virtually work showing computerbased algorithms could surpass human performance unconstrained face datasets exhibits natural variations pose lighting expression race ethnicity gender clothing hairstyles parameters. much work dealing multifactor variations face veriﬁcation. example simonyan applied fisher vector face veriﬁcation achieved good performance however fisher vector derived gaussian mixture model number gaussians need speciﬁed users means cannot cover complex data automatically. proposed non-parametric subspace analysis linear transformation cannot cover complex distributions. besides also exist approaches utilizing plentiful sourcedomain data. based joint bayesian algorithm proposed transfer learning approach merging source-domain data limited target-domain data. since transfer learning approach based joint bayesian model original visual features suitable handling complex nonlinear data data complex manifold structures. moreover transfer learning approach considered formally two-class classiﬁcation suppose training observations i-th input point corresponding output binary class other. matrix vectors represent input points column vector outputs. deﬁne latent variable input point sigmoid function imposed squash output latent function assuming data i.i.d joint likelihood factorizes principle clustering based observation variances predictive values smaller dense areas larger sparse areas. variances employed good estimate support different domains restricting wider applications largescale data multiple domains. recently learned transformation face images various poses lighting conditions canonical view deep convolutional network. learned face representation deep model face identiﬁcation challenging multi-class prediction task. taigman ﬁrst utilized explicit face modeling apply piecewise afﬁne transformation derived face representation nine-layer deep neural network. although methods achieved high performances many parameters must determined advance less ﬂexible dealing complex data distributions. core algorithm gps. best knowledge methods multi-task learning related methods applied face veriﬁcation. actually mtgp/gps extensively studied machine learning computer vision recent years however considered symmetric multi-task learning means tasks assumed equal importance whereas purpose enhance performance target task given source tasks. leen proposed mtgp model asymmetric setting focus improving performance target task developed model clustering methods take discriminative information covariance function special account like dgplvm. although discriminative information considered apply multi-task learning improve performance. salakhutdinov used deep belief learn good covariance kernel limitation deep methods hard determine architecture network optimal. also multi-task learning constraint considered section brieﬂy review gaussian processes classiﬁcation clustering gaussian process latent variable model method mainly following three notable advantages. firstly mentioned previously nonparametric method means adapts complexity ﬂexibly complex data distributions real-world without heuristics manual tuning parameters. secondly method computed effectively closed-form marginal probability computation. furthermore hyper-parameters learned data automatically without using model selection methods cross validation thereby avoiding high computational cost. thirdly inference based bayesian order automatically learn discriminative features covariance function take advantage sourcedomain data improve performance face veriﬁcation develop principled gaussianface model including multi-task learning constraint discriminative gaussian process latent variable model dgplvm extension gplvm discriminative prior placed latent positions rather simple spherical gaussian prior. dgplvm uses discriminative prior encourage latent positions class close different classes far. since face veriﬁcation binary classiﬁcation problem mainly depend kernel function natural kernel fisher discriminant analysis model class structures kernel spaces. simplicity inference followings introduce another equivalent formulation kfda replace kfda kernelized version linear discriminant analysis method. ﬁnds direction deﬁned kernel feature space onto projections positive negative classes well separated maximizing ratio between-class variance within-class variance. formally zn+} denote positive class {zn++ negative class numbers positive negative classes respectively. kernel matrix. therefore feature space sets represent positive class negative class respectively. optimization criterion kfda maximize ratio betweenclass variance within-class variance paper however focus covariance function rather latent positions. simplify calculations represent equation kernel function kernel function form covariance function. therefore natural introduce efﬁcient equivalent form kfda certain assumptions points i.e. maximizing equation equivalent maximizing following theorem guarantees almost trajectories approach stable equilibrium points detected equation data point ﬁnds corresponding stable equilibrium point employ complete graph assign cluster labels data points stable equilibrium points. obviously variance function equation completely determines performance clustering. denote matrix whose rows represent corresponding positions latent space gaussian process latent variable model interpreted gaussian process mapping dimensional latent space high dimensional data locale points latent space determined maximizing gaussian process likelihood respect given covariance function gaussian process denoted likelihood data given latent positions follows normalization constant uninformative priors simple spherical gaussian priors introduced obtain optimal need optimize likelihood respect respectively. represents domain-relevant latent space. source-domain data target-domain data covariance functions form share hyper-parameters paper widely used kernel covariance matrix obtained dgplvm discriminative ﬂexible used conventional classiﬁcation since learned based discriminative criterion degrees freedom estimated conventional kernel hyper-parameters. asymmetric multi-task learning perspective tasks allowed share common hyper-parameters covariance function. moreover information theory perspective information cost target task multiple source tasks minimized. natural quantify information cost mutual entropy measure mutual dependence distributions. multi-task learning extend mutual entropy multiple distributions follows speedup inference optimizing equation need invert matrix inference take k-means clustering centers anchors form substituting using woodbury identity speedup prediction compute predictive variance need invert matrix time method section calculate accurate clustering centers regarded anchors. using woodbury identity again obtain wqwq)−qw matrix inverse matrix computed efﬁciently. section describe applications binary face image ﬁrst normalized size afﬁne transformation based landmarks image divided overlapped patches pixels stride pixels. patch within image mapped vector certain descriptor vector regarded feature patch denoted number patches within face image paper multi-scale feature patch extracted difference multi-scale descriptors extracted center patch instead accurate landmarks. gaussianface model binary classiﬁer classiﬁcation model regarded approach learn covariance function shown figure here pair face images person similarity vector input data point gaussianface model similarity optimize equation respect hyper-parameters latent positions scaled conjugate gradient technique. since focus covariance matrix paper present derivations hyper-parameters. easy large matrix inference prediction. large problems storing matrix solving associated linear systems computationally prohibitive. paper anchor graphs method speed process. simply ﬁrst select anchors cover cloud data points form matrix latent data points anchors respectively. original kernel matrix approximated corresponding output learned hyper-parameters covariance function training data given un-seen pair face images ﬁrst compute similarity vector using method estimate latent representation using method ﬁnally predict whether pair person equation paper prescribe sigmoid function cumulative gaussian distribution solved analytically ˜k−k k∗k−ˆf equation call method gaussianface-bc. feature extractor model regarded approach automatically extract facial features shown figure here pair face images person regard joint feature input data point vector enhance robustness approach ﬂipped form example hyper-parameters method section group latent data points different clusters automatically. suppose ﬁnally obtain clusters. centers clusters denoted {ci}c weights {wi}c ratio number latent data points i-th cluster number latent data points. refer input equation obtain corresponding probability variance fact {ci}c regarded codebook generated model. un-seen pair face images also ﬁrst compute joint feature vector pair patches estimate latent representation compute ﬁrst-order second-order statistics centers. similarly regard input equation also obtain corresponding probability variance statistics variance represented high-dimensional facial features denoted concatenate high-dimensional features pair patches form ﬁnal high-dimensional feature pair face images. high-dimensional facial features describe distribution features un-seen face image differs distribution ﬁtted features training images also encode predictive information including probabilities label uncertainty. call approach gaussianface-fe. section conduct experiments face veriﬁcation. start introducing source-domain datasets target-domain dataset experiments source-domain datasets include four different types datasets follows multi-pie dataset contains face images subjects view points illumination conditions four recording sessions. images collected controlled conditions. morph morph database contains images people within ranges average images individual. images. dataset contains around facial images subjects; approximately images person. images collected signiﬁcant variations pose expression illumination conditions. life photos. dataset contains approximately images subjects collected online. subject roughly images. setting regularization parameter ﬁxed reﬂects tradeoff method’s ability discriminate ability generalize balances relative importance target-domain data multi-task learning constraint. therefore validation used selecting time different number source-domain datasets training corresponding optimal selected validation set. since collected large number image pairs training model based kernel method thus important consideration efﬁciently approximate kernel matrix using low-rank method limited space time. adopt anchor graphs method kernel approximation. experiments take steps determine number anchor points. ﬁrst step optimal selected validation experiment. second step tune number anchor points. vary number anchor points train model training test validation set. report average accuracy model trials. consider trade-off memory running time practice number anchor points best average accuracy determined experiments. since model based natural compare model four popular models mtgp prediction gplvm dgplvm fair comparisons models trained multiple source-domain datasets using methods gaussianface model described section hyper-parameters covariance function learnt model regard model binary classiﬁer feature extractor like ours respectively. figure shows model signiﬁcantly outperforms four models superiority model becomes obvious number source-domain datasets increases. benchmark face veriﬁcation follows dataset contains uncontrolled face images public ﬁgures variety pose lighting expression race ethnicity gender clothing hairstyles parameters. images collected web. dataset target-domain dataset well known challenging benchmark. using also allows compare directly existing face veriﬁcation methods besides dataset provides large relatively unconstrained face images complex variations described above proven difﬁcult automatic face veriﬁcation methods experiments conducted strictly follow standard unrestricted protocol precisely training procedure four source-domain datasets images multipie morph life photos target-domain dataset training view validation test view lfw. test time follow standard -fold cross-validation protocol test model view lfw. four source-domain datasets randomly sample pairs matched images pairs mismatched images. training partition testing partition experiments mutually exclusive. words identity overlap among partitions. experiments below number means number source-domain datasets gaussianface model training. parity reasoning number means ﬁrst source-domain datasets used model training. therefore number models trained training data target-domain data only. figure accuracy rate gaussianface-bc model competing mtgp/gp methods binary classiﬁer. accuracy rate gaussianface-fe model competing mtgp/gp methods feature extractor. relative improvement method binary classiﬁer increasing number compared performance number relative improvement method feature extractor increasing number compared performance number binary classiﬁers. paper chose three popular representatives logistic regression adaboost table demonstrates performance method gaussianface-bc much better classiﬁers. furthermore experimental results demonstrates effectiveness multi-task learning constraint. example gaussianface-bc improvement four source-domain datasets used training best three binary classiﬁers around improvement. three popular clustering methods k-means random projection tree gaussian mixture model since method determine number clusters automatically fair comparison methods generate number clusters ours. shown table method gaussianface-fe signiﬁcantly outperforms compared approaches veriﬁes effectiveness method feature extractor. results also proved multi-task learning constraint effective. time different type source-domain dataset added training performance improved signiﬁcantly. gaussianface-fe model achieves improvement number varies much higher improvement methods. classiﬁed model. obviously even humans also difﬁcult verify them. here emphasize centers patches instead accurate dense facial landmarks like utilized extract multi-scale features method. makes method simpler easier use. prove validity model also consider treat multi-pie morph respectively target-domain dataset others sourcedomain datasets. target-domain dataset split mutually exclusive parts consisting matched pairs mismatched pairs used training used test. test similar protocol select mutually exclusive subsets subset consists matched pairs mismatched pairs. experimental results presented figure time dataset added training performance improved even though types data different training set. motivated appealing performance gaussianface-bc gaussianface-fe combine face veriﬁcation. speciﬁcally facial features extracted using gaussianface-fe gaussianfacebc used make ﬁnal decision. figure shows results combination compared state-of-theart methods best published result benchmark achieved gaussianface model improve accuracy ﬁrst time beats human-level performance figure presents example pairs always incorrectly implicit belief among many psychologists computer scientists human face veriﬁcation abilities currently beyond existing computer-based face veriﬁcation algorithms belief however supported anecdotal impression scientiﬁc evidence. contrast already number papers comparing human computer-based face veriﬁcation performance shown best current face veriﬁcation algorithms perform better humans good moderate conditions. really difﬁcult beat human performance speciﬁc scenarios. indeed contrast performance unfamiliar faces human face veriﬁcation abilities familiar faces relatively robust changes viewing parameters illumination pose. example bruce found human recognition memory unfamiliar faces dropped substantially changes viewing parameters. besides humans take advantages non-face conﬁgurable information combination face body also examined human performance drops hence experiments comparing human computer performance show human face veriﬁcation skill best humans asked match cropped faces people previously unfamiliar them. contrary experiments fully show performance computer-based face veriﬁcation algorithms. first algorithms exploit information enough training images variations viewing parameters improve face veriﬁcation performance similar information humans acquire developing face veriﬁcation skills becoming familiar individuals. second algorithms might exploit useful subtle image-based detailed information give slight consistent advantage humans. therefore surpassing human-level performance symbolically signiﬁcant. reality challenges still ahead. compete successfully humans factors robustness familiar faces usage non-face information need considered developing future face veriﬁcation algorithms. variable model named gaussianface face veriﬁcation including computationally efﬁcient equivalent form kfda multi-task learning constraint dgplvm model. gaussian processes approximation anchor graphs speed inference prediction model. based gaussianface model propose different approaches face veriﬁcation. extensive experiments challenging datasets validate efﬁcacy model. gaussianface model ﬁnally surpassed human-level face veriﬁcation accuracy thanks exploiting additional data multiple source-domains improve generalization performance face veriﬁcation target-domain adapting automatically complex face variations. although several techniques laplace approximation anchor graph introduced speed process inference prediction gaussianface model still takes long time train model high performance. addition large memory also necessary. therefore speciﬁc application needs balance three dimensions memory running time performance. generally speaking higher performance requires memory running time. future issue running time addressed distributed parallel algorithm implementation large matrix inversion. address issue memory online algorithms training need developed. another intuitive method seek efﬁcient sparse representation large covariance matrix. would like thank deli zhao chen change insightful discussions. work partially supported cuhk computer vision cooperation grant huawei general research fund sponsored", "year": 2014}