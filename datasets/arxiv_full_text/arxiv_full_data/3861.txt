{"title": "Robustness in sparse linear models: relative efficiency based on robust  approximate message passing", "tag": ["math.ST", "cs.AI", "cs.LG", "stat.ME", "stat.ML", "stat.TH"], "abstract": "Understanding efficiency in high dimensional linear models is a longstanding problem of interest. Classical work with smaller dimensional problems dating back to Huber and Bickel has illustrated the benefits of efficient loss functions. When the number of parameters $p$ is of the same order as the sample size $n$, $p \\approx n$, an efficiency pattern different from the one of Huber was recently established. In this work, we consider the effects of model selection on the estimation efficiency of penalized methods. In particular, we explore whether sparsity, results in new efficiency patterns when $p > n$. In the interest of deriving the asymptotic mean squared error for regularized M-estimators, we use the powerful framework of approximate message passing. We propose a novel, robust and sparse approximate message passing algorithm (RAMP), that is adaptive to the error distribution. Our algorithm includes many non-quadratic and non-differentiable loss functions. We derive its asymptotic mean squared error and show its convergence, while allowing $p, n, s \\to \\infty$, with $n/p \\in (0,1)$ and $n/s \\in (1,\\infty)$. We identify new patterns of relative efficiency regarding a number of penalized $M$ estimators, when $p$ is much larger than $n$. We show that the classical information bound is no longer reachable, even for light--tailed error distributions. We show that the penalized least absolute deviation estimator dominates the penalized least square estimator, in cases of heavy--tailed distributions. We observe this pattern for all choices of the number of non-zero parameters $s$, both $s \\leq n$ and $s \\approx n$. In non-penalized problems where $s =p \\approx n$, the opposite regime holds. Therefore, we discover that the presence of model selection significantly changes the efficiency patterns.", "text": "understanding eﬃciency high dimensional linear models longstanding problem interest. classical work smaller dimensional problems dating back huber bickel illustrated clear beneﬁts eﬃcient loss functions. number parameters huber recently established. work consider eﬀects model selection estimation eﬃciency penalized methods. particular explore whether sparsity results eﬃciency patterns interest deriving asymptotic mean squared error regularized m-estimators powerful framework approximate message passing. propose novel robust sparse approximate message passing algorithm adaptive error distribution. algorithm includes many non-quadratic non-diﬀerentiable loss functions therefore extending previous work mostly concentrates least square loss. derive asymptotic mean squared error show convergence relative eﬃciency regarding number penalized estimators much larger show classical information bound longer reachable even light–tailed error distributions. show penalized least absolute deviation estimator dominates penalized least square estimator cases heavy tailed distributions. observe pattern recent years scientiﬁc communities face major challenge size complexity data analyzed. size contemporary datasets number variables collected makes search exploitation sparsity vital statistical analysis. moreover presence heterogeneity outliers anomalous data samples common. however statistical estimators designed sparsity data irregularities simultaneously give biased results depending magnitude deviation sensitivity method. example early work robust statistics andersen specifically argue good statistical procedure insensitive changes involving parameters eﬀective sensitive changes parameters estimated. estimators based minimization non-diﬀerentiable loss functions common example discussed telling example frequency events could utterly destroy average performance optimal statistical estimators. observations number papers huber hampel bickel laid comprehensive foundations theory robust statistics. particular huber’s seminal work m-estimators established asymptotic properties class m-estimators situation number parameters ﬁxed number samples tends inﬁnity. since then numerous important steps taken toward analyzing quantifying robust statistical methods notably work donoho rousseeuw yohai among others. even today exist several mathematical concepts robustness recently work karoui donoho montanari illuminated surprising novel robustness properties least squares estimator number parameters close number samples. illustrates diverse rich aspects robustness intricate dependence dimensionality parameter space. classical m-estimation theory ignored model selection necessity. modern computational power allows statisticians deal model-selection problems realistically. hence statisticians moved away m-estimators started working penalized m-estimators; moreover allow number parameters grow sample size focus penalized m-estimators consider linear regression model estimators includes penalized least squares estimators various penalties including lpenalty lasso concave penalty scad adaptive penalty elastic penalty many more. however error distribution deviates normal distribution loss function unknown method adapts many diﬀerent distributions needed. following classical literature m-estimators penalized robust methods penalized quantile regression penalized least absolute deviation estimator ar-lasso estimator robust adaptive lasso many more proposed. methods penalize convex loss function following manner despite substantial body work robust m-estimators little work robust properties penalized m-estimators. robust assessments penalized statistical estimators customarily made ignoring model selection. typical properties discussed model selection consistency tight upper bounds statistical estimation error chen lambert-lacroix zwald lerman negahban wang particular existing work primarily reduced tools intrinsic huber’s m-estimators. order that authors establish model selection consistency reduce analysis selected model assuming selected model true model. however analysis dissatisfactory necessary assumptions model selection consistency restrictive. hence departures considerations highly desirable. also diﬃcult achieve analysis needs factor model selection bias. work makes progress. including bias model selection analysis able answer question like high dimensional regime estimator preferred? low-dimensional setting several independent lines work provide reasons using distributionally robust estimators least-squares alternatives however high dimensional setting remains open question advantages using complicated loss function simple loss function squared loss? better understand diﬀerences probability distributions aﬀect penalized m-estimators? powerful justiﬁcation exists using point view statistical eﬃciency. huber introduced concept minimax asymptotic variance estimator achieves minimal asymptotic variance least favorable distribution; smaller variance robust estimator shrinks many coeﬃcients zero. estimators parameters hodge’s super–eﬃciency occurs measure zero. hence asymptotic variance optimal criterion comparison. suggest diﬀerent criterion comparison needs make strong connection penalized least squares algorithm donoho however algorithm bayati montanari cannot recover signal distribution noise arbitrary. settings design robust sparse approximate message passing algorithm. proposed ramp algorithm ﬁrst algorithm consider improving framework means adapting problem diﬀerent loss function; however ﬁrst simultaneously allows shrinkage estimation. donoho montanari propose three-stage algorithm matches classical m-estimators; however merely applies stages match proposed algorithm belongs general class ﬁrstorder approximate massage passing algorithms. however contrast existing methods three-steps. iterations based gradient descent objective scaled regularized version original loss function moreover allows non-diﬀerentiable loss functions. three–step estimation method ramp longer simple proxy one-step show amse depends distribution eﬀective score takes form much diﬀerent classical also depends sparsity parameter moreover present detailed study relative eﬃciency penalized least squares method penalized least absolute deviation method. discover regimes preferred match classical ﬁndings huber. several important insights follow immediately relative eﬃciency considerably aﬀected model selection step; optimal loss function longer negative likelihood function; even sparsest high dimensional estimators additional gaussian component asymptotic mean squared error disappear asymptotically. vector moreover given deﬁne moreover subgradient taken coordinate-wise bivariate function deﬁne partial derivate respect ﬁrst argument; similarly partial derivate respect cumulative distribution function density function standard normal random variable. paper investigates eﬀects penalization robustness properties penalized estimators particular incorporate bias induced penalization exploration robustness. present scaled min-regularized necessarily diﬀerentiable robust loss functions penalized m-estimation corresponding approximate massage passing algorithm adaptable diﬀerent loss functions sparsity simultaneously. four examples min-regularized losses include least absolute deviation quantile loss introduced section corresponding algorithm modiﬁed form robust losses oﬀers oﬀers general framework standard method discussed section section studies number important theoretical results concerning ramp algorithm well convergence properties connections penalized m-estimators. section studies relative eﬃciency establishes lower bounds amse. moreover section also presents results relative eﬃciency penalized least absolute deviations estimator respect penalized least squares estimator. p-ls preferred p-lad error distribution light-tailed breakdown point methods indistinguishable; furthermore p-ls never preferred p-lad error distribution heavy-tailed. section contains detailed numerical experiments number ramp losses including huber number quantiles number error distribution including normal mixture normals student. demonstrate ramp method practice analyze ﬁnite sample convergence properties. second subsection involves study state-evolution equation whereas third subsection involves study amse. studies ramp works extremely well. demonstrate good properties ramp algorithm varying error distribution distribution design matrix lastly present analysis relative eﬃciency p-ls p-lad estimators consider demonstrate results ramp match bean karoui whereas results establish patterns according section family often named moreau envelope moreau-yosida regularization. moreau envelope continuously diﬀerentiable even not. addition sets minimizers same. related family regularized loss functions proximal mapping operator functions deﬁned proximal mapping operator widely used non-diﬀerentiable convex optimization deﬁning proximal-gradient methods. parameter controls extent proximal operator maps points towards minimum smaller values providing smaller movement towards minimum. finally ﬁxed points proximal operator precisely minimizers appropriate choice proximal minimization scheme converges optimum least geometric possibly superlinear rates iusem teboulle deﬁne corresponding eﬀective score function score functions used donoho montanari times diﬀerentiable losses deﬁne iteration step robust approximate message passing algorithm therein. paper extend method sparse estimation discuss loss functions necessarily diﬀerentiable necessarily satisfy restricted strong convexity condition important examples loss functions absolute deviation quantile loss neither diﬀerentiable satisfy restricted strong convexity condition. extension signiﬁcantly complicated ﬁxed points proximal operator longer necessarily sparse; moreover trivial inclusion norm deﬁnition provide algorithm belongs approximate message passing family converges penalized m-estimator |x|. according observe proximal mapping operator satisﬁes consider ﬁrst. observe indicates sign) sign. substituting previous equation sign. next observe ∂/∂x substituting proximal mapping equation obtain order establish theoretical properties impose number conditions density error term class robust loss functions design matrix precisely impose following conditions. absolutely continuous derivative continuous piecewise linear continuous function constant outside bounded interval nondecreasing step function. details ﬁrst condition depict explicitly trade–oﬀ smoothness smoothness assumption covers classical huber’s hampel’s loss functions. although allow necessarily diﬀerentiable loss functions consider class loss functions good robust properties resulting estimator. least squares loss satisfy property iteration least squares loss studied bayati montanari asymptotic mean squared error derived therein. losses discussed above property holds. third condition assure uniqueness population parameter wish estimate. fourth condition essentially moment condition holds example moreover vector empirical cumulative distribution function converges weakly distribution additionally denotes distributions whose mass zero greater equal exactly sparse signals. setting admittedly speciﬁc careful study matrix ensembles long tradition statistics communications theory borrowed formulation simpliﬁes analysis signiﬁcantly relaxed needed. particular implies restricted eigenvalue condition design matrix high probability long sample size satisﬁes universal constant integer plays role upper bound sparsity vector coeﬃcients note that square submatrices size matrix non-diﬀerentiable losses consider adaptations. first allow parameter controls amount regularization robust loss function adaptive iteration second consider population equivalent ﬁrst design estimator solve ﬁxed point equation. details estimation step algorithm introduces necessary thresholding step needed inducing sparsity estimator however contrast existing methods adjusted appropriately scaled regularized robust score function δφ/ω. three–step estimation method ramp longer simple proxy oneinstead work soft thresholded alternative ensure approximate sparsity iterate. furthermore residuals require additional scaling i.e. multiply scaling donoho montanari factor proportional fraction sparse elements current iterate term absolutely necessary eﬀect regularization absorbed θt−. rescaling needed prove connection general algorithms bayati montanari existing algorithms scaling appear special case least squares loss gets canceled constant denoting distribution density functions residuals given sample adjusted residuals provided iteration easily formulate empirz using standard non-parametric continuous functions hence solve ﬁxed point equations implement simple grid search average ﬁrst value grid estimated function bellow last value grid estimated function s/n. correction residual called onsager reaction term. term generated theory belief propagation factor graphical models procedure generation shown adding onsager reaction term iteration main diﬀerence iteration soft thresholding iteration. intuition term step considering undersampling sparsity simultaneously. following lemma shows relationship onsager reaction term donoho’s term undersampling–sparsity. state evolution formalism introduced donoho donoho used predict dynamical behavior numerous observables approximate message passing algorithms. formalism asymptotic distribution residual asymptotic considered state algorithm predicts whether algorithm converges not. details asymptotic mean squared error deﬁned function state evolution parameter show proposed ramp algorithm contains three steps belongs general class message passing algorithms. oﬀer compute novel iteration scheme adjusted robust lemma conditions hold. then ramp algorithm deﬁned equations belongs general recursion bayati montanari standard normal random variable. then state evolution sequence notice function depends distribution true signal error distribution loss function; however depend design matrix therefore believe assumptions gaussian design released. ﬁxed points sequence {xt}. case simple lasso estimator becomes similar rescaled least squares loss result bayati montanari paper donoho montanari considers nonpenalized estimates strongly convex loss functions excludes least absolute deviation quantile loss particular. provide details behavior ﬁxed point section cases non-diﬀerentiable loss functions. moreover relate properties relative eﬃciency l-penalized m-estimators section next show iteration distribution ¯τt−z. enables provide characterization eﬀective slope algorithm. measures value min-regularization parameter satisﬁes population analog step ramp algorithm. section relate state evolution properties distance measure similarly existing literature approximate message passing measure distance done pseudo-lipschitz function function pseudo-lipschitz exist constant +y)x− theorem conditions hold pseudo-lipschitz function. {xt}t≥ sequence ramp estimates indexed iteration number then display presents asymptotic mean squared error sequence solutions ramp algorithm. next connect sequence ramp algorithm l-penalized m-estimator demonstrated estimator ramp optimal solution lemma turn measure distance ramp iteration penalized estimator. norm measurement distance. robustness properties sparse high-dimensional estimators diﬃcult quantify shrinkage eﬀects subsequent bias estimation. whenever eﬃciency deﬁned though asymptotic variance shrinkage known lead super-eﬃciency phenomena. relative eﬃciency capture size bias variance together leading relevant robustness evaluation. estimator dominates other asymptotic mean squared error smaller. state evolution ramp algorithm provides useful iterative scheme computing value asymptotic mean squared error. according theorem asymptotic mean squared error penalized -estimators denotes continuous part i.e. moreover fct−φ denotes density random variable ¯σtz. hence high dimensional θt)}t≥ asymptotic mean squared error mapping allowing sequence {amse grow setting identity function twice-diﬀerentiable losses also setting asymptotic mean squared error mapping takes form variance mapping presented donoho montanari observing bias estimation disappears. speciﬁcally recover result mentioned paper identify additional gaussian component variance mapping. dimensional problems. hence observe phase transition regarding robustness high dimensional sparse problems. inequality above eﬀect sparsity extremely clear. notice sparse high dimensional setting distribution represented convex combination dirac measure measure doesn’t mass zero. denote random variables measures above. then asymptotic mean squared error satisﬁes explore representation study relative eﬃciency p-ls p-lad estimators. relative eﬃciency p-ls w.r.t. p-lad deﬁned quotient asymptotic mean squared errors. results previous sections amounts quotient p-lad. evaluate quotient study behavior independently. order need preparatory lemma below. larger equal displays universally better eﬃciency p-lad p-ls heavy-tailed distributions donoho montanari unpenalized universal guarantees exist also dimensionality dependent. however presence model selection obtain behavior p-lad achieves better asymptotic within section we’d like show ﬁnite sample performance ramp following aspects. first discuss select tuning parameter show existence uniqueness state evolution parameters allowing diﬀerent loss functions. second show limit behaviors iterative parameters ramp diﬀerent loss functions. third compare performance ramp algorithm diﬀerent error distribution settings includes light–tailed heavy–tailed. fourth release assumption gaussian design matrix show distribution design matrix eﬀect asymptotic performance distribution. finally discuss relative eﬃciency ramp estimators diﬀerent undersampling sparsity setting. policy choose thresholds based donoho sets α¯τt taken ﬁxed. donoho authors choose grid starting αmin grid iterative parameters. mimic approach oﬀers within interval ramp estimator iterative parameters ¯σt. parameters evaluate amse tune optimal minimizing amse. words calculated recursion right hand side equation calculated equation following simulation sections substitute tuning parameter based lemma order easy comparison huber loss least squares loss quantile loss. simulation examples implement equations diﬀerent cases loss functions hard simplify expression equations except error normal donoho subsection oﬀer plots recursion unique iteration goes diﬀerentiable non-diﬀerentiable loss functions. choose illustrate worst case behavior. follows least squares loss considered results state evolution equations presented figures below gaussian setting above consider least squares loss huber loss least absolute deviation loss quantile losses observe unique value state-evolution recursions easily found even non-diﬀerentiable losses recommendations section figures right panel shows evolves ﬁxed point near starting case least squares loss ﬁxed point near case huber least absolute deviations quantile losses respectively. simultaneously mapping evolves ﬁxed points near losses considered including non-diﬀerentiable losses. moreover figure illustrate loss great even start randomly chosen starting value. perform eﬃciency study subsection simulation step follows. based setting equation generate generate series regard threshold ¯τt. then iteration lemma stable point stopping |¯τt ¯τt−| small positive number taken here. lastly expression α¯τ∗ω expression amse theorem amse. penalized -estimators theory suggest cross-validation optimal values value corresponding amse present table below. table compares several necessary parameters iteration ramp algorithm. contrast four diﬀerent loss functions least squares loss huber loss least absolute deviation loss quantile loss results presented table averages repetitions. notice within twenty iteration steps ramp algorithm becomes stable matter loss function considered. furthermore present values number additionally present figure show empirical convergence amse respect optimal tuning parameter diﬀerent loss functions. plots illustrate becomes larger amse decreases dramatically stabilizes around reason amse becomes ﬁxed ramp algorithm shrinks estimator zero vector; hence amse ||xt|| large enough. moreover notice loss functions ramp algorithm chooses optimal oﬀer minimum amse. therefore ramp algorithm maintains advantage turns oﬀers optimal solution problem further know using square loss solve problem sensitive respect error distribution reason release loss function least squares loss general convex loss function satisfying condition consider robustness solution tail error model varies. results experiment presented figure observations immediately follow. lasso estimator sensitive heavy tail error distribution whereas huber loss least absolute deviation loss perform better tail error distribution becomes heavier. moreover larger tails least absolute deviation loss clearly preferred huber least squares loss whereas situation reverses tails light. mixture normals errors particularly diﬃcult bimodality error distribution. light heavy tales cases mixture distribution huber loss preferred least squares loss. lastly tails becomes even heavier estimators face problem estimating unknown parameter accurately. proved case gaussian design matrix element mean variance ramp algorithm recovers penalized m-estimator theorem release restriction design matrix generate three diﬀerent scenarios first case i.i.d. following whereas last composed cases random matrix entry i.i.d normal design. even though proved diﬀerent design matrix eﬀect performance ramp estimator central limit theorems eﬀects observe diminished inﬂuence results. ramp iteration calculate relative eﬃciency least square estimator versus least absolute estimator. known least square estimator preferable normal error assumption least absolute estimator beats least square estimator double-exponential error assumption classical low-dimensional setting. ﬁrst rows table normal error setting least square estimator preferable relative eﬃciency least square estimator w.r.t. least deviation estimator around further double exponential error setting least square estimator performs worse. result matches classical inference. last rows least squares estimator preferable matter error distribution. result foreseen donoho montanari karoui remarkably table discuss high-dimensional sparse case provides number non-zeros true parameter choose variety options range low-sparsity high sparsity ﬁrst rows table normal error setting penalized least squares estimator longer preferred settings. sparsity high reaches penalized least absolute deviations estimator preferred whereas sparsity p-ls estimator preferred. however last rows setting laplace distribution p-lad estimator always preferred matter size contradicts ﬁndings table shows model selection aﬀects choice optimal loss function. proof lemma immediate application state evolution deﬁned bayati montanari considers general recursions. hence suﬃces show proposed algorithm special case original notation bayati montanari generalized recursions studied ﬁrst argument. according bayati montanari state evolution recursion ramp algorithm involves variables special case recursion specify components general recursion initial condition verify simpliﬁcation series equations oﬀer ramp algorithm iterations. discuss ﬁrst step algorithm third whereas leave discussion second step last. observe proof lemma statement lemma follows successfully show total ﬁrst derivative strictly positive large enough; function concave smooth loss functions non-smooth loss functions observe strongly convex function bounded level sets. bayati montanari derive concave large strictly increasing. hence ¯rν+ made large positive large values turn ¯rν+ made strictly positive large values together condition convexity ready conclude next observe made positive large positive. moreover bayati montanari prove strictly concave condition implies guarantees hence made positive large next suﬃces observe total derivative given marginal derivatives made positive. careful inspection second derivative provides details second derivative negative i.e. function concave smooth necessarily negative non-smooth loss functions show analysis marginals analysis rest done equivalently. ¯rν+ otherwise. moreover symmetric densities ¯rν+ moreover ¯rν+ ¯rν+ opposite inequalities hold positive axis ¯rν+ additionally proxy rox− concave negative second derivative furthermore denotes function right hand side moreover bayati montanari show decreasing function hence limit well observing remaining terms independent proof lemma proof relies lemma simple modiﬁcation theorem bayati montanari theorem provides state evolution equation general recursion algorithm. lemma establishes connections algorithm general recursion proof simple application theorem bayati montanari simple relaxation conditions. moments. careful inspection proof lemma bayati montanari shows function uniformly bounded restriction moments unnecessary. version hoeﬀding’s inequality suﬃces applied independent not-necessarily equally distributed random variables proof theorem proof split parts. ﬁrst step show proposed algorithm belongs class generalized recursions deﬁned bayati montanari result presented lemma proof theorem shorter statements proof statements abbreviated notation bivariate function deviate notation proof statement denotes cumulative distribution function standard normal. well deﬁned information matrix errors distribution errors proof lemma notice sparse high dimensional setting distribution represented convex combination dirac measure measure doesn’t mass zero. denote random variables measures above. proof lemma notice sparse high dimensional setting distribution represented convex combination dirac measure measure doesn’t mass zero. denote random variables measures above. ﬁnish proof discussing p-ls estimator. lemma special case ramp algorithm loss function approximate message passing algorithm bayati montanari hence results apply algorithm bayati montanari apply. particular recent work zheng discusses properties case without loss generality assume single jump-point. according vector structure suﬃces show coordinate uniform asymptotic linearity result holds p]p. prove uniform asymptotic linearity resort known weak convergence properties empirical cumulative distribution functions brownian motion uniform decompositions work belloni chernozhukov probability converging proof lemma observe algebra contains{x design ramp algorithms. therefore contains vector lemma empirical distribution converges weakly need check", "year": 2015}