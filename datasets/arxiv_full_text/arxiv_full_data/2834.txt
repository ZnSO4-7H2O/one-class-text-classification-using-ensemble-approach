{"title": "A Unified View of Causal and Non-causal Feature Selection", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "In this paper, we unify causal and non-causal feature selection methods based on the Bayesian network framework. We first show that the objectives of causal and non-causal feature selection methods are equal and are to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We demonstrate that causal and non-causal feature selection take different assumptions of dependency among features to find Markov blanket, and their algorithms are shown different level of approximation for finding Markov blanket. In this framework, we are able to analyze the sample and error bounds of casual and non-causal methods. We conducted extensive experiments to show the correctness of our theoretical analysis.", "text": "paper unify causal non-causal feature selection methods based bayesian network framework. ﬁrst show objectives causal non-causal feature selection methods equal markov blanket class attribute theoretically optimal feature classiﬁcation. demonstrate causal non-causal feature selection take diﬀerent assumptions dependency among features markov blanket algorithms shown diﬀerent level approximation ﬁnding markov blanket. framework able analyze sample error bounds casual non-causal methods. conducted extensive experiments show correctness theoretical analysis. keywords causal feature selection non-causal feature selection mutual information markov blanket bayesian network feature selection essential data analysis identify subset features original features small possible building models data understanding data feature selection pressing ever since high-dimensional datasets become ubiquitous various applications example cancer genomics gene expression data easily features spam corpus collected approximately million features malicious detection high dimensionality incurs high computational cost memory usage also deteriorates generalization ability prediction models many feature selection methods proposed mainly divided three categories ﬁlter wrapper embedding data high dimensionality emerging ﬁlter method attracted attentions ever fast processing speed high-dimensionality prediction method independence robustness overﬁtting paper focus ﬁlter methods. last decades ﬁlter feature selection well studied achieved great success. successful class ﬁlter methods classical ﬁlter methods. classical ﬁlter feature selection input feature divided strongly relevant feature weakly relevant feature irrelevant feature respect class attribute common family classical ﬁlter algorithms ranks features according feature relevance features iteratively selects inclusion relevant features another emerging successful class ﬁlter approach identify markov blanket class attribute notion invented pearl causal bayesian network feature consists parents children spouses feature tying feature predictive power causality together discovery approach achieve parsimonious feature subset interpretable robust prediction models classical ﬁlter method since discovery approach explicitly induces local causal relations features target feature classical ﬁlter method paper call discovery approach causal feature selection classical ﬁlter method non-casual feature selection although causal non-causal feature selection studied decades little work systematically study relations them. tsamardinos aliferis ﬁrst link causal feature selection strongly relevant features non-causal feature selection. aliferis discussed empirically compared several causal feature selection methods embedded feature selection methods. guyon reviews discuss local causal discovery techniques application feature selection. pioneer work provides excellent perspective studying causal non-causal ﬁlter methods following fundamental questions classes ﬁlter methods still unexplored. firstly ﬁnding optimal feature selection objective function causal non-causal feature selection? rationale behind classiﬁcation? secondly achieve objective function subset search mechanisms behind existing major causal non-causal feature selection methods? thirdly assumptions bounds errors data requirements behind search mechanisms respectively? fourthly pros cons existing major causal non-causal feature selection methods? answer questions paper propose uniﬁed view systematically study causal non-causal feature selection objective functions search strategies assumptions bounds bayesian network framework. speciﬁcally major contributions concluded follows. firstly unify causal non-causal feature selection single objective function based mutual information discuss rationale behind them. secondly based uniﬁed objective function analyze search strategies existing major causal non-causal feature selection methods unify diﬀerent level approximations discovery. thirdly analyze assumptions behind search strategies existing causal non-causal ﬁlter algorithms unify algorithms diﬀerent level bayesian network structure assumptions. fourthly analyze propose sample error bounds causal non-causal feature selections. ﬁnally conduct extensive experiments using benchmark bayesian networks diﬀerent types real-world datasets validate analysis examine pros cons methods. paper organized follows. section discusses related work section presents notations deﬁnitions. section analyzes objective function rationale ﬁlter methods. section discusses search strategies existing major causal non-causal feature selection methods. section proposes assumptions behind search strategies existing methods. section discusses proposes sample requirement error bounds. section conducts extensive experiments studying pros cons existing major causal non-causal feature selection methods section concludes paper. simple mechanism computational eﬃciency ﬁlter methods well studied past decades. excellent reviews feature selection algorithms found reference therein. focus causal non-causal feature selection methods. general ﬁlter method consists elements search strategy feature subset generation evaluation criterion measuring feature relevance. evaluation criterion estimate potentially useful feature feature subset used learning algorithm independent learning models design eﬀective evaluation criterion plays role ﬁlter methods. past decades diﬀerent evaluation criteria ﬁlter methods proposed distance mutual information dependency consistency since mutual information general measure feature relevance several unique properties signiﬁcant amount work mutual information-based ﬁlter methods past twenty years exhaustive list). section focus simply review non-causal ﬁlter methods based mutual information. many advances ﬁeld reported since pioneer work lewis battiti lewis proposed criterion. simply ranks features order scores selects relevant features original feature set. however considers feature relevance features class attribute. battiti proposed mifs criterion considers feature relevance also adds penalty evaluate redundancy selected features. mifs greedy selects features sequentially iteratively constructs ﬁnal feature subset alternative evaluate combinatorial explosion feature subsets belonging original set. based mifs criterion many variants mifs proposed representative algorithms include mrmr cife fcbf mimr proposed. later yang moody proposed criterion. compared mifs criterion criterion evaluates classconditional relevancy. class-conditional relevancy considers situation feature provides predictive information jointly another feature respect class attribute i.e. complementary information features. disr cmim relaxmrmr included variants criterion. owing diﬃculty estimating mutual information high dimensions existing mutual information-based ﬁlter methods based various low-order approximations mutual information. approximations successful certain applications heuristic nature lack theoretical guarantees. thus main problems majority mutual information-based ﬁlter methods cases unknown consists optimal feature selection solution independently class models ﬁtted conditions ﬁltering method output optimal feature classiﬁcation emerging successful class ﬁlter methods causal feature selection attracted much attention recent years since bringing causality play causal feature selection naturally provides local causal interpretability features class attribute better understanding underlying mechanisms behind data. causal feature selection class attribute consider causal bayesian network edge indicates direct cause feature consists direct causes direct eﬀects direct causes direct eﬀects feature. therefore feature provides complete picture local causal structure around feature. feature minimal features renders feature statistically independent remaining features conditioned feature theoretically class attribute optimal feature subset classifcation accordingly discovery actually procedure feature selection koller sahami ﬁrst feature selection proposed koller-sahami algorithm promising paradise causal feature selection. however algorithm focuses optimal feature selection guarantee actual margaritis invented ﬁrst sound discovery algorithm gsmb design bayesian network structure learning feature selection. cause feature selection become practical tsamardinos aliferis linked discovery feature selection faithful assumption introduced several improvements iamb iamb family algorithms directly discovers achieves fast running speed. however approaches required number data samples least exponential size would scale thousands variables real datasets limited samples sizes. divide-and conquer approach proposed mitigate problem requiring sample exponential size local neighborhood representative algorithms include hition-mb mmmb ipcmb stmb idea behind algorithm described follows. firstly algorithms ﬁnds parents children feature interest secondly discover spouses feature. thus divide-and conquer approach return sets given target. eﬃciently eﬀectively target type approach. thus pc-simple mmpc hiton-pc semi-hiton-pc algorithms proposed discovery. since feature includes parents children feature discovering eﬃcient ﬁnding pc-simple mmpc hiton-pc applied feature selection approximate achieved desirable results. compared noncausal feature selection causal feature selection shown theoretically optimal thus answers questions consists optimal feature selection solution conditions ﬁlter method output optimal feature classiﬁcation. class attribute distinct class labels feature number features. assuming training data deﬁned number data instances multidimensional vector deﬁned class label associated notations below indicates feature overall indicates features except deﬁnition give probability distribution distinct features said conditionally independent subset \\{fi represents conditional probability given paper notation ⊥⊥fj|s denotes conditionally independent given subset \\{fi ⊥⊥fj|s represents conditionally dependent given indicates features except joint probability distribution features directed acyclic graph call triplet bayesian network satisﬁes markov condition every feature independent subset non-descendant features conditioned parents bayesian network encodes joint probability features decomposes product conditional probability distributions feature given parents parents then written deﬁnition given bayesian network faithful every independence present entailed markov condition. faithful exists directed acyclic graph faithful deﬁnition path said dseparated vertices contains chain fork middle vertex contains inverted fork middle vertex descendant said d-separate blocks every path vertex vertex theorem faithfulness condition given bayesian networks dseparation captures conditional dependence independence relations encoded implies variables d-separated given subset \\{fi conditionally independent conditioned proposition illustrates properties parents children bayesian networks proposition concludes idea discover spouses markov blanket given feature. traditional feature selection input feature always classiﬁed three categories strongly relevant weakly relevant irrelevant features respect deﬁned follows terms conditional independence. strongly relevant features aﬀects conditional class distribution provide unique information i.e. cannot replaced features. thus always necessary optimal subset. weakly relevant features informative redundant since replaced features without losing information irrelevant feature unnecessary included optimal subset. instead probabilistic approach recently alternative deﬁnition relevance given framework mutual information discussing alternative deﬁnitions ﬁrst give concepts mutual information below. assuming {ssel ssel} ssel denotes selected feature ssel represents remaining features ssel. given ssel conditional likelihood sel) represents value denotes value ssel data instance. ﬁrst term likelihood ratio true predicted class distributions given ssel averaged input data space. second term equals conditional mutual information ssel given ssel ﬁnal term conditional entropy given features irreducible constant. deﬁnition re-written follows. minimizing equivalent maximizing holds maximal. accordingly maximizing equivalent maximizing conditional likelihood eq.. accordingly using mutual information objective function non-causal feature selection re-formulated below. able solve ﬁltering feature selection problem classiﬁcation? discuss question perspective bayes error rate. given classiﬁcation problem minimum achievable classiﬁcation error classiﬁer called bayes error rate. choose bayes error rate justifying since tightest possible classiﬁer-independent lower-bound depending predictive features class attribute alone. fano hellman proposed lower upper bounds connect shannon conditional entropy bayes error rate. large kl||q) approach zero. thus conclude minimizing conditional entropy class attribute given predictive feature ssel equivalent maximizing conditional likelihood minimizing bayes error rate simultaneously. since maximizing equals minimizing upper bound pber. thus justiﬁes feature selected classiﬁcation best facilitate minimizing bayes error rate. illustrates relationships among pber denote equivalent causal non-causal ﬁlter methods ﬁnding subset ssel maximizes challenging combinatorial optimization problem. exhaustive search time complexity restricting maximum size ssel reduce time complexity denotes number subsets sizes less equal within still expensive even high-dimensional mutual information easily estimable. therefore causal non-causal feature selection methods adopt sequential greedy strategy considering features one-by-one addition removal optimize given currently selected ssel fi−} already selected features time time feature chosen maximizes following objective function maxfi∈f \\ssel general maximizing maxfi∈f \\ssel includes steps however computing multidimensional mutual information challenging. size ssel increases computing becomes impractical complexity demands large training sample sizes grows exponentially number features ssel. tackle challenging problem causal feature selection reducing problem estimating high-order optimization problem discovering exact markov blanket bayesian networks non-causal feature selection computes linear combination low-order mutual information terms approximate strongly relevant features approximate markov blanket. figure shows existing causal non-causal feature selection methods adopt series sequential greedy strategies high-order low-order respectively optimizes eq.. assuming ssel currently selected feature time table summarized objectives time complexity representative algorithms causal non-causal feature selection methods. theorem shows subsumes information remaining features koller sahami proposed paradise method ﬁnding markov blankets feature selection ssel current features ssel beginning phase exists markov blanket feature ssel included ssel. koller-sahami algorithm ﬁrst markov blanket feature selection. evaluating whether removed current ssel necessary check subsets ssel backward removing ssel s.t. forward adding maxx sseli backward removing ssel s.t. interleave forward backward steps iamb search forward adding corr minz⊆ssel\\x ssel maxx∈f \\ssel corr backward removing ssel ssel search interleave forward backward steps pcmb search interleave forward backward forward adding {sel} backward removing {sel} {sel} search mmmb search ssel |ssel| ssel ssel s.t. remove ssel search mmmb search ipc-mb search ﬁnding candidate feature ssel ssel removing false parents children ssel {ssel s.t. removed ssel. removing false positive satisfying removing false positive {sel} satisfying {sel} objective forward backward maximum size subset needed checked limited backward maximum size subset needed checked limited backward step step stmb maximum size subset needed checked limited subsets |ssel|) determine subset subsumes predictive information i.e. markov blanket handling larger number features computationally prohibitive check subsets ssel. moreover algorithm explicitly claim identify provide theoretical guarantees. knowledge gsmb algorithm ﬁrst provably correct discovery algorithm designed eﬃcient bayesian network structure learning. based gsmb many eﬃcient eﬀective algorithms discovering bayesian networks proposed. following sections retroﬁt several representative strategies markov blanket discovery feature selection perspective explain strategies greedily optimize optimal feature selection. theorems lemmas discussed following assumptions faithfulness condition; causal suﬃciency assumption; independence tests reliable. strategy directly markov blankets bayesian networks adopts forward backward steps optimize i.e. sequentially maximizing forward step minimizing backward step conditioned full current selected ssel. proof since directly connected edge proposition must exist subset make hold. descendant markov condition guarantees proposition conditionally dependent given subset contains common child thus lemma proven. iamb strategy forward backward steps. iamb strategy exact forward backward steps sequential optimization eq.. assuming ssel denotes currently selected markov blanket time time forward backward steps implemented follows. forward step. gsmb algorithm time ssel added ssel satisfying forward step terminated ssel forward step gsmb statically orders features admits ssel next satisﬁes iamb algorithm enhances gsmb dynamic heuristic time selects next feature highest mutual information conditioned current ssel maxx∈f \\ssel ssel diﬀerence iamb gsmb heuristic used forward phase. gsmb statically orders variables iamb uses dynamic heuristic. dynamic heuristic makes features within enter ssel early possible false positives enter ssel much possible thus leads size ssel small possible. forward step iamb algorithm time feature ssel added ssel. lemma parents children gradually selected added ssel. lemma parents children added ssel spouses added ssel ﬁnally. greedily sequential search strategy positives enter ﬁnal selected ssel forward step. example assuming descendant time ssel time holds added ssel time added ssel. thus false positives removed backward step. time ssel same. iamb algorithm time feature maximizes equivalent maximizing thus forward step iamb greedily maximizes gsmb algorithm unlike iamb time implicitly greedily maximizes adding feature ssel satisﬁes ssel gsmb iamb algorithms terminate forward step backward step time ssel same. thus time minimizing equivalent minimizing c|{ssel gsmb iamb removed ssel. step give feature ssel false positives ssel achieved forward step removed. ssel exact markov blanket theorem proved. forward step makes size ssel possibly become high-dimensional. interiamb strategy mitigates problem interleaving forward backward steps iamb keep ssel small possible maximizes ssel minimizes ssel simultaneously. proof time forward step maximizes adding feature ssel maximizes ssel. feature added ssel backward step triggered immediately minimizes redundancy removing features ssel satisfy {ssel s.t. maximizing minimizing simultaneously strategy convergence ssel ssel exact markov blanket theorem proved. time complexity strategies measured number conditional independence tests executed. gsmb iamb interiamb average time complexity worst time complexity total number features worst case |ssel| practice size ssel always relative small total number features gsmb iamb inter-iamb entire current selected features ssel conditional test. strategy results number data instances increase exponentially size conditional sets. mitigate drawback improved strategy adopts divide conquer strategy including ﬁnding independently instead directly ﬁnding markov blankets using feature-subset enumeration possible subsets within currently selected feature ssel discover instead using full selected feature ssel. example calculate entire ssel conditional perform multiple tests using possible subsets within ssel. four representative approaches instantiate strategy max-min heuristic simple max-heuristic backward heuristic k-greedy heuristic. representative discovery algorithms include mmmb hiton-mb ipc-mb stmb. forward step. max-min heuristic selects feature maximizes minimum correlation conditioned ssel. speciﬁcally minimum correlation denoted corr conditioned possible subsets ssel calculated below. finding spouses. step max-min heuristic ﬁrstly ﬁnds parents children feature within ssel found step assuming make holds spouse satisﬁes following equation. proof ssel lower bound correlation conditioned current selected feature ssel. ssel corr added ssel. forward step terminate feature ssel corr holds. lemma makes added ssel forward step. markov condition suﬃcient conditional block descendants added ssel since bayesian network minimal conditional shield remaining features assuming descendant holds cannot removed current ssel forward backward steps includes features within figure illustrates assuming target feature child spouse descendant respectively enter remain ﬁnal output ssel using max-min heuristic. dependent conditioned subsets parents children namely empty set. problem conditioning empty path d-connects conditioning path d-connects however independent conditioned including spouse i.e. remove symmetry correction always employed. idea behind symmetry correction bayesian network proof theorem ssel include descendants accordingly markov condition sets parents children descendants found using max-min heuristic include since ancestors. symmetry correction ﬁning ssel spouse-search phase constructing triple lemma spouse must included ssel. otherwise thus satisfy eq.. three cases enter ssel. thus spouse-search phase ssel interleaving max-heuristic. phase ﬁnding strategy interleaves forward backward strategy. forward step feature added current feature backward strategy triggered. representative algorithm using strategy hiton-mb algorithm. finding interleaving max-heuristic uses simpler forward strategy max-min heuristic mentioned above. interleaving forward backward steps heuristic computers sorts features descending order value heuristic features satisfy discarded never considered again. heuristic interleaves follows empty. theorem symmetry correction interleaving max-heuristic ssel. proof ﬁrstly prove interleaving max-heuristic ssel. holds enter ssel forward step. backward step lemma cannot removed ssel. thus features within added ssel forward backward step. similar proofs theorem interleaving max-heuristic ssel include descendants able removed discovery step using interleaving max-heuristic. since ssel discovery phase similar proofs theorem symmetry correction ssel holds. comparing direct approach discover section strategies section perform exhaustive enumeration subsets within current ssel instead conditioning full ssel. thus max-min heuristic interleaving version time complexity bounded |ssel| denotes largest size ssel forward backward steps. ipc-mb stmb strategy. ipc-mb stmb algorithms employ backward strategy discover instead using forward-backward strategy. beginning setting ssel backward step ssel removed ssel satisfying assuming conditional computing ipc-mb stmb strategies sizes |ssel| backward step ﬁnding performed follows. spouses ipc-mb adopts idea mmmb hiton-mb. mmmb hiton-mb ipc-mb ﬁrstly parents children feature found ssel perform symmetry correction ﬁnally spouses parents children feature ssel. however stmb ﬁnds spouses ssel instead parents children feature ssel remove false positives ssel using found spouses instead symmetry correction. detailed idea given follows. finding spouses removing false positives ssel feature ssel s.t. added next added feature ssel {ssel s.t. removed ssel. process terminates features ssel checked. time complexity ipc-mb includes ﬁnding complexity largest size conditional search. worst time complexity ipc-mb features parents children stmb average time complexity worst time complexity conditioning possible subsets performing feature-subset enumeration within current ssel computationally expensive prohibitive size ssel becomes large. example check whether able added ssel worst case total number subsets checked |ssel|. accordingly methods γ-greedy search natural problem setting. γ-greedy search checks subsets size less equal user-deﬁned parameter maximum size subsets needed checked. theorem ssel found using γ-greedy heuristic proof since holds. γgreedy heuristic features within added ssel. accordingly spouses also added ssel spouse discovery step. assuming added ssel forward step. exists subset make hold threshold suﬃces condition three features case able removed ssel backward step. thus ssel theorem theorem proven. controlling size conditioning limit number candidate markov blankets feature. size becomes large still computationally prohibitive evaluate subsets size comparing direct approach discover section strategies section perform exhaustive enumeration feature subsets within current instead conditioned whole current thus max-min heuristic interleaving version time complexity bounded o||p c|). employing γ-greedy search strategy time complexity bounded o||p c|γ) denotes subsets sizes less equal within holds maximize causal feature selection uses high-order mutual information terms compute forward backward steps exact instead non-causal feature selection integrates forward backward steps objective function using linear combination low-order mutual information terms attain approximate denotes class-conditional relevancy. class-conditional relevancy considers situation feature provides predictive information jointly another feature respect less means unnecessary duplicated information ssel. otherwise holds indicates ssel combine well i+i−i+i provides information eq.. reduce computational costs diﬀerent non-causal feature selection methods proposed diﬀerent level approximations decompose linear combination low-order mutual information terms. brown proposed many mutual information-based causal feature selection methods within following parameterized criterion. play role balancing factor represents three-way interaction among existing features ssel class attribute candidate feature considered inclusion ssel. using feature selection representative algorithm algorithm omit last term becomes cording mutual information score selects inclusion features algorithms suboptimal many ways particularly naive approximations uses need specify good value number features mutual information based feature selection methods cannot simply reduced framework representative algorithm fast correlation based filter diﬀerent algorithms designed framework predeﬁned threshold fcbf algorithm specify number selected features advance. algorithm works follows. although cannot reduce fcbf framework directly following shed light fcbf causal interpretation. limit consider three-way interaction among features since fcbf restrict consider interaction terms involving candidate feature previously selected features estimation computational reasons. deﬁnitions imply prefer select direct causes direct eﬀects spouses prior features accurate prediction. spouse common child thus even provides information though spouse direct cause direct eﬀect class-conditional relevancy view deﬁnition conﬁrms spouses strongly relevant features. deﬁnition given ancestor descendant provide information deﬁnitions discuss fcbf causal interpretability follows. following observations validated section fcbf ssel empty would ﬁrst feature added ssel. since ssel empty deﬁnition ancestor descendant holds. thus feature within largest mutual information would added ssel. according deﬁnitions term used parents children term employed discover spouses speciﬁcally fcbf approximate without attempting identifying spouses since deﬁnitions show belong holds. spouse deﬁnition attempt also discover spouses since although spouse exists feature common child make hold. fcbf thus structure added ssel regard meanwhile ancestor regarding added ssel. example given structure currently ssel similar analysis above added ssel. since diﬃcult specify good value approximate thus using ﬁnal selected feature include direct causes direct eﬀects indirect causes indirect eﬀects remove indirect causes indirect eﬀects results returned eq.. using ﬁnal selected feature include direct causes direct eﬀects spouses finally remove false positives results returned eq.. time complexity non-causal feature selection depends value needs perform mutual information computations. need perform pairwise comparisons implements pairwise comparisons. optimize object function maxfi+∈f time causal feature selection uses high-order feature interactions exact non-causal feature selection employs low-order feature interactions approximate therefore causal non-causal feature selection criteria make diﬀerent assumptions interactions features underlying data distributions. following using bayesian networks propose structure assumption understand diﬀerent interaction features implied causal non-causal feature selection criteria discussed solve object function maxfi+∈f bayesian network represents joint probability distribution bayesian network classiﬁer correctly predict value designated discrete class variable given vector predictive features maxci∈c bayesian network represent interactions features section explore assumptions diﬀerent non-causal feature selection algorithms diﬀerent naive bayesian networks. naive bayes shown figure simplest bayesian network since feature parent variable class attribute features assumed conditionally independent give tree-augmented naive bayes network figure relaxes assumption feature interactions allows feature depend feature addition class attribute k-dependence bayesian network contains structure relaxes nb’s tan’s independence assumptions allowing every feature conditioned class attribute maximum features parents generalize higher degrees feature interactions. clearly particular cases k-db respectively. k-db bayesian network becomes since time ssel ﬁrst terms same assumptions decomposed pairwise mutual /|ssel| holds. meanwhile size ssel increases assumption tend asymptotically towards assumption implying stronger belief assumption assumption holds thus structure assumptions make hold decomposed sums low-dimensional mutual information terms? bayesian network relaxes assumption assumption allows feature depend feature addition class attribute transformed eq.. root attribute tree-augmented naive bayes. bayesian network assumption assumption states features within ssel classconditionally independent given unselected feature xand explores k-db structure assumptions relax nb’s tan’s independence assumptions allowing feature features parentsi.e. addition class attribute. thus -dependence k-db structure transformed eq.. section concentrated attention structure assumptions kdependency. based assumptions analyze series represented objection functions non-causal feature selection. restrictive assumptions motivated mainly computational considerations approximate however increasing challenging non-causal feature selection framework based estimate high-dimensional mutual information especially small samples machine learning. therefore causal feature selection addresses challenge relaxing ﬁxed number parents k-db place limitations feature interactions among predictor features general bayesian network structure assumption. causal feature selection leverages structure topology assumptions bayesian networks deal high-dimensional mutual information ﬁnding exact class attribute instead framework structure assumptions. causal feature selection relaxes structure assumptions perspectives. represent feature interactions among predictor features using general bayesian network structures figure maintain without parents bayesian network-augmented bayesian network-augmented becomes accordingly using bayesian network-augmented represent interactions features existing causal feature selection designed based faithful assumption global local markov assumptions follows. following structure assumptions discuss rationales behind existing causal feature selection methods. assumptions iamb strategy. discussed section direct approach causal feature selection methods mainly includes iamb algorithms. unselected feature holds. maximize direct approach uses assumption designs forward backward steps compute greedily discovering assumption property assumption holds. forward backward steps decompose assumption assumptions below. assumption bayesian network global markov assumption states d-separated active path given based assumption section greedily constructs ssel unselected feature ssel ssel given ssel exists ssel. thus based assumption iamb strategy able ssel found forward step. based assumption section employed backward step remove false positives ssel. assumptions inter-iamb strategy. inter-iamb strategy interleaves forward backward steps used iamb strategy. diﬀerence strategies lies assumption backward step inter-iamb strategy uses assumption below. assumption inter-iamb strategy wait possible features found forward step. instead feature added ssel examines whether feature ssel {ssel thus compared iamb strategy inter-iamb strategy employs assumption make size ssel small possible iteration. makes inter-iamb strategy data eﬃcient iamb strategy. assumptions strategies theoretically guarantee iamb inter-iamb sound complete. direct approach discussed directly employs idea global markov assumption design algorithms discovery conditioning entire ssel. sample complexity type algorithms exponentially increases size given target. thus divide conquer method perform conditional tests using possible subsets ssel ﬁnds separately based assumption v-structure assumption mmmb hiton-mb ipc-mb stmb discovery phase algorithms greedily bulid parent ssel unselected feature ssel ssel ssel. feature parents enter ssel ancestors removed assumption however descendant except feature ssel include parents descendant thus descendant cannot removed ssel. accordingly illustrates limitation using local markov assumption explains reason divide conquer method symmetry constraint correct outputs removing descendants. discovery phase mmmb hiton-mb ipc-mb stmb directly v-structure assumption. ﬁrst phase correct v-structure assumption able guarantee second phase discover correct based discussion above non-causal feature selection causal feature selection assumptions behind types methods simplest structure assumption complex full bayesian network structure assumption. addition equations select features generates selective bayesian network structure conditional likelihood close possible thus non-causal causal feature selection methods designed assumptions equivalent greedily iterative optimization selective structure approximate full structure including features space possible selective structure avoiding exhaustive search. example figure illustrates series examples full structure selective structure non-causal causal feature selection algorithms. meanwhile figure summarizes discussion figure section focus discuss lower sample bounds discrete data causal non-causal feature selection. discrete data test test always used determine independence features. statistic able perform reliable conditional independence test given current ssel lower bound data samples given below. represent discrete values take respectively ssel. constant always suggested agresti minimum average number instances statistic distribution requirement signiﬁcance calculated. thus considering constant lower bound required data samples determined rssel rssel plays role eq.. guarantee mutual information computations reliable algorithm table lower bound data samples determined mutual information computation largest value rssel among tests checked. assuming xmax feature takes largest discrete values conditional tests true thus following discuss lower bounds causal non-causal feature selection follows. best situation forward step largest conditional ssel equals thus number data samples bounded rxmax average situation number data samples bounded rxmax rssel. condition subsets current ssel. type methods includes steps discover discovery step number data samples bounded rxmax rsmax smax ssel largest size conditional searching. discovery step steps. ﬁrst step candidate spouses discovering feature ssel discovery step). step bounded largest size conditional features take largest discrete values step candidate spouse discovering. second step discover spouses. sample bound largest size conditional features take largest discrete values step spouse mining. thus analysis above average case lower bound mmmb pcmb hiton-mb ipc-mb max{rxmax rsmax max}}. since stmb algorithm adopts diﬀerent method spouses r{s′′ number data samples determined step ﬁnding spouse removing false largest positives ssel thus average case size conditional steps ﬁnding spouses removing false positives. condition subsets ssel given size case analysis lower bound data samples case conditioning subsets current ssel. diﬀerence given size maximum size conditional sets limited |ssel| thus type methods subset smax used situation check subsets current ssel. size needs least number data samples shown figure although cannot theoretically prove intuition directly paper experiments various types data section validate intuition. section discuss error bounds non-causal causal feature selection. since methods independent classiﬁers analyze bounds information gain approximate exact feature selection proxy error bounds using bayes error rate pber conditional log-likelihood section presented subset ssel maximizing maximizes equivalent minimizing pber. based using information gain discuss bounds approximate markov blanket exact markov blanket follows. according analysis section certain assumptions causal feature selection algorithms designed conditioning full ssel subsets exact data. moreover algorithms conditioning subsets ssel also able exact set. thus section discuss bounds information gain exact exact follows. since pber theorem gives minimum upper bound pber. conditioning subsets ssel size strategy return approximate approximate theorem holds. since theorem illustrates minimum upper bound pber thus conditioning subset size search strategies employed non-casual feature selection algorithms discussed section attempt spouses diﬀerent values strategies return approximate superset subset section focus discussing bounds superset subset found strategies. section select representative causal feature selection methods three representative non-causal feature selection algorithms follows evaluate using various types benchmark datasets including synthetic data sampled real-world bayesian network real-world datasets including datasets large data samples datasets extreme small samples datasets multiple classes class-imbalanced datasets. discretize continuous features discretization method causal explorer toolkit proposed aliferis since svms random forests decision tree implicitly embed feature selection process themselves simple assumptions knn. conduct exhaustive analysis causal non-causal feature selection methods classiﬁcation experiments nave bayes k-nearest neighbor avoid complex parameter tuning employ following metrics validate compared methods. kappa statistic measure consistency amongst diﬀerent raters classiﬁer stability taking account agreement occurring chance kappa statistic standardized scale perfect agreement exactly would expected chance negative values indicate agreement less chance. values kappa statistics corresponding kappa agreements shown table currently existing stability measures feature selection always require feature sets comparison contain number features eight feature selection methods returns diﬀerent feature sets respectively. thus following experiments kappa statistics compute stabilities classiﬁer illustrate stability feature selected feature selection algorithm. experiments performed window dell workstation intel core .ghz processor eight feature selection methods comparison classiﬁers implemented matlab. section sample training cases distributions alarm network employed real decision support systems since exactly know markov blanket variable network gold-standard known quantify classiﬁcation performance true markov blankets selected feature subsets causal non-causal feature selection methods. alarm bayesian network designed provide alarm message system patient monitoring. alarm network includes variables complete structure network shown figure network choose variable class attributes classiﬁcation variable takes three class labels normal high largest among variables including parent four children three spouses. randomly sampled datasets training cases training cases test cases alarm network respectively reported prediction accuracy average runs algorithm training cases test cases. following tables truepc truemb denote ground-truths sets network respectively k=nmb k=npc represent parameter i.e. numbers features selected mrmr cmim equal sizes true true sets network respectively. precision metric number true positives output alarm network) divided number variables output algorithm. precision evaluates number false positives output algorithm. recall metric number true positives output divided number true positives alarm network). recall reports number true positives output algorithm. notation denotes average performance algorithm data prediction accuracy precision recall represents corresponding standard deviations average performance. table reports average prediction accuracies standard deviations using training cases. table reports performance precision recall algorithm discovery. table shows number parents children spouses false positives found feature sets algorithm. table illustrates true achieves highest prediction accuracy among selected feature sets. using cases mmmb hiton-mb exact shown tables thus higher prediction accuracy rivals table table expect iamb algorithm feature sets found algorithms include variables within explains reason iamb mrmr cmim competitive prediction accuracy. cmim mrmr cannot spouses using cases validates analysis section using cases table mrmr gets highest prediction accuracy iamb mmmb hiton-mb cmim since ﬁnds almost rivals achieves fewest false positives among algorithms shown table mmmb ﬁnds spouses also includes false positives rivals. mrmr gets highest prediction accuracy iamb mmmb hition-mb cmim. results discussed above target feature plays role predicting target spouses target provides complementary prediction information predicting target. thus table report prediction accuracies using mmpc hiton-pc employed mmmb hitonmb discovery. table reports precision recall algorithm discovery. table reports using cases algorithms exact thus prediction accuracy true achieved table using cases tables illustrate features found higher prediction accuracy achieved. therefore tables conﬁrms true target feature features prediction. section examine prediction bound approximate exact target feature. achieve goal consider prediction accuracy true baseline since using prediction accuracy almost using set. check diﬀerent prediction accuracies diﬀerent feature sets varying sizes selected feature sets mmrm cmim jmi. figures prediction accuracies mrmr cmim bounded prediction accuracy true set. highest accuracy three algorithms begins selected features includes features features. thus results conﬁrm bounds proposed section section extensively evaluate causal non-causal feature selection algorithms using four types datasets datasets small feature-to-sample ratio e.g. n≫p; datasets small sample-to-feature ratio e.g. n≪p; datasets multiple classes; extremely class-imbalanced datasets. select datasets large-sized data samples machine learning repository shown table experiment mrmr cmim since hard select suitable parameter advance i.e. number selected features user-deﬁned parameter features algorithms choose feature subsets highest prediction accuracy record running time size feature running time number selected features mrmr cmim respectively. tables report prediction accuracies algorithm using using table illustrates non-causal feature selection methods almost performance causal feature selection algorithms especially iamb gives little better performance mrmr cmim jmi. meanwhile mmpc hiton-pc also achieve good prediction accuracies. using table mmmb hiton-mb better accuracy mrmr cmim jmi. note causal feature selection algorithms mmpc hiton-pc mmmb hiton-mb iamb need specify user-deﬁned parameter advance number selected features. however non-causal feature selection algorithms mrmr cmim hard select good value moreover accuracies listed tables highest prediction accuracies selected features selected mrmr cmim jmi. even like this mrmr cmim signiﬁcantly better causal feature selection algorithms. tables report kappa statistic algorithm using respectively. table mmpc hiton-pc achieve better kappa statistics algorithms except madelon dataset. table hitonmb mmmb better algorithms except madelon dataset. results consistency discussed tables tables shows running time algorithm. clearly among causal feature selection algorithms iamb fastest since number selected features iamb exceed leads time complexity almost linear number features dataset. mmpc hiton-pc hiton-mb mmmb need check subsets within current selected feature table features selected expensive costs mmpc hiton-pc hiton-mb mmmb spend. clearly discussed before mrmr cmim pairwise comparisons thus faster causal feature selection algorithms. table shows number selected features algorithm. since user-deﬁned parameter features mrmr cmim choose feature subsets highest prediction accuracy record running time size feature running time number selected features mrmr cmim respectively table denotes represents number features highest accuracy corresponding algorithm using number features highest accuracy corresponding algorithm using knn. mmpc hiton-pc select features mmmb hitonmb iamb selects fewest features among eight algorithms. mrmr cmim returning mulitple feature sets needs selected features high accuracy gisstee data mmpc hiton-pc mmmb hiton-mb select signiﬁcantly features algorithms since features dataset highly correlated. results iamb selects features conditional large make conditional tests unreliable. also explains reason prediction accuracy iamb table signiﬁcantly algorithms. finally figures compare causal feature selection methods mrmr cmim varying numbers selected features. figures dataset compare causal feature selection method highest prediction accuracy mrmr cmim jmi. large number data samples dataset always causal feature selection method gets highest prediction accuracy except madelon dataset although mrmr cmim vary numbers selected features. illustrates enough data samples causal feature selection methods prefer optimal feature selection. table gives datasets high dimensionality small number data samples. following tables notation denotes algorithm fails dataset excessive running time. since mrmr cmim uses user-deﬁned parameter control size ﬁnal selected feature subset i.e. number selected features features selecting feature highest prediction accuracy reporting result. tables using non-causal feature selection methods mrmr cmim better causal feature selection algorithms mmpc hiton-pc mmmb hiton-mb iamb. illustrates high dimensionality small number data samples number data instances enough support causal feature selection algorithms conduct reliable conditional independent tests. serious problem mrmr cmim using pairwise comparisons. furthermore mrmr cmim tune parameter control size selected feature trade-oﬀ search eﬃciency prediction accuracy. accordingly tables mrmr cmim achieve better kappa statistic mmpc hiton-pc mmmb hiton-mb iamb. furthermore table shows hiton-pc mmmb hiton-mb expensive computation even prohibitive data sets prostate dorothea leukemia. explanation datasets larger shown table figures compare causal feature selection methods mrmr cmim varying numbers selected features. dataset select causal feature selection method highest prediction accuracy comparing mrmr cmim jmi. small-sized data samples high dimensionality best causal feature selection method worse mrmr cmim jmi. results figures validate using high-order feature interactions exact causal feature selection methods meet data ineﬃcient problem needs data samples non-causal feature selection methods. accordingly largesized data sample causal feature selection methods tend exact small-sized data samples high-dimensionality causal feature selection methods not. thus non-casual feature selection deal datasets small-sized data samples high-dimensionality better causal feature selection. small number data instances. mrmr cmim i.e. number selected features features selecting feature highest prediction accuracy reporting result. tables given small number features large number data instances even data large number classes mmpc hitonpc mmmb hitom-mb almost prediction accuracy three non-causal feature selection even better datasets landsat dataset classes. meanwhile tables shows mmpc hiton-pc mmmb hitom-mb iamb achieve better kappa statitic connect- splice waveform landsat datasets using given dataset small number data instances larger number classes mmpc hiton-pc mmmb hitom-mb iamb fail select features mrmr cmim seem work well especially cmim. tables report number selected features running time algorithm. mrmr cmim faster mmpc hiton-pc mmmb hitom-mb iamb. figures shows given dataset multiple classes dataset large-sized data samples causal feature selection work well even better noncausal feature selection datasets however dataset enough data samples causal feature selection fails dataset non-causal feature selection work well dataset. table gives class-imbalanced datasets. following tables notation denotes algorithm fails dataset excessive running time. mrmr cmim i.e. number selected features features selecting feature highest prediction accuracy reporting result. tables using although eight algorithms good prediction accuracy algorithms achieve show tables furthermore prediction accuracy causal feature selection methods three non-causal feature selection algorithms achieve almost performance. tables report number selected features running time algorithm. mmmb hiton-mb slowest algorithm among nine algorithms comparison. therefore conclude whatever causal feature selection methods non-causal feature selection algorithms cannot deal class-imbalanced datasets well. figures shows dataset class-imbalanced varying number selected features non-causal feature selection cannot work well. causal feature selection worse non-causal feature selection methods. paper proposed uniﬁed perspective understand relations causal non-causal feature selection bayesian network framework. bayesian network framework using markov blanket core concept analyzing causal non-causal feature selection methods found classes ﬁlter methods takes diﬀerent assumptions perform low-order high-order approximation strategy discovery. high-order approximation causal feature selection able exact non-causal feature selection presents approximate solution low-order approximation. according theoretical experimental analysis causal feature selection cannot deal well dataset high dimensionality small-sized data samples non-causal feature selection non-causal ﬁlter methods hard determine suitable value user-deﬁned parameter meanwhile existing causal non-causal feature selection methods cannot handle class-imbalanced datasets well. thus future largesized discovering small-sized samples identifying class-imbalanced data feature stability causal feature selection need worth exploring.", "year": 2018}