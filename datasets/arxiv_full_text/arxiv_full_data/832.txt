{"title": "Continual Reinforcement Learning with Complex Synapses", "tag": ["cs.AI", "cs.LG", "cs.NE"], "abstract": "Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna & Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.", "text": "unlike humans capable continual learning lifetimes artiﬁcial neural networks long known suffer phenomenon known catastrophic forgetting whereby learning lead abrupt erasure previously acquired knowledge. whereas neural network parameters typically modelled scalar values individual synapse brain comprises complex network interacting biochemical components evolve different timescales. paper show equipping tabular deep reinforcement learning agents synaptic model incorporates biological complexity catastrophic forgetting mitigated multiple timescales. particular well enabling continual learning across sequential training simple tasks also used overcome within-task forgetting reducing need experience replay database. outstanding enigmas computational neuroscience brain capable continual lifelong learning acquiring memories skills quickly robustly preserving ones. synaptic plasticity ability connections neurons change strength time widely considered physical basis learning brain knowledge thought distributed across neuronal networks individual synapses participating storage several memories. given overlapping nature memory storage would seem synapses need labile response experiences stable enough retain memories paradox often referred stabilityplasticity dilemma trained nonstationary data distribution distinct tasks sequence network quickly forget learnt earlier data. reinforcement learning data typically accumulated online agent interacts environment distribution experiences often nonstationary training single task well across tasks since experiences correlated time agent’s policy changes learns. typical addressing nonstationarity data deep store experiences replay database interleave data data training however solution scale well computationally number tasks grows data might also become unavailable point. furthermore explain brain achieves continual learning since question remains ever-growing dataset stored without catastrophic forgetting. potential answer arise experimental observations synaptic plasticity occurs range different timescales including short-term plasticity long-term plasticity synaptic consolidation intuitively slow components plasticity could ensure synapse retains memory long history modiﬁcations fast components render synapse highly adaptable formation memories perhaps providing solution stability-plasticity dilemma. paper explore whether biologically plausible synaptic model abstractly models plasticity range timescales applied mitigate catastrophic forgetting reinforcement learning context. work intended proof principle incorporation biological complexity agent’s parameters useful tackling lifelong learning problem. running experiments tabular deep agents model helps continual learning across simple tasks well within single task allaying necessity experience replay database indicating incorporation different timescales plasticity correspondingly result improved behavioural memory distinct timescales. furthermore achieved even though process synaptic consolidation prior knowledge timing changes data distribution. importantly model abstracts away causes synaptic modiﬁcations amenable testing different learning settings. original paper model shown extend lifetimes random uncorrelated memories perceptron hopﬁeld network work test capacity model mitigate behavioural forgetting realistic tasks synaptic updates unlikely uncorrelated. paper make synaptic model originally derived maximise expected signal noise ratio memories time population synapses undergoing continual plasticity form random uncorrelated modiﬁcations model assumes synaptic weight time determined history modiﬁcations time ﬁltered kernel constraining variance synaptic weights ﬁnite expected area time curve given memory typically maximised i.e. kernel decays power law. implementing model directly impractical unrealistic since would require recording time size every synaptic modiﬁcation; however authors show power decay closely approximated synaptic model consisting ﬁnite chain communicating dynamic variables dynamics variable chain determined interaction neighbours chain experiments paper conducted paradigm. setting formalised markov decision process deﬁned tuple whereby time step agent observes state takes action resulting reward transition next state probability goal agent policy deﬁned probability distribution actions given state maximises expected discounted future rewards corresponds continuous form dwext updates leak term constructed setting synaptic weight read value variables hidden effect regularising value weight history modiﬁcations. mechanical perspective draw comparison dynamics chain variables liquid ﬂowing series beakers different base areas connected tubes widths gk−k gkk+ value variable corresponds level liquid beaker given ﬁnite number beakers synapse best approximation power decay achieved exponentially increasing base areas beakers exponentially decreasing tube widths move chain gkk+ −k−. beakers wide bases connected smaller tubes necessarily evolve longer timescales. biological perspective dynamic variables likened reversible changing target values. experience replay database records agent’s experiences fifo queue sampled random training. consecutive experiences usually highly correlated another thus training online fashion cause network overﬁt recent data; jumbling together data database thus plays essential role decorrelating updates network preventing catastrophic forgetting older experiences. experiments described later show equipping parameters network bennafusi model attenuate need experience replay database better retention older memories. constant controls balance reward entropy maximisation. cost function similar except value function calculated soft-max rather hard-max q-values beneﬁt soft q-learning generate robust policy encourages agent learn multiple solutions task. experiments used soft q-learning objective found helped stabilise performance time. overarching goal experiments test whether applying benna-fusi model agent’s parameters could enhance ability learn continually setting. demonstrate potential model enabling continual learning reason tested relatively simple settings catastrophic forgetting nevertheless still issue. common policy used training used paper \u0001-greedy whereby probability agent chooses action highest q-value probability chooses action uniformly random. paper variant q-learning called ‘naive’ speed convergence q-learning involves maintaining eligibility trace state-action pair. time step eligibility traces updated follows high-dimensional continuous state spaces infeasible maintain table q-values state-action pairs; order learn good policy agent must able experience generalise previously unseen situations. deep networks artiﬁcial neural networks trained approximate mapping states q-values optimising following cost function e)∼d principle means gaining intuition mechanics model visualisation. subsequently tested deep agent evaluate effect agent’s ability learn continually across simple tasks also within single task. ﬁrst experiments conducted order test whether applying benna-fusi model tabular q-values could used facilitate continual reinforcement learning simple grid-world setting. environment consisted states organised two-dimensional grid agent equipped actions deterministically move agent vertically horizontally adjacent state last pick-up action must chosen collect reward correct location. agent trained alternately different tasks; ﬁrst reward located upper right-hand corner grid second bottom left-hand corner. episode terminated agent reached goal state successfully picked reward took maximum number steps without reaching goal. order test agent’s ability learn continually goal location switched every episodes time taken agent relearn capture reward measured. benna-fusi agent also trained naive tabular q-values modelled benna-fusi synapses chain interacting dynamic variables. given state-action pair denote ﬁrst variable chain corresponds equation ‘visible’ q-value determines agent’s policy time. q-learning updates ηδet correspond modiﬁcations. deeper variables chain thought ‘hidden’ q-values ‘remember’ visible q-value function longer timescales regularise history. modiﬁed benna-fusi agent whereby every time step variables chain scaled multiple eligibility trace shallow variables deeper variables chain thought process consolidation synapse case q-value. rationale modulating eligibility trace makes sense consolidate parameters actually used modiﬁed; example state visited long time become increasingly sure q-values benna-fusi chain length determine shortest longest memory timescales hidden variables respectively. experiments correspond roughly minimum number q-learning updates epoch number variables chain initialised odes numerically integrated every q-learning update time step table parameters used simulation shown table benna-fusi agents learned switch good policies task signiﬁcantly faster control agent modiﬁed benna-fusi agent quickest relocate reward ﬁrst time beginning epoch agents learned perform task ﬁrst epoch takes long time reward location switched opposite corner beginning second epoch since policies initially tuned move actively away reward. subsequent reward switches however control agent continues take long time relearn good policy negative transfer tasks benna-fusi agents learn re-attain good level performance task much faster initial task-switch order visualise role hidden variables benna-fusi model enabling continual learning deﬁne maxa qk); simply corresponds traditional value function interpret ‘hidden’ value function records value function longer timescales. figure snapshot values training depicted. plot indicates reward currently upper right hand corner grid location since value function almost monotonically slopes upwards point; hand evolve slower timescales hidden value functions also slope towards lower left hand corner indicating still ‘remember’ reward location. reward switches back downward pressure q-learning updates values lower left hand corner ease memory high values area grid hidden value corresponding q-values pole attached ends cart onedimensional track; agent move cart left right receives reward long pole kept roughly upright. catcher fruit falls random locations screen agent receives reward catches fruit paddle move along bottom. similarly tabular q-learning experiments agent trained alternately tasks measure ability learn continually time taken agent learn task every switch recorded. task deemed learnt moving average reward episode moved predetermined level tasks four-dimensional continuous state space discrete two-dimensional action space making suitable training network. experiments types agent control agent benna-fusi agent. order ensure difference performance agents differences effective learning rate control agent several different learning rates. benna-fusi agent control agent essentially fully connected hidden layers relus respectively number modiﬁcations made order give good chance possible learn continually. rather standard q-learning objective network trained soft q-learning found helped stabilise learning task presumably maintaining diverse experiences replay database. furthermore network weights shared tasks layer network allowed utilise task-speciﬁc gains biases computations layer form figure long took agent relearn navigate ﬁrst reward beginning epoch. many time steps took -episode moving average episode lengths drop measure long took learn good policy. mean runs s.d. error bars. next experiments test could observe similar beneﬁts continual learning benna-fusi model applied parameters deep agent alternately performing simple tasks. better memory retention tabular q-values direct impact agent’s ability recall previous policy less obvious longer memory lifetimes individual synapses yield better behavioural memory distributed system deep q-network. figure surface plots snapshot visible hidden values state training. appears retain information current reward still remember value reward location switched back deeper variables chain back make easier agent recall previous reward location. supplementary video animation values training every episode. crucially database cleared every epoch order ensure agent training task time. agent \u0001-greedy respect stochastic soft q-learning policy decayed almost course epoch. finally ‘soft’ target network updates used rather hard periodic updates used original dqn. full table parameters used seen table benna-fusi agent identical control agent except network parameter modelled bennafusi synapse variables ensuring longest timescale comfortably exceeded total number updates training tabular q-learning experiments variables initialised over-consolidated weights could learn experiments; instead initial values sampled normal distributions variances decaying linearly depth chain approximately matches equilibrium distribution shown random uncorrelated memories original paper order speed computation rather simulate time steps benna-fusi odes every replay batch approximated conducting integration update task control agents thus demonstrating better ability continual learning. interestingly control agents able learn cart-pole beginning training subsequent training catcher left network starting point made hard impossible agents relearn cart-pole exhibiting severe case catastrophic forgetting. benna-fusi agent display behaviour instead relearned task quickly epochs. important note parameters chosen control agents capable learning good policy either task trained scratch. catcher benna-fusi agent took longer converge good performance ﬁrst epochs training subsequently became faster control agents recalling perform task. continual learning problem normally posed challenge learning perform series well-deﬁned tasks sequence; however issue nonstationary data often occurs within training task. effect occurs primarily strong correlation time consecutive states changes agent’s policy altering distribution experiences. common deal problem experience replay database decorrelate data without agent struggle learn stable policy policy cart-pole online setting catcher could training data nonstationary thus agents prone catastrophic forgetting learn. common aspect among control tasks cartpole successful policy often involves restricting experiences small part state space example cart-pole keep pole upright agent trains good policy begin overwrite knowledge q-values states pole signiﬁcantly tilted. since agent constantly learning could point make update causes make wrong action causes pole tilt angle experienced while. point agent might perform poorly since forgotten correct policy region state space policy might destabilised training ‘new’ experiences. furthermore stage exploration rate might decayed level making harder relearn. idea decay practice found solve problem actually make learning less stable could agent still overﬁts states experienced good policy extra exploration serves perturb negative spiral described faster otherwise noted control tasks policy often needs ﬁne-tuned unstable region state space; requires high-frequency sampling good policy makes excessive exploration undesirable cart-pole benna-fusi agent succeeds honing performance recent experiences good policy simultaneously remaining robust perturbations maintaining memory suboptimal situations experienced while. catcher good policy still visit large part state space consecutive states also less correlated time since fruit falls random locations screen. explain control agent problem learning task successfully. concept synaptic consolidation applied number recent works tackle continual learning problem adding quadratic terms cost function selectively penalise moving parameters according important recall previous tasks importance parameter task proportional term diagonal fisher information matrix training. importance factor parameter calculated online fashion determining figure long took agents relearn task beginning epoch; training episodes needed test-episode moving average reward surpass threshold plotted runs agent. runs relearn within marked reward episode averaged epoch task; means s.d. error bars runs. control benna-fusi agents trained cart-pole catcher separately online setting experience replay database agents trained every time step recent experience. architectures control benna-fusi agents previous experiments couple differences network smaller benna-fusi agent larger value order able remember experiences shorter timescales. none control agents able learn maintain consistently good policy cart-pole task benna-fusi agent learned perform task perfection cases catcher however agents able learn consistently good policy control agent learning faster adam soft target updates also effectively remember parameter values longer timescales memory declines exponentially i.e. much faster power decay benna-fusi model. approaches continual learning problem include networks grow incrementally learn skills implicitly training multiple models network building generative models mimic datasets longer available orthogonal approach used paper could combined paper took inspiration computational model biological synapses show expressing parameter tabular deep agent dynamical system interacting variables rather scalar value help mitigate catastrophic forgetting multiple timescales. work intended proof concept envisage extended several ways. first important test model’s capabilities challenging setting increasing number complexity tasks potentially using different architectures state-of-the-art actorcritic models well model facilitate transfer learning series related tasks. furthermore sensitivity continual learning performance parameters model number hidden variables analysed order optimise investigation information content held different depths chain could yield effective readout schemes value weight. finally would interesting adapt model light fact synaptic consolidation known regulated neuromodulators dopamine which example associated reward prediction error exposure novel stimuli could modulate hidden variables model factors these importance factors cited previous section order consolidate memory selectively efﬁciently. figure test-episode moving average reward cartpole benna-fusi agent control agents different learning rates; means s.d. errorbars runs agent. contribution drop loss function training; contrast uses local approximation importance method provides global interpretation considering parameter’s impact whole learning trajectory. importance made proportional derivative lnorm network output respect parameter; relying loss function consolidation method ﬂexibly used constrain model different data trained benna-fusi model also constrains parameters close previous values contrast approaches described above consolidation occurs range timescales without derived importance factors without knowledge task boundaries. characteristics useful situations prior knowledge timescale training data change possibly realistic assumption robots deployed learn real world. furthermore importance factors derived works could feasibly used modulate hidden variables combining approaches. must noted idea modelling plasticity different timescales mitigate catastrophic forgetting neural network weight split separate ‘fast’ ‘slow’ components allows network retrieve memories quickly training data. however model tested simple setting matching random binary inputs outputs shown allowing different components interact theoretically yields much longer memory lifetimes keeping separate. momentum variables bliss lømo long-lasting potentiation synaptic transmission dentate area anaesthetized rabbit following stimulation perforant path. journal physiology clopath ziegler vasilaki b¨using gerstner tag-trigger-consolidation model early late long-term-potentiation depression. plos computational biology bruin kober tuyls babuˇska importance experience replay database composition deep reinforcement learning. deep reinforcement learning workshop advances neural information processing systems bruin kober tuyls babuˇska offpolicy experience retention deep actor-critic learning. deep reinforcement learning workshop advances neural information processing systems fernando banarse blundell zwols rusu pritzel wierstra pathnet evolution channels gradient descent super neural networks. arxiv preprint arxiv. figure test-episode moving average reward episode catcher benna-fusi agent best control agent. control agent learns faster learning good policy. figure test-episode moving average reward cartpole control agents different sized experience replay databases benna-fusi agent online setting. experiments experience sampled training database every time step. control cases database small agent attain stable performance task benna-fusi agent can. figure test-episode moving average reward episode cart-pole control agents epsilon allowed decay different minimum values. none runs yielded good stable performance. parameter epochs episodes/epoch time steps episode cart-pole catcher initial \u0001-decay episode minimum neuron type width hidden layer width hidden layer optimiser learning rate adam adam experience replay size replay batch size* soft target update soft q-learning benna-fusi variables benna-fusi test frequency", "year": 2018}