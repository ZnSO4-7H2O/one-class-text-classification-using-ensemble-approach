{"title": "A Unified Approach to Interpreting Model Predictions", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.", "text": "understanding model makes certain prediction crucial prediction’s accuracy many applications. however highest accuracy large modern datasets often achieved complex models even experts struggle interpret ensemble deep learning models creating tension accuracy interpretability. response various methods recently proposed help users interpret predictions complex models often unclear methods related method preferable another. address problem present uniﬁed framework interpreting predictions shap shap assigns feature importance value particular prediction. novel components include identiﬁcation class additive feature importance measures theoretical results showing unique solution class desirable properties. class uniﬁes existing methods notable several recent methods class lack proposed desirable properties. based insights uniﬁcation present methods show improved computational performance and/or better consistency human intuition previous approaches. ability correctly interpret prediction model’s output extremely important. engenders appropriate user trust provides insight model improved supports understanding process modeled. applications simple models often preferred ease interpretation even less accurate complex ones. however growing availability data increased beneﬁts using complex models bringing forefront trade-off accuracy interpretability model’s output. wide variety different methods recently proposed address issue understanding methods relate method preferable another still lacking. here present novel uniﬁed approach interpreting model predictions. approach leads three potentially surprising results bring clarity growing space methods introduce perspective viewing explanation model’s prediction model itself term explanation model. lets deﬁne class additive feature attribution methods uniﬁes current methods. show game theory results guaranteeing unique solution apply entire class additive feature attribution methods propose shap values uniﬁed measure feature importance various methods approximate propose shap value estimation methods demonstrate better aligned human intuition measured user studies effectually discriminate among model output classes several existing methods best explanation simple model model itself; perfectly represents easy understand. complex models ensemble methods deep networks cannot original model best explanation easy understand. instead must simpler explanation model deﬁne interpretable approximation original model. show current explanation methods literature explanation model. previously unappreciated unity interesting implications describe later sections. original prediction model explained explanation model. here focus local methods designed explain prediction based single input proposed lime explanation models often simpliﬁed inputs original inputs mapping function hx). local methods ensure whenever even though contain less information speciﬁc current input methods explanation models matching deﬁnition attribute effect feature summing effects feature attributions approximates output original model. many current methods match deﬁnition several discussed below. lime method interprets individual model predictions based locally approximating model around given prediction local linear explanation model lime uses adheres equation exactly thus additive feature attribution method. lime refers simpliﬁed inputs interpretable inputs mapping converts binary vector interpretable inputs original input space. different types mappings used different input spaces. words text features converts vector original word count simpliﬁed input zero simpliﬁed input zero. images treats image super pixels; maps leaving super pixel original value replacing super pixel average neighboring pixels lime minimizes following objective function faithfulness explanation model original model enforced loss samples simpliﬁed input space weighted local kernel penalizes complexity since lime follows equation squared loss equation solved using penalized linear regression. deeplift recently proposed recursive prediction explanation method deep learning attributes input value c∆xi∆y represents effect input reference value opposed original value. means deeplift mapping converts binary values original inputs indicates input takes original value indicates takes reference value. reference value though chosen user represents typical uninformative background value feature. deeplift uses \"summation-to-delta\" property states layer-wise relevance propagation method interprets predictions deep networks noted shrikumar menthod equivalent deeplift reference activations neurons ﬁxed zero. thus converts binary values original input space means input takes original value means input takes value. layer-wise relevance propagation’s explanation model like deeplift’s matches equation three previous methods classic equations cooperative game theory compute explanations model predictions shapley regression values shapley sampling values quantitative input inﬂuence shapley regression values feature importances linear models presence multicollinearity. method requires retraining model feature subsets features. assigns importance value feature represents effect model prediction including feature. compute effect model fs∪{i} trained feature present another model trained feature withheld. then predictions models compared current input fs∪{i} represents values input features since effect withholding feature depends features model preceding differences computed possible subsets {i}. shapley values computed used feature attributions. weighted average possible differences shapley regression values maps original input space indicates input included model indicates exclusion model. shapley regression values match equation hence additive feature attribution method. shapley sampling values meant explain model applying sampling approximations equation approximating effect removing variable model integrating samples training dataset. eliminates need retrain model allows fewer differences computed. since explanation model form shapley sampling values shapley regression values also additive feature attribution method. quantitative input inﬂuence broader framework addresses feature attributions. however part method independently proposes sampling approximation shapley values nearly identical shapley sampling values. thus another additive feature attribution method. surprising attribute class additive feature attribution methods presence single unique solution class three desirable properties properties familiar classical shapley value estimation methods previously unknown additive feature attribution methods. ﬁrst desirable property local accuracy. approximating original model speciﬁc input local accuracy requires explanation model least match output simpliﬁed input second property missingness. simpliﬁed inputs represent feature presence missingness requires features missing original input impact. methods described section obey missingness property. third property consistency. consistency states model changes simpliﬁed input’s contribution increases stays regardless inputs input’s attribution decrease. property denote setting models theorem follows combined cooperative game theory results values known shapley values young demonstrated shapley values values satisfy three axioms similar property property ﬁnal property show redundant setting property required adapt shapley proofs class additive feature attribution methods. properties given simpliﬁed input mapping theorem shows possible additive feature attribution method. result implies methods based shapley values violate local accuracy and/or consistency following section proposes uniﬁed approach improves previous methods preventing unintentionally violating properties propose shap values uniﬁed measure feature importance. shapley values conditional expectation function original model; thus solution equation figure shap values attribute feature change expected model prediction conditioning feature. explain base value would predicted know features current output diagram shows single ordering. model non-linear input features independent however order features added expectation matters shap values arise averaging values across possible orderings. non-zero indexes based sections shap values provide unique additive feature importance measure adheres properties uses conditional expectations deﬁne simpliﬁed inputs. implicit deﬁnition shap values simpliﬁed input mapping missing values features since models cannot handle arbitrary patterns missing input values approximate zs]. deﬁnition shap values designed closely align shapley regression shapley sampling quantitative input inﬂuence feature attributions also allowing connections lime deeplift layer-wise relevance propagation. exact computation shap values challenging. however combining insights current additive feature attribution methods approximate them. describe model-agnostic approximation methods already known another novel also describe four model-type-speciﬁc approximation methods novel using methods feature independence model linearity optional assumptions simplifying computation expected values assume feature independence approximating conditional expectations shap values estimated directly using shapley sampling values method equivalently quantitative input inﬂuence method methods sampling approximation permutation version classic shapley value equations separate sampling estimates performed feature attribution. reasonable compute small number inputs kernel shap method described next requires fewer evaluations original model obtain similar approximation accuracy linear lime uses linear explanation model locally approximate local measured simpliﬁed binary input space. ﬁrst glance regression formulation lime equation seems different classical shapley value formulation equation however since linear lime additive feature attribution method know shapley values possible solution equation satisﬁes properties local accuracy missingness consistency. natural question pose whether solution equation recovers values. answer depends choice loss function weighting kernel regularization term lime choices parameters made heuristically; using choices equation recover shapley values. consequence local accuracy and/or consistency violated turn leads unintuitive behavior certain circumstances eliminating variables using constraints. since theorem assumed follow linear form squared loss equation still solved using linear regression. consequence shapley values game theory computed using weighted linear regression. since lime uses simpliﬁed input mapping equivalent approximation shap mapping given equation enables regression-based model-agnostic estimation shap values. jointly estimating shap values using regression provides better sample efﬁciency direct classical shapley equations intuitive connection linear regression shapley values equation difference means. since mean also best least squares point estimate data points natural search weighting kernel causes linear least squares regression recapitulate shapley values. leads kernel distinctly differs previous heuristically chosen kernels kernel shap improves sample efﬁciency model-agnostic estimations shap values restricting attention speciﬁc model types develop faster model-speciﬁc approximation methods. figure shapley kernel weighting symmetric possible vectors ordered cardinality vectors example. distinctly different previous heuristically chosen kernels. compositional models deep neural networks comprised many simple components. given analytic solutions shapley values components fast approximations full model made using deeplift’s style back-propagation. using permutation formulation shapley values calculate probability input increase maximum value every input. sorted order input values lets compute shapley values function inputs time instead supplementary material full algorithm. kernel shap used model including deep models natural whether leverage extra knowledge compositional nature deep networks improve computational performance. answer question previously unappreciated connection shapley values deeplift interpret reference value equation representing equation deeplift approximates shap values assuming input features independent another deep model linear. deeplift uses linear composition rule equivalent linearizing non-linear components neural network. back-propagation rules deﬁning component linearized intuitive heuristically chosen. since deeplift additive feature attribution method satisﬁes local accuracy missingness know shapley values represent attribution values satisfy consistency. motivates adapting deeplift become compositional approximation shap values leading deep shap. deep shap combines shap values computed smaller components network shap values whole network. recursively passing deeplift’s multipliers deﬁned terms shap values backwards network figure since shap values simple network components efﬁciently solved analytically linear pooling activation function input composition rule enables fast approximation values whole model. deep shap avoids need heuristically choose ways linearize components. instead derives effective linearization shap values computed component. function offers example leads improved attributions figure comparison three additive feature attribution methods kernel shap shapley sampling values lime feature importance estimates shown feature models number evaluations original model function increases. percentiles shown replicate estimates sample size. decision tree model using input features explained single input. decision tree using input features explained single input. evaluated beneﬁts shap values using kernel shap deep shap approximation methods. first compared computational efﬁciency accuracy kernel shap lime shapley sampling values. second designed user studies compare shap values alternative feature importance allocations represented deeplift lime. might expected shap values prove consistent human intuition methods fail meet properties finally mnist digit image classiﬁcation compare shap deeplift lime. theorem connects shapley values game theory weighted linear regression. kernal shap uses connection compute feature importance. leads accurate estimates fewer evaluations original model previous sampling-based estimates equation particularly regularization added linear model comparing shapley sampling shap lime dense sparse decision tree models illustrates improved sample efﬁciency kernel shap values lime differ signiﬁcantly shap values satisfy local accuracy consistency. theorem provides strong incentive additive feature attribution methods shap values. lime deeplift originally demonstrated compute different feature importance values. validate importance theorem compared explanations lime deeplift shap user explanations simple models testing assumes good model explanations consistent explanations humans understand model. compared lime deeplift shap human explanations settings. ﬁrst setting used sickness score higher symptoms present second used allocation problem deeplift applied. participants told short story three made money based maximum score achieved cases participants asked assign credit output among inputs found much stronger agreement human explanations shap methods. shap’s improved performance functions addresses open problem pooling functions deeplift figure human feature impact estimates shown common explanation given among random individuals respectively. feature attributions model output value model output fever cough present fever cough present otherwise. attributions proﬁt among three given according maximum number questions right. ﬁrst questions right second questions third none right proﬁt figure explaining output convolutional network trained mnist digit dataset. orig. deeplift explicit shapley approximations deeplift seeks better approximate shapley values. areas increase probability class blue areas decrease probability. masked removes pixels order change odds masking random images supports better estimates shap values. includes updates better match shapley values figure extends deeplift’s convolutional network example highlight increased performance estimates closer shap values. pre-trained model figure example used inputs normalized convolution layers dense layers followed -way softmax output layer. deeplift versions explain normalized version linear layer shap lime explain model’s output. shap lime samples improve performance lime modiﬁed single pixel segmentation digit pixels. match masked pixels chosen switch predicted class according feature attribution given method. growing tension accuracy interpretability model predictions motivated development methods help users interpret predictions. shap framework identiﬁes class additive feature importance methods shows unique solution class adheres desirable properties. thread unity shap weaves literature encouraging sign common principles model interpretation inform development future methods. presented several different estimation methods shap values along proofs experiments showing values desirable. promising next steps involve developing faster model-type-speciﬁc estimation methods make fewer assumptions integrating work estimating interaction effects game theory deﬁning explanation model classes. work supported national science foundation dbi- career dbi- american cancer society -rsg----tbg national institute health graduate research fellowship. would like thank marco ribeiro erik štrumbelj avanti shrikumar yair zick nips reviewers feedback signiﬁcantly improved work.", "year": 2017}