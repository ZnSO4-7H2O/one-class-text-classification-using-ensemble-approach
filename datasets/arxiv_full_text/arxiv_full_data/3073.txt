{"title": "Bridging Cognitive Programs and Machine Learning", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "While great advances are made in pattern recognition and machine learning, the successes of such fields remain restricted to narrow applications and seem to break down when training data is scarce, a shift in domain occurs, or when intelligent reasoning is required for rapid adaptation to new environments. In this work, we list several of the shortcomings of modern machine-learning solutions, specifically in the contexts of computer vision and in reinforcement learning and suggest directions to explore in order to try to ameliorate these weaknesses.", "text": "computer vision) intelligent agents. purpose work bridge worlds machine learning modeling human beings solve visual tasks. speciﬁcally providing general enough solution problem coming cognitive programs enable solving visual tasks given speciﬁcation. constraining machine learning methods used solve tasks using known biological vision beneﬁt models done right improve performance perhaps allow gain insights. next sections attempt brieﬂy overview star model well recent trends machine learning. remainder report shall show best worlds star machine learning brought together create working model agent able perform various visual tasks. iii. selective tuning cognitive programs selective tuning theoretical model explain predict behavior human visual system performing task visual input. speciﬁcally focuses phenomena visual attention includes overt attention covert attention neural modulation feedback facilitates processes. model derived ﬁrst principles involve analysis computational complexity general vision tasks well biological constraints known experimental observation human subjects. following constraints aims biologically plausible ensuring runtime practical solve various vision tasks. extended star model include capacity cognitive programs. describe main components star. description draw high-level picture means complete. reader interested delving details please refer theoretical justiﬁcations broad discussion read great advances made pattern recognition machine learning successes ﬁelds remain restricted narrow applications seem break training data scarce shift domain occurs intelligent reasoning required rapid adaptation environments. work list several shortcomings modern machine-learning solutions speciﬁcally contexts computer vision reinforcement learning suggest directions explore order ameliorate weaknesses. selective tuning attentive reference model attention theoretical computational model designed reproduce predict characteristics human visual system observing image video possibly task hand. based psycho-physical observations constraints amount nature computations carried human brain. model contains multiple sub-modules visual hierarchy visual working memory ﬁxation controller other. model describes data diﬀerent components aﬀect other. model given various tasks executive controller orchestrates action diﬀerent modules. viewed general purpose processor able reason task hand formulate called cognitive programs cognitive programs made language describing steps required control visual system obtain required information track sequence observations desired goal achieved. recent years methods pattern recognition taken large step forward terms performance. visual recognition thousands object classes well detection segmentation made much reliable past. related ﬁeld artiﬁcial intelligence progress made marriage reinforcement learning deep learning allowing agents successfully play multitude game solve complex environments without need manually crafting feature spaces adding prior knowledge speciﬁc task. much progress still made mentioned models namely description components. model described extended concept cognitive programs allows controller break visual tasks sequence actions designed solve them. fig. describes information star architecture high-level. central architecture visual hierarchy. meant represent ventral dorsal streams processing brain implemented neural network feedforward recurrent connections. structure designed allow recurrent localization input stimuli well discrimination categorization identiﬁcation. single feed-forward pass suﬃce tasks other visual search multiple forward-backward passes required. tuning allowed perform better speciﬁc tasks. recurrent tracing neuron activation along hierarchy performed using θ-wta decision process. induces attentional sample represents neurons whose response matches currently attended stimulus. fixation control mechanism main components. peripheral priority represents saliency peripheral visual ﬁeld. history biased priority combines focus attention derived central visual ﬁeld foci attention derived peripheral visual ﬁeld together produce based previous ﬁxations setting priority next gaze. perform task visual hierarchy fixation controller need controlled process receives task breaks sequence methods basic procedures commonly used across wide range visual tasks. method applied degree tuning match speciﬁc task hand whereas become executable script. functional sub-modules required execution cp’s. controller orchestrating execution tasks called visual task executive given task selects appropriate methods tunes scripts controls execution scripts using several sub-modules. script initiates attentive cycle sends element task required attentive tuning visual attention executive primes visual hierarchy top-down signals reﬂecting expectations stimulus instructions sets required parameters. meanwhile current attention disengaged feature surround suppression imposed previous stimuli lifted. completed feedforward signal enters tuned feed-forward pass completed θ-wta process selects makes decision attend passes choice next stage. monitoring execution scripts decide based information whether task completed not. visual working memory contains representations fixation history stores last several ﬁxation locations decaying time. allows location based inhibition return second representation blackboard store current attentional sample task working memory includes active script notepad might several compartments. compartment would store active scripts pointers indicate progress along sequence. another might store information relevant script progress including sequence attentional samples ﬁxation changes occur process fulﬁlling task. another might store relevant world knowledge might used executing active script notepad would provide information required monitor task progress take corrective actions task progress unsatisfactory. finally visual attention executive contains cycle controller responsible starting terminating stage process. also initiates monitors recurrent localization process detailed view entire architecure seen selective tuning cognitive programs framework allows rich visual tasks solved given correct sequence methods performed. recent realization cognitive programs challenging environments presented agent able successfully play video games using methods control tune visual hierarchy decide next move player. nevertheless open questions remain design structure parameters given proper task-biasing priming deal broad range visual inputs? learn type tuning applied given task create visual task executive able appropriately select methods accomplish visual task. seems main questions posed control planning solution given tools well ﬁtting models complex data natural proceed recent trends machine learning facilitate solution problems. following descriptions meant in-depth descriptions respective methods section provide overview main methods machine learning relevant require intelligent agent observe world perform various given complex tasks. seems like broad subject certainly fully solved notable progress made recent years machine learning pattern recognition. short exposition mention main methods interest deem relevant current goal work. deep learning certainly ﬁeld roots back various reasons scope work always popular today certainly still claim current hype around exaggerated. turning point responsible current surge popularity paper imagenet largescale visual recognition challenge. massive benchmark computer-vision methods classiﬁer required predict class object image possible diﬀerent classes. signiﬁcantly outperforming results work spurred avalanche followups modiﬁcations optimization point view diﬀerent architectures well theoretical works attempting justify success methods others. date rare leading method computer vision based deep learning sub-tasks object recognition detection segmentation tracking d-reconstruction face recognition ﬁne-grained categorization others. speciﬁcally deep convolutional neural networks certain form neural nets exploits assumptions structure natural images main class deep networks. success deep learning also spread media audio natural language processing subﬁelds involving pharmaceutical medical applications etc. literature recent years deep learning vast reader encouraged turn crux various deep-learning based methods need massive amounts supervised data. obtain good performance tens thousands example required. semisupervised methods suggested approached performance fully supervised ones. utility discarded contrary believe play major role developments near future. additional issue lies current seemingly inherent inability adjust kinds data apply compositions already learned solutions problems weakenesses deep learning systems discussed well discussion major diﬀerences humans machines solve problems semi-supervised unsupervised learning variant machine learning potentially holds promise ameliorate need supervision scale required methods deep learning. methods attempt perform learning receiving much smaller amount supervision. example learning distinguish data classes learning datasets labeled rest not. done exploiting observed similarities underlying data assuming regularities smoothness etc. extreme case would using labeled data however point task system learn supervised manner utility unsupervised learning measured ﬁnding beneﬁts supervised learner. another form unsupervised learning generative models able produce test time data points whose properties ideally resemble observed training time though course identical them. example semisupervised learning ladder-networks unsupervised loss added network addition supervised loss. notable method recently gained popularity generative adversarial networks networks constantly compete goal generator network generate images realistic possible sense resemble images training discriminator network whose goal tell apart images generator images real dataset. quickly evolved produce impressive results recent fig. results. reinforcement learning refers classical well studied methods ﬁeld control systems artiﬁcial intelligence. general setting agent supposed take actions given environment. result agent encounter situations given reward actions agent aﬀect environment. agent necessarily entire environment times rather access input current observation. loop act-observereceive reward agent must increase total future reward. setting general sense limited richness environment agent. extreme example environment planet earth agent human animal. simulating either seems like virtual impossibility model robots closed well deﬁned environments anything between. much research gone making agents learn able perform well various environments well making robust control systems. also ﬁelds beneﬁt regrow popularity following success deep learning leading method called deep reinforcement learning. ﬁrst widely known success method published agent shown able learn perform well multiple atari video games outperforming many previous methods. notably system learned end-to-end without input except pixel data score game. cases even learned outperform human players. reported performance result using good policy would maximize discounted future reward. value-action function deﬁned function assigns maximum discounted future reward action performed state meaning function holds generate optimal policy. discrete number states actions simple method known value iteration know converge optimal policy given state/action pair visited inﬁnite number times. simply implemented continuously updating stopping criteria q−q]. methods work well ﬁnite number states actions. however many interesting environments challenging deﬁne states discrete naively would result exponential number. examples task play video game available actions form small enumerating number possible stimuli would easily lead intractable numbers possible combinations pixel values screen. mind turn deep reinforcement learning. here instead representing state explicitly symbol large neural network learned predict q-value state applied directly input frame hence state represented implicitly network’s weights structure. allows agent learn environment without privileged knowledge speciﬁc inner workings. many variants improvements idea though basic setting remains same. follows highlight challenges shortcomings current approaches deep eﬃcient exploration challenge currently holding back methods huge exploration space potentially sought order produce good policy. chicken problem sorts exploration needed good policy good policy required able suﬃcient exploration; consider even simple game atari breakout player able move paddle left right single architecture hyper-parameters. although many game method performed poorly time signiﬁcant results lead others beating human expert game widely acknowledged long standing challenge artiﬁcial intelligence community. recent overview subject please refer formally assumes following setting agent interact environment time applying action given observation note entire state environment observed context state represents observation agent directly measure. interaction agent leads another state agent perform another action markov decision process deﬁned environment probability next state fully determined current state action note markov property i.e. state dependent previous ones that. example game chess entire board observed state nothing needs known previous steps game determine next move. action agent receives reward real scalar take value positive negative zero. hence entire sequence actions agent environment ball doesn’t fall bottom screen. nothing else known would take amount exploration paddle bounce ball avoid losing. that agent probably start moving randomly left right. rewards game sparse agent encounters ﬁrst reward able update policy. reason take many iterations start learning avoid losing quickly. continue explore states game could even reached passed ﬁrst steps hitting ball. \u0001greedy strategy somewhat improves problem choosing random move probability hyper-parameter usually decayed time system learns. helps getting local minima exploration space though general problem described certainly solved. exploration representation problem exploration exacerbated case deep discrete search space state well recognized encountered. state space represented implicitly deep network evolution function tied representation environment deep network. means updating function lead unstable results. strategy address reducing frequency network chooses next action updated suggest couple additional strategies strong visual representation visual system human beings strong able represent stimuli robustly owing evolution learning prior experience. agent usually learns visual representation environment scratch. certainly able robustly represent observations right start would allow agent focus planning less learning representation. nevertheless representation continue evolving agent encounters situations. allow starting point pre-trained visual representation supervised unsupervised manner adapt needed task. symbolic representation allowing agent group observations equivalence classes assigning symbols compact representations would allow policies learned eﬃciently probably converge higher level performance. done implicit manner attempting cluster representation environment informative clusters carry maximal information respect task. explicitly observation somehow parsed objects background possibly agents etc. representation scene made properties constituents scene. latter would probably carry meaning prior external knowledge child able learn play game reasonably well within minutes current methods require many millions frames succeed all. besides reasons stated above claim additional forms prior experience useful. form experience solved tasks past related current task. indeed recently shown eﬀective single network learns mimic behaviour multiple expert networks pre-trained single tasks. thus network represents simultaneously knowledge solve learned tasks relatively compact manner. cases network shown learn tasks much faster randomly initialzied version well converge stable manner. world knowledge also plays major role understanding situation. factual knowledge gain experience written list many diﬀerent facts rules would probably make long one. examples intuitive understanding newtonian physics even children understand object tend continue general direction tend fall going move pushed external force etc. relations interactions objects doors diﬃcult imagine learned stored brains relevant facts come play abundance diﬀerent situations encounter. able eﬀectively utilize vast knowledge-base behavior world would doubt intelligent agents many environments. attempts using external knowledge tasks already made computer vision image captioning visual question answering zero-shot learning general gain knowledge unseen objects categories comparing detected attributes known ones world-knowledge collected either data-driven approches wordvec word relation graphs datasets collected manually scanning online knowledge collections wikipedia conceptnet collections linguistic factual knowledge certainly help agent quickly reason surrounding environment able link observations items knowledge base. interesting person acquires knowledge ﬁrst years his/her lifetime experience quite diﬀerent simply explosed millions online articles. somehow collection useful facts rules picked experience despite drowned pool distracting noisy signals. recent work demonstrated prior knowledge quite critical success humans simple games. work devises ways remove semantics gameplay replacing graphical elements game semantically meaningless ones. example switch piece texture game random makes game screen appear meaningless human observer. performance humans modiﬁed games dropped signiﬁcanlty tested machine-learning based method remained same. another type modiﬁcation switching elements elements diﬀerent meaning. example replacing appearance ladder climbed column ﬂames transposing screen gravity appears work sideways. though one-to-one translation original modiﬁed version game human players much worse semantically modiﬁed examples others. demonstrates heavy reliance humans prior knowledge. context learning game scratch without prior knowledge unfair machinelearning methods. nevertheless knowledge bases still account intuitive physical understanding seems require type experience. knowledge either pre-injected agent children come equipped knowledge believe agent learn rules physical interactions experience observations. interesting attempt direction seen robots gain reportedly intuitive understanding physical interactions attempting perform simple tasks objects moving around. high level reasoning control planning performed several levels granularity. certainly human animal think terms force needs applied muscels order pick object. rather seems plans made higher level abstraction process breaks motor commands everything required carried out. motor commands also grouped logical units basic ones fully stretch left translated level commands. newborns able control limbs ﬁngers immediately time acquire ability perform tasks seamless movements usually dedicating little concious thought movement muscles. similarly exploration goes early life agent allow agent learn perform simple common actions store routines later used elaborate plans. end-to-end learning simplest level high-level control. advances would requires strategic thinking terms long-range goals actions. claim cannot done eﬀectively without ﬁrst obtaining hierarchy basic control agents actions able predict quite reliably immediate future eﬀect.", "year": 2018}