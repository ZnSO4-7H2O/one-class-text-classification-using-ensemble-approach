{"title": "Recursive Autoconvolution for Unsupervised Learning of Convolutional  Neural Networks", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "In visual recognition tasks, such as image classification, unsupervised learning exploits cheap unlabeled data and can help to solve these tasks more efficiently. We show that the recursive autoconvolution operator, adopted from physics, boosts existing unsupervised methods by learning more discriminative filters. We take well established convolutional neural networks and train their filters layer-wise. In addition, based on previous works we design a network which extracts more than 600k features per sample, but with the total number of trainable parameters greatly reduced by introducing shared filters in higher layers. We evaluate our networks on the MNIST, CIFAR-10, CIFAR-100 and STL-10 image classification benchmarks and report several state of the art results among other unsupervised methods.", "text": "fig. recursive autoconvolution orders applied samples mnist cifar- stl- note patterns obtained similar mid-level features cnns novelty work learn ﬁlters patterns. steadily approaching performance levels supervised models including convolutional neural networks trained backpropagation thousands labeled samples provide evidence unsupervised learning promising building efﬁcient visual representations. main contribution work adaptation recursive autoconvolution operator convolutional architectures. concretely demonstrate operator used together existing clustering methods learning methods train ﬁlters resemble ones learned cnns consequently discriminative secondly substantially reduce total number learned ﬁlters higher layers networks without loss classiﬁcation accuracy allows train larger models finally report several state results among unsupervised methods keeping computational cost relatively unsupervised learning used quite often additional regularizer form weights initialization reconstruction cost independent visual model trained still images image sequences abstract—in visual recognition tasks image classiﬁcation unsupervised learning exploits cheap unlabeled data help solve tasks efﬁciently. show recursive autoconvolution operator adopted physics boosts existing unsupervised methods learning discriminative ﬁlters. take well established convolutional neural networks train ﬁlters layer-wise. addition based previous works design network extracts features sample total number trainable parameters greatly reduced introducing shared ﬁlters higher layers. evaluate networks mnist cifar cifar- stl- image classiﬁcation benchmarks report several state results among unsupervised methods. large-scale visual tasks solved deep neural networks thousands labeled samples available training time issue. efﬁcient implementations standard computational blocks make training testing feasible. major drawback supervised neural networks heavily rely labeled data. true real applications really matter methods used achieve desired outcome. cases labeling expensive process. however visual data full abstract features unrelated object classes. unsupervised learning exploits abundant amounts cheap unlabeled data help solve tasks efﬁciently. general unsupervised learning important moving towards artiﬁcial intelligence work learn visual representation model image classiﬁcation particularly effective number labels relatively small. result model potentially successfully applied tasks label information often scarce expensive. inspired previous works network ﬁlters learned layer-wise without label information accordance works thoroughly validate models conﬁrm advantage tasks labeled training samples stl- reduced variants mnist cifar addition full variants datasets cifar- demonstrate unsupervised learning works learning ﬁlters clustering methods k-means standard approach reason make comparison results easier k-means also adopted work default method. moreover clustering methods learn overcomplete dictionaries without additional modiﬁcations done nevertheless since also common practice learn ﬁlters conduct couple simple experiments method well principal component analysis probe novel idea thoroughly. contrast various popular coding schemes forward pass built upon well established supervised method convolutional neural network recently convolutional networks successfully trained layer-wise unsupervised works well work forward pass mostly kept standard methods learn stronger ﬁlters developed. instance k-means enhanced introducing convolutional clustering. convolutional extension clustering coding methods ways reduce redundancy ﬁlters improve classiﬁcation e.g. convolutional sparse coding work suggest another concept making ﬁlters powerful namely recursive autoconvolution applied image patches unsupervised learning. autoconvolution properties seem ﬁrst analyzed physics later function optimization problem deautoconvolution arose. operator also appeared visual tasks extract invariant patterns best knowledge recursive version used pillar work ﬁrst suggested parametric description images temporal sequences. contrast operator learn convolution kernels multilayer refer autocnn. ﬁlter learning procedure discussed section iv-a largely based autoconvolution recursive extension. next sections formulate basics operators general terms. ﬁrst describe routine processing arbitrary discrete data based autoconvolution. convenient consider autoconvolution frequency domain. according convolution theorem n-dimensional discrete signals images kf◦f n-dimensional forward discrete fourier transform point-wise matrix product normalizing coefﬁcient hence autoconvolution deﬁned fig. examples ﬁlters size pixels learned using k-means mnist cifar- patches without whitening recursive autoconvolution orders case ﬁlters learned. note ﬁlters sparse higher similar ﬁlters learned ﬁrst layer autocnn networks. extract patterns necessary make sure mean computing also compute linear autoconvolution ﬁrst zero-padded i.e. dimensional case padded zeros length length padding. image patterns extracted using operator used parametric description images. novelty work extracted patterns convolution kernels i.e. ﬁlters noticed applying image patches provides sparse wavelet-like patterns usually learned ﬁrst layer fig. fig. unsupervised learning methods e.g. fig. sparse coding fig. addition patterns similar complex mid-level features cnns section describe architecture multilayer convolutional neural network train unsupervised feature extractor based classical cnns however backward pass neither ﬁlters connections trained using labels. addition based design networks features randomly split several groups typical cnns important work. since forward pass limited differentiable functions nonlinearities next according statistics extracted patches presented fig. autoconvolution order inversely proportional joint spatial frequency resolution i.e. σxσy eigenvalues weighted covariance matrix spatial domain; analogously σuv. therefore cover wider range spatio-frequency properties learn diverse ﬁlters patches extracted several orders combined global set. take results several orders patches instead global autoconvolutional patches. note case extract patches make total number input data points k-means combinations orders. global patches ﬁrst scaled values range zca-whitened even though extract gabor-like patterns without whitening preprocessing essential k-means learn less correlated ﬁlters. whitened k-means produces data points rsl×sl×dl×kl. data points ﬁrst l-normalized used convolution kernels layer grouping feature maps filters second following layers trained either ﬁrst layer using slightly modiﬁed procedure borrowed concretely features split groups. useful practice ﬁlters layer smaller depth kl/gl smaller overall dimensionality easier learn ﬁlters kmeans. time number features becomes larger factor features usually improve classiﬁcation case unsupervised learning. shared ﬁlters disadvantage splitting ﬁlters learned group independently i.e. necessary k-means times. instance second layer rs×s×d×k×g i.e. total number ﬁlters equals contribution instead treating groups independently learn ﬁlters groups taken together without negative effect classiﬁcation accuracy case patches feature groups concatenated clustering k-means once. thus total number ﬁlters layer becomes equal i.e. ﬁlters shared groups. trick enables learn large autocnn-l feasible time. evaluate method four image classiﬁcation benchmarks mnist cifar- cifar- stl- demonstrate unsupervised learning particularly effective datasets training samples training images class test method reduced versions mnist cifar- namely fig. overview best architecture cifar- filters learned unsupervised learning recursive autoconvolution features pooled concatenated form large feature vector steps usually missing cnns bold. standardization employed networks convolutions entire batch treated vector. cases follow apply local contrast normalization layers pooling speedup. training network analogous previous works layerwise learning start learning ﬁlters layer using training images process images learned ﬁlters followed rectiﬁcation pooling normalization steps yield ﬁrst layer feature maps using features train ﬁlters layer process learned ﬁlters forth. features concatenated layers used classiﬁcation support vector machines training ﬁlters rs×s×d×k layer ﬁlters size color channels performed three steps random patches rs×s×d extracted training samples; recursive autoconvolution applied them; unsupervised learning methods applied autoconvolutional patches. filters following layers trained according procedure well. unless otherwise stated k-means euclidean distance employed ﬁlter learning. learning recursive autoconvolution learning ﬁlters novel idea explain steps efﬁcient application tasks. issues spatial size image patches doubled iteration zero-padding learn ﬁlters patches need patches ﬁxed size simply take central part result resize original size iteration randomly choose options make patches richer. fig. recursive autoconvolution makes ﬁlters localized sparse shown distributions joint spatial frequency resolution σxyσuv ﬁrst layer ﬁlters learned cifar- k-means different orders simple single layer network improves results wide range ﬁlter sizes number ﬁlters three datasets mnist cifar- stl- cases results better without even latter times ﬁlters. number training/test samples input image sizes datasets indicated parentheses reference. mnist mnist cifar- training images class respectively. follow experimental protocol previous works e.g. random subsets training drawn test remains ﬁxed. stl- folds predeﬁned. unless otherwise stated report average classiﬁcation accuracies percent; reduced datasets results averaged runs. tables ia-v better results indicated bold. images datasets except mnist zca-whitened previous works previous works labeled training samples typically used unlabeled data unsupervised learning found enough samples learn ﬁlters experiments including stl- contains unlabeled samples. train models layers according proposed architecture details models presented table vii. network parameters chosen make consistent previous works within work. case multilayer network multidictionary features classiﬁcation standard approach unsupervised learning example case three layers features bottom layers pooled spatial size equals size third layer features afterwards concatenated. first experiment simple single layer architecture effect recursive autoconvolution classiﬁcation performance depending ﬁlter size number ﬁlters experiment case mnist cifar- fold cross-validation performed training test samples drawn original training sets. original predeﬁned folds. observe recursive autoconvolution consistently improves classiﬁcation. evident especially larger ﬁlters makes ﬁlters localized sparse experiment also shows results generalize well across different datasets preprocessing strategies. recursive autoconvolution demonstrate generalizability method investigated recursive autoconvolution able improve learning methods. purpose trained ﬁlters principal independent component analysis patches using procedure k-means filter sizes chosen based results previous experiment however larger ﬁlters satisfy s×s× since learn overcomplete dictionaries. even though improve ﬁlters reasons case results performed series experiments convolutional architectures match similar ones previous works unsupervised learning allows fairly compare features beneﬁt multilayer autocnn. comparison exemplar-cnn compared exemplarcnns results appear slightly inferior ﬁrst networks layers exploit data augmentation extensively. instance color augmentation improve results trained larger layer network able outperform exemplar-cnns cifar-. note however models table results better without model settings kept identical. fact cifar- network without recursive autoconvolution virtually doubled size order catch score network using small autocnn-s network estimated contribution rootsift results removing pipeline. boosts classiﬁcation also allows recursive autoconvolution work efﬁciently multilayer models. however note plots fig. results mnist obtained without rootsift always required. comparison conv-wta compared winner takes autoencoder networks efﬁcient even smaller number ﬁlters results improved. cases project features dimensions classiﬁcation cifar- features memory. undesirable operation degrades features. conv-wta mnist full mnist layer autocnns recursive autoconvolution yields better results conv-wta even though model times fewer ﬁlters second layer error full mnist competitive even compared supervised learning. unsupervised layer-wise learning suffer overﬁtting making layers wider autocnn-l computationally demanding. meanwhile splitting features groups provides features certain settings also beneﬁts classiﬁcation. designed large autocnn-l network groups. feature splitting idea shared ﬁlters introduced earlier work training time remains comparable autocnn-l. also tried architectures present results best one. dimension reduction classiﬁcation output features autocnn-l prohibitively large ﬁrst apply randomized principal component analysis together whitening train projected data. case rbf-svm efﬁcient implementation nonlinear chosen performs better projected features. regularization constant ﬁxed width kernel chosen dimensionality projected feature vector input svm. evaluation autocnn-l achieves competitive results reduced full datasets found layers particular model better train ﬁlters rather k-means. surprising according results single layer model worse k-means task. notably previous works classiﬁcation results good either simple grayscale images complex colored datasets smaller larger datasets only. exception seems ladder networks much larger deeper models semi-supervised. also better results achieved mnist quite easy tune simple task models especially effective cifar showing performance close ladder networks. results dataset several percent higher compared unsupervised models. full cifar- cifar- training labels class respectively models also outperform advanced fully supervised cnns better comparable cnns pretrained unsupervised large-scale dataset million training samples stl- network compares favorably layer convolutional autoencoder previous works showing higher accuracies either fully semi-supervised. drawbacks work best results cifar stl- obtained nonlinear believe given overall simplicity models reasonable complex kernel. moreover experience kernel always improve classiﬁcation results used features case. addition show competitive results linear tables ia-iv. also thoroughly validate model components main intention work show recursive autoconvolution boosts classiﬁcation number different settings important achieve best results. work show good results simple complex datasets well smaller larger ones using model tuning parameters. among unsupervised learning methods without data augmentation report state results datasets. main reasons results recursive autoconvolution allows learn ﬁlters rich spatio-frequency properties. autocnns learned ﬁlters joint spatial frequency resolution close theoretical minimum i.e. resemble simple edge detectors gabor ﬁlters. ﬁlters uniformly distributed across wide range resolutions typically complex meaningful shapes learning large sets ﬁlters models able detect highly diverse features easily choose discriminative ones. computational complexity. note numbers network ﬁlters presented tables ii-v always reﬂect total number trainable parameters total computational cost. layer autocnn-l model ﬁlters cnns convolutional layers parameters largest exemplarcnn conv-wta models parameters training large layer network full cifar- data augmentation takes minutes nvidia intel xeon matlab implementation vlfeat matconvnet gtsvm it’s also important computational cost recursive autoconvolution ﬁlter learning less overall training time. importance unsupervised learning visual tasks increasing development driven necessity better exploit massive amounts unlabeled data. propose novel idea unsupervised feature learning report competitive results several image classiﬁcation tasks among works relying supervised learning. moreover report state results among unsupervised methods without data augmentation. adopt recursive autoconvolution demonstrate great utility unsupervised learning methods k-means ica. argue also integrated recent learning methods convolutional clustering boost performance. furthermore signiﬁcantly reduce total number trainable parameters using shared ﬁlters. result proposed autoconvolutional network performs better unsupervised several supervised models various classiﬁcation tasks also thousands labeled samples. labusch erhardt barth thomas martinetz. simple method high-performance digit recognition based sparse coding. neural networks ieee transactions adam coates andrew honglak lee. analysis single-layer networks unsupervised feature learning. international conference artiﬁcial intelligence statistics pages alexey dosovitskiy philipp fischer jost tobias springenberg martin riedmiller thomas brox. discriminative unsupervised feature learning exemplar convolutional neural networks. arxiv preprint arxiv.v xiaolong wang abhinav gupta. unsupervised learning visual representations using videos. proceedings ieee international conference computer vision pages hilton bristow anders eriksson simon lucey. fast proceedings ieee convolutional sparse coding. conference computer vision pattern recognition pages janne heikkila. multi-scale auto-convolution afﬁne invariant pattern recognition. pattern recognition proceedings. international conference volume pages ieee relja arandjelovi´c andrew zisserman. three things everyone know improve object retrieval. computer vision pattern recognition ieee conference pages ieee alec radford luke metz soumith chintala. unsupervised representation learning deep convolutional generative adversarial networks. arxiv preprint arxiv. lars hertel erhardt barth thomas käster thomas martinetz. deep convolutional neural networks generic feature extractors. international joint conference neural networks pages ieee nathan halko per-gunnar martinsson joel tropp. finding structure randomness probabilistic algorithms constructing approximate matrix decompositions. siam review andrea vedaldi brian fulkerson. vlfeat open portable library computer vision algorithms. proceedings international conference multimedia pages sxsx relu autocnn-s autocnn-s- relu autocnn-s- relu relu autocnn-s relu autocnn-m relu autocnn-s relu autocnn-m relu autocnn-l autocnn-l relu number layers; number ﬁlters layer ﬁlter size depth layer recursive autoconvolution order learn ﬁlters layer pooling size stride stl- case data augmentation instead ﬁrst layer; number feature groups; rectiﬁer layer absolute values);", "year": 2016}