{"title": "Learning Convolutional Text Representations for Visual Question  Answering", "tag": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "abstract": "Visual question answering is a recently proposed artificial intelligence task that requires a deep understanding of both images and texts. In deep learning, images are typically modeled through convolutional neural networks, and texts are typically modeled through recurrent neural networks. While the requirement for modeling images is similar to traditional computer vision tasks, such as object recognition and image classification, visual question answering raises a different need for textual representation as compared to other natural language processing tasks. In this work, we perform a detailed analysis on natural language questions in visual question answering. Based on the analysis, we propose to rely on convolutional neural networks for learning textual representations. By exploring the various properties of convolutional neural networks specialized for text data, such as width and depth, we present our \"CNN Inception + Gate\" model. We show that our model improves question representations and thus the overall accuracy of visual question answering models. We also show that the text representation requirement in visual question answering is more complicated and comprehensive than that in conventional natural language processing tasks, making it a better task to evaluate textual representation methods. Shallow models like fastText, which can obtain comparable results with deep learning models in tasks like text classification, are not suitable in visual question answering.", "text": "abstract visual question answering recently proposed articial intelligence task requires deep understanding images texts. deep learning images typically modeled convolutional neural networks texts typically modeled recurrent neural networks. requirement modeling images similar traditional computer vision tasks object recognition image classication visual question answering raises dierent need textual representation compared natural language processing tasks. work perform detailed analysis natural language questions visual question answering. based analysis propose rely convolutional neural networks learning textual representations. exploring various properties convolutional neural networks specialized text data width depth present inception gate model. show model improves question representations thus overall accuracy visual question answering models. also show text representation requirement visual question answering complicated comprehensive conventional natural language processing tasks making beer task evaluate textual representation methods. shallow models like fasttext obtain comparable results deep learning models tasks like text classication suitable visual question answering. introduction visual question answering asks agent generate accurate answer natural language question queries image composite task involves variety articial intelligence elds computer vision natural language answering knowledge representation reasoning. great success deep learning elds eective agent built applications deep learning models. typical design answer generator based joint representation visual textual inputs considerable body research conducted appropriately combining visual textual representations fundamental question learning representations specically visual question answering generated interests. work perform detailed analysis text data design textual representation methods appropriate task. subtask extracting visual information well addressed models commonly used computer vision tasks like object detection image classication share similar demands visual representations. deep convolutional neural networks achieved signicant breakthroughs computer vision directly used vqa. natural language processing recurrent neural networks widely used learn textual representations tasks like text sentiment classication language modeling machine translation parallel image representations previous deep learning models directly rely rnns extract textual information. however detailed analysis text data reveals special properties text data vqa. results indicate rnns best text representation vqa. analysis insights propose apply cnns learning textual representations vqa. cnns texts explored simple text classication task shown comparable results rnns. experiments show simple cnn-based model outperforms rnn-based model much parameters result consistent analysis. work incorporate techniques cnns images rnns make specialized improvements build wider deeper networks. dierent methods text vectorization also explored analyzed. best model yields substantial improvement compared models rnnbased textual representations. recent study text classication also shows shallow model like fasttext achieve comparable accuracy deep learning models text classication. result contradicts common belief deep learning models higher representation power shallow ones. speculated simple text classication needs shallow representation power. conduct experiments learning textual representations using fasttext observe signicant decrease accuracy. demonstrates dierent requirements textual representations makes deep learning models necessary. argue appropriate task evaluate dierent textual representation models. background section describe basic families neural networks namely convolutional neural networks recurrent neural networks visual question answering units natural language processing text data naturally type sequential data rnns widely used tasks like text sentiment classication language modeling machine translation similar cnns image data rnns work feature extractors tasks encoding sequential data vector representations. addition generative models rnns also used decoders language modeling machine translation. compared using cnns sequential data applying rnns image data natural. terms data size cnns sequences must solve problem variable lengths rnns images problem. dierent ways serialize matrix data rnns shown eective extent. meanwhile video data combination rnns cnns active research topic visual estion answering visual question answering requires agent answer natural language question based corresponding image. drawn considerable aention articial intelligence deep learning computer vision natural language processing research communities. compared traditional computer vision natural language processing agents agent solves problems likely pass turing test since visual textual understanding knowledge reasoning needed vqa. deep learning shown power variety tasks. however training deep learning models demands large amount data. various datasets aimed collected made available dataset well-dened quantitative evaluation metric made available. makes feasible develop computational methods based recent achievements deep learning. also held public challenge based dataset current deep learning models coco-vqa share similar paern. usually consist four basic components image feature extractor text feature extractor feature combiner classier. image feature extractor eective cnn-based visual representation models computer vision resnet goolenet used image feature extractor. performance consistent among tasks; beer image classication models yield beer results used models. implies image classication task shares similar requirements task feature extraction image information. however case text side discussed section text feature extractor rnns like lstms grus commonly used text feature extractor. best knowledge simple model used achieved similar performance rnns. work provides detailed exploration cnn-based text feature extractor obtains considerably beer results tasks. feature combiner since initially proposed eorts devoted improving method combining image text feature vectors joint representation contains information needed answer question. convolutional neural networks image data processing convolutional kernel useful tools feature detection. combination several kernels detect simple features like corners edges lead detector specic shapes even complex features. based motivation convolutional neural networks apply trainable convolutional kernels neural networks outperformed many methods computer vision tasks object detection image classication image captioning module serves feature extractor image inputs tasks. unlike image data processing convolutional kernels hardwired primitive feature detectors cnns learn parameters kernels training deciding kinds features important specic task. stacking several convolution layers cnns compute hierarchy increasingly highlevel features images. high-level features represented feature maps used inputs next module classier text generator decoder depending tasks. success image tasks cnns considered natural choice matrix data sizes. sequential data like text data usually variable lengths recurrent neural networks commonly considered beer choice. currently applications cnns sequential data explored much. however recent studies shown applying cnns sequential data appropriate pooling layer feasible eective. discuss detail section recurrent neural networks cnns specialized matrix data recurrent neural networks specialized processing sequential data. similar human beings process sequence rnns process sequence values obtain comprehensive representation data. requires rnns memory processed data sequence. practice requirement causes problems called gradient vanishing explosion leads development improved rnns like long short-term memories gated recurrent element-wise multiplication concatenation proposed. multimodal compact bilinear pooling approximately computes out-product vectors proposed challenge coco-vqa improved adding parameters layer. contrast rnnbased episodic memory architecture proposed used combiner. moreover aention mechanism especially spatial visual aention shown eective part combiner ability extract highly related information images classier proposed training frequent answers training chosen form answer vocabulary. casts problem k-class classication problem. classier used models somax function typically used classication problems like image classication text sentiment classication. testing accuracy computed specic evaluation metric discussed section text representations section discuss special properties text data vqa. motivates apply cnns text feature extractor. perform detailed analysis text data develop models achieve promising performance. analysis texts natural language questions dierent text data several aspects. first people tend short questions according dierent datasets example longest question training coco-vqa contains words average length questions words. second required level textual understanding diers conventional natural language processing tasks text sentiment classication. sentiment analysis movie reviews learned text feature vector given classier determine whether author post likes dislikes movie. agent feature extractor focus emotional words lile aention contents. order answer question comprehensive understanding required since question anything. result text feature extractor powerful computes comprehensive information texts. questions dierent declarative sentences. words question highly related contents corresponding image. based properties argue that compared rnns cnns beer choices text feature extraction vqa. analyzing human beings process questions observe keys understanding questions understanding question type usually determined words understanding objects mentioned question relationships among them. many cases question type directly describes answer looks like answers questions starting there typically what number many questions must numbers answers. estions beginning what color what animal what sport explicitly indicate answers’ categories. meanwhile objects relationships usually nouns prepositional phrases respectively question sentence. provide guidance locating answerrelated facts image fundamental module aention mechanism models. task text feature extraction becomes clear; obtain feature vector consisting information question type objects queried. specic textual representation supposed extract starting words nouns well prepositional phrases represent. considering words phrases features text model specializing feature detection appropriate choice. cnns rnns serve feature extractors general usage rnns including lstms grus explicit feature extraction units. contrast convolutional connections cnns connections within units rnns mostly fully-connected. summarize cnns conceptually appropriate text feature extractor also validated experiments. additional advantages provided cnns fewer parameters easily parallelable accelerate training testing reducing risk over-ing. transforming text data challenge applying cnns text data convert texts format cnns take originally designed xed-size matrix data like images. apply cnns texts directly need represent text data image data represented. image typically -dimensional tensor three dimensions correspond height width number channels respectively. elements tensor scalar pixel values. example image stored tensor whose elements values every location channel. pixel image actually represented -component vector corresponding channels. intuitively text sentence considered image height equal inspired bag-of-words model natural language processing vocabulary built transform texts pseudo images. vocabulary either word-based contains words appearing texts character-based particular language. also reasonable vocabulary include punctuation single words characters. width text data dened based vocabulary; word-level representations width number words sentence; character-level representation count number characters. make concrete take word-based vocabulary example character-based case easily generalized section similar pixels image convert word vector length vector number channels. problem reduced word vectorization usually done one-hot vectorization. given vocabulary word represented one-hot vector; namely |-component vector position corresponding index word entries size one-hot vectorization number channels becomes result sentence words treated pseudo image channels given cnns directly modifying height convolutional kernels consistently. although one-hot embedding works well inputs cnns cases sometimes preferable lower dimensional embedding. primary reasons. first large usually case word-based vocabulary computation eciency sparsity high dimensionality inputs. second one-hot embedding semantically meaningless. extra embedding layer usually inserted cnns. layer maps |component vectors d-component vectors much smaller embedding layer basically multiplication one-hot vectors matrix perform look-up operation. distributed representations capture variety syntactic semantic relationships words. embedding matrix trained part networks task-specialized pre-trained using word embedding like wordvec glove embedding layer pseudo images channels. figure provides complete view transformations. note vocabulary built remaining process transform text data follows path dierent vocabularies. clear vocabulary denes pixels pseudo image obtained text data. example word becomes pixel. vocabulary character-based character including space character single punctuation pixel. case suppose sentence characters total resulting pseudo image size channels. embedding layer converts channels channels. main advantage character-based vocabulary produces much longer inputs since usually larger makes possible using deeper models. long texts transforming text data using character-based vocabulary applying deep cnns leads impressive performance another advantage characters include knowledge form words. however short texts size transformed data still small even character-based vocabulary. experiments show models work well long texts character-based vocabulary fail obtain high performance believed inputs short models learn space delimiter words naturally given word-based vocabulary case. combination character-based word-based vocabularies short tests explored achieved comparable results. method characters corresponding word grouped together. instead computing sentence representation directly characters word representation generated intermediately. group characters transformed character-based vocabulary smaller textual representation model generate word vector. word vector concatenated corresponding word embedding word-based vocabulary form larger word representation. details given figure methods involve character-based vocabulary handling variable-length inputs another problem text data sentence composed dierent numbers words leads variable sizes inputs outputs convolution layers. however outputs whole module expected xed-sized order serve inputs next module. moreover sizes inputs cnns also consistent consideration training. inspired pooling layers applied down-sampling convolution layers cnns images several pooling layers specialized text data variable lengths proposed basic idea apply pooling whole sentence select largest values instead performing pooling repeatedly local area images. called k-max pooling. xing last pooling layer cnn-based module requirement xed-sized outputs satised. results max-pooling whole length. details given figure pooling layers provide xed-sized outputs regardless size inputs size inputs also required particular optimization techniques batch training. solution requirement perform padding cropping. cropping usually used case long texts especially character-based vocabulary simply cuts part longer length. short texts like questions zero padding typically used input length longest sentence. involves minor problem aware longest length training longer data test. practice combination padding cropping used. note xed-sized inputs pooling local area images also feasible work perform zero-padding max-pooling whole sentence experiments based properties texts vqa. cropping used validation testing. deeper networks short texts note training coco-vqa average length questions words longest consists words. character-based vocabulary results longer inputs average longest training sample coco-vqa becomes characters. batch training texts zero-padding heavily used cases. padding lengths inputs seem appropriate model multiple convolution layers. however analysis experiments show adding layers actually leads worse results. first local max-pooling added convolution layers usually hurts performance since local area zero-padded makes outputs meaningless. second global max-pooling applied layers also work well. case zeropadding aect outputs module actually obtain results applied directly original short texts. deeper networks known suer over-ing input size small. fact comparing long texts figure example applying cnns text sentiment classication. given word-based vocabulary whose size transform -word sentence image channels one-hot vectorization. blue units represent white units represent layer. embedding layer number channels reduced able apply cnns inputs like images. note convolutional kernel since height pseudo image always convolution max-pooling whole sentence performed provide x-sized inputs classier wider deeper convolution layers added model easily. part dotted illustrates case character-based vocabulary used supplement word embedding vectors word-based vocabulary addition word-based vocabulary character-based vocabulary whose size provided. transforms -character word image channels one-hot vectorization process. embedding layer changes number channels cnn-based module followed max-pooling whole word generates word embedding concatenated word embedding obtained word-based vocabulary generating pseudo image channels. process words. note module shared among dierent words. samples characters multi-layer cnns work well length texts enough obtaining promising outcomes multi-layer cnns observations imply cnns texts residual networks long texts images deeper networks important benecial. obstacles going deeper deep networks become hard train suer degradation problem residual networks overcame obstacles adding skip connections inputs outputs layer several layers. skip connections named residual connections. enable cnns hundreds layers trained eciently avoid accuracy saturation problem. modied resnet layers long texts explored text classication character-based vocabulary note sample text data used characters. experiment resnet layers texts character-based vocabulary. results indicate inputs short deeper networks suer over-ing instead training degradation problems. also residual connections used layer one-layer models also hurts performances. turns that unlike mappings learned intermediate layers deep models mappings learned text feature extractor similar identity function making application skip connections inappropriate. inception modules inception modules proposed involve combining convolutional kernels dierent sizes convolution layer. technique enables wider convolution layers. motivation using inception modules texts straight-forward; dierentsized kernels extract features phrases dierent lengths. based interpretation choice number kernels corresponding sizes data-dependent dierent-sized phrases diverse importance various text data. explore seings several improvements experiments. gated convolutional units lstms grus improve rnns adding gates control information particular output gate controls information along sequential dimension. functionality output gate used deep learning models. output gate also applied cnns. unlike lstms grus fully-connected connections convolutional connections used generating output gates cnns. given input cnns case transformed data r×l×d text data independent convolutional kernels used form output convolution layer follows output gate sigmoid function represents convolution denotes element-wise multiplication bias terms. gated convolutional networks language modeling proposed activation function original outputs removed. replaced fasttext commonly believed deep models like cnns rnns powerful general. shallow model termed fasttext proposed achieved comparable results deep learning models several text classication tasks. fasttext embedding vectors text data used inputs module replaced simple average operation. formally word-based vocabulary since pseudo image channels actually concatenation d-component word vectors average word vectors results d-component sentence vector. sentence representation given directly classier. compared deep learning models cnns rnns fasttext obtains improvements terms accuracy achieving -fold speed-up small number parameters. performance fasttext casts doubts using deep models argued simple text classication tasks take full advantage higher representation power deep learning stated section task text understanding much complicated comprehensive makes beer evaluate capability dierent models. according experiments deep learning methods superior fasttext result consistent analysis. experimental studies general settings report experimental results coco-vqa dataset consists mscoco real images questions. data divided subsets training validation testing cocovqa answers dierent individuals collected question ground truths. training frequent answers among answers training chosen build answer vocabulary. iteration in-vocabulary answer sampled label ground truths question. answers answer vocabulary question skipped. evaluate accuracy generated answer following evaluation metric proposed generated answer compared ground truth answers corresponding accuracy computed. since evaluation testing processed remote servers challenge testing labels published choose train validate models training instead training+validation like test validation set. baseline model challenge winner uses -layer lstm text feature extractor. model retrained training only. meanwhile unlike additional data sources like pre-trained word embedding dataset augment training. order explore power models argue additional data narrow performance dierent models. comparison improve text feature extractors using cnn-based models experiments. results reported table retrained baseline model shown lstm part table. code publicly available. word-based models several cnn-based text feature extractors word-based vocabulary implemented. word-based vocabulary includes words appear training size word embedding dimension dropout applied textual representations given next module. part table shows results models. non-inception model one-layer model convolutional kernel. max-pooling whole sentence produces -component textual vector representation. simple cnn-based model already outperforms baseline model demonstrating cnn-based model beer rnn-based vqa. inception model explores wider cnns replacing single kernel non-inception model several dierent-sized kernels layer stated section dierent kernel seings explored results given table seings named format width kernel note height kernel always resulting textual vector representation components. models outperform non-inception model showing features extracted phrases dierent lengths complement other. table includes best results. models using inception modules inception residual model tries going deeper. adds identical layer residual connection inputs outputs inception model best kernel seing +++. extra layer supposed extract text features hurt performance experiments. conjecture need deeper short inputs vqa. character-based vocabulary result longer inputs deeper models discussed section inception bottleneck model inspired bottleneck architecture proposed apply boleneck convolution layer inception model kernel seing deep models image tasks architecture improves accuracies reducing number parameters. however causes signicant decrease accuracy one-layer model indicates boleneck design suitable deep models. inception gate model inception gate model cnn-based models output gates introduced section eqs. respectively. note combine gate architecture inception module kernel convolution layer corresponding methods improve inception model adding output gates. achieve best text feature extractor accuracy. figure comparison accuracy question type inception gate model lstm model. question types inception gate model outperforms lstm model. character-based models results models involve character-based vocabulary reported parts table models part character-based vocabulary only model part uses combination vocabularies character-based vocabulary collects characters lowercase characters english punctuation well space character. kernel seings inception-like models dropout also applied. inception model applies inception module inception model replaces word-based inputs character-based inputs. accuracy drops drastically. explained section short length inputs enough model learn separate characters words. deep residual model aempts take advantage longer inputs provided character-based vocabulary. stack convolution layers residual connections local pooling layers build deep model. contrast results model fails work well. again comparison indicates input length cause failure. inception model makes wordbased character-based vocabularies shown figure model characters word generate -component word embedding vector concatenated -component word embedding word-based vocabulary form -component vector representing word. compared inception model leads slight accuracy decrease. demonstrates using character-based vocabulary able provide useful information constituent characters word. based experiments conclude character-based vocabulary helpful short input cases like texts vqa. compare numbers parameters cnn-based text feature extractor lstm-based ones table cnns improve accuracy much fewer parameters needed train them. reduces risk over-ing. deep learning models versus fasttext introduced section fasttext shallow model achieves comparable results deep learning models like cnns rnns text classication tasks result contradicts common belief deep models learn beer representations. conjectured simple text classication task right evaluate textual representation methods. given higher requirement textual understanding compare models vqa. addition original fasttext model averages word embedding obtain sentence representation also explore fasttext character-based vocabulary. similar idea section character embedding word averaged generate part word embedding. results given table performance deep learning models fasttext. clearly appropriate task evaluate textual representation methods demonstrates power deep models. conclusions propose apply cnns textual representations text feature extractor vqa. incorporating recent research achievements cnns images best model improves textual representations overall accuracy vqa. comparing deep models fasttext show beer textual representations lead beer results turn appropriate task evaluate textual representation methods comprehensive requirement texts. based research believe cnn-based textual representation methods extensively used learning textual representations tasks texts similar properties. acknowledgments work supported part national science foundation grant iis- washington state university. gratefully acknowledge support nvidia corporation donation tesla used research. ankit kumar ozan irsoy jonathan james bradbury robert english brian pierce peter ondruska ishaan gulrajani richard socher. anything dynamic memory networks natural language processing. arxiv preprint arxiv. tsung-yu aruni roychowdhury subhransu maji. bilinear models ne-grained visual recognition. proceedings ieee international conference computer vision. jiasen jianwei yang dhruv batra devi parikh. hierarchical estion-image co-aention visual estion answering. arxiv preprint arxiv. tomas mikolov ilya sutskever chen greg corrado dean. distributed representations words phrases compositionality. advances neural information processing systems. tomas mikolov wen-tau georey zweig. linguistic regularities continuous space word representations.. hlt-naacl vol. jerey pennington richard socher christopher manning. glove global vectors word representation.. emnlp vol. karen simonyan andrew zisserman. deep convolutional networks large-scale image recognition. arxiv preprint arxiv. richard socher alex perelygin jean jason chuang christopher manning andrew christopher others. recursive deep models semantic compositionality sentiment treebank. proceedings conference empirical methods natural language processing vol. citeseer ilya sutskever oriol vinyals sequence sequence learning neural networks. advances neural information processing systems. christian szegedy yangqing pierre sermanet reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. proceedings ieee conference computer vision paern recognition. aaron oord kalchbrenner lasse espeholt oriol vinyals alex graves others. conditional image generation pixelcnn decoders. advances neural information processing systems. yonghui mike schuster zhifeng chen mohammad norouzi wolfgang macherey maxim krikun yuan klaus macherey others. google’s neural machine translation system bridging human machine translation. arxiv preprint arxiv. kelvin jimmy ryan kiros kyunghyun aaron courville ruslan salakhutdinov richard zemel yoshua bengio. show aend tell neural image caption generation visual aention. arxiv preprint arxiv. references stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. proceedings ieee international conference computer vision. dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. arxiv preprint arxiv. yoshua bengio r´ejean ducharme pascal vincent christian jauvin. neural probabilistic language model. journal machine learning research junyoung chung caglar gulcehre kyunghyun yoshua bengio. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. jason weston l´eon boou michael karlen koray kavukcuoglu pavel kuksa. natural language processing scratch. journal machine learning research alexis conneau holger schwenk lo¨ıc barrault yann lecun. deep convolutional networks natural language processing. arxiv preprint arxiv. yann dauphin angela michael auli david grangier. language modeling gated convolutional networks. arxiv preprint arxiv. jerey donahue lisa anne hendricks sergio guadarrama marcus rohrbach subhashini venugopalan kate saenko trevor darrell. long-term recurrent convolutional networks visual recognition description. proceedings ieee conference computer vision paern recognition. akira fukui dong park daylen yang anna rohrbach trevor darrell marcus rohrbach. multimodal compact bilinear pooling visual estion answering visual grounding. arxiv preprint arxiv. johnson tong zhang. semi-supervised convolutional neural networks text categorization region embedding. advances neural information processing systems. armand joulin edouard grave piotr bojanowski tomas mikolov. tricks ecient text classication. arxiv preprint arxiv. kushal christopher kanan. visual estion answering datasets kalchbrenner edward grefenstee phil blunsom. convolutional neural network modelling sentences. arxiv preprint arxiv. jin-hwa kyoung-woon jeonghee jung-woo byoung-tak zhang. hadamard product low-rank bilinear pooling. arxiv preprint arxiv. ranjay krishna yuke oliver groth justin johnson kenji hata joshua kravitz stephanie chen yannis kalantidis li-jia david shamma others. visual genome connecting language vision using crowdsourced dense image annotations. arxiv preprint arxiv. alex krizhevsky ilya sutskever georey hinton. imagenet classication deep convolutional neural networks. advances neural information processing systems.", "year": 2017}