{"title": "The Optimal Reward Baseline for Gradient-Based Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "There exist a number of reinforcement learning algorithms which learnby climbing the gradient of expected reward. Their long-runconvergence has been proved, even in partially observableenvironments with non-deterministic actions, and without the need fora system model. However, the variance of the gradient estimator hasbeen found to be a significant practical problem. Recent approacheshave discounted future rewards, introducing a bias-variance trade-offinto the gradient estimate. We incorporate a reward baseline into thelearning system, and show that it affects variance without introducingfurther bias. In particular, as we approach the zero-bias,high-variance parameterization, the optimal (or variance minimizing)constant reward baseline is equal to the long-term average expectedreward. Modified policy-gradient algorithms are presented, and anumber of experiments demonstrate their improvement over previous work.", "text": "exist number reinforcement algorithms expected reward. long-run convergence proved even partiaijy actions non-deterministic vironments without need system model. how­ ever variance gradient estimator found significant recent approaches wards introducing gradient baseline affects variance without introducing bias. particular bias high-variance line equal long-term reward. modified policy-gradient algorithms presented strate improvement exist number reinforcement learn climbing gradient methods. thus categorized reinforce solved immedi­ earliest reward learning problem delayed reward prob­ lems provided gradient estimates entered identified number similar algorithms discounting future rewards introduced variance recent remove however variance gradient estimator significant tions although discounting counting future rewards introduces variance gradient estimates heavily discounting biased; bias heavily ments discounting reward baseline times before effect variance context gradient algorithms. gated inclusion several stochastic learning equations result ment tasks. proposed average reward itive good value comparison provide analytical reason demonstrated approach introduce closely related present here explicitly although limits '-armed bandits' immediate reward problems. result summarized section olpomdp algorithm bartlett explicitly stead updates policy parameters directly every time step. proven converge local maximum baxter applying modifica­ generated tions system) respect standard could determine constant mean error variance figure baseline near theorem stant reward approaches pect garb result consistent consequently conjpomdp using gpomdp garb sup­ gradient trained average rewards experienced conjpomdp gpomdp garb supplying gradient estimates. garb yields consistent initial trollers order demonstrate acrobat problem analogous gym­ nast swinging high bar. involves simulating two­ link underactuated second joint implementation scription modifications actions chosen every simulated much finer granularity. sys­ motion simulated continuously reward given simu­ lating effect action; reward sim­ height acrobat's lowest pos­ sible position. since links metre length figures show average controllers whose olgarb respectively. randomly discount seen figures exception runs controllers policies equal best hand-coded oped. contrast controllers less reliably demonstrates standard groups trollers clearly derived take advantage vantage exhibit. paring gradient policy improvement system. puckworld working pomdps require avoid unacceptable levels", "year": 2013}