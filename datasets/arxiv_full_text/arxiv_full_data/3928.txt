{"title": "A Learning Algorithm for Evolving Cascade Neural Networks", "tag": ["cs.NE", "cs.AI"], "abstract": "A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is described. An ECNN starts to learn with one input node and then adding new inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly minimal number of input and hidden neurons as well as connections. The algorithm was successfully applied to classify artifacts and normal segments in clinical electroencephalograms (EEGs). The EEG segments were visually labeled by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing segments. It is slightly better than a standard fully connected neural network.", "text": "abstract. learning algorithm evolving cascade neural networks described. ecnn starts learn input node adding inputs well hidden neurons evolves trained ecnn nearly minimal number input hidden neurons well connections. algorithm successfully applied classify artifacts normal segments clinical electroencephalograms segments visually labeled eeg-viewer. trained ecnn correctly classified testing segments. slightly better standard fully connected neural network. words neural network cascade architecture evolving feature selection electroencephalogram build feed-forward neural networks cascade-correlation learning algorithm suggested creates hidden neurons needed. however algorithm applied real-world problems often over-fit cascade networks testing error becomes training error know over-fitting occur training data characterized many irrelevant noisy features. overcome problem different techniques pre-processing data developed able select informative features. example feature selection algorithms however results classification algorithms depend special conditions example order features processed. prevent cascade neural networks over-fitting method based combination algorithms early stopping ensemble averaging developed. authors showed method improves prediction ability neural networks. also proposed algorithm estimate generalization ability method using leaveone-out technique. pruning methods described developed networks trained cascade-correlation learning algorithm. methods used estimate importance large sets initial variables characterize quantitative relationships. results calculated cascade-correlation networks compared performance fixed-size neural networks. developed methods successfully used optimize initial variables. variables developed methods selected results improvement prediction ability neural network. avoid over-fitting group method data handling suggested allows generate neural networks appropriate complexity. learn networks generalize well gmdh exploits regularity criterion calculated training validating examples. gmdh-type neural networks successfully applied real world problems paper describe learning algorithm able select informative features cascade network learning. guess effective prevent networks over-fitting. contrast fully connected cascade networks case cascade network starts learn small number inputs. network uses features learning reason call networks evolving cascade neural networks training algorithm applied classify normal segments artifacts clinical electroencephalograms features calculated characterize segments irrelevant noisy redundant recordings used taken patients. segments visually labeled eeg-expert. ecnn learned recognize artifacts correctly classified testing segments. section introduce describe ecnns section describe detail fitting ecnn training algorithms developed. section apply algorithm real-world problem cleaning. section compare ecnn standard feed-forward neural network technique data. finally section discuss results effectiveness algorithm developed train ecnns. define cascade network architecture consisting neurons whose number inputs increased layer next. neuron first layer connected input nodes first inputs provides minimal single neuron error. neuron next layer linked first input well output previous neuron. thus r-th neuron connected input nodes also outputs previous neurons. following architecture describe output r-th neuron inputs follows vector r-th neuron components weight vector irrelevant features cause over-fitting neural networks experimentally estimate significance trial-and-error manner. accordingly algorithm outline section starts train output neuron input then step-by-step inputs well hidden neurons. therefore algorithm builds internal connections cascade neural network according values predefined fitness function. fitness function could defined regularity criterion used gmdh. regularity criterion denoted calculated neuron unseen examples used fitting connection weights. case value criterion depends generalization ability trained neuron given connections value criterion increased number misclassified examples increases. words neuron irrelevant connections able properly classify unseen examples hence value criterion higher. idea behind algorithm regularity criterion selecting neurons relevant connections. according idea value calculated r-th neuron less value calculated previous neuron features feed r-th neuron relevant else irrelevant. described following inequality inequality connections weights r-th neuron saved neuron added network. case neuron satisfies inequality algorithm stopped. result neuron minimal value criterion assigned output one. note noise training data inequality neuron r-th neuron. case trained network nearly optimal. however easily increase chance finding proper network increasing number unseen examples well number training runs. experiments described section kept half training data unseen examples ecnn algorithm times. priori information noise training dataset projection method described allows effectively neuron weights presence unknown noise. implement regularity criterion training dataset must divided least subsets subset used fitting neuron weights validating neuron. case output neuron irrelevant connections calculated validating dataset considerably different desirable values. thus calculating residual error validating dataset control training neurons network. training dataset consisting examples n·m-matrix input data target vector. divide non) intersecting subsets since training validating neuron realized different subsets denote input data respectively vectors. correspondingly i-th example denote using notations describe basic steps fitting algorithm. know level noise training dataset. however preset constant defines minimal decrement error calculated step respectively. goal training algorithm achieved error difference between step less learning rate p·na matrix input data ||ua|| long condition satisfied. step neurons start learn input. following steps growing network involves features well neurons value error decreasing. rons. network know able generalize well. real-world task data characterized many irrelevant features. application electroencephalograms used clinical recordings made standard electrodes following segment represented calculated features related power spectral densities calculated -second segments frequency bands sub-delta delta theta alpha beta beta power densities calculated electrodes well total sum. original features relative absolute power values well variances calculated. data finally normalized. experiment recordings made patients. artifacts recordings manually labeled eeg-viewer. recordings merged dataset divided training testing subsets consisting randomly selected segments including artifacts respectively. subsets composed even training examples respectively. since initial weights ecnn randomly assigned training algorithm times. selected trained ecnn whose error rate training minimal. experiment equal training examples. testing consisted examples error rate network equal fig. depicts structure network containing four input nodes three hidden output neurons. training algorithm selected four features input network. inputs first hidden neuron connected inputs inputs output neuron connected outputs hidden neurons inputs uring runs sizes trained ecnns varied neuron note ecnn consisting four neurons appears cases. distribution network sizes depicted histogram fig. time training testing error rates also varied runs. distribution errors depicted histogram fig. minimal training testing error rates equal respectively. used feed-forward neural networks hidden layer output neuron. number hidden neurons varied neurons. neurons implemented standard sigmoid transfer function. remove contribution correlated inputs improve results applied standard preprocessing technique principle component analysis find best performance given different fractions total variation training dataset number main components varied training exploited fast levenberg-marquardt algorithm provided matlab. order prevent network over-fitting algorithm used early stopping rule. reason non-intersecting fractions dataset preserved training testing validating fnn. trained makes errors testing dataset ecnn. hence conclude standard neural network technique described above provide over-complicated classifiers. experiments error trained fnns varied distributions training testing error rates depicted histograms fig. ecnn outperforms fully connected testing dataset minimal errors ecnn equal respectively. ecnn algorithm developed able better prevent over-fitting neural networks standard neural network technique. secondly suggested algorithm selected initial features relevant features found hidden neurons. algorithm automatically discovers neural network structure appropriate training dataset. thirdly discovered structure fully connected fnn* reduces testing error algorithm preprocessing technique effective standard neural network technique based pca. computational experiments described carried matlab experiments learning time ecnns exceed time spent fast algorithm. depends learning parameters equation nevertheless training examples ecnn took longer seconds learn. developed algorithm training cascade neural networks avoid noisy redundant features learning. network starts learn small number inputs adds inputs well hidden neurons learning process thus evolving larger network structure. result trained network nearly optimal architecture. training algorithm applied real-world problem related classification normal segments artifacts recordings. segments characterized several noisy irrelevant features. artifacts recordings patients visually labeled eeg-viewer. ecnn successfully learned automatically classify segments. ecnn trained segments correctly classified testing segments. standard feed-forward network using preprocessing applied datasets provided correct classifications. thus ecnn algorithm applied problem performed slightly better standard neural network technique. conclude algorithm effectively used train cascade neural networks applied real-world problems characterized many features. author grateful frank pasemann theorilabor fruitful enlightening discussions joachim frenzel burghart scheidt pediatric clinic university jena making available recordings. continues algorithm ieee transactions neural networks jang honovar feature subset selection using genetic algorithm proceedings genetic programming stanford tetko i.v. villa a.e. enhancement generalization ability cascadecorrelation algorithm avoidance overfitting problem neural processing letters tetko i.v. variable selection cascade-correlation learning architecture proceedings european symposium quantitative structure-activity relationships copenhagen denmark schetinin polynomial neural networks classifying signals proceedings nimia-sc nato advanced study institute neural networks instrumentation measurement related industrial applications crema italy galicki witte dörschel doering eiselt grießbach common optimization adaptive preprocessing units neural network learning period application pattern recognition neural networks riddington ifeachor allen hudson mapps fuzzy expert system interpretation proceedings int. conference neural networks expert systems medicine healthcare university plymouth", "year": 2005}