{"title": "Knapsack based Optimal Policies for Budget-Limited Multi-Armed Bandits", "tag": ["cs.AI", "cs.LG"], "abstract": "In budget-limited multi-armed bandit (MAB) problems, the learner's actions are costly and constrained by a fixed budget. Consequently, an optimal exploitation policy may not be to pull the optimal arm repeatedly, as is the case in other variants of MAB, but rather to pull the sequence of different arms that maximises the agent's total reward within the budget. This difference from existing MABs means that new approaches to maximising the total reward are required. Given this, we develop two pulling policies, namely: (i) KUBE; and (ii) fractional KUBE. Whereas the former provides better performance up to 40% in our experimental settings, the latter is computationally less expensive. We also prove logarithmic upper bounds for the regret of both policies, and show that these bounds are asymptotically optimal (i.e. they only differ from the best possible regret by a constant factor).", "text": "budget–limited multi–armed bandit problems learner’s actions costly constrained ﬁxed budget. consequently optimal exploitation policy pull optimal repeatedly case variants rather pull sequence diﬀerent arms maximises agent’s total reward within budget. diﬀerence existing mabs means approaches maximising total reward required. given this develop pulling policies namely kube; fractional kube. whereas former provides better performance experimental settings latter computationally less expensive. also prove logarithmic upper bounds regret policies show bounds asymptotically optimal standard multi–armed bandit problem originally proposed robbins presents clearest examples trade–oﬀ between exploration exploitation reinforcement learning. standard problem arms single machine delivers rewards independently drawn unknown distribution machine pulled. given this agent must choose arms pull. time step pulls machine’s arms receives reward payoﬀ. agent’s goal maximise return; expected rewards receives sequence pulls. reward distributions diﬀer goal highest expected payoﬀ early possible keep playing using best arm. however agent know rewards arms must sample order learn optimal one. words order choose optimal agent ﬁrst estimate mean rewards arms standard trade–oﬀ eﬀectively balanced decision–making policies upper conﬁdence bound ǫn–greedy however model gives incomplete description sequential decision–making problem facing agent many real–world scenarios. variety related models studied recently particular number researchers focused mabs budget constraints arm–pulling costly limited ﬁxed budget models agent’s exploration budget limits number times sample arms order estimate rewards deﬁnes initial exploration phase. subsequent cost–free exploitation phase agent’s policy simply pull highest expected reward. however many settings exploration phase exploitation phase also limited cost budget. address limitation bandit model budget–limited introduced model pulling costly crucially exploration exploitation phases limited single budget. type limitation well motivated several real–world applications. example many wireless sensor network applications sensor node’s actions sampling data forwarding consume energy therefore number actions limited capacity sensor’s batteries furthermore many scenarios require sensors learn optimal sequence actions performed goal maximising long term value actions take settings action considered cost equal amount energy needed perform task. battery limited exploration exploitation phases budget limited. background tran-thanh showed budget– limited cannot derived existing model therefore previous learning methods suitable eﬃciently deal problem. thus proposed simple budget–limited ε–ﬁrst approach budget–limited mab. splits overall budget portions ﬁrst used exploration remaining exploitation. however budget–limited ε–ﬁrst method suﬀers number drawbacks. first performance ε–ﬁrst approaches depend value chosen. particular high values guarantee accurate exploration ineﬃcient exploitation vice versa. given this ﬁnding suitable particular problem instance challenge since settings diﬀerent budget limits costs typically require different values addition even good value method typically provides poor eﬃciency terms minimising performance regret standard measure performance. particular regret bound ration exploitation) fractional kube explicitly separate exploration exploitation. instead explore exploit time adaptively choosing pull next based current estimates arms’ rewards. detail time step kube calculates best arms provides highest total upper conﬁdence bound estimated expected reward still residual budget using unbounded knapsack model determine best however since unbounded knapsack problems known np–hard algorithm uses eﬃcient approximation method taken knapsack literature called density–ordered greedy approach order estimate best following this kube uses frequency occurs within approximated best probability randomly choose pull next time step. reward received used update estimate upper conﬁdence bound pulled arm’s expected reward unbounded knapsack problem solved again. intuition behind algorithm know real value arms budget–limited reduced unbounded knapsack problem optimal solution subsequently pull arms forms solution knapsack problem. given this randomly choosing next current best time step agent generates accurate estimate true optimal solution accordingly sequence pulled arms converge optimal set. similar vein fractional kube also estimates best arms provides highest total upper conﬁdence bound estimated expected reward time step uses frequency occurs within approximated best probability randomly pull arms. however instead using density–ordered greedy solve underlying unbounded knapsack problem fractional kube relies computationally less expensive approach namely fractional relaxation based algorithm given this fractional kube requires less computation kube. analyse performance kube fractional counterpart terms minimising regret devise proveably asymptotically optimal upper bounds performance regret. proposed upper bounds differ best possible constant factor. following this numerically evaluate performance proposed algorithms state–of–the–art method namely buget–limited ε–ﬁrst approach order demonstrate algorithms ﬁrst achieve optimal bound. addition show kube typically outperforms fractional counterpart however results increased computational cost given this main contributions paper demonstrate increased computational cost kube outperforms fractional kube experiments. also show algorithms achieve logarithmic regret bounds buget–limited ﬁrst approaches fail paper organised follows next describe budget–limited mab. introduce learning algorithms section section provide regret bounds performance proposed algorithms. following this section presents empirical comparison kube fractional counterpart ε–ﬁrst approach. section concludes. budget–limited model consists machine arms must pulled agent time step. pulling agent pulling cost denoted receives non–negative reward drawn distribution associated speciﬁc arm. agent cost budget cannot exceed operation time since reward values typically bounded real–world applications assume arm’s reward distribution bounded supports. denote mean value rewards agent receives pulling within model agent’s goal maximise rewards earns pulling arms machine respect budget however agent initial knowledge must learn values order deduce policy maximises rewards. given this objective optimal pulling algorithm maximises expectation total reward agent achieve without exceeding cost budget formally arm–pulling algorithm giving ﬁnite sequence pulls. random variable represents number pulls respect budget limit since total cost sequence cannot exceed have given model described previous section introduce learning methods kube fractional kube eﬃciently deal challenges discussed section recall time step algorithms determine optimal arms provides best total estimated expected reward. similarities unbounded knapsack problems rewards known techniques taken unbounded knapsack domain. thus section ﬁrst introduce unbounded knapsack problem show knapsack methods algorithms. unbounded knapsack problem formulated follows. knapsack weight capacity ﬁlled diﬀerent types items. item type corresponding value weight problem select maximises total value items knapsack total weight exceed knapsack capacity goal non–negative integers {xi}k note problem generalisation standard knapsack problem item type contains item either choose not. unbounded knapsack problem –hard. however near–optimal approximation methods proposed solve among approximation methods simple eﬃcient approach density–ordered greedy algorithm make method. detail density– ordered greedy algorithm computational complexity number item types algorithm works follows. vi/wi denote density type begin item types sorted order density operation computational complexity. next ﬁrst round algorithm many units highest density item selected feasible without exceeding knapsack capacity. then second round densest item remaining feasible items identiﬁed many units possible selected. step repeated feasible items left another approximate optimal solution unbounded knapsack problem fractional relaxation based algorithm. relaxes original problem fractional version. particular within fractional unbounded knapsack problem allow fractional. easy show optimal solution fractional unbounded knapsack solely vi/wi choose maxi denotes optimal solution fractional unbounded knapsack c/wi∗ given this within original unbounded knapsack problem fractional relaxation based algorithm chooses ⌊c/wi∗ easily shown complexity algorithm cost determining highest density type. kube algorithm depicted algorithm here denote time step denote residual budget time respectively. note start total budget limit. subsequent time step kube ﬁrst checks pulling feasible. feasible least arms pulled remaining budget. speciﬁcally minj kube stops pulling still feasible kube ﬁrst pulls initial phase following this time step estimates best arms according upper conﬁdence bound using density–ordered greedy approximation method applied following problem upper conﬁdence interval. goal then integers {mit}i∈k equation maximised respect residual budget limit since problem np–hard density–ordered greedy method near–optimal arms note upper conﬁdence bound density method’s solution problem equation giving desired arms index indicates many times taken account within set. using kube randomly chooses next pull selecting probability intuition behind kube following. repeatedly drawing next pull distribution formed current estimated approximate best expected reward kube equals average reward following optimal solution corresponding unbounded knapsack problem given current reward estimates. true values arms known would imply average performance kube eﬃciently converges optimal solution unbounded knapsack problem reduced budget–limited model. easy show optimal solution knapsack model forms theoretical optimal policy budget–limited case full information. diﬀerently mean reward value known budget–limited problem reduced unbounded knapsack problem thus optimal solution knapsack problem optimal solution budget–limited well. addition combining upper conﬁdence bound estimated mean values arms guarantee sampled many times pulled frequently since upper conﬁdence interval large. thus explore exploit time note that using density–ordered greedy method kube achieves computational cost time step. turn fractional version kube follows underlying concept kube. also approximates underlying unbounded knapsack problem time step order determine frequency arms within estimated best arms. however diﬀers kube using fractional relaxation based method approximate unbounded knapsack step algorithm crucially fractional kube uses fractional relaxation based algorithm solve following fractional unbounded knapsack problem recall within kube frequency arms within approximated solution unbounded knapsack forms probability distribution agent randomly pulls next arm. since fractional relaxation based algorithm solely chooses highest estimated conﬁdence bound–cost ratio fractional kube need randomly choose arm. instead time step pulls computation–wise replacing density–ordered greedy fractional relaxation based algorithm fractional kube decreases computational cost time step. follows show kube fractional counterpart achieve asymptotically optimal regret bounds. focus analysis expected regret kube fractional kube deﬁned equation section derive upper bound regret algorithms show bounds asymptotically optimal. begin state simplifying assumptions deﬁne useful terms. without loss generality ease exposition assume reward distribution support pulling cost maxi µi/ci highest true mean value density. sake simplicity assume unique dmin minj=i {µi∗/ci∗ µj/cj} denote minimal true mean value density diﬀerence addition cmin minj cmax maxj denote smallest largest pulling costs respectively. diﬀerence pulling cost minimal pulling cost. similarly denote diﬀerence highest true mean value note could negative values since necessarily highest true mean value smallest pulling cost. addition denote ﬁnite–time operating time agent. ﬁrst analyse performance kube. follows ﬁrst estimate number times pull instead based result estimate average number pulls kube. bound guarantees kube always pulls enough arms diﬀerence number pulls theoretical optimal solution kube small compared size budget. using estimated value show kube achieves worst case regret average. detail performance regret kube upper–bounded prove theorem make following version chernoﬀ– hoeﬀding concentration inequality bounded random variables follows ﬁrst devise upper bound estimate number times pull instead based result estimate average number pulls kube bound guarantees kube always pulls enough arms diﬀerence number pulls theoretical optimal solution kube small compared size budget. using estimated value show kube achieves worst case regret average. state following number times kube pulls prove lemma ﬁrst refresh terms used pulled kube time refering arms {mjt} number pulls density–ordered greedy approximate solution unbounded knapsack problem equation proof lemma assume value given. slight abuse notation drop conditional notation simplify proof explicitly denote necessary. first consider particular value thus have recall density–ordered greedy approach ﬁrst repeatedly adds {mit} feasible. easy show adding many times possible times) residual budget time). therefore proof lemma assume value already given. again slight abuse notation drop conditional notation simplify proof explicitly denote necessary. case proof theorem particular value along lines theorem particular recall denotes expectation number times kube pulls time step given this following equation obtained lemma equation comes fact probability addition third inequality obtained cmin fact smaller thus upper bound summing arms dividing sides obtain fractional easy show optimal pulling policy relaxed model repeatedly pull only. number pulls optimal policy. lemma indicates number pulls kube produces signiﬁcantly diﬀer optimal policy fractional budget–limited derive regret bound kube lemma follows proof theorem follow concept similar proof theorem given this highlight steps diﬀerent previous proofs. sake simplicity notations previously introduced performance analysis kube. particular denote random variable represents number pulls fractional kube uses. denote number times corresponding pulling algorithm pulls time step similar lemma ﬁrst show within fractional kube algorithm have proof theorem setting arms’ pulling costs equal standard problem reduced budget–limited mab. implies number pulls within guaranteed according best possible regret pulling algorithm achieve within domain standard mabs therefore algorithm within domain budget– results theorem interpreted standard domain follows. standard reduced budget–limited setting pulling costs same. given this b/cmin sequence pulls. implies kube fractional kube achieve regret within standard domain optimal constant factor within regret bound fractional kube smaller kube). however indicate fractional kube better performance practice. possible reason bounds tight. fact demonstrate section kube typically outperforms fractional counterpart previous section showed algorithms provide asymptotically optimal regret bounds theoretical regret bound fractional kube tighter kube. addition also demonstrated fractional kube outperforms kube terms computational complexity. however might case bounds tight thus fractional kube less practical kube real–world applications case standard algorithm simple optimal methods typically outperform advanced theoretically optimal algorithms ucb). given this evaluate performance algorithms extensive simulations order determine eﬃciency practice. also compare performance proposed algorithms diﬀerent budget–limited ε–ﬁrst approaches. particular show algorithms outperform budget–limited ε–ﬁrst algorithms. addition also demonstrate kube typically achieves lower regret fractional counterpart. note pulling costs homogeneous pulling cost arms signiﬁcantly diﬀer performance density–ordered greedy algorithm signiﬁcantly diﬀer fractional relaxation based indeed since pulling costs similar easy show density–ordered greedy approach typically stops round thus results similar behaviour fractional relaxation based method. hand pulling costs diverse performance density–ordered greedy algorithm becomes eﬃcient fractional relaxation based algorithm. given this order compare performance kube fractional counterpart three test cases namely bandits homogeneous pulling costs; moderately diverse pulling costs; extremely diverse costs. particular within homogeneous case pulling costs randomly independently chosen interval addition pulling costs within moderately diverse case extremely diverse case respectively. reward distribution truncated gaussian mean randomly taken interval vari= ance supports addition number arms constant factor. numerical results kube fractional kube diﬀer best possible solution small constant factors since limit convergence typically compared regret value algorithm. addition also fractional kube algorithm typically outperformed kube. reason density–ordered greedy algorithm provides better approximation fractional relaxation based approach underlying unbounded knapsack problem. implies kube converges optimal pulling policy faster fractional counterpart. particular expected performance algorithms similar homogeneous case density–ordered greedy method shows similar behaviour fractional relaxation based approach. contrast kube clearly achieves better performance within diverse cases. speciﬁcally within moderately diverse case kube outperforms fractional counterpart addition performance improvement kube typically around extremely diverse case. implies that although current theoretical regret bounds asymptotically optimal tight. apart this also observe algorithms outperform budget–limited ε–ﬁrst approaches. particular kube fractional counterpart typically achieves less regret budget–limited ε–ﬁrst approaches respectively. note performance proposed algorithms typically line theoretical upper bound performance regret algorithms budget limit. addition proved provided bounds asymptotically optimal diﬀer best possible regret constant factor. finally simulation demonstrated kube typically outperforms fractional counterpart however increased computational cost. particular average computational complexity kube time step value fractional kube. implications numerical results although fractional kube better bound performance regret kube latter typically ourperforms former practice. given this future work consists improving results theorems determine tighter upper bounds found. addition extend budget–limited model settings reward distributions dynamically changing case numer real–world problems. this however trivial since algorithms rely assumption expected value rewards static thus estimates converge real value.", "year": 2012}