{"title": "Weakly Submodular Maximization Beyond Cardinality Constraints: Does  Randomization Help Greedy?", "tag": ["cs.DM", "cs.AI", "cs.DS", "cs.LG", "stat.ML"], "abstract": "Submodular functions are a broad class of set functions, which naturally arise in diverse areas. Many algorithms have been suggested for the maximization of these functions. Unfortunately, once the function deviates from submodularity, the known algorithms may perform arbitrarily poorly. Amending this issue, by obtaining approximation results for set functions generalizing submodular functions, has been the focus of recent works.  One such class, known as weakly submodular functions, has received a lot of attention. A key result proved by Das and Kempe (2011) showed that the approximation ratio of the greedy algorithm for weakly submodular maximization subject to a cardinality constraint degrades smoothly with the distance from submodularity. However, no results have been obtained for maximization subject to constraints beyond cardinality. In particular, it is not known whether the greedy algorithm achieves any non-trivial approximation ratio for such constraints.  In this paper, we prove that a randomized version of the greedy algorithm (previously used by Buchbinder et al. (2014) for a different problem) achieves an approximation ratio of $(1 + 1/\\gamma)^{-2}$ for the maximization of a weakly submodular function subject to a general matroid constraint, where $\\gamma$ is a parameter measuring the distance of the function from submodularity. Moreover, we also experimentally compare the performance of this version of the greedy algorithm on real world problems against natural benchmarks, and show that the algorithm we study performs well also in practice. To the best of our knowledge, this is the first algorithm with a non-trivial approximation guarantee for maximizing a weakly submodular function subject to a constraint other than the simple cardinality constraint. In particular, it is the first algorithm with such a guarantee for the important and broad class of matroid constraints.", "text": "submodular functions broad class functions naturally arise diverse areas economics operations research game theory. many algorithms suggested maximization functions achieving strong theoretical guarantees good practical performance. unfortunately function deviates submodularity known algorithms submodular maximization perform arbitrarily poorly. amending issue obtaining approximation results classes functions generalizing submoduolar functions focus several recent works. class known class weakly submodular functions received attention machine learning community strong connections restricted strong convexity sparse reconstruction. result proved kempe showed approximation ratio standard greedy algorithm problem maximizing weakly submodular function subject cardinality constraint degrades smoothly distance function submodularity. however results obtained maximization weakly submodular functions subject constraints beyond cardinality. particular known whether greedy algorithm achieves non-trivial approximation ratio constraints. paper prove randomized version greedy algorithm diﬀerent problem) achieves approximation ratio maximization weakly submodular function subject general matroid constraint parameter measuring distance function submodularity. moreover also experimentally compare performance version greedy algorithm real world problems gene splice site detection video summarization natural benchmarks show algorithm study performs well also practice. best knowledge ﬁrst algorithm non-trivial approximation guarantee maximizing weakly submodular function subject constraint simple cardinality constraint. particular ﬁrst algorithm guarantee important broad class matroid constraints. †depart. mathematics computer science open university israel. e-mail moranfeopenu.ac.il. ‡yale institute network science yale university. e-mail amin.karbasiyale.edu. motivated frequent appearances submodular functions theoretical practical settings last decade seen proliferation works maximization submodular functions. particular algorithms maximizing submodular function subject various constraints found many applications machine learning data mining including data summarization document summarization sensor placement network reconstruction crowd teaching spread inﬂuence article recommendation despite mentioned abundance settings give raise submodular functions observed also many settings inducing functions close submodular strictly submodular. unfortunately algorithms developed maximization true submodular functions often fail miserably given function close submodular. hurdle motivated development algorithms whose guarantee degrades gracefully distance function submodularity. particular algorithms developed functions close submodular distance measure known supermodular degree close submodular function multiplicative factor noisy versions submodular functions various noise models particularly important class close submodular functions known γ-weakly submodular functions received attention machine learning community. weakly submodular functions originally introduced work kempe showed standard greedy tions subject cardinality constraint. works developed sophisticated algorithms maximization problem demonstrated large repertoire applications captured example elenberg described streaming algorithm maximization problem used faster algorithm interpreting outputs neural networks. relying previous works showed submodular functions maximized faster versions standard greedy algorithm either stochastic distributed khanna showed faster versions greedy algorithm also used maximizing weakly submodular functions. proposed algorithm achieved anytime linear prediction sequencing computation feature groups whose theoretical guarantee established based weak submodularity finally qian leveraged weak submodularity design approach parallel pareto optimization problem subset selection emerged powerful approximate solver subset selection problem. best knowledge existing works regarding maximization γ-weakly submodular functions assume simple cardinality constraint thus cannot applied applications require involved constraints. paper make ﬁrst step towards amending situation. speciﬁcally show algorithm called residual random greedy submodular maximization) yields involved analysis ﬁrst nontrivial approximation ratio maximizing γ-weakly submodular function subject general matroid constraint. random greedy viewed randomized version greedy algorithm. makes possible view analysis residual random greedy evidence standard greedy algorithm works time. words expect greedy algorithm produce good approximation ratio instances speciﬁcally engineered make perform poorly. whether indeed case conducted four sets experiments. ﬁrst studies linear regression problem synthetic data sets correspond real-world application scenarios real data expected experiments show residual random greedy greedy algorithm comparable performance real-world instances. moreover turns greedy algorithm even manages outperform residual random greedy many instances. function ground monotone every sets furthermore given subsets denote marginal contribution adding elements formally many cases subset deﬁnition singleton {u}. cases write simplicity instead additionally occasionally shorthands union expression respectively. matroids important capture many natural structures. example forests graph form matroid known graphical matroid graph. consequently maximization various functions subject general matroid constraint studied extensively paper interested problem second remark regarding theorem related deﬁnition γ-weak submodularity given above. mentioned deﬁnition slightly diﬀerent original deﬁnition γweakly submodular functions original deﬁnition weaker sense required inequality hold small sets i.e. sets whose size comparable size largest possible feasible solution. sake keeping deﬁnition clean possible dropped extra complication deﬁnition weak submodularity. however note that employs algorithms weak submodular optimization solve real-world problems often useful weakest possible deﬁnition makes likely real-world objective function fall deﬁnition. thus would like point theorem applies even objective function obeys following weaker deﬁnition matroid uniform matroid general matroid respectively. approximation ratio uniform matroid constraints discovered roughly time optimal however question regarding optimality -approximation algorithm general matroid constraints remained open many years. decade question ﬁnally solved maximizing monotone submodular function subject general matroid constraint. result proved exactly approximation ratio achieved maximization monotone submodular functions subject uniform general matroid constraints implies sense general matroid constraints diﬃcult uniform matroid constraints. nevertheless still signiﬁcant time complexities fastest algorithms known types constraints closing remains intriguing open question future research. result motivated long series works maximization non-monotone submodular functions subject matroid constraint currently best algorithm kind achieves approximation ratio general matroid constraints better approximation guarantee known uniform matroid constraints. inapproximability side known polynomial time algorithm achieve approximation ratios better maximization non-monotone submodular functions subject uniform general matroid constraints respectively. hence non-monotone submodular functions still large best known approximation inapproximability results. somewhat bridged result giving .-approximation algorithm maximization special class non-monotone submodular functions known symmetric submodular functions subject general matroid constraint. paper organization. rest paper organized follows. section present residual random greedy analyze formally section describes experimental results. finally section contains concluding remarks points interesting direction future research. informally algorithm grows solution rounds round consists steps. ﬁrst step algorithm assigns element weight equal marginal contribution element current solution then second step round algorithm ﬁnds maximum weight among sets whose union current solution independent adds uniformly random element weights independence oralce queries done using greedy algorithm). thus execution line requires oracle queries observation follows since line executed times. algorithm’s behavior. assume already constructed function mapping every element element base. observe existence function follows immediately lemma since bases guarantees base promised. next lemma proves lower bound expected values sets constructed. proof lemma similar proof lemma nevertheless current lemma manages tighter bound obtained assume inequality holds prove holds also observe uniformly random subset size uniformly random subset size thus think obtained adding uniformly random element ti+. taking point view every size bound given observation uses sets ti}k observation every e]−e] proof. arbitrary event ﬁxing random decisions algorithm iteration probabilities expectations random quantities ﬁrst part proof implicitly conditioned ai−. γ-weak submodularity implies second inequality follows monotonicity since subset adding last inequality induction hypothesis proves inequality holds thus completes proof induction inequality conducted four sets experiments. ﬁrst studies linear regression synthetic data three sets correspond real-world application scenarios real data. application scenarios studied sets video summarization splice site detection black-box interpretation images coeﬃcients noise vector i.i.d. standard gaussian random variables. general people interested problem given matrix vector recovering assumption sparse sense. current experiment call vector sparse support supp independent input matroid words want among vectors whose support independent vector likely vector used generate constructed matrix independently according autoregressive process noise variance support vector chosen randomly using mentioned algorithm random non-zero value assigned uniformly experiment used graphic matroid generate graph underlying graphic matroid started empty graph vertices added random edges edge chosen independently connected uniformly random pair distinct vertices. experiment used partition matroid partitions denote generate partition matroid used steps. first uniformly sampled random distribution possible distributions partitions then created elements assigned partitions according mentioned distribution. finally every partition sampled capacity—i.e. maximum number elements partition appear independent set—from results experiments illustrated figure plots ﬁgure show normalized log-likelihood varies algorithms select elements constraints corresponding matroids. plots black line denotes normalized log-likelihood achieved ground truth observe residual random greedy standard greedy yield comparable performance outperform random. particular terminate normalized likelihoods attained residual random greedy standard greedy almost equal. given frames video represented frame p-dimensional vector. rn×n gramian matrix resulting vectors gaussian kernel; i.e. value gaussian kernel i-th j-th vectors. objective function given determinant function principal submatrix indexed note identity matrix added objective make sure function monotone. moreover function shown weakly submodular although might submodular. light non-submodularity determinant function rather optimize directly prior works considered known submodular allows standard submodular optimization techniques guarantee approximation ratio original objective function. fortunately help residual random greedy maximize determinant function directly guranteed approximation ratio. video selected experiment lasts roughly minutes half chose created summary extracting representative frame every seconds. words constraint allowed summerization given partition matroid figure illustrates performance residual random greedy benchmark algorithms applied problem. frames selected three algorithms shown figures algorithm selects frame seconds selected frames arranged images chronological order left right bottom. quite easy observe residual random greedy standard greedy produce summaries higher diversity random. example ﬁrst frames selected random young lady residual random greedy standard greedy choose young lady show studio; again frames selected random lady black residual random greedy standard greedy produce duplications allows cover content. comparing outputs residual random greedy standard greedy subtle result residual random greedy seems slightly better. frames selected residual random greedy show participants recognized summaries; contrast standard greedy chooses frames show guests sitting behind long blue desk studio reduces diversity frames. important problem computational biology identiﬁcation true splice sites similar decoy splice sites nascent precursor messenger transcripts. splice sites nucleotide sequences mark beginnings ends introns general ends sequence known ’-end ’-end. case introns ends also known splice donor site splice acceptor site respectively. interested problem identifying splice donor splice acceptor sites. words given sequence nucleotides want determine whether sequence represents splice donor/acceptor site. splice donor site always includes nucleotide sequence ’-end splice acceptor site sequence ’-end. however kinds sites include additional nucleotides whose identify taken account deciding whether given sequence nucleotides splice donor/acceptor site memset dateset provides instances true false splice donor/acceptor sites. note false splice donor/acceptor sites also include compulsory gt/ag sequences diﬀer true sites nucleotides. detailed description dataset presented experiments used logistic regression memset dateset determine nucleotide values largest inﬂuence categorization splice sites true false sites. preprocessing step removed consensus sequences. then considered natural explanatory variables problem i.e. single variable taking four values every nucleotide splice site. explanatory variables categorical; converted four binary variables one-hot encoding. words explanatory variable created four binary dummy variables takes value exactly given encoding natural constraint four binary variables one; partition matroid constraint. denote j-th binary dummy variables corresponding outcome respectively. standard logistic regression assume weak submodularity objective function shown figure present result applying residual random greedy benchmark algorithms mentioned optimization problem. ranks partition matroids donor acceptor sites figure respectively number nucleotides provided kinds sites memset dataset. note standard greedy residual random greedy exhibit comparable performance consistently outperform random. experiments consider problem interpreting predictions black-box machine learning algorithms—i.e. explaining reasons prediction. speciﬁcally follow setting given image label lime framework outputs likelihood image label example labels assigned lime framework figure bernese mountain entlebucher greater swiss mountain appenzeller egyptian parts image best explain likely label bernese mountain dog; denote label applied slic algorithm image algorithm segmented image superpixels task select superpixels best explain label following maximization problem max|s|≤k applied residual random greedy standard greedy random optimization problem; superpixels selected three algorithms visualized figures respectively. note function depends black-box machine learning algorithm thus weakly submodular even monotone general. nevertheless residual random greedy benchmark algorithms still produce interesting results used optimize recall label explain bernese mountain dog. superpixels selected residual random greedy include parts image form head bernese mountain superpixels selected standard greedy cover nose small portion body. additionally also incorrectly include head cat. performance random worst mostly selects superpixels irrelevant dog. also illustrate figure likelihood subimage induced selected superpixels label versus number superpixels selected. observed residual random greedy outperforms standard greedy superpixels selected. also noteworthy observe likelihood achieved random remains almost zero number selected superpixels varies reaching paper proved ﬁrst non-trivial approximation ratio maximizing γ-weakly submodular function subject general matroid constraint. result opens door applications also suggests greedy algorithm performs well practice problem. moreover able demonstrate experimentally multiple applications suggested good behavior greedy algorithm. signiﬁcant question leave open whether greedy algorithm good provable approximation ratio problem. note necessarily implied good practical behavior greedy algorithm. example closely related problem maximizing non-monotone submodular function greedy algorithm performs well practice despite unbounded theoretical approximation ratio. personally tend believe greedy algorithm good provable approximation ratio problem unable design example approximation ratio greedy algorithm non-constant however proving formally likely require ideas thus interesting area future work.", "year": 2017}