{"title": "Dense Associative Memory is Robust to Adversarial Inputs", "tag": ["cs.LG", "cs.CR", "cs.CV", "q-bio.NC", "stat.ML"], "abstract": "Deep neural networks (DNN) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation, so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNN and humans classify patterns, and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods.  Our paper examines these questions within the framework of Dense Associative Memory (DAM) models. These models are defined by the energy function, with higher order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units (ReLU), fail to transfer to and fool the models with higher order interactions. This opens up a possibility to use higher order models for detecting and stopping malicious adversarial attacks. The presented results suggest that DAM with higher order energy functions are closer to human visual perception than DNN with ReLUs.", "text": "deep neural networks trained supervised suﬀer known problems. first minima objective function used learning correspond data points lack semantic similarity training data. second clean input changed small often imperceptible human vision perturbation resulting deformed input misclassiﬁed network. ﬁndings emphasize diﬀerences ways humans classify patterns raise question designing learning algorithms accurately mimic human perception compared existing methods. paper examines questions within framework dense associative memory models. models deﬁned energy function higher order interactions neurons. show limit power interaction vertex energy function suﬃciently large models following three properties. first minima objective function free rubbish images minimum semantically meaningful pattern. second artiﬁcial patterns poised precisely decision boundary look ambiguous human subjects share aspects classes separated decision boundary. third adversarial images constructed models small power interaction vertex equivalent rectiﬁed linear units fail transfer fool models higher order interactions. opens possibility higher order models detecting stopping malicious adversarial attacks. presented results suggest higher order energy functions closer human visual perception relus. deep neural networks highly sensitive small well-designed perturbations inputs known adversarial perturbations lead misclassiﬁcations perturbed inputs. consider images example data. image correctly classiﬁed neural network modiﬁed small resulting deformed image classiﬁed diﬀerent class related problem classify certain images belonging class although unrecognizable humans exemplars class rubbish fooling images correspond patches image space small value objective function used training located away training data. observations challenged integrity classiﬁcation opinion predictions untrustworthy though similar problems shared many machine learning techniques logistic regression support vector machines k-nearest neighbors others simons center systems biology institute advanced study princeton krotovias.edu princeton neuroscience institute princeton university princeton hopﬁeldprinceton.edu investigation shown adversarial images rubbish images transferred many diﬀerent models distinct architectures diﬀerent hyperparameters even trained diﬀerent training sets moreover adversarial rubbish images transferred diverse models machine learning opens possibility potential adversarial attack hacker train model create images misclassiﬁed deploy images another victim model also misclassify them. realistic attacks studied often require internal knowledge victim model. addition security issue transferability suggests various computational models learn similar representations data. also suggests order address problems might design training algorithms learn diﬀerent representation data compared existing methods. ideal solution issues algorithm assigns small values objective function areas image space recognizable humans images corresponding class. also require substantial recognizable humans deformation initial correctly classiﬁed image towards diﬀerent target class label changes. spite substantial amount work problems algorithm identiﬁed satisﬁes requirements time competitive state algorithms terms classiﬁcation accuracy. recent paper proposed dense associative memory models higher order interactions energy function learn representations data strongly depend power interaction vertex. network extracts features form data small values power power interaction vertex increased gradual shift prototype-based representation extreme regimes pattern recognition known cognitive psychology. remarkably broad range powers energy function representation data already prototype regime accuracy classiﬁcation still competitive best available algorithms suggests models might behave diﬀerently compared standard methods used deep learning respect adversarial deformations. present paper report three main results. first using gradient decent pixel space rubbish images constructed correspond minima objective function used training. done mnist dataset handwritten digits using diﬀerent values power interaction vertex denoted small values power images indeed look like speckled rubbish noise semantic content human vision result consistent however power interaction vertex increased images gradually become less speckled semantically meaningful. images handwritten digits could possibly produced human. second starting clean images dataset adversarial images constructed image placed exactly decision boundary label classes. small powers images look similar initial clean image little speckled noise added misclassiﬁed neural network result consistent however power interaction vertex increased adversarial images become less less similar initial clean image. limit large powers adversarial images look either like morphed image digits initial digit superimposed ghost image target class. either interpretation artiﬁcial patterns generated neural decision boundary requires presence another digit target class addition initial seed dataset cannot explained simply adding noise initial clean image. third adversarial rubbish images generated models small transferred fool another model small value however fail transfer models large thus rubbish adversarial images generated models small cannot fool models large contrast rubbish images generated models large transferred models small problem since rubbish images actually rubbish look like credible handwritten digits. results suggest dams large power interaction vertex energy function better mimic psychology human visual perception dams small power. latter equivalent dnns relus order illustrate results simplest possible setting trained family networks several values power energy function mnist pixel permutation invariant task. model deﬁned weights feedforward update figure architecture neural network. visible neurons equal intensities pixels -bit intensities linearly mapped onto segment visible neurons classiﬁcation neurons initialized state updated ﬁnal state using model memories visible parts index ...k. parameter regulates slope tanh function. explained argument update rule diﬀerence energies corresponding initial ﬁnal states neurons. reason functions called energy functions rest paper integer called power interaction vertex. weights learned using backpropagation algorithm investigated model varying powers energy functions discovered network learns feature-based representations data small prototype-based representations large clear fig.. small feature detector describes feature useful recognizing several diﬀerent digit classes. power figure feature prototype transition powers energy function model randomly selected feature detectors shown. value i-th element detector plotted location i-th pixel couples update rule weights normalized increased feature detectors specialize become responsible recognizing possible prototype constructed representation simply copy image training set. importantly throughout range classiﬁcation accuracy training complete neural network used inspect minima objective function. order deﬁne objective functions penalizing deviations corresponding class random image generated gaussian distribution deformed images corresponding label classes following gradient objective functions according iterative rule unit vector points direction gradient objective function size update step. dynamics ﬂows towards minimum objective function thus good learning algorithm expect recognizable image corresponding digit reaches ﬁxed point. actual results shown fig.. objective functions corresponding digit classes model ﬁnal image labeled. labels clear images. throughout paper grayscale intensities representation image plane. confused color images like fig. shows network’s weights. small dynamics converges local minima recognizable digits humans. rubbish minima training algorithm learned trying minimize classiﬁcation error training set. contrast large dynamics ﬂows towards minimum objective function corresponds recognizable image corresponding digit. thus simple experiment demonstrates large minima objective function semantic meaning image space models small minima semantically meaningless. achieved training network purely supervised pixel permutation invariant task. clean image classiﬁed neural network belonging certain class modiﬁed small perturbation deformed image classiﬁed diﬀerent class. makes statement problem perturbation suﬃcient changing label small typically speckled pattern lacks semantic meaning. common technique generating adversarial images sign gradient method purposes necessary generate images placed exactly decision boundary classes vicinity. reason slightly diﬀerent method. input image vector labels calculated using elements vector sorted largest smallest value. largest value output corresponds classiﬁcation choice network second largest output corresponds second choice etc. initial image iteratively deformed along gradient figure logarithm top-choice second-choice objective functions iterative dynamics following negative gradient second-choice objective function progresses. crossing point deﬁnes decision boundary. triplets images three model. ﬁrst image triplet natural image dataset middle image artiﬁcial image corresponding crossing point objective functions third image corresponds ﬁnal point iterative dynamics second-choice objective function reaches zero. objective function corresponding second choice label made network. iterative process ﬁrst choice objective function increase second choice objective function decrease image gradually change initial clean image image network thinks correspond second choice label. iteration objective functions become equal. mathematical deﬁnition decision boundary. point view neural network image corresponds point deformation trajectory exactly middle classes. question whether human observer would agree interpretation method generating adversarial images advantage compared since guarantees image crossing point objective functions placed exactly decision boundary close also contrast require careful choice iteration step provided suﬃciently small. procedure simply need steps order correct image corresponding crossing point step small. drawback method compared slower. fig. triplets images generated models diﬀerent values power triplet ﬁrst image initial clean image dataset second image neural network generated decision boundary third image network generated iterative procedure converged second choice label initial clean image. observations made ﬁgure. first power interaction vertex increased third image triplet becomes semantically meaningful. accord results fig.. second power interaction vertex increased adversarial image decision boundary becomes meaningful well. middle image looks almost initial clean image little added speckled noise. noise share much similarity second choice label. contrast deformation look like noise all. middle image looks digits morphed together image second digit added ghost image initial digit. results suggest models large powers learn semantically meaningful minima objective function also learn semantically meaningful deformations classes. achieved training network pixel permutation invariant classiﬁcation task training generative model. principle target class necessarily coincide second choice initial classiﬁcation decision. convention used convenience. best knowledge results change qualitatively instead second choice label initial image deformed towards label one. intriguing ﬁnding rubbish adversarial images transferred diverse machine learning models. order test phenomenon within framework designed experiments concerns transfer adversarial images concerns transfer rubbish images. ﬁrst experiment mnist test used generate adversarial examples four models resulting four datasets images. clean image test procedure described section used generate artiﬁcial image placed iteration step behind decision boundary deﬁned fig.. resulting image thus classiﬁed second choice network original image. four adversarial datasets used cross-classiﬁcation four models. error rates shown fig.. diagonal elements table close guaranteed design figure transfer table adversarial examples. mnist test used construct four test sets adversarial images poised step behind decision boundary four datasets cross-classiﬁed four models. dataset images. number intersection i-th j-th column error rate j-th model adversarial dataset constructed using i-th model. right error rates four models original mnist test set. datasets. important aspect table adversarial images generated models transfer model images share semantic similarity clean image used initial seed crafting them generally semantic features target class deformation. model detect similarity initial image still correctly classify cases. contrast adversarial images crafted using model transferred models however argued previous section expected since images share semantic similarities initial seed target class deformation. thus machine learning algorithm human subject misclassify substantial fraction them. second experiment artiﬁcial dataset created data correspond minima objective functions sample random noise image generated gaussian distribution iteratively changed direction negative gradient according convergence. dataset images label class images total. procedure repeated value resulting four datasets used cross-classiﬁcation four models. results shown fig. discuss need ﬁrst introduce notion conﬁdence. output network collection numbers following discussion convenient deﬁne measure conﬁdence neural network making classiﬁcation decision. actual outputs approximately equal target output reason error rate around instead classiﬁcation error clean dataset cases second choice network correct answer. thus rare cases adversarial deformation used generate dataset actually turns incorrect answer correct one. figure transfer table. corresponds model used generate artiﬁcial data following gradient objective function. column corresponds model used recognize images. number intersection mean conﬁdence produced test model. right sample artiﬁcial dataset label class. labels images shown green bottom column. single value used experiment. results remain qualitatively similar model uses value test time. average conﬁdence network dataset matches error test set. outputs network exactly equal target outputs inputs average conﬁdence dataset would equal equation. results since outputs networks general slightly diﬀerent target outputs actual values diﬀerent four models discussed. however conclusions presented remain true values long stay within probability error decreases conﬁdence increases result suggesting conﬁdence measure meaningful quantity. return discussion second experiment pertaining analysis rubbish images. fig. mean conﬁdence cross-classiﬁcation pairs reported together sample images dataset class four data sets. bottom corresponds data generated model samples look like rubbish images accord fig.. rubbish images recognized network average conﬁdence result particular choice value βsm. rubbish images transferred model also conﬁdent correspond correct labels. however tries transfer rubbish images models higher order models immediately detect images behavior human subject would immediately detect rubbish images semantically meaningless thus belong class. conclusion applies dataset constructed model contrast dataset constructed model composed nice images digits recognizable humans. security perspective results suggest models large used detect stop potential hacker attack devised exploiting conventional machine learning techniques time adversary tries models large mounting attack deploys models possess security issue interpretations model gives images consistent human visual perception. accuracy slightly lower models small used pair another model good classiﬁcation accuracy vulnerable adversarial/rubbish examples. ﬁrst model detects potential adversarial attack. cases conﬁdent label disagrees accurate model prediction accurate model ﬁnal classiﬁcation decision. however large model unconﬁdent particular input input labeled junk processed accurate/vulnerable model. although modern machine learning techniques outperform humans many classiﬁcation tasks serious concern understand structure training data. clear demonstration lack understanding presented showed examples nonsensical predictions dnns contradict human visual perception adversarial images rubbish images. present paper propose higher order interactions energy function produce sensible interpretations adversarial rubbish images. argue models better mimic human visual perception dnns relus. possible explanation adversarial examples pertaining neural networks linear given explanation follows line thought diﬀerences. result dams large powers interaction vertex energy function dual feed-forward neural nets highly non-linear activation functions rectiﬁed polynomials higher degrees. perspective duality might expect simply replacing relus dnns higher rectiﬁed polynomials might solve problem adversarial rubbish images suﬃciently large power activation function. tried discovered although dnns higher rectiﬁed polynomials alone perform better dnns relus adversarial perspective worse dams update rule observations need comprehensive investigation. thus simply changing relus higher rectiﬁed polynomials enough adversarial problems aspects training algorithm presented section important. thinking neurobiology energy functions higher order interactions considered paper thought eﬀective theories arise excluding auxiliary variables microscopic description. microscopic realization terms biological neurons discussed elsewhere. straightforward ideas possible extensions work. first would interesting complement proposed training procedure adversarial training. words train network using algorithm section combination clean images adversarial images along lines expect increase robustness adversarial examples increase classiﬁcation accuracy clean images. second would interesting investigate proposed methods convolutional setting. naively expects adversarial problems severe fully connected networks convolutional networks. reason used fully connected networks experiments. expect training algorithm section combined convolutional layers better describe images. believe present paper identiﬁed promising computational regime neural networks signiﬁcantly mitigates vulnerability dnns adversarial rubbish images remains little investigated.", "year": 2017}