{"title": "A Novel Stochastic Stratified Average Gradient Method: Convergence Rate  and Its Complexity", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "SGD (Stochastic Gradient Descent) is a popular algorithm for large scale optimization problems due to its low iterative cost. However, SGD can not achieve linear convergence rate as FGD (Full Gradient Descent) because of the inherent gradient variance. To attack the problem, mini-batch SGD was proposed to get a trade-off in terms of convergence rate and iteration cost. In this paper, a general CVI (Convergence-Variance Inequality) equation is presented to state formally the interaction of convergence rate and gradient variance. Then a novel algorithm named SSAG (Stochastic Stratified Average Gradient) is introduced to reduce gradient variance based on two techniques, stratified sampling and averaging over iterations that is a key idea in SAG (Stochastic Average Gradient). Furthermore, SSAG can achieve linear convergence rate of $\\mathcal {O}((1-\\frac{\\mu}{8CL})^k)$ at smaller storage and iterative costs, where $C\\geq 2$ is the category number of training data. This convergence rate depends mainly on the variance between classes, but not on the variance within the classes. In the case of $C\\ll N$ ($N$ is the training data size), SSAG's convergence rate is much better than SAG's convergence rate of $\\mathcal {O}((1-\\frac{\\mu}{8NL})^k)$. Our experimental results show SSAG outperforms SAG and many other algorithms.", "text": "popular algorithm large scale optimization problems iterative cost. however achieve linear convergence rate inherent gradient variance. attack problem mini-batch proposed trade-oﬀ terms convergence rate iteration cost. paper general equation presented state formally interaction convergence rate gradient variance. novel algorithm named ssag introduced reduce gradient variance based techniques stratiﬁed sampling averaging iterations idea furthermore ssag achieve linear convergence rate smaller storage iterative costs category number training data. convergence rate depends mainly ssag’s convergence rate much better sag’s convergence rate ok). experimental results show ssag outperforms recently development increasing popularity deep learning quite routine large data train deep neural network attaining better model applying deep learning practical problems image understanding natural language processing speech recognition training deep model seen optimization problem therefore corresponding large scale optimization problems solved. thus important develop novel optimization algorithm fast convergence rate retaining iteration costs storage requirements train deep model using large data. descent) achieve sub-linear convergence rate existence gradient variance. interesting challenging problem improve sgd’s convergence rate retaining iteration cost. research eﬀort dedicated addressing issue. mini-batch sgd-ss reach linear convergence rate iteration costs increase batch size. svrg saga achieve linear convergence theoretically lose merit practically training size large enough. therefore quite essential control gradient variance eﬀectively retaining iterative costs improve convergence rate gradient methods. paper general presented ﬁrst time state formally interaction convergence rate gradient variance. techniques stratiﬁed sampling averaging history proposed control gradient variance resulting novel algorithm called ssag signiﬁcance approach follows. firstly word stratiﬁed means ssag uses stratiﬁed sampling method instead simple random sampling svrg saga select training example. insight behind reduce harmful gradient variance ﬁrst place using better sampling method. statistics stratiﬁed sampling method smaller design eﬀect simple randomly sampling one. secondly averaging history commonly seen literatures adopted store gradient values calculated diﬀerent iterations compute mean guide algorithm’s search. theoretical experimental results show ssag achieves linear convergence rate independent training data size preserving iteration costs storage requirements. paper organized follows. section introduces ssag algorithm including ssag’s optimization object function iteration formulae pseudo code implementation. section discusses closely-related work literatures. section give main technical theorems. details experiments described section interrelation distinction ssag algorithms discussed section finally conclude paper section proofs main theorems presented appendix. optimized parameters loss function sample number size training data. optimized model three layers neural network input nodes hidden nodes output nodes rs×s×s. without loss generality instead denote object function simplicity. paper focus cases smooth average function strongly-convex. extensive list convex loss functions used deep learning given non-smooth loss functions apply approach adaptively using smooth approximations. update direction ssag determined calculating mean value means needs maintain c-dimensions vector iterations. iteration ssag chooses class randomly calculates mini-batch gradient mean samples class item updated others remain unchanged. proof theorem section shows batch size ssag eﬀect convergence rate. means ssag still linear convergence rate even batch size compared sag’s requirement storing -dimension vector ssag needs store c-dimension vector greatly decreases amount storage especially massive data set. idea behind sample gradient estimator population gradient iteration cost independent data size however achieves sub-linear convergence rate practically existence gradient variance.p mini-batch uses mean sample gradients guiding direction achieve linear convergence rate sample size increases. however terms passes data mini-batch sgd’s faster convergence rate oﬀset higher iteration cost associated next section starting uniﬁed formulae derive inequality equation named stated theorem ﬁrst attempt clarify gradient variance inﬂuences convergence rate exactly. diﬀerent mini-batch sgd’s averaging samples inner iteration averages gradients iterations. iteration randomly selects sample calculates gradient sample stores updates corresponding item -dimension vector item vector calculated diﬀerent iteration. mean value vector parameters’ update direction sag. uses iterations form depends data size case approaching inﬁnity loses advantage fast linear convergence. another limit needs maintain -dimension vector keeping track gradient information calculated diﬀerent iterations storage requirement huge massive data setting. svrg double loop algorithm. applies subtracting idea decrease gradient variance. outer loop svrg computes records full gradient referenced network inner loop svrg calculates gradient diﬀerence current network referenced network randomly sample. finally svrg obtains unbiased estimator guiding direction adding diﬀerence full gradient pre-computed outer loop. ssentially saga midpoint svrg update value time index picked whereas svrg updates batch. similar saga also achieve linear convergence rate )k). result depends data size analysing convergence rate ssag ﬁrstly present general convergent result section refers minibatch sgd. general result gradient variance impacts convergence rate algorithm. following theorem formalizes relationship gradient variance convergence rate smaller gradient variance approaches closer optimal solution gradient variance reduced zero achieve linear convergence rate. theorem general result mini-batch sgd. case sample size equal data size sampling ratio equals leads linear convergence rate fgd. case sample size equal number ceases decay cannot achieve linear convergence rate. mini-batch sample size random number number reduced inﬁnitely close zero position denominator mini-batch converge optimal solution well. conclusion theorem reveals important eﬀectively reduce gradient variation designing novel algorithm. ssag uses techniques averaging history stratiﬁed sampling control gradient variance. ssag independent mini-batch size used stratiﬁed sampling seen proof theorem order inequality equation formulae shrink batch size unity ﬁnal bound means ssag still remains linear convergence rate even batch size unity. theoretical result veriﬁed experimental evidence later about reasonable explanation variance classes instead within classes main factor aﬀecting ssag’s convergence rate variance within classes together batch size ssag. theorem also category number factor ssag’s convercl )k). many deem unreasonable ssag converge faster category number decreases. fact classiﬁcation problems large category number complex small category number. ssag converge faster category number smaller. best convergence rate ssag case category number people also argue possibility ssag’s linear convergence rate without using full gradients especially data size tends inﬁnity. reasonable explanation that category number ﬁxed redundant degree data increasing data size highly redundant data random samples approximate full data arbitrary precision sample capacity large enough relatively small. worth mentioning thatwhen deep neural network working unsupervised learning mode training data instead case convergence rate ssag equal lose linear convergence rate tend inﬁnity. section carry empirical evaluations ssag iterations platform deep learning system. adopted data mnist database handwritten digits contains training examples test examples. ﬁrst compare convergence implementation ssag iterations one. proceed evaluate eﬀect diﬀerent algorithmic conﬁgurations step size mini-batches network’s depth. illustrate ssag’s performance algorithm together ones three layers network input nodes hidden nodes output nodes. pass training samples uniformly randomly drew handwritten pictures constant sampling ratio epoches handwritten pictures test trained networks. record test accuracy networks trained ssag diﬀerent step-size. data collected table training errors further error almost remains level. contrast substantially reduce error even iterations phenomenon explained variance shrinking eﬀect gradient variance decays zero suﬃcient large number iterations. curve ssag favors large step size performs best step size experiments small step size slow learning process. reason optimization direction determined ssag accurate others. relatively large step size acceptable lead region solution space. theoretical analysis asserts convergence rate ssag independent mini-batch size used stratiﬁed samples assertion seems counterintuitive. however justiﬁed experimentation. running ssag three layers’ network step size test performance ssag deployed diﬀerent batch size compare test error curves ssag varying batch size experimental results plotted figure error curves figure drop fast means ssag remains linear convergence rate matter batch size also ssag converges fastest batch size unity. result means ssag cannot beneﬁt much increasing batch size. reason behind convergence rate ssag mainly determined variance classes variance within class little eﬀect convergence rate. function optimized network. neural networks many layers extremely steep regions resembling cliﬀs ssag easily close cliﬀ region batch size ssag also experiments show ssag diﬀerent optimal step-size given batch size optimal step-size increase batch size. figure compare ssag’s performance diﬀerent step-size batch size ﬁxed figure optimal step-size batch size equal reason behind gradient variances within class impacts step-size larger batch size suggests smaller gradient variances within class accurate search direction. case ssag algorithm takes large step-size without deviating paths optimal solutions. further pick best step-size batch-size= optimal step-size batch-size= plot performance curves settings figure curves nearly coincident phenomenon shows gradient variances classes dominate ssag’s convergence rate veriﬁes assertion figure pink curve learning curve ssag batch-size= step-size= blue curve learning curve ssag batch-size= step-size= ..the curves nearly coincident ssag also performs well deep neural networks. upper left picture figure ssag’s performance curves diﬀerent depth network. ssag even train layers deeper model achieve better recognition rate much better model. test performance ssag network step-size varying depth network. comparisons shown three pictures figure comparisons obvious ssag outperforms depth model increases. sgd-ss mini-batch utilize averaging idea reduce gradient variance distinction among lies average over mini-batch sgd-ss average samples iteration works ssag averaging history diﬀerent iterations. well-known sampling theory stratiﬁed sampling method small design eﬀect especially case variance within class samples small. sgd-ss adopts stratiﬁed sampling uniformly sampling used mini-batch reduce variance. averaging iterations makes ssag achieve linear convergence rate retaining sgd’s iteration cost. reason behind that ssag like need calculate sample’s gradient iteration. however needs store historical gradient computed diﬀerent iterations maintain -dimension gradient vector upper case size training data leading huge storage requirement massive data set. ssag needs c-dimension vector thus much smaller )k). theoretically linear convergence rate loses linear convergence advantage approaches inﬁnity. svrg applies completely diﬀerent tactic shrink gradient variance. uses subtracting averaging idea control gradient variance. speciﬁcally svrg needs store network named referenced network. iteration svrg calculates diﬀerence subtracting gradient referenced network randomly selected sample gradient current network sample. diﬀerence added average gradient whole training data pre-computed outer loop determine ﬁnal update direction. role average gradient referenced network keep expectation update direction unbiased. subtracting reduce gradient eﬀective svrg reaches linear convergence rate. compared saga svrg’s convergence rate independent training data size svrg needs maintain referenced network calculate gradient twice randomly selected sample iteration. requirements issue practical situations especially setting large scale data train deep network. saga adds additional operator called prox determine solution satisﬁes sparse property deﬁned given measure. essentially saga midpoint svrg sag. saga svrg share common drawbacks. paper present formulate relationship gradient variance convergence develop novel algorithm called ssag accordingly. ssag utilizes techniques averaging history stratiﬁed sampling reduce gradient variance. leads ssag converging linear rate depends category number instead data size retaining iteration costs storage requirements sgd. outline proof that starting continuity assumption constantly change inequality using known conditions conclusions well strongly convex property thus form decreasing series expectation main proof proof. investigate convergence rate need show k+−w decay iterations. order this need lyapunov function sequence decreases linear rate tion schur complement schur complment submatrix i−bt given symmetrical matrix schur complement condition says positive deﬁnite positive deﬁnite also positive deﬁnite. choose appropriate positive deﬁnite. i−hi−", "year": 2017}