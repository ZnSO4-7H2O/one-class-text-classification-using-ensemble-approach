{"title": "Investigating Recurrence and Eligibility Traces in Deep Q-Networks", "tag": ["cs.AI", "cs.LG"], "abstract": "Eligibility traces in reinforcement learning are used as a bias-variance trade-off and can often speed up training time by propagating knowledge back over time-steps in a single update. We investigate the use of eligibility traces in combination with recurrent networks in the Atari domain. We illustrate the benefits of both recurrent nets and eligibility traces in some Atari games, and highlight also the importance of the optimization used in the training.", "text": "eligibility traces reinforcement learning used bias-variance trade-off often speed training time propagating knowledge back time-steps single update. investigate eligibility traces combination recurrent networks atari domain. illustrate beneﬁts recurrent nets eligibility traces atari games highlight also importance optimization used training. deep reinforcement learning many practical successes game playing silver robotics interest exploring algorithms context environments sparse rewards partial observability. investigate methods known mitigate problems recurrent networks provide form memory summarizing past experiences eligibility traces allow information propagate multiple time steps. eligibility traces shown empirically provide faster learning preparation) deep limited hausknecht stone provide experiments atari domain showing eligibility traces boost performance deep also demonstrate surprisingly strong effect optimization method performance recurrent networks. paper structured follows. sec. provide background notation needed paper. sec. describes algorithms use. sec. present discuss experimental results. sec. conclude present avenues future work. background markov decision process consists tuple states actions reward function transition function discount factor. reinforcement learning framework solving unknown mdps means ﬁnding good behaving also called policy. works obtaining transitions environment using them order compute policy similar state values conditioned also initial action policy derived values picking always action best estimated value state. monte carlo temporal difference standard methods updating value function data. entire trajectory’s return used target value current state. instance off-policy learning agent gathers data exploratory policy randomizes choice action updates estimates constructing targets according differnet policy results target taking yields target. eligibility traces geometric weighting n-step returns weight k-step return times weight -step return. using reduces using n-steps weight appealing effects using eligibility traces single update allows states many steps behind reward signal receive credit. propagates knowledge back faster rate allowing accelerated learning. especially environments rewards sparse and/or delayed eligibility traces help assign credit past states actions. without traces seeing sparse reward propagate value back step turn needs sampled send value back second step viewing eligibility traces called forward view states looking ahead rewards received future. forward view rarely used requires state wait future unfold calculating update requires memory store experience. equivalent view called backward view allows calculate updates every previous state take single action. requires memory lets perform updates without wait future. however view limited success neural network setting requires using trace neuron network tend dense heavily used step resulting noisy signals. reason eligibility traces aren’t heavily used using deep learning despite potential beneﬁts. variant q-learning eligibility traces used calculate error. mentioned previously backwards view traces traditionally used. versions exist used watkins’s q-learning off-policy sequence actions used past trajectory used calculate trace might different actions current policy might take. case using trajectory past point actions differ. handle case watkins’s sets trace action current policy would select different used past. mnih introduced deep q-networks ﬁrst successful reinforcement learning algorithms deep learning function approximation general enough applicable variety environments. applying atari games used convolutional neural network took input last four frames game output q-values possible action. equation shows cost function optimizing parameters. parameters represent frozen q-value weights update chosen frequency. introduced hausknecht stone deep recurrent q-networks modiﬁcation single frames passed generates feature vector ﬁnally outputs q-values. architecture gives agent memory allowing learn long-term temporal effects handle partial observability case many environments. authors showed randomly blanking frames difﬁcult overcome drqn learned handle without issue. train drqn proposed variants experience replay. ﬁrst sample entire trajectories end. however computationally demanding trajectories steps long. second alternative sample sub-trajectories instead single transitions. required needs hidden state allow understand temporal aspect data. stochastic gradient descent generally algorithm used optimize neural networks. however information lost process past gradients might signal weight drastically needs change oscillating requiring decrease learning rate. adaptive algorithms built information. rmsprop uses geometric averaging gradients squared divides current gradient square root. perform rmsprop ﬁrst calculate averaging update parameters introduced variant rmsprop gradient instead divided standard deviation running average. first calculate running averages βm+∇θ βg+∇θ update parameters using rest paper mentioning rmsprop we’ll referring version. finally kingma introduced adam essentially rmsprop coupled nesterov momentum along running averages corrected bias. term rate momentum running averages. calculate update adam start updating averages correct biases pong tennis. architecture used similar used hausknecht stone frames converted gray-scale re-sized ﬁrst layer ﬁlters stride followed ﬁlters stride ﬁlters stride output ﬂattened single dense layer output neurons ﬁnally lstm cells. last linear layer takes output recurrent layer output q-values. layers lstm activated using rectiﬁed linear units mentioned subsection also altered experience replay sample sub-trajectories. backprop time train train sub-trajectory experience. runtime large sequence inputs hidden state problematic always trained empty hidden state. like lample singh chaplot therefore sample slightly longer length trajectory ﬁrst states hidden state. experiments selected trajectory lengths ﬁrst states used ﬁller remaining used traces costs. used batch size experiments using eligibility traces furthermore watkins’s limit computation costs using traces trace becomes small. experiments choose limit allows traces affect states ahead calculate trace every state trajectory except beginning hidden state rnn. using rmsprop used momentum epsilon learning rate using adam used momentum gradients momentum squared gradients epsilon learning rate testing phases consistent across models score average game played frames. also action selection. terminal training complete algorithm deep recurrent q-networks forward view eligibility traces atari. eligibility traces calculated using n-step return function time-step described section first tested model trained rmsprop. figure shows model without trace learned rate model traces learned substantially faster stability without exhibiting epochs depressed performance. probably eligibility traces propagating rewards back many steps single update. pong agent hits ball must wait several time-steps ball gets either past opponent. happens agent must assign credit event back time ball actions performed ball already left. traces clearly help send signal back faster. tested models using adam optimizer instead rmsprop. models learn much faster setting. however model trace gains signiﬁcantly model trace. current intuition hyper-parameters frozen network’s update frequency limiting rate model learn. note also model also learns faster adam optimizer remains quite unstable comparison recurrent models. finally results table show using eligibility traces adam provide performance improvements. training rmsprop model traces gets near optimal performance twice quickly models. adam model learns optimal epochs. table number epochs getting points pong. chose points threshold represents near-optimal strategy. testing performed \u0001-greedy policy stopping agent perfect score. second atari game tested tennis. match consists player ﬁrst \"games\" score ranges given difference number balls players. pong ﬁrst tried trained rmsprop standard learning rate without eligibility traces figure shows models learned optimal scores epochs. contrast never seems able pass threshold large ﬂuctuations ranging visually inspecting games played testing phase noticed agent gets stuck loop exchanges ball opponent timer runs out. case agent minimizes number points scored against never learns beat opponent. score ﬂuctuations depend points agent allows entering loop. suspect agent gets stuck policy reward trying defeat opponent delayed waiting ball reach opponent past furthermore experiences getting point relatively sparse. together makes difﬁcult propagate reward back action hitting ball correctly. also notice without eligibility traces manage learn near-optimal policy without getting stuck policy. capacity sending signal back past states bptt allowing credit assignment implicitly might explain ability escape policy. remarkably algorithm succeeds getting near-optimal scores tennis variants munos wang mnih schaul tend stuck policy delaying. model without traces learned faster pace traces arriving score epochs opposed counterpart. it’s possible updates model traces smaller weighting target values indirectly leading lower learning rate. also trained models rmsprop higher learning rate model traces getting points epochs model without traces lost ability optimal scores never passed threshold. tried using adam optimizer original learning rate models learned substantially faster rmsprop traces getting nearoptimal performance epochs. adam gradient positive stored momentum part equation quite time. momentum gradient part many updates makes enough overtake safe strategy. also notice model traces much stable counterpart. model without traces fell back policy delaying game occasions learned beat opponent. finally trained adam model acted trained rmsprop. paper analyzed effects using eligibility traces different optimization functions. showed eligibility traces improve stabilize learning using adam strongly accelerate learning. shown pong results model using eligibility traces didn’t gain much performance using adam. possible cause frozen network. stabilizing effect stopping policies drastically changing single update also stops newly learned values propagated back. double seems partially around issue allowing policy next state change keeping values frozen. future experiments must consider eliminating increasing frozen network’s update frequency. would also interesting reduce size experience replay increased learning speed observations become off-policy barely used eligibility traces. references marc bellemare yavar naddaf joel veness michael bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research sergey levine pieter abbeel. learning neural network policies guided policy search unknown dynamics. advances neural information processing systems volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature volodymyr mnih adria puigdomenech badia mehdi mirza alex graves timothy lillicrap harley david silver koray kavukcuoglu. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. david silver huang chris maddison arthur guez laurent sifre george driessche julian schrittwieser ioannis antonoglou veda panneershelvam marc lanctot mastering game deep neural networks tree search. nature tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning", "year": 2017}