{"title": "Meta-Reinforcement Learning of Structured Exploration Strategies", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "Exploration is a fundamental challenge in reinforcement learning (RL). Many of the current exploration methods for deep RL use task-agnostic objectives, such as information gain or bonuses based on state visitation. However, many practical applications of RL involve learning more than a single task, and prior tasks can be used to inform how exploration should be performed in new tasks. In this work, we explore how prior tasks can inform an agent about how to explore effectively in new situations. We introduce a novel gradient-based fast adaptation algorithm -- model agnostic exploration with structured noise (MAESN) -- to learn exploration strategies from prior experience. The prior experience is used both to initialize a policy and to acquire a latent exploration space that can inject structured stochasticity into a policy, producing exploration strategies that are informed by prior knowledge and are more effective than random action-space noise. We show that MAESN is more effective at learning exploration strategies when compared to prior meta-RL methods, RL without learned exploration strategies, and task-agnostic exploration methods. We evaluate our method on a variety of simulated tasks: locomotion with a wheeled robot, locomotion with a quadrupedal walker, and object manipulation.", "text": "exploration fundamental challenge reinforcement learning many current exploration methods deep task-agnostic objectives information gain bonuses based state visitation. however many practical applications involve learning single task prior tasks used inform exploration performed tasks. work explore prior tasks inform agent explore effectively situations. introduce novel gradientbased fast adaptation algorithm model agnostic exploration structured noise learn exploration strategies prior experience. prior experience used initialize policy acquire latent exploration space inject structured stochasticity policy producing exploration strategies informed prior knowledge effective random action-space noise. show maesn effective learning exploration strategies compared prior meta-rl methods without learned exploration strategies task-agnostic exploration methods. evaluate method variety simulated tasks locomotion wheeled robot locomotion quadrupedal walker object manipulation. deep reinforcement learning methods shown learn complex tasks ranging games robotic control minimal supervision simply exploring environment experiencing rewards. task becomes complex temporally extended na¨ıve exploration strategies become less effective. devising effective exploration methods therefore critical challenge reinforcement learning. prior works proposed guiding exploration based criteria intrinsic motivation state-visitation counts thompson sampling bootstrapped models optimism face uncertainty parameter space exploration exploration strategies largely task agnostic provide good exploration without exploiting particular structure task itself. however intelligent agent interacting real world likely need learn many tasks case prior tasks used inform exploration tasks performed. example robot tasked learning household chore likely prior experience learning related chores. draw experiences order decide explore environment acquire skill quickly. similarly walking robot previously learned navigate different buildings doesn’t need reacquire skill walking must learn navigate maze simply needs explore space navigation strategies. work study experience multiple distinct related prior tasks used autonomously acquire directed exploration strategies meta-learning. meta-learning learning learn refers problem learning strategies fast adaptation using prior tasks several methods aimed address meta-learning contexts training recurrent models ingest past states actions rewards predict actions maximize rewards task hand. methods ideal learning explore illustrate conceptually empirically. main reasons this. first good exploration strategies qualitatively different optimal policies optimal policy typically deterministic fully observed environments exploration depends critically stochasticity. methods simply recast meta-rl problem problem generally acquire behaviors exhibit insufﬁcient variability explore effectively settings difﬁcult tasks. policy represent highly exploratory behavior adapt quickly optimal behavior becomes difﬁcult typical time-invariant representations action distributions. second many current meta-rl methods learn entire learning algorithm example using recurrent model. allows adapt quickly single forward pass greatly limits asymptotic performance compared learning scratch since learned algorithm generally correspond convergent iterative optimization procedure unlike standard method. address challenges devising meta-rl algorithm adapts tasks following policy gradient also injecting learned structured stochasticity latent space enable effective exploration. algorithm call model agnostic exploration structured noise uses prior experience initialize policy learn latent exploration space sample temporally coherent structured behaviors producing exploration strategies stochastic informed prior knowledge effective random noise. importantly policy latent space explicitly trained adapt quickly tasks policy gradient. since adaptation performed following policy gradient method achieves least asymptotic performance learning scratch structured stochasticity allows randomized task-aware exploration. experimental evaluation shows existing meta-rl methods including maml rnnbased algorithms limited ability acquire complex exploratory policies likely limitations ability acquire strategy stochastic structured policy parameterizations introduce time-invariant stochasticity action space. principle certain based architectures could capture time-correlated stochasticity experimentally current methods fall short. effective exploration strategies must select randomly among potentially useful behaviors avoiding behaviors highly unlikely succeed. maesn leverages insight acquire signiﬁcantly better exploration strategies incorporating learned timecorrelated noise meta-learned latent space training policy parameters latent exploration space explicitly fast adaptation. show combining mechanisms together produces metalearning algorithm learn explore substantially better prior meta-learning methods adapt quickly tasks. experiments able explore coherently adapt quickly number simuexploration fundamental problem simple methods \u0001-greedy gaussian exploration used widely often insufﬁcient environments signiﬁcant complexity delayed rewards. wide range sophisticated strategies proposed based optimism face uncertainty intrinsic motivation thompson sampling information gain parameter space exploration many methods effective improving exploration task agnostic therefore utilize prior knowledge world gained tasks. maesn instead aims incorporate experience distinct structurally similar prior tasks learn exploration strategies. spirit similar parameter-space exploration maesn injects temporally correlated noise randomize exploration strategies noise utilized sampled determined meta-learning process informed past experience. algorithm based framework metalearning aims learn models adapt quickly tasks. meta-learning algorithms learn optimizers update rules entire algorithms methods effective solving supervised learning problems samples approach closely related model-agnostic meta-learning directly trains model parameters adapt quickly standard gradient descent. method beneﬁt allowing similar asymptotic performance learning scratch especially task differs training task still enabling acceleration meta-training. however experiments show maml alone well prior meta-rl methods effective learning explore lack structured stochasticity. proposed method introduces structured stochasticity meta-learning learned latent space. latent space models explored several prior works though context meta-learning learning exploration strategies prior methods generally focus tasks either trials sufﬁcient identify goals task policy acquire consistent search strategy example exit mazes adaptation regimes differ substantially stochastic exploration. tasks discovering goal requires exploration stochastic structured cannot easily captured methods demonstrated experiments. speciﬁcally major shortcomings methods stochasticity policy limited time-invariant noise action distributions fundamentally limexploratory behavior represent. based methods policy limited ability adapt environments since adaptation performed forward pass recurrent network. single forward pass produce good behavior mechanism improvement. methods adapt gradient descent maml simply revert standard policy gradient make slow steady improvement worst case address section introduce novel method learning structured exploration behavior based gradient based meta-learning able learn good exploratory behavior adapt quickly tasks require signiﬁcant exploration. algorithm call model agnostic exploration structured noise combines structured stochasticity maml. maesn gradient-based meta-learning algorithm introduces stochasticity perturbing actions also learned latent space. policy latent space trained meta-learning provide fast adaptation tasks. solving tasks meta-test time different sample generated latent space trial providing structured temporally correlated stochasticity. distribution latent variables adapted task policy gradient updates. ﬁrst show structured stochasticity introduced latent spaces describe policy latent space meta-trained form overall algorithm. rensa kolter methods explicitly train fast adaptation comparisons section illustrate advantages method. concurrently work stadie also explores meta-learning exploration unpublished manuscript introduce structured stochasticity presents results show signiﬁcant improvement maml meta-learning algorithms. contrast results show maesn substantially outperforms prior meta-learning methods. meta-rl consider distribution tasks task different markov decision process state space action space transition distribution reward function reward function transitions vary across tasks. metarl aims learn policy adapt maximize expected reward novel tasks efﬁciently possible. done number ways using gradient descent based methods recurrent models ingest past experience build gradient-based meta-learning framework maml trains model adapt quickly standard gradient descent corresponds policy gradient. meta-training objective maml written intuition behind optimization objective that since policy adapted meta-test time using policy gradient optimize policy parameters step policy gradient improves performance meta-training task much possible. since maml reverts conventional policy gradient faced out-of-distribution tasks provides natural starting point consider design metaexploration algorithm starting method essentially task-agnostic methods learn scratch worst case improve incorporate ability acquire stochastic exploration strategies experience discussed following section typical stochastic policies parameterize action distributions using action distribution independent time step. however representation notion temporally coherent randomness throughout trajectory since noise added independently time step. greatly limits exploratory power since policy essentially changes mind wants explore every time step. distribution also typically represented simple parametric distributions unimodal gaussians restricts ability model task-dependent covariances. temporally coherent exploration behavior allow policy model complex time-correlated stochastic condition policy per-episode random variables drawn learned latent distribution. since latent variables sampled episode provide temporally coherent stochasticity. intuitively policy decides episode sticks plan. furthermore since random sample provided input nonlinear neural network policy transform random variable arbitrarily complex distributions. resulting policies written learnable parameters. structured stochasticity provide coherent exploration sampling entire behaviors goals rather simply relying independent random actions. related policy representations explored prior work however take design step meta-learning latent space efﬁcient adaptation. given latent variable conditioned policy described above goal train capture coherent exploration strategies enable fast adaptation tasks. combination variational inference gradient based meta-learning achieve this. speciﬁcally meta-train policy parameters make latent variables perform coherent exploration task adapt fast possible. learn latent space distribution parameters task optimal performance policy gradient figure computation graph maesn. tasks latent distribution parameters policy parameters updated inner loop meta-learning overall parameters pre-update variational parameters sampling procedure actions introduces time correlated noise conditioning policy latent variable kept ﬁxed episode. action still drawn time-step overall exploration time-correlated adaptation step. procedure encourages policy actually make latent variables exploration. perspective maesn understood augmenting maml latent space inject structured noise different perspective amounts learning structured latent space trained quick adaptation tasks policy gradient. formalize objective meta-training introduce per-task variational parameters deﬁne per-task latent variable distribution task meta-training involves optimizing initial policy parameters shared tasks per-task initial variational parameters ...} maximize expected reward policy gradient update. standard variational inference also objective kl-divergence gaussian distribution corresponding pre-update variational parameters latent variable prior simply unit gaussian. intuitively means every iteration meta-training sample latent variable conditioned policies represented perform inner gradient update variational parameters task post-update parameters metaupdate parameters expected task rewards tasks using updated latent-conditioned policies maximized. maml involves differentiating policy gradient. meta-training inner update corresponds standard reinforce policy gradient straightforward differentiate meta-optimizer powerful trust region policy optimization algorithm updated even initially matches prior match prior update. allows succeed tasks meta-test time good initialization choice begin prior discussed next section. maesn performs variational inference likelihood reward updated policy latent variational parameters although might seem unconventional treat reward values likelihoods made formal equivalence entropymaximizing probabilistic graphical models. full derivation equivalence outside scope work refer reader prior work details note maesn introduces exploration latent space also optimizes policy parameters latent space task reward maximized gradient update. ensures fast adaptation learning tasks policy gradients. consider task reward learned model policy parameters variational parameters exploration task initialize latent distribution prior since regularization drives variational parameters prior meta-training. adaptation task done simply using policy gradient adapt objective terms post-update expected reward task kl-divergence task’s variational parameters prior. values per-parameter step sizes elementwise product. last update optional. found could fact obtain better results simply omitting update corresponds meta-training initial policy parameters simply latent space efﬁciently without training parameters explicitly fast adaptation. including update makes resulting optimization problem challenging found necessary employ stage-wise training procedure case policy ﬁrst meta-trained without update update added. however even case policy’s fast adaptation performance actually improve simply omitting update meta-training procedure experiments. also found meta-learning step size separately parameter crucial achieve good performance. note exploration inner loop happens exploration action space well latent space latent space exploration temporally coherent. maesn objective enables structured exploration noise latent space explicitly training fast adaptation policy gradient. could principle train model without meta-training adaptation resembles model proposed hausman show experimental evaluation meta-training produces substantially better results. interestingly course meta-training variational parameters task usually close prior convergence contrast non-meta-training approach model trained figure left object manipulation robotic gripper pushing various colored blocks goal square. right distribution blocks goals across task distribution indicating task diversity robotic manipulation. goal tasks push blocks target locations robotic hand. task several blocks placed random positions. block relevant task block must moved goal location different tasks distribution require pushing different blocks different positions different goals. state consists positions objects goal position conﬁguration hand. metatraining reward function corresponds negative distance relevant block goal meta-testing sparse reward function reaching goal correct goal provided. coherent exploration strategy pick random blocks move goal location trying different blocks episode discover right one. task generally representative exploration challenges robotic manipulation robot might perform variety different manipulation skills generally motions actually interact objects world useful coherent exploration. full state representation task leaving vision based policies future work. wheeled locomotion. second task distribution consider wheeled robot navigate different goals. task different goal location. robot controls wheels independently using move turn. task family illustrated fig. state contain goal instead agent must explore different locations locate goal own. reward meta-test time provided agent reaches within small distance goal. coherent exploration family tasks requires driving random locations world requires coordinated pattern actions difﬁcult achieve purely action-space noise. full state representation task leaving vision based policies future work. represents rewards along trajectory. since meta-trained adapt inner loop adapt parameters meta-test time well. compute gradients respect need backpropagate sampling operation using either likelihood ratio reparameterization trick. likelihood ratio update ∇µση eat∼π ﬁnal detail meta-learning exploration strategies question rewards. goal adapt quickly sparse delayed rewards meta-test time goal poses major challenge meta-training time tasks difﬁcult learn scratch also difﬁcult solve meta-training time making hard meta-learner make progress. issue could potentially addressed using many samples existing task-agnostic exploration strategies meta-training only even simpler solution introduce amount reward shaping metatraining show experiments exploration strategies metatrained reward shaping actually generalize effectively sparse delayed rewards despite mismatch reward function. viewed kind mild instrumentation meta-training setup could replaced mechanisms large sample sizes task-agnostic exploration bonuses future work. experiments comparatively evaluate metalearning method study following questions meta-learned exploration strategies structured noise explore coherently adapt quickly tasks providing signiﬁcant advantage learning scratch? meta-learning maesn compare prior meta-learning methods maml well latent space learning methods visualize exploration behavior coherent exploration strategies maesn? better understand components maesn critical? evaluated method three task distributions family tasks used distinct meta-training tasks different reward function metatraining particular distribution tasks maesn able study quickly learn behaviors tasks sparse reward meta-trained maesn compared prior methods. since reward function sparse success requires good exploration. plot performance methods terms reward methods obtain adapting tasks drawn validation figure results three tasks discussed show maesn able explore adapt quickly sparse reward environments. comparison maml don’t learn behaviors explore effectively. pure latent spaces model achieves reasonable performance limited terms capacity improve beyond initial identiﬁcation latent space parameters optimized fast gradient-based adaptation latent space. since maesn train latent space explicitly fast adaptation achieve better results faster. also observe that many tasks learning scratch actually provides competitive baseline prior metalearning methods terms asymptotic performance. indicates task distributions quite challenging simply memorizing meta-training tasks insufﬁcient succeed. however cases maesn able outperform learning scratch task-agnostic exploration terms learning speed asymptotic performance. manipulation task learning scratch strongest alternative method achieving asymptotic performance close maesn slowly. challenging legged locomotion task requires coherent walking behaviors random locations world discover sparse rewards maesn able adapt effectively. better understand types exploration strategies learned maesn visualize trajectories obtained sampling meta-learned latent-conditioned policy latent distribution prior resulting trajectories show position hand block pushing task position center mass locomotion tasks. task distributions family tasks shown trajectories learned exploration strategies explore space coherent behaviors broadly effectively especially comparison random exploration standard maml. figure left locomotion wheeled robot navigating goal depicted green sphere right depiction distribution tasks. points indicate various goals agent might need reach task legged locomotion. understand whether scale complex locomotion tasks higher dimensionality last family tasks involves simulated quadruped tasked walk randomly placed goals similar setup wheeled robot. task presents exploration challenge since carefully coordinated motion actually produces movement different positions world ideal exploration strategy would always walk would walk different places. meta-training time agent receives negative distance goal reward meta-test time reward given within small distance goal. figure left legged locomotion quadruped navigating goal depicted green sphere right depiction distribution tasks. points indicate various goals agent might need reach task compare method number prior methods meta-learning multi-task learning learning scratch using task agnostic exploration strategies. compare maesn maml simply learning latent spaces without fast adaptation analogously hausman discussed section learning explore requires actually solving exploration problem meta-training tasks exploration extremely challenging still challenging meta-training time. reason methods provided dense rewards meta-training meta-testing still done sparse rewards. training scratch compare trpo reinforce training scratch vime details comparisons experimental setup found appendix. figure learning progress novel tasks sparse rewards wheeled locomotion legged locomotion object manipulation tasks. rewards averaged validation tasks sparse rewards described supplementary materials. maesn learns signiﬁcantly better policies learns much quicker prior meta-learning approaches learning scratch.on robotic manipulation task vime performs worse trpo implementation trpo built performs poorly. performance base implementation plotted trpo comparison figure block manipulation latent distributions update maesn visualized latent space. ellipses represent latent distributions left pre-update latents right post update latents. pre-update latents driven prior adapted different post update latents policy gradient figure plot exploration behavior visualizing position manipulator locomotion maesn maml random initialization. block manipulation middle locomotion bottom wheeled locomotion. goals indicated translucent overlays. maesn better captures task distribution methods. investigate structure learned latent space object manipulation task visualizing pre-update post-update parameters latent space. variational distributions plotted ellipses space. seen pre-update parameters driven prior post-update parameters move different locations latent space adapt respective tasks. indicates metatraining process effectively utilizes latent variables also effectively minimizes kl-divergence prior ensuring initializing prior task still produce effective exploration. figure role structured noise exploration maesn legged robot. left visitations using structured noise. right visitations structured noise. increased spread exploration wider trajectory distribution suggests structured noise used. better understand importance components maesn evaluate whether noise injected latent space learned maesn actually used exploration. observe exploratory behavior displayed policy trained maesn latent variable kept ﬁxed compared sampled learned latent distribution. fig. that although random exploration even without latent space sampling range trajectories substantially broader sampled prior. detailed ablation study found supplementary materials. learning learned latent exploration space. maesn learns latent space used inject temporally correlated coherent stochasticity policy explore effectively meta-test time. intelligent coherent exploration strategy must randomly sample among useful behaviors omitting behaviors never useful. experimental evaluation illustrates maesn precisely this outperforming prior meta-learning methods algorithms learn scratch including methods task-agnostic exploration strategies based intrinsic motivation it’s worth noting however approach mutually exclusive intrinsic motivation fact promising direction future work would combine approach novelty based exploration methods vime pseudocount-based exploration potentially metalearning aiding acquisition effective intrinsic motivation strategies. authors would like thank chelsea finn gregory kahn ignasi clavera thoughtful discussions justin marvin zhang comments early version paper. work supported national science foundation graduate research fellowship abhishek gupta pecase award pieter abbeel national science foundation iis- well young investigator program award sergey levine. references andrychowicz marcin denil misha colmenarejo sergio gomez hoffman matthew pfau david schaul freitas nando. learning learn gradient descent gradient descent. daniel sugiyama masashi luxburg ulrike guyon isabelle garnett roman nips bellemare marc srinivasan sriram ostrovski georg schaul saxton david munos r´emi. unifying count-based exploration intrinsic motivation. nips bengio samy bengio yoshua cloutier jocelyn gecsei jan. optimization synaptic learning rule. levine elsberry optimality biological artiﬁcial networks. lawrence erlbaum chapelle olivier lihong. empirical evaluation thompson sampling. shawe-taylor zemel bartlett pereira weinberger advances neural information processing systems curran associates inc. finn chelsea tianhe zhang tianhao abbeel pieter levine sergey. one-shot visual imitation learnst annual conference meta-learning. robot learning corl mountain view california november proceedings fortunato meire azar mohammad gheshlaghi piot bilal menick jacob osband graves alex mnih vlad munos r´emi hassabis demis pietquin olivier blundell charles legg shane. noisy networks exploration. corr abs/. hausman karol springenberg jost tobias ziyu wang nicolas heess riedmiller martin. learning embedding space transferable robot skills. proceedings international conference learning representations iclr lillicrap timothy hunt jonathan pritzel alexander heess nicolas erez tassa yuval silver david wierstra daan. continuous control deep reinforcement learning. corr abs/. lopes manuel lang tobias toussaint marc yves oudeyer pierre. exploration model-based reinforcement learning empirically estimating learning progress. nips. mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature plappert matthias houthooft rein dhariwal prafulla sidor szymon chen richard chen asfour tamim abbeel pieter andrychowicz marcin. parameter space noise exploration. corr abs/. santoro adam bartunov sergey botvinick matthew wierstra daan lillicrap timothy meta-learning memory-augmented neural networks. balcan maria-florina weinberger kilian icml schmidhuber jurgen. evolutionary principles selfreferential learning. learning learn metameta-meta...-hook. diploma thesis technische universitat munchen germany stadie bradly yang houthooft rein chen duan yuhuai abbeel pieter sutskever ilya. considerations learning explore meta-reinforcement learning. https //openreview.net/pdf?id=skkjmw. vinyals oriol blundell charles lillicrap kavukcuoglu koray wierstra daan. matching networks shot learning. daniel sugiyama masashi luxburg ulrike guyon isabelle garnett roman nips wang jane kurth-nelson tirumala dhruva soyer hubert leibo joel munos r´emi blundell charles kumaran dharshan botvinick matthew. learning reinforcement learn. corr abs/. built implementation open source implementation rllab maml policies feedforward policies layers hundred units relu nonlinearities. performed meta-training single step adaptation though longer could done principle. since maesn introduces number components adaptive step size latent space exploration framework maml perform ablations make major difference. adding learned latent space explored latent space stochastic making non-helpful exploration. figure ablation study comparing adaptation performance novel tasks maesn number variants maml using bias transformation adaptive stepsize combination found although adding bias transformation maml helpful match performance maesn. variants considered standard maml maml bias transform adaptive stepsize adapting parameters inner update maml bias transform adaptive stepsize adapting bias parameters inner update xobj xgoal xobj xgoal xcom xgoal xobj xgoal xobj xgoal xobj xgoal −cmax uninformative large negative constant reward. reward uninformative agent/object", "year": 2018}