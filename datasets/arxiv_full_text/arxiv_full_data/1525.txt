{"title": "Inducing a Semantically Annotated Lexicon via EM-Based Clustering", "tag": ["cs.CL", "cs.AI", "cs.LG", "I.2.6; I.2.7; I.5.3"], "abstract": "We present a technique for automatic induction of slot annotations for subcategorization frames, based on induction of hidden classes in the EM framework of statistical estimation. The models are empirically evalutated by a general decision test. Induction of slot labeling for subcategorization frames is accomplished by a further application of EM, and applied experimentally on frame observations derived from parsing large corpora. We outline an interpretation of the learned representations as theoretical-linguistic decompositional lexical entries.", "text": "present technique automatic induction slot annotations subcategorization frames based induction hidden classes framework statistical estimation. models empirically evalutated general decision test. induction slot labeling subcategorization frames accomplished application applied experimentally frame observations derived parsing large corpora. outline interpretation learned representations theoretical-linguistic decompositional lexical entries. important challenge computational linguistics concerns construction large-scale computational lexicons numerous natural languages large samples language available. resnik initiated research automatic acquisition semantic selectional restrictions. ribas presented approach takes account syntactic position elements whose semantic relation acquired. however following approaches require prerequisite ﬁxed taxonomy semantic relations. problem entailment hierarchies presently available languages regard open question whether degree existing designs lexical hierarchies appropriate representing lexical meaning. considerations suggest relevance inductive experimental approaches construction lexicons semantic information. statistical subcat-induction system estimates probability distributions corpus frequencies pairs head subcat frame statistical parser also collect frequencies nominal ﬁllers slots subcat frame. induction labels slots frame based upon estimation probability distribution tuples consisting class label selecting head grammatical relation ﬁller head. class label treated hidden data emframework statistical estimation. clustering approach classes derived directly distributional data—a sample pairs verbs nouns gathered parsing unannotated corpus extracting ﬁllers grammatical relations. semantic classes corresponding pairs viewed hidden variables unobserved data context maximum likelihood estimation incomplete data algorithm. approach allows work mathematically well-deﬁned framework statistical inference i.e. standard monotonicity convergence results algorithm extend method. main tasks em-based clustering induction smooth probability model data automatic discovery class-structure data. aspects respected application lexicon induction. basic ideas em-based clustering approach presented rooth approach contrasts merely heuristic empirical justiﬁcation similarity-based approaches clustering clear probabilistic interpretation given. probability model found earlier pereira however contrast approach statistical inference method clustering formalized clearly em-algorithm. approaches probabilistic clustering similar presented recently saul pereira hofmann puzicha also em-algorithms similar probability models derived applied simpler tasks involving combination em-based clustering models lexicon induction experiment. applications clustering model rooth seek derive joint distribution verbnoun pairs large sample pairs verbs nouns idea view conditioned hidden class classes given prior interpretation. semantically smoothed probability pair deﬁned class model follows. given sample space observed incomplete data corresponding pairs sample space unobserved complete data corresponding triples complete data related observation complete-data speciﬁcation corresponding joint probability parameter-vector θnc|c incomplete data speciﬁcation related complete-data speciﬁcation marginal probability prescribed algorithm parameters estimated indirectly proceeding iteratively terms complete-data estimation auxiliary function conditional expectation complete-data log-likelihood given observed data current parameter values auxiliary function iteratively maximized function iteration deﬁned pating grammatical relations intransitive transitive verbs subjectobject-ﬁllers. data gathered maximal-probability parses head-lexicalized probabilistic context-free grammar gave british national corpus intuitively conditional expectation number times particular choice made derivation prorated conditionally expected total number times choice kind made. shown baum expectations calculated eﬃciently using dynamic programming techniques. every maximization step increases log-likelihood function sequence re-estimates eventually converges maximum fig. shows induced semantic class model classes. listed probable nouns distribution probabilities left probable verbs distribution. class index. verb-noun pairs seen training data appear class matrix. verbs suﬃx indicate subject slot active intransitive. similarily .aso denotes subject slot active transitive .aso denotes object slot active transitive. choices made. fig. shows evaluation results models trained iterations averaged starting values plotted class cardinality. diﬀerent starting values eﬀect performance test. obtained value accuracy models classes. models classes show small stable overﬁtting eﬀect. second experiment addressed smoothing power model counting number pairs possible combinations verbs nouns received positive joint probability model. space clustering models included million combinations; approximated smoothing size model randomly sampling pairs returning percentage positively assigned pairs random sample. fig. plots smoothing results models number classes. starting values inﬂuence performance. given proportion number types training corpus -space without clustering smoothing power whereas example model classes iterations smoothing power corresponding maximum likelihood paradigm number training iterations decreasing eﬀect smoothing performance whereas accuracy pseudodisambiguation increasing number iterations. found number iterations good compromise trade-oﬀ. thus discussion actually consists combination verb subcat frame slot induced classes often basis lexical semantics; class interpreted clustering agents denoted proper names woman together verbs denoting communicative action. fig. shows cluster involving verbs scalar change things move along scales. fig. interpreted involving different dispositions modes execution. evaluated clustering models pseudo-disambiguation task similar performed pereira diﬀering detail. task judge verbs likely take given noun argument pair original corpus pair constructed pairing randomly chosen verb combination completely unseen. thus test evaluates well models generalize unseen verbs. data test built follows. constructed evaluation corpus triples randomly cutting test corpus pairs original corpus tokens leaving training corpus tokens. noun test corpus combined verb randomly chosen according frequency pair appear neither training test corpus. however elements required part training corpus. furthermore restricted verbs nouns evaluation corpus ones occurred least times times verb-functor training corpus. resulting evaluation triples used evaluate sequence clustering models trained training corpus. clustering models evaluated parameterized starting values training algorithm number classes model number iteration steps resulting sequence models. starting lower bound random choice accuracy calculated number times model decided induce latent classes subject slot ﬁxed intransitive verb following statistical inference step performed. given latent class model verb-noun pairs sample subjects ﬁxed intransitive verb calculate probability arbitrary subject estimation parameter-vector hθc|c formalized framework viewing function ﬁxed plc. re-estimation formulae resulting incomplete data estimation probability functions following form frequency sample subjects ﬁxed verb) similar induction process applied also pairs nouns thus enabling induction latent semantic annotations transitive verb frames. given model verb-noun pairs sample noun arguments ﬁxed transitive verb calculate probability noun argument pairs again parameter-vector hθcc|c formalized framework viewing function ﬁxed plc. re-estimation formulae resulting incomplete data estimation problem following simple form experiments used model classes. maximal probability parses british national corpus derived statistical parser extracted frequency tables intransitive verb/subject pairs transitive verb/subject/object triples. frequent verbs selected slot labeling. fig. shows verbs probable class label class earlier described communicative action together estimated frequencies nouns estimated frequency highest. verbs semantically coherent. compared levin top-level verb classes found agreement classiﬁcation class verbs changes state except last three verbs list fig. sorted probability class label. similar results german intransitive scalar motion verbs shown fig. data experiments extracted maximal-probability parses million word corpus german subordinate clauses yielding tokens pairs verbs adjectives nouns. lexicalized probabilistic grammar german used described beil compared german example scalar motion verbs linguistic classiﬁcation verbs given schuhmacher found agreement classiﬁcation class einfache anderungsverben except verbs anwachsen stagnieren classiﬁed all. fig. shows probable pair classes increase transitive verb together estimated frequencies head ﬁller pair. note object label class found intransitive scalar motion verbs; correspondence exploited next section. figure first tree linguistic lexical entry transitive verb increase. second corresponding lexical entry induced classes relational constants. third indexed open class root added conjunct transitive scalar motion increase. fourth induced entry related intransitive increase. causative/inchoative verb increase composed actor/causative verb combining one-place predicate structure left fig. linguistically representations motivated argument alternations case linking deep word order language acquistion scope ambiguity desire represent aspects lexical meaning fact languages postulated decomposed representations overt primitive predicate corresponding morpheme. references recent discussion kind theory hale keyser kural sketch understanding lexical representations induced latent-class labeling terms linguistic theories mentioned above aiming interpretation combines computational learnability linguistic motivation denotational-semantic adequacy. basic idea latent classes computational models atomic relation symbols occurring lexical-semantic representations. ﬁrst implementation consider replacing relation symbols ﬁrst tree fig. relation symbols derived latent class labeling. second tree relation symbols indices derived labeling procedure sect. representations semantically interpreted standard ways instance interpreting relation symbols denoting relations events individuals. representations semantically inadequate reasons given philosophical critiques decomposed linguistic representations; fodor recent discussion. lexicon estimated many primitive relations latent classes. guess hundred classes approximately complete lexicon fodor’s arguments based limited degree genuine interdeﬁnability lexical items putnam’s arguments contextual determination lexical meaning indicate number basic concepts order magnitude lexicon itself. concretely lexicon constructed along principles would identify verbs labelled latent classes; instance might identify representations grab touch. reasons semantically adequate lexicon must include additional relational constants. meet requirement simple including conjunct unique constant derived open-class root third tree fig. introduce indexing open class root order homophony open class roots result common conjuncts semantic representations—for instance don’t want senses decline exempliﬁed decline proposal decline percent common entailment represented common conjunct. indexing method works long labeling process produces diﬀerent latent class labels diﬀerent senses. last tree fig. learned representation scalar motion sense intransitive verb increase. approach learning argument alternation relating transitive increase intransitive increase amounts learning repproposed procedure maps observations subcategorization frames complement ﬁllers structured lexical entries. believe method scientiﬁcally interesting practically useful ﬂexible because model induction algorithm foundations theory parameterized families probability distributions statistical estimation. exempliﬁed paper learning disambiguation evaluation given simple motivated formulations. derived lexical representations linguistically interpretable. suggests possibility large-scale modeling observational experiments bearing questions arising linguistic theories lexicon. used induced lexical entries could incorporated lexicalized syntax-based probabilistic language models particular head-lexicalized models. provides potential application many areas. method applicable natural language text samples suﬃcient size computational morphology robust parser capable extracting subcategorization frames ﬁllers available.", "year": 1999}