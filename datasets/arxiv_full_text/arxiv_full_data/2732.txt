{"title": "Sparse Markov Decision Processes with Causal Sparse Tsallis Entropy  Regularization for Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In this paper, a sparse Markov decision process (MDP) with novel causal sparse Tsallis entropy regularization is proposed.The proposed policy regularization induces a sparse and multi-modal optimal policy distribution of a sparse MDP. The full mathematical analysis of the proposed sparse MDP is provided.We first analyze the optimality condition of a sparse MDP. Then, we propose a sparse value iteration method which solves a sparse MDP and then prove the convergence and optimality of sparse value iteration using the Banach fixed point theorem. The proposed sparse MDP is compared to soft MDPs which utilize causal entropy regularization. We show that the performance error of a sparse MDP has a constant bound, while the error of a soft MDP increases logarithmically with respect to the number of actions, where this performance error is caused by the introduced regularization term. In experiments, we apply sparse MDPs to reinforcement learning problems. The proposed method outperforms existing methods in terms of the convergence speed and performance.", "text": "abstract—in paper sparse markov decision process novel causal sparse tsallis entropy regularization proposed. proposed policy regularization induces sparse multi-modal optimal policy distribution sparse mdp. full mathematical analysis proposed sparse provided. ﬁrst analyze optimality condition sparse mdp. then propose sparse value iteration method solves sparse prove convergence optimality sparse value iteration using banach ﬁxed point theorem. proposed sparse compared soft mdps utilize causal entropy regularization. show performance error sparse constant bound error soft increases logarithmically respect number actions performance error caused introduced regularization term. experiments apply sparse mdps reinforcement learning problems. proposed method outperforms existing methods terms convergence speed performance. used mathematical framework solve stochastic sequential decision problems autonomous driving path planning quadrotor control general goal optimal policy function maximizes expected return. expected return performance measure policy function often deﬁned expected discounted rewards. often used formulate reinforcement learning aims optimal policy without explicit speciﬁcation stochasticity environment inverse reinforcement learning whose goal search proper reward function explain behavior expert follows underlying optimal policy. optimal solution deterministic policy desirable apply problems multiple optimal actions. perspective knowledge multiple optimal actions makes possible cope unexpected situations. example suppose autonomous vehicle multiple optimal routes reach given goal. trafﬁc accident occurs currently selected optimal route possible avoid accident choosing another safe optimal route without additional computation optimal route. reason desirable learn possible optimal actions terms robustness policy function. perspective since experts often make multiple decisions situation deterministic policy limitation expressing expert’s behavior. reason indispensable model policy function expert multi-modal distribution. reasons give rise necessity multi-modal policy model. order address issues deterministic policy function causal entropy regularization method utilized mainly fact optimal solution causal entropy regularization becomes softmax distribution state-action values i.e. exp)) often referred soft softmax distribution widely used model stochastic policy weakness modeling policy function number actions large. words policy function modeled softmax distribution prone assign non-negligible probability mass non-optimal actions even state-action values actions dismissible. tendency gets worse number actions increases demonstrated figure paper propose sparse presenting novel causal sparse tsallis entropy regularization method interpreted special case tsallis generalized entropy proposed regularization method unique property resulting policy distribution becomes sparse distribution. words supporting action non-zero probability mass contains sparse subset action space. provide full mathematical analysis proposed sparse mdp. ﬁrst derive optimality condition sparse named sparse bellman equation. show sparse bellman equation approximation original bellman equation. interestingly connection optimality condition sparse probability simplex projection problem present sparse value iteration method solving sparse problem optimality convergence proven using banach ﬁxed point theorem analyze performance gaps expected return optimal policies obtained sparse soft compared original mdp. particular prove performance proposed sparse original constant bound number actions increases whereas performance soft original grows logarithmically. property sparse mdps beneﬁts soft mdps fig. -dimensional multi-objective environment point mass dynamics. state location action velocity bounded left ﬁgure shows reward four maxima action space discretized levels middle ﬁgure shows optimal action value state indicated cross point number action ﬁrst third ﬁgure indicate proposed policy distributions state induced action values second fourth ﬁgure show performance difference proposed policy optimal policy state number action respectively. larger error brighter color state. ﬁgures obtained replacing proposed policy softmax policy. example shows proposed policy model less affected number actions increases. used soft causal entropy regularization introduced obtain multi-modal policy distribution i.e. since causal entropy regularization penalizes deterministic distribution makes optimal policy soft softmax distribution. soft formulated follows maximize subject validate effectiveness sparse apply proposed method exploration strategy update rule q-learning compare \u0001-greedy method softmax policy proposed method also compared deep deterministic policy gradient method designed operate continuous action space without discretization. proposed method shows state performance compared methods discretization level action space increases. markov decision process widely used formulate sequential decision making problem. characterized tuple {sfa state space corresponding feature space action space distribution initial state transition probability taking discount factor reward function. objective γtr|π policy mapping state space action space. notational simplicity denote expectation discounted summation function i.e. function state action reward function indicator function {s=s}. also denote expectation discounted summation function conditioned eπ|s finding optimal policy formulated follows value function expected discounted rewards initial state given state-action value function expected discounted rewards initial state action given respectively. note optimal solution deterministic function referred deterministic policy. ﬁrst introduce notations properties used paper. table notations deﬁnitions summarized. utility value state visitation compactly expressed terms vectors matrices sparse bellman equation derived necessary conditions optimal solution sparse mdp. carefully investigate karush kuhn tucker conditions indicate necessary conditions solution optimal regularity conditions feasible satisﬁed. feasible sparse satisﬁes linearity constraint qualiﬁcation since feasible consists linear aﬁne functions. regards optimal solution sparse necessarily satisfy conditions follows. theorem policy distribution optimal solution sparse corresponding sparse necessarily satisfy following equations value function state action pairs cardinality full proof theorem provided appendix a-a. proof depends condition derivative lagrangian objective function respect policy becomes zero optimal solution stationary condition. shown optimal solution obtained sparse assigns zero probability action whose action value threshold optimal policy assigns positive probabilτ near optimal actions proportion action values threshold determines range near optimal actions. property makes optimal policy sparse distribution prevents performance drop caused assigning non-negligible positive probabilities non-optimal actions often occurs soft mdp. deﬁnitions observe interesting connection sparse bellman equation probability simplex projection problem log)|s soft value indicating expected rewards including entropy policy obtained starting state qsof soft state-action value expected rewards obtained starting state taking action note optimal policy distribution softmax distribution. soft value iteration method also proposed optimality soft value iteration proved. using causal entropy regularization optimal policy distribution soft able represent multi-modal distribution. causal entropy regularization effect making resulting policy soft closer uniform distribution number actions increases. handle issue propose novel regularization method whose resulting policy distribution still multiple modes performance loss less softmax policy distribution. regularization coefﬁcient. ﬁrst derive sparse bellman equation necessary condition observing connection sparse bellman equation probability simplex projection show optimal policy becomes sparsemax distribution sparsity controlled addition present sparse value iteration algorithm optimality guaranteed using banach’s ﬁxed point theorem. detailed derivations lemmas theorems paper found appendix expected state reward policy regularization approximation value iteration operator state visitation state action visitation transition probability given supporting sparse actions nonzero probabilities cardinality controlled regularization coefﬁcient supporting soft always entire action space. sparse actions assigned non-zero probability must satisfy following inequality probability simplex projection well known problem projecting d-dimensional vector dimensional probability simplex euclidean metric sense. probability simplex projection problem deﬁned follows interestingly optimal solution supporting supp precisely matched sparse bellman equation observation shown optimal policy distribution sparse probability simplex. projection note refer sparsemax distribution. cardinality supporting increases since action values satisfy increase. conversely decreases supporting decreases. extreme cases goes zero included goes inﬁnity entire actions included hand soft supporting softmax distribution cannot controlled regularization coefﬁcient even sharpness softmax distribution adjusted. property makes sparse mdps advantage soft mdps since give zero probability non-optimal actions controlling notion tsallis entropy introduced tsallis general extension entropy tsallis entropy widely used describe thermodynamic systems molecular motions. surprisingly proposed regularization closely related special case tsallis entropy. tsallis entropy deﬁned follows section propose algorithm solving causal sparse tsallis entropy regularized problem. similar original soft sparse version value iteration induced sparse bellman equation. ﬁrst deﬁne sparse bellman operation r|s| r|s| section prove convergence optimality sparse value iteration method. ﬁrst show monotonic discounting properties using properties prove contraction. then banach ﬁxed point theorem repeatedly applying arbitrary initial point always converges unique ﬁxed point. lemma monotone r|s| indicates element-wise inequality. lemma constant r|s| vector ones. full proofs found appendix a-e. proofs lemma lemma rely bounded property sparsemax operation. possible prove sparse bellman operator contraction using lemma lemma follows lemma γ-contraction mapping unique ﬁxed point deﬁnition. prove bounds performance policy obtained sparse policy obtained original performance error caused regularization. boundedness plays crucial role prove error bounds. performance bounds derived bounds sparsemax. similar approach applied prove error bounds soft since log-sum-exp function also bounded approximation operation. comparison log-sum-exp sparsemax operation provided appendix explaining performance error bounds introduce useful propositions employed prove performance error bounds sparse soft mdp. ﬁrst prove important fact shows optimal values sparse value iteration soft value iteration greater original mdp. lemma bellman operations original soft respectively that state proofs theorem theorem found appendix a-f. error bounds show expected return optimal policy sparse always tighter error bounds soft mdp. moreover also known bounds proposed sparse converges constant number actions increases whereas error bounds soft grows logarithmically. property clear beneﬁt sparse applied robotic problem continuous action space. apply continuous action space discretization action space essential discretization required obtain solution closer underlying continuous optimal policy. accordingly number actions becomes larger level discretization increases. case sparse advantages soft performance error sparse bounded constant factor number actions increases whereas performance error optimal policy soft grows logarithmically. section ﬁrst propose sparse q-learning extend sparse deep q-learning sparsemax policy sparse bellman equation employed exploration method update rule. sparse q-learning model free method solve proposed sparse without knowledge transition probabilities. words transition probability unknown sampling possible sparse q-learning estimates optimal sparse using sampling q-learning ﬁnds approximated value optimal conventional mdp. similar q-learning update equation sparse q-learning derived detailed proof provided appendix a-f. lemma obtained shows optimal values sparse value iteration soft value iteration always greater original optimal value intuitively speaking reason inequality regularization term i.e. added objective function. sample action excute observe next state reward experiences replay memory initial importance weight sample mini-batch based importance weight target value γαspmax using gradient descent method ﬁrst verify theorem theorem effect simulation. veriﬁcation theorem theorem measure performance expected return increasing number actions |a|. veriﬁcation effect cardinality supporting optimal policies sparse soft compared different values investigate effectiveness proposed method test sparsemax exploration sparse bellman update rule reinforcement learning continuous action space. apply q-learning continuous action space discretization necessary obtain solution closer original continuous optimal policy. level discretization increases number actions explored becomes larger. regards efﬁcient exploration method required obtain high performance. compare method exploration methods respect convergence speed expected rewards. check effect update rule. experiments performance bounds supporting verify theorem performance error bounds create transition model discretization unicycle dynamics deﬁned continuous state action space solve original soft sparse predeﬁned rewards increasing discretization level action space. reward function deﬁned linear combination squared exponential functions i.e. location unicycle goal point point avoid scale parameters. reward function designed agent navigate towards avoiding absolute value differences expected return original sparse measured. shown figure performance sparse converges constant bound performance soft grows logarithmically. note performance gaps sparse soft always smaller error bounds. supporting experiments conducted using discretized unicycle dynamics. cardinality optimal policies measured varies figure ratio supporting soft changed ratio sparse changed demonstrating sparseness proposed sparse mdps compared soft mdps. test method mujoco physics-based simulator using problems continuous action space inverted pendulum reacher. action space discretized apply q-learning continuous action space experiments conducted four different discretization levels validate effectiveness sparsemax exploration sparse bellman update rule. fig. performance calculated absolute value difference performance sparse soft performance original mdp. ratio number supporting actions total number actions shown. action space unicycle dynamics discretized actions. indicates number iterations learning then number samples increases inﬁnity sparse q-learning converges optimal solution sparse mdp. proof convergence optimality sparse q-learning standard qlearning proposed sparse q-learning easily extended sparse deep q-learning using deep neural network estimator sparse value. iteration sparse deep q-learning performs gradient descent step minimize squared loss parameter network. here target value deﬁned follows moreover employ sparsemax policy exploration strategy policy distribution computed action values estimated deep network. sparsemax policy excludes action whose estimated action value re-explored assigning zero probability mass. effectiveness sparsemax exploration investigated section vii. stable convergence network utilize double qlearning parameter obtaining policy parameter computing target value separated updated every predetermined iterations. words double q-learning prevents instability deep q-learning slowly updating target value. prioritized experience replay also applied optimization network proceeds consideration importance experience. whole process sparse deep q-learning summarized algorithm fig. inverted pendulum problem. algorithms named <exploration method>+<update rule>+<α>. average performance algorithm episodes. performance ddpg scale. average number episodes required reach threshold value original mdp. also proven performance sparse strictly smaller soft mdp. experiments veriﬁed theoretical performance gaps sparse soft original correct. applied sparsemax policy sparse bellman equation deep q-learning exploration strategy update rule respectively shown proposed exploration method shows signiﬁcantly better performance compared \u0001-greedy softmax exploration ddpg number actions increases. analysis experiments demonstrated proposed sparse efﬁcient alternative problems large number possible actions even continuous action space. compare sparsemax exploration method \u0001greedy method softmax exploration compare sparse bellman update rule original bellman update rule soft bellman update rule addition three different regularization coefﬁcient settings experimented. total test combinations variants deep q-learning combining three exploration methods three update rules three different regularization coefﬁcients deep deterministic policy gradient method operates continuous action space without discretization action space also compared. hence total algorithms tested. results shown figure figure inverted pendulum reacher respectively algorithms plotted point graph obtained averaging values three independent runs different random seeds. results algorithms provided appendix network dimensional hidden layers used inverted pendulum problem network four dimensional hidden layers used reacher problem. q-learning algorithm utilizes network topology. inverted pendulum since problem easier reacher problem algorithms converge maximum return discretization level shown figure four algorithms utilize proposed sparsemax exploration. methods utilizes softmax exploration. figure number episodes required reach near optimal return shown. sparsemax exploration requires less number episodes obtain near optimal value \u0001-greedy softmax exploration. reacher problem algorithms sparsemax exploration slightly outperforms \u0001-greedy methods performance softmax exploration included shown figure terms number required episodes sparsemax exploration outperforms epsilon greedy methods shown figure threshold return ddpg shows poor performances problems since number sampled episodes insufﬁcient. regards deep q-learning sparsemax exploration outperforms ddpg less number episodes. experiments known sparsemax exploration method advantage softmax exploration \u0001-greedy method ddpg respect number episodes required reach optimal performance. paper proposed novel causal sparse tsallis entropy regularization induces sparse multi-modal optimal policy distribution. addition provided full mathematical analysis proposed sparse mdps optimality condition sparse mdps given sparse bellman equation sparse value iteration convergence optimality properties performance bounds propose section prove lower upper bounds spmax deﬁned would like mention proof lower bound provided however another interesting prove using cauchy-schwartz inequality nonnegative property quadratic equation. ﬁrst prove spmax next prove spmax without loss generality assume original inequalities simply obtained replacing lower bound sparsemax operation. spmax holds. lagrangian multipliers equality inequality constraints respectively feasibility primal variables feasibility dual variables complementary slackness stationarity condition. lagrangian function written follows ﬁrst third terms quadratic always nonnegative. second term also always nonnegative cauchy-schwartz inequality. cauchy-schwartz ||p||||q||. inequality written then setting dimensional vector ones shown second term nonnegative. therefore spmax always nonnegative since three remaining terms always nonnegative completing proof. would like note sparsemax tighter bounds log-sum-exp always satisﬁed that log. intuitively approximation error logd− sum-exp increases dimension input space increases. however approximation error sparsemax approaches dimension input space goes inﬁnity. fact plays crucial role comparing performance error bounds sparse soft mdp. proof lemma first prove γcontraction mapping respect dmax. without loss generality proof discussed general function r|s| r|s| discounting monotone properties. dmax then satisﬁed. monotone discounting properties following inequality mappings established. proof theorem sparse value iteration converges ﬁxed point contraction property. ﬁxed point deﬁnition point satisﬁes sparse bellman equation i.e. hence theorem satisﬁes necessity conditions optimal solution. banach ﬁxed point theorem unique point satisﬁes necessity conditions optimal solution. particular precisely equivalent sparse bellman equation. words point satisﬁes sparse bellman equation. therefore optimal value sparse mdp. section prove performance error bounds sparse value iteration soft value iteration. ﬁrst show optimal values sparse soft greater original mdp. smart fixed point theorems. archive vol. lillicrap hunt pritzel heess erez tassa silver wierstra continuous control deep reinforcement learning arxiv preprint arxiv. constraint qualiﬁcations necessary optimality conditions optimization problems variational inequality constraints siam journal optimization vol. section present full experimental results reinforcement learning continuous action space. performe experiments inverted pendulum reacher algorithms tested including sparse exploration method sparse bellman update rule. brechtel gindele dillmann probabilistic decision-making uncertainty autonomous driving using continuous pomdps international conference intelligent transportation systems october schulman abbeel chen equivalence policy gradients soft q-learning arxiv preprint arxiv. tokic palm value-difference based exploration adaptive control epsilon-greedy softmax advances artiﬁcial intelligence annual german conference october number action sparse+sparsebellman- sparse+sparsebellman-. sparse+sparsebellman-. sparse+softbellman- sparse+softbellman-. sparse+softbellman-. sparse+bellman- sparse+bellman-. sparse+bellman-. soft+sparsebellman- soft+sparsebellman-. soft+sparsebellman-. soft+softbellman- soft+softbellman-. soft+softbellman-. soft+bellman- soft+bellman-. soft+bellman-. epsgrdy+sparsebellman- epsgrdy+sparsebellman-. epsgrdy+sparsebellman-. epsgrdy+softbellman- epsgrdy+softbellman-. epsgrdy+softbellman-. epsgrdy+bellman- epsgrdy+bellman-. epsgrdy+bellman-. ddpg number action sparse+sparsebellman- sparse+sparsebellman-. sparse+sparsebellman-. sparse+softbellman- sparse+softbellman-. sparse+softbellman-. sparse+bellman- sparse+bellman-. sparse+bellman-. soft+sparsebellman- soft+sparsebellman-. soft+sparsebellman-. soft+softbellman- soft+softbellman-. soft+softbellman-. soft+bellman- soft+bellman-. soft+bellman-. epsgrdy+sparsebellman- epsgrdy+sparsebellman-. epsgrdy+sparsebellman-. epsgrdy+softbellman- epsgrdy+softbellman-. epsgrdy+softbellman-. epsgrdy+bellman- epsgrdy+bellman-. epsgrdy+bellman-. number action sparse+sparsebellman- sparse+sparsebellman-. sparse+sparsebellman-. sparse+softbellman- sparse+softbellman-. sparse+softbellman-. sparse+bellman- sparse+bellman-. sparse+bellman-. soft+sparsebellman- soft+sparsebellman-. soft+sparsebellman-. soft+softbellman- soft+softbellman-. soft+softbellman-. soft+bellman- soft+bellman-. soft+bellman-. epsgrdy+sparsebellman- epsgrdy+sparsebellman-. epsgrdy+sparsebellman-. epsgrdy+softbellman- epsgrdy+softbellman-. epsgrdy+softbellman-. epsgrdy+bellman epsgrdy+bellman epsgrdy+bellman ddpg number action sparse+sparsebellman- sparse+sparsebellman-. sparse+sparsebellman-. sparse+softbellman- sparse+softbellman-. sparse+softbellman-. sparse+bellman- sparse+bellman-. sparse+bellman-. soft+sparsebellman- soft+sparsebellman-. soft+sparsebellman-. soft+softbellman- soft+softbellman-. soft+softbellman-. soft+bellman- soft+bellman-. soft+bellman-. epsgrdy+sparsebellman- epsgrdy+sparsebellman-. epsgrdy+sparsebellman-. epsgrdy+softbellman- epsgrdy+softbellman-. epsgrdy+softbellman-. epsgrdy+bellman epsgrdy+bellman epsgrdy+bellman", "year": 2017}