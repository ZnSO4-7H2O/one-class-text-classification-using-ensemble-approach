{"title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data", "tag": ["cs.CV", "cs.IR", "cs.LG", "cs.NE"], "abstract": "Topic modeling based on latent Dirichlet allocation (LDA) has been a framework of choice to deal with multimodal data, such as in image annotation tasks. Another popular approach to model the multimodal data is through deep neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type of topic model called the Document Neural Autoregressive Distribution Estimator (DocNADE) was proposed and demonstrated state-of-the-art performance for text document modeling. In this work, we show how to successfully apply and extend this model to multimodal data, such as simultaneous image classification and annotation. First, we propose SupDocNADE, a supervised extension of DocNADE, that increases the discriminative power of the learned hidden topic features and show how to employ it to learn a joint representation from image visual words, annotation words and class label information. We test our model on the LabelMe and UIUC-Sports data sets and show that it compares favorably to other topic models. Second, we propose a deep extension of our model and provide an efficient way of training the deep model. Experimental results show that our deep model outperforms its shallow version and reaches state-of-the-art performance on the Multimedia Information Retrieval (MIR) Flickr data set.", "text": "topic modeling based latent dirichlet allocation framework choice deal multimodal data image annotation tasks. another popular approach model multimodal data deep neural networks deep boltzmann machine recently type topic model called document neural autoregressive distribution estimator proposed demonstrated state-of-theart performance text document modeling. work show successfully apply extend model multimodal data simultaneous image classiﬁcation annotation. first propose supdocnade supervised extension docnade increases discriminative power learned hidden topic features show employ learn joint representation image visual words annotation words class label information. test model labelme uiuc-sports data sets show compares favorably topic models. second propose deep extension model provide efﬁcient training deep model. experimental results show deep model outperforms shallow version reaches state-of-the-art performance multimedia information retrieval flickr data set. multimodal data modeling combines information different sources increasingly attracting attention computer vision leading approaches based topic modelling popular model latent dirichlet allocation generative model documents originates natural language processing community great success computer vision models document multinomial distribution topics topic multinomial distribution words. distribution topics speciﬁc document topic-dependent distributions words shared across documents. topic models thus extract meaningful semantic representation document inferring latent distribution topics words contains. context computer vision used ﬁrst extracting so-called visual words images convert images visual word documents training topic model bags visual words. deal multimodal data variants proposed recently instance correspondence proposed discover relationship images annotation modalities assuming image topic must corresponding text topic. multimodal generalizes corr-lda learning regression module relating topics different figure illustration single hidden layer supdocnade model multimodal image data. visual words anp. conditionals modeled using neural networks shared weights. predictive word conditional follows tree decomposition leaf possible word. test time annotation words used compute image’s topic feature representation. modalities. multimodal document random field model also proposed deal multimodal data learns cross-modality similarities document corpus containing multinomial data. besides annotation words class label modality also embedded supervised modeling image visual words annotation words class labels discriminative power learned image representations could thus improved. heart topic models generative story image’s latent representation generated ﬁrst visual words subsequently produced representation. appeal approach task extracting representation observations easily framed probabilistic inference problem many general purpose solutions exist. disadvantage however model becomes sophisticated inference becomes less trivial computationally expensive. instance inference distribution topics closed-form solution must approximated either using variational approximate inference mcmc sampling. model actually relatively simple making certain simplifying independence assumptions conditional independence visual words given image’s latent distribution topics. another approach model statistical structure words distributed representations modeled artiﬁcial neurons. realm document modeling salakhutdinov hinton proposed so-called replicated softmax model bags words. model later used multimodal data modeling pairs images text annotations modeled jointly within deep boltzmann machine deep learning approach generative modeling multimodal data achieved state-of-the-art performance flickr data hand also shares different extensions reliance stochastic latent representation data requiring variational approximations mcmc sampling training test time. another neural network based state-of-the-art multimodal data modeling approach multimodal deep recurrent neural network aims predicting missing data modalities rest data modalities minimizing variation information rather maximizing likelihood. recently alternative generative modeling approach documents proposed larochelle lauly work document neural autoregressive distribution estimator proposed models directly joint distribution words document decomposing product conditional distributions modeling conditional using neural network. hence docnade doesn’t incorporate latent random variables potentially expensive inference must performed. instead document representation computed efﬁciently simple feed-forward fashion using value neural figure illustration deep extension supervised docnade model. training phase input ﬁrst shufﬂed randomly based ordering randomly split parts vo<d vo≥d. compute conditionals equation backpropagation optimize parameters model. deal imbalance visual annotation words histogram vo<d vo≥d weighted test time words model compute discriminative deep representation. besides visual annotation words global features also leveraged model. network’s hidden layer. larochelle lauly also show docnade better generative model text documents model extract useful representation text information retrieval. paper consider application docnade deal multimodal data computer vision. speciﬁcally ﬁrst propose supervised variant docnade used model joint distribution image’s visual words annotation words class label. model illustrated figure investigate successfully incorporate spatial information visual words highlight importance calibrating generative discriminative components training objective. results conﬁrm approach outperform topic models supervised variant lda. moreover propose deep extension supdocnade learns deep discriminative representation pairs images annotation words. deep version supdocnade illustrated figure outperforms shallow achieves state-of-the-art performance challenging flickr data set. previously mentioned multimodal data often modeled using extensions basic topic model corr-lda multimodal mdrf paper focus learning joint representation three different modalities image visual words annotations class labels. class label describes image globally single descriptive label annotation focuses tagging local content within image. wang proposed supervised formulation tackle problem. wang opted instead maximum margin formulation work also belongs line work extending topic models supervised variant ﬁrst contribution paper thus distinguishes docnade topic models reliance autoregressive neural network architecture. recently deep neural networks increasingly used probabilistic modeling images text review). work srivastava salakhutdinov dbms sohn mdrnn good recent examples. ngiam also proposed deep autoencoder networks multimodal learning though approach recently shown outperformed dbms mdrnn although docnade shows favorable performance topic models lack efﬁcient deep formulation reduces ability modeling multimodal data especially compared deep neural network based models thus second contribution paper propose efﬁcient deep version docnade supervised variant. we’ll deep version docnade model outperform approach srivastava salakhutdinov section describe original docnade model. larochelle lauly docnade used model documents real words belonging predeﬁned vocabulary. model image data assume images ﬁrst converted visual words. standard approach learn vocabulary visual words performing k-means clustering sift descriptors densely exacted training images. section details procedure. point image thus represented visual words index closest k-means cluster sift descriptor extracted image number extracted descriptors image equation true distribution based probability chain rule. hence main assumption made docnade form conditionals. speciﬁcally docnade assumes conditional modeled learned feedforward neural network. element-wise non-linear activation function rh×q rq×h connection parameter matrices bias parameter vectors number hidden units computing distribution equation requires time linear practice expensive since must computed visual words address issue larochelle lauly propose balanced binary tree decompose computation conditionals obtain complexity logarithmic achieved randomly assigning visual words different leaf binary tree. given tree probability word modeled probability reaching associated leaf root. larochelle lauly model left/right transition probabilities binary tree using binary logistic regressors taking hidden layer input. probability given word obtained multiplying probabilities left/right choices associated tree path. speciﬁcally sequence tree nodes path root leaf sequence binary left/right choices internal nodes along path. example always root node binary tree word leaf left subtree otherwise. rt×h matrix containing logistic regression weights vector containing biases number inner nodes binary tree number hidden units. probability modeled internal node logistic regression outputs sigm sigmoid function. using balanced tree guaranteed computing equation involves logistic regression outputs. could attempt optimize organization words within tree random assignment words leaves works well practice document docnade. train parameters docnade simply optimize average negative log-likelihood training documents using stochastic gradient descent. equations indicate conditional probability word requires computing position dependent hidden layer extracts representation previous visual words v<i. since computing average hidden layers compute naive procedure computing hidden layers would logistic regressions equation total complexity computing ohdv). practice length document number hidden units tends small small even large vocabularies. thus docnade used trained efﬁciently. representation could standard classiﬁer perform supervised computer vision task. index used highlight representation used predict class label image. section describe approach paper inspired docnade learn jointly multimodal data. here concentrate single layer version model discuss deep extension later section first describe supervised extension docnade incorporates class label modality training learn discriminative hidden features classiﬁcation. describe exploit observed learning image feature representations using unsupervised topic models perform worse training classiﬁer directly visual words themselves using appropriate kernel pyramid kernel reason unsupervised topic features trained explain much entire statistical structure images possible might model well particular discriminative structure computer vision task. issue addressed literature devising supervised variants supervised slda docnade also unsupervised topic model propose supervised variant docnade supdocnade attempt make learned image representation discriminative purpose image classiﬁcation. docnade conditional modeled neural network. architecture regular docnade. need deﬁne model since image representation we’ll perform classiﬁcation propose model multiclass logistic regression output computed bias parameter vector supervised layer differently modeled regular multiclass neural network taking input visual words crucial difference however regular neural network parameters also used model visual word conditionals averaged training images. known generative learning ﬁrst term purely discriminative term second unsupervised understood regularizer encourages solution also explains unsupervised statistical structure within visual words. practice regularizer bias solution strongly away discriminative solution generalizes well. hence similarly previous work hybrid generative/discriminative learning propose instead weight importance generative term optimizing training average equation performed stochastic gradient descent using backpropagation compute parameter derivatives. regular docnade computation training objective gradient requires deﬁne ordering visual words. though could deﬁned arbitrary path across image order words follow larochelle lauly randomly permute words every stochastic gradient update. implication model effectively trained good inference model conditional ordering words often outperforms activation functions shown work well image data since piece-wise linear function gradient respect input needed backpropagation compute parameter gradients simply spatial information plays important role understanding image. example often appear part image often appear bottom. previous work exploited intuition successfully. example seminal work spatial pyramids shown extracting different visual word histograms distinct regions instead single image-wide histogram yield substantial gains performance. follow similar approach whereby model presence visual words identity region appear speciﬁcally let’s assume image divided several distinct regions number regions. image represented region visual word extracted. model joint distribution treat possible visual word/region pair distinct word. implication binary tree visual words must larger leaf possible visual word/region pair. fortunately since computations grow logarithmically size tree problem still deal large number regions. speciﬁcally predeﬁned vocabulary annotation words note annotation given image number words annotation. thus image annotation represented mixed visual annotation words embed annotation words supdocnade framework treat annotation word deal visual words. speciﬁcally joint indexing visual annotation words larger binary word tree augment leaves annotation words. training supdocnade joint image/annotation representation learn relationship labels spatially-embedded visual words annotation words. test time annotation words given wish predict them. achieve this compute document representation based visual words compute possible annotation word probability would next observed word based tree decomposition equation words compute probability paths reach leaf corresponding annotation word rank annotation words decreasing order probability select words predicted annotation. although supdocnade achieved better performance topic models previous work lack efﬁcient deep formulation supdocnade reduces capability modeling multimodal data especially compared models based deep neural network recently uria proposed efﬁcient extension original nade model binary vector observations docnade derived. take inspiration uria propose supdeepdocnade i.e. supervised deep autoregressive neural topic model multimodal data modeling. section introduce deep extension docnade describe incorporate supervised information training. also discuss deal inbalance number visual words annotation words order obtain good performances. start discussion note notation denotes words image includes visual words annotation words image following section discussed section section mentioned words randomly permuted every stochastic gradient update make docnade good inference model ordering words. uria notice think many orderings instantiation many different docnade models distinct ordering. point view training single parameters orderings effectively employing parameter sharing strategy across models training process interpreted training factorial number docnade models simultaneously. make notion ordering explicit notation. following uria denote joint distribution docnade model words image given parameters vo<d subvector previous words extracted ordered word vector word notice ordering treated explicitly random variable. thus training docnade stochastically sampled orderings corresponds expectation minimize negao∈o inside summation conditionals expectation split three parts standing ﬁrst indices ordering index ordering standing remaining indices ordering. practice equation still sums number terms large performed exhaustively. training thus stochastic estimation replace expectations/sums samples. hand innermost expectation obtained cheaply. indeed given value terms number words words equation measures ability model predict ﬁxed random context words vo<d remaining words image/annotation. this training docnade performed stochastic gradient descent. given training example sample uniformly separates parts vo<d inputs vo≥d outputs; compute conditionals equation vo≥d compute gradients conditionals equation rescale contrast procedure described section prescribed stochastic estimation respect possible orderings words exhaustive predicting words sequence. here opposite stochastic predicting subset words exhaustive implicitly summing gradient contributions several orderings sharing permutation position shown section training docnade performed randomly splitting words parts vo<d vo≥d applying stochastic gradient descent loss function equation thus training procedure corresponds neural network vo<d input vo≥d output’s target. advantage approach docnade easily extended deep version refer deepdocnade. unlike original training procedure docnade training update requires computation single hidden layer instead hidden layers. adding hidden layers additive instead multiplicative effect complexity training update. hidden layers added regular deep feedforward neural networks follows experiments visual words annotation words represented words fashion. shown section training processing actually equals generating word vector shufﬂing word vector splitting regenerating inefﬁcient processing samples mini-batch fashion. hence practice split original histogram directly uniformly sampling many left split individual word. equivalent mentioned paper works well practice. equation obtaining hidden representation future words sparse binary tree output model might efﬁcient full vocabulary size computation speciﬁc scenario however going back softmax model conditionals preferable. indeed since conditionals equation share hidden representation thus normalization term softmax future words another advantage softmax binary tree softmax amenable efﬁcient implementation also speed training process. similar equation ﬁrst term equation supervised second term unsupervised interpreted regularizer. thus also weight importance unsupervised part hyperparameter obtain hybrid cost function mentioned section annotation words embedded framework supdocnade treating deal visual words. practice however number visual words could much larger annotation words. example flickr data experimental setup srivastava salakhutdinov average number visual words image much larger average number annotation words image imbalance visual words annotation words might cause problems. example contribution hidden representation annotation words small might ignored compared contribution huge mount visual words gradients coming annotation words might also small meaningful effect increasing conditionals probability annotation words. weighting annotation words histogram model attention annotation words reducing problem caused imbalance visual annotation words. practice weight hyper-parameter selected cross-validation. we’ll section weighting annotation words heavily signiﬁcantly improve performance. besides spatial information annotation embedded framework docnade section section bottom-up global features gist mpeg- descriptors also play important role multimodal data modeling global features among things complement local information extracted patch-based visual words. section describe embed features framework model. connection matrix speciﬁc global features. understood hidden layer whose hidden unit biases conditioned image’s global features vector thus whole model conditioned previous words also global features section compare performance model models multimodal data modeling. specifically ﬁrst test ability single hidden layer supdocnade learn multimodal data real-world data sets widely used research topic models. test performance supdeepdocnade largescale multimedia informaton retrieval flickr data show supdeepdocnade achieves state-of-the-art performance. code download data sets supdocnade supdeepdocnade available https//sites.google.com/site/zhengyin/home/supdeepdocnade. test ability single hidden layer supdocnade learn multimodal data measured performance simultaneous image classiﬁcation annotation tasks. tested model real-world data sets subset labelme data uiuc-sports data labelme uiuc-sports come annotations popular classiﬁcation annotation benchmarks. performed extensive quantitative comparisons supdocnade original docnade model supervised also provide comparisons mmlda spatial pyramid matching approach data sets description following wang constructed labelme data using online tool obtain images size pixels following classes highway inside city coast forest tall building street open country mountain. class images randomly selected split evenly training test sets yielding total images. uiuc-sports data contains images classiﬁed classes badminton bocce croquet polo rockclimbing rowing sailing snowboarding following previous work maximum side image resized pixels maintaining aspect ratio. randomly split images class evenly training test sets. labelme uiuc-sports data sets removed annotation words occurring less times wang following wang dimensional densely extracted sift features used extract visual words. step patch size dense sift extraction respectively. dense sift features training quantized clusters construct visual word vocabulary using k-means. divided image grid extract spatial position information described section produced different visual word/region pairs. classiﬁcation accuracy evaluate performance image classiﬁcation average f-measure predicted annotations evaluate annotation performance previous work. f-measure image deﬁned recall percentage correctly predicted annotations ground-truth annotations image precision percentage correctly predicted annotations predicted annotations. used random train/test splits estimate average accuracy f-measure. image classiﬁcation supdocnade performed feeding learned document representations kernel svm. experiments hyper-parameters chosen cross validation. emphasize that following wang annotation words available test time methods predict image’s class based solely visual words. mention shown slda performs better corr-lda. moreover found multimodal improve performance corr-lda. finally slda distinguishes models fact also supports class label modality code available online. hence compare directly slda only. figure classiﬁcation performance comparison labelme uiuc-sports left compare classiﬁcation performance supdocnade docnade slda. right compare performance different variants supdocnade. varies means unsupervised weight equation chosen cross-validation. section describe quantitative comparison supdocnade docnade slda. used implementation slda available http//www.cs.cmu.edu/˜chongw/slda/ comparison visual annotation words docnade supdocnade. classiﬁcation results illustrated figure similarly observe supdocnade outperforms docnade slda. tuning trade-off generative discriminative learning exploiting position information usually beneﬁcial. exception labelme hidden topic units using grid slightly outperforms grid. image annotation computed performance model topics. shown table supdocnade obtains -measure labelme uiuc-sports data sets respectively. slightly superior regular docnade. since code performing image annotation using slda publicly available compare directly results found corresponding paper wang report -measures slda supdocnade large margin. also compare mmlda applied image classiﬁcation annotation separately. reported classiﬁcation accuracy mmlda less supdocnade shown table performance annotation reported wang better supdocnade labelme worse uiuc-sports. highlight mmlda deal class label annotation word modalities jointly different modalities treated separately. spatial pyramid approach lazebnik could also adapted perform image classiﬁcation annotation. used code lazebnik generate two-layer representations vocabulary size conﬁguration used models. image classiﬁcation figure predicted class annotation supdocnade labelme data set. list correctly incorrectly classiﬁed images. predicted ground-truth class labels annotation words presented image. histogram intersection kernel adopted classiﬁer lazebnik annotation used nearest neighbor prediction annotation words test images. speciﬁcally frequent annotation words among nearest images training selected prediction test image’s annotation words. number selected cross validation random splits. shown table achieves classiﬁcation accuracy labelme uiuc-sports lower supdocnade. annotation -measure also lower supdocnade labelme uiuc-sports respectively. extracted visual/annotation words strongly associated certain class labels within supdocnade follows. given class label street corresponds column matrix selected topics largest connection weight then averaged columns matrix figure visualization learned representations. class labels colored red. class list visual words annotation words strongly associated class. sec. details. corresponding hidden topics selected visual/annotation words largest averaged weight connection. results procedure classes street sailing forest highway illustrated figure visualize visual words show image patches belonging visual word’s cluster extracted k-means. learned associations intuitive example class street associated annotation words building buildings window person walking visual words showcase parts buildings windows. test performance supdeepdocnade deep extension supdocnade large-scale flickr data flickr challenging benchmark multimodal data modeling task. section show supdeepdocnade achieves state-of-the-art performance flickr data strong baselines apporach srivastava salakhutdinov mdrnn tagprop multiple kernel learning approach verbeek flickr data contains million real images collected image hosting website flickr. social tags image also collected used annotations experiments. among million images images labeled classes bird people animals etc. giving subset labeled images. image labeled subset multiple class labels. experiments used images training images testing. remaining images labels thus used unsupervised pretraining supdeepdocnade frequent tags collected annotation vocabulary following previous work averaged number annotations order compare directly approach srivastava salakhutdinov experimental conﬁguration. speciﬁcally images flickr ﬁrst rescaled make maximum side image pixels keeping aspect ratio. then dimensional sift features densely sampled images extract visual words. following srivastava salakhutdinov used different scales patch size pixels respectively patch step ﬁxed pixels. sift features unlabeled images quantized clusters used visual word vocabulary. thus image modality represented visual words representation using vocabulary. preliminary experiments suggested spatial information wasn’t useful flickr data opted using here. similarly text modality supdeepdocnade represented using annotation vocabulary built upon frequent tags mentioned section visual words annotation words combined together treated input supdeepdocnade. used hidden layers architecture experiments size hidden layer note also hidden layers hidden units layer thus comparison fair. activation function hidden units rectiﬁed linear function. used softmax output layer experiments unlabeled images used unsupervised pretraining. achieved ﬁrst training deepdocnade model without output layer predicting class labels. result training used initialize parameters supdeepdocnade model ﬁnetuned labeled training based loss equation training ﬁnalized hidden representation hidden layer observing words image feed linear compute conﬁdences image belonging class. average precision class obtained based conﬁdences area precision-recall curve. that mean average precision classes computed used metric measure performance model. used training/validation/test splits labeled subset flickr srivastava salakhutdinov report average performance splits. connection matrix lθwθ number rows columns respectively matrix respectively uniform distribution. practice we’ve also found useful normalize input histograms hidden layers. also maintained exponentially decaying average parameter values throughout gradient decent training procedure used averaged parameters test time. corresponds polyak averaging linear average replaced weighting puts emphasis recent parameter values. annotation weight ﬁxed approximately ratio averaged visual words annotation words data set. investigate impact annotation weight performance section table presents comparison performance supdeepdocnade approach srivastava salakhutdinov mdrnn sohn well strong baselines terms performance. also provide simple popular tf-idf baseline table make comparison complete. tf-idf baseline conducted bag-of-words representations images without global features. feed tf-idf representations linear obtain conﬁdences image belonging class compute mean supdeepdocnade. supdeepdocnade achieves best performance among methods. speciﬁcally ﬁrst pretrained model epochs unlabeled data hidden layers. results illustrated table show supdeepdocnade outperforms baseline large margin. moreover supdeepdocnade hidden layers performs better hidden layer epochs pretraining. pretrained model epochs unlabeled data shown table pretraining epochs deeper model performs even better. conﬁrms deep architecture beneﬁcial. number pretraining epochs reaches supdeepdocnade model hidden layers achieves outperforms strong baselines increases performance -hidden-layers model. tabel also performance -layers supdeepdocnade improve much -layers supdeepdocnade number pretraining epochs increases figure shows performance supdeepdocnade w.r.t number pretraining epochs. figure epochs pretraining performance -layers supdeepdocnade increases faster -layers models indicates capacity -layers supdeepdocnade bigger -layers model capacity could leveraged pretraining. figure also suggests performance supdeepdocnade could even better pretraining epochs. left-side row. reasons failure local texture/color ambiguous misleading. example ﬁrst image blue color upper side wall misleads model predict conﬁdence another type failure shown middle figure caused images abstract illustration class. instance model fails recognize bird tree images middle respectively images merely abstract illustrations concepts. third reason illustrated bottom class takes small portion image making likely ignored. example female face stamp ﬁrst image bottom small recognized model. note illustrated failed examples might kinds failures. practice also images correctly labeled might also cause failures. established supdeepdocnade achieves state-of-the-art performance flickr data also discussed failed examples explore details properties following sections. section proposed weight differently annotation words deal problem imbalance number visual annotation words. part investigate inﬂuence annotation weight performance. speciﬁcally annotation weight show performance annotation weight values. note annotation weight equals compensation imbalance visual words annotation words. experimental conﬁgurations section figure shows performance comparison different annotation weights. expected supdeepdocnade performs extremely annotation equals annotation weight increased performance gets better. among chosen annotation weights performs best achieves annotation weights also achieves good performance compared model annotation weight values respectively. figure illustration failed examples supdeepdocnade. reasons failure listed leftside row. reason list examples. text image conﬁdence either wrongly predicted class ground truth class maximum value conﬁdence minimum since supdeepdocnade used multimodal data modeling illustrate results multimodal data retrieval tasks. speciﬁcally show qualitative results multimodal data retrieval scenarios multimodal data query generation text images. multimodal data query given query corresponding image/annotation pair task retrieve similar pairs collection using hidden representation learned supdeepdocnade. task cosine similarity adopted similarity metric. experiment query corresponds individual test example collection corresponds rest test set. figure illustrates retrieval results multimodal data query task show similar images query input testset. generating text image supdeepdocnade learns relationship image text modalities test ability generate text given images. task implemented feeding supdeepdocnade visual words selecting annotation words according probability next word similarly section figure illustrates ground truth annotation probable annotations generated supdeepdocnade. supdeepdocnade generated meaningful texts according image modality shows effectively learned statistical structure modalities. paper proposed supdocnade supervised extension docnade learn jointly visual words annotations class labels. moreover proposed deep extension supdocnade outperforms shallow version trained efﬁciently. although supdocnade supdeepdocnade nature supdeepdocnade differs single layer version training process. speciﬁcally training process supdeepdocnade performed subset words summing gradients several orderings sharing permutation randomly selected position single layer version opposite exploits single randomly selected ordering updates conditionals words. like topic models model trained model distribution bag-of-word representations images extract meaningful representation unlike topic models however image representation modeled latent random variable model instead hidden layer neural autoregressive network. distinctive advantage supdocnade require iterative approximate inference procedure compute image’s representation. experiments conﬁrm supdocnade competitive approach multimodal data modeling supdeepdocnade achieves state-of-the-art performance challenging multimodal data benchmark flickr.", "year": 2014}