{"title": "Deep Reinforcement Learning From Raw Pixels in Doom", "tag": ["cs.LG", "cs.AI"], "abstract": "Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.", "text": "using current reinforcement learning methods recently become possible learn play unknown games pixels. work study challenges arise complex environments summarize current methods approach these. choose task within doom game approached yet. goal agent ﬁght enemies world consisting rooms. train lstmac algorithms task. results show algorithms learn sensible policies fail achieve high scores given amount training. provide insights learned behavior serve valuable starting point research doom domain. agent environment value-based methods policy iteration exploration versus exploitation temporal diﬀerence learning eligibility traces policy-based methods actor-critic methods understanding human-like thinking behavior biggest challenges. scientists approach problem diverse disciplines including psychology philosophy neuroscience cognitive science computer science. computer science community tends model behavior farther away biological example. however models commonly evaluated complex tasks proving eﬀectivity. speciﬁcally reinforcement learning intelligent control communities within machine learning generally artiﬁcial intelligence focus ﬁnding strategies behave unknown environments. general setting allows methods applied ﬁnancial trading advertising robotics power plant optimization aircraft design structure work follows motivate introduce setting chapter explain fundamentals methods chapter next discuss challenges arise complex environments like doom chapter describe state-of-the-art algorithms chapter conduct experiments chapter close conclusion chapter setting deﬁnes environment agent interacts environment problem solve. example could racing track advertising network stock market. environment reveals information agent camera image perspective proﬁle user want display current stock prices. examples agent might control steering wheel accelerator choose display sell shares. moreover agent receives reward signal depends outcome actions. problem learn choose best action sequences initially unknown environment. ﬁeld control theory also refer control problem agent tries control environment using available actions. used model behavior humans artiﬁcial agents. doing assumes humans optimize reward signal. signal arbitrarily complex could learned lifetime evolution course generations. example neurotransmitter dopamine known play critical role motivation related reward system human brain. modeling human behavior problem complex reward function completely agreed however. behavior modeled following reward function simpler underlying principles could exist. principles might valuable model human behavior build intelligent agents. moreover current algorithms hardly compared human behavior. example common approach algorithms called probability matching agent tries choose actions relative probabilities reward. humans tend prefer small chance high reward highest expected reward. details please refer shteingart loewenstein supervised learning dominant framework ﬁeld machine learning. fueled successes domains like computer vision natural language processing recent break-through deep neural networks. learn labeled examples assume independent. objective either classify unseen examples predict scalar property them. comparison general deﬁning sequential problems. always useful could model problem one-step problem. another connection frameworks many algorithms internally function approximation orthogonal framework examples unlabeled. thus unsupervised learning algorithms make sense data compression reconstruction prediction unsupervised objectives. especially complex environments employ methods unsupervised learning extract good representations base decisions literature uses diﬀerent testbeds evaluate compare algorithms. traditional work often focuses simple tasks balancing pole cart however want build agents cope additional diﬃculties arise complex environments video games provide convenient evaluate algorithms complex environments interface clearly deﬁned many games deﬁne score forward agent reward signal. board games also commonly addressed using approaches considered work rules known advance. notably atari environment provided consists low-resolution games. agent learn observing either screen pixels main memory used game. environments agent observes perspective pixel images include driving simulator torcs several similar block-world games refer minecraft domain ﬁrst-person shooter game doom ﬁeld reinforcement learning provides general framework models interaction agent unknown environment. multiple time steps agent receives observations environment responds actions receives rewards. actions aﬀect internal environment state. example time step agent receives pixel view doom environment chooses available keyboard keys press. environment advances game time step. agent killed enemy reward positive signal otherwise then environment sends next frame agent choose next action. observation space further deﬁne transition function dist also refer dynamics observation function dist reward function dist dist refers space random variables denote initial state model terminal states implicitly chooses actions based previous observations environment. convention denote variable current action given previous observations probability choosing particular action at). initially agents provides action discrete time step environment observes state agent observes reward observation environment observes agent’s policy agent’s objective maximize expected return policy. solution depends choice values close encourage long-sighted actions values close encourage short-sighted actions. methods roughly separated value-based policy-based ones. theory often assumes fully-observable environments assume agent found reconstruct observations ...xt. course depending might completely possible. discuss challenge partial observability later section interestingly need know dynamics environment order optimally includes weighting transition probabilities implicitly. idea algorithms introduce next approximate greedily respect order approximate policyiteration algorithm starts iteration perform steps evaluation estimate observed interactions example using monte-carlo policyiteration learning stagnate early visit qstates again. particular might never visit states following problem visiting states called exploration opposed exploitation means acting greedily respect current q-value estimate. fundamental tradeoﬀ exploration exploitation point might either choose follow policy current think best perform actions think worse potential discovering better action sequences. complex environments exploration biggest challenges discuss advanced approaches section dominant approach exploration straightforward epsilongreedy strategy agent picks random action probability action according normal policy otherwise. decay policyiteration combined epsilongreedy exploration strategy ﬁnds optimal policy eventually monte-carlo estimates comparatively high variance need observe q-state often order converge optimal policy. improve data eﬃciency using idea bootstrapping estimate q-values single transition approximation considerably less variance introduces bias initial approximated q-values might arbitrarily wrong. practice bootstrapping common since monte-carlo estimates tractable. sarsa approximates expected returns using current approximation q-value policy. common modiﬁcation known q-learning proposed watkins dayan here bootstrap using think best action rather action observed policy. therefore directly approximate denoting current q-learning algorithm might important breakthroughs allows learn optimal behave observing transitions arbitrary policy. policy still eﬀects q-states visit update needed exploration. problem temporal diﬀerence methods like sarsa q-learning updates approximated q-function aﬀect q-values predecessor states directly. long time gaps good actions corresponding rewards many updates q-function required rewards propagate backward good actions. instance fundamental credit assignment problem machine learning. agent receives positive reward needs ﬁgure state action reward make likely. algorithm provides answer assigning eligibilities q-state. common assign eligibilities based duration visiting q-states receiving reward rt+. state receive reward eligibility eligibility previous states decays exponentially time factor implement adjusting eligibilities time step assigning eligibilities known replacing traces reset eligibility encountered state alternatives include accumulating traces dutch traces shown figure cases keep existing eligibility value visited q-state increment dutch traces additionally scale result factor here scalar oﬀset estimate previous given temporal diﬀerence error algorithm like sarsa qlearning small learning rate. background function approximation using neural networks scope work. contrast value-based methods policy-based methods parameterize policy directly. depending problem ﬁnding good policy easier approximating q-function ﬁrst. using parameterized function approximator good parameters several methods searching space possible policy parameters explored including random search evolutionary search gradientbased search following focus gradient-based methods. policygradient algorithm basic gradient-based method policy search. idea reward signals objective tweak parameters using gradient ascent. work interested gradient expected reward policy respect parameters denotes probability state following course cannot gradient analytically agent interacts unknown usually non-diﬀerentiable environment. however possible obtain estimate gradient using score-function shown equation require gradient policy. using derivable function approximator sample trajectories environment obtain monte-carlo estimate states equation yields reinforce algorithm proposed williams value-based methods monte-carlo estimate comparatively slow update estimates episode. improve that combine approach eligibility traces also reduce variance monte-carlo estimates described next section. actor-critic methods improve data eﬃciency algorithm subtracting baseline reward reduces variance expectation. approximated function call approximator critic approximator policy function actor. introduce bias gradient reward gradient baseline respect policy must traditional benchmark problems include tasks like mountain cart pole. tasks observation action spaces small dynamics described simple formulas. moreover environments usually fully observed agent could reconstruct system dynamics perfectly given enough transitions. contrast environments like doom complex underlying dynamics large state spaces agent partially observe. therefore need advanced methods learn successful policies discuss challenges arise complex environments methods approach them. doom environment agents observe perspective projections position world pixel matrices. large state space makes tabular versions algorithms intractable. adjust algorithms function approximators make tractable. however agent receives tens thousands pixels every time step. computational challenge even case function approximation. downsampling input images provides partial solution since must preserve information necessary eﬀective control. would like agents small representations observations helpful action selection. therefore abstraction individual pixels necessary. convolutional neural networks provide computationally eﬀective learn abstractions comparison normal fully-connected neural networks cnns consist convolutional layers learn multiple ﬁlters. ﬁlter shifted whole input previous layer produce feature map. feature maps optionally followed pooling layers downsample feature applying ﬁlters across whole input previous layer means must learn small ﬁlter kernel. number parameters kernel depend input size. allows eﬃcient computation faster learning compared fully-connected networks layers adds amount parameters quadratic number layer size. moreover cnns exploit fact nearby pixels observed images correlated. example walls surfaces result evenly textured regions. pooling layers help reduce dimensionality keeping information represented small details important. ﬁlter learns high-resolution feature downsampled individually. complex environments observations fully reveal state environment. perspective view agent observes doom environment contains reduced information multiple ways learn good policy agent detect spatial temporal correlations input. example would beneﬁcial know position objects agent environment. knowing positions velocities enemies would certainly help aiming. using hierarchical function approximators like neural networks allows learning high-level representations like existence enemy ﬁeld view. high-level representations neural networks need layer layer learn linear combinations input previous layer. complex features might impossible construct address temporal incompleteness observations frame skipping collect multiple images show stack observation agent. agent decides action repeat collecting next stack inputs also common recurrent neural networks address problem partial observability. neurons architectures self-connections allow activations span multiple time steps. output function current input previous state network itself. particular variant called long short-term memory variations like gated recurrent unit proven eﬀective wide range sequential problems recent advancement applying memory network architectures problems minecraft environment approximators consist write read external memory. allows network clearly carry information long durations. various forms neural networks successfully applied supervised unsupervised learning problems. applications dataset often known prior training decorrelated many machine learning algorithms expect independent identically distributed data. problematic setting want improve policy collecting observations sequentially. observations highly correlated sequential nature underlying mdp. step agent samples random batch transitions memory uses training. initialize memory random policy training phase. note transitions still biased start distribution mdp. mentioned work ﬁrst managed learn play several atari games without hand-crafted features. also applied simple tasks minecraft doom domains. hand replay memory memory intense seen unsatisfyingly humans process information. addition mnih used idea target network. training approximators bootstrapping targets estimated function approximator. thus training step target computation changes prevent convergence approximator trained toward moving target. keep previous version approximator obtain targets. every time steps update target network current version training approximator. recently mnih proposed alternative using replay memories involves multiple versions agent simultaneously interacting copies environment. agents apply gradient updates shared parameters. collecting data multiple environments time suﬃciently decorrelated data learn better policies replay memory target network able ﬁnd. problem non-informative feedback tied environments particular constitutes signiﬁcant challenge. sometimes help guide agent rewarding actions positive negative rewards. many real-world tasks agent might receive zero rewards time binary feedback episode. rare feedback common hand-crafting informative reward signal straightforward want bias solutions agent might ﬁnd. environments agent assign credit among previous actions ﬁnally receiving reward. apply eligibility traces function approximation keeping track transitions since start episode. agent receives reward incorporates updates stored states relative decayed reward. algorithms like q-learning optimal tabular case assumption state visited again eventually complex environments impossible visit many states. since function approximation agent already generalize similar states. several paradigms problem eﬀectively ﬁnding interesting unknown experiences help improve policy. simple environments epsilongreedy might actually visit state often enough derive optimal policy. complex environments even visit state reasonable time. visiting novel states would important discover better policies. reason random exploration still works reasonably well complex environments partly attributed function approximation. function approximator generalizes similar states visiting state also improves estimate similar states. however elaborate methods exist result better results. simple paradigm encourage exploration value-based algorithms optimistically initialize estimated q-values. either pre-training function approximator adding positive bias outputs. whenever agent visits unknown state correct q-value downward. less visited states still high values assigned agent tends visiting facing choice. bayesian approach count visits state compute uncertainty value estimate combined q-learning converges true q-function given enough random samples unfortunately hard obtain truly random samples general sequential nature. another problem counting state space large continuous want generalize similar states. bellemare recently suggested sequential density estimation model derive pseudo-counts state non-tabular setting. method signiﬁcantly outperforms existing methods games atari domain exploration challenging. encourages uniformly distributed policy policy gradient updates outweigh regularization. method successfully applied atari visual domain labyrinth mnih related paradigm explore order attain knowledge environment absence external rewards. usually modelbased approach directly encourages novel states based function approximators. ﬁrst approximator called model tries predict next observation thus approximates transition function mdp. second approximator called controller performs control. objective maximize expected returns cause highest reduction prediction error model therefore tries provide observations model novel learnable inspired humans bored known knowledge knowledge cannot understand model-controller architecture extended multiple ways. combined planning escape known areas state space eﬀectively. schmidhuber recently proposed shared neurons model controller networks allows controller arbitrarily exploit model control. algorithms directly produce action time step humans plan actions slower time scale. adapting property might necessary human level control complex environments. mentioned exploration methods determine next exploration action time step. temporally extended actions could beneﬁcial exploration exploitation common framework temporal abstraction literature options framework proposed sutton idea learn multiple low-level policies named options interact world. high-level policy observes inputs options actions choose from. high-level policy decides option corresponding low-level policy executed ﬁxed random number time steps. multiple ways obtain options recent approaches shown work complex environments. krishnamurthy used spectral clustering group states cluster centers representing options. instead learning individual low-level policies agent greedily follows distance measure low-level states cluster centers given clustering algorithm. also building options framework tessler trained multiple cnns simple tasks minecraft domain. so-called deep skill networks added addition low-level actions high-level policy choose from. authors report promising results navigation task. limitation options framework single level hierarchy. realistically multi-level hierarchical algorithms mainly explored ﬁelds cognitive science computational neuroscience. rasmussen eliasmith propose architecture show able learn simple visual tasks. explained background methods chapter described challenges arise complex environments chapter already outlined intuition behind current algorithms. section build explain state-of-the-art algorithms successfully applied domains. currently common algorithm learning high-dimension state spaces deep q-network algorithm suggested mnih based traditional q-learning algorithm function approximation epsilongreedy exploration. initial form eligibility traces. decorrelate transitions agent collects play uses large replay memory. time step select batch transitions replay memory randomly. temporal diﬀerence q-learning rule update neural network. initialize start annealing replay memory ﬁlled. order stabilize training uses target network compute temporal diﬀerence targets. copy weights primary network target network initialization every time steps. initially proposed algorithms synchronizes target network every frame targets computed using network weights last time step. health gathering task agent must collect health items multiple open rooms used three convolutional layers followed max-pooling layers fully-connected layer. small learning rate replay memory entries million training steps agent learned successful policy. barron applied tasks minecraft domain collecting many blocks certain color possible navigating forward pathway without falling. tasks learned successful policies. depending width pathway larger convolutional network several days training needed learn successful policies. mnih actor-critic method considerable memory-eﬃcient require replay memory. instead transitions decorrelated training multiple versions environment parallel asynchronously updating shared actor-critic model. entropy regularization employed encourage exploration. originally threads manages copy model interacts instance environment. threads collects transitions computing eligibility returns computing gradients according algorithm based current copy model. applies gradient shared model updates copy current version shared model. version uses network architecture except using softmax activation function last layer model policy rather than. critic model shares convolutional layers adds linear fully-connected layer size trained estimate value function. authors also proposed version named lstm-ac adds lstm layer convolutional layers output layers approach problem partial observability. addition atari continuous control domain evaluated labyrinth domain maze goal collect many apples possible. lstm-ac able learn successful policy task avoids walk walls turns around facing dead ends. given recent proposal algorithm likely algorithm applied domains yet. introduce doom domain detail focusing task used experiments. explain methods necessary train algorithms stable manner. agents reach particularly high scores average learn range interesting behaviors examine section doom domain provides tasks simulated game mechanics ﬁrst-person shooter doom. game features diﬀerent kinds enemies items weapons. level editor used create custom tasks. deathmatch task deﬁned collection ﬁrst describe doom domain general. doom agent observes image frames perspective projections world agent’s position. frame also contains user interface elements bottom including amount ammunition agent’s selected weapon remaining health points agent additional game-speciﬁc information. extract information explicitly. frame represented tensor dimensions screen width screen height color channel. choose width height available screen resolutions. color channel represents values pixels thus always size three. actions arbitrary combinations available user inputs game. actions binary values represent whether given keyboard pressed released position. actions represent focus deathmatch task agent faces multiple kinds enemies attack agent must shoot enemy multiple times depending kind enemy kill receive reward enemy attacks reduces agent’s health points eventually causing episode. agent receive reward episode. episode also ends exceeding time steps. shown figure world consists large hall enemies appear regularly four small rooms sides. small rooms contain items agent restore health points. rooms contain ammunition stronger weapons pistol agent begins with. collect items ammunition weapons agent must walk them. agent starts random position hall facing random direction. figure three observations deathmatch task unprocessed frame downsampled grayscale frame downsampled grayscale delta frame based human testing preprocessing methods retain observations suﬃcient successful play. multiple frames stack them show agent observation. repeat agent’s action choice next time steps collecting stack images. perform random action ﬁrst stack episode. history frames multiple beneﬁts include temporal information allow eﬃcient data processing cause actions extended multiple time steps resulting smooth behavior. history frames extend dimensionality observations. grayscale images compensate keep computation requirements manageable. color information could beneﬁcial agent expect temporal information contained history frames valuable especially state-less algorithm. experiments would needed test hypothesis. remove unnecessary actions action space speed learning leaving actions attack move left move right move forward move backward turn left turn right. note include actions rotate view upward downward agent learn look upright. lstm-ac sensitive choice hyper parameters training times order hours days tractable perform excessive hyper parameter search researcher. normalize observations rewards described previous section make likely hyper parameters transferred tasks domains. lstm-ac learning threads apply accumulated gradients every observations using shared optimizer statistics entropy regularization factor further scale loss critic resulting comparable running times algorithms. epoch evaluate learned policies observations respectively. measure score collected rewards average training episodes. uses evaluation. conducted experiments algorithms able learn interesting policies epochs. describe discuss instances policies section. results lstm-ac similar many cases suggesting algorithms discover common structures deathmatch task. agents show particular changes receiving delta frames inputs. might given history frames neural networks easily learn compute diﬀerences frames expected agents develop tendency enemies. agent needs directly look toward enemies order shoot them always faces enemy time step receiving positive reward. however aiming inaccurate agents tend look past enemies left right side alternatingly. would interesting conduct experiments without stacking history frames understand whether learned policies limited action repeat. several instances agents lose enemies ﬁeld view turning usually toward enemies. trained agent found memorize enemies seeming anymore. result agents often attacked back causing episode end. surprisingly lstm-ac agents tend attack facing enemy sometimes miss chance contrast agents found shoot regularly often lstm-ac. experiments would needed verify whether behavior persists training episodes. agents learn avoid running walls facing them suggesting least limited amount spatial awareness. comparison random agent repeatedly runs wall gets stuck position episode. instances agent walks backward wall angle slides along wall reaching entrance side room. behavior overﬁts particular environment deathmatch task represents reasonable policy pick powerful weapons inside side rooms. testing episodes agent used strategy enter rooms containing health restoring items. could attributed lack negative reward agent dies. weapon using wall-sliding strategy starts collecting agents collecting weapon. training might successful episodes agent initially collected weapon. another reason could ﬁrst enemies appear couple time steps reason beginning episode. general tested algorithms lstm-ac able learn interesting policies task doom domain relatively short amounts training time. hand agents ways eﬀectively shoot multiple enemies row. evaluation multiple choices hyper parameters longer training times would sensible next step. started introducing reinforcement learning summarized basic reinforcement learning methods. compared traditional benchmarks complex environments challenging many ways. analyze challenges review current algorithms ideas approach these. following background select algorithms successfully applied learn pixels environments. apply algorithms challenging task within recently presented doom domain based ﬁrst-person-shooter game. applying selected algorithms yields policies reach particularly high scores show interesting behavior. agents tend enemies avoid getting stuck walls surprising strategies within doom task. also assess using delta frames aﬀect training much combined stacking multiple images together input agent. would interesting using fewer action repeats allows accurate aiming. drawn conclusions ﬁnal hyper parameters longer training might allow algorithms perform better. analyze instances learned behavior believe observations reported work provide valuable starting point experiments doom domain. barron whitehead yeung. deep reinforcement learning blockworld environment. deep reinforcement learning frontiers challenges ijcai bellemare naddaf veness bowling. arcade learning environment evaluation platform general agents. journal artiﬁcial intelligence research krishnamurthy lakshminarayanan kumar ravindran. hierarchical reinforcement learning using spatio-temporal abstractions deep neural networks. arxiv preprint arxiv. mnih badia mirza graves lillicrap harley silver kavukcuoglu. asynchronous methods deep reinforcement learning. proceedings international conference machine learning luciw f¨orster schmidhuber. conﬁdence-based progress-driven self-generated goals skill acquisition developmental robots. frontiers psychology rasmussen eliasmith. neural model hierarchical reinforcement learning. proceedings annual conference cognitive science society pages cognitive science society austin schmidhuber. possibility implementing curiosity boredom model-building neural controllers. animals animats proceedings ﬁrst international conference simulation adaptive behavior. citeseer schmidhuber. learning think algorithmic information theory novel combinations reinforcement learning controllers recurrent neural world models. arxiv preprint arxiv.", "year": 2016}