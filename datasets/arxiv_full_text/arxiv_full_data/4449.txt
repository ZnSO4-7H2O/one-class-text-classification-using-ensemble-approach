{"title": "Label Efficient Learning of Transferable Representations across Domains  and Tasks", "tag": ["stat.ML", "cs.CV"], "abstract": "We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.", "text": "propose framework learns representation transferable across different domains tasks label efﬁcient manner. approach battles domain shift domain adversarial loss generalizes embedding novel task using metric learning-based approach. model simultaneously optimized labeled source data unlabeled sparsely labeled data target domain. method shows compelling results novel classes within domain even labeled examples class available outperforming prevalent ﬁne-tuning approach. addition demonstrate effectiveness framework transfer learning task image object recognition video action recognition. humans exceptional visual learners capable generalizing learned knowledge novel domains concepts capable learning examples. recent years computational models based end-to-end learnable convolutional networks made signiﬁcant improvements visual recognition shown demonstrate cross-task generalizations enabling faster learning subsequent tasks frequently evidenced ﬁnetuning however efforts focus supervised learning scenario closed world assumption made training time domain interest tasks learned. thus generalization ability models observed byproduct. large push research community address generalizing adapting deep models across different domains learn tasks data efﬁcient shot learning generically transfer information across tasks approaches consider scenarios isolation directly tackle joint problem adapting novel domain tasks annotations. given large labeled source dataset annotations task seek transfer knowledge sparsely labeled target domain possibly wholly task setting line intuition able learn reusable general purpose representations enable faster learning future tasks requiring less human intervention. addition setting matches closely common practical approach training deep models large labeled source dataset train initial representation continue supervised learning data often concepts. approach jointly adapt source representation distinct target domain using multilayer unsupervised domain adversarial formulation introducing novel cross-domain within domain class similarity objective. objective applied even target domain non-overlapping classes source domain. evaluate approach challenging setting joint transfer across domains tasks demonstrate ability successfully transfer reducing need annotated data target domain tasks. present results transferring subset google street view house numbers containing digits subset mnist containing digits secondly present results challenging setting adapting imagenet object-centric images ucf- videos action recognition. domain adaptation. domain adaptation seeks learn related source domains well performing model target data distribution existing work often assumes domains deﬁned task labeled data target domain sparse non-existent several methods tackled problem maximum mean discrepancy loss source target domain. weight sharing parameters minimizing distribution discrepancy network activations also shown convincing results. adversarial generative models generating source-like data target data training generator discriminator simultaneously adversarial discriminative models focus aligning embedding feature representations target domain source domain. inspired adversarial discriminative models propose method aligns domain features multi-layer information. transfer learning. transfer learning aims transfer knowledge leveraging existing labeled data related task domain computer vision examples transfer learning include overcome deﬁcit training samples categories adapting classiﬁers trained categories power deep supervised learning imagenet dataset learned knowledge even transfer totally different task image classiﬁcation semantic segmentation achieve state-of-the-art performance. paper focus setting source target domains differing label spaces label spaces share structure. namely adapting classifying different category sets transferring classiﬁcation localization plus classiﬁcation task. few-shot learning. few-shot learning seeks learn concepts annotated examples. deep siamese networks trained rank similarity examples. matching networks learns network maps small labeled support unlabeled example label. aside metric learning-based methods meta-learning also served essential part. ravi propose learn lstm meta-learner learn update rule learner. finn tries good initialization point easily ﬁne-tune examples tasks. exists domain shift results prior few-shot learning methods often degraded. unsupervised learning. many unsupervised learning algorithms focused modeling data using reconstruction objectives probabilistic models include restricted boltzmann machines deep boltzmann machines gans autoregressive models also popular. alternative approach often terms self-supervised learning deﬁnes pretext task predicting patch ordering frame ordering motion dynamics colorization form indirect supervision. compared approaches unsupervised learning method rely exploiting spatial temporal structure data therefore generic. introduce semi-supervised learning algorithm transfers information large labeled source domain sparsely labeled target domain goal learn strong target figure proposed learning framework joint transfer across domains semantic transfer across source target across target labeled unlabeled data. introduce domain discriminator aligns source target representations across multiple layers network domain adversarial learning. enable semantic transfer minimizing entropy pairwise similarity unlabeled labeled target images temperature softmax similarity vector allow non-overlapping label spaces. classiﬁer without requiring large annotation overhead required standard supervised learning approaches. fact setting commonly explored convolutional network based recognition methods. learning convnets usual learning procedure large labeled dataset initial training network parameters learned weights used initialization continued learning data tasks called ﬁne-tuning. fine-tuning broadly applied reduce number labeled examples needed learning tasks recognizing object categories imagenet pre-training learning label structures detection classﬁciation pretraining focus transfer case shared label structure assume source domain contains images associated labels similarly target domain consists unlabeled images well images associated labels assume target domain sparsely labeled number image-label pairs much smaller number unlabeled images additionally number source labeled images assumed much larger number target labeled images unlike standard domain adaptation approaches transfer knowledge source target domains assuming marginal conditional distribution shift shared label space tackle joint image feature space adaptation well transfer across semantic spaces. namely consider case source target label spaces equal even challenging case sets non-overlapping approach consists unsupervised feature alignment source target well semantic transfer unlabeled target data either labeled target labeled source data. introduce multi-layer domain discriminator used domain alignment following recent domain adversarial learning approaches next introduce semantic transfer learning objective uses cross category similarity tuned account varying size label overlap. depict overall model figure take source labeled examples target labeled examples unlabeled target images {˜xt} input. learn initial layered source representation classiﬁcation network using standard supervised techniques. initialize target model source parameters begin adaptive transfer learning. model jointly optimizes target supervised loss lsup domain transfer objective ﬁnally semantic transfer objective lst. thus total objective written follows lsup αldt βlst hyperparameters determine inﬂuence domain transfer loss semantic transfer loss respectively. following sections elaborate domain semantic transfer objectives. deﬁne novel domain alignment objective function called multi-layer domain adversarial loss. recent efforts deep domain adaptation shown strong performance using feature space domain adversarial objectives methods learn target representation target distribution viewed model aligned source distribution viewed source representation. alignment accomplished adversarial minimization across domain analogous prevalent generative adversarial approaches particular domain discriminator trained classify whether particular data point arises source target domain. simultaneously target embedding function consider representation domain invariant domain discriminator distinguish examples domains. prior work considers alignment single layer embedding time learns domain discriminator takes output corresponding source target layers input. separately domain alignment methods focus ﬁrst second order statistics shown improved performance applying domain alignment independently multiple layers network rather learning independent discriminators layer network propose simultaneous alignment multiple layers multi-layer discriminator. layer multi-layer domain discriminator information accumulated output previous discriminator layer well source target activations corresponding layer respective embeddings. thus output discriminator layer deﬁned current layer activation function decay factor represents concatenation element-wise summation taken either source data target data notice intermediate discriminator layers share structure corresponding encoding layers match dimensions. thus following loss functions proposed optimize multi-layer domain discriminator embeddings respectively according domain transfer objective outputs last layer source target multi-layer domain discriminator. note losses placed ﬁnal domain discriminator layer last embedding layer produce gradients back-propagate throughout relevant lower layer parameters. losses together comprise iterative optimization procedure involved. multi-layer discriminator allows deeper alignment source target representations empirically results improved target classiﬁcation performance well stable adversarial learning. figure illustrate purpose temperature pairwise similarity vector. consider example target unlabeled point similarity four labeled source points show here original unnormalized scores well similarity scores applying softmax different temperatures notice entropy values higher variance scores normalized small temperature softmax. previous section introduced method transferring embedding source target domain. however enforces alignment global domain statistics class speciﬁc transfer. here deﬁne semantic transfer objective transfers information labeled data unlabeled data minimizing entropy softmax temperature similarity vector unlabeled point labeled points. thus loss applied either source unlabeled target data labeled unlabeled target data. unlabeled target image compute similarity labeled example prototypical example class labeled set. simplicity presentation consider semantic transfer source target domain ﬁrst. target unlabeled image compute similarity vector element similarity target image labeled source image semantic transfer loss deﬁned follows where information entropy function softmax function temperature softmax. note temperature used directly control percentage source examples expect target example similar entropy minimization widely used unsupervised semi-supervised learning encouraging density separation clusters classes. recently principle entropy minimization applied unsupervised adaptation here source target domains assumed share label space unlabeled target example passed initial source classiﬁer entropy softmax output scores minimized. contrast assume shared label space source target domains assume target image maps single source label. instead compute pairwise similarities target points source points across features spaces aligned multi-layer domain adversarial transfer. tune softmax temperature based expected similarity source target labeled set. example source target label overlap small temperature encourage target point similar source class whereas larger temperature allow target points similar multiple source classes. semantic transfer within target domain utilize metric-based cross entropy loss labeled target examples stabilize improve learning. labeled target example addition traditional cross entropy loss also calculate metric-based cross entropy loss assume labeled examples class target domain. compute embedding example centroid class embedding space. thus compute similarity vector labeled example element similarity labeled example centroid class calculate metric based cross entropy loss metric-based cross entropy loss introduce constraint target domain data similar embedding space. also loss provide guidance unsupervised semantic transfer learn stable way. combination lstunsupervised source-target lstsupervised source-target lstunsupervised target-target i.e. section structured follows. section show method outperform ﬁne-tuning approach large margin parts method necessary. section show method generalized bigger datasets. section show multi-layer domain adversarial method outperforms state-of-the-art domain adversarial approaches. datasets perform adaptation experiments across different paired data settings. first adaptation across different digit domains mnist google street view house numbers mnist handwritten digits database training examples test examples. digits size-normalized centered ﬁxed-size images. svhn real-world image dataset machine learning object recognition algorithms minimal requirement data preprocessing formatting. digits training digits testing. second experimental setup consider adaptation object centric images imagenet action recognition video using ucf- dataset. imagenet large benchmark object classiﬁcation task. task split ilsvrc. ucf- action recognition dataset collected youtube. videos action categories ucf- provides large diversity terms actions presence large variations camera motion object appearance pose object scale viewpoint cluttered background illumination conditions etc. implementation details pre-train source domain embedding function cross-entropy loss. domain adversarial loss discriminator takes last three layer activations input number output classes source target tasks takes second last third last layer activations different. similarity score chosen product normalized support features unnormalized target feature. temperature source-target semantic transfer within target transfer label space shared. objective function. network trained adam optimizer learning rate conduct experiments pytorch framework. svhn mnist experimental setting. experiment deﬁne three datasets labeled data source domain labeled data target domain unlabeled data target domain take training split svhn dataset dataset fairly compare traditional learning paradigm episodic training subsample examples class construct dataset perform traditional training episodic -shot learning. experiment corresponds labeled examples total training data respectively. since approach involves using annotations small subset data randomly subsample different subsets training split mnist dataset remaining data note source domain target domain non-overlapping classes utilize digits svhn digits mnist. baselines prior work. compare different methods target only model trained scratch; fine-tune model pretrained ﬁne-tuned matching networks ﬁrst pretrain model support matching networks; fine-tuned matching networks baseline except model ﬁne-tuned -way -shot learning examples class randomly selected support last example class used query set; fine-tune adversarial addition baseline model also trained domain adversarial loss; full model ﬁne-tune model proposed multi-layer domain adversarial loss. results analysis. calculate mean standard error accuracies across sets data shown table domain shift matching networks perform poorly without ﬁne-tuning ﬁne-tuning marginally better training scratch. method multi-layer adversarial improves overall performance sensitive subsampled data. method achieves signiﬁcant performance gain especially number labeled examples small reference ﬁne-tuning full target dataset gives accuracy table test accuracies baseline models method. correspond methods proposed section note accuracies matching methods calculated based nearest neighbors support set. report mean standard error method across different subsampled data. figure t-sne visualization different feature embeddings. source domain embedding. target domain embedding using encoder trained source domain domain. target domain embedding using encoder ﬁne-tuned target domain data. target domain embedding using encoder trained method. overlap overlap problem analysis. many recent works study domain shift images video object detection settings. compared still images videos provide several advantages motion provides information foreground background segmentation videos often show multiple views thus provide information. hand video frames usually suffer from motion blur; compression artifacts; objects out-of-focus out-of-frame. experimental setting. experiment focus three dataset splits imagenet training labeled data source domain video clips class randomly sampled ucf- training labeled data target domain remaining videos ucf- training unlabeled data target domain experiment corresponds video clips total training data respectively. experiment times baselines prior work. compare method baseline methods target only model trained scratch; fine-tune model ﬁrst pre-trained ﬁne-tuned reference report performance fully supervised method results analysis. accuracy model shown table also ﬁne-tune model labeled data comparison. per-frame performance average-across-frame performance reported. note calculate average-across-frame performance averaging softmax score frame video. method achieves signiﬁcant improvement average-across-frame performance standard ﬁne-tuning value note compared ﬁne-tuning method bigger per-frame per-video accuracy. believe semantic transfer entropy loss encourages sharper softmax variance among per-frame softmax scores video making conﬁdent predictions among frames method achieves signiﬁcant gain respective per-video performance even little change per-frame prediction. table accuracy ucf- action classiﬁcation. results two-stream spatial model taken vary depending hyperparameters. report mean standard error method across different subsampled data. validate multi-layer domain adversarial loss objective conduct ablation experiment unsupervised domain adaptation. compare multiple recent domain adversarial unsupervised adaptation methods. experiment ﬁrst pretrain source embedding training split svhn adapt target embedding mnist performing adversarial domain adaptation. evaluate classiﬁcation performance test split mnist follow training strategy model architecture embedding network models two-step training strategy share ﬁrst stage. adda optimizes encoder classiﬁer simultaneously. also propose similar method optimize encoder only. model classiﬁer last layer choose decay factor model. accuracy model shown table method achieve performance gain best competing domain adversarial approach indicating multilayer objective indeed contributes overall performance. addition experiments found multilayer approach improved overall optimization stability evidenced small standard error. table experimental results unsupervised domain adaptation svhn mnist. results gradient reversal domain confusion adda results methods experiments across different subsampled data. paper propose method learn representation transferable across different domains tasks data efﬁcient manner. framework trained jointly minimize domain shift transfer knowledge task learn large amounts unlabeled data. show superior performance popular ﬁne-tuning approach. hope keep improving method future work. would like start thanking sponsors stanford computer science department stanford program ai-assisted care next specially thank de-an huang kenji hata serena yeung ozan sener members stanford vision learning insightful discussion feedback. lastly thank anonymous reviewers valuable comments. references yusuf aytar andrew zisserman. tabula rasa model transfer object category detection. computer vision ieee international conference pages ieee konstantinos bousmalis nathan silberman david dohan dumitru erhan dilip krishnan. unsupervised pixel-level domain adaptation generative adversarial networks. arxiv preprint arxiv. lluis castrejon yusuf aytar carl vondrick hamed pirsiavash antonio torralba. learning aligned cross-modal representations weakly aligned data. proceedings ieee conference computer vision pattern recognition pages carl doersch abhinav gupta alexei efros. unsupervised visual representation learning context prediction. proceedings ieee international conference computer vision pages jeff donahue yangqing oriol vinyals judy hoffman ning zhang eric tzeng trevor darrell. decaf deep convolutional activation feature generic visual recognition. proceedings international conference international conference machine learning volume icml’ pages i––i–. jmlr.org yaroslav ganin evgeniya ustinova hana ajakan pascal germain hugo larochelle françois laviolette mario marchand victor lempitsky. domain-adversarial training neural networks. journal machine learning research ross girshick jeff donahue trevor darrell jitendra malik. rich feature hierarchies accurate object detection semantic segmentation. computer vision pattern recognition goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems pages gretton a.j. smola huang marcel schmittfull k.m. borgwardt schölkopf quiñonero candela sugiyama schwaighofer lawrence. covariate shift kernel mean matching. dataset shift machine learning kaiming xiangyu zhang shaoqing jian sun. deep residual learning image recognition. proceedings ieee conference computer vision pattern recognition pages judy hoffman saurabh gupta trevor darrell. learning side information modality hallucination. proceedings ieee conference computer vision pattern recognition pages judy hoffman saurabh gupta jian leong sergio guadarrama trevor darrell. crossmodal adaptation rgb-d detection. robotics automation ieee international conference pages ieee vicky kalogeiton vittorio ferrari cordelia schmid. analysing domain shift factors videos images object detection. ieee transactions pattern analysis machine intelligence joseph ruslan salakhutdinov antonio torralba. transfer learning borrowing examples multiclass object detection. proceedings international conference neural information processing systems pages curran associates inc. dragomir anguelov dumitru erhan christian szegedy scott reed cheng-yang alexander berg. single shot multibox detector. european conference computer vision pages springer jonathan long evan shelhamer trevor darrell. fully convolutional networks semantic segmentation. proceedings ieee conference computer vision pattern recognition pages mingsheng long jianmin wang michael jordan. unsupervised domain adaptation residual transfer networks. advances neural information processing systems pages ishan misra lawrence zitnick martial hebert. shufﬂe learn unsupervised learning using temporal order veriﬁcation. european conference computer vision pages springer yuval netzer wang adam coates alessandro bissacco andrew reading digits natural images unsupervised feature learning. nips workshop deep learning unsupervised feature learning volume page maxime oquab leon bottou ivan laptev josef sivic. learning transferring midlevel image representations using convolutional neural networks. proceedings ieee conference computer vision pattern recognition pages sharif razavian hossein azizpour josephine sullivan stefan carlsson. features off-the-shelf astounding baseline recognition. ieee conference computer vision pattern recognition cvpr workshops columbus june pages joseph redmon santosh divvala ross girshick farhadi. look once uniﬁed real-time object detection. proceedings ieee conference computer vision pattern recognition pages shaoqing kaiming ross girshick jian sun. faster r-cnn towards real-time object detection region proposal networks. advances neural information processing systems pages olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein imagenet large scale visual recognition challenge. international journal computer vision karen simonyan andrew zisserman. two-stream convolutional networks action recognition videos. advances neural information processing systems pages kevin tang vignesh ramanathan fei-fei daphne koller. shifting weights adapting object detectors image video. advances neural information processing systems pages tatiana tommasi francesco orabona barbara caputo. safety numbers learning categories examples multi model knowledge transfer. computer vision pattern recognition ieee conference pages ieee eric tzeng judy hoffman trevor darrell kate saenko. simultaneous deep transfer across domains tasks. proceedings ieee international conference computer vision pages aaron oord kalchbrenner lasse espeholt oriol vinyals alex graves conditional image generation pixelcnn decoders. advances neural information processing systems pages pascal vincent hugo larochelle yoshua bengio pierre-antoine manzagol. extracting composing robust features denoising autoencoders. proceedings international conference machine learning pages", "year": 2017}