{"title": "EDEN: Evolutionary Deep Networks for Efficient Machine Learning", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "Deep neural networks continue to show improved performance with increasing depth, an encouraging trend that implies an explosion in the possible permutations of network architectures and hyperparameters for which there is little intuitive guidance. To address this increasing complexity, we propose Evolutionary DEep Networks (EDEN), a computationally efficient neuro-evolutionary algorithm which interfaces to any deep neural network platform, such as TensorFlow. We show that EDEN evolves simple yet successful architectures built from embedding, 1D and 2D convolutional, max pooling and fully connected layers along with their hyperparameters. Evaluation of EDEN across seven image and sentiment classification datasets shows that it reliably finds good networks -- and in three cases achieves state-of-the-art results -- even on a single GPU, in just 6-24 hours. Our study provides a first attempt at applying neuro-evolution to the creation of 1D convolutional networks for sentiment analysis including the optimisation of the embedding layer.", "text": "networks image classiﬁcation problems using parallel system executed computers achieved considerable success cifar image problems. zoph instead recurrent neural networks along reinforcement learning learn good architectures. eight hundred networks trained gpus. miikkulainen propose codeepneat population modules blueprints evolved. blueprints made several nodes point particular modules representing neural networks. thus proposed approach allows evolution repetitive structures enabling blueprints reuse evolved modules. desell proposed exact neuro-evolutionary algorithm deployment distributed cluster executed across volunteered computers evolved networks tackle mnist dataset. approach pooling layers limited dimensional input ﬁlters. finally note single recently evolved deep networks accurately identify whether supervised machine learning challenge requires regression classiﬁcation achieving average accuracy across diverse tasks. direct precursor current work given sufﬁcient computing resources seamlessly integrated network optimisation discuss here. work propose evolutionary deep networks neuro-evolutionary algorithm combines strengths genetic algorithms deep neural networks explore search space neural network architectures associated hyperparameters number epochs applied. study explore additional features optimisation embedding layers increase complexity existing research. eden interested addressing questions evolve generally good architectures hyperparameters broad range problems successfully achieved single opposed large clusters used previous studies? interface eden tensorflow thus layers functions features easily incorporated controlled eden represent function calls respective tensorflow functions. additionally eden limited tensorflow modern deep neural network abstract—deep neural networks continue show improved performance increasing depth encouraging trend implies explosion possible permutations network architectures hyperparameters little intuitive guidance. address increasing complexity propose evolutionary deep networks computationally efﬁcient neuro-evolutionary algorithm interfaces deep neural network platform tensorflow. show eden evolves simple successful architectures built embedding convolutional pooling fully connected layers along hyperparameters. evaluation eden across seven image sentiment classiﬁcation datasets shows reliably ﬁnds good networks three cases achieves state-of-the-art results even single hours. study provides ﬁrst attempt applying neuro-evolution creation convolutional networks sentiment analysis including optimisation embedding layer. deep neural networks powerful unintuitive beasts whose wrangling requires experience signiﬁcant trial error achieve good performance. performance networks continued improve depth increased e.g. along rising inﬂuence deep learning ﬁelds means becoming important develop methods automatically design optimal nearoptimal network architectures hyperparameters. deciding exact nature order layers choice activation functions number units fully connected layers number ﬁlters convolutional layers variables creating deep neural networks non-trivial. given huge computing resources possible simply vast number possible combinations. competitive small amount computing power single gpu? solution pursue here evolve good neural networks evolutionary algorithms neuro-evolutionary algorithms spanning nearly three decades e.g. beginning study evolved weights neural network brieﬂy summarise recent related work neuroevolutionary algorithms which contrast study used signiﬁcant computing resources. real proposed neuro-evolutionary approach optimise neural associated video illustrates evolution chromosomes execution eden mnist image classiﬁcation problem showing population converging towards efﬁcient solution made primarily twodimensional convolutional layers. algorithm modiﬁed genetic algorithm used study input epochs number neural network epochs input population size population size input generation maximum number generation epochs epochs. population size population size. create initial population chromosomes. evaluate initial population. generation generation genes encoded. provide advantage optimisation algorithms ﬂuently handle complex combinations discrete continuous search spaces making ideal neuro-evolutionary studies; e.g. eden chromosome made genes genes constitute required components optimise single neural network given input classiﬁcation dataset. genes encode learning rate network architecture. learning rate denotes value applied training optimisation. architecture represents exact order neural network layers operations. following layers operations made available eden two-dimension convolution one-dimension convolution fully connected dropout one- twodimension pooling embedding inappropriate choices penalised described sentiment analysis tasks instead using pre-trained vectors wordvec setting pre-determined embedding dimension size decided allow eden learn dimension word embeddings part optimisation. created dictionary mapping unique words frequency count training data. took fig. eden chromosome contains genes encoding learning rate neural network. ﬁgure illustrates example neural network evolved using eden sentiment analysis task. eden created embedding layer output dimension followed three convolutional layers. eden evolved number ﬁlters ﬁlters’ dimension along corresponding activation function. last layer selected activation function eden determined sigmoid function. learning rate chromosome genetic algorithm evolutionary algorithm applied solve optimisation problems. population chromosomes randomly initialised. chromosome represents candidate solution optimisation problem. ﬁtness function used evaluate chromosome determine extent chromosome solve problem. generational model iterates multiple times known generations predetermined condition chromosome made several genes genes altered using genetic operator. resulting chromosome application genetic operator known offspring. multiple offspring created based population size. offspring replace current chromosome population generation. study used traditional additionally increment number neural network epochs along number generations explore best value number epochs. algorithm presents used. frequent words used encode text vectors integers. enabling eden optimise vocabulary size embedding would result signiﬁcant computation time hence included study. layer randomly generated activation function also randomly selected. convolutional layers choose following functions {linear leaky relu prelu relu}. fully connected layers choose from {linear sigmoid softmax relu}. last fully connected layer network {linear sigmoid softmax}. functions selected commonly used literature. however possible include larger number activations functions. ﬁrst phase evolutionary algorithm create initial population chromosomes. chromosomes denote ﬁrst generation solutions optimisation problem i.e. case generate neural networks correctly classify data. number chromosomes create initial population user-deﬁned parameter. chromosome created evaluated determine close optimal solution section iv-d. initial population generation method used study inspired ramped-half-and-half method proposed koza enables creation candidate solutions various sizes. similar manner implemented initial population generation method would create neural network architectures various sizes increase amount diversity initial population algorithm outlines pseudocode chromosome randomly generated algorithm presents initial population generation method used study. certain cases invalid architectures generated invalid architectures discarded generated. given computational limitations limit search space setting bounds certain variables. real implement limitations however worth noting study used machines. keep probability dropout randomly generated acceptable values. bounds variable listed below. number ﬁlters convolution ﬁlter size convolution kernel size pooling number units fully connected layers embedding layer output size generation parents must selected create offspring using genetic operator. parents obtained using parent selection method. three common parent selection methods ﬁtness-proportionate rank tournament selection study tournament selection algorithm works follows. number chromosomes randomly selected current population. tournament size user-deﬁned parameter. chromosomes randomly selected evaluated using ﬁtness function. chromosome smallest ﬁtness returned parent used genetic operator. recombination genetic operator included study similar real execution mutation operator single parent obtained using tournament selection. mutation operator applied parent generate offspring. mutation operator applied offspring consequently creates offspring. ﬁtness offspring offspring original parent chromosome compared. chromosome lowest ﬁtness returned placed population. preliminary experiments revealed performing mutation parent prohibited algorithm sufﬁciently exploring search space. reason repeated mutation operator generate variations offspring. details mutation operator changes chromosome follows. given chromosome operator randomly changes either chromosome’s learning rate neural network layers. case learning rate selected value learning rate randomly generated discussed section iii. case neural network layers selected operator either adds layer deletes layer replaces one. choice made based size architecture. size chromosome’s architecture reached maximum size layer either deleted replaced. however size less maximum allowed size layer either added deleted replaced. constraint place mutation operator cannot remove ﬁrst last layers. deletion performed randomly selecting layer removing network architecture chromosome. replacement performed randomly selecting layer within architecture removing entirely layer generated inserted position removed. addition generates layer adds anywhere architecture. possible randomness within mutation results invalid neural network architectures. application mutation operator check performed assess validity resulting architecture. mutation generates invalid architecture mutation operator applied valid generated. number neural network parameters computed offspring created. parameters represent number trainable weights neural network. larger values denote complex models small numbers consequently denote less complex ones. ﬁtness function required steer eden towards optimal solution. function computes ﬁtness score numerical value denotes ‘good’ chromosome study chose ﬁtness function makes error validation well number trainable parameters. relative importance controlled complexity parameter. ﬁtness function rewards less complex accurate models compared complex less accurate ones. furthermore helps break ties chromosomes validation error. changed depending relative importance performance versus need small networks problem hand. architecture hyperparameters encoded chromosome used train neural network using tensorflow. training data used training network. categorical cross entropy loss function used training. network done training neural network evaluated validation data using ﬁtness function. ﬁtness obtained function stored dataset executed eden times averaged results eden evaluated single machine geforce ram. evolutionary process experiment used based dataset utilisation varied percent training neural networks. algorithm developed python tensorflow keras keras used determine number parameters neural networks contained chromosome. operating system ubuntu lts. table presents datasets eden evaluated imdb electronics sentiment analysis datasets. datasets namely mnist cifar- fashion-mnist emnist datasets image classiﬁcation problems. dataset eden trained training data validation used evaluate performance chromosomes test used reporting results. training testing split presented table. datasets contain missing values class values converted respective one-hot encoded values. table present neural network parameters used throughout study. preliminary runs performed obtain values. purpose eden amongst things evolve neural network’s hyperparameters thus parameters presented table values input eden. table presents average test accuracy number trainable parameters. best results obtained using adam. eden initially conﬁgured include optimiser function chromosome results revealed neural network parameters used study. conducted additional experiments select parameters. number generations high value avoid extreme runtimes. neural network parameters used table shows results. eden achieved stateof-the-art results emnist-balanced emnist-digits fashion-mnist datasets. sentiment analysis tasks eden evolved neural networks produced good sub-state-of-the-art accuracy despite eden’s ability optimise embedding layer. future work determine effect also allowing eden optimise vocabulary size. average evolved learning rate ranged average execution times hours single eden experiment generations imdb elec emnist-balanced emnist-digits cifar- fashionmnist respectively. addition enforce constraint networks receive epochs training. result eden took average hours mnist dataset signiﬁcantly less month execution time exact achieved similar accuracy eden however produce competitive results cifar- obtaining average test accuracy training epochs ﬁnal network evolved hours compared current state-of-the-art primarily layer depth constraint imposed running eden single gpu. result best model eden evolved parameters million used state-of-the-art figures illustrate change ﬁtness learning rate evolutionary process. ﬁgures show convergence population. ﬁtness rapidly decreases random initial population generation ﬁtness decreases slower rate. fig. illustrating change mean ﬁtness generations mnist data. error bars show percentile values ﬁtness across population. initially signiﬁcant variance ﬁtness reduces solutions improve population converges. also show three networks sampled initial mid-point ﬁnal generations along associated hyperparameters. show best evolved network three stages evolution. represent convolution pooling drop fully connected layers respectively. determining optimal efﬁcient deep neural network architectures hyperparameters challenging task. researchers practitioners creation deep neural networks beneﬁt algorithms automatically create architectures determine hyperparameters. study propose eden neuro-evolutionary algorithm interfaces tensorflow deep neural network platform automatically create architectures optimise hyperparameters. eden evaluated classiﬁcation problems easily applied regression problems. eden designed evolve efﬁcient deep networks dataset executed single running hours less. ﬁndings reveal competitive results obtained using signiﬁcantly less computational power deployed neuro-evolutionary studfig. change mean learning rate generations. error bars show percentile value terms learning rate variance population. initially chromosomes random variance learning rate. changes population converges towards better solutions. ies. evaluated image classiﬁcation sentiment analysis problems eden achieves state-of-the-art results three seven datasets. study also ﬁrst attempt applying neuro-evolution creation convolutional networks sentiment analysis optimising embedding layer sentiment analysis. future work intend extending eden evolve generative adversarial networks architectures well exploring parallel implementations. ﬁnancial assistance national research foundation towards research hereby acknowledged. opinions expressed conclusions arrived authors necessarily attributed nrf. srivastava greff schmidhuber training deep networks proceedings international conference neural information processing systems ser. nips’. cambridge press available http//dl.acm.org/citation.cfm?id=. miller todd hegde designing neural networks using genetic algorithms proceedings international conference genetic algorithms. francisco morgan kaufmann publishers inc. arifovic genay using genetic algorithms select architecture feedforward artiﬁcial neural network physica statistical mechanics applications vol. dufourq bassett automated problem identiﬁcation regression classiﬁcation evolutionary deep networks proceedings annual conference south african institute computer scientists information technologists maas learning word vectors sentiment analysis proceedings association computational linguistics human language technologies volume ser. stroudsburg association computational linguistics zhong comparison performance different selection strategies simple genetic algorithms international conference computational intelligence modelling control automation international conference intelligent agents technologies internet commerce vol. chollet keras https//github.com/fchollet/keras maas learning word vectors sentiment analysis association proceedings computational linguistics human language technologies. portland oregon association computational linguistics xiao fashion-mnist novel image dataset benchmarking machine learning algorithms arxiv preprint arxiv. cohen emnist extension mnist handwritten letters al.regularization neural networks using dropconnect proceedings international conference international conference machine learning volume ser. icml’. jmlr.org iii––iii–.", "year": 2017}