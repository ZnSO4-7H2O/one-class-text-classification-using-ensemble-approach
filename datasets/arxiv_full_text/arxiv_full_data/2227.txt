{"title": "The Uncertainty Bellman Equation and Exploration", "tag": ["cs.AI", "cs.LG", "math.OC", "stat.ML"], "abstract": "We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar uncertainty Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the estimated value of any fixed policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for $\\epsilon$-greedy improves DQN performance on 51 out of 57 games in the Atari suite.", "text": "consider exploration/exploitation problem reinforcement learning. exploitation well known bellman equation connects value time-step expected value subsequent time-steps. paper consider similar uncertainty bellman equation connects uncertainty time-step expected uncertainties subsequent time-steps thereby extending potential exploratory beneﬁt policy beyond individual time-steps. prove unique ﬁxed point yields upper bound variance estimated value ﬁxed policy. bound much tighter traditional count-based bonuses compound standard deviation rather variance. importantly unlike several existing approaches optimism method scales naturally large systems complex generalization. substituting ube-exploration strategy \u0001-greedy improves performance games atari suite. consider reinforcement learning problem agent interacting environment maximize cumulative rewards time model environment markov decision process agent initially uncertain true dynamics time-step agent performs action receives reward moves next state; data learn actions lead higher payoffs. leads exploration versus exploitation trade-off agent investigate poorly understood states actions improve future performance instead take actions maximize rewards given current knowledge? separating estimation control ‘greedy’ algorithms lead premature suboptimal exploitation. offset this majority practical implementations introduce random noise dithering action selection algorithms eventually explore every reachable state action inﬁnitely often take exponentially long learn optimal policy contrast prior beliefs optimal exploration policy computed directly dynamic programming bayesian belief space. however approach computationally intractable even small problems direct computational approximations fail spectacularly badly reason provably-efﬁcient approaches reinforcement learning rely upon optimism face uncertainty heuristic algorithms give bonus poorly-understood states actions subsequently follow policy optimal augmented optimistic mdp. optimism incentivises exploration agent learns environment scale bonus decrease agent’s performance approach optimality. high level approaches ofu-rl build conﬁdence sets contain true high probability techniques provide performance guarantees ‘near-optimal’ terms problem parameters.however apart simple ‘multi-armed bandit’ setting state still signiﬁcant upper lower bounds algorithms inefﬁciency algorithms that although concentration tight state action independently combination simultaneously optimistic estimates result extremely over-optimistic estimate whole works suggested bayesian posterior sampling approach suffer inefﬁciencies lead performance improvements methods paper explore alternative approach harnesses simple relationship uncertainty bellman equation deﬁne uncertainty variance value estimator agent learning sense similar parametric variance mannor intuitively speaking agent high uncertainty region state-space explore there order better estimate q-values. show that bellman equation relates value policy beyond single time-step uncertainty bellman equation propagate uncertainty values multiple time-steps thereby facilitating ‘deep exploration’ beneﬁt approach harness existing machinery deep reinforcement learning minimal change existing network architectures. resulting algorithm shares intimate connection existing literature intrinsic motivation recent work connected approaches notion ‘pseudo-count’ generalization number visits state action. rather pseudo-count work builds upon idea fundamental quantity relates uncertainty estimated value function naively compounding count-based bonuses lead inefﬁcient conﬁdence sets difference compounds variances step rather standard deviation. observation higher moments value function also satisfy form bellman equation observed early papers subject unlike prior work focus upon epistemic uncertainty mean value function rather higher moments reward-to-go application rich environments complex generalization deep learning architecture learn solution according observed data style problem formulation consider inﬁnite horizon discounted ﬁnite state action space state space action space rewards time period denoted policy mapping state-action pair probability taking action state. time-step agent receives state reward selects action policy agent moves next state probability transitioning state state taking action goal agent maximize expected total discounted return policy policy state optimal action-value function maxπ policy achieves maximum optimal policy bellman operator policy relates value time-step value subsequent expectation taken next state reward action policy bellman operator contraction therefore unique ﬁxed point several reinforcement learning algorithms designed around minimizing bellman residual propagate knowledge immediate rewards long term value next section examine similar relationship propagating uncertainties estimator call relationship uncertainty bellman equation. section derive bellman-style relationship propagates uncertainty qvalue estimator across multiple time-steps. propagating potential value exploration many timesteps deep exploration important statistically efﬁcient main result state theorem based upon nothing dynamic programming recursion crude upper bounds several intermediate terms. show even simple settings approach result well-calibrated uncertainty estimates common count-based bonuses inefﬁcient begin motivating uncertainty estimator useful exploration consider problem learning value policy data true unknown values satisfy bellman equation estimate random variable potentially bias variance. write bellman residual could calculate exactly everywhere could solve bellman equation obtain true q-values. however true intractable calculate since requires exact application bellman operator. instead consider case estimate moments bellman residual easily even though can’t calculate exactly. therefore know bound variance bias estimator construct intervals contain true q-values high-probability. agent apply ofu-principle prioritize exploration towards potentially rewarding policies argue that many settings interest error dominated variance term that case several simplifying relationships emerge. purposes analysis assume bellman residuals state-action pair uncorrelated. property certainly hold settings reasonable approximation many settings interest. assumption pointwise. proof. show three essential properties bellman operator ﬁxed policy first bellman operator γ-contraction norm ﬁxed point exists unique. second value iteration converges starting finally bellman operator monotonically increasing arguments i.e. pointwise pointwise. variance satisﬁes bellman inequality consider simple decision problem known deterministic transitions unknown rewards actions. imagine agent gathered data produces unbiased value estimates. according estimates ﬁrst action leads single reward expectation zero variance second action leads inﬁnite chain independent states expectation zero variance numbers chosen variance estimated value action mean zero variance optimistic agent motivated reason value action other. nonetheless existing approaches optimism work exploration bonus would lead inconsistent decision rule setting rather consider variance value whole majority existing approaches provide exploration bonuses state action independently combine estimates union bound. context even state algorithm ucrl would afford state bonus proportional standard deviation estimate. action would proportional action would proportional tabular setting variance estimated reward state approximated inverse ‘count’ visits state action effectively means action would visited factor often obtain level conﬁdence estimation uncertainty bellman equation essential issue that unlike variance classic exploration bonus based upon standard deviation obey bellman-style relationship. section outlines uncertainty bellman equation used propagate local estimates variance global estimates uncertainty. section present pragmatic approaches estimating local uncertainty practical learning algorithms inspired theorem claim approaches approaches estimating local uncertainty even simple approximations sense ‘best’. investigating choices important area future research outside scope short paper. present simple progression tabular representations linear function approximation non-linear neural network architectures. tabular value estimate. consider setting learned separately. model bellman error variance bounded apply bound count number visits state taking action proxy local uncertainty theorem linear value estimate. consider linear value function estimator state action ﬁxed basis functions learned weights action. setting allows generalization states actions basis functions. ﬁxed dataset least squares solution action neural networks value estimate. approximating q-value function using neural network analysis hold. however last layer network linear q-values approximated weights last layer associate action output network last layer state words think neural network learning useful basis functions linear combination approximates q-values. then ignore uncertainty mapping reuse analysis purely linear case derive approximate measure local uncertainty might useful practice. scheme advantages. agent progresses learning state representation helps achieve goal maximizing return. agent learn attention small important details learn ignore large irrelevant changes desirable property point view using features drive exploration states differ irrelevant ways aliased state representation states differ small important ways mapped quite different state vectors permitting task-relevant measure local uncertainty. section describe exploration heuristic deep reinforcement learning whereby attempt learn q-values uncertainties associated simultaneously goal agent explore areas learns higher uncertainty. contrast commonly used \u0001-greedy boltzmann exploration strategies simply inject noise agents actions. policy uses thompson sampling action selected technique described pseudo-code algorithm refer technique one-step since uncertainty values updated using one-step sarsa bellman backup easily extendable n-step case. algorithm takes input neural network output heads attempting learn optimal q-values normal attempting learn uncertainty values current policy allow gradients uncertainty head trunk; ensures q-value estimates perturbed changing uncertainty signal. local uncertainty measure linear basis approximation described previously. dropped constant uncertainty bellman equation unknown term local-uncertainty equation absorbed hyper-parameter policy. assumptions allowed bound true q-values equation violated scheme particular ignored bias term policy changing q-values change. however might expect strategy provide useful signal novelty agent therefore perform well practice. input neural network outputting estimates input q-value learning subroutine qlearn input thompson sampling hyper-parameter initialize initial state take initial action repeat retrieve feature mapping input last layer q-head receive state reward calculate q-value estimates uncertainty estimates calculate action argmaxb b)/) calculate take gradient step subnetwork respect error update q-values using qlearn update according take action tmax present results algorithm atari suite games network attempting learn q-values uncertainties simultaneously. change vanilla made replace \u0001-greedy policy thompson sampling learned uncertainty values constant chosen games parameter sweep. used exact network architecture learning rate optimizer pre-processing replay scheme uncertainty head used single fully connected hidden layer hidden units followed output layer. trained uncertainty head using separate rmsprop optimizer learning rate addition uncertainty head computation associated reduced frame-rate compared vanilla speed cost approach negligible. compare versions approach -step method n-step method n-step method accumulates uncertainty signal time-steps performing update lead uncertainty signal propagating earlier encountered states faster expense increased variance signal. note cases q-learning update always -step; n-step implementation affects uncertainty update. compare approaches vanilla also exploration bonus intrinsic motivation approach agent receives augmented reward consisting extrinsic reward square root linear uncertainty scaled hyper-parameter chosen sweep. case stochastic policy still required good performance used \u0001-greedy annealing schedule. recent work bellemare follow-up work authors intrinsic motivation signal dqn-style agent modiﬁed full monte carlo return episode learning q-values. using monte carlo returns dramatically improves performance unrelated exploration change can’t compare numerical results directly. order point comparison implemented intrinisic motivation exploration signal discussed above. similarly can’t compare directly numerical results obtained bootstrap since agent using double-dqn variant achieves higher performance unrelated exploration. however note approach achieves higher evaluation score games tested bootstrap paper despite using inferior base implementation runs signiﬁcantly lower computational memory cost. trained strategies frames game strategy tested three times method hyper-parameters different random seeds plots scores correspond average seeds. scores normalized subtracting average score achieved agent takes actions uniformly random. every frames agents saved evaluated frames episode started random start condition described ﬁnal scores presented correspond ﬁrst averaging evalution score period across seeds taking average episodic score observed evalution period. tested strategies n-step approach highest performer games -step approach games game exploration bonus strategy games ties. table give mean median normalized scores percentage expert human normalized score across games number games agent ‘super-human’ tested algorithm. note mean scores signiﬁcantly affected single outlier high score therefore median score better indicator agent performance.in figure plot number games super-human performance frames method figure plot median performance across games frames methods. results across games well learning curves games given appendix. particular interest game ‘montezuma’s revenge’ notoriously difﬁcult exploration game one-step algorithm managed learn anything. -step strategy learns frames policy able consistently points score agent gets picking ﬁrst moving second room. figure show learning progress agents frames thompson sampling parameter slightly higher; instead frames n-step agent consistently getting around points several rooms progress. scores close state-of-the-art state-of-the-art one-step methods best knowledge. paper derived bellman equation uncertainties estimator q-values policy conditions. allows agent propagate uncertainty across many time-steps socalled ‘deep-exploration’ value propagates time standard dynamic programming recursion. uncertainty used agent make decisions states actions explore order gather data environment learn better policy. since uncertainty satisﬁes bellman recursion agent learn using reinforcement learning machinery developed value functions. showed algorithm based learned uncertainty boost performance standard deep-rl techniques. technique able improve average performance across atari suite games compared using \u0001greedy.", "year": 2017}