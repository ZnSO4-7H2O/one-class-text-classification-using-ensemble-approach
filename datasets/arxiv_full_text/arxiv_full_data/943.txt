{"title": "Optimizing Neural Networks with Kronecker-factored Approximate Curvature", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-Factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.", "text": "propose efﬁcient method approximating natural gradient descent neural networks call kronecker-factored approximate curvature k-fac based efﬁciently invertible approximation neural network’s fisher information matrix neither diagonal low-rank cases completely non-sparse. derived approximating various large blocks fisher kronecker product much smaller matrices. several times expensive compute plain stochastic gradient updates produced k-fac make much progress optimizing objective results algorithm much faster stochastic gradient descent momentum practice. unlike previously proposed approximate natural-gradient/newton methods high-quality non-diagonal curvature matrices k-fac works well highly stochastic optimization regimes. cost storing inverting k-fac’s approximation curvature matrix depend amount data used estimate feature typically associated diagonal low-rank approximations curvature matrix. problem training neural networks important highly investigated ones machine learning. despite work layer-wise pretraining schemes various sophisticated optimization methods approximate newton-raphson updates natural gradient updates stochastic gradient descent possibly augmented momentum remains method choice large-scale neural network training vinyals povey know updates computed using local curvature information make much progress iteration scaled gradient. reason sees fewer practical applications twofold. firstly updates much expensive compute involve running linear conjugate gradient potentially hundreds iterations requires matrix-vector product curvature matrix secondly hf’s estimate curvature matrix must remain ﬁxed iterates thus method able much less data comparable amount time making less well suited stochastic optimizations. discussed martens sutskever sutskever potential much faster local optimization gradient descent applied quadratic objective functions. thus insofar objective locally approximated quadratic step could potentially work iteration would result much faster overall sgd. however examples quadratic functions characterized curvature matrices highly spread-out eigenvalue distributions distinct advantage well-tuned gradient descent momentum. thus insofar quadratic functions optimized within character shouldn’t principle faster well-tuned momentum. extent neural network objective functions give rise quadratics unclear although sutskever provides preliminary evidence falls victim worst-case analysis ﬁrst-order method. motivates consider methods don’t rely ﬁrst-order methods like primary engines optimization. class methods widely studied work directly inverting diagonal block-diagonal low-rank approximation curvature matrix fact diagonal approximation fisher information matrix used within preconditioner however methods provide limited performance improvement practice especially compared momentum many practitioners tend forgo favor momentum. know curvature associated neural network objective functions highly nondiagonal updates properly respect account non-diagonal curvature generated make much progress minimizing objective plain gradient updates computed diagonal approximations curvature thus efﬁcient direct compute inverse high-quality non-diagonal approximation curvature matrix could potentially yield optimization method whose updates would large powerful like hf’s cheap compute stochastic gradient. work develop method call kronecker-factored approximate curvature show method much faster practice even highly tuned implementations momentum certain standard neural network optimization benchmarks. main ingredient k-fac sophisticated approximation fisher information matrix despite neither diagonal low-rank even block-diagonal small blocks inverted efﬁciently estimated online fashion using arbitrarily large subsets training data approximation built stages. ﬁrst rows columns fisher divided groups corresponds weights given layer gives rise block-partitioning matrix ollivier blocks approximated kronecker products much smaller matrices show equivalent making certain approximating assumptions regarding statistics network’s gradients. second stage matrix approximated inverse either block-diagonal block-tridiagonal. justify approximation careful examination relationships inverse covariances tree-structured graphical models linear regression. notably justiﬁcation doesn’t apply fisher itself experiments conﬁrm inverse fisher indeed possess structure fisher not. rest paper organized follows. section gives basic background notation neural networks natural gradient. section describes initial kronecker product approximation fisher. section describes block-diagonal block-tridiagonal approximations inverse fisher used derive efﬁcient inversion algorithm. section describes compute online estimates quantities required inverse fisher approximation large window previously processed mini-batches section describes approximate fisher obtain practical robust optimization algorithm requires little manual tuning careful application various theoretically well-founded damping techniques standard optimization literature. note damping techniques compensate local quadratic approximation implicitly made objective approximation fisher non-optional essentially nd-order method like k-fac work properly well established theory practice within optimization literature section describes simple effective adding type momentum k-fac found works well practice. section describes computational costs associated k-fac various ways reduce point update several times expensive compute stochastic gradient. section gives complete high-level pseudocode k-fac. section characterizes broad class network transformations reparameterizations k-fac essentially invariant. section neural network transforms input output series layers consists bank units/neurons. units receive input weighted outputs units previous layer compute output nonlinear activation function. denote vector weighted sums i-th layer vector unit outputs precise computation performed layer given follows element-wise nonlinear function weight matrix deﬁned vector formed appending additional homogeneous coordinate value note include explicit bias parameters captured implicitly homogeneous coordinates. particular last column weight matrix corresponds usually thought bias vector. figure illustrates deﬁnition denote loss function measures disagreement prediction made network target training objective function average losses respect training distribution ˆqxy input-target pairs proxy objective actually care don’t access expectation loss taken respect true data distribution qxy. ry|z’s density function. case standard least-squares crossentropy objective functions predictive distributions multivariate normal multinomial respectively. py|x ry|f denote conditional distribution deﬁned neural network parameterized density function. note minimizing objective function seen maximum likelihood learning model py|x. here expectation taken respect data distribution inputs model’s predictive distribution py|x since usually don’t access expectation would likely intractable even instead compute using training distribution inputs well-known natural gradient deﬁned −∇h. motivated perspective information geometry natural gradient deﬁnes direction parameter space gives largest change objective unit change model measured kl-divergence. contrasted standard gradient deﬁned direction parameter space gives largest change objective unit change parameters measured standard euclidean metric. shown fisher equivalent generalized gauss-newton matrix certain important cases well-known positive semi-deﬁnite approximation hessian objective function. particular showed deﬁned network linearized loss function loss function corresponds negative probability observations exponential family model ry|z representing natural parameters fisher corresponds exactly ggn. served curvature matrix choice related methods light equivalence fisher nd-order methods seen approximate natural gradient methods. perhaps importantly practical perspective natural gradientbased optimization methods conversely viewed nd-order optimization methods pointed martens brings bare vast wisdom accumulated make methods work well theory practice section productively make connections order design robust highly effective optimization method using approximation natural gradient/fisher main computational challenge associated using natural gradient computing large networks potentially millions parameters computing inverse naively computationally impractical. section develop initial approximation ingredient deriving efﬁciently computable approximation natural gradient. note condition represents natural parameters might require formally include nonlinear transformation usually performed ﬁnal nonlinearity network part loss function instead. equivalently could linearize network input computing expectation kronecker product general equal kronecker product expectations indeed major approximation make likely won’t become exact realistic assumptions limiting case kind asymptotic analysis. nevertheless seems fairly accurate practice able successfully capture coarse structure fisher demonstrated figure example network. later sections approximation leads signiﬁcant computational savings terms storage inversion able leverage order design efﬁcient algorithm computing approximation natural gradient. figure comparison exact fisher block-wise kronecker-factored approximation middle layers standard deep neural network partially trained classify down-scaled version mnist. network trained iterations k-fac batch mode achieving error network architecture ------ uses standard tanh units. left exact fisher middle approximation right difference these. dashed lines delineate blocks. note purposes visibility plot absolute values entries white level corresponding linearly size values denotes cumulant arguments. cumulants natural generalization concept mean variance higher orders indeed st-order cumulants means nd-order cumulants covariances. intuitively cumulants order measure degree interaction variables intrinsically order opposed arising many lower-order interactions. higher-order cumulants zero variables jointly distributed according multivariate gaussian follows upper bound approximation error small insofar joint distribution well approximated multivariate gaussian. aware argument case practice seem case example network figure size error well predicted size higher-order cumulants. particular total approximation error summed pairs weights middle layers roughly size corresponding upper bound whose size tied higher order cumulants best knowledge efﬁcient general method inverting khatri-rao product like thus must make approximations hope obtain efﬁciently computable approximation inverse fisher. following subsections argue inverse reasonably approximated special structures either make efﬁciently computable. second slightly less restrictive ﬁrst cost additional complexity. show matrix-vector products approximate inverses efﬁciently computed thus give efﬁcient algorithm computing approximation natural gradient. deﬁne matrix coefﬁcient j-th variable optimal linear predictor i-th variable variables deﬁne matrix diagonal matrix variance error associated predictor i-th variable. intuitively result says inverse covariance given coefﬁcients optimal linear predictor i-th variable others scaling factor. j-th variable much less useful variables predicting i-th variable expect entry inverse covariance relatively small. note usefulness subtle property informally deﬁned particular equivalent degree correlation j-th i-th variables simple measure. simple example consider case j-th variable equal k-th variable plus independent gaussian noise. since linear predictor achieve lower variance simply shifting weight j-th variable k-th variable j-th variable useful task predicting i-th variable setting noting fisher covariance matrix w.r.t. model’s distribution lemma thus apply analysis distribution gain insight approximate structure extension approximation consider derivative loss respect weights layer intuitively trying predict entries entries entries also likely useful regard. thus stands reason largest entries diagonal blocks well approximated block-diagonal block corresponding different dwi. beyond entries entries dwi+ dwi− arguably useful predicting given entry dwi. true process computing loss gradient uses information layer layer thus approximating block-tridiagonal seems like reasonable milder alternative taking block-diagonal. indeed approximation would exact distribution given directed graphical model generated dwi’s layer time either dwi+ dwi−. equivalently distributed according undirected gaussian graphical model binary potentials entries adjacent layers. models depicted figure reality dwi’s generated using information adjacent layers according process neither linear gaussian nonetheless stands reason joint statistics might reasonably approximated model. fact idea approximating distribution loss gradients directed graphical model forms basis recent fang method grosse salakhutdinov figure comparison block-wise kronecker-factored approximation inverse using example neural network figure left middle exact inverse right matrix containing averages absolute values entries block inverse. predicted theory inverse exhibits approximate block-tridiagonal structure whereas not. note corresponding plots exact inverse look similar. small blocks visible diagonal inverse correspond weights outgoing connections particular unit. inverse computed subject factored tikhonov damping technique described sections using value used k-fac iteration example taken note purposes visibility plot absolute values entries white level corresponding linearly size values following subsections show block-diagonal block-tridiagonal approximations give rise computationally efﬁcient methods computing matrix-vector products section present ﬁgures examine quality approximations example network. note block-diagonal approximations fisher information proposed tonga block corresponds weights associated particular unit. block-diagonal approximation blocks correspond parameters given layer thus much larger. fact large would impractical invert general matrices. note unlike block-diagonal case approximating block-tridiagonal equivalent approximating block-tridiagonal. thus require sophisticated approach deal approximation. develop approach subsection. start deﬁne matrix agrees tridiagonal blocks satisﬁes property block-tridiagonal. note deﬁnition implies certain values off-tridiagonal blocks differ insofar actually block-tridiagonal. figure diagram depicting uggm corresponding equivalent dggm. uggm’s edges labeled corresponding weights model here denotes block dggm’s edges labeled matrices specify linear mapping source node conditional mean destination node establish matrix well deﬁned inverted efﬁciently ﬁrst observe assuming block-tridiagonal equivalent assuming precision matrix undirected gaussian graphical model whose density function proportional −dθ). graphical model tree structure equivalent directed graphical model distribution graphical structure directionality edges given directed acyclic graph moreover equivalent directed model also linear/gaussian hence directed gaussian graphical model next show parameters dggm corresponding efﬁciently recovered tridiagonal blocks uniquely determined blocks assume direction edges higher layers lower ones. note different choice directions would yield superﬁcially different algorithm computing inverse would nonetheless yield output. denote conditional covariance matrix σi|i+ linear coefﬁcients matrix ψii+ conditional distributions deﬁning model i|i+. slightly tricky σi|i+ difference kronecker products cannot straightforward identity fortunately efﬁcient techniques inverting matrices discuss detail appendix figure compares directly exactly capture diagonal tridiagonal blocks must deﬁnition ends approximating off-tridiagonal blocks well too. likely owed fact approximating assumption used derive block-tridiagonal reasonable practice figure compares paints arguably interesting relevant picture quality approximation natural gradient roughly proportional quality approximation inverse fisher. ﬁgure approximate block-diagonal structure actually reasonably good approximation despite rather poor approximation meanwhile accounting tri-diagonal blocks indeed signiﬁcantly better approximation even diagonal blocks. figure comparison block-wise kronecker-factored approximation approximations using example neural network figure left middle approximation right absolute difference these. compares bottom compares diagonal blocks right matrix tridiagonal blocks bottom right matrix exactly zero constructed off-tridiagonal blocks bottom right matrix close zero actually non-zero note purposes visibility plot absolute values entries white level corresponding linearly size values figure comparison exact inverse block-wise kronecker-factored approximation block-diagonal block-tridiagonal approximations using example neural network figure left middle approximation right absolute difference these. compares bottom compares inverse computed subject factored tikhonov damping technique described sections using value used k-fac iteration example taken note purposes visibility plot absolute values entries white level corresponding linearly size values computing matrix-vector products could done exactly efﬁciently given input adapting methods schraudolph doesn’t seem sufﬁciently efﬁcient method computing entire matrix itself. indeed hardness results martens suggest would require example mini-batch work asymptotically equivalent matrix-matrix multiplication involving matrices size gij. small constant number multiplications arguably acceptable cost number grows size mini-batch would instead approximate expectation standard monte-carlo estimate obtained sampling network’s predictive distribution rerunning backwards phase backpropagation training targets. note computing/estimating required ¯aij/gij’s involves computing averages outer products various ¯ai’s network’s usual forward pass gi’s modiﬁed backwards pass thus compute/estimate quantities input data used compute gradient cost additional backwards passes additional outer-product averages. fortunately turns quite inexpensive found modiﬁed backwards pass sufﬁcient obtain good quality estimate practice required outer-product averages similar already used compute gradient usual backpropagation algorithm. case online/stochastic optimization found best strategy maintain running estimates required ¯aij’s gij’s using simple exponentially decaying averaging scheme. particular take running estimate weighted plus estimate mini-batch weighted experiments used min{ iteration number. note naive averaging scheme estimates iteration given equal weight would inappropriate here. ¯aij’s gij’s depend network’s parameters slowly change time optimization proceeds estimates computed many iterations become stale. kind exponentially decaying averaging scheme commonly used methods involving diagonal block-diagonal approximations curvature matrix schemes desirable property allow curvature estimate depend much data qy|x qy|x). using training/data distribution would perhaps give approximation quantity known empirical fisher information matrix lacks previously discussed equivalence generalized gaussnewton matrix would compatible theoretical analysis performed section moreover choice would give rise usually thought natural gradient based ﬁndings martens would likely perform worse practice part optimization algorithm. martens detailed discussion empirical fisher reasons poor choice curvature matrix compared standard fisher. notably methods like deal exact fisher indirectly matrix-vector products scheme would impossible implement efﬁciently exact fisher matrix seemingly cannot summarized using compact data structure whose size independent amount data used estimate indeed seems representation exact fisher would independent amount data used estimate would explicit matrix this related methods must base curvature estimates subsets data reasonably processed once limits effectiveness stochastic optimization regime. idealized natural gradient approach follow smooth path riemannian manifold generated taking series inﬁnitesimally small steps direction natural gradient clearly impractical real optimization method take larger steps still follow paths approximately. experience obtain update satisﬁes minimal requirement worsening objective function value often case must make step size small resulting optimization algorithm performs poorly practice. fortunately observed martens natural gradient understood using traditional optimization-theoretic perspective implies used generate updates useful larger distances. particular ry|z exponential family model natural parameters martens showed fisher becomes equivalent generalized gauss-newton matrix positive semi-deﬁnite approximation hessian additionally well-known fact negative log-likelihood function associated given pair hessian fisher closely related sense expected hessian training distribution ˆqxy expected hessian model’s distribution pq). which interpretation geodesic riemannian manifold current predictive distribution towards training distribution using likelihood kl-divergence based objective function viewed convex approximation nd-order taylor series expansion whose minimizer natural gradient −∇h. note weight-decay regularization term form similarly viewed approximation hessian replacing yields approximation nd-order taylor series whose minimizer kind regularized natural gradient −−∇h using practice. interpretation natural gradient minimizer fails useful local update insofar fails good local approximation argued martens natural make various damping techniques developed optimization literature dealing breakdowns local quadratic approximations inevitably occur optimization. notably breakusually won’t occur ﬁnal local convergence stage optimization function becomes well approximated convex quadratic within sufﬁciently large neighborhood local optimum. phase traditionally analyzed theoretical results important optimizer able converge well ﬁnal phase arguably much important practical standpoint behaves sensibly phase. initial exploration phase damping techniques help ways apparent asymptotic convergence theorems alone strong mathematical arguments support particular exploration phase often still true accurately approximated convex quadratic locally within region around therefor optimization efﬁciently performed minimizing sequence convex quadratic approximations within adaptively sized local regions. note well designed damping techniques ones employ automatically adapt local properties function effectively turn quadratic model becomes sufﬁciently accurate local approximation allowing optimizer achieve desired asymptotic convergence behavior levenbergmarquardt style adaptation line-searches trust regions truncation etc. tend much effective practice merely applying learning rate update adding ﬁxed multiple identity curvature matrix. indeed subset techniques exploited work martens primitive versions appeared implicitly older works becker lecun also many recent diagonal methods like zeiler although often without good understanding help. crucially powerful nd-order optimizers like k-fac capability taking much larger steps st-order methods require sophisticated damping solutions work well usually completely fail without them consistent predictions made various theoretical analyses analogy think powerful nd-order optimizers extremely fast racing cars need sophisticated control systems standard cars prevent ﬂying road. arguably reasons high-powered nd-order optimization methods historically tended under-perform machine learning applications neural network training particular designers understand take seriously issue quadratic model approximation quality employ sophisticated effective damping techniques available deal issue. methods like exact fisher seem work reasonably well adaptive tikhonov regularization technique added adapted according levenberg-marquardt style adjustment rule. common well-studied method shown equivalent imposing adaptive spherical region constrains optimization quadratic model however found simple technique insufﬁcient used approximate natural gradient update proposals. particular found never seems good choice gives rise updates quality comparable produced methods exact fisher possible explanation ﬁnding that unlike quadratic models based exact fisher underlying k-fac guarantee accurate nd-order. thus must remain large order compensate intrinsic nd-order inaccuracy model side effect washing small eigenvalues fortunately trial error able relatively simple highly effective damping scheme combines several different techniques works well within k-fac. scheme works computing initial update proposal using version described adaptive tikhonov damping/regularization method re-scaling according quadratic model computed using exact fisher. second step made practical fact requires single matrix-vector product exact fisher computed efﬁciently using standard methods. discuss details scheme following subsections. ﬁrst stage damping scheme generate candidate update proposal applying slightly modiﬁed form tikhononv damping approximate fisher multiplying inverse. usual tikhonov regularization/damping technique adds curvature matrix regularization) equivalent adding term form corresponding quadratic model replaced approximation). block-diagonal approximation amounts adding individual diagonal blocks gives modiﬁed diagonal blocks form apply tikhonov technique sophisticated approximation adding diagonal blocks longer clear efﬁciently invert instead solution found works well practice diagonal block giving choice simple sometimes work well practice slightly principled choice found minimizing obvious upper bound matrix norm residual expression matrix norm gives interestingly found factored approximate tikhonov approach originally motivated computational concerns often works better exact version practice. reasons still somewhat mysterious fact inverse product quantities often robustly estimated inverse product individually regularized estimates. re-scaling according exact given update proposal produced multiplying negative gradient approximate fisher inverse second stage proposed damping scheme re-scales according quadratic model computed exact produce ﬁnal update computed using estimate exact fisher regularization tikhonov term -dimensional quadratic minimization problem formula optimal computed efﬁciently evaluate formula current stochastic gradient compute matrix-vector products using input data minibatch. using mini-batch compute gets away idea basing estimate curvature long history data made slightly less objectionable fact using estimate single scalar quantity contrasted methods like perform long careful optimization using estimate intuitively second stage damping scheme effectively compensates intrinsic inaccuracy approximate quadratic model used generate initial update proposal essentially falling back accurate quadratic model based exact fisher. interestingly re-scaling according k-fac viewed version uses approximate fisher preconditioning matrix runs step initializing observation suggests running longer thus obtaining algorithm even closer indeed approach works reasonably well experience suffers problems stochastic setting much stronger mini-batch–estimated exact figure demonstrates effectiveness re-scaling technique versus simpler method using update proposal. without re-scaled poor update won’t even give improvement objective function unless strength factored tikhonov damping terms made large. hand update re-scaled afford compute using much smaller strength factored tikhonov damping terms overall yields much larger effective update tikhonov damping interpreted implementing trust-region constraint update particular constraint imposed depends curvature matrix approaches adjust seek matching often simpler adjust directly precise relationship complicated curvature matrix constantly evolving optimization takes place. figure comparison effectiveness proposed damping scheme without rescaling techniques described section network used comparison produced iteration k-fac mnist autoencoder problem described section y-axis improvement objective function produced update x-axis strength constant used factored tikhonov damping technique legend moment. indicates momentum technique developed k-fac section used. intuitively rule tries make small possible maintaining property quadratic model remains good local approximation gets chosen iteration). desirable property optimization enters ﬁnal convergence stage becomes almost exact approximation sufﬁciently large neighborhood local minimum value rapidly enough towards doesn’t interfere asymptotic local convergence theory enjoyed nd-order methods experiments applied rule every iterations k-fac starting value note optimal value starting value application dependent setting inappropriately could signiﬁcantly slow k-fac practice. available usual forward pass. remaining quantity needed evaluate thus require additional forward pass. fortunately need perform every iterations. scheme described previous sections works reasonably well situations found order avoid certain failure cases truly robust large variety situations tikhonov damping strength parameter factored tikhonov technique described section maintained adjusted independently replace expression adjusted using different rule described section. reasoning behind modiﬁcation follows. role according levenberg marquardt theory small possible maintaining property quadratic model remains trust-worthy approximation true objective. meanwhile role ensure initial update proposal good approximation possible true optimum particular re-scaling performed section benign possible. might hope adding multiple identity approximate fisher exact would produce best regard isn’t obviously case. particular using larger multiple help compensate approximation making fisher computing thus help produce conservative ultimately useful initial update proposal observe happens practice. simple measure quality choice value quadratic model optimally chosen adjust based measure simple greedy adjustment rule. particular every iterations optimization different values current value) choose best these measured quality metric. experiments used found works well practice measure quality added bonus computed essentially additional cost incidental quantities already computed solving optimal initial experiments found using gave similar results obtained using obvious measures quality sutskever found momentum helpful context stochastic gradient descent optimization deep neural networks. version momentum also present original method plays arguably even important role stochastic versions practice take update ﬁnal update computed previous iteration chosen minimize allows k-fac effectively build better solution local quadratic optimization problem minδ many iterations somewhat similarly matrix momentum main cost evaluating formula computing matrix-vector products fortunately technique discussed appendix applied compute required scalars cost forwards passes empirically found type momentum provides substantial acceleration regimes gradient signal noise signal ratio usually case early stages stochastic optimization also case later stages mini-batch size made sufﬁciently large. ﬁndings consistent predictions made convex optimization theory older empirical work done neural network optimization interestingly quadratic function remains ﬁxed iteration) quantities computed deterministically using type momentum makes k-fac equivalent performing preconditioned linear preconditioner given approximate fisher. follows fact linear interpreted momentum method learning rate momentum decay coefﬁcient chosen jointly minimize current iteration. typical number units layer mini-batch size. signiﬁcant computational tasks required compute single update/iteration k-fac rough estimates associated computational costs follows various matrix-matrix products required compute matrix-vector product approximate inverse stochastic gradient block-diagonal inverse block-tridiagonal inverse various constants account implementation details assuming naive cubic matrix-matrix multiplication inversion algorithms producing cost estimates. note hard assign precise values constants much depend various tasks implemented. note computations required tasks sped greatly performing parallel across units layers training cases these. cost estimates however measure sequential operations thus accurately reﬂect true computation times enjoyed parallel implementation. experiments used vectorized implementation performed computations parallel units training cases although layers costs tasks similar slightly smaller tasks signiﬁcantly reduce random subset current mini-batch size update estimates required ¯aij’s gij’s. similarly reduce cost task computing matrix-vector product using subset size although recommend proceeding caution this using inconsistent sets data quadratic linear terms hypothetically cause instability problems avoided using consistent data section experiments section used seemed negligible effect quality resultant updates signiﬁcantly reducing per-iteration computation time. separate unreported experiments found certain situations regularization isn’t used network starts heavily overﬁtting data smaller mini-batches used revert using prevent signiﬁcant deterioration quality updates. costs tasks hard compare directly costs associated computing gradient relative sizes depend factors architecture neural network trained well particulars implementation. however quick observation make tasks involve computations performed parallel across different layers contrasted many tasks require sequential passes layers network. clearly cost tasks becomes negligible comparison others. however often case comparable perhaps smaller moreover algorithms inverses svds tend asymptotic cost matrix-matrix multiplication least several times expensive practice addition harder parallelize modern architectures thus typically larger basic/naive implementation k-fac task dominate overall per-iteration cost. fortunately several possible ways mitigate cost task mentioned above perform computations layer parallel even simultaneously gradient computation tasks. case block-tridiagonal approximation inverse avoid computing svds matrix square roots using iterative stein-equation solver also ways reducing matrix-inversion short sequence matrix-matrix multiplications using iterative methods furthermore matrices question change slowly time consider hot-starting iterative inversion methods previous solutions. extreme case large also consider using low-rank diagonal approximations ¯aij matrices maintained online inverses and/or svds easily computed. although based experience approximations cases lead substantial degradation quality updates. ideas work reasonably well practice perhaps simplest method ended settling experiments simply recompute approximate fisher inverse every iterations turns curvature objective stays relatively stable optimization especially later stages experience strategy results modest decrease quality updates. much smaller costs associated task begin dominate unlike task task must performed every iteration. simplest solution increase case block-diagonal inverse turns change cost task taking advantage low-rank structure stochastic gradient. method described below. matrices whose columns ¯ai’s gi’s associated current mini-batch. ∇wih denote gradient respect shaped matrix estimate ∇wih mini-batch given rank-m. section computing amounts computing note standard weight-decay compatible trick. contribution weight-decay term ∇wih typically lowrank. possible ways around issue include computing weight-decay contribution separately refreshing occasionally using different regularization method drop-out weight-magnitude constraints. note adjustment technique described section requires that every iterations compute different versions update candidate values ideal implementation could computed parallel other although summary analysis assume computed serially. summarizing various efﬁciency improvements discussed section average per-iteration computational cost k-fac terms serial arithmetic operations pseudocode k-fac algorithm gives high-level pseudocode k-fac method details perform computations required major step left respective sections. choose mini-batch size select random mini-batch training cases size select random subset size τ|s| select random subset size τ|s| perform forward backward pass estimate gradient perform additional backwards pass using random targets generated model’s predictive distribution update estimates required ¯aij’s gij’s using ai’s computed forward pass gi’s computed additional backwards pass choose candidate described section compute update proposal multiplying current estimate approximate fisher inverse estimate layers size consider using trick described section increased efﬁciency. compute ﬁnal update described section matrix-vector products estimated using ai’s computed forward pass select computing loop correspond lowest value updating iteration computed exact fisher natural gradient speciﬁes direction space predictive distributions invariant speciﬁc model parameterized. invariance means smooth path distribution space produced following natural gradient inﬁnitesimally small steps similarly invariant. practical natural gradient based optimization method takes large discrete steps direction natural gradient invariance optimization path hold approximately. shown martens approximation error zero effects damping diminish reparameterizing function tends locally linear function. note latter happen becomes smoother local region containing update shrinks zero. k-fac uses approximation natural gradient invariance results applicable case. fortunately shown martens establish invariance update direction respect given reparameterization model verifying certain simple properties curvature matrix used compute update. result show that assumption damping absent k-fac invariant broad natural class transformations network. function computes appends homogeneous coordinate arbitrary invertible matrices appropriate sizes multiplies addition arbitrary linear transformations. figure illustrates modiﬁed network deﬁnition here going forward superscript network-dependent quantity order denote analogous version computed transformed network. note identiﬁcation loss derivative formulas transformed network analogous original network various fisher approximations still well deﬁned. theorem exists invertible linear function thus transformed network viewed reparameterization original network moreover additively updating original network equivalent additively updating †−∇h† †−∇h† transformed network sense version k-fac given class network transformations. corollary optimization path taken k-fac space predictive distributions default network transformed network assumes equivalent initialization basic version k-fac damping absent negligible effect momentum used learning rates chosen independent network’s parameterization. corollary assumes ωi’s φi’s ﬁxed relax assumption allowed vary smoothly smooth function discussed martens invariance optimization path hold approximately depends smoothness size update. moreover invariance hold exactly limit learning rate goes note network transformations interpreted replacing network’s nonlinearity layer transformed version since well-known logistic sigmoid tanh functions related transformation immediate consequence corollary k-fac invariant choice logistic sigmoid tanh activation functions also note network inputs also transformed k-fac thus invariant arbitrary afﬁne transformations input includes many popular training data preprocessing techniques. many natural network transformations ones center normalize unit activities mean variance described using diagonal choices ωi’s φi’s vary smoothly addition approximately invariant transformations k-fac similarly invariant general class transformations transform units mean centered covariance matrix whitened much stronger condition variances individual units particular updates produced k-fac equivalent produced standard gradient descent using network transformed unit activities unitgradients centered whitened stated formally following corollary. corollary additively updating original network equivalent additively updating gradient descent update −α∇h† theorem transformed version network unit activities centered whitened respect model’s distribution. hessian-free optimization method martens uses linear conjugate gradient optimize local quadratic models form eqn. lieu directly solving using matrix inverses. discussed introduction main advantages k-fac twofold. firstly k-fac uses efﬁciently computable direct solution inverse curvature matrix thus avoids costly matrix-vector products associated running within secondly estimate curvature matrix data using online exponentially-decayed average opposed relatively small-sized ﬁxed mini-batches used cost course inexact approximation curvature matrix. roux proposed neural network optimization method known tonga based block-diagonal approximation empirical fisher block corresponds weights associated particular unit. contrast k-fac uses much larger blocks corresponds weights within particular layer. matrices inverted k-fac roughly size inverted tonga rather unit tonga layer. therefore k-fac signiﬁcantly less computationally intensive tonga despite using arguably much accurate approximation fisher. note help mitigate cost many matrix inversions requires tonga approximates blocks low-rank plus diagonal term although introduces approximation error. centering methods work either modifying gradient dynamically reparameterizing network various unit-wise scalar quantities like activities local derivatives i’s) average appear formula gradient. typically methods require introduction additional skip connections order preserve expressive power/efﬁciency network transformations applied. closely resemble natural gradient. however argument uses strong approximating assumption correlations various network-dependent quantities activities different units within given layer zero. notation would like assuming gii’s diagonal ¯aii’s rank- plus diagonal term. indeed using approximation within block-diagonal version k-fac would yield algorithm similar standard centering although without need skip connections shown corollary k-fac also interpreted using gradient transformed network update direction although gi’s ai’s centered whitened intuitively whitening accounts correlations activities within given layer. ollivier proposed neural network optimization method uses block-diagonal approximation fisher blocks corresponding incoming weights unit. method similar tonga except approximates fisher instead empirical fisher discussion difference these). computing blocks fisher expensive method uses biased deterministic approximation computed efﬁciently similar spirit deterministic approximation used lecun note approximation could hypothetically used within k-fac compute gij’s found basic unbiased stochastic approximation works nearly well exact values practice. work closely related heskes proposed approximation fisher feed-forward neural networks similar kronecker-factored blockdiagonal approximation section used derive efﬁcient approximate naturalgradient based optimization method exploiting identity k-fac differs heskes’ method several important ways turn crucial working well practice. heskes’ method update damping accomplished using basic factored tikhonov technique added ¯aii ﬁxed parameter hand. contrast k-fac uses factored tikhonov technique adapted dynamically described section combined re-scaling technique based local quadratic model computed using exact fisher note adaptation important since constitutes good even merely acceptable value change signiﬁcantly course optimization. re-scaling technique something similar also crucial observed empirically basic tikhonov damping incapable producing high quality updates itself even chosen optimally iteration also heskes’ method computes gii’s exactly k-fac uses stochastic approximation scales efﬁciently neural networks much higher-dimensional outputs advances introduced include accurate block-tridiagonal approximation inverse fisher parameter-free type momentum online estimation ¯aii matrices various improvements computational efﬁciency found additional elements important order k-fac work well various settings. concurrently work povey developed neural network optimization method uses block-diagonal kronecker-factored approximation similar heskes approach differs k-fac numerous ways including empirical fisher basic factored tikhonov damping technique without adaptive re-scaling form momentum. interesting idea introduced povey particular method maintaining online low-rank plus diagonal approximation factor matrices block allows inverses computed efﬁciently experiments similar kinds methods maintaining online estimates found performed poorly practice compared solution refreshing inverses occasionally particular developed povey could potentially still work well especially useful networks wide layers. heskes discussed alternative interpretation block-diagonal approximation yields useful insight complement theoretical analysis. particular observed block-diagonal fisher approximation curvature matrix corresponding following quadratic function measures difference parameter value current value instead expression would correspond basic layer-wise block-diagonal approximation blocks computed exactly approximate fisher would interpretation hessian w.r.t. either measures note term either sums function measuring intrinsic quantity overall intrinsic measures except insofar assume divided independent groups parameterize different predictive distributions clear whether kronecker-factorizing structure similarly interpreted hessian self-evidently intrinsic measure. could would considerably simplify proof theorem note doesn’t work isn’t obviously intrinsic. despite this shown section advanced approximation produce updates strong invariance properties. investigate practical performance k-fac applied deep autoencoder optimization problems hinton salakhutdinov mnist curves faces datasets respectively complete description network architectures datasets). high difﬁculty performance problems become standard benchmark neural network optimization methods included regularization coefﬁcient three optimization problems lower used martens higher used sutskever baseline used version momentum based nesterov’s accelerated gradient described sutskever calibrated work well particular deep autoencoder problems. problem followed prescription given sutskever determining learning rate increasing schedule decay parameter compare methods based diagonal approximations curvature matrix experience methods tend perform well kinds implementation k-fac used efﬁciency improvements described section except tasks computed serially mini-batch size tended comparable larger typical/average layer size technique described section accelerating computation approximate inverse improves efﬁciency case otherwise decrease efﬁciency. k-fac baseline implemented using vectorized matlab code accelerated package jacket. code k-fac available download. tests performed single computer core intel nvidia memory. method used initial parameter setting generated using sparse initialization technique martens help mitigate detrimental effect noise stochastic gradient convergence baseline used exponentially decayed iterate averaging approach based loosely polyak averaging particular iteration took averaged parameter estimate previous estimate multiplied plus iterate produced optimizer multiplied since training error associated optimizer’s current iterate sometimes lower training error associated averaged estimate report minimum quantities. consistent numbers given previous papers report reconstruction error instead actual objective function value report error training opposed test chieﬂy interested optimization speed generalization capabilities networks themselves. ﬁrst experiment examined relationship mini-batch size per-iteration rate progress made k-fac baseline mnist problem. results experiment plotted figure strongly suggest per-iteration rate progress k-fac tends superlinear function contrasted baseline increasing much smaller effect per-iteration rate progress k-fac without momentum per-iteration rate progress seems linear slightly sublinear function thus appears main limiting factor convergence k-fac noise gradient least later stages optimization true baseline nearly extent. would seem suggest k-fac much would beneﬁt massively parallel distributed implementation makes computational resources single gpu. figure results ﬁrst experiment examining relationship mini-batch size per-iteration progress per-training case progress progress made kfac mnist deep autoencoder problem. here blk-tridiag k-fac block-tridiagonal version k-fac blk-diag k-fac block-diagonal version moment. indicates momentum used. bottom consists zoomed-in versions right plot left plot concentrating beginning stage optimization right plot concentrating later stage. note x-axes last plots signiﬁcantly different scales even single cpu/gpu setting fact per-iteration rate progress tends superlinear function per-iteration computational cost k-fac roughly linear function suggests order obtain best per-second rate progress kfac rapidly increasing schedule designed exponentially increasing schedule given min/b)|s|) current iteration chosen |s|. approach increasing mini-batch size analyzed friedlander schmidt note neural network optimization problems ones involving larger training datasets autoencoder problems slowly increasing schedule stops increasing well reaches appropriate. also consider using approach similar byrd adaptively determining suitable mini-batch size. second experiment evaluated performance implementation k-fac versus baseline deep autoencoder problems used described exponentially increasing schedule k-fac ﬁxed setting baseline momentum-less k-fac relatively high values chosen baseline reﬂect fact implementation baseline uses highperformance highly optimized linear algebra package allows many training cases efﬁciently processed parallel. indeed certain point making much smaller didn’t result signiﬁcant reduction baseline’s per-iteration computation time. note order process large mini-batches required exponentially increasing schedule without overwhelming memory partitioned mini-batches smaller chunks performed computations involving mini-batches subsets thereof chunk time. results second experiment plotted figures problem k-fac per-iteration rate progress orders magnitude higher baseline’s provided momentum used translated overall much higher per-second rate progress despite higher cost k-fac’s iterations note polyak averaging didn’t produce signiﬁcant increase convergence rate k-fac second experiment increasing schedule provided much effective solution problem noise gradient. importance using form momentum problems emphasized experiments fact without momentum technique developed section k-fac wasn’t signiﬁcantly faster baseline results echo sutskever found without momentum orders magnitude slower particular problems. indeed included results baseline without momentum wouldn’t even appeared axes boundaries plots figure figure results second experiment showing training error versus iteration curves mnist faces deep autoencoder problems. plots right zoomed versions left highlight difference per-iteration progress made different versions k-fac. recall type momentum used k-fac compensates inexactness approximation fisher allowing k-fac build better solution exact quadratic model minimization problem across many iterations. thus much stronger approximation fisher computing update proposals beneﬁt using type momentum would likely much smaller observed. might hypothesize particular type momentum used k-fac mostly responsible advantages baseline. however testing found conventional type momentum used sutskever performs signiﬁcantly better. figure block-tridiagonal version k-fac per-iteration rate progress typically larger simpler block-diagonal version. observation provides empirical support idea block-tridiagonal approximate inverse fisher accurate approximation block-diagonal approximation however higher cost iterations block-tridiagonal version overall persecond rate progress seems moderately higher block-diagonal version’s depending problem. note matrix-matrix multiplication matrix inverse computation computational complexity practice costs differ signiﬁcantly computation approximate fisher inverse performed experiments every iterations requires matrix inverses blockdiagonal version svds block-tridiagonal version. faces problem layers many units accounted signiﬁcant portion difference average per-iteration computational cost versions results suggest block-diagonal version probably better option overall greater simplicity situation different given efﬁcient implementation k-fac expensive svds required tri-diagonal version computed approximately and/or parallel tasks perhaps even network optimized. paper developed k-fac approximate natural gradient-based optimization method. started developing efﬁciently invertible approximation neural network’s fisher information matrix justiﬁed theoretical empirical examination statistics gradient neural network. then exploiting interpretation fisher approximation hessian designed developed complete optimization algorithm using quadratic model-based damping/regularization techniques yielded highly effective robust method virtually free need hyper-parameter tuning. showed k-fac preserves many natural gradient descent’s appealing theoretical properties invariance certain reparameterizations network. finally showed k-fac combined form momentum increasing schedule mini-batch size surpasses performance well-tuned version momentum difﬁcult deep auto-encoder optimization benchmarks moreover results demonstrated k-fac requires orders magnitude fewer total updates/iterations momentum making ideally suited massively distributed implementation synchronization main bottleneck. implementation better exploits opportunities parallelism described section exploitation massively distributed computation order compute high-quality estimates", "year": 2015}