{"title": "One-Shot Imitation from Observing Humans via Domain-Adaptive  Meta-Learning", "tag": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "abstract": "Humans and animals are capable of learning a new behavior by observing others perform the skill just once. We consider the problem of allowing a robot to do the same -- learning from a raw video pixels of a human, even when there is substantial domain shift in the perspective, environment, and embodiment between the robot and the observed human. Prior approaches to this problem have hand-specified how human and robot actions correspond and often relied on explicit human pose detection systems. In this work, we present an approach for one-shot learning from a video of a human by using human and robot demonstration data from a variety of previous tasks to build up prior knowledge through meta-learning. Then, combining this prior knowledge and only a single video demonstration from a human, the robot can perform the task that the human demonstrated. We show experiments on both a PR2 arm and a Sawyer arm, demonstrating that after meta-learning, the robot can learn to place, push, and pick-and-place new objects using just one video of a human performing the manipulation.", "text": "single approach based meta-learning. instead manually specifying correspondence human robot particularly complex skills different morphologies require different strategies propose datadriven approach. approach acquire skills video human. enable this builds rich prior tasks meta-training phase human demonstrations teleoperated demonstrations available variety other structurally similar tasks. essence robot learns learn humans using data. meta-training phase robot acquire skills combining learned prior knowledge video human performing skill. main contribution paper system learning robotic manipulation skills single video human leveraging large amounts prior meta-training data collected different tasks. deployed robot adapt particular task novel objects using single video human performing task objects video human need perspective robot even room. robot trained using videos humans performing tasks various objects along demonstrations robot performing task. experiments real robotic platforms demonstrate ability learn directly videos humans handle novel objects novel humans videos humans novel scenes. videos results found supplementary website. abstract—humans animals capable learning behavior observing others perform skill once. consider problem allowing robot learning video pixels human even substantial domain shift perspective environment embodiment robot observed human. prior approaches problem hand-speciﬁed human robot actions correspond often relied explicit human pose detection systems. work present approach oneshot learning video human using human robot demonstration data variety previous tasks build prior knowledge meta-learning. then combining prior knowledge single video demonstration human robot perform task human demonstrated. show experiments sawyer demonstrating meta-learning robot learn place push pick-and-place objects using video human performing manipulation. demonstrations provide descriptive medium specifying robotic tasks. prior work shown robots acquire range complex skills demonstration table tennis lane following pouring water drawer opening multi-stage manipulation tasks however effective methods robot imitation differ signiﬁcantly humans animals might imitate behaviors robots typically need receive demonstrations form kinesthetic teaching teleoperation humans animals acquire gist behavior simply watching someone else. fact adapt variations morphology context task details effortlessly compensating whatever domain shift present recovering skill situations additionally small number demonstrations often one. endow robots ability learn behaviors third person observations human demonstrators? acquiring skills camera observations presents major challenges. first difference appearance morphology human demonstrator robot introduces systematic domain shift namely correspondence problem second inputs typically requires substantial amount data modern deep learning vision systems using hundreds thousands millions images paper demonstrate begin address challenges imitation learning learning demonstration methods operate level conﬁguration-space trajectories typically collected using kinesthetic teaching teleoperation sensors demonstrator instead allow robots imitate watching demonstrator perform task? focus problem learning video demonstration human performing task combination human robot demonstration data collected tasks. prior work proposed resolve correspondence problem hand example manually specifying human grasp poses correspond robot grasps manually deﬁning human activities commands translate robot actions utilizing demonstration data humans robots perform task approach learns correspondence human robot implicitly. many prior approaches also explicit handtracking systems carefully engineered pipelines visual activity recognition contrast approaches rely precise hand detection pre-built vision system approach trained end-to-end seeking extract aspects human’s activity relevant choosing actions. places less demand vision system requiring implicitly deduce task accomplish rather precisely tracking human’s body nearby objects. prior approaches sought solve problem learning human demonstrations explicitly determining goal reward underlying human behavior optimizing reward reinforcement learning example rhinehart kitani learn model predicts outcome human’s demonstration particular scene. similarly works learned reward function based human demonstrations system learned reward function desired outcome underlying given task robot runs form reinforcement learning maximize reward reach desired outcome. optimization typically requires substantial experience collected using robot individual task. approaches assume known model perform trajectory optimization reach inferred goal methods consider single tasks isolation often require multiple human demonstrations task method requires demonstration task setting test time require additional experience robot known model. crucially data used approach amortized across tasks amount data needed given individual task quite small. contrast prior reward-learning methods handle single-task setting considerable amount data must collected individual task. shown ability learn demonstration using prior knowledge built many demonstrations tasks. particular extend approach model-agnostic meta-learning approaches considered problem domain shift demonstration used training testing e.g. learning videos humans. handling domain shift aspect problem shift visual scene embodiment human/robot including degrees freedom physics. domain adaptation received signiﬁcant interest within machine learning community especially varying visual domains visual shift simulation reality many techniques representation invariant domain approaches sought datapoints domain another human imitation problem involve developing invariances example background lighting conditions human robot’s environments. however physical correspondence human robot call invariant representation direct mapping domains. many scenarios direct physical correspondence robot human poses might exist. instead system must implicitly recognize goal human video determine appropriate action. approach builds upon prior work learning learn meta-learning order learn infer robot policy human demonstration. particular present extension model-agnostic meta-learning algorithm allows ability handle domain shift provided data evaluation setting ability learn effectively without labels section overview general metalearning problem maml algorithm. meta-learning algorithms optimize ability learn tasks quickly efﬁciently. data collected across wide range meta-training tasks evaluated based ability learn meta-test tasks. meta-learning assumes meta-training meta-test tasks drawn distribution generally metalearning viewed discovering structure exists tasks that model presented task meta-test known structure quickly learn task. maml achieves optimizing deep network’s initial parameter setting steps gradient descent datapoints leads effective generalization then meta-training learned parameters ﬁne-tuned data task. metalearning process formalized learning prior functions ﬁne-tuning process inference learned prior known dynamics model. however type manual knowledge encoding task-speciﬁc time-consuming beneﬁt data. instead study learn prior automatically using human robot demonstration data variety tasks. formally deﬁne demonstration human sequence image observations robot demonstration sequence image observations robot states robot actions robot state includes robot’s body conﬁguration joint angles include object information must inferred image. make assumptions similarities differences human robot observations; contain substantial domain shift e.g. differences appearance arms background clutter camera viewpoint. approach consists phases. first metatraining phase goal acquire prior policies using human robot demonstration data used quickly learn imitate tasks human demonstrations. meta-training assume distribution tasks tasks {ti} drawn task small datasets containing several human robot demonstrations respectively meta-training phase learned prior used second phase method provided human demonstration task drawn robot must combine prior human demonstration infer policy parameters solve task. next discuss approach detail. develop domain-adaptive meta-learning method allow handle setting learning video demonstrations humans. extend maml algorithm purpose idea approach applicable meta-learning algorithms. like maml algorithm learn initial parameters steps gradient descent human demonstration model effectively perform task. since access human’s actions. even knew human’s actions typically correspond directly robot’s actions. instead propose metalearn adaptation objective require actions instead operates policy activations. intuition behind meta-learning loss function acquire function needs look available inputs still produce gradients suitable updating policy parameters produce effective actions gradient update. might seem like impossible task important concretely consider supervised learning problem loss function denoted denotes model parameters denotes labeled data. few-shot supervised learning problem maml assumes access small amount data large number tasks. meta-training task sampled along data task randomly partitioned sets dval. assume examples. maml optimizes model parameters gradient steps produces good performance dval. effectively maml optimizes generalization examples. thus using denote updated parameters maml objective following step size hyperparameter learned. moving forward refer inner loss function adaptation objective outer objective meta-objective. subsequently meta-test time examples held-out task ttest presented gradient descent starting infer model parameters task finn applied maml algorithm one-shot imitation learning problem using robot demonstrations collected teleoperation mean-squared error behavioral cloning objective loss enables learning robot demonstration meta-test time allow robot learn video human handle domain shift demonstration medium robot. next present approach one-shot imitation learning video domain shift. problem learning human video viewed inference problem goal infer robot policy parameters accomplish task incorporating prior knowledge small amount evidence form human demonstration. order effectively learn video human need rich prior encapsulates visual physical understanding world kinds outcomes human might want accomplish actions might allow robot bring outcome. could choose encode prior knowledge manually example using pre-deﬁned vision system pre-determined human objectives algorithm learning human video meta-learning require meta-learned initial policy parameters require learned adaptation objective require video human demo task compute policy parameters α∇θlψ return must solve harder task since must provide policy suitable gradient information without access true actions. discussed previously still possible since policy trained output good actions meta-training. learned loss must simply supply gradients needed modify perceptual components policy attend right objects scene action output actually performs right task. however determining behavior demonstrated objects relevant often require examining multiple frames time determine human’s motion. incorporate temporal information learned adaptation objective therefore couples multiple time steps together operating policy activations multiple time steps. since temporal convolutions shown effective processing temporal sequential data choose adopt convolutional network represent adaptation objective using multiple layers convolutions time. choose temporal convolutions traditional recurrent neural network like lstm since simpler usually parameter efﬁcient figure visualization. prior work introduced two-head architecture one-shot imitation head used pre-update demonstration head used post-update policy twohead architecture interpreted learned linear loss function operating last hidden layer policy network particular timestep. loss gradient computed averaging timesteps demonstration. discussed previously single timestep observed video often sufﬁcient learning video demonstrations without actions. thus simple averaging scheme effective integrating temporal information. section show learned temporal loss enable effective learning demonstrations without actions remember meta-training process still supervises policy true robot actions meta-training. role adaptation loss therefore interpreted simply directing policy parameter update modify policy pick right visual cues scene metatrained action output produce right actions. discuss particular form following section. meta-training phase learn initialization parameters adaptation objective parameters optimized choosing actions match robot demonstrations dvalt meta-training parameters retained data discarded. human demonstration provided task infer policy parameters task gradient descent starting using learned loss human demonstration α∇θlψ. optimize task performance meta-training using behavioral cloning objective maximizes probability expert actions dval. particular policy parameterized outputs distribution actions behavioral cloning objective =lbc putting together inner gradient descent adaptation meta-training objective following algorithm optimizing meta-objective summarized algorithm whereas procedure learning humans meta-test time shown algorithm next discuss form learned loss function critical effective learning. learn video human need adaptation objective effectively capture relevant information video intention human task-relevant objects. standard behavior cloning loss applied time step independently learned adaptation objective role learned adaptation objective induce joint factor observations policy parameters visual illustration corresponding graphical model shown figure policy architecture convolutional neural network maps images distribution actions. convolutional network begins convolutional layers channelwise spatial soft-argmax extracts feature points channel last convolution layer prior work shown spatial soft-argmax particularly effective parameter-efﬁcient learning represent positions objects robotics domains following prior work concatenate feature points robot conﬁguration consists pose endeffector represented position non-axis-aligned points gripper. then pass concatenated feature points robot pose multiple fully connected layers. distribution actions predicted linearly last hidden layer initialize ﬁrst convolutional layer network trained imagenet. experiments using continuous action space linear angular velocity robot’s gripper discrete action space gripper open/close action. gaussian mixtures better model multi-modal action distributions compared gaussian distributions used previous imitation learning works thus continuous actions mixture density network represent output distribution actions. discrete action opening closing gripper sigmoid output cross-entropy loss. following prior work additionally model predict pose gripper contacts target object and/or container. part outer metaobjective easily provide supervision using robot demonstration. note supervision needed meta-test time robot learning video human. placing pick-and-place tasks target container located ﬁnal end-effector pose. thus last end-effector pose supervision. pushing pick-and-place demonstrator manually speciﬁes time gripper initially contacts object endeffector pose time step used. model predicts intermediate gripper pose linearly feature points predicted pose back policy. architecture details included section fig. graphical model underlying approach. meta-training observations actions observed method learns meta-testing observations available method combines learned prior factor infer task-speciﬁc policy parameters interpret meta-learning learned adaptation objectives casting framework probabilistic graphical models. accomplish building derivation proposed prior work frames maml approximately inferring posterior policy parameters starting approximately equivalent maximum posteriori inference induces gaussian prior weights mean covariance depends step size number gradient steps. derivation outside scope paper refer reader prior work details approach adaptation involves gradient descent learned loss rather likelihood since still take ﬁxed number steps gradient descent starting result prior work still implies approximately imposing gaussian prior therefore performing approximate inference following joint distribution partially directed factor graph learned factor dtrt log-energy bayesian inference would require integrating inference provides tractable alternative still produces good results practice training performed directly maximizing behavior cloning loss corresponds likelihood actions gaussian mixture policy directly train prior log-energy inference maximizes probability actions. note that since inference training model necessarily provide well-calibrated probabilities. however probabilistic interpretation still helps shed light perception layers) ﬁnal hidden layer policy allows learned loss directly adapt weights convolutional layers bypassing control layers. updated task parameters computed using temporal adaptation objective decompose objective parts architecture illustrated figure learned objective consists three layers temporal convolutions ﬁrst ﬁlters third ﬁlters. norm output convolutions computed produce scalar objective value. experiments address three main questions approach effectively learn prior enables robot learn manipulate objects seeing video human? approach generalize human demonstrations different perspective robot novel backgrounds human demonstrators? proposed approach compare alternative approaches meta-learning? order understand method applicability additionally evaluate understand important temporal adaptation objective? approach used robot platform either kinesthetic teleoperated demonstrations meta-training? answer questions experiments primarily -dof robot demonstrations collected teleoperation images collected consumergrade camera sawyer robot kinesthetic demonstrations study compare following meta-learning approaches contextual policy feedforward network takes input robot’s observation ﬁnal image human demo outputs predicted action. da-lstm policy recurrent network directly ingests human demonstration video current robot observation outputs predicted robot action. methods mixture density network represent action space actions correspond linear rotational velocity robot gripper -dimensional continuous action space. discussed section network also trained predict ﬁnal end-effector pose using meansquared error objective. train policy using adam optimizer default hyperparameters methods data receive supervision. measuring generalization meta-training meta-testing held-out objects evaluations seeing meta-training illustrated figure human demonstrators. provide full experimental details hyperparameters architecture information appendix code method released upon publication encourage reader view supplementary video. ﬁrst consider three different task settings placing held object container avoiding distractor containers pushing object amid distractor picking object placing target container amid distractor containers. tasks illustrated figure initial experiment collect human demonstrations perspective robot’s camera. placing pushing images whereas pick-andplace rgb-d used. meta-training collected dataset hundreds objects consisting robot demonstrations placing pushing pick-and-place respectively equal number human demonstrations. following metrics deﬁne success task placing pick place success object landed part correct container; pushing success correct item pushed past within robot’s left gripper. fig. example placing pushing pick-and-place tasks robot’s perspective. shows human demonstrations used section vi-a bottom shows robot demonstration. consider challenging setting human demonstrations collected different room different camera camera perspective robot. result background lighting vary substantially robot’s environment. mounted cell-phone camera record sequences images different table textures illustrated figure corresponding view demonstrations shown figure consider pushing task described section vi-a reusing robot demonstrations collecting equal number demonstrations. evaluate performance novel objects human demonstrator seen novel backgrounds shown figure like previous experiment evaluated novel objects trials object. used different object pairs previous pushing experiment performance directly comparable results table results experiment summarized table seen supplementary video robot able successfully learn demonstrations different viewpoint background. performance degrades using novel background causes varied shift domain robot still able perform task time. table also include analysis failure modes approach including number failures caused incorrect task identiﬁcation misidentifying object versus control failures object clearly correctly identiﬁed robot failed effectively push that human demonstrations previously seen background robot fails identify object trials whereas failures kind frequent novel backgrounds. collecting data diverse array backgrounds using form background augmentation would likely reduce types failures. number control failures similar backgrounds likely indicative challenge physically maneuvering variety previously unseen objects. table one-shot success rate robot pushing using videos human demonstrations different scene camera seen novel backgrounds. evaluated using held-out objects novel human. evaluation used novel target objects placing pushing pick-and-place respectively collected human demonstration object evaluated three trials policy inferred human demonstration. report results table results show that across board robot able learn interact novel objects using video human demo object pick-and-place difﬁcult task. da-lstm contextual policies struggle likely require data effectively infer task. ﬁnding consistent previous work results also indicate importance integrating temporal information observing human demonstration linear loss performs poorly compared using temporal adaptation objective. adaptation loss perform experiment simulation setting without domain shift. simulated pushing task proposed finn mujoco physics engine brieﬂy summarize experimental set-up imitation problem involves controlling -dof robot torque-control push object ﬁxed target position amid distractor using images input. initial positions objects randomized texture shape size mass friction. meta-training uses object meshes held-out meshes multiple heldtextures used meta-testing. push considered successful target object lands target position least timesteps within -timestep episode. results table demonstrate absolute improvement success using temporal adaptation objective indicating importance integrating temporal information learning video. presented approach enables robot learning visually recognize manipulate object observing video demonstration human user. enable this method uses meta-training phase acquires rich prior human imitation using human robot demonstrations involving objects. method extends prior meta-learning approach allow learning crossdomain correspondences includes temporal adaptation loss function. experiments demonstrate that metalearning robots acquire vision-based manipulation skills object using video human demonstrator substantially different setting. limitations work enables one-shot learning manipulating objects video human current experiments demonstrate ability learn entirely motions shot. behaviors meta-test time structurally similar observed meta-training time though involve previously unseen objects demonstrators. expect data higher-capacity model would likely help enable extension. however leave future work. additional challenge approach amount demonstration data needed meta-training. experiments used thousand demonstrations robots humans. however total amount data per-object quite orders magnitude less number demonstrations per-object used recent single-task imitation learning works thus goal enable generalist robot adapt diverse range objects approach substantially practical. fig. human robot demonstrations used meta-training experiments section vi-b large domain shift. used different diverse backgrounds collecting human demonstrations. fig. frames human demos used evaluation section vi-b illustrating background scenes. leftmost background metatraining whereas right backgrounds novel objects human demonstrator novel. goal experiment evaluate generality method different robot different form robot demonstration collection. -dof sawyer robot kinesthetic teaching record robot demonstrations introduces additional challenges presence human demonstrator recorded images. human demonstrations collected perspective robot. consider placing task described section vi-a. unlike experiments action space single commanded pose endeffector mean-squared error outer metaobjective. since thoroughly compared method benchmarks evaluate proposed method experiment. evaluated method using held-out objects trials object. result placing success rate indicating method successfully applied sawyer robot handle kinesthetic demonstrations meta-training. learned adaptation objective ablation fig. sawyer robot set-up. left right human demo robot’s perspective policy execution robot’s perspective photo illustrating experimental set-up. beyond human imitation experiments focus imitating humans proposed method speciﬁc perceiving humans could also used example imitating animals simulated robot simulation real world transfer. beyond imitation believe approach likely broadly applicable problems involve inferring information out-of-domain data oneshot object recognition product images problem faced teams amazon robotics challenge akgun cakmak thomaz. trajectories keyframes kinesthetic teaching human-robot interaction perspective. international conference humanrobot interaction bishop. mixture density networks. bousmalis silberman dohan erhan krishnan. unsupervised pixel-level domain adaptation generative adversarial networks. arxiv. calinon billard. teaching humanoid robot recognize reproduce social cues. international symposium robot human interactive communication calinon evrard gribovskaya billard kheddar. learning collaborative manipulation tasks demonstrainternational conference tion using haptic interface. advanced robotics fernando habrard sebban tuytelaars. unsupervised visual domain adaptation using subspace alignment. international conference computer vision finn duan darrell levine abbeel. deep spatial autoencoders visuomotor learning. international conference robotics automation finn abbeel levine. model-agnostic metainternational grant finn levine darrell grifﬁths. recasting gradient-based meta-learning hierarchical bayes. international conference learning representations muelling venkatraman j.-s. valois downey weiss javdani hebert schwartz collinger bagnell. autonomy infused teleoperation application manipulation. autonomous robots m¨ulling kober kroemer peters. learning select generalize striking movements robot table tennis. international journal robotics research nehaniv dautenhahn correspondence pastor righetti kalakrishnan schaal. online movement adaptation based previous sensor experiences. international conference intelligent robots systems rahmatizadeh abolghasemi b¨ol¨oni levine. vision-based multi-task manipulation inexpensive robots using end-to-end learning demonstration. arxiv. zeng song k.-t. donlon hogan bauza taylor romo robotic pick-andplace novel objects clutter multi-affordance grasping cross-domain image matching. international conference robotics automation zhang mccarthy goldberg abbeel. deep imitation learning complex manipulation international confertasks virtual reality teleoperation. ence robotics automation ramirez-amaro beetz cheng. transferring skills humanoid robots extracting semantic representations observations human activities. artiﬁcial intelligence rana mukadam ahmadzadeh chernova boots. towards robust skill generalization unifying learning demonstration motion planning. proceedings conference robot learning rothfuss ferreira erdal askoy zhou asfour. deep episodic memory encoding recalling predicting episodic experiences robot action execution. arxiv. salakhutdinov tenenbaum torralba. one-shot learning hierarchical nonparametric bayesian model. icml workshop unsupervised transfer learning equivalence regularization truncated iteration general ill-posed problems. linear algebra applications schaal ijspeert billard. computational approaches motor learning imitation. philosophical transactions royal society london biological sciences shrivastava pﬁster tuzel susskind wang webb. learning simulated unsupervised images computer vision pattern adversarial training. recognition human robot demonstrations collecting take approximately seconds. then meta-training randomly sampled frames corresponding actions used demonstration. robot policy executed include details experiments section vi-a placing pushing pick-and-place tasks. architecture hyperparameters selected evaluating end-effector loss control loss validation objects placing pushing pick-and-place respectively sampled held training data. placing pushing inputs images size pick-and-place rgb-d images size train separate models three settings. placing policy architecture uses convolutional layers convolutional ﬁlters layer ﬁrst three layers stride last layer stride ﬁrst convolutional layer uses pretrained weights vgg-. also uses fully connected layers size learned adaptation objective layers convolutional ﬁlters followed layer convolutions. pushing policy architecture uses convolutional network fully connected layers size learned adaptation objective layers convolutional ﬁlters action ﬁnal gripper pose prediction respectively followed layer convolutions. pick-and-place policy architecture uses convolutional network except ﬁrst convolutional layer pretrained. also uses convolutional ﬁlters operate depth input concatenates depth stream stream ﬁrst convolutional layer channel-wise following approach zhang pick-and-place gripper poses intermediate gripper contacts item pick ﬁnal pose trajectory. architecture also uses fully connected layers size learned adaptation objective layers convolutional ﬁlters action ﬁnal gripper pose pickup gripper pose prediction respectively followed layer convolutions. architectures relu nonlinearities layer normalization. baseline methods architecture convolutional layers daml experiment. daml linear adaptation objective also uses fullyconnected architecture daml temporal adaptation objective except learned adaptation objective consists linear layer. lstm uses lstm hidden units experiments. contextual model uses fully connected layers size experiments. placing behavioral cloning loss combination losses loss scaled factor following prior work pushing pick-and-place experiments methods mixture density network mentioned section last fullyconnected layer modes negative likelihood mixture density network behavioral cloning loss. test time time step sample actions learned mixture distribution choose action highest probability. daml linear temporal adaptation objective step size placing pushing pick-and-place inner gradient clipping within range tasks meta batch iteration placing pushing pick-and-place respectively. methods human demonstration robot demonstration sampled task. train model iterations placing placing iterations pick-and-place. inner gradient update steps bias transformation dimension experiments. since don’t robot state human demonstrations state input computing inner gradient update feed robot states policy update policy parameters robot demonstrations. include details experiments section vi-b using diverse human demonstrations. meta-training eight pushing demonstrations taken total objects grouped pairs. demo shot front randomly selected background among backgrounds. viewpoint human demos held ﬁxed phone camera mounted tripod. model training images modiﬁed noise sampled uniformly range lighting. color augmentation process helps model perform robustly different light conditions. input image size policy architecture pushing experiment described section vi-a appendix include details experiments section vi-c sawyer robot. primary differences experiment previous placing experiments robot used demonstrations collected. robot demonstrations taken using teleoperation interface sawyer controlled kinesthetically humans demonstrator guided sawyer perform goal action. demonstrations collected saved timestep monocular image taken kinect sensor robot’s joint angles joint velocities gripper pose. architecture hyperparameters tuned evaluating gripper pose loss held validation objects. policy architecture takes images size uses convolutional fullyconnected layers well squared error behavioral cloning loss model placing experiment learned adaptation objective three layers convolutional ﬁlters followed layer convolutions. step size inner gradient clipping within range meta batch size human demonstration well robot demonstration sampled task. train model iterations. training augmented images using random color augmentation adding noise uniformly sampled saturation value. images used evaluation modiﬁed. control robot evaluation image frame used predict ﬁnal end-effector pose robot. robot reaches predicted gripper pose robot controlled using prediction actions continuous end-effector velocities. point gripper opened robot drops held item target container. demonstrations computed using reinforcement learning. following compute reported success rates tasks trials task totalling trials. time horizon trial considered successful target object lands target position least timesteps within -timestep episode. inputs images size policy architecture uses convolutional layers ﬁlters stride followed convolutional layer ﬁlters stride also fully-connected layers size learned adaptation objective layers convolutional ﬁlters followed layer convolutions. policy operates image along robot joint angles joint velocities end-effector pose. behavioral cloning loss mean squared error predicted actions ground truth robot commands. step size inner gradient clipping within range inner gradient update step. meta batch size different robot demonstrations sampled task. train policy iterations.", "year": 2018}