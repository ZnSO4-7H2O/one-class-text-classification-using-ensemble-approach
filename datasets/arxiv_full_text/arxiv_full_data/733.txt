{"title": "Adaptive Online Sequential ELM for Concept Drift Tackling", "tag": ["cs.AI", "cs.LG", "cs.NE", "68Txx", "I.2.4"], "abstract": "A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and hybrid drift. The AOS-ELM also works well for sudden drift and recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using concept drift public data set (SEA and STAGGER) and other public data sets such as MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition.", "text": "machine learning method needs adapt time changes environment. changes known concept drift. approach concept drift handling feeding whole training data learning machine retraining. another approach rebuilding ensemble classifiers adapt training data set. either approach retraining rebuilding classifiers expensive practical. paper propose enhancement online-sequential extreme learning machine variant constructive enhancement os-elm adding adaptive capability classification regression problem. scheme named adaptive os-elm single classifier scheme works well handle real drift virtual drift drifts occurred time aos-elm also works well sudden drift well recurrent context change type. scheme simple unified method implemented simple lines code. evaluated aos-elm regression classification problem using various public dataset widely used concept drift verification stagger; public datasets mnist usps. experiments show method gives higher kappa value compared multi-classifier ensemble. even though aos-elm practice need hidden nodes increase address issues related increasing hidden nodes error condition rank values. propose take rank pseudo inverse matrix indicator parameter detect ’under-fitting’ condition. data stream mining data mining technique trained model updated whenever data arrive. however trained model must work dynamic environments vast amount data continuously generated also keep changing. challenging issue known concept drift statistical properties input attributes target classes shifted time. shifts make trained model becoming less accurate. methods pursue accurate simple fast flexible retain classification performance drift occurs. ensemble classifier well-known retain classification performance. combined decision many single classifiers accurate single classifier however higher complexity handling multiple concept drifts. online sequential extreme learning machine constructive enhancement os-elm named adaptive os-elm aos-elm capability handle multiple concept drift problems either changes number attributes number target classes time also recurrent context sudden drift scope attribute changes discussed paper feature space concatenation widely used data fusion kernel fusion ensemble learning feature selection methods compared performance nonadaptive sequential os-elm ceos-elm. also compared performance classifier ensembles common adaptive approach concept drift solution. present study although focus adaptation aspect address possible change detection mechanisms suitable method. proceedings paper introduced scenarios consecutive drifts either recurrent sudden drift scenarios well theoretical background explanation. main contributions research area summarized follows ceos-elm addressing concept drifts issue. unlike ensemble systems need manage complex combination vast number classifiers pursue single classifier simple implementation retaining comparable performance handling multiple drifts. transfer learning focuses extracting knowledge source task domains applies knowledge different target task domain concept drift focuses time-varying domain small number current data available. contrast transfer learn drift type. e.g. concept virtual drift event replaced concept replaced concept concept recurrently shuffled composition requires entire training testing data example transfer learning using strategy transition different data sources still related purpose. paper discussed transfer learning numeric handwritten mnist alpha-numeric handwritten usps recognition. devised aos-elm strategy handle sudden drift scenario introducing output marginalization method. method also applicable concept drift regression problem. studied effect increasing number hidden nodes treated learning parameters improve accuracy proposed evaluation parameter predict accuracy training completed. applied assessment parameter actually prevent ’under-fitting’ nonconvergence condition learning parameter changes hidden nodes increased. concept drift background sequential learning. section presents background theory algorithm derivation proposed method. section focus empirical experiments prove methods research questions regression classification problem. artificial real data set. artificial data sets streaming ensemble algorithm stagger commonly used benchmark sequential learning. real data sets handwritten recognition data mnist numeric usps alpha-numeric classes studied effect hidden nodes increase important learning parameter section section discusses research challenges future directions. conclusion presents highlights section learning method updating model every time training pairs seen. subset input data time initialization stage. xx...x subset input data next sequential time. subset different number quantity. corresponding label data section briefly explained various concept drift solution strategies. gama explained many concept drift methods developed terminologies well established. according gama basic concept drift based bayesian decision theory classification problem class output incoming data different class data come alternately known recurrent context. drift conditional probabilities replaces previous conditional probabilities number class remained same known sudden drift. terms concept shift conditional change virtual drift refers changes distribution incoming data changes). changes incomplete partial feature representation current data distribution. trained model built additional data environment without overlapping true class boundaries. terms feature change temporary drift sampling shift. figure taxonomy quadrant adaptive supervised learning techniques. popular concept drift handling methods indicated ellipses proposed method aos-elm indicated dark blue diamond. kuncheva explained various configuration patterns data sources time random noise random trends random substitutions systematic trends random noise simply filtered out. gradual drift occurs many concepts re-occur alternately gradual stage certain period. consecutive drift takes place many previously active concepts might keep changing alternately time. sudden drift type time concept suddenly replaced another concept. describes methods based model switched learners adapt training formation design parametrization base learner ’when’ axis spans drift handling trigger based evolving based methods. ’how’ axis spans drift handling training formation model manipulation methods. organized four modules memory change detection learning loss estimation modular components integrated permuted combined other. modules learning change detection modules. methods focused subset often mixtures many types within certain concept drifts. predictive model. learning module categorized based model updated data points available retraining incremental modes. behavior predictive models time-evolving data blind based module informed based module. iii) techniques maintaining active predictive models single model ensemble model. change detection module refers drift detection. change detection identifies change points small time intervals changes occur. drift employed different solution strategies. solution entirely different systematic changes likely reappear want keep past successful classifiers simply reuse them. changes gradual moving window strategy training data. changes abrupt pause existing static classifiers retrain classifier using training data. thus hard combine simultaneously many strategies time solve many types concept drift simple platform. simplicity. huang explained term ’extreme’ meant move beyond conventional artificial neural network learning required iterative tuning. moves toward brain like learning hidden neurons need tuned. sub-matrix zero block matrix simplify computation accordance fact previous data related hidden nodes. additional hidden nodes block matrix data relation additional hidden nodes then rewrite opium based greville’s method incremental solutions compute pseudo inverse matrix. pseudo inverse computation solved incrementally linear regression problems adaptive allows stationary data. derivation opium equivalent os-elm condition iteration. condition implies linear combination previous hidden layer simpler derivation right pseudo inverse become on-diagonal element schaik et.al. applied opium light non-stationary data using different weight determining recent pair appropriate non-stationary mapping weight training uncertain data solve problem concept drift. second uncertainty classifier based algorithm designed classification unknown data streams considers attribute value uncertainty thus improving efficiency accuracy. concept drift occurs wec-elm dynamically adjust classifiers weight training data thus classifier added ensemble reached preset maximum removed worst-performing classifier. uc-elm designed classification uncertain data streams attributes uncertainty values. uc-elm evaluated uncertainty value every newly arrived attribute decided based probability attributes belonging class thus improving efficiency accuracy. opinion wec-elm categorized evolving based method selecting best-performing classifier uc-elm addressed virtual drift problem using uncertainty attributes selection. model concept bounded hypothesis space feature space learning model concept bounded hypothesis space feature space defined real drift hypothesis space changed scoped definition dimension changes. virtual drift feature space changed scoped definition dimension changes. achieve consistency minimized square error hypothesis space feature space learning model needs transition former space space. learning model needs transition space converges learning model transition space idea inspired design space. first approach suitable scenario assigned random coordinates input weight parameters. second approach suitable situation setting equivalent projection coordinates space coordinate corresponding projection coordinates according theorem learning principle theory input weight bias hidden nodes parameters independent training samples learning environment randomization. independence initial training also sequential training stages. thus adjust input weight bias pair bi}l sequential stages still make sure probability theorem given nonconstant piecewise continuous function span dense continuous target function function sequence {g}l randomly generated according continuous sampling distribution liml→∞ f−fl holds probability output weights determined ordinary least square minimize x)−σl based theorem inspired related works devised aos-elm real drift capability modifying output matrix zero block matrix concatenation change size dimension matrix without changing value. zero block matrix meant previous knowledge concept. approximate complex decision boundary long output weights determined ordinary least square keep minimum. codes tackling virtual drift; aoselmrdseq sequential patterns used various data starting synthetic data real data handwritten recognition data different drift characteristics. experiment presented section also demonstrated aos-elm capability drift detection role section using data set. simulation benchmark tests datasets commonly used concept drift handling stream data e.g. stagger datasets binary classification problem. inputs random integer values stagger three inputs multiple category values stagger examples concept drift caused discriminant function changes number attributes classes concepts still same. change type sudden drift. expected result classifier good performance newest concept used original grey-level image attributes mnist data combination additional attributes bins histogram orientated gradients grey-level image features usps added data gaussian random salt-pepper noises. refer table detail data information. designed initial input weights bias based robust os-elm regularization scalar based initial random normal distribution activation function sigmoid. pseudo inverse function orthogonal projection using ridge regularization. followed simulated concept drift methods dries et.al simulated sudden drift splitting composition groups e.g. recurring context shuffled composition sequential training flow following drift equation followed simulated concept drift methods dries et.al simulated sudden drift splitting composition groups e.g. recurring context shuffled composition sequential training flow following drift equation cohen’s kappa show quantitative measurement. predictive accuracy demonstrate trend line chart. sudden drift performance based forgetting capability compared testing accuracy latest concept previous concepts. offline version classifier. performance expectation sequential version classifier approximate offline version classifier also compared adaptive ensemble method designed hierarchical ensemble using models classifier different roles first role binary classifier concepts thus effective consecutive concept drift case e.g. concepts. ensemble also applied outdated classifier pruning ensemble detects previous attributes need replaced. addressed question whether non-adaptive os-elm ceos-elm increase could handle concept drift situation. compared aos-elm increase increase used -fold cross-validation compared norm parameter. parameters increase drift. stagger parameters hidden nodes increase drift. ceos-elm whereas non-adaptive os-elm fails aos-elm improved forgetting capability better aos-elm. comparison kolter et.al result using dynamically weighted majority naive bayes aos-elm result near result. comparison inducing decision trees stagger aos-elm outperformed dwm. evaluate intersection point accuracy decrease increase fig. consecutive loss performance exceeded certain threshold drift warning status triggered. measured output performance concept output compared previous output. certain criteria aos-elm committed. otherwise previous aos-elm rolled back. aos-elm cohen’s kappa testing accuracy approximated non-adaptive offline version hidden nodes number better accuracy single attribute proves explanation theoretical background section figure predictive accuracy aos-elm concept output consider intersection point accuracy decrease previous concept accuracy increase current concept change point aos-elm better cohen’s kappa performance numeric alphabet concepts ensemble approximate non-adaptive offline elm. aos-elm shows better recovery time ensemble fig. initial size hidden nodes selection important good generalization performance. studies suggested hidden node size equal least rank value training data. however data stream hard determine fixed number hidden nodes following suggestion. larger requires computation resources processing time probably giving significant result end. thus requirement increase sequential stage table predictive accuracy performance aos-elm mnist using different parameters rank increase. experiment repeated trials probability predictive accuracy sequential phase table investigated particular condition aos-elm classifier result. realized performance dependent upon finding general matrix inverse based orthogonal projection method ceos-elm employ rank value evaluation parameter detect ’under-fitting’. approximation matrix thus full rank anymore. sequential learning compare rank hidden nodes increase. expected result positive increment. rank value becomes lower hidden nodes increase higher probability ’under-fitting’ condition occur. rank determined block size training data number hidden nodes increment scalar selection parameter activation function input weight bias random assignment method. experiment focused block size number hidden nodes scalar selection. using rank evaluation parameter efficient need compute similar real drift scenario output marginalization output amplification solve concept drift problem regression. experiment used aos-elm single input node single output node concept. defined following concept presented result following figures compare performance concept training experiment. objective show aos-elm regression capability keep previous regression concept knowledge. select constant value giving best regression result concept. aos-elm sigmoid function. drifts occurred weaken older concepts. thus needs larger amplifier constant value. network security technology scans network packet traffic detect potential exploits sending alarm taking active action intrusion prevention system. machine learning methods applied hope improving detection rates adaptive capability attack names based signature-based detection) dataset control information header delivering data numerical multi-categorical values features. focused service names attributes specific differentiators applications. number attack classes stationary. analyzed data growing service names number class attack whole dataset fig. challenge dataset imbalanced data classes. highest number data ’normal’ class lowest number ’spy’ class simplify experiment oversampling adding data based random normal distribution packet signatures sampling approaches dropping samples randomly. based growing service names number classes analysis designed drift scenarios based concepts classes service names classes service names. total training data concept packets. data repetition previous event except sequential training. composition event validation data selected packets minority classes randomly selected original majority classes used holdout method trials. used aos-elm aos-elm aos-elm result experiment approximate non-adaptive os-elm proposed method gives better adaptive capability adaptive os-elm ceos-elm term retaining recognition performance handling concept drifts. uses simple line code easy deploy especially consecutive drifts compared adaptive ensemble methods. adaptive classifiers work differently virtual real drift hybrid drift scenarios aos-elm tackles drifts simple block matrix reconstruction rank evaluation. aos-elm satisfied requirement criteria term accuracy simplicity fast flexible. however certain cases aos-elm accuracy exceed adaptive sequential include future training data. cases aos-elm better accuracy. real data implementation non-adaptive better preferred know exactly future behavior data. however predict precisely. believe using larger training data aos-elm performance approximate expected value non-adaptive sequential offline future training data. aos-elm also learning adaptation function previous offline learning model. makes aos-elm excellent choice unpredictable situation. aos-elm tackles sudden drift change type well recurrent context change type. output marginalization strategy implemented simply shifting output nodes belonging latest concept. aos-elm need increase hidden nodes improve forgetting capability sudden drift change type. make sure convergence expected learning model proposed rank value pseudo inverse autocorrelation hidden nodes matrix evaluation parameter prevent ’under-fitting’ condition makes accuracy performance dropped. shared interconnected hidden nodes ensemble members. implement aos-elm similar fashion compared ensemble adaptive learning scheme better performance simpler resource efficient. however aos-elm drawbacks. hidden node changes could impact notions.", "year": 2016}