{"title": "Learning Robust Representations for Computer Vision", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Unsupervised learning techniques in computer vision often require learning latent representations, such as low-dimensional linear and non-linear subspaces. Noise and outliers in the data can frustrate these approaches by obscuring the latent spaces.  Our main goal is deeper understanding and new development of robust approaches for representation learning. We provide a new interpretation for existing robust approaches and present two specific contributions: a new robust PCA approach, which can separate foreground features from dynamic background, and a novel robust spectral clustering method, that can cluster facial images with high accuracy. Both contributions show superior performance to standard methods on real-world test sets.", "text": "unsupervised learning techniques computer vision often require learning latent representations lowdimensional linear non-linear subspaces. noise outliers data frustrate approaches obscuring latent spaces. main goal deeper understanding development robust approaches representation learning. provide interpretation existing robust approaches present speciﬁc contributions robust approach separate foreground features dynamic background novel robust spectral clustering method cluster facial images high accuracy. contributions show superior performance standard methods real-world test sets. supervised learning particular deep learning successful computer vision. applications include autoencoders noisy clean images convolutional networks image/video analysis generative adversarial networks synthesize real world-like images contrast unsupervised learning still poses signiﬁcant challenges. broadly unsupervised learning seeks discover hidden structure data without using ground truth labels thereby revealing features interest. paper consider unsupervised representation learning methods used along centroidbased clustering summarize data distribution using characteristic samples. deep embedding-based clustering strategies spectral clustering methods neighborhood graphs learn underlying representation approach used image segmentation mesh segmentation subspace clustering methods model dataset union low-dimensional linear subspaces utilize sparse low-rank methods obtain representation; model used facial clustering recognition learning effective latent representations hinges accurately modeling noise outliers. further practice data satisfy structural assumptions approximately. adopting robust optimization strategies natural combat challenges. example consider principal component analysis prototypical representation learning method based matrix factorization. given low-rank data contaminated outliers classical method fail consequently robust method decomposes data rank sparse components preferred practice e.g. background/foreground separation similarly data assumed union subspaces contaminated outliers allowing sparse outliers optimization leads accurate recovery subspaces e.g. face classiﬁcation goal develop effective robust formulations unsupervised representation learning tasks computer vision; interested complex situations data corrupted combination sparse outliers dense noise. contributions. ﬁrst review relationship outlier models statistically robust formulations. particular show rpca formulation equivalent solving huber regression problem low-rank representation learning. using connection develop nonconvex penalty dubbed tiber designed aggresfigure robust penalties left huber right tiber. grow linearly outside interval containing origin. tiber penalizes ‘mid-sized’ errors within region aggressively huber; penalty must necessarily non-convex. second contribution design philosophy behind robust low-rank representation learning develop formulation robust clustering. formulate classic spectral analysis optimization problem modify problem robust outliers. advantages shown using synthetic clustering example. combine robust spectral clustering robust subspace clustering achieve superior performance face recognition tasks surpassing prior work without data pre-processing; section table many tasks computer vision depend unsupervised representation learning. well-known example background/foreground separation often solved robust principal component analysis rpca learns low-rank representations decomposing data matrix low-rank sparse components. low-rank component represents background sparse component represents foreground section show rpca equivalent robust regression problem solving huber-robust regression background representation completely equivalent full rpca solution. equivalence design robust penalty based statistical descriptions signals interest. illustrate beneﬁts using non-convex penalty separating foreground dynamic background using real datasets. huber rpca background/foreground separation widely used detecting moving objects videos stationary cameras. broad range techniques developed tackle task ranging simple thresholding mixtures particular rpca gaussian models. widely adopted solve problem denote given video stream rn×m frames reshaped vector size many variants rpca stable principal component pursuit formulation represents background foreground. regularizations used formulation ensure chosen rank designed sparse; using quadratic penalty residual data error level. want move dynamics low-rank background term. however huber quadratic near origin small perturbations significantly affect objective value; solving leaves terms residual thresholding terms either aggressive lenient leaving dynamics foreground better penalty would rise steeply small values without signiﬁcantly affecting tail behavior. tiber rpca propose penalty call tiber. huber deﬁned partially minimizing -norm quadratic tiber replaces quadratic nonconvex function. resulting penalty match tail behavior huber different properties around origin tiber better suited background/foreground separation problems dynamic background. deﬁne penalty follows tiber parametrized thresholding parameter scale parameter huber expressed value function minimization problem. replace quadratic penalty claim smooth nonconvex penalty log). simplicity result below. approach illustrated left panels figure although residual noisy sparse applying sparse component would solving original formulation statistical perspective equivalence rpca huber means residual contains random noise modeled heavy tailed error distribution. claim suppose {ri}l tribution density claim follows immediately taking negative maximum likelihood. claim means solving equivalent assuming elements i.i.d. samples laplace density although huber distribution detect sparse outliers model small errors well. many background/foreground separation problems must cope dynamic background small dynamic background perturbations correspond motion care much interested detecting cars people animals moving scene. data convert grey scale reshape column vectors matrix compare formulations proximal alternating linearized minimization algorithm used solve optimization problems. rank experiments. manually tuned parameters achieve best possible recovery formulation. huber selected nearby values tiber selected resulting threshold parameter results shown figure task identifying avoiding interference moving leaves. huber unable separate leaves threshold values choose much information giving incomplete make less conservative choice leave much dynamic noise obscures van. finding meaningful similarity matrix crucial success spectral clustering. ideally block diagonal matrix blocks. rarely happens real applications; even underlying structure present obscured noise small number points don’t follow general pattern. centroid-based clustering e.g. k-means standard tool partition summarize datasets. given high dimensionality complexity data computer vision applications necessary learn latent representations underlying metric prior clustering. clustering performed latent space. develop approach robust spectral clustering. illustrate advantages using synthetic dataset combine approach robust subspace clustering achieve perfect performance face recognition tasks. spectral clustering approach effective. consider following clustering experiment generate clusters points group. make problem challenging move clusters close together much trying tell apart naked hard true clusters appear figure bottom. classic spectral clustering uses eigenvalue decomposition step fails detect true relationships robust spectral clustering using huber penalty much better subspace clustering subspace clustering looks dimensional representation high dimensional data grouping points along low-dimensional subspaces. given data matrix rn×m section optimization subspace clustering given formulation looks sparse representation dataset members sci. avoid trivial solution require diagonal identically obtaining post-processed similarity matrix constructed ideally close block-diagonal block represents lambertian assumption pictures obtained person different illuminations close dimensional subspace practice spaces hard detect noise images robust approach required. robust subspace clustering face images obtain sparse representation using better intuition method plot similarity matrix corresponding figure clearly three blocks along diagonal correspond three face clusters. resulting projected obtained eigenvalue decomposition similarity matrix shown figure three clusters clearly well separated. ﬁnal algorithm perfect accuracy example. robust approaches essential unsupervised learning designed using optimization formulations. example rpca robust spectral learning eigenvalue decomposition ﬁrst characterized using optimization reformulated robust losses. several tasks approach difﬁcult. first need tune parameters optimization formulations. example tiber depends parameters automatic ways tune parameters make robust unsupervised learning portable. second optimization problems solve largescale; time required robust subspace clustering images scales non-linearly number size images. designing non-smooth stochastic algorithms take structure problems account essential.", "year": 2017}