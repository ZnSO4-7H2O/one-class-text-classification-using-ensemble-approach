{"title": "Robust Task Clustering for Deep Many-Task Learning", "tag": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "abstract": "We investigate task clustering for deep-learning based multi-task and few-shot learning in a many-task setting. We propose a new method to measure task similarities with cross-task transfer performance matrix for the deep learning scenario. Although this matrix provides us critical information regarding similarity between tasks, its asymmetric property and unreliable performance scores can affect conventional clustering methods adversely. Additionally, the uncertain task-pairs, i.e., the ones with extremely asymmetric transfer scores, may collectively mislead clustering algorithms to output an inaccurate task-partition. To overcome these limitations, we propose a novel task-clustering algorithm by using the matrix completion technique. The proposed algorithm constructs a partially-observed similarity matrix based on the certainty of cluster membership of the task-pairs. We then use a matrix completion algorithm to complete the similarity matrix. Our theoretical analysis shows that under mild constraints, the proposed algorithm will perfectly recover the underlying \"true\" similarity matrix with a high probability. Our results show that the new task clustering method can discover task clusters for training flexible and superior neural network models in a multi-task learning setup for sentiment classification and dialog intent classification tasks. Our task clustering approach also extends metric-based few-shot learning methods to adapt multiple metrics, which demonstrates empirical advantages when the tasks are diverse.", "text": "investigate task clustering deep-learning based multi-task few-shot learning many-task setting. propose method measure task similarities cross-task transfer performance matrix deep learning scenario. although matrix provides critical information regarding similarity tasks asymmetric property unreliable performance scores aﬀect conventional clustering methods adversely. additionally uncertain task-pairs i.e. ones extremely asymmetric transfer scores collectively mislead clustering algorithms output inaccurate task-partition. overcome limitations propose novel task-clustering algorithm using matrix completion technique. proposed algorithm constructs partially-observed similarity matrix based certainty cluster membership task-pairs. matrix completion algorithm complete similarity matrix. theoretical analysis shows mild constraints proposed algorithm perfectly recover underlying true similarity matrix high probability. results show task clustering method discover task clusters training ﬂexible superior neural network models multi-task learning setup sentiment classiﬁcation dialog intent classiﬁcation tasks. task clustering approach also extends metric-based few-shot learning methods adapt multiple metrics demonstrates empirical advantages tasks diverse. paper leverages knowledge distilled large number learning tasks many task learning achieve goals improving overall performance tasks multi-task learning rapid-adaptation task using previously learned knowledge similar few-shot learning transfer learning. previous work multi-task learning transfer learning used small numbers related tasks picked human experts. contrast matl tackles hundreds thousands tasks unknown relatedness pairs tasks introducing challenges task diversity model ineﬃciency. matl scenarios increasingly common wide range machine learning applications potentially huge impact. examples include reinforcement learning game playing many numbers sub-goals treated tasks agents joint-learning e.g. achieved state-of-the-art pac-man game using multi-task learning architecture approximate rewards sub-goals another important example corporate cloud services many clients submit various tasks/datasets train machine learning models business-speciﬁc purposes. clients could companies want know opinion customers products services agencies monitor public reactions policy changes ﬁnancial analysts analyze news potentially inﬂuence stock-market. matl-based services thus need handle diverse nature clients’ tasks. task-clustering popular technique matl handle task diversity. task-clustering seeks share common knowledge across similar tasks within cluster preserving task-speciﬁc knowledge among diﬀerent clusters given carefully designed metric task similarities. standard task similarities include model parameter similarity co-training objectives however mainly deﬁned convex models leveraging fact global optimal solution could serve unique model signature. moreover methods usually cannot handle tasks varying sets class labels common real-world scenarios. adopt diﬀerent measure task similarity based cross-task transfer performance. cross-task transfer performance matrix whose -entry estimated performance i-th task model j-th task. based measure various numbers labels among diﬀerent tasks could handled transferring common representation function e.g. text classiﬁcation representation function i-th model encodes sentence hidden representation could transfered j-th task. although cross-task transfer performance provide critical information task similarities cannot directly input conventional clustering methods because matrix asymmetric fact performance transferring task task usually diﬀerent transferring task task estimated cross-task performance often unreliable small data size inaccurate class labels importantly hard decide whether uncertain task-pairs cluster i.e. pairs tasks transfer performance high low. number uncertain task pairs large collectively mislead clustering algorithm output incorrect task-partition. address aforementioned challenges propose novel task-clustering algorithm based theory matrix completion given tasks basic idea ﬁrst construct partiallyobserved similarity matrix observed entries generated based reliable task pairs i.e. pairs tasks either high enough enough. complete generated partially-observed matrix using robust matrix completion approach generate ﬁnal task partition applying spectral clustering completed similarity matrix. proposed approach -fold advantage. first ﬁltering uncertain task pairs proposed algorithm less sensitive noise leading robust performance. second converting asymmetric transfer performance matrix symmetric similarity matrix many widely-used similaritybased clustering algorithms applied task-partitioning. third method carries strong theoretical guarantee showing full similarity matrix perfectly recovered number observed correct entries partially observed similarity matrix least results show proposed method discover task-clusters ﬂexible neural network models giving signiﬁcantly better multi-task learning algorithms sentiment classiﬁcation intent classiﬁcation tasks. task-clustering approach also extends metric-based few-shot learning methods adapt multiple metrics demonstrates empirical advantage diverse task setting. related work task/dataset clustering model parameters class task clustering methods measure task relationships terms model parameter similarities individual tasks. given parameters convex models task clusters cluster assignments could derived matrix decomposition k-means based approach parameter similarity based task clustering method deep neural networks applied low-rank tensor decomposition model layers multiple tasks. method infeasible matl setting high computation complexity respect number tasks inherent requirement closely related tasks parameter-similarity based approach. task/dataset clustering training objectives another class task clustering methods joint assign task clusters train model parameters cluster minimize training loss within cluster k-means based approach minimize overall training loss combined sparse low-ranker regularizers convex optimization deep neural networks ﬂexible representation power overﬁt arbitrary cluster assignment consider training loss alone. also methods require identical class label sets across diﬀerent tasks hold real-world matl settings. shot learning aims learn classiﬁers classes training examples class. bayesian program induction represents concepts simple programs best explain observed examples bayesian criterion. siamese neural networks rank similarity inputs matching networks maps small labeled support unlabeled example label obviating need ﬁne-tuning adapt class types. approaches essentially learn metric figure convolutional neural networks used work single-task cnn; multi-task encoder component takes sentence input outputs ﬁxed-length sentence embedding vector classiﬁer component predicts class labels sentence embedding. tasks sub-optimal tasks diverse. lstm-based meta-learner learns exact optimization algorithm used train another learner neural-network classiﬁer few-shot setting. however requires uniform classes across tasks. approach handle challenges diversity varying sets class labels. previous multi-task learning few-shot learning research usually work homogeneous tasks e.g. tasks binary classiﬁcation problems tasks close positive transfer tasks guaranteed. large number tasks many-task setting assumption hold. therefore necessary matl algorithms adapt heterogeneous tasks. speciﬁcally paper hope handle following challenges tasks varying numbers labels tasks heterogeneous diﬀerent tasks could diﬀerent numbers labels; labels might deﬁned diﬀerent label spaces without relatedness. existing multi-task few-shot learning methods fail setting. tasks positive negative transfers since tasks guaranteed similar many-task setting always able help trained together. problem called negative transfer tasks. observation important reason negative transfer diﬀerences conditional distributions means heterogeneous tasks could vary terms conditional distribution instead input distribution input label. example dialog services sentences what fast food nearby could indian food belong diﬀerent classes fast_food indian_food restaurant recommendation service city; travel-guide service park sentences could belong class food_options illustrated figure case tasks could hurt trained jointly single representation function since ﬁrst task turns give similar representations sentences second turns distinguish representation space. solution problem shown section deal negative transfer problem novel task clustering algorithm; handle problem varying numbers labels evaluation task similarity based transferability learned representations task clustering; metric-learning nearest neighbor based classiﬁer need handle tasks few-shot learning setting. input sentence document label. ﬁrst train yields models m··· mn}. classiﬁcation model training dtrain convolutional neural network reported results near state-of-the-art text classiﬁcation cnns also train faster recurrent neural networks making large-n matl scenarios feasible. figure shows architecture. following model consists convolution layer max-pooling operation entire sentence. model parts broad deﬁnitions encoder part classiﬁer part. hence model {menc encompasses classiﬁcation tasks classiﬁcation models propose task-clustering framework multi-task learning few-shot learning settings. framework algorithms summarized algorithm task-clustering framework serves initial step algorithm. cross-task transfer-performance matrix estimation using single-task models compute performance scores adapting task forms pair-wise classiﬁcation performance matrix called transferperformance matrix. note asymmetric since usually sji. dtrain train classiﬁer layer. gives task model test model dvalid accuracy transfer-performance sij. score shows representations learned task adapted task thus indicating similarity tasks. learning single-task models train single-task models task evaluation transfer-performance matrix performance matrix score filtering filter uncertain scores construct symmetric matrix matrix completion complete similar matrix task clustering spectralclustering robust task clustering matrix completion address uncertainty task pairs propose novel task clustering approach based matrix completion given tasks idea proposed algorithm ﬁrst construct partially observed similarity matrix entries associated reliable task relationships marked observed. apply matrix completion approach unobserved entries. finally spectral clustering applied completed similarity matrix generate ﬁnal clusters. below describe algorithm detail. first reliable task pairs generate partially observed similarity matrix. speciﬁcally high enough likely tasks belong cluster share signiﬁcant information. conversely enough tend belong diﬀerent clusters. need design mechanism determine performance high enough. since diﬀerent tasks vary diﬃculty ﬁxed threshold suitable. hence deﬁne dynamic threshold using mean standard deviation target task performance i.e. mean j-th column introduce positive parameters deﬁne high performance greater lower respectively. high enough pairwise similarity enough pairwise similarity task pairs treated uncertain task pairs marked unobserved inﬂuence clustering method leading robust clustering performance. resulting partially observed similarity matrix given given partially observed matrix reconstruct full similarity matrix rn×n. ﬁrst note similarity matrix low-rank additionally since observed entries generated based high enough performance safe assume observed entries correct would incorrect. introduce sparse matrix capture observed incorrect entries combining observations decomposed matrices rank matrix storing similarities task pairs sparse matrix captures errors matrix reconstruction problem cast convex optimization problem following theorem shows perfect recovery guarantee problem theorem rn×n rank matrix singular value decomposition rn×k rn×k left right singular vectors respectively. similar many related works matrix completion assume following assumptions satisﬁed suppose entries observed locations sampled uniformly random among observed entries randomly sampled entries corrupted. using resulting partially observed matrix input problem probability least underlying matrix perfectly recovered given theorem implies even observed entries computed incorrect problem still perfectly recover underlying similarity matrix number observed correct entries least matl large implies tiny fraction task pairs needed reliably infer similarities task pairs. moreover completed similarity matrix symmetric symmetry input matrix enables analysis similarity-based clustering algorithms spectral clustering. multi-task learning based tasks clusters cluster train model tasks cluster encourage parameter sharing. call cluster-model. evaluated setting suﬃcient data train taskspeciﬁc classiﬁer share encoder part distinct task-speciﬁc classiﬁers task-speciﬁc classiﬁers provide ﬂexibility handle varying number labels. few-shot learning based tasks clusters access limited number training samples few-shot learning setting impractical train well-performing task-speciﬁc classiﬁers multi-task learning setting. instead make prediction task linearly combining prediction learned clusters. alternatives train cluster-models could better suit method. tasks identical label sets train single classiﬁcation model tasks like previous work predictor directly derived cluster-model. tasks diﬀerent label sets train metric-learning model like among tasks consist shared encoding function λenc aiming make example closer examples label compared ones diﬀerent labels. experiment setup data test methods conducting experiments three text classiﬁcation datasets. data-preprocessing step used nltk toolkit tokenization. setting tasks used clustering model training. setting task divided training tasks testing tasks training tasks used clustering model training testing tasks few-shot learning ones used evaluating method amazon review sentiment classiﬁcation first following construct multi-task learning setting multi-domain sentiment classiﬁcation data set. dataset consists amazon product reviews types products domain construct three binary classiﬁcation tasks diﬀerent thresholds ratings tasks consider review positive belongs following buckets stars stars stars review-buckets form basis task-setup matl giving tasks total. domain distribute reviews uniformly three tasks. evaluation select tasks domains target tasks domains. evaluation create ﬁve-shot learning tasks selected target tasks. cluster-models evaluation standard cnns figure share output layer evaluate probability tasks number labels. diverse real-world tasks user intent classiﬁcation dialog system second dataset on-line service trains serves intent classiﬁcation models various clients. dataset comprises recorded conversations human users dialog systems various domains ranging personal assistant complex service-ordering customer-service request scenarios. classiﬁcation intent-labels assigned user utterances total tasks diﬀerent clients randomly sample tasks target tasks. task randomly sample data training validation rest test number labels tasks vary hence adapt scenario keep example label plus randomly picked labeled examples create training data. believe fairly realistic estimate labeled examples client could provide easily. since deal various number labels setting chose matching networks cluster-models. extra-large number real-world tasks similar second dataset collect intent classiﬁcation tasks on-line service. setting mainly used verify robustness task clustering method since diﬃcult estimate full transfer-performance matrix setting therefore order extract task clusters randomly sample task pairs data obtain entries means k/.m entries observed. number chosen close theoretical bound theorem could also verify tightness bound empirically. make best sampled pairs setting modiﬁed entry otherwise. could determined number entries well since sampled pairs correspond observed entries setting dataset. baselines setting compare method following baselines single-task training model task individually; holistic mtl-cnn training mtl-cnn model tasks; holistic mtl-cnn training mtl-cnn model target tasks. setting baselines consist single-task training model task individually; single-task fasttext training fasttext model ﬁxed embeddings individual task; fine-tuned holistic mtl-cnn ﬁne-tuning classiﬁer layer target task training initial mtl-cnn model training tasks; matching network metric-learning based few-shot learning model trained training tasks. initialize models pre-trained -dim glove embeddings intent classiﬁcation tasks usually various numbers labels best knowledge proposed method supporting task clustering setting; hence compare baselines. since sentiment classiﬁcation involves binary labels compare method state-of-the-art logistic regression based task clustering method also hyper-parameter tuning experiments parameters strikes balance obtaining enough observed entries ensuring retained similarities consistent cluster membership. settings tune parameters like window size hidden layer size learning rate initialization embeddings based average accuracy union tasks’ sets order best identical setting tasks. finally window size hidden units. learning rate selected models random initialized word embeddings sentiment classiﬁcation glove embeddings initialization intent classiﬁcation likely training sets intent tasks usually small. also used early stopping criterion based previous condition. setting hyper-parameter selection diﬃcult since validation data case preselect subset training tasks validation tasks tune learning rate training epochs validation tasks. testing phase selected hyper-parameter values algorithms. out-of-vocabulary transfer-performance evaluation text classiﬁcation tasks transferring encoder ﬁne-tuned word embeddings task another work signiﬁcant diﬀerence vocabularies. hence learning single-task models always cnns ﬁxed pre-trained embeddings. sentiment classiﬁcation amazon product reviews improving observed tasks table shows results target tasks tasks used training. since tasks signiﬁcant amount training data single-task baselines achieve good results. conﬂicts among tasks holistic mtl-cnn show accuracy improvements compared single-task methods. also lags behind holistic mtl-cnn model trained target domains indicates holistic mtl-cnn cannot leverage large number background tasks. robusttc-mtl method based task clustering achieves signiﬁcant improvement baselines. asap-mtlr could improve single-task linear models similar merit method. however restricted representative strength linear models overall result lower deep learning baselines. adaptation tasks table shows results ﬁve-shot tasks leveraging learned knowledge previously observed tasks. limited training resources baselines perform poorly. robusttc-fsl gives better results compared baselines also signiﬁcantly better applying without clustering i.e. using single-task model task instead cluster-models comparison asap clusters clustering-based approaches also work asap clusters replace task clusters task clusters generated asap-mtlr. setting slightly lower performance compared robusttc-based ones settings overall performs better baseline models. show that apart ability handle varying number class labels robusttc model also generate better clusters mtl/fsl deep networks even setting tasks number labels. worth note table training cnns asap clusters gives better results compared training logistic regression models clusters despite clusters optimized cnns. result emphasizes importance task clustering deep models better performance could achieved models. user intent classiﬁcation diverse real-world online services table show results dialog intent classiﬁcation demonstrates trends similar sentiment classiﬁcation tasks. note holistic methods achieves much better results compared single-task cnns. tasks usually smaller training development sets model parameters learned training hyper-parameters selected development easily lead over-ﬁtting. robusttc-mtl achieves large improvement best baseline tasks diverse sentiment classiﬁcation tasks task-clustering greatly reduces conﬂicts irrelevant tasks. although robusttc-fsl improves baselines setting margin smaller. huge diversity among tasks looking training accuracy found several tasks failed none clusters could provide metric suits training examples. deal problem hope algorithm automatically decide whether task belongs task-clusters. task doesn’t belong clusters would beneﬁt previous knowledge fall back single-task cnn. task treated out-of-cluster none clusters could achieve higher accuracy training data. call method adaptive robusttc-fsl gives performance boost best robusttc-fsl result. discussion clustering-based single metric based method achieved success homogeneous few-shot tasks like omniglot miniimagenet performs poorly experiments. indicates important maintain multiple metrics few-shot learning problems diverse tasks similar few-shot problems investigated paper. clustering-based approach maintains diverse metrics keeping model simple small number parameters. worthwhile study problems make few-shot learning diﬃcult/heterogeneous; well method generalize non-nlp problems like miniimagenet. leave topics future work. large-scale user intent classiﬁcation task-pair sampling table shows results extra-large dialog intent classiﬁcation dataset. compared results tasks holistic mtl-cnn achieves larger improvement single-task cnns stronger baseline. similar observation tasks main reason improvement consistent development test performance holistic multi-task training approach single-task holistic multi-task model achieve around average accuracy development sets. unlike experiments section evaluate full transfer-performance matrix subset corresponding task-pairs. however signiﬁcant improvement achieved applying robusttc-mtl algorithm. note result achieved sampling task pairs conﬁrms empirical advantage multi-task learning algorithm also veriﬁes correctness theoretical bound theorem paper propose robust task-clustering method strong theoretical guarantees also demonstrates signiﬁcantly empirical improvements equipped algorithms. empirical studies verify proposed task clustering approach eﬀective many-task learning setting especially tasks diverse cross-task transfer performance serve powerful task similarity measure. work opens many future research directions like supporting on-line many-task learning computation similarities small proportion task pairs also worth investigating combination clustering approach recent learning-to-learn methods help enhance methods learning meta-learner task cluster.", "year": 2017}