{"title": "Learning Bayesian Networks with Restricted Causal Interactions", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "A major problem for the learning of Bayesian networks (BNs) is the exponential number of parameters needed for conditional probability tables. Recent research reduces this complexity by modeling local structure in the probability tables. We examine the use of log-linear local models. While log-linear models in this context are not new (Whittaker, 1990; Buntine, 1991; Neal, 1992; Heckerman and Meek, 1997), for structure learning they are generally subsumed under a naive Bayes model. We describe an alternative interpretation, and use a Minimum Message Length (MML) (Wallace, 1987) metric for structure learning of networks exhibiting causal independence, which we term first-order networks (FONs). We also investigate local model selection on a node-by-node basis.", "text": "examine local models learning particular used extensively gate interactions log-linear generally structure lauritzen structure learning log-linear hayes approach restricted describe exponential parameters ability tables. complexity probability log-linear models context sumed naive bayes model. scribe alternative using minimum mes­ sage length metric selection causal independence first-order foms full conditional by-node basis. methods learning inference probabilistic prove application problems exponential specify recent research number parameters ture probability tables. complexity modeling bns. parameter lying causal benefits discovery parameters otherwise. buntine perimental directed termed linear causal interactions pletely important ditional model's structure. isfy markov property dependency population conditional capable representing accordingly cally equivalent. case identical identical adjacent -and therefore arcs. contrast models distinguishes tation necessarily single differ even orientation andy-+ model cause cause distinguish equivalent rely considerations effort reduce size cpts local probability effect interact networks restrictive parameter advantage variable recently decompositions poole meek heckerman specifically friedman goldszmidt graphs efit first induction causal interaction surprisingly log-linear learning. log-linear cial sciences tion contingency tests bayesian laskey martignon contingency resentations jog-linear ture learning theoretical approach presented placed context naive-bayes model comparative traditional oy\"l<; instantiation des�ribes prior knowledge {oy\"l<;} represent distribution complete parameters. complete represent parameterization andy. independencies known representation equally rich jog-linear logit model makes independencies inherent ini­ sake simplicity parameterization. binary effect values tially consider binary causes logit model p+ct+dt gument false conjunction evident model flecting propensity term zeroth-order true gives tendency true first-order lastly association joint effect seen control interaction. thus second-order number parents requires rameters logit model several warranted ture setting number parameters eter estimation smaller samples. models parameters removed models parameter involving second desirable property realistic allows incorporate causal interaction model generally conditional i.e. given parent states dependent state single parent effect altering priori assumed completely effect identically parent occur commoner pattern least direction effect change causal factor dependent values causal factors. discrete tation effects factor commonly fairly consistent mildly affected causal factors. tations causal interaction prior distribution lower order. expectation model full conditional causal discovery teraction structure statistical tests bayesian unsaturated space possible unsaturated putationally since intend search simultaneously ture. consider conditional second order negligible. nary model equivalent given joint distribution shown coincide distribution known interactions exhibit order model full conditional parent accurately pressible generalizing replace assume notation arity respectively. first­ order probability ak+hw+ck. corresponding common constrain parameters several constraints parameters variable. bkz} free parameters <>'. straightforward larger parent sets transformation markably similar cant drawback performing necessity transformed pendent another intended predicting ditional bution explicitly requisite however causal independence application naive model logit parameterization conditional distribution restrictions naive-bayes model rather assume maximum tropy distribution vxexp known. esti­ mating assuming knowledge rather extracting distribution inferred ie>' number free parameters nktr number cases nk\". message length calculation describe fom. wallace freeman derive expres­ sion evaluating broad regularity imates message length optimally parameter parameters bayesian analysis assesses belief evaluating task requires models suffices calculate likelihood given full bayesian possible trast wallace's known priori. eterizations assuming i.i.d data pendent parameters prior cooper herskovits full conditional eral result message exceeds length non-estimating nits posterior parameter full message length transmitting parameter pected fisher information constant function parameter parameter would prefer conjugate ables arbitrary arity symmetric consider parameters variance distributions suitably number offree parameters difficult fisher informa­ derive closed form tion possible minimize define directly offer alterna­ maximum likelihood tive estimates data counts ny\"'' turn zero rameter estimates magnitude. could find parameter gate gradient parameter parameter values calculate ton's method using decomposition message length network structure number linear extensions prior belief presence number variables number arcs network. model selection equation allows compare traditional networksterm first-order net­ works substituting relevant local stochastic approach similar wallace korb causal models real-valued metropolis sampling posterior term mml-sampling. algorithm incorporates equivalence coming). alence class first consider significant. moval results term network insignificant clean model. cleaning equivalence clean model posterior unclean variants justified sion model account small portion posterior possible substantial. substantial group performs outstandingly. single model particular node cleaning parent list sequence insignificant insignificant parents left last parent tested. simple process biased ways first examine subsets second subsets examine depend upon order however proved effective identifying accurately statistically equivalence conditional equivalence evidence suggests assume does. summary mml-sampling networks ensuring probability estimated mml-equivalence best classes message length. three databases results selecting ability results reported several databases warrant discussion. hierar­ nursery database synthetic model ranking applications chical decision nurs­ schools. complete database instance correct favor nits demonstrate interactions model. every higher-order methods identified sarde network structure direct parents class attribute tributes arcs. network prohibitively expen­ sive describe table probabilities ined sampling. message length even though infer structure. recalling model node uses full conditional parent difference caused coding non-class parents. model takes nits using first-order variables encoded full condi­ tional model despite fact models equivalent power variables. difference part parameter priors assumed model part estimates first-order estimates. traditional error must kept mind methods. effect also alleviated first-order model conjugate popularity database surveys factors attributes four attributes represent ranking grades sports looks effect popularity. four variables accurately ability interactions mous difference message lengths. outperformed tified non-monotonic emphasizing compare traditional probability selected repositories inferred performance cross-validation technique dataset training port results sets database. equivalence first-order training gorithm. posterior reported inferred training keep complexity tical limit inferred parents node limit full contain less parameters. table compares posterior negative three types network. sults indicate ability data give indication message length results model improving models general. inferred fons cant non-monotonic interactions able partially model non-monotonic including siblings variable's parent set. nodes share siblings variable additional direct parents. introduces paths parents variable allow extended interaction mechanism. trend table partly account namely infer arcs tbn. trend however nursery data gives counter example sample better modeled single large parent cannot identify. light discussion sider relevance networks variables edged sufficient investigated logit model causal interac­ tion derived metric restricted model exhibiting causal independence. metropolis sampling approach networks tion node network incorporating stricted first-order model select local model node-by-node sis. tests datasets taken machine learning repositories first-order mated posterior four cases inferred likely work demonstrated appropriate competitive often significantly scores unseen test data. results benefits increased expressive power offered dual model restricted first­ order network traditional bayesian net­ work cases dual network perform quite ences found significant. arcs indicated average number inferred arcs tendency identify perhaps attempt overcome limitations local distribution. also tracked dual network difference parameter prior estimation sion approach less restrictive first-order", "year": 2013}