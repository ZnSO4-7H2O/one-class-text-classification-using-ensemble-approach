{"title": "Gaussian Process Neurons Learn Stochastic Activation Functions", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "We propose stochastic, non-parametric activation functions that are fully learnable and individual to each neuron. Complexity and the risk of overfitting are controlled by placing a Gaussian process prior over these functions. The result is the Gaussian process neuron, a probabilistic unit that can be used as the basic building block for probabilistic graphical models that resemble the structure of neural networks. The proposed model can intrinsically handle uncertainties in its inputs and self-estimate the confidence of its predictions. Using variational Bayesian inference and the central limit theorem, a fully deterministic loss function is derived, allowing it to be trained as efficiently as a conventional neural network using mini-batch gradient descent. The posterior distribution of activation functions is inferred from the training data alongside the weights of the network.  The proposed model favorably compares to deep Gaussian processes, both in model complexity and efficiency of inference. It can be directly applied to recurrent or convolutional network structures, allowing its use in audio and image processing tasks.  As an preliminary empirical evaluation we present experiments on regression and classification tasks, in which our model achieves performance comparable to or better than a Dropout regularized neural network with a fixed activation function. Experiments are ongoing and results will be added as they become available.", "text": "propose stochastic non-parametric activation functions fully learnable individual neuron. complexity risk overﬁtting controlled placing gaussian process prior functions. result gaussian process neuron probabilistic unit used basic building block probabilistic graphical models resemble structure neural networks. proposed model intrinsically handle uncertainties inputs self-estimate conﬁdence predictions. using variational bayesian inference central limit theorem fully deterministic loss function derived allowing trained efﬁciently conventional neural network using mini-batch gradient descent. posterior distribution activation functions inferred training data alongside weights network. proposed model favorably compares deep gaussian processes model complexity efﬁciency inference. directly applied recurrent convolutional network structures allowing audio image processing tasks. preliminary empirical evaluation present experiments regression classiﬁcation tasks model achieves performance comparable better dropout regularized neural network ﬁxed activation function. experiments ongoing results added become available. introduce neural activation function learned completely training data alongside weights neural network. number constraints form learnable functions kept possible allow highest amount ﬂexibility. however want neural network trainable using stochastic gradient descent thus require activation function least continuously differentiable. naturally increasing ﬂexibility model risk overﬁtting also increased. hence keep risk check apply prior space activation functions. choose gaussian process zero mean squared exponential covariance function prior since encourages smooth functions small magnitude. probabilistic treatment transforms neuron probabilistic unit call gaussian process neuron consequently neural network built gpns becomes probabilistic graphical model intrinsically handle training test data afﬂicted uncertainties estimate conﬁdence predictions. since non-parametric model introduces dependencies samples model. consequently even learning network weights predictions test input directly depend training samples inference performed using monte carlo methods. since impractical introduce parametric auxiliary model applies methods sparse regression represent activation function using trainable parameters. resulting model approximation trained using stochastic gradient descent sampling gradient. combine ideas fast dropout sparse variational make nonparametric network trainable using variational bayesian inference. result fully deterministic loss function eliminates need sample gradient thus makes training efﬁcient. although model treated fully probabilistically loss function retains functional structure neural network making directly applicable network architectures recurrent neural networks convolutional neural networks proposed model shares commonalities deep gaussian processes economical terms model parameters considerably efﬁcient train discussed section overview family introduced models relationships shown paper. since dawn neural network research commonly used activation functions logistic function sigmoid functions. sigmoid refers s-shaped functions example hyperbolic tangent choice activation functions seriously challenged researchers recently introduced rectiﬁed linear unit neuron activation function linear positive inputs zero negative inputs. showed relus produce signiﬁcantly better results image recognition tasks using deep networks common sigmoid-shaped activation functions. achievement wave follow-up research activation functions speciﬁcally tailored deep networks. relu solved problem vanishing gradients positive values completely gradient negative ones; thus neuron enters negative regime samples training signal pass mitigate problem introduced leaky relu also linear negative values small although non-zero slope; positive values behaves like relu. soon demonstrated advantageous make slope negative part leaky relu additional parameter neuron. parameter trained alongside weights biases neural network using gradient descent. using so-called parametric relus ﬁrst surpass human-level performance imagenet classiﬁcation task thus natural even ﬂexible activation functions beneﬁcial. question answered afﬁrmative cifar- cifar- benchmarks authors introduced piecewise linear activation functions arbitrary number points function changes slope. points associated slopes inferred training data stochastic gradient descent. instead ﬁxed parameter negative slope relu introduced stochasticity activation function sampling value slope training iteration ﬁxed uniform distribution. replaced negative part relus scaled exponential function showed that certain conditions leads automatic renormalization inputs following layer thereby simpliﬁes training neural networks. furthermore introduced parametric activation function smoothly interpolate operation neuron performs inputs addition multiplication thus allowing appropriate operation determined training. nearly fully adaptable activation functions proposed authors fourier basis expansion represent activation function; thus enough coefﬁcients activation function represented. coefﬁcients expansion trained network parameters using stochastic gradient descent. similarly also basis expansion gaussian kernels equally distributed preset input range. work need arises slice tensors along dimension. star used place index select indices dimension. provide examples. given matrix rn×m notation denotes i-th similarly denotes j-th column also extended tensors star used multiple times. example consider tensor rn×n×n×n. rn×n denotes matrix obtained ﬁxing ﬁrst dimension third dimension gaussian process describes distribution scalarvalued functions multivariate inputs consider matrix rn×d input values dimensions. ﬁnite number function values joint multivariate normal distribution deﬁned mean function covariance function vector function values given function follows used perform regression conditioning observed function values. consider function distributed according given ﬁnite observation points corresponding observed values conditional distribution predicted function identity matrix. follows directly equations mean covariance conditional multivariate normal distribution since ﬁnite number points must consistent introduce gaussian process neurons context feed-forward neural networks i.e. assume gpns arranged layers outputs layer inputs next layer. probabilistic unit receives multiple inputs computes output distribution conditioned values inputs. given multiple input samples output samples become correlated i.e. output distribution independent identically distributed samples. probabilistic graphical model corresponding single within layer three inputs three samples shown figure within layer three inputs shown three samples probabilistic graphical model. inputs outputs previous layer represented activations sample reprerandom variables depend deterministically inputs. sented random variables responses samples conditioned activations; ﬁgure represented markov random ﬁeld shown undirected connection samples. outputs represented random variables note structure deep however proposed covariance function differs automatic relevance determination covariance function used deep multiplying weights inputs taking square. show section difference results model considerably efﬁcient train. furthermore note lengthscale redundant magnitude weight vector now. however since useful parametric version model introduced later keep performing training inference non-parametric model computationally intensn denotes n-th layer corresponding training sample test samples represented random variables layer corresponds inputs values observed training test samples. top-most layer represents outputs. deﬁning distributions occurring model convenient concatenate training test samples analogous standard neural networks inputs gpns i.e. layers stacked form multi-layer feed-forward network. joint probability models hybrid parametric non-parametric models. contain parameters consisting weights variances lengthscales layer. however predictions test samples also depend directly training samples since forms markov random ﬁeld samples. occur conventional artiﬁcial neural networks predictions samples independent identically distributed. require latent variables marginalized out. unfortunately occurrence covariance matrix analytic integration intractable. inverse covariance matrix appears probability density function normal distribution thus dependency highly non-linear analytic calculation integral feasible. possible estimate likelihood derivatives sampling model using hamiltonian monte carlo sampling. using estimate gradient w.r.t. parameters likelihood maximized iteratively using stochastic gradient ascent. predictions test set. however approach attractive gpns used place conventional neurons since sampling considerably expensive standard backpropagation algorithm non-parametric nature requires consult whole training prediction thus limiting scalability model datasets arbitrary size. issues described section arise inter-sample dependencies nonparametric model. consequently desirable break however ensured activation function still consistent across samples. make scalable large datasets proposed sparse approximation training data base predicitons solely here method make parametric thus output independent training data. later show approach extended make analytic marginalization latent variables tractable. three parameter vectors introduced gpn. purpose explicitly parameterize points activation function representing mode distribution. virtual observation point parameterization consists inducing point corresponding activation observation target corresponding response given activation variance around response. thus assume making observations activation function given observations form stack virtual observations thus indices layer within layer virtual observation variables. consequently obtain tensors r-th inducing point target variance respectively layer training points regression performing marginalizaactivation function evaluated test points tions activations responses done calculate obtain distribution outputs given inputs note inducing points affected weights always dimensional matter dimensionality inputs parameters parametric layer include virtual observations besides weights thus parameters layer need estimated training. overview notation used provided table since model fully parametric training information training samples stored parameters model hence necessary keep training samples prediction. number parameters depends number gpns many virtual observations used independent number training samples. allows parametric networks scale datasets arbitrary size like conventional neural networks prediction model training sample corresponding target. loss measure assigns loss value sample based task-dependent difference model’s prediction ground truth. since dealing probabilistic model provides predictive distribution training performed considering expectation loss i.e. objective minimize note information conveyed sample sample solely model pasn rameters since marginal multivariate normal respect furthermore quantities depend sample depends layers thus objective becomes figure principle normal approximation feed-forward network. props activation functions results aragating distribution activations bitrary distributions however given enough gpns layer responses weakly correlated thus distribution activations resemble normal distribution central limit theorem. random variables omitted graph. straightpossible approximate expectation sampling normal distributions sampled sequentially layer thus maximum likelihood estimate model parameters obtained performing stochastic gradient descent using sampled gradients applying reparameterization trick stochastic training proven parameter optima generalize better unseen data samples also slows training considerably leads noisy estimates gradient which turn require learning rate kept least magnitude lower compared noise-free algorithms prevent trajectory parameter space becoming unstable. increase speed training make comparable conventional neural network therefore demonstrate analytically calculate propagating distributions parametric network following layer applying weights dashed blue line best-ﬁt normal distribution data. weights initialized randomly iid. distribution clear condition holds training performed thus converges normal distribution. however possible predict weight distribution develops training. thus necessary turn empirical analysis behavior. performed analysis activations conventional neural network using sigmoidal activation function. showed empirically activation neuron remains normally distributed even network trained convergence provided number incoming connections large enough. normal distribution perform following experiment. instantiate three-layer feed-forward network different numbers gpns layer weights sampled standard normal distribution activation function randomly sampled zero-mean covariance function unit lengthscale. random input vector drawn parameters distribution propagated network calculated. samples drawn reaching resulting empirical cdfs networks shown together best-ﬁt normal cdfs. small network gpns layers apparent activations normally distributed. number gpns increases empirical distribution approaches normal distribution. gpns layers enough distribution resemble gaussian closely. whereµx andσx mean covariance necessarily normally distributed. calculate given σal. note imply normally distributed symbolsµx andσx denote mean istically chosen points represent normal distribution function using transformed points estimate mean covariance transformed distribution. applying unscented transform loss becomes cholesky decomposition scaled covariance matrix. cholesky decomposition differentiable thus derivative loss calculated assumed diagonal cholesky decomposition given square root diagonal elements thus inexpensive compute. conclusion derived completely deterministic training objectives feedforward parametric network using analytic propagation means covariances layer layer resulting loss functions minimized using mini-batch gradient descent derivatives computed using automatic differentiation. minimizing derived losses predicted variance model predictions taken account. prediction ground truth penalized stronger stack simultaneously predicts variance i.e. high conﬁdence time. consequently training model learns predict targets also self-estimate conﬁdence predictions. represent activation functions parametric layer requires parameters number virtual observations. gpns require bias term equivalent offset targets corresponding inducing points. reduce number parameters locations inducing points ﬁxed example using equidistantly figure common activation functions approximated parametric virtual observations respectively. dotted line shows actual activation functions line approximation parametric gpn. standard deviation shown using gray shading. placed points parameters required. furthermore standard deviations targets shared inducing points resulting parameters gpn. finally reduce number required parameters apply weight sharing idea cnns common virtual observations thus activation functions within group gpns even within whole layer. since ﬂexibility parametric controlled care must taken choose small since would limit activation functions representable thus power feed-forward network constructed gpns. good heuristic choosing able represent common activation functions currently use. purpose empirically modeled hyperbolic tangent rectiﬁer activation functions using varying number inducing points compared resulting approximation original function. virtual observations equidistant inducing points enough represent activation functions high accuracy. computational complexities propagating mean covariance layer layer shown table complexity calculating responses signiﬁcantly reduced keeping inducing points variances ﬁxed case tensor given ﬁxed precomputed. another method save computational complexity propagate variances i.e. diagonal covariance matrix stack. reduces complexity computing activations complexity computing responses note number parameters computational complexity propagating means covariances depend number virtual observations parametric gpns; therefore memory requirement independent number training samples required training time epoch scales linearly number training samples. thus like conventional neural network parametric feed-forward inherently trained datasets unlimited size. introducing learnable activation functions neural network increases expressive power thus associated risk overﬁtting maximum likelihood training performed. thus sensible turn bayesian inference instead gives posterior distribution model parameters instead point estimate. bayesian inference requires prior distribution parameters. original concept non-parametric prior activation function unit; hence natural choice prior virtual observations parametric prior activation function restored virtual observations marginalized out. indeed possible ﬁrst prior-restoring technique ﬁnding inducing points variational sparse regression graphical model corresponding distribution training targets observed shown exact inference intractable variational inference compute approximate bayesian posterior model parameters. since information activation functions learned training data mediated virtual observation targets variational posterior must adaptable order store information. hence choose normal distribution factorized units within layer free mean covariance variational posterior allows inducing targets correlated covariance matrix constrained diagonal desired reduce number model parameters. keep rest model distribution unchanged prior. thus overall variational figure feed-forward network distribution three layers variational approximations posterior. node corresponds samples units within layer. variational parameters shown dotted nodes. exact posterior distribution results non-parametric feed-forward network marginalized {u}l variational approximation inducing targets using central limit distribution marginals latent activations variational mean-ﬁeld approximation performed deep factorizes variables graphical model corresponding variational posterior shown note differs variational approximation employed deep uses mean-ﬁeld approach factorizes variables shown term lreg identiﬁed kl-divergences virtual observation targets prior variational posterior. purpose keep approximative posterior close prior thus understood regularization term. disregarding additive constant value given concludes calculation terms variational lower bound resulting variational objective fully deterministic function parameters. training model performed maximizing −lreg lpred lreg given lpred performed using gradient-descent based algorithm mini-batch training routine. assumed regression problem classiﬁcation task apply unscented transform described section evaluate resulting lpred term. deep gaussian processes framework hierarchical composition functions. similar model outputs used input another one; thus graphical models resembling structure feed-forward neural network formed. deep also employ variational sparse method using inducing points developed make inference tractable. however show gpns number advantages deep model complexity efﬁciency inference. within deep framework takes multidimensional input i.e. input connection adds input dimension connects connections deep weights compute weighted done model. instead uses covariance function individual lengthscale parameter input dimension. interpreting lengthscale covariance function inverse weight write covariance function deep thus taking square summation distinguishes gpns deep essence. although ﬁrst glance seems rather small difference leads series consequences clearly distinguishes models. ﬁrst consequence deep framework works multidimensional function space. dimensionality determined number input connections thus feed-forward model equals number units previous layer. hence inducing points virtual observations used efﬁcient inference also multidimensional. implies number virtual observations required evenly cover input space scales exponentially number input dimensions thus incoming connections. figure shows predictive mean two-dimensional covariance function four observations. moves away observations predictive mean returns zero. hand like every artiﬁcial neuron computes projection inputs onto weight vector resulting scalar value. thus matter many input connections present always works one-dimensional function space. hence inducing points virtual observations also one-dimensional number virtual observations unaffected number incoming connections. figure shows predictive mean using projection two-dimensional input space four observations. thus inducing points become inducing lines inducing hyperplanes dimensions concerned. might argued expressive power vastly reduced using projection however case feed-forward network following argument demonstrates. figure also shows predictive mean using projection two-dimensional input space different weights. assume outputs gpns located layer. sake argument assume particular layer consists gpns. activations subsequent layer formed linear combination output gpns. resulting activation shown seen produces functions varying input dimensions. here number virtual observations required evenly cover input space scales linearly number input dimensions. furthermore virtual observations interpreted grid input space thus making unlikely input point located away inducing hyperplanes. second consequence inputs deep model cannot converge normal distribution linear combination performed. leaves methods training inference deep stochastic variational inference computationally expensive sampling using mean-ﬁeld variational posterior work well practice. furthermore mean-ﬁeld outputs within deep must inferred alongside model parameters minimization variational objective function leading many parameters optimize. instance observed deep difﬁcult train reasons reverted stochastic inference algorithm avoid problem albeit signiﬁcantly higher computational costs. hand central limit theorem applicable activation guaranteeing activations converge normal distribution sufﬁcient number input connection. leads marginally normal variational posterior derived section variational objective function resembles structure neural network makes gpns directly usable network architectures rnns cnns simply adjusting accordingly. taken together consequences show design leads sound efﬁcient training procedure fewer parameters optimize compared deep advances possible using weight projection inside standard covariance function instead covariance function. crucial difference enabled derive efﬁcient variational objective. figure covariance function occurs deep versus gpns using projections two-dimensional space. predictive mean twodimensional space using covariance function four observations placed twodimensional space. predictive mean using projection two-dimensional inputs four one-dimensional observations represented lines input space. shows using different projection. linear combination predictive means occurs activation receiving inputs gpns shown taken together observations form grid two-dimensional space. section describes experiments performed estimate performance feedforward networks. datasets models evaluated introduced performance compared conventional feed-forward neural networks. experiments performed using maximum likelihood inference using parametric model. later experimental results variational bayesian training model. furthermore section discusses empirical computational memory requirements. evaluate well parametric model performs real-world classiﬁcation problems test three datasets machine learning repository well mnist database handwritten digits hereby beat current state performance datasets since literature shows trainable activation function mostly beneﬁcial large convolutional models image classiﬁcation. instead focus verifying implementation efﬁciency trade-offs parametric feed-forward models different kinds classiﬁcation datasets compared conventional neural networks ﬁxed sigmoidal hyperbolic tangent activation function regularized dropout technique datasets primarily chosen differ size also cover different kinds features targets. consequently successful training selection datasets shows gpns applicable variety tasks worthwhile implement convolutional architectures based gpns tackle current image classiﬁcation problems large datasets cifar- imagenet shortly describe properties dataset continuing training procedures. letter recognition dataset ﬁrst used consists samples continuous input features sample. features calculated pixel images capital characters english alphabet image showing single letter different fonts. furthermore character images randomly distorted increase variation dataset. precomputed features consisting statistical moments edge counts used input classiﬁer. objective identify character. adult dataset introduced consists continuous categorical features containing census data taken u.s. citizens collected year continuous features consists amongst others weight work hours week years educations. categorical features include information highest obtained degree martial status race country origin. binary target objective predict whether person’s income exceeded not. dataset contains missing features samples replaced additional unknown category categorical features zero continuous features. contrast previous datasets connect- dataset consists categorical features. features represents state ﬁeld board connect- game board size categories encode whether position currently occupied player player free. dataset contains legal positions neither player next move forced; total dataset contains samples. data used classify game result player plays optimally three classes loss draw. mnist database handwritten digits commonly used machine learning datasets image classiﬁcation. consists training test examples. split training examples training validation set. input consists pixel image handwritten digit classiﬁed. task similar classiﬁcation letter-recognition dataset main difference being instead using precomputed image features model receives grayscale image data input. mnist dataset widely used evaluate performance neural network based classiﬁers thus natural choice evaluating trainable activation functions neural architecture. continuous input features datasets rescaled shifted interval categorical features one-hot encoding scheme used. encodes categorical feature vector many entries categories entry active category entries zero. split training test kept provided datasets; furthermore original training randomly split section perform experiments using parametric central limit activations developed section goal evaluating feasibility using gpns drop-in replacement conventional artiﬁcial neurons. stated theoretical analysis computational complexity table propagating mean variance full covariance matrix layer layer comes different computational memory requirements. analyze trade-offs runtime prediction accuracy different approximations train parametric gpns propagating mean propagating mean diagonal covariance matrix propagating assumed zero eqs. evaluated layer; thereby probabilistic nature model eliminated gpns become conventional neurons activation function given interpolating inducing points targets. computations done setting off-diagonal elements layer covariance matrices zero using eqs. full model layer layer employing eqs. wall clock time preparatory experiments became apparent virtual observation variances driven zero leading overﬁtting. problems common maximum likelihood inference thus penalty form virtual observations parametric model shared different gpns resulting gpns activation function. obviously reduces number model parameters furthermore computational complexity. assess impact sharing model accuracy train variants feed-forward networks implementation performed custom framework generates cuda kernels complex expressions like eqs. contrary established frameworks like theano tensorflow expression derivatives evaluated fully inside single cuda kernel proved essential obtain acceptable performance otherwise hampered occurrence many small tensors corresponding virtual observations computational graph. derivatives expressions calculated using method described remained mostly preparatory experiments showed inducing points unchanged training; hence inducing points initialized using linear spacing interval kept ﬁxed training. corresponding targets either initialized standard normal distribution equal resulting identity function. also tried initializing targets values well-known activation function hyperbolic tangent rectiﬁer found signiﬁcant beneﬁt. virtual optimized observation variances training alongside targets. bengio training deep neural network using hyperbolic tangent activation function. motivation behind choosing described ensure beginning training activations neurons start linear range hyperbolic tangent function. although using function desirable activations gpns fall within range inducing points; thus weight initialization method applicable here. training performed minimizing expected loss calculated unscented transform softmax cross-entropy loss using adam optimizer initial learning rate decreased factor time validation loss stops improving predeﬁned number training iterations. learning rate reaches improvement seen validation training terminated model parameters best iteration seen validation used calculated reported classiﬁcation accuracies. experiment repeated times different random seeds initialization. comparison also train conventional neural network architecture ﬁxed hyperbolic tangent activation function regularized using fast dropout method results preliminary hyperparameter tuning within architecture. exemplary loss curve training rate schedule parametric feed-forward network shown example initial learning rate automatically decreased iterations iterations training terminated iterations. like conventional feed-forward neural network losses decrease smoothly fully deterministic loss. accuracies experiments reported table resource usage different propagation approaches shown table expected conventional neural network ﬁxed activation function fastest; however relative using four times slower propagating means variances. includes times forward propagation calculating gradient w.r.t. weights virtual observations using back propagation. propagation mean variance leads signiﬁcantly better results propagation mean alone datasets. however propagation full covariance matrix times computationally expensive propagation mean variance show signiﬁcant beneﬁts accuracy model preparatory experiments. sharing virtual observations gpns within layer provide beneﬁts test accuracy experiments thus suggesting ﬂexibility separate activation function beneﬁcial model increase number parameters lead overﬁtting. adult dataset gpns proﬁted initializing virtual observations initial activation function identity function; however letter recognition dataset yield signiﬁcant improvement. figure shows examples activation functions commonly encountered feed-forward network training connect- dataset. activation functions ﬁrst layer vary much stronger upper layers. commonly functions ﬁrst layer resemble sine-like functions approximately axis-symmetric w.r.t. y-axis. move second third layer sigmoid-shaped linear functions become common. might indicate ﬁrst layer exploits periodicity input data layers feature-detectors gating inputs. validationlosswereused. experimentsandwithaconﬁdenceintervalof%.forevaluationthemodelparametersatthetrainingiterationwiththelowest asthenumberofmisclassiﬁedsamplesdividedbythenumberoftotalsamples.theerrorratesaregivenastheaverageofﬁve tablemisclassiﬁcationratesofdifferentparametricgpnmodelvariantsonthebenchmarkdatasets.theerroriscalculated ﬁxedtanh gpnmean+variance gpnmean+variance ﬁxedtanh ﬁxedtanh gpnmean+variance gpnmean+variance ﬁxedtanh ﬁxedtanh gpnmean+variance gpnmean+variance gpnmean+variance gpnmeanonly ﬁxedtanh ﬁxedtanh table memory usage time performing iteration forwardbackpropagation layer neurons gpns using different propagation methods. values used relative comparisons within table since irrelevant operations data reading included memory usage iterations time. memory usage includes memory used storing intermediate results calculation derivatives using backpropagation. figure training validation test losses training parametric feed-forward network mean variance propagation connect- dataset. lower panel shows scheduling learning rate. progression loss smooth stable fully deterministic objective. figure three activation function layer parametric feed-forward network trained connect- dataset. virtual observations shown dots together standard deviation. results presented show parametric gpns consistently better performance conventional neural network using ﬁxed activation function real-world datasets small size. execution speed network conventional ﬁxed activation function. propagating mean variances network signiﬁcantly improves quality predictions generalization ability. propagating full covariance matrices yields marginal improvements cannot justiﬁed vastly increased usage computational time memory. beneﬁt activation functions initialized dataset-dependend sharing virtual observations yield improvements. conventional neural network regularized using fast dropout method gpns datasets conventional neural networks perform better others. strongly suspect generalization capabilities gpns much outperform deep neural networks with e.g. relu functions stronger nonlinearities neurons. future experiments conducted investigate assumption esp. related cnns. note experiments performed small datasets using maximum likelihood inference bayesian inference described section future experiments concentrate testing gpns cnns using large datasets historically novel learnable activation functions shown signiﬁcantly improve models. proposed place prior activation function neuron. three consequences. first neuron using activation function becomes probabilistic unit allowing handle uncertain inputs estimate conﬁdence output. second complexity activation function penalized probabilistically sound bayesian setting; guards model overﬁtting. third squared exponential covariance function ensures activation functions smooth therefore continuous derivatives available. resulted non-parametric model shows theoretically attractive properties performing inference expensive non-parametric nature. overview course action took make gpns tractable shown starting non-parametric model derived variational approximation posterior. based methods proposed sparse regression introduced auxiliary model parametric provides inexpensive inference also less attractive since inference performed maximizing likelihood. showed possible recover non-parametric model placing appropriate prior parameters parametric gpn. furthermore showed distribution activations randomly initialized fully trained neural networks closely resembles normal distribution central limit theorem. taken together steps allowed derive fully deterministic variational objective train non-parametric stochastic gradient descent. objective functional structure conventional neural network thus gpns directly included cnns rnns architecture uses neurons. shown that although networks similar deep need signiﬁcantly fewer parameters training vastly efﬁcient. result using projections inside covariance function instead dimension-dependent lengthscales. preliminary experimental results small datasets using parametric gpns trained maximizing parameter likelihood show model performs consistently better conventional neural network ﬁxed activation function. experiments larger datasets using approximate bayesian inference ongoing results presented become available. summary neural network viewpoint introduced novel stochastic learnable self-regularizing activation function integratable existing neural models modest effort. viewpoint introduced idea learnable projections deep gaussian processes allowing derive novel variational posterior makes accessible easy train neural networks.", "year": 2017}