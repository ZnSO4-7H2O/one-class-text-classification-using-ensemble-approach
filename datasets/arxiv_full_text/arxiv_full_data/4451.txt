{"title": "Folded Recurrent Neural Networks for Future Video Prediction", "tag": ["cs.CV", "stat.ML"], "abstract": "Future video prediction is an ill-posed Computer Vision problem that recently received much attention. Its main challenges are the high variability in video content, the propagation of errors through time, and the non-specificity of the future frames: given a sequence of past frames there is a continuous distribution of possible futures. This work introduces bijective Gated Recurrent Units, a double mapping between the input and output of a GRU layer. This allows for recurrent auto-encoders with state sharing between encoder and decoder, stratifying the sequence representation and helping to prevent capacity problems. We show how with this topology only the encoder or decoder needs to be applied for input encoding and prediction, respectively. This reduces the computational cost and avoids re-encoding the predictions when generating a sequence of frames, mitigating the propagation of errors. Furthermore, it is possible to remove layers from an already trained model, giving an insight to the role performed by each layer and making the model more explainable. We evaluate our approach on three video datasets, outperforming state of the art prediction results on MMNIST and UCF101, and obtaining competitive results on KTH with 2 and 3 times less memory usage and computational cost than the best scored approach.", "text": "abstract. main challenges future video prediction high variability videos temporal propagation errors non-speciﬁcity future frames. work introduces bijective gated recurrent units standard grus update state exposed output given input. extend considering input another recurrent state update given output using extra logic gates. stacking multiple layers results recurrent auto-encoder operators updating outputs comprise encoder ones updating inputs form decoder. encoder decoder states shared representation stratiﬁed learning information passed next layers. show encoder decoder needs applied encoding prediction. reduces computational cost avoids re-encoding predictions generating multiple frames mitigating error propagation. furthermore possible remove layers trained model giving insight role layer. approach improves state results mmnist competitive times less memory usage computational cost best scored approach. ossible leverage large volumes unlabelled data video-related tasks action gesture recognition task planning weather prediction optical estimation view synthesis main problems task need expensive models terms memory computational power order capture variability present video data. another problem propagation errors recurrent models tied inherent uncertainty video prediction given series previous frames multiple feasible futures. this left unchecked results blurry prediction averaging space possible futures propagates back network predicting subsequent frames. work propose approach recurrent auto-encoders state sharing encoder decoder. show exposed state gated recurrent units used create bijective mapping input output layer. input treated recurrent state adding another logic gates update based output. creating stack layers allows bidirectional information. using forward gates encode inputs backward ones generate predictions obtain structure similar many inherent advantages. reduces memory computational costs training testing encoder decoder executed input encoding prediction respectively. furthermore representation stratiﬁed encoding part input layer level information necessary capture higher level dynamics passed next layer. also naturally provides noisy identity mapping input facilitating initial stages training input ﬁrst bgru holds last encoded frame preceded convolutional layers over-complete representation same. generation ﬁrst untrained bgru randomly modiﬁes last input introducing noise signal. approach also mitigates propagation errors solve problem blur prevents magniﬁcation subsequent predictions. moreover trained network deconstructed order analyse role layer ﬁnal predictions making model explainable. since encoder decoder states shared architecture thought recurrent folded half encoder decoder layers overlapping. call method folded recurrent neural network main contributions shared-state recurrent lower memory computational costs. mitigation error propagation time. naturally provides identity function training. model explainability optimisation layer removal. demonstration representation stratiﬁcation. building blocks. characteristics problem setting widely used encoder extracts valuable information input decoder produces frames. generally encoder decoder cnns tackle spatial dimension. lstms commonly used handle temporal dynamics project representations future. works compute temporal dynamics deep representation bridging encoder decoder others jointly handle space time using convolutional lstms convolutional kernels gates. instance lotter recurrent residual network convolutional lstm layer minimises discrepancies previous block predictions. common variations also include conditional term guide temporal transform time diﬀerential prior knowledge scene events reducing space possible futures. predict future frames atari games conditioning action taken player. works propose action conditioned models foreseeing application autonomous agents learning unsupervised fashion finn predict sequence future frames within physical system based previous frames actions taken robotic interacting scene. method recently applied task planning adapted perform stochastic future frame prediction bridge connections. introducing bridge connections also common allows stratiﬁed representation input sequence reducing capacity needs subsequent layers. video ladder networks convolutional topology implementing skip connections. pairs convolutions grouped residual blocks horizontally passing information corresponding blocks directly using recurrent bridge layer. topology further extended recurrent ladder networks recurrent bridge connections removed residual blocks replaced recurrent layers. propose alternative bridge connections completely sharing state encoder decoder reducing computational needs maintaining stratiﬁcation ability. share similarities approach propose recurrent bridge connections encoder decoder. however using skip connections instead state sharing disadvantages higher number parameters memory requirements impossibility skip encoding/decoding steps reduced explainability allowing layers removed training. finally bridge connections provide initial identity function training. makes hard model converge cases background homogeneous model learn proper initial mapping input output weights zero adjust bias last layer eliminating gradient process. prediction atom. proposed architectures future frame generation directly predict pixel level however models designed predict motion transform input future frames. instance using input sequence anticipate optical convolutional kernels methods propose mapping input sequence onto predeﬁned feature spaces aﬃne transforms human pose vectors systems sequences features instead working directly pixel level. then predicted feature vectors generate next frames. loss gans. commonly used loss functions tend average space possible futures. reason works propose using generative adversarial networks help traditional losses choose among possible futures ensuring realistic looking frames coherent sequences. mathieu plain multi-scale adversarial setting propose gradient diﬀerence loss sharpen predictions. disentangled motion/content. authors encode content motion separately. villegas architecture two-stream encoder motion lstm encodes diﬀerence images; appearance plain encodes last input frame. similar fashion denton separate encoders adversarial setting obtain disentangled representation content motion. alternatively works predict motion content parallel beneﬁt combined strengths tasks. sedaghat propose using encoding dual objective liang dual setting combine predicted frame motion generate actual next frame. feedback predictions. finally important aspect recurrent-based models based feedback predictions. general model trained predict speciﬁc number time-steps future. order predict time need predictions input. this handled properly accentuate small mistakes causing predictions quickly deteriorate time. model solves enabling encoder decoder executed number times independently. similar proposal srivastava uses recurrent approach input sequence encoded state copied decoder. decoder applied generate given number frames. however limited single recurrent layer part. propose architecture based recurrent convolutional deal network capacity error propagation problems future video prediction. consists series bijective layers allow bidirectional information input output consider input recurrent state update using extra gates. stacked forming encoder decoder using respectively forward backward fig. left scheme bgru. shadowed areas illustrate multiple bgru layers stacked. right frnn topology. recurrent states encoder decoder shared resulting bidirectional mapping states. shadowed areas represent unnecessary circuitry re-encoding predictions avoided thanks decoder updating states. left-right blue correspond forward backward gates respectively. rectangles represent recurrent state cell. functions bijective grus call folded recurrent neural network state sharing encoder decoder topology allows stratiﬁcation encoded information lower memory computational requirements compared regular recurrent mitigated propagation errors increased explainability layer removal. grus state fully exposed output. allows deﬁne bidirectional mapping input output replicating logic gates layer. consider input state itself. lets deﬁne output layer time step given input state previous time step second weights used deﬁne inverse mapping using output forward function current time step update input treated hidden state inverse function. illustrated fig. refer double mapping bijective etwork order generate following predictions. states shared decoder already updates states except bridge state encoder decoder. bridge state updated applying last layer encoder generating next prediction. shadowed area fig. shows section computational graph required performing multiple sequential predictions. reason considering multiple sequential elements prediction encoder required. network updates states higher level representations lowest ones prediction errors introduced given layer generation propagated back deeper layers leaving higher-level dynamics unaﬀected. model implicitly provides noisy identity model training shown fig. bgru layers removed. input state ﬁrst bgru layer either input image ﬁrst applying series convolutional layers over-complete representation input. noise signal introduced representation backward function untrained ﬁrst bgru layer. consequently providing model initial identity model. show section helps model converge datasets like mmnist background shared across instances prevents model killing gradients adjusting biases match background setting weights zero. approach shares similarities rln. them part information passed directly corresponding layers encoder decoder encode full representation input deepest layer. however model implicitly passes information shared recurrent states making bridge connections unnecessary. compared equivalent recurrent bridge connections results much lower computational memory cost. speciﬁcally number weights pair forward backward functions equal case bgru corresponds state size layer using bridge connections case value incremented corresponds increase number parameters state double size other size. furthermore encoder decoder must applied time step. thus memory usage doubled computational cost increased factor regular recurrent ground truth frame introduced time step applying encoder decoder. output used supervision point comparing next ground truth frame sequence. implies predictions single time step last ground truth prediction. here propose training approach frnns exploits ability topology skipping model encoder decoder given time step. first ground truth frames shown network passing encoder. decoder applied times producing predictions. results half memory requirements either encoder decoder applied step never both. advantage approach srivastava recurrently applying decoder without ground truth inputs encourages network learn video dynamics. also prevents network learning identity model i.e. copying last input output. here ﬁrst discuss data evaluation protocol methods. provide detailed quantitative qualitative evaluation. ﬁnish brief analysis stratiﬁcation sequence representation among bgru layers. displaying pairs digits moving around image. sequences generated randomly sampling pairs digits trajectories. contains ﬁxed test partition sequences. generated million extra samples training. consists videos seconds subjects performing actions diﬀerent settings. videos grayscale resolution pixels left right borders resizing. displays actions playing instruments weight lifting sports. challenging dataset considered high intra-class variability. contains training sequences test sequences. resolution methods tested using input frames generate following frames. common metrics video prediction analysis mean squared error peak signal-to-noise ratio structural dissimilarity psnr objective measurements reconstruction quality. dssim measure perceived quality. dssim gaussian table parameters topology used experiments. decoder applies topology reverse using nearest neighbours interpolation transposed convolutions revert pooling convolutional layers. every layers. topology details shown table deconvolution nearest neighbours interpolation invert convolutional pooling layers respectively. train loss. evaluation include stub baseline model predicting last input frame design second baseline evaluate advantages using state sharing. rladder topology frnn model uses bridge connections instead state sharing. note keep state size layers using bridge connections doubles memory size almost triples computational cost similar works using regular conv layers decoder. also compare srivastava mathieu former handles temporal dimension explicitly lstms latter treats spatial dimensions using cnn. next compare villegas which contrary proposal uses feedback predictions. finally compare lotter based residual error reduction. adapted train using frames input predicting next using topologies parameters deﬁned authors. ﬁrst fig. displays results mmnist dataset proposed method baselines state alternatives. mean scores shown table frnn performs best time steps metrics followed srivastava methods provide valid predictions dataset. methods predict black frame mathieu progressively blurring digits. caused loss gradient ﬁrst stages training. complex datasets methods start learning identity function reﬁning results. possible since many sequences frame remains unchanged. case mmnist background homogeneous much easier models weights output layer zero biases match background colour. cuts gradient prevents learning. srivastava auxiliary decoder reconstruct input frames forcing model learn identity function. this discussed section implicitly handled method giving initial solution improve fig. quantitative results considered datasets terms number time steps since last input frame. bottom mmnist ucf. left right psnr dssim. mmnist rladder pre-trained learn initial identity mapping allowing converge. preventing models learning black image. order verify eﬀect pre-trained rladder dataset. dataset completely diﬀerent dynamics initial step solve problem remains providing identity function. afterwards model ﬁne-tuned mmnist dataset. shown fig. results model converging accuracy comparable srivastava evaluation metrics. dataset table shows best approach rladder baseline followed frnn villegas similar results villegas slightly lower higher psnr frnn lower dssim. approaches obtain comparable average results error increases faster time case villegas mathieu obtains good scores psnr much worse dssim. dataset shown table frnn approach best performing metrics. looking third fig. villegas starts results similar frnn ﬁrst frame case mmnist predictions degrade faster proposed approach. methods display performance cases. lotter works well ﬁrst predicted frame case error rapidly increases following predictions. magniﬁcation artefacts introduced ﬁrst prediction making method unable predict multiple frames without supervision. case srivastava problem capacity uses fully connected lstm layers making number parameters explode quickly state cell size. severely limits representation capacity complex datasets ucf. overall considered methods frnn best performing mminst later complex datasets. achieved results simple topology apart proposed bgru layers conventional pooling loss. normalisation regularisation mechanisms specialised activation functions complex topologies image transform operators. case mmnist frnn shows ability good initial representation converges good predictions methods fail. case frnn overall accuracy comparable villegas stable time. surpassed proposed rladder baseline method equivalent frnn times memory computational requirements. section evaluate approach qualitatively samples three considered datasets. fig. shows last input frames mmnist sequences along next ground truth frames corresponding frnn predictions. predictions generated sequentially withshowing previous ground truth/prediction network using decoder. seen digits maintain sharpness across sequence predictions. also bounces edges image done correctly digits distort deform crossing. shows network internally encodes appearance digit making possible reconstruct sharing region image plane. qualitative examples frnn predictions dataset shown fig. shows three actions hand waving walking boxing. blur stops increasing ﬁrst three predictions generating plausible motions corresponding actions background artefacts introduced. although movement patterns type action wide range variability trajectory bgru gives relatively sharp predictions limbs. ﬁrst third examples also show ability model recover blur. blur slightly increases arms action performed decreases reach ﬁnal position. fig. shows frnn predictions dataset. correspond diﬀerent physical exercises girl playing piano. common predictions static parts lose sharpness time background properly reconstructed occlusion. network correctly predicts actions variability shown rows repetitive movement performed last girl recovers correct body posture. dynamic regions introduce blur uncertainty action averaging possible futures. ﬁrst also shows interesting behaviour woman standing upper body becomes blurry uncertainty woman ﬁnishes motion ends expected upright position frames sharpen again. since model propagate errors deeper layers makes previous predictions following ones introduction blur imply blur propagated. example middle motion could multiple predictions depending movement pace inclination body performing ﬁnal body pose lower uncertainty. consider villegas lotter since methods fail successfully converge predict sequence black frames. rest approaches frnn obtains best predictions little blur distortion. rladder baseline second best approach. introduce blur heavily deforms digits cross. srivastava mathieu accumulate blur time former smaller degree later makes digits unrecognisable frames. villegas obtains outstanding qualitative results. predicts plausible dynamics maintains sharpness individual background. frnn rladder follow closely predicting plausible dynamics good villegas maintaining sharpness individual. best prediction obtained model little blur distortion compared methods. second best villegas successfully capturing movement patterns introducing blur important distorsions last frame. looking background frnn proposes plausible initial estimation progressively completes woman moves. hand villegas modiﬁes already generated regions background uncovered generating unrealistic sequence regarding background. srivastava lotter fail ucf. srivastava heavily distort frames. discussed section fully connected recurrent layers constrains state size prevents model encoding relevant information complex scenarios. case lotter makes good predictions ﬁrst frame rapidly accumulates artefacts. analyse stratiﬁcation sequence representation among bgru layers. bgru units allow bijective mapping states possible remove deepest layers trained network allowing check predictions aﬀected providing insight dynamics captured layer. speciﬁcally sequences predicted multiple shown fig. mmnist dataset. analysed model consists layers convolutional layers bgru layers. firstly removing last bgru layers signiﬁcant impact prediction. shows that simple dataset network higher capacity required. removing layers result loss pixel-level information progressive loss behaviours complex simpler ones. means information given level abstraction encoded higher level layers. removing third deepest bgru layer digits stop bouncing keep linear trajectories exiting image. indicates layer charge encoding information bouncing dynamics. removing next layer digits stop behaving correctly boundaries image. parts digit bounce others keep previous trajectory. also bouncing dynamics layer seems charge recognising digits single units following movement pattern. removed diﬀerent segments digit allowed move separate elements. finally fig. moving mnist predictions frnn layer removal. removing bgru layers leaves convolutional layers transposed convolutions providing identity mapping. bgru layers digits distorted various ways. layers left general linear dynamics still captured model. leaving single bgru layer linear dynamics lost. according results linear movement dynamics captured pixel level ﬁrst bgru layers. next start aggregating movement patterns single-trajectory components preventing distortion. collision components image bounds also detected. ﬁfth layer aggregates single-motion components digits forcing follow motion. seems eﬀect preventing bounces likely presented folded recurrent neural networks recurrent architecture video prediction lower computational memory cost compared equivalent recurrent models. achieved using proposed bijective grus horizontally pass information encoder decoder. eliminates need using entire given step encoder decoder needs executed input encoding prediction respectively. also facilitates convergence naturally providing noisy identity function training. evaluated approach three video datasets outperforming state prediction results mmnist obtaining competitive results times less memory usage computational cost best scored approach. qualitatively model limit recover blur preventing propagation high level dynamics. also demonstrated stratiﬁcation representation topology optimisation model explainability layer removal. layer shown modify state previous adding complex behaviours removing layer eliminates behaviours leaves lower-level ones untouched.", "year": 2017}