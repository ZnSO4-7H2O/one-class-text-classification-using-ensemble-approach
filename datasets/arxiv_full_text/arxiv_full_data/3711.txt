{"title": "Guided Deep Reinforcement Learning for Swarm Systems", "tag": ["cs.MA", "cs.AI", "cs.LG", "cs.SY", "stat.ML"], "abstract": "In this paper, we investigate how to learn to control a group of cooperative agents with limited sensing capabilities such as robot swarms. The agents have only very basic sensor capabilities, yet in a group they can accomplish sophisticated tasks, such as distributed assembly or search and rescue tasks. Learning a policy for a group of agents is difficult due to distributed partial observability of the state. Here, we follow a guided approach where a critic has central access to the global state during learning, which simplifies the policy evaluation problem from a reinforcement learning point of view. For example, we can get the positions of all robots of the swarm using a camera image of a scene. This camera image is only available to the critic and not to the control policies of the robots. We follow an actor-critic approach, where the actors base their decisions only on locally sensed information. In contrast, the critic is learned based on the true global state. Our algorithm uses deep reinforcement learning to approximate both the Q-function and the policy. The performance of the algorithm is evaluated on two tasks with simple simulated 2D agents: 1) finding and maintaining a certain distance to each others and 2) locating a target.", "text": "abstract. paper investigate learn control group cooperative agents limited sensing capabilities robot swarms. agents basic sensor capabilities group accomplish sophisticated tasks distributed assembly search rescue tasks. learning policy group agents diﬃcult distributed partial observability state. here follow guided approach critic central access global state learning simpliﬁes policy evaluation problem reinforcement learning point view. example positions robots swarm using camera image scene. camera image available critic control policies robots. follow actor-critic approach actors base decisions locally sensed information. contrast critic learned based true global state. algorithm uses deep reinforcement learning approximate q-function policy. performance algorithm evaluated tasks simple simulated agents ﬁnding maintaining certain distance others locating target. keywords deep reinforcement learning multi-agent deep reinforcement learning multi-agent learning swarm intelligence swarm learning swarm robotics nature provides many examples performance collective limited beings exceeds capabilities individual. ants transport prey size single could carry termites build nests nine meters height bees able regulate temperature hive. common phenomena fact individual basic local sensing environment limited communication capabilities. inspired biological processes swarm robotics tries emulate complex behavior swarm rather simple entities. typically robots basic movement communication capabilities sense parts environment distances agents. often designed small even without memory systems agents access short horizon perception. common approach imitate systems extracting rules observed behavior. kube example investigate cooperative prey retrieval ants infer rules swarm robots fulﬁll task cooperative box-pushing. work found however deﬁning behaviors manually tedious complexity tasks solve manually limited. paper follow data-driven reinforcement learning approach solving cooperative multi-agent tasks based locally sensed information agent. tasks challenging learning problems since dimensionality problem grows exponentially number agents agents partially observe global state. however many scenarios fully observed global system state available training example camera ﬁlming whole swarm. using global state information simplify problem evaluating actions robots similarly trainer observes team football players coordinates actions tactics training process. context reinforcement learning mechanism formulated actor-critic learning algorithm actor learns decentralized control policy operating local observations provide critic full system state guide swarm learning process. propose deep reinforcement learning framework learn policies homogeneous agents fulﬁll cooperative task. compact representation full system state example cartesian coordinates robots learn q-function actions agents solely based local observations agent. classical reinforcement learning algorithms heavily rely quality feature representation. instead approach follows recent developments deep reinforcement learning successfully combines techniques deep learning algorithms reinforcement learning. particularly appealing approach ability learn policies end-to-end fashion mappings high-dimensional sensory input actions without need complex feature engineering. examples deep q-learning algorithm deep deterministic policy gradient algorithm recapitulated section exist extensions deal partial observability formulated single-agent scenarios. demonstrate framework formulate tasks simulated swarm environment inspired kilobot robot platform agents move forward turn left right sense distances neighboring agents within certain radius. using information learn policy able establish maintain certain distance agents cooperatively locate target. section summarize algorithms method based important single-agent reinforcement learning algorithms algorithm ddpg algorithm explained following. single-agent reinforcement learning task formulated markov decision process tuple states actions transition density function reward function discount factor. γk−tr cumulative discounted reward discounted return agent achieves. q-function deﬁned expected total discounted reward following policy core problem policy agent maximizes expected return applied learn policy. techniques like target network experience replay used make update q-function stable deep neural network non-linear function approximator. loss function using example adaptive learning rate method like adam here denotes parameters target network copy q-function network updated much slower rate ensure stability algorithm. gradient loss function given stored buﬀer sampled mini-batches update. learning begins warm-up phase used replay memory samples created initial exploration policy. ﬁnal policy greedy strategy maxa learning epsilon greedy strategy typically used exploration. deep deterministic policy gradient algorithm ddpg algorithm actor-critic learning algorithm learning deterministic continuous control policies single-agent environments extending continuous action spaces. q-function policy learned simultaneously represented neural networks parameters respectively. again target networks whose parameters slowly track parameters original networks experience replay used. update rules parameters given learning algorithm guides learning process homogeneous swarms exploiting global state information assume available centrally training. swarm robotics information provided example camera ﬁlming scene. simulated experiments assume global state given positions orientations robots. information shared agents used evaluating actions agents reinforcement learning process. following similar scheme ddpg algorithm learn q-function based global state information evaluate whole swarm’s behavior. however actions chosen agents determined based observation histories only. allows guidance swarm learning process. formulate swarm system swarm similar deﬁnition) seen special case decentralized partially observed markov decision process agent swarm deﬁned tuple where local states space local observations local actions agent. observation model deﬁnes observation probabilities agent given global state. note system invariant order agents i.e. given local state agents observation probabilities same. swarm deﬁned number agents global environment state consisting local states agents possibly additional states environment transition density function. agent uses distributed policy maps histories local observations actions single agents action execute. denote horizon agent’s history space i.e. contrast dec-pomdp solving algorithms accordance idea swarm intelligence want learn simple policies explicitly tackle problem information gathering swarm agents assumed identical. reward function swarm depends target values deﬁned target network target policy. parameters target networks follow original parameters slow updates similar manner ddpg. hence resulting algorithm learning q-function used ddpg diﬀerence single policy function used compute actions agents whereas ddpg view actions agents large combined action vector policy function would diﬀerent dimension action vector. resulting actions guided critic approach course diﬀer agents histories agents diﬀerent. gradient averaged gradients single actors. given speciﬁc global state algorithm able improve policy histories state contain expectation enough information determine high quality action. previous equation condition formalized diﬀerentially i.e. policy improvement step successful gradient actors correlated gradient q-function. modeling communication swarm environment agents cannot access global state. instead agent able sense local environment. inspired kilobot platform scenarios agent sense distances neighboring agents within certain range naturally number observed neighbored agents time step changing course episode resulting varying amount observations i.e. distance measurements agent. dealing neural networks limited ﬁxed input dimensionality observation representation. thus instead collecting vector distance measurements time step histogram distances ﬁxed dimension. histogram distances agent given vector consisting entries paper simulated swarms agents whose capabilities inspired kilobot robots kilobot three legged robot diameter circular base. moves help vibrating motors controlled independently. motors robot move forward maximum speed cm/s turn ◦/s. ambient light sensor placed robot. infrared sensors transmitting receiving messages placed beneath. messages travel reﬂection ground surface distance depending composition. also possible determine distance transmitting robot based intensity received message’s signal. videos learned policies found dropbox folder here. experimental results paper consider agents move world arranged torus. global state comprised local states individual agent additional state environment example location object agents localize. agent’s action dimensional vector |aφ| global action given communication radius chosen histogram distances consists entries. ﬁrst task agents maintain certain distance other. agent seen node graph. speciﬁed distance agents edge nodes. task maximize number edges. figure shows scene simulation agents. agents depicted green dots arrow show orientation. outer circle indicates communication radius inner light green ring area another agent right distance edge created. agent’s local state given position fig. visualization simulation environment. left ﬁgure shows graph task agents depicted green dots orientation arrow. outer circle shows communication radius inner light green area distance valid edge established. right ﬁgure shows localization task. long agents found target depicted ﬁnding target color changed green. outer circle shows communication radius. target depicted blue dot. second task requires agents cooperatively locate target. figure shows scene task agents. agents depicted green dots arrow show orientation outer circle indicates communication radius. agent sees target ﬁrst time depicted green dot. target depicted blue dot. agent’s local localization distance target within communication radius neighboring agent currently sees target distribution distances agents within communication range. reward function given initialize feed-forward neural networks following architectures parameters. critic three actor four hidden layers neurons hidden layers respectively. input data layer processed fully connected layer followed exponential linear units activation functions. parameters hidden dimensionality input layer. output layer critic also fully connected layer linear activation function initialized uniform tasks conducted experiments agents learning simultaneously environment. scenario evaluated four independent training trials. evaluations unfortunately possible high computational demands deep learning architectures. episode length time steps horizon agent’s history chosen time steps. beginning episode agents placed randomly world random orientations. fig. progression episode agents executing policy learned agents. beginning randomly placed scene. course episode groups agents established. groups move circular patterns trying keep distance other. cross evaluation using learned policies diﬀerent conﬁgurations found figure shows results policy scenario executed times averaged return episode. policies learned number agents perform better scenarios agents policies learned agents perform better scenarios agents. policies learned agents reach quality policy learned agents. figure shows example scenario agents diﬀerent time stages episode executing policy learned agents respectively. valid edges agents indicated black line. policy learned agents makes agents collect small groups moving circular pattern. policy learned agents however tries accumulate many agents possible group resulting edges compared smaller groups task second task conducted array experiments. figure shows learning curves localization task averaged trials each. figure shows evaluation learned policies executed diﬀerent numbers agents. additionally learned policy agent whose observation already seen target distance target number agents able process information scenarios agents enough agents take part learning process beneﬁts communication policies outperform policy without inter-agent communication. resulting behavior lets agents already found target stay vicinity target. order show eﬃcacy approach tried solve edge building task agents using joint action-observation history only i.e. learning q-function instead however even multiple diﬀerent model architectures unable learn meaningful policies. architectures included variants fully connected networks processing joint histories weight sharing diﬀerent agents’ local histories connecting later hidden layers fully connected output layers using diﬀerent numbers hidden layers neurons. exemplary learning curves compared learning curve using guided approach figure suppose major factors play role lead failure approach. first dimensionality problem increases huge factor compared second problem learning q-function changes fully observed problem partially observed problem history length contains little information global state. fig. evaluation learned policies localization task executed agents. policy times plots show mean return learned policies times standard deviation. dashed lines show mean return policy without inter-agent communication. paper presented idea guided policy learning homogeneous swarm systems. proposed actor-critic learning approach showed q-function learned using global state information actors base decisions locally sensed information only. using architecture could learn distributed control policies limited sensing capabilities. results indicate feasible non-guided approach. believe guiding learning process global state information potential scale current successes deep reinforcement learning also cooperative multi-agent scenarios robot swarms beyond reach current methods. many scenarios state information acquired learning example using additional sensors.", "year": 2017}