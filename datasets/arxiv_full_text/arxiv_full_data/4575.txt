{"title": "Online Learning with Preference Feedback", "tag": ["cs.LG", "cs.AI"], "abstract": "We propose a new online learning model for learning with preference feedback. The model is especially suited for applications like web search and recommender systems, where preference data is readily available from implicit user feedback (e.g. clicks). In particular, at each time step a potentially structured object (e.g. a ranking) is presented to the user in response to a context (e.g. query), providing him or her with some unobserved amount of utility. As feedback the algorithm receives an improved object that would have provided higher utility. We propose a learning algorithm with provable regret bounds for this online learning setting and demonstrate its effectiveness on a web-search application. The new learning model also applies to many other interactive learning problems and admits several interesting extensions.", "text": "propose online learning model learning preference feedback. model especially suited applications like search recommender systems preference data readily available implicit user feedback particular time step potentially structured object presented user response context providing unobserved amount utility. feedback algorithm receives improved object would provided higher utility. propose learning algorithm provable regret bounds online learning setting demonstrate effectiveness web-search application. learning model also applies many interactive learning problems admits several interesting extensions. learning model motivated users interact web-search engine recommender system. time step user issues query system responds supplying list results. user views results selects prefers. examples cases user feedback comes form preference. search example infer user would preferred ranking presented recommendation example movie preferred movie cardinal utilities predictions however never observed algorithm typically optimal ranking/movie feedback. preference feedback different conventional online learning models. simplest form multi-armed bandit problem algorithm chooses action observes reward action. conversely rewards possible actions revealed case learning expert advice model ordering arms revealed sits expert bandit setting. similar relationship holds online convex optimization online convex optimization bandit setting viewed continuous extensions expert bandit problems respectively since rely observing either full convex function value convex functions iteration. closely related work dueling bandits setting existing algorithms known converge rather slowly. following formally deﬁne online preference learning model notion regret propose simple algorithm prove regret bound empirically evaluate algorithm web-search problem. online preference learning model deﬁned follows. round learning algorithm receives context presents object response user returns object algorithm receives feedback. example web-search user issues query presented ranked list url’s user interacts ranking provided clicking results relevant her. user interaction allows infer better ranking user. assume user evaluates rankings according utility function unknown learning algorithm. natural deﬁne regret model based difference utility object present best possible objects intuitively deﬁnition describes quality feedback much utility user feedback higher algorithm’s prediction terms fraction maximum possible utility range. note slack variable captures noise feedback. propose algorithm figure online preference learning problem. maintains vector predicts object highest utility according iteration receives feedback updates direction φ−φ. theorem α-informative feedback algorithm figure regret proof theorem provided appendix user feedback noise free ﬁrst term right hand side bound vanishes. average regret case approaches zero rate addition result following extensions cannot provide space limitations possible weaken requirement feedback. instead requiring αinformative feedback user required give α-informative feedback expectation. show result similar theorem case. applied preference perceptron algorithm yahoo learning rank dataset dataset consists query-url features relevance rating ranges zero four ﬁrst computed best least squares relevance labels features using entire dataset utilities experiment reported respect equation denotes ranking. particular index placed position ranking. thus measure considers urls query computes score based graded relevance. feature-map utility motivated deﬁnition effectively utility score mimics replacing relevance label linear prediction based features. query time step preference perceptron algorithm present ranking maxixqt note merely amounts sorting documents scores mizes done efﬁciently. ranking presented user user returns ranking exact nature user feedback differed experiments; details feedback found below. query ordering randomly permuted twenty times results reported average runs. given denotes optimal ranking respect also present results another quantity refer dcg* regret. since every query-url pair manual relevance judgment dataset optimal computed sorting relevance score. dcg* regret measure difference optimal ranking rankings present step. α-informative feedback goal ﬁrst experiment regret algorithm changes assuming α-informative feedback without noise. ranking presented feedback obtained follows given ranked list simulated user would list would stop found url’s that placed list gave noise free α-informative feedback based figure shows results experiment different values. expected regret lower compared regret respect note however difference curves much smaller factor ten. because strictly α-informative feedback also strictly β-informative feedback could relevance label feedback experiment feedback based actual relevance labels dataset follows given ranking query user would list inspecting urls. five url’s highest relevance labels placed locations user feedback. note noisy version feedback since linear cannot describe labels exactly dataset. baseline ranking trained repeatedly. ﬁrst iteration random ranking presented feedback ranking obtained. trained based pair examples ranking presented based prediction previously trained ranking svm. user always returned ranking based relevance labels mentioned above; pairs examples stored every iteration. note training ranking iteration would prohibitive since involves cross-validating parameter trades-off margin slacks. thus trained whenever examples added training previous training. value parameter obtained ﬁve-fold cross-validation. value determined trained training examples available time used predict rankings next training. results experiment presented figure provided mean regret well standard deviation experiment. since feedback based relevance labels utility regret converges non-zero value. also noticed preference perceptron performs signiﬁcantly better compared svm. might possible improve performance training often. however would extremely prohibitive. instance perceptron algorithm took around minutes whereas version took hours proposed model online learning preferences especially suitable implicit user feedback. efﬁcient algorithm proposed provably minimizes regret. experiments showed effectiveness web-search ranking.", "year": 2011}