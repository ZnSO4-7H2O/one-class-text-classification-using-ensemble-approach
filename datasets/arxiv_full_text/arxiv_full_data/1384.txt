{"title": "Measuring Neural Net Robustness with Constraints", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness \"overfit\" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.", "text": "despite high accuracy neural nets shown susceptible adversarial examples small perturbation input cause become mislabeled. propose metrics measuring robustness neural devise novel algorithm approximating metrics based encoding robustness linear program. show metrics used evaluate robustness deep neural nets experiments mnist cifar- datasets. algorithm generates informative estimates robustness metrics compared estimates based existing algorithms. furthermore show existing approaches improving robustness overﬁt adversarial examples generated using speciﬁc algorithm. finally show techniques used additionally improve neural robustness according metrics propose also according previously proposed metrics. recent work shows often possible construct input mislabeled neural perturbing correctly labeled input tiny amount carefully chosen direction. lack robustness problematic variety settings changing camera lens lighting conditions successive frames video adversarial attacks security-critical applications number approaches since proposed improve robustness however work direction handicapped lack objective measures robustness. typical approach improving robustness neural algorithm adversarial examples augment training examples train neural robustness evaluated using algorithm adversarial examples f—if discovers fewer adversarial examples concluded robust however overﬁt adversarial examples generated a—in particular different algorithm many adversarial examples objective robustness measure vital reliably compare different algorithms also understand robustness production neural nets—e.g. deploying login system based face recognition security team need evaluate risk attack using adversarial examples. paper study problem measuring robustness. propose statistics robustness point ﬁrst measures frequency adversarial examples occur; measures severity adversarial examples. statistics depend parameter intuitively speciﬁes threshold adversarial examples exist challenge efﬁciently computing give exact formulation problem intractable optimization problem. recover tractability approximate optimization problem constraining search convex region around furthermore devise iterative approach solving resulting linear program produces order magnitude speed-up. common neural nets fact piecewise linear functions choose region around linear. since linear nature neural nets often cause adversarial examples choice focuses search adversarial examples likely exist. evaluate approach deep convolutional neural network mnist. estimate using algorithm algorithm al-bfgs introduced show produces substantially accurate estimate al-bfgs. data augmentation algorithm improve robustness resulting ﬁne-tuned neural nets fl-bfgs. according al-bfgs fl-bfgs robust according alp. words fl-bfgs overﬁts adversarial examples computed using al-bfgs. contrast robust according al-bfgs alp. furthermore demonstrate scalability apply approach evaluate robustness -layer network-in-network neural cifar- reveal surprising lack robustness. ﬁne-tune show robustness improves albeit small amount. summary contributions formalize notion pointwise robustness studied previous work show computing pointwise robustness encoded constraint system approximate constraint system tractable linear program devise optimization solving linear program order magnitude faster demonstrate experimentally algorithm produces substantially accurate measures robustness compared algorithms based previous work show evidence neural nets ﬁne-tuned improve robustness overﬁt adversarial examples identiﬁed speciﬁc algorithm susceptibility neural nets adversarial examples discovered given test point predicted label adversarial example input predicted label adversarial perturbation small then devises approximate algorithm ﬁnding smallest possible adversarial perturbation approach minimize combined objective loss instance box-constrained convex optimization solved using l-bfgs-b. constant optimized using line search. formalization robustness corresponds notion ﬁnding minimal propose exact algorithm computing well tractable approximation. algorithm also used approximate show experimentally algorithm substantially accurate range subsequent work studying robustness; devises algorithm ﬁnding purely synthetic adversarial examples searches adversarial examples using random perturbations showing adversarial examples fact exist large regions pixel space shows even intermediate layers neural nets robust adversarial noise seeks explain neural nets generalize well despite poor robustness properties. starting major focus devising faster algorithms ﬁnding adversarial examples. idea adversarial examples computed on-the-ﬂy used training examples analogous data augmentation approaches typically used train neural nets adversarial examples quickly chooses adversarial perturbation direction signed gradient loss ﬁxed magnitude. intuitively given gradient loss function choice likely produce adversarial example direction improves upon taking multiple gradient steps extends idea norms beyond norm takes approach ﬁxes formalizes robust optimization. shortcoming lines work robustness typically measured using algorithm used adversarial examples case resulting neural overﬁt adversarial examples generating using algorithm. example shows improved accuracy adversarial examples generated using signed gradient method consider whether robustness increases adversarial examples generated using precise approaches similarly compares accuracy adversarial examples generated using considers accuracy adversarial examples generated using approach baseline network. paper provide metrics evaluating robustness demonstrate importance using impartial measures compare robustness. additionally work designing neural network architectures learning procedures improve robustness adversarial perturbations though obtain state-of-theart accuracy unperturbed test sets. also work using smoothness regularization related train neural nets focusing improving accuracy rather robustness robustness also studied general contexts; studies connection robustness generalization establishes theoretical lower bounds robustness linear quadratic classiﬁers seeks improve robustness promoting resiliance deleting features training. broadly robustness identiﬁed desirable property classiﬁers beyond prediction accuracy. traditional metrics accuracy precision recall help users assess prediction accuracy trained models; work aims develop analogous metrics assessing robustness. robustness metrics consider classiﬁer input space labels. assume training test points distribution ﬁrst formalize notion robustness point describe statistics measure robustness. statistics depend parameter captures idea care robustness certain threshold—we disregard adversarial examples whose distance greater experiments mnist cifar- pointwise robustness. intuitively robust small perturbation affect assigned label. interested perturbations sufﬁciently small affect human classiﬁcation; established condition parameter formally -robust every finally pointwise robustness minimum fails -robust measures severity fails robust conditioned -robust. condition pointwise robustness since -robust degree robust matter. smaller corresponds worse adversarial severity since susceptible adversarial examples distances nearest adversarial example small. frequency severity capture different robustness behaviors. neural high adversarial frequency adversarial severity indicating adversarial examples distance away original point conversely neural adversarial frequency high adversarial severity indicating typically robust occasionally severely fails robust. frequency typically important metric since neural adversarial frequency robust time. indeed adversarial frequency corresponds figure neural single hidden layer relu activations trained dataset binary labels. training data loss surface. linear region corresponding training point. figure mnist image classiﬁed adversarial example classifed adversarial perturbation. cifar- image classiﬁed automobile adversarial example classiﬁed truck adversarial perturbation. accuracy adversarial examples used measure robustness severity used differentiate neural nets similar adversarial frequency. given samples drawn i.i.d. estimate using following standard estimators assuming compute consider training points figure colored based ground truth label. classify data train two-layer neural max{)} relu function applied pointwise. figure includes contours per-point loss function neural net. exhaustively searching input space determine distance nearest adversarial example input intractable. recall neural nets rectiﬁed-linear units activations piecewise linear since adversarial examples exist linearity neural restrict search region around neural linear. region around deﬁned activation relu function constrain half-space intersection half-spaces convex admits efﬁcient search. figure shows convex region additionally labeled exactly constraints linear since linear therefore distance nearest input label minimizing finally perform search label though efﬁciency take label assigned second-highest score figure shows adversarial example found algorithm running example. figure note direction nearest adversarial example necessary aligned signed gradient loss function observed others network function rni− |l|. describe encoding fully-connected relu layers; convolutional layers encoded similarly fully-connected layers max-pooling layers encoded similarly relu layers. introduce variables constraints interpretation represents output vector layer network; i.e. constraint encodes input layer. layer encode computation given constraint approximate computation pointwise robustness convex restriction. challenge solving non-convexity feasible recover tractability approximate constraining feasible carefully chosen constraints convex feasible set. call convex restriction sense convex restriction opposite convex relaxation. then approximately compute robustness choice construct feasible constraints i.e. describe construct note convex sets. furthermore convex conjunction however disjunction convex; example potential non-convexity disjunctions makes difﬁcult optimize. eliminate disjunction operations choosing disjuncts hold. example note words replace either feasible resulting constraints become smaller. taking effectively replaces restrict every disjunction systematically choose either replace constraint particular choose satisﬁes choose otherwise. constraints disjunctions always mutually exclusive never simultaneously satisﬁes take conjunction choices. resulting constraints contains conjunctions linear relations feasible convex. fact expressed linear program solved using standard solver. example consider rectiﬁed linear layer original constraint added unit rectiﬁed linear layer iterative constraint solving. implement optimization solving lazily adding constraints necessary. given constraints start solving subset equality constraints yields solution feasible also optimal solution original otherwise constraints satisﬁed repeat process. process always yields correct solution since worst case becomes equal practice optimization order magnitude faster directly solving constraints single target label. simplicity rather minimize second probable label i.e. approximate robustness statistics. statistics deﬁned overapproximation estimates unbiased show empirically algorithm produces substantially less biased estimates existing algorithms ﬁnding adversarial examples. finding adversarial examples. algorithm estimating compute adversarial examples. given value computed optimization procedure used solve adversarial example finetuning. ﬁne-tuning reduce neural net’s susceptability adversarial examples. first algorithm compute adversarial examples xtrain training set. then continue training network augmented training reduced training rate. repeat process multiple rounds round consider original training table evaluation ﬁne-tuned networks. method discovers adversarial examples baseline neural hence producing better estimates. lenet ﬁne-tuned rounds exhibit notable increase robustness compared original lenet. figure cumulative number test points function neural nets original lenet lenet ﬁne-tuned baseline lenet ﬁne-tuned algorithm measured using baseline measured using algorithm. neural nets original ﬁnetuned algorithm estimated using algorithm. rounding errors. mnist images represented integers must round perturbation obtain image oftentimes results non-adversarial examples. ﬁne-tuning eliminates problem ensuring neural constraint high conﬁdence adversarial examples. experiments similarly modiﬁed l-bfgs-b baseline line search count choose since larger causes adversarial baseline signiﬁcantly fewer adversarial examples small results smaller improvement robustness. choice rounding errors occur adversarial examples mnist training set. adversarial examples neural lenet trained classify mnist network-in-network neural trained classify cifar- neural nets trained using caffe mnist figure shows adversarial example image figure labeled figure shows corresponding adversarial perturbation scaled difference visible cifar- figure shows adversarial example labeled truck image figure labeled automobile figure shows corresponding scaled adversarial perturbation compare algorithm estimating baseline l-bfgs-b algorithm proposed tool provided compute baseline. algorithms adversarial target label lenet comparisons since substantially robust neural nets considered previous work also versions lenet ﬁne-tuned using algorithm baseline focus severe adversarial examples stricter threshold robustness pixels. performed similar comparison signed gradient algorithm proposed lenet algorithm found adversarial example mnist test four adversarial examples mnist training omit results figure plot number test points function results. estimated using baseline algorithm. plots compare robustness neural network function table show results evaluating robustness neural including adversarial frequency adversarial severity. running time algorithm baseline algorithm similar; cases computing single input takes seconds. comparison without iterative constraint solving optimization algorithm took minutes run. discussion. every neural algorithm produces substantially higher estimates adversarial frequency. words algorithm estimates substantially better accuracy compared baseline. according baseline metrics shown figure baseline neural similarly robust neural robust original lenet neural actually robust baseline neural smaller values whereas baseline neural eventually becomes slightly robust behavior captured robustness statistics—the baseline neural lower adversarial frequency also worse adversarial severity however according metrics shown figure neural substantially robust baseline neural net. again reﬂected statistics—our neural substantially lower adversarial frequency compared baseline neural maintaining similar adversarial severity. taken together results suggest baseline neural overﬁtting adversarial examples found baseline algorithm. particular baseline neural learn adversarial examples found algorithm. hand neural learns adversarial examples found algorithm found baseline algorithm. also implemented approach cifar- network-in-network neural obtains test accuracy. computing single input takes seconds -core cpu. unlike lenet suffers severely adversarial examples—we measure adversarial frequency adversarial severity pixels. neural test accuracy similar test accuracy original nin. seen figure neural improves slightly terms robustness especially smaller before improvements reﬂected metrics—the adversarial frequency neural drops slightly adversarial severity improves nevertheless unlike lenet ﬁne-tuned version remains prone adversarial examples. case believe techniques required signiﬁcantly improve robustness. shown formulate efﬁciently estimate improve robustness neural nets using encoding robustness property constraint system. future work includes devising better approaches improving robustness large neural nets studying properties beyond robustness.", "year": 2016}