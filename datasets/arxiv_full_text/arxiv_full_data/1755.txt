{"title": "Linear Algebraic Structure of Word Senses, with Applications to Polysemy", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Word embeddings are ubiquitous in NLP and information retrieval, but it's unclear what they represent when the word is polysemous, i.e., has multiple senses. Here it is shown that multiple word senses reside in linear superposition within the word embedding and can be recovered by simple sparse coding.  The success of the method ---which applies to several embedding methods including word2vec--- is mathematically explained using the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each word sense is also accompanied by one of about 2000 discourse atoms that give a succinct description of which other words co-occur with that word sense. Discourse atoms seem of independent interest, and make the method potentially more useful than the traditional clustering-based approaches to polysemy.", "text": "word embeddings ubiquitous information retrieval it’s unclear represent word polysemous i.e. multiple senses. shown multiple word senses reside linear superposition within word embedding recovered simple sparse coding. success method —which applies several embedding methods including wordvec— mathematically explained using random walk discourses model novel aspect technique word sense also accompanied discourse atoms give succinct description words co-occur word sense. discourse atoms seem independent interest make method potentially useful traditional clustering-based approaches polysemy. word embeddings represent meaning word real-valued vector. construction typically uses firth’s hypothesis word’s sense captured distribution words around classical vector space models simple linear algebra matrix word-word co-occurrence counts whereas recent neural network energy-based models wordvec nonconvex optimization word embeddings useful many tasks seem involved understanding neural encoding semantics however unclear embeddings represent word polysemous i.e. multiple word senses. monolithic approach representing word single word vector inner information extracted inner product felt fail capturing ﬁner structure current paper goes beyond monolithic view describing multiple senses word actually reside within word embedding linear superposition recovered simple sparse coding. linear structure revealed section surprising thought experiment. work inspired discovery word analogies solved linear algebraic methods however mathematical explanation eﬃcacy method uses recent random walk discourses model arora experiments -dimensional embeddings created using wikipedia corpus billion tokens using embedding word senses also recovered loss performance recent embeddings wordvec glove well older vector space methods methods known interrelated; work supported part grants ccf- ccf- ccf- dms- simons investigator award simons collaboration grant. tengyu also supported simons award graduate students theoretical computer science. automatic learning word senses usually done variants exemplar approach yarowsky identiﬁes multiple word senses clustering neighboring words. firth’s hypothesis justiﬁes approach since senses diﬀerent contexts involve diﬀerent word distributions. following quote abstract representative general feeling word embeddings word embeddings nature unable capture polysemy diﬀerent meanings word conﬂated single representation. line work tries capture word senses augmenting embeddings complicated representations single vector however relate diﬀerent word senses sense inventory seems require external source wordnet problematic languages lack wordnets. contrast approach purely unsupervised exhibits word senses reside within existing embeddings including classical pmi-based ones. important feature approach corpus used create -dimensional embedding word. contrast graph-theoretic clustering approaches work directly corpus orders magnitude slower. section explains interpret sparse coding word embeddings linear algebraic surrogate graph clustering. another diﬀerence past clustering approaches senses diﬀerent words recovered method interconnected notion atoms discourse notion introduced section give example article clothing sense automatically related words like trousers blouse etc. word sense disambiguation supervised problem identifying sense word used occurrence document left future work. techniques help create resources useful annotated corpora wordnet lacking many languages. sparse coding applied word embeddings used getting representations section propose thought experiment uncovers qualitatively quantitatively linear algebraic structure polysemous word respect senses embedding space. consider polysemous word refer article clothing drawn match physical act. let’s take viewpoint —simplistic instructive— single lexical token represents unrelated words .... describe surprising experiment suggests embedding approximately weighted embeddings .... experiment consists creating artiﬁcial polysemous word wnew combining random words frequent every occurrence counts occurrence wnew. next embedding computed wnew using embedding method deleting embeddings preserving embeddings words. experiment repeated wide range values ratio frequencies always found vector vwnew lies close subspace spanned cosine angle subspace average standard deviation thus vwnew question depend upon ratio frequencies figure shows conferences orchestra meetings philharmonic seminars philharmonia workshops conductor exhibitions symphony organizes orchestras toscanini concerts concertgebouw lectures solti actually good feature allows less dominant/frequent sense superproportionate contribution vwnew thus making detectable principle despite noise/error. figure thought experiment creating artiﬁcial polysemous word wnew merging words ﬁtted word vector vwnew found close plot shows coeﬃcients artiﬁcial polysemous words versus ratio frequencies insuﬃcient mathematically senses since vtie expressed inﬁnitely many ways combination. senses interrelate senses diﬀerent words —for example relate article clothing sense shoe jacket etc. relies recent paper connects senses directions embedding space. random walk discourses model. arora introduce random walk discourses model suggests directions embedding space correspond local topic structure corpus term discourse. model ﬁtted real corpora wikipedia found distribution though gives nonzero probability every word fairly concentrated small number words around vector deﬁnes fairly distinct narrow discourse whose meaning discerned examining closest word-vectors discourses deﬁned look similar humans inner product quite diﬀerent inner product extracting word senses. recall senses tie. correspond diﬀerent word distributions occuring around interpret distinct discourses imagines clothing discourse high probability outputting sense also outputting related words shoe jacket etc. probability output discourse determined inner product expects vector clothing discourse high inner product shoe jacket etc. thus stand surrogate vtie motivates following global optimization. given word vectors totaling case sparsity parameter upper bound coeﬃcients nonzero noise vector. aj’s αwj’s unknowns optimization problem non-convex. nothing sparse coding useful neuroscience also image processing computer vision etc. solve using standard k-svd algorithm using optimization surrogate desired expansion vtie hope represent important discourses corpus refer atoms discourse consequently contain atoms corresponding clothing sports matches etc. high inner product respectively. furthermore restricting much smaller number words ensures discourse needs used multiple words. representative atoms appear table represents clear narrow topic. word represented using atoms usually capture distinct senses table senses recovered tie. beginning dampers scoreline wires goalless cables months equaliser wiring earlier clinching electrical contralto year scoreless wire baritone coloratura last cable objective function reveals sparse coding linear algebraic analogue overlapping clustering whereby ai’s cluster centers assigned soft linear combination them. fact clustering viewpoint also basis alternating minimization algorithm. distance.) practice sparse coding optimization produces coeﬃcients αij’s almost positive even though they’re unconstrained. furthermore usual sparse coding nonzero coeﬃcients usually correspond basis vectors pairwise inner product meaning pairwise quite diﬀerent. eﬀectively sparse coding ends writing vi’s weighted sums fairly diﬀerent aj’s positive inner product. intuitive connection classical approaches would cluster words appearing around corpus clustering weighted graph whose edge weights similarity scores word pairs. approach word embedding contains summary often appears discourses corpus implicitly captures similarity words. cluster centers aj’s softly assigned represent interesting discourses good probability occurring fact interesting signiﬁcant discourse guaranteed fact sparse coding ensures basis element chosen many words addition tie. atoms discourse reminiscent results automated methods obtaining thematic understanding text topic modeling described survey meta atoms highly reminiscent past results hierarchical topic models indeed model used compute word embeddings related log-linear topic model however discourses computed sparse coding word embeddings distinct topic modeling. atoms also reminiscent coherent word clusters detected past using brown clustering even sparse coding novelty current paper clear interpretation sparse coding results atoms discourse— well capture diﬀerent word senses. initializations whereupon substantial overlap found bases large fraction vectors basis found close vector other. thus combining bases merging duplicates yielded basis size. around atoms used large number words close words. appear semantically meaningless thus ﬁltered. mentioned refer signiﬁcant discourses represented basis vectors atoms discourse. content discourse atom discerned looking nearby words table contains examples discourse atoms table shows discourse atoms linked words spring. hierarchy discourse atoms. atoms fairly ﬁne-grained possible extract coarse-grained discourses. instance discourse atoms found jazz rock classical country related atoms about mathematics. again model suggests similar discourse atoms higher inner product other thus sparse coding able identify similarities create meta-discourse vectors music. experimentation best results involve sparse coding discourse atoms using hard sparsity allowing basis size turn meta-discourse vectors. figure shows example using atoms scientiﬁc ﬁelds; examples found full version. discourse interdisciplinary science like biochemistry turns approximately linear combinations meta-discourses biology chemistry. figure atoms discourse related scientiﬁc ﬁelds meta-atoms found second level sparse coding. connecting line corresponds discourse atoms meta atoms coding. thus atom biochemistry found expressed using linear combination meta discourse vectors chemistry biology outputting relevant sentences method construct dictionary completely automated would want polysemous word representative sentences illustrating various senses. done follows. noting sentences general correspond discourse atom deﬁne semantic representation sentence best rank- approximation ring name mexican professional wrestler gloria alvarado nava. principal component analysis) subspace spanned word embeddings words. given polysemous word take atoms well atoms inﬂectional forms yields atoms word whereupon sentence scored respect atom appropriate cosine similarity atom semantic representation sentence. finally output sentences highest scores. table presents sentences found word ring. examples found full version. main purpose paper show linear algebraic structure word senses within existing embeddings desirable quantiﬁcation eﬃcacy approach. runs well-known diﬃculties reﬂected changing metrics senseval tests years metrics involve custom similarity score based upon wordnet hard interpret available languages. simple test —–inspired word-intrusion tests topic coherence proposed advantages easy understand also administered humans. testbed uses polysemous words senses according wordnet. sense represented related words; collected wordnet online dictionaries college students told identify relevant words occurring online deﬁnitions word sense well accompanying illustrative sentences. considered ground truth representation word sense. words typically synonyms; e.g. tool/weapon sense were handle harvest cutting split tool wood battle chop. quantitative test called police line algorithm given random polysemous words senses contain true sense word well distractors algorithm identify word’s true senses set. method described algorithm precision recall diﬀerent presented figure algorithm succeeds precision recall performance remains reasonable giving test humans suggests method performs similarly non-native speakers. comparison using graph clustering. tests performed using code released recent papers given target polysemous word methods ﬁrst construct co-occurrence graph words related target word figure precision recall polysemy test. polysemous word senses containing ground truth senses word presented. human subjects told average word senses asked choose senses thought true. algorithms select senses algorithm times performance method clustering algorithm graph produce several clusters words regarded senses used grouping search results. tested three clustering algorithms provided. given lineup senses identiﬁed clusters used provided matching algorithm match sense cluster chose senses according metric used matching. hyperparameters enumerated among times default values software matching algorithm enumerated among {word overlap degree overlap} provided best results also plotted recognizable sense word typical example dogs breed cluster abstract. issue primary goal papers —clustering search results—since search abstract would return pages related dogs breed etc. test greatly hurt performance. investigate improve clustering algorithms implicit algorithm match given words cluster. word embeddings. constructed using many past methods. used generate atoms discourse given police lineup test. slightly lower performance. precision/recall lower following amounts glove .%/.% wordvec .%/.% nnse matrix factorization rank %/%. section concerned experiment artiﬁcial polysemous word merging unrelated words frequency ratio embeddings words give theoretical explanation. exposes parameters aﬀect allows explain general behaviour spread actual values observed. claim like course impossible prove unless make assumptions wordoccurrences patterns word vectors review model make simplifying assumptions justify intuitively. first assume original word vectors trained ﬁtting model arora based method-of-moments performed yielding optimization reminiscent classical method next review artiﬁcial word relates pmi. construction artiﬁcial word co-occurrence probability every word marginal distribution vector obtained optimizing using co-occurrence probabilities. word vectors remained unchanged optimization reduces furthermore since unrelated co-occurence matrix sparse words appear corpus smaller words appear exactly them. accordingly make following simplifcation. justify simpliﬁcation note arora shows evidence word vectors distributed fairly uniformly space bulk many properties similar random gaussian vectors. since words cooccur nontrivial inner product word vectors expected behave like scalar spherical gaussian vector comparable norm indeed simpliﬁcation follows distributed scalar spherical gaussian random variable expected norm order roughly γ/d. moreover compute value roughly constant) leads justiﬁed simpliﬁcation return proof. quadratic form speciﬁes better understood picking orthogonal basis diagonalized. matrix satisﬁes letting remains show additive structure obviously false general holds case interact rather limited way. high level turns sensitive component component hence minimizer achieved right projection formally simpliﬁed therefore presence rest minimization objective forces close either small. coordinate minimizer approximately therefore obtain minimizer approximately finally argue indeed expected smaller discussed above approximately therefore translates roughly moreover case comparable much larger still analytically solve minimizer obtain word embeddings obtained —and also several earlier methods—have shown contain information diﬀerent senses word extractable simple sparse coding. currently seems better nouns parts speech improving left future work. novel aspect approach word senses interrelated using discourse vectors give succinct description words appear neighborhood sense. makes method potentially useful tasks natural language processing well automated creation wordnets languages. method useful —and accurate—in semi-automated mode since human helpers better recognizing presented word sense coming complete list. thus instead listing senses word given sparse coding algorithm human helper examine list closest discourse atoms usually gives high recall senses missing semi-supervised version seems promising fast create wordnets languages. mentioned section logarithm embedding methods well method seems success approach logarithm allows less frequent senses superproportionate weight linear superposition. relevance neuroscience word embeddings used fmri studies subject thinking words study largely monosemous nuanced word embeddings discourse vectors introduced work useful explorations especially since sparse coding well logarithms thought neurally plausible.", "year": 2016}