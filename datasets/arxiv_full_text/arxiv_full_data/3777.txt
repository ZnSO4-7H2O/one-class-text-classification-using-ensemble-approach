{"title": "Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence", "tag": ["stat.ML", "cs.CV", "cs.LG", "cs.RO", "cs.SY"], "abstract": "We propose a greedy mixture reduction algorithm which is capable of pruning mixture components as well as merging them based on the Kullback-Leibler divergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD based method since it is not restricted to merging operations. The capability of pruning (in addition to merging) gives the algorithm the ability of preserving the peaks of the original mixture during the reduction. Analytical approximations are derived to circumvent the computational intractability of the KLD which results in a computationally efficient method. The proposed algorithm is compared with Runnalls' and Williams' methods in two numerical examples, using both simulated and real world data. The results indicate that the performance and computational complexity of the proposed approach make it an efficient alternative to existing mixture reduction methods.", "text": "bound runnalls’ algorithm based premise gaussian mixtures calculated analytically. runnalls’ approach dedicated minimize original mixture approximate refer forward-kld choice cost function results algorithm reduces number components merging reduction step. paper propose based algorithm. efﬁcient method minimizing approximate mixture original refer reverse-kld resulting algorithm ability choose pruning merging components step reduction unlike runnalls’ algorithm. enables example possibility prune low-weight components keeping heavy-weight components unaltered. furthermore present approximations required overcome analytical intractability rkld gaussian mixtures making implementation fast efﬁcient. rest paper organized follows. section present necessary background required problem intend solve paper. relevant works strengths weaknesses described section iii. proposed solution strengths presented section approximations fast computation proposed divergence given section proposed algorithm using approximations evaluated compared alternatives numerical examples section paper concluded section vii. terms positive weights summing unity parameters component density mixture reduction problem approximation original mixture using fewer components. ideally formulated nonlinear optimization problem cost function measuring distance original approximate mixture minimized. optimization problem solved numerical solvers problem analytically tractable. numerical optimization based approaches computationally quite expensive particular high dimensional data generally suffer problem local optima hence common alternative solution greedy iterative approach. abstract—we propose greedy mixture reduction algorithm capable pruning mixture components well merging based kullback-leibler divergence algorithm distinct well-known runnalls’ based method since restricted merging operations. capability pruning gives algorithm ability preserving peaks original mixture reduction. analytical approximations derived circumvent computational intractability results computationally efﬁcient method. proposed algorithm compared runnalls’ williams’ methods numerical examples using simulated real world data. results indicate performance computational complexity proposed approach make efﬁcient alternative existing mixture reduction methods. mixture densities appear various problems estimation theory. existing solutions problems often require efﬁcient strategies reduce number components mixture representation computational limits. example time series problems involve ever increasing number components mixture time. gaussian ﬁlter nonlinear state estimation; multihypotheses tracking gaussian mixture probability hypothesis density ﬁlter multiple target tracking listed examples algorithms require mixture reduction implementation. several methods proposed literature addressing problem. components mixture successively merged pairs minimizing cost function. gaussian algorithm using homotopy avoid local minima suggested merging statistics greedy multiple target tracking discussed gaussian algorithm using clustering techniques proposed crouse presented survey gaussian algorithms west’s algorithm constraint optimized weight adaptation runnalls’ algorithm gaussian mixture reduction clustering compared detail. williams maybeck proposed using integral square error approach multiple hypothesis tracking context. distinctive feature method availability exact analytical expressions evaluating cost function gaussian mixtures. whereas runnalls proposed using upper bound kullbackleibler divergence distance measure original mixture density reduced form step reduction motivation choice upper approach involves components original mixture global approaches general computationally costly. hand since global properties original mixture taken account provide better performance. following propose global greedy method implemented efﬁciently. runnalls’ method global greedy algorithm minimizes fkld ||ˆp). unfortunately gaussian mixtures calculated analytically. runnalls uses analytical upper-bound used comparing merging hypotheses. upper bound dkl||ˆp) given used cost merging components merged component density. hence original global decision statistics dkl||ˆp) merging replaced local approximation obtain decision rule follows properties metric symmetry triangle inequality analytically tractable gaussian mixtures. williams’ method minimizes dise||ˆp) pruning merging hypotheses i.e. subsection ﬁrst illustrate behavior runnalls’ willams’ methods simple example. second provide brief discussion characteristics observed examples along implications. greedy approach number components mixture reduced time. applying procedure again desired number components reached. reduce number components decision made among types operations; namely pruning merging operations. operations considered hypothesis greedy denoted pruning pruning simplest operation reducing number components mixture density. denoted hypothesis sequel. pruning component mixture removed weights remaining components rescaled mixture integrates unity. example choosing hypothesis i.e. pruning component results reduced mixture merging merging operation approximates pair components mixture density single component type. denoted hypothesis sequel. general optimization problem minimizing divergence normalized pair mixture single component used purpose. choosing fkld cost function merging components leads moment matching operation. speciﬁcally hypothesis selected i.e. components chosen merged parameters merged component found minimizing divergence kernel single weighted component follows different types greedy approaches literature namely local global approaches. local approaches consider merging hypotheses general merging hypothesis provides smallest divergence dlocal||q) selected. divergence considers components merged neglects others. therefore methods called local. well-known examples local approaches given global approaches pruning merging operations considered. divergence original reduced mixtures i.e. dglobal||ˆp) minimized decision. decision criterion global solution density best reduced mixture respect fkld. similarly runnalls’ method would select best mixture reduction hypothesis considers merging hypotheses. example consider problem example williams’ method. original mixture reduced mixture written basically illustrated example fkld tendency towards merging operation matter separated components original mixture are. similarly runnalls’ method considers merging operations ruling pruning hypotheses start. signiﬁcant preference towards merging operation tends produce reduced mixtures signiﬁcant support regions original mixture negligible probability mass. called zero-avoiding behavior literature tendency preferable applications minimum mean square error based estimation. hand also lead loss important details original mixture e.g. mode might less desirable applications maximum posteriori estimation. applications pruning operation preferable alternative might preserve signiﬁcant information time keeping reasonable level overlap supports original reduced mixtures. example illustrated similar tendency toward merging appear case williams’ method speciﬁc cases must mentioned that example weights components same williams’ method would select prune component smaller weight. therefore tendency toward merging signiﬁcantly less williams’ method fkld runnalls’ method. also important mention that target tracking algorithms gm-phd ﬁlters mixtures components identical weights commonly encountered. williams’ method global greedy approach computationally quite expensive mixture densities many components. computational burden results following facts. reducing mixture components mixture components involves hyload calculating potheses. since computational mixtures components reducing mixture components mixture components computation complexity williams’ method. hand using upper bound runnalls’ method avoids computations associated components directly involved merging operation resulting computations reduction. another disadvantage williams’ method scale dimension nicely pointed example propose greedy global algorithm based give credit pruning operations avoid merging components mixture away other. method propose minimizes reduced mixture original mixture i.e. rkld. hence solve following optimization problem. using rkld cost function enable pruning avoid ever merging behavior runnalls’ method unless found necessary. illustrate characteristics cost function following examples. example consider problem example small. sufﬁciently close zero express dkl||p) using secondorder taylor series approximation around follows because equal therefore dkl||p) since dkl||p) minimized ﬁrst derivative dkl||p) also vanishes second derivative term given tedious straightforward calculations noting logarithm function monotonic also approximations given right hand sides equations upper bounds corresponding rklds. dkl||p) lower bound greater sufﬁciently large. dkl||p) −eh] plot function positive makes right hand side inﬁnity tends inﬁnity. consequently cost merging operation exceeds pruning hypotheses sufﬁciently large values. therefore component minimum weight pruned components sufﬁciently separated. rkld gaussian mixtures analytically intractable except trivial cases. therefore able rkld approximations necessary case fkld runnalls’ method. propose approximations rkld pruning merging operations following section. sections speciﬁcally tailored approximations cost functions pruning merging hypotheses derived respectively. proceeding further would like introduce lemma used derivations. lemma three probability distributions positive real numbers. following inequality holds here functions denote probability density function cumulative distribution function gaussian random variable zero mean unity variance respectively. substituting upper bound obtain dkl||p) proposed approximate divergence pruning component denoted rest paper. following illustrate advantage proposed approximation numerical example. example consider example mixture hypothesis figure exact divergence dkl||p) computed numerically crude approximation given proposed approximation shown different values exact divergence upper bound converge pruned component small overlapping probability mass component. bound brings signiﬁcant improvement crude bound amount overlap components increases. interested rkld approximations quantity different accuracy computational cost given sections v-b. simple upper bound compute bound dkl||p) follows dkl||p) variational approximation upper bound derived previous subsection rather loose particularly components other. replacement second term approximation function dkl. figure exact divergence dkl||p) simple approximation given proposed approximation plotted different values addition exact pruning divergence dkl||p) crude pruning upper bound proposed pruning upper bound plotted. substitute approximation proposed approximate divergence merging becomes dkl||p) wij× exp)) proposed approximate divergence merging components denoted rest paper. example consider example mixture exact divergence hypothesis dkl||p) computed numerically simple approximation given proposed approximation shown different values note range approximations interesting study dkl||p) log. fact dkl||p) pruning hypothesis selected. range axes figure chosen according aforementioned range interest. proposed variational approximation performs well compared simple upper bound grows fast increasing values. furthermore points curves exact pruning merging divergences cross close crossing point curves best approximation quantities figure approximate proposed work uses approximations pruning merging divergences refer algorithm approximate reverse kullback-leibler mra. precision arkl traded less computation time replacing approximations pruning merging hypotheses less precise counterparts given sections v-b. section performance proposed arkl compared runnalls’ williams’ algorithms examples. ﬁrst example real world data terrain-referenced navigation. second involved calculation integral grows much support shares signiﬁcant support similarly integral grows much support shares signiﬁcant support problems propose following procedure approximating using boundedness component densities. proposed variational approximation ﬁrst upper bound obtained using log-sum inequality integrands equal almost unity everywhere except support respectively shrink zero. excessive growth divergences upper bound reduced. note also terms qmax readily available maximum values gaussian component densities. finally minimum right hand side obtained respect optimization problem solved taking derivative respect equating result zero gives following optimal value figure data original mixture density courtesy andrew runnalls shows horizontal position errors terrain-referenced navigation. unit axes kilometers represented national grid coordinates. original mixture reduced using different mras. example runnalls used repeated comparison arkl runnalls’ williams’ algorithms. example gaussian mixture consisting components represents terrainreferenced navigation system’s state estimate reduced components. random variable whose distribution represented dimensional elements visualized marginal distribution visualized random variables illustrate differences mras. figure shows original mixture density. gaussian component represented mean weight proportional area marker representing mean percentile contour thick gray curves figure percentile contour thick black curves show percentile contour mixture density. williams’ algorithm algorithm preserves second mode percentile contour distorted percentile contour most. percentile contour arkl similar original density. also shape percentile contour well preserved arkl well runnalls’. interesting observation percentile contour arkl sharper corners obtained runnalls’ algorithm manifestation pruning decisions arkl opposed merging decisions runnalls’ algorithm small-weight components. mras compared quantitatively terms divergences fkld rkld table fkld rkld calculated using integration calculated using analytical expression. naturally mras obtain least divergence respective criteria. runnalls’ algorithm obtains lowest fkld williams’ algorithm obtains lowest arkl obtains lowest rkld. terms runnalls’ algorithm difference figures exclusion pruning hypotheses implementation williams’ algorithm fact using marginal distribution position error elements random variable. arkl seems close implies large weight components algorithms work similarly. terms fkld williams’ algorithm obtains much closer value runnalls’ algorithm arkl means merging properties williams’ algorithm closer runnalls’ algorithm. finally terms rkld williams’ algorithm runnalls’ algorithm similar larger values arkl. makes conclude pruning ability williams’ algorithm runnalls’ algorithm example. consider clustering problem number clusters known priori. also assume data corrupted small number spurious measurements data points belonging true clusters considered outliers. clustering task retrieve clusters corrupted data. straightforward approach problem neglect existence spurious measurements cluster corrupted data directly clusters. since solution take spurious measurements account resulting clusters might represent true data satisfactorily. another solution problem ﬁrst cluster data points clusters reduce number clusters used clustering appropriate used reduce number clusters prune components small weights distant rest probability mass good candidate. hand reduces mixture using merging operations preferable spurious measurements would corrupt true clusters. furthermore component pruned/discarded data points associated pruned component density also discarded potential spurious data points. consequently expected arkl williams’ algorithm might perform better runnalls’ algorithm example pruning capability. gaussian mixture components {xi}n simulation data points generated parameters given table data points augmented spurious data points sampled uniformly multivariate uniform distribution square region i.e. {yi}m center square origin side units long. clustering results following methods presented. figure contour plots densities along corresponding data points illustrated. figure contour plots true clusters plotted. true spurious data points shown figure figure contour plot obtained algorithm clustering corrupted data directly clusters shown corresponding cluster data points direct clustering corrupted data components results clustering capture true clusters since spurious cluster large covariance formed spurious data points remedy data ﬁrst clustered clusters using algorithm resulting component reduced components using runnalls’ algorithm williams’ algorithm arkl algorithms component pruned data points associated component discarded well. hand component merged another component data points associated merged component reassigned updated clusters according proximity measured mahalanobis distance. ﬁnal clustering obtained runnalls’ algorithm almost result obtained algorithm clusters. observation line maximum likelihood interpretation/justiﬁcation runnalls’ algorithm given williams’ algorithm arkl capture shape original clusters much better runnalls’ algorithm algorithms prune runnalls’ algorithm merge. spurious clusters/measurements merged true clusters williams’ algorithm almost spurious clusters/measurements pruned arkl. lastly aforementioned computation complexity williams’ method reducing mixture components mixture components. however runnalls’ arkl methods complexity makes preferable fast reduction algorithm needed reducing mixture many components. upper bound minimized respect obtain best upper bound. minimum obtained taking derivative respect equating result zero solving gives substituting gives upper bound investigated using reverse kullback-leibler divergence cost function greedy mixture reduction. proposed analytical approximations cost function results efﬁcient algorithm capable merging pruning components mixture. property help preserving peaks original mixture reduction missing fkld based methods. compared method well-known mixture reduction algorithms illustrated advantages simulated real data scenarios. hershey olsen approximating kullback leibler divergence gaussian mixture models proceedings ieee international conference acoustics speech signal processing vol. apr. iv––iv–.", "year": 2015}