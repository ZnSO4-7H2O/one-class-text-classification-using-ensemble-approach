{"title": "Invariant backpropagation: how to train a transformation-invariant  neural network", "tag": ["stat.ML", "cs.LG", "cs.NE"], "abstract": "In many classification problems a classifier should be robust to small variations in the input vector. This is a desired property not only for particular transformations, such as translation and rotation in image classification problems, but also for all others for which the change is small enough to retain the object perceptually indistinguishable. We propose two extensions of the backpropagation algorithm that train a neural network to be robust to variations in the feature vector. While the first of them enforces robustness of the loss function to all variations, the second method trains the predictions to be robust to a particular variation which changes the loss function the most. The second methods demonstrates better results, but is slightly slower. We analytically compare the proposed algorithm with two the most similar approaches (Tangent BP and Adversarial Training), and propose their fast versions. In the experimental part we perform comparison of all algorithms in terms of classification accuracy and robustness to noise on MNIST and CIFAR-10 datasets. Additionally we analyze how the performance of the proposed algorithm depends on the dataset size and data augmentation.", "text": "james bailey ramamohanarao kotagiri christopher leckie department computing information systems university melbourne parkville australia {baileyj kotagiri caleckie}unimelb.edu.au many classiﬁcation problems classiﬁer robust small variations input vector. desired property particular transformations translation rotation image classiﬁcation problems also others change small enough retain object perceptually indistinguishable. propose extensions backpropagation algorithm train neural network robust variations feature vector. ﬁrst enforces robustness loss function variations second method trains predictions robust particular variation changes loss function most. second methods demonstrates better results slightly slower. analytically compare proposed algorithm similar approaches propose fast versions. experimental part perform comparison algorithms terms classiﬁcation accuracy robustness noise mnist cifar- datasets. additionally analyze performance proposed algorithm depends dataset size data augmentation. showing neural networks widely used machine learning. example best results image classiﬁcation image labeling speech recognition. deep neural networks applied large datasets automatically learn huge number features allow represent complex relations input data output classes. however also means deep neural networks suffer overﬁtting different regularization techniques crucially important good performance. often case exist number variations given object preserve label. example image labels usually invariant small variations location image size angle brightness etc. area voice recognition result invariant speech tone speed accent. moreover predictions always robust random noise. however knowledge incorporated learning process. work propose methods achieving local invariance extending standard backpropagation algorithm. first enforces robustness loss function variations input vector. second methods trains predictions robust variation input vector direction changes loss function most. refer loss invariant backpropagation prediction ibp. faster demonstrates better performance. methods applied types neural networks combination regularization technique. denote number layers neural network activation vectors layer. activation ﬁrst layer input vector input image consists feature maps still consider vector traversing maps concatenating together. transformation layers might different convolution matrix multiplication non-linear transformation etc. assume weights empty. computation layer activations ﬁrst pass backpropagation algorithm. moreover loss function also considered layer length forward pass thus calculation composition functions applied input vector denote vectors derivatives respect layer values ∂l/∂yi dyi. then similar forward propagating functions deﬁne backward propagating functions dyi− ˜fi. refer reverse functions. according chain rule obtain matrix form backward pass i=k+ layer functions computed points yi−. note ﬁrst jacobian vector derivatives ∂l/∂yk loss function respect predictions last vector i=k+ contains derivatives loss function respect next also denote vector weight gradients ∂l/∂wi dwi. write chain rule matrix form jacobian matrix derivatives respect weights however linear function jacobian equivalent vector number techniques allow achieve robustness particular variations proposed. convolutional neural networks consist pairs convolutional subsampling layers commonly used one. provide robustness small shifts scaling also signiﬁcantly reduce number training parameters compared fully-connected perceptrons. however able deal types variations. another popular method data augmentation. assumes training objects artiﬁcially generated existing training using transformation functions. unfortunately generation always possible. exist approaches also attempt solve problem analytically using gradients loss function respect input. discuss below. ﬁrst approach tangent backpropagation algorithm allows train network robust predeﬁned transformations. authors consider invariant transformation function s.t. must preserve predictions within local neighborhood since predictions neighborhood must also constant necessary condition network last term depends function input value therefore computed advance. authors refer ∇θg|θ= tangent vectors. authors propose compute additional loss term initializing network tangent vector ∇θxt propagating linearized network i.e. consecutively multiplying transposed jacobians main drawback tangent computational complexity. seen deﬁnition linearly depends number transformations classiﬁer learns invariant authors describe example training network image classiﬁcation robust transformations translations scalings rotation. case required learning time times larger standard usage tangent vectors also makes tangent difﬁcult implement. achieve this authors suggest obtain continuous image representation applying gaussian ﬁlter requires additional preprocessing hyperparameter basic transformation operators given simple operators transformations require additional coding. second algorithm recently proposed adversarial training authors described interesting phenomena possible artiﬁcially generate image indistinguishable image dataset trained network’s prediction completely wrong. course people never make kinds mistakes. objects called adversarial examples. authors showed possible generate adversarial examples moving direction given loss function gradient ∇xl) i.e. high dimensional space even small move signiﬁcantly change loss function deal problem adversarial examples authors propose algorithm adversarial training idea algorithm additionally train network adversarial examples quickly generated using gradients ∇xl) obtained backward pass. adversarial training uses labels object original object loss function same. updated loss function thus adversarial training quite similar tangent propagation algorithm differs couple aspects. first adversarial training uses gradients loss function ∇xl) tangent uses tangent vectors ∇θxt second adversarial training propagates objects original network tangent propagates gradients ∇θxt linearized network. proposed prediction algorithm also derived combining properties. network predictions robust variation direction speciﬁed ∇xl). versions gradients ∇xl) differ loss functions computational complexity also experimental results. many classiﬁcation problems large number features. formally means input vectors come high dimensional vector space. space every vector move huge number directions change vector’s label. goal algorithm make classiﬁer robust variations. consider k-layer neural network input predictions using vector true labels compute loss function backward pass backpropagation algorithm obtain vector gradients i=k+ vector deﬁnes direction changes loss function length speciﬁes large change small neighborhood assume ∇xlt ∇xl) small change cause smaller change thus smaller vector length corresponds robust classiﬁer vice versa. specify additional loss function note similar frobenius norm jacobian matrix used regularization term contractive autoencoders minimization encourages classiﬁer invariant changes input vector directions known invariant. time minimization ensures predictions change move towards samples different class classiﬁer invariant directions. combination loss functions aims ensure good performance. order minimize joint loss function need additionally obtain derivatives additional loss function respect section discuss efﬁciently compute them using weights additional forward pass. derivatives computed update weights using rule coefﬁcient controls strength regularization plays crucial role achieving good performance. note algorithm equivalent standard backpropagation. since additional loss function aims minimize gradients main loss function ∇xl) call algorithm loss ibp. loss makes main loss function robust variations necessarily imply robustness predictions themselves. unfortunately cannot compute gradients predictions respect input vector dimensionality large. however compute gradients predictions direction given ∇xl). shown section movement direction generate adversarial examples whose predictions signiﬁcantly differ thus introduce another additional loss function call algorithm loss function prediction ibp. difference prediction tangent initial vector third pass. tangent uses precomputed tangent vectors prediction uses vector gradients ∇xl) obtained backward pass. weight gradients additional loss function computed computed tangent therefore prediction always requires times computation time standard section show efﬁciently compute weight gradients additional loss function optimize need look backward pass another point view. consider derivatives ﬁrst layer reverse neural network output. indeed transformation functions reverse pairs used propagate derivatives consider pairs original transformation functions inverse pairs ˜˜fi. therefore consider derivatives activations backward pass forward pass reverse network. standard backpropagation forward pass compute loss function next step quite natural need initialize input vector perform another backward pass direction gradients original forward pass. time derivatives respect weights must computed. fig. shows general scheme derivative computation. part corresponds standard backpropagation procedure. important subset transformation functions linear functions. includes convolutional layers fully connected layers subsampling layers types. section show function linear i.e. therefore case linear function propagate third pass activations ﬁrst pass i.e. multiplying matrix weights statement remains true element-wise multiplication considered matrix multiplication well. weight derivatives also computed standard algorithm. fact allows easily implement loss using procedures standard moreover section also show function symmetric jacobian ˜˜fi ˜fi. property useful implementation non-linear functions. summary loss algorithm given algorithm easy compare computation time standard loss ibp. know convolution matrix multiplication operations occupy almost processing time. needs forward pass calculation weight gradients. assume layer forward pass backward pass calculation derivatives take approximately time requires time train network. experiments shown additional time less approximated versions contain ﬁxed time procedures batch composing data augmentation etc. time loss faster prediction approximately therefore directly computed backward pass multiplying gradient tangent vector ∇θx. section show modiﬁcation tangent equivalent loss additional loss function instead therefore version tangent implemented using less time easy notice usage lmin instead scales hyperparameter needs tuned anyway. time calculation gradients takes computation time. therefore adversarial training algorithm sped avoiding calculation using gradients compared originally proposed loss lmin optimal parameter must times lower. similar tangent trick also saves difference loss adversarial training. loss minimizes ﬁrst derivative affect higher orders derivatives loss functions curvature adversarial training essentially minimizes orders derivatives ∂nl)/∂nx predeﬁned weight coefﬁcients them. case highly nonlinear true data distribution might disadvantage. section show none algorithms outperform another cases. experimental part compared algorithms modiﬁcations different aspects. performed experiments benchmark image classiﬁcation datasets mnist cifar- using convnet toolbox matlab experiments used following parameters batch size initial learning rate momentum exponential decrease learning rate i.e. convolutional layer followed scaling layer aggregation function among region size stride relu nonlinear functions internal layers ﬁnal softmax layer combined negative log-likelihood loss function. trained classiﬁers epochs coefﬁcient ﬁnal learning rate experiments mnist employed network convolutional layers ﬁlters size ﬁlters size internal layer length experiments cifar performed network convolutional layers ﬁlter size internal layer length experiments used l-norm additional loss function found always works better l-norm. tangent algorithm used tangent vectors image training corresponding shifts scaling rotation. employed value standard deviation gaussian ﬁlter numerical stability reasons omitted multiplication softmax gradients additional forward backward passes prediction original algorithms. first compared performance algorithms modiﬁcations. trained networks different subsets mnist cifar- datasets size different initial weights shufﬂing order. dataset ﬁrst normalized pixel values within mean pixel value subtracted images. results presented table table mean errors best parameters computation time epoch mnist cifar- datasets standard backpropagation version invariant backpropagation adversarial training tangent backpropagation each. cifar- first algorithms except fast decrease classiﬁcation error compared standard suppose lack improvement fast explained weak connection behavior loss function predictions themselves. trained robust predeﬁned transformations predictions might remain sensitive them. discuss original tbp. second notice original fast demonstrate identical performance thus conﬁrming suggestion possibility speed algorithm. achieved speed also best parameters mnist datasets differ times also predicted considerations. differentiate original fast refer third conclude prediction shows better results loss slightly slower since prediction seen modiﬁcation original loss equivalent modiﬁcation fast reason might also weak connection forth observe algorithms demonstrate different performance mnist cifar- datasets. best results mnist achieved prediction best result cifar- achieved tangent notice improvement tangent cifar- dataset much larger next best result prediction time algorithm could improve accuracy achieving best accuracy using lowest possible value parameter however tangent algorithm works much slower competitors. suppose results explained high non-linearity decision function. shown section minimizes ﬁrst order loss function derivatives also orders thus preventing classiﬁer learning non-linearity. time prediction makes predictions less sensitive variations input vector speciﬁed ∇xl). case highly non-linear decision function might necessary. unlike tangent uses prior knowledge train invariance directions predictions must always invariant allows achieve best performance cifar-. next measured sensitivity algorithms adversarial noise. employed classiﬁers trained section parameters yield best accuracy measured performance classiﬁers test sets corrupted adversarial noise. adversarial examples generated using results presented fig. show errors variation important keep mind performance classiﬁers signiﬁcantly depends value regularization parameter. firstly notice cifar- classiﬁers much sensitive adversarial noise trained mnist dataset. expected robust classiﬁer trained adversarial training algorithm. constantly remains better standard classiﬁer. classiﬁers show better results certain point level noise becomes high. interestingly tangent demonstrates best results cifar- dataset performance degrades much faster performance classiﬁers mnist cifar-. note despite ratio best values prediction loss cases demonstrate different behavior. measured sensitivity classiﬁers gaussian noise. results presented fig. surprisingly robust classiﬁer mnist dataset trained standard thus robustness adversarial noise predeﬁned transformations makes classiﬁer sensitive gaussian noise. time tangent classiﬁer remains sensitive gaussian noise well. cifar- dataset classiﬁer degrades signiﬁcantly faster others. also established dataset size data augmentation affects prediction improvement. performed experiments subsets mnist dataset using parameters section data augmentation regime randomly modiﬁed training object every time accessed according following parameters range shift central position dimension pixels range scaling dimension range rotation angle degrees pixel value pixel original image order decrease variance trained networks epochs without data augmentation epochs results summarized fig. without data augmentation smaller datasets require regularization i.e. larger relative improvement also higher samples thus larger dataset less network overﬁts less improvement obtain regularization. data augmentation improvement less converge even full training used. interestingly optimal value remains approximately level dataset sizes. therefore conclude data augmentation cannot completely substitute regularization last enforces robustness variations represented additionally generated objects. proposed versions invariant backpropagation algorithm extends standard backpropagation order enforce robustness classiﬁer variations input vector. loss trains main loss function insensitive variations prediction trains predictions insensitive variations direction gradient ∇xl). demonstrated weight gradients loss efﬁciently computed using additional forward pass identical original forward pass majority layer types. experimentally established prediction achieves higher classiﬁcation accuracy mnist cifar- datasets requires time loss ibp. additionally proposed fast versions tangent adversarial training algorithms. fast version tangent improve classiﬁcation accuracy modiﬁcation adversarial training algorithm demonstrates performance originally proposed algorithm faster. experimental part performed comparison algorithms modiﬁcations terms classiﬁcation accuracy robustness noise. found none algorithms outperforms others cases. best results mnist achieved prediction adversarial training tangent signiﬁcantly outperformed others cifar-. time tangent classiﬁer sensitive gaussian adversarial noise datasets. additionally demonstrated regularization effect prediction remains visible even full size mnist dataset data augmentation methods applied together. choice particular regularizer depends properties dataset. simard patrice lecun yann denker john victorri bernard. transformation invariance pattern recognition–tangent distance tangent propagation. neural networks tricks trade springer szegedy christian zaremba wojciech sutskever ilya bruna joan erhan dumitru goodfellow fergus rob. intriguing properties neural networks. arxiv preprint arxiv. szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. arxiv preprint arxiv. first notice forward backward passes loss performed standard backpropagation algorithm. additional loss function computed derivatives used input propagation third pass. follows gradients consider double reverse functions d˜yi ˜˜fi. compared linear reverse function multiplies ﬁrst argument transposed parameter. true double reverse function compared i.e. consider backward pass forward pass reverse net. since function linear formula derivative calculation reverse also however follows reverse uses transposed matrix weights forward propagation result derivative calculation also transposed respect matrix also note since acts activations reverse pass ﬁrst argument d˜yi− second. therefore proof. indeed know reverse function linear argument multiplied jacobian theorem also know reverse linear function multiplies argument transposed weights i.e. d˜yi d˜yi− therefore jacobian symmetric ˜˜fi ˜fi. fully connected layer standard linear layer transforms input multiplication matrix weights vector biases. notice backward pass bias propagate derivatives third pass well compute additional bias derivatives. difference ﬁrst third passes. dropout used third pass dropout matrix used ﬁrst pass. non-linear activation functions considered separate layer even usually implemented part layer type. contain weights write common functions sigmoid rectiﬁed linear unit symmetric jacobian matrix according theorem third pass backward pass. example case relu function means d˜yi d˜yi− element-wise multiplication used. convolution layers perform ﬁltering activation maps matrices weights. since element linear combination elements convolution also linear transformation. linearity immediately gives ˜˜fi dyi. therefore third pass convolutional layer repeats ﬁrst pass i.e. performed d˜yt convolving d˜yi− ﬁlters using stride padding. fully connected layers biases resulting maps compute derivatives. scaling layer aggregates values region single value. typical aggregation functions mean max. follows deﬁnition also perform linear transformations d˜yi notice case function means third pass elements d˜yi− chosen propagation d˜yi ﬁrst pass regardless value have. common loss functions predictions true labels squared loss applied number neurons output layer yk−. ﬁrst case second case show ∇yk− therefore strength ˜l-function loss regularization decreases predictions approach true labels property prevents overregularization classiﬁer achieves high accuracy. notice network hidden layers ∇xyk− i.e. case ||∇xl|| penalty term considered weight decay regularizer multiplied model single neuron derive another interesting property. demonstrated single neuron l-norm loss function noise injection equiva regularization. section show negative log-loss lent weight decay ||w|| function used noise injection becomes equivalent loss regularizer. notice last expression uses instead however expressions equal. therefore negative log-likelihood function applied single neuron without non-linear transfer function gaussian noise added input vector equivalent regularization term ||∇xl|| result supported discussion fawzi authors show linear classiﬁer robustness adversarial examples bounded robustness random noise. however since expected value quality approximation also depends number iterations. section showed gradient ∇θl))|θ= efﬁciently computed multiplying gradient ∇xl) obtained backward pass tangent vector ∇θx. demonstrate loss additional loss function ·∇θx equivalent fast tangent additional loss function fast tangent perform additional iteration backpropagation linearized network applied tangent vector ∇θx. additional forward pass computes following values", "year": 2015}