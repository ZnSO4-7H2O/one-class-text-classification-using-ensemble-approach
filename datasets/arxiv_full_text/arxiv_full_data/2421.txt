{"title": "On the Convergence Properties of Optimal AdaBoost", "tag": ["cs.LG", "cs.AI", "stat.ML", "68Q32 (Primary) 68T05, 37A99 (Secondary)", "I.2.6"], "abstract": "AdaBoost is one of the most popular machine-learning algorithms. It is simple to implement and often found very effective by practitioners, while still being mathematically elegant and theoretically sound. AdaBoost's behavior in practice, and in particular the test-error behavior, has puzzled many eminent researchers for over a decade: It seems to defy our general intuition in machine learning regarding the fundamental trade-off between model complexity and generalization performance. In this paper, we establish the convergence of \"Optimal AdaBoost,\" a term coined by Rudin, Daubechies, and Schapire in 2004. We prove the convergence, with the number of rounds, of the classifier itself, its generalization error, and its resulting margins for fixed data sets, under certain reasonable conditions. More generally, we prove that the time/per-round average of almost any function of the example weights converges. Our approach is to frame AdaBoost as a dynamical system, to provide sufficient conditions for the existence of an invariant measure, and to employ tools from ergodic theory. Unlike previous work, we do not assume AdaBoost cycles; actually, we present empirical evidence against it on real-world datasets. Our main theoretical results hold under a weaker condition. We show sufficient empirical evidence that Optimal AdaBoost always met the condition on every real-world dataset we tried. Our results formally ground future convergence-rate analyses, and may even provide opportunities for slight algorithmic modifications to optimize the generalization ability of AdaBoost classifiers, thus reducing a practitioner's burden of deciding how long to run the algorithm.", "text": "adaboost popular machine-learning algorithms. simple implement often found effective practitioners still mathematically elegant theoretically sound. adaboost’s behavior practice particular test-error behavior puzzled many eminent researchers decade seems defy general intuition machine learning regarding fundamental trade-off model complexity generalization performance. paper establish convergence optimal adaboost term coined rudin daubechies schapire prove convergence number rounds classiﬁer itself generalization error resulting margins ﬁxed data sets certain reasonable conditions. generally prove time/per-round average almost function example weights converges. approach frame adaboost dynamical system provide sufﬁcient conditions existence invariant measure employ tools ergodic theory. unlike previous work assume adaboost cycles; actually present empirical evidence real-world datasets. main theoretical results hold weaker condition. show sufﬁcient empirical evidence optimal adaboost always condition every real-world dataset tried. results formally ground future convergencerate analyses even provide opportunities slight algorithmic modiﬁcations optimize generalization ability adaboost classiﬁers thus reducing practitioner’s burden deciding long algorithm. wants place broad impact overall signiﬁcance adaboost perspective following quote hard beat. forms part statement acmsigact awarding committee g¨odel prize presented yoav freund robert schapire creators adaboost original paper introduce algorithm established theoretical foundations properties algorithm demonstrated novel possibilities analysing data permanent contribution science even beyond computer science. because combination features including elegance simplicity implementation wide applicability striking success reducing errors benchmark applications even theoretical assumptions known hold algorithm explosion research ﬁelds statistics artiﬁcial intelligence experimental machine learning data mining. algorithm widely used practice. late eminent statistician breiman called adaboost best off-the-shelf classiﬁer wide variety datasets fifteen years later adaboost still widely used simplicity speed theoretical guarantees good performance reinforcing essence quote above. however despite overwhelming popularity still mystery surrounding generalization performance stated slide presented introduction here breiman considered question adaboost performs well general important open problem machine learning. adaboost stands adaptive boosting meta-algorithm works rounds sequentially combining called base weak classiﬁers round fig. presents algorithmic description adaboost binary classiﬁcation. implicit function weaklearn called weak hypothesis class functions input features binary outputs belongs. case weak hypothesis weaklearn returns achieves minimum weighted-error respect among hypothesis resulting algorithm becomes called optimal adaboost term coined rudin along attempts explain posit views subject present contributions argue signiﬁcance within context state-of-the-art area section places work within machine learning. section places work particularly within context existing work variants adaboost work actually related article. section starts introduce mathematical preliminaries needed state prove convergence results. section formulates adaboost dynamical system. section establishes time-average/per-round convergence optimal adaboost classiﬁer along many quantities margins generalization error mild condition provide substantial empirical evidence real-world datasets. section provides formal proofs mathematical tool used obtain convergence results including lower bound weighted-error weak-hypothesis optimal adaboost obtains weak learner every round section presents empirical results several real-world datasets provide substantial evidence adaboost-cycle conjecture favor no-ties condition employ. section provides closing remarks including summary discussion results open problems left future work. shown fig. round adaboost adds hypothesis generated weak learning algorithm running linear combination hypotheses. common intuition would suggest complexity combination hypotheses would increase longer algorithm runs. meanwhile practice generalization performance ensemble tends improve remain stationary large number iterations. behavior goes general theoretically-inspired intuition accumulated knowledge machine learning complexity model increases appears case surface least adaboost classiﬁer generalization error also increases. behavior contradict standard theoretical bounds based vc-dimension adaboost classiﬁers suggest seems futile attempt apply standard view context. cases generalization error continues decrease long training error corresponding adaboost classiﬁer reached zero vc-dimension based bounds cannot really explain behavior fact behavior seems generally inconsistent fundamental nature bounds insights gained computational learning theory. common graph depicting behavior fig. remarkably complicated combination trees generalizes better simpler combination training test error adaboosting letter figure dataset rounds plot originally appeared schapire still featured many tutorials talks without deﬁnitive formal explanation. refer reader breiman grove schuurmans experimental evidence max-margin theory originally forward explanation behavior schapire believe contribution convergence properties adaboost discussed later provides rather convincing formal explanation behavior. heart-disease dataset test error. test error adaboosting decision-stumps heartdisease dataset rounds note converging behavior exhibited letter data converging behavior typically observed empirical studies adaboost. note however time unlike previous/canonical ﬁgure adaboost seems overﬁtting. margin explanation ingenious work shapire et.al. derived upper bound error rate convex combination predictors terms dimension predictor ensemble margin distribution. solving apparent paradox partially described above driving force behind boosting research various explanations proposed popular among theory margins generalization error convex combination functions bounded function margins training examples independent number classiﬁers ensemble. adaboost provably produces reasonably large margins tends continue improve margins even training error reached zero margin theory effective explaining adaboost’s generalization performance high level. still downsides breiman’s quote indicates. evidence power margin theory predict quality generalization performance striking problem margin bound loose explain precise behavior error. example looking fig. couple questions arise. test error ﬂuctuating wildly underneath bound induced margin? even test error approaching bound? remarkably error neither things seems converge stationary value. phenomenon unique dataset. convergence seen many different datasets natural synthetic. even cases adaboost seems overﬁtting generalization performance tends stabilize. take example fig. ﬁrst rounds appears algorithm overﬁtting. unfortunate breiman’s notation consistent traditionally used presentation adaboost machine learning community. example number samples training datatset used denote training dataset number samples. similarly number rounds adaboost used quoting slide using notation context paper. breiman conjectured adaboost ergodic dynamical system. argued case dynamics weights examples behaves like selecting probability distribution. therefore adaboost treated random forest. using strong large numbers follows generalization error adaboost converges certain weak learners. following quote breiman section sub-section technical report mostly superseded breiman proved convergence optimal adaboost classiﬁer itself convergence bayes-error risk class l-measurable functions respect probability measure condition inﬁnite amount data. unfortunately breiman stated manuscript fundamental ointment trying transfer result ﬁnite-size datasets. theoretical results indicate sample size goes inﬁnity generalization error adaboost converge bayes risk. data sets adaboost converge. instead it’s behavior resembles ergodic dynamical system. mechanism producing behavior understood. also conjecture equalization property gives adaboost ergodicity. consider ﬁnite number classiﬁers {ht} associated misclassiﬁcation iteration lowest weight weight increased instances complement weights decreased. thus selected moves weight heap move reach bottom heap bounced top. cycling among produces ergodic behavior. owever understand connection ﬁnite sample size equalization goes inﬁnity case. equalization combined ergodicity produces generalization error major unsolved problem machine learning. ﬁrst note empirical evidence favor breiman’s observation experiments. logarithmic growth number decision stumps partially presented fig. suggests observation least questionable. addition although prove optimal adaboost measure-preserving dynamical system make birkhoff’s ergodic theorem suggests form ergodic behavior could prove optimal adaboost ergodic dynamical system breiman conjectured last paragraph. turns need condition establish convergence number round increases ﬁnite datasets condition no-ties best weak-classiﬁer/hypothesis relative weighted error respect example weights round sufﬁcient. note description optimal adaboost considered here. note also attempted make notation consistent traditional notation machine learning literature here. convergence adaboost classiﬁer itself mild conditions also turned rather universal characteristic independent least directly dependent hypothesis class weak learner uses pleasant surprise. adaboost exhibits tendency resist overﬁtting many datasets others found overﬁt several others real-world datasets test error heart-disease dataset fig. example. train-test error plot parkinson dataset plot second ﬁrst column fig. provides another example overﬁtting behavior real-world data even though still stability test error. large empirical evidence stable behavior little theoretical understanding stability existed work. believe stability also help explain cases adaboost resist overﬁtting. adaboost appear resistant overﬁtting converges stable behavior small number iterations. paper follow similar general approach pioneered rudin sense frame adaboost dynamical system. however apply different techniques mathematical tools. using mathematical tools real-analysis measure theory establish sufﬁcient conditions invariant measure dynamical system. require measure ergodic weaker breiman’s requirement. using tools ergodic theory show measure implies convergence time/per-round average function weights examples. particular general result show margin every example converges. also show adaboost classiﬁer converging weak learner satisﬁes certain conditions; provide empirical evidence favor conditions holding high-dimensional real-word datasets. ultimately prove main result asymptotic stability generalization error. important emphasize using different tool ergodic theory birkhoff ergodic theorem used rudin able easily extend convergence results condition studied adaboost cycle. signiﬁcant also provide convergence results non-cyclic chaotic conditions. last case introduce technical condition grew experience running adaboost high-dimensional real-world datasets. delay technical deﬁnition condition related concepts needed technical sections. here technical condition essentially states optimal adaboost eventually round large enough number rounds face ties selection best weak hypothesis ﬁnite subclass effective representative hypotheses roughly round summary several fundamental questions remained open result work presented here adaboost classiﬁer converge? generalization error converge? provide positive answers questions long explain stable behavior practice. results also start endeavor study rates convergence happens seems quite fast practice. reasonably strong positive answers fundamental long-standing questions stated more. many years answers questions remained wide open results help explain behavior adaboost consistently observe know result reasonable condition guarantees convergence actual adaboost classiﬁer generalization error cyclic non-cyclic chaotic behavior except ours. result provides more reasonable mild condition result extends convergence time-average almost arbitrary function weights pretty remarkable property adaboost explains converging behavior test error many community seeing practice since creation learning algorithm. condition results hold reasonable given empirical evidence seen know reasonable non-trivial condition achieves same. also know simple non-trivial example clearly falsiﬁes condition. interested trivial synthetic low-dimensional examples/cases know must exist. instead interest real-world high-dimensional datasets practical implications. ultimate goal convergence rates coming machine learning computer science perspective nature emphasize study ﬁnite running times space computations. ideally ultimate goal derive convergence rates adaboost classiﬁer generalization error. mention section knew something asymptotic convergence behavior mostly variants adaboost respect different loss functions versions algorithm optimization proxy training error. forget convergence rates knew almost nothing convergence properties simple original version optimal adaboost classiﬁer generalization error foundational boosting algorithm widespread effective practical use. given state affairs work presented paper because need start somewhere recognize usefulness asymptotic analysis initial important signiﬁcant step toward better understanding adaboost. cannot convergence rates without convergence work known whether adaboost classiﬁer generalization error would converge reasonable conditions. know does move theoretical empirical analysis convergence rates. community deﬁnitely ignorant relevance scientiﬁc signiﬁcance study asymptotic convergence properties iterative-based algorithms ﬁnite size datasets. statistical community went long period least decade analysis proof asymptotic convergence expectation-maximization algorithm many variants forms non-linear optimizations neural-networks community similarly studies asymptotic potentially convergent behavior gradient-based iterative-update algorithms asymptotic convergence properties viewed feature practical applications provide degree certainty behavior learning algorithm despite often knowing much convergence rate’s guarantees asymptotically ﬁnite number updates rounds. hence nature work results present manuscript well within focus interest community study asymptotic behavior core/fundamental/central many areas within community. improved non-asymptotic large-deviation data-dependent uniform-convergence bounds performance guarantees adaboost classiﬁers section addition also remind reader establish asymptotic convergence believe mild condition refer no-ties condition informally discussed next paragraph formally stated technical sections. also another quite reasonable condition convergence generalization error mainly decision boundary adaboost classiﬁer measure zero. also employ no-ties condition without formal proof adaboost almost-always satisﬁes thus theoretically unproven provide considerable empirical evidence favor section held high-dimensional real-world datasets tried also found natural given practical experience adaboost. reasons despite fully meeting ultimate goal manuscript believe work results actually presented here taken whole within context previous work state-of-the-art adaboost signiﬁcant community. suspect contributions would already particular interest computational-learning-theory community least. seminal work rudin leaves formal theoretical study potential proof adaboost-cycles conjecture/condition future work. no-ties condition. also rudin work convergence rates adaboost-cycles condition. similarly leave study convergence rates no-ties condition open problem community. statistical consistency asymptotic property statistical estimator model parameter characteristic value converge corresponding exact/true value respect speciﬁc class models training-examples generated number training examples increases inﬁnity least roughly speaking believe last statement captures essence/nature standard/traditional deﬁnition consistency statistics. hand interested asymptotic behavior adaboost number rounds increases ﬁxed training-data sizes. several reasons interest. first number rounds adaboost something practitioner control. rarely case amount data. second often know hypothesis class/class models unknown ground-truth model supposedly generated data belongs. equally often recovering true model main interest care learn models perform well respect measure interest misclassiﬁcation error classiﬁcation mean-squareserror regression regardless whether model recovers ground-truth model data collected. again practice pretending know certainty class ground truth model comes wishful thinking view. statistics revolutionized theory practice certainly statistical concepts extremely useful tools within context fundamental nature work presented paper however statistical consistency plays role. that rudin provide evidence cycling carefully constructed synthetic often low-dimensional settings also show adin obvious example question open know thus theoretically unproven. core work computational complexity. also foundation immense number computational-complexity results problems almost every area science engineering. condition employ course even close level relevance using unproven question example help emphasize point simple fact something theoretically unproven unveriﬁed conjecture oxymoron diminish signiﬁcance disqualify resulting work basis. aboost exhibit chaotic non-cyclic behavior randomly-generated synthetic highdimensional datasets. provide empirical evidence adaboostcycles condition high-dimensional real-world datasets manuscript. remind reader alone introducing technical conditions analysis optimal adaboost varying degree evidence support them technical empirical. related work rudin formally empirically show conditions carry unrealistic setting realistic also seek results would state something along lines adaboost cycles several conditions hold adaboost maximizes margins. interest. stated previously maximizing margin adaboost context turn good idea work established stability generalization error adaboost certain weak conditions. frame adaboost dynamical system provide sufﬁcient conditions existence invariant measure. generally prove time average practically function weights examples converges. weak learner satisﬁes common conditions generalization error stable. what ultimate signiﬁcance results statistics communities could argue surely interesting prove learned classiﬁer converges sense limit inﬁnitely many iterations. also interesting show generalization performance well per-sample margins tend limits. still wonder useful? theoretical results asymptotic behavior adaboost ﬁnite datasets signiﬁcant practical implications. prior theoretical justiﬁcation stable converging behavior adaboost consistently figure results single random training-test data split breast cancer wisconsin parkinsons sonar spambase data sets ﬁgure shows train test error number unique decision stumps selected number round shown log-scale value weighted error weak/base classiﬁer found round function number rounds spambase pair examples input different output; note approaches error number rounds case. training test errors appear stable still logarithmic growth number unique stumps selected. taken together empirical results suggest adaboost cycling real-world datasets easy dimiss general behavior exhibited plots simply result numerical errors. reasonably certain case took every precaution avoid errors calculations. believe experimental results provide empirical evidence open problem whether adaboost cycles exhibited practical applications now. results provide previously unavailable formal guarantees adaboost would stable high-dimensional real-world datasets. practitioners formal guarantees soundly grounded theory adaboost classiﬁer generalization error example margins similarly important quantities would unstable behave chaotically number rounds increase. even know whether process learning method even converges begin analyze converging behavior convergence rates reach classiﬁer generalization error? understanding convergence properties requires establishing convergence even possible utimportance. establishing convergence considerably signiﬁcant certainly non-trivial contribution given result concentrate understanding convergence rates optimal adaboost generalization error among important characteristics without worry pursuing impossible task certain establishing convergence rates possible. thus contribution establishing type convergence actual adaboost classiﬁer generalization error concrete important ﬁrst step. useful foundational theoretical practical study adaboost. study behavior adaboost seems nature fundamental scientiﬁcally interesting endeavor venture guess reading relevant literature also members related communities. summary statistics community results relevant given emphasis consistency deals different type asymptotics happens number samples sometime addition number rounds goes inﬁnity? hand believe community intended audience work main audience journal results interest type asymptotics study happens number rounds goes inﬁnity number training examples remain ﬁxed? last question seems relevant former study type asymptotics last question considers common quite time. breiman’s stated wald lecture slide quoted beginning manuscript adaboost hundreds thousands papers. years vast literature overwhelming newcomers. thus scope paper mention include every single paper adaboost. likely even papers would appear right submission which needless unaware time submission. begin discussing existing literature time writing forms convergence least state-of-the-art variants understand discuss believe relevant related literature previous results concerns results presented manuscript. also work context present discuss existing literature research work. speciﬁc ﬂurry convergence results adaboost within last years believe result unique. discuss section number convergence results concern variants adaboost regularized boosting. convergence results concern adaboost original form show convergence aspects algorithm exponential loss. walk recent research adaboost. bulk asymptotic analysis adaboost focused minimizes different types loss functions emphasis exponential loss. breiman others demonstrated view adaboost coordinate-descent algorithm iteratively minimizes exponential loss called weak-learning assumption formally state context condition section minimization procedure well understood fast convergence rate exponential loss upper-bound misclassiﬁcation error rate training dataset goes zero exponentially fast. later collins zhang showed adaboost minimizes exponential loss even without weak-learning assumption unrealizable non-separable case provide convergence rates. finally mukherjee proved adaboost enjoys rate polynomial telgarsky achieves similar result exploring primal-dual relationship implicit adaboost. results concern convergence several types loss functions exponential loss variants perhaps receiving attention. even recent work telgarsky deals convergence terms variety loss functions classiﬁer generalization error. note work came attention completed work presented here completed writing ﬁrst version manuscript december submitted review posted online arxiv repository technical report send telgarsky himself. rounds adaboost simultaneously letting number training examples inﬁnity. perspective dataset training examples inﬁnite size inﬁnite number training examples disposal statistician often called version adaboost population version calling version considers ﬁnite training examples sample version. slightly technical often addition letting work concerns consistency speciﬁcally statistical consistency various forms asymptotic behavior adaboost. number papers show variants adaboost consistent bickel recently bartlett traskin shows adaboost consistent stopped time number examples training set. consistency inherently statistical concept distinct notion convergence paper. also various notions consistency. context adaboost literature bayes-consistency predictor particular interest. statistics algorithm predictor estimator bayes-consistent produces hypothesis whose generalization error approaches bayes risk limit number examples training dataset; said differently study statistical consistency nature condition inﬁnite-size training datasets concern convergence generalization error produced hypothesis limit number iterations algorithm ﬁxed-size training dataset. related work presented brieﬂy discussed work related convergence loss functions convergence versions adaboost population version course would like start section clarifying facts regarding state-of-the-art formally proven prior paper regarding convergence properties adaboost number rounds ﬁnite sample case. here contrast state-of-theart work contributions presented paper. start discussion previous results reminding reader interest margin maximization paper reasons previously discussed. using approach prove convergence margins necessarily leading maximum-margin solution similar statements support vectors adaboost cycles. also prove cases adaboost cycle need no-ties condition prove results adaboost cycles mean adaboost cycling implies no-ties condition adaboost-cycles assumption apply birkhoff ergodic theorem slightly different way. fact adaboost cycling easy case respect formal mathematical approach application birkhoff ergodic theorem becomes straightforward fact easy case theorem. even need theorem case adaboost cycling hence using approach easily extend convergence margins behavior related support vectors rudin established. signiﬁcantly more relatively easier cycling case using approach easily establish convergence adaboost classiﬁer itself generalization error time-averaged function l-measurable function weights simple application birkhoff ergodic theorem. class l-measurable functions large importantly contains standard/typical quantities involved adaboost algorithm selected weak-hypothesis errors αt’s weak-hypothesis/error-matrix-dichotomy selector prove paper among many others. safely investigate different types histograms functional characteristics shed light behavior adaboost practice. notation used rudin called mistake matrix syntactically different used here. notation element matrix equals depending whether hypothesis indexed correctly incorrectly classiﬁes example respectively. case replace elements transpose mistake matrix; values mistake matrix depending whether hypothesis indexed correctly incorrectly classiﬁes example respectively. regret change notation semantic differences found syntactic changes notation prove extremely convenient simplifying presentation technical results proofs. remind reader convergence results cover non-cyclic chaotic conditions stated earlier. also stated earlier introduce technical condition grew experience running adaboost high-dimensional real-world datasets. already stated condition informally introduction; section presents formal statement. importantly provide concrete empirical evidence validity condition section contrary might appear readers experimental observations suggest quantities αt’s selector’s great value signiﬁcance. experience suggests quantities central quest understand self-control resistance overﬁtting often practice. hence establishing convergence quantities fundamental analysis kind others involving aspects convergence rates; closer potential answer worked well mystery part question breiman posited slide quoted beginning article. section introduce basic concepts throughout article. refer reader appendix brief overview other general mathematical concepts technical results. denote feature space output labels. simplify notation possible input-output pairs. want learn given ﬁxed dataset training examples input-output pair examples make standard assumption example comes probability space outcome space possible events respect probability measure mapping convenience denote dataset input examples training dataset also convel={x} standard mments potentially selected weak hypotheses. impose following natural conditions rarely used conditions prove useful speciﬁc parts analysis regarding continuity certain functions related optimal adaboost important example-weight update performed round algorithm. conditions essential main function avoid dealing discontinuities probability distributions m-simplex weighted error hypothesis zero. theorem stated later shows implementation optimal adaboost consistent standard implementations example-weights update stays away discontinuities establishing lower bound weighted error generated algorithm. condition ﬁrst part condition easy satisfy already there. second part condition similarly easy satisfy. note ﬁrst second parts condition imply constant all-negative hypothesis also every example training dataset incorrectly classiﬁed weak hypothesis exists think converse third part condition). third part condition natural because perfect optimal adaboost would stop immediately ﬁrst iteration weighted error initialization would zero another implication condition simplify presentation assume without explicitly stating satisﬁes natural conditions listed above. hypotheses ﬁnite inﬁnite induces label dichotomies training dataset input examples dichotomy deﬁned m-dimensional vector output labels training examples formally denote ﬁnite label dichotomies note explicitly compute sets practice; convenient mathematical abstraction needed characterize actual full behavior optimal adaboost. said differently ﬁnal classiﬁer output optimal adaboost using mathematical abstraction implicitly provided ﬁnite label dichotomies exactly produced learning algorithm practice. parts condition imply vector vector dich. dichotomy dich convenient associate representative hypothesis among hypothesis produces dichotomy return concept representative hypothesis later extend convergence results various functions including adaboost classiﬁer generalization error concept margins training examples whole feature space call adaboost extensively uses weighted error hypothesis example-weight update. typical expression weighted error hypothesis respect distribution examples corresponding mistake dichotomy equivalently compute weighted error respect part condition implies introduce concept dominated mistake dichotomies. mistake dichotomy dominates another mistake dichotomy mistakes associated subset associated studying optimal adaboost eliminate dominated mistake dichotomies removal sound note dominated hypothesis would never selected optimal adaboost execution. optimal adaboost generate execution strictly positive thus dominates weighted error would always strictly smaller weighted error every round optimal adaboost. result removals call effective mistake dichotomies dominates ηr}. efective mistake dichotomy corresponding label dichotomy turns representative hypothesis become convenient denote representative hypothesis ﬁnal classiﬁer output optimal adaboost. deﬁne effective mistake matrix {}r×m similarly matrix form pair indexes column matrix respectively. note construction matrix vector corresponding effective mistake dichotomy call effective mistake dichotomy indicating representative hypothesis incorrect dataset input examples said differently noted earlier footnote deﬁnition mistake matrix differs syntactically rudin semantically equivalent. stated earlier footnote obtain matrix their’s replacing matrix transposing resulting matrix. whenever selected weak learner. also ﬁnite number hypothesis selection candidates case effectively reducing hypothesis space effective representative hypotheses {hηhη hηn}. common selection scheme case decision stumps. case construct matrix similar then applying pruning procedure removes repeated dominated mistake dichotomies. paper studies optimal adaboost dynamical system weights examples also refer example sample weights similar previous work section show frame optimal adaboost dynamical system. therefore ﬁxing much analysis reduce adaboost using rows equivalently elements proxy representative hypotheses weight update. sound one-to-one relationship discussed said differently weak learner guaranteed output hypotheses whose weighted binary-classiﬁcation error strictly better random guessing regardless dataset examples weight distribution examples. value often called edge weak learner. general form assumption sometimes referred weak-learning hypothesis. want emphasize that attempted weakened simply remove weak-learning assumption discussed section context study forms convergence adaboost. recall main focus paper convergence respect number rounds ﬁxed arbitrary datatset drawn probability space generalization error optimal-adaboost classiﬁer optimal-adaboost classiﬁer itself related characteristic quantities general interest margins. assumption remains standard study type convergence optimal adaboost considered paper start introducing dynamical-system view optimal adaboost requires make slight change traditional initialization weights training examples uniform distribution training examples presented fig. particular replace initialization presented ﬁgure initialize uniform. note that almost surely change initialization affect adaboost property driving training error zero fundamental effect learning algorithm. implementation procedure weaklearn used optimal adaboost must decide select output hypothesis whenever hypothesis achieve minimum error training dataset respect example weights consider typical function implementation weaklearn mapfunction implementation uses. view deterministic selection scheme introduce bias hypothesis class whatever speciﬁc function implementation weaklearn rest assure classiﬁer generalization error time per-round average l-measurable function example weights matter converge according main results formally presented later. point need deﬁnition weaklearn function part simplify analysis presentation. note assume adaboost cycle weak learner essentially behaves like function input example weights different throughout execution algorithm. recall stated earlier informally state formally later adaboost cycles results falls approach almost immediately appendix information cycling case. said differently implementation weaklearn function described above leads implementation adaboost implicitly deﬁnes notion best equivalently best mistake dichotomy weight function implementation weaklearn general standard notion best mistake dichotomy argminη∈e necessarily singleton multiple rows multiple mistake dichotomies set. thus implementation weaklearn implicitly imposes initial sample weights drawn uniformly random m-simplex. minor modiﬁcation standard derivation optimal adaboost upper bound classiﬁer’s misclassiﬁcation error yields m×minl=...m note elements non-zero almost surely draw denominator upper bound remains constant throughout adaboost process numerator results weak-learning assumption goes zero exponentially fast policy break ties mistake-matrix-rows/mistake-dichotomies lowest error. hence assume exists tie-breaking function adaselect denotes power serves mathematical function proxy implementation weaklearn. deﬁnition given weight deﬁne notion best study convergence adaboost classiﬁer implications main goal next section. provide preliminary deﬁnitions introduce useful concepts mathematical results. recall ﬁnal classiﬁer adaboost output rounds denote labels input examples computing adaboost built classiﬁcation αtht function sign standard sign otherwise. carefully note deﬁnition above weak-hypothesis corredich. certainly converge total number rounds adaboost approaches inﬁnity. fact weak-learning assumption holds value must growing least linearly mean convergence adaboost classiﬁer exactly then? replace sign weights well-deﬁned think probability distribution rounds deﬁne margin function rounds optimal adaboost margint margint margin input respect range margint range think deﬁning expression expectation weak-hypotheses selected round respect normalized example weights. similarly deﬁne empirical margin function rounds optimal adaboost margint margint margint empirical margin input example respect hence equivalently sign instead classiﬁcation. then shortly prove certain conditions margint converges sign sign respectively. formally state conditions shortly discuss theoretically practically reasonable present strong empirical evidence mean convergence adaboost classiﬁer. margint almost surely training dataset establish convergence functions almost surely outside training dataset upshot given convergence results something strong generalization error optimal-adaboost classiﬁer behaves limit. intuitively optimal-adaboost classiﬁer effectively converging generalization error. outlines main contribution paper. extension convergence results whole input space instead unique input instances training come without difﬁculty. know correspond directly however outside mistake dichotomies longer deﬁned simply vectors examples evaluate arbitrary must appeal hypotheses selected hypothesis space mistake dichotomies produced. class perspective optimal adaboost indistinguishable sense picking result change trajectory however weak learner might bias towards certain hypotheses classes. example induces equivalence relation partition pair hypotheses equivalence perhaps weak learner always pick simplest hypothesis based weak learner must corresponding representative hypothesis docode adaselectargminη∈e w;weaklearn selection procedure naturally partitions regions different mistakematrix-rows/mistake-dichotomies best sense would selected adaselect. deﬁnition given arbitrary deﬁne ∆m|η note open closed different depending ties broken using adaselect. closure formally deﬁne also play important role. deﬁnition weights non-zero error effective mistake dichotomies ∆m|η important note complement respect i.e. ∆m−π borel measure zero. indeed condition implies depart standard notation adaboost weight update. notation convenient main proofs paper. first notion hypothetical weight update. given assume would adaboost weight update take deﬁnition given arbitrary mistake dichotomy deﬁne component-wise component update certainly trace actual trajectory adaboost weights. true update ﬁrst ﬁnds best mistake dichotomy applies tηw. deﬁnition adaboost weight update deﬁned tηw. trace trajectory adaboost weights repeatedly applying initial weight picked within taken initial point rederive original formulation algorithm denotes composing times. value sequences described fact called secondary quantities derive solely weight trajectory. seek understand convergence properties secondary quantities number rounds optimal adaboost increases given ﬁxed arbitrary dataset drawn respect probability space presented previous section properties mapping causes converging behavior. characterizing inverse optimal-adaboost update studying dynamics adaboost update natural given analysis inverse essential establishing invariant measure used following sections. particular allow establish existence measure interest trapping/attracting measurepreserving. approach problem decompose inverse union line segments. proposition suppose proof {ρw− consider element then clearly errw− using fact clude )))). setting clean inverse simply line simplex space. important note hypothetical asking where would true adaboost weight update regardless inverse decompose union line segments. proposition π∗). then therefore ∩π∗). must case possible namely deﬁnition implies therefore mentioned previous section express secondary quantities optimal adaboost functions based solely trajectory applied initial empirical evidence suggests averages respect number rounds adaboost quantities/parameters converging optimal-adaboost classiﬁer converging crucial understanding convergence sense previously discussed section birkhoff’s ergodic theorem stated theorem below. theorem gives sufﬁcient conditions convergence apply secondary quantities. taking center stage theorem notion measure measure-preserving dynamical system. able apply birkhoff’s ergodic theorem need show existence measure measure-preserving dynamical system deﬁned function existence measure given proposition relies condition discuss context surrounding greater detail shortly. equally important birkhoff’s ergodic theorem notion integrability captured notation notation says integrable respect measure precise meaning that ﬁrst foremost measurcare asymptotic behavior optimal adaboost want disregard transient states context means exists therefore would like look subset state space dynamics limit towards stay within. following intuitively non-transient states speciﬁcally states optimal adaboost reach time step adaboost important establish invariant measure section difﬁcult anything important yet. turns discontinuities many points state space. difﬁcult point yields mistake dichotomy argminη∈e discontinuity. similarly point also discontinuity. motivates following deﬁnition. deﬁnition theorem suppose weak-learning assumption hold. optimal adaboost continuous points int)∩ proof int)∩ take {wi} arbitrary sequence limi→∞ tail {wi} contained within i.e. exists ﬁnite then following next section prove weight trajectories optimal adaboost eventually bounded away type- discontinuities. assume adaboost satisﬁes condition bounded away type- discontinuities. condition instrumental analysis gives proving existence invariant measure essential satisfying theorem condition formalized below. roughly speaking condition essentially says that sufﬁciently long number rounds either dichotomy corresponding optimal weak hypothesis round unique respect weights round dichotomies corresponding hypothesis tied optimal essentially respect weights remark found condition always holds high-dimensional real-world datasets tried practice. indeed unlike case cycling optimal adaboost provide strong empirical evidence justifying validity reasonableness condition practice using high-dimension real-world datasets section note part condition allows reduce label dichotomies never become effectively standpoint optimal adaboost dealing remark roughly speaking condition states effective mistake dichotomies either never tied best within tied must case lower-dimensional subspace implication part condition follows exists least thus least elements positive probability optimal-adaboost example-weights update maps positive must case least converged case would behave starting hence equivalently reset learning problem removing tying selected adaselect leading effective remove training dataset example create would dynamical system state space corresponding lower-dimensional subspace note process removing dichotomies examples reveal dominated hypotheses mistake hypotheses lowerdimensional space -dimensional vector course must also remove dominated mistake dichotomies continuing/re-starting process lower-dimensional space. note also process removal cannot continue forever resulting sets become empty; fact show properties example-weight update must exist least positive negative example positive weight every round. consecutive effective mistake dichotomies selected consecutive rounds must different every round ηt+. simplify presentation avoid clutter repeating phrases explicitly state condition technical results remaining section next section; mention condition respective proof. state propositions cover conditions required apply birkhoff’s ergodic theorem take propositions granted delay formal proof next section. proposition covers ﬁrst condition theorem measure-preserving dynamical system measure proposition sufﬁcient yield theorem captures main results paper. afterwards proposition covers second condition secondary quantities optimal adaboost essential impending analysis. combine theorem proposition prove rest main convergence results. follows show extend construction assigns measure zero reach ﬁnite time. leave open whether deﬁne non-zero mesure set. said that even clear whether makes sense place measure set. fact discuss giving zero measure makes sense practical perspective return point stating next result form next upcoming theorem. deﬁne n∪{∞} inf{τ ω∞}; ﬁrst-entrance time sequence optimal adaboost updates starting note denote deﬁne measure follows. uniform/lebesgue probability measure borel σ-algebra denote simplify denominator inside integral zero hence term corresponds conditional probability undeﬁned dµ∞) said differently integral well-deﬁned probability measure absolutely continuous respect uniform/lebesgue probability measure means every measurable implicit deﬁnition measure lemma measure proper probability measure measurable space every technical result stated remaining section uses condition indirectly. except krylov-bogolyubov theorem results stated next section lemma theorem directly lemma theorem condition all. given proof first note proposition birkhoff ergodic theorem last statement implies that ω−ω∞ converges +t)) does fact would converge value. using lemma recalling deﬁnition form partition yields result. note convergence results regarding optimal adaboost stated section qualiﬁed statement ν-almost surely include phrase formal statements avoid numerous repetitions would clutter presentation. often include shorten version phrase proofs stating ν-a.s. keeping rest statement implicit. theorem last proposition obtain convergence result training examples stated next theorem. note next theorem depart standard notations αtht notation deﬁnes terms effective mistake dichotomies constructed label dichotomies dich learner optimal adaboost. elements mistake dichotomies deﬁned unlike hypotheses whose output {−}. thus need scale translate appropriately. notation results exact values deﬁned selected effective representative hypotheses. avoid confusion denote corresponding function input training optimal-adaboost classiﬁer function round deﬁned input training examples terms corresponding mistake dichotomy representative hypothesis output weak learner that exists. proof αχπ∗. imposed conditions proposition easy deduce measurable. furthermore whereby follows α·χπ∗∈ w∈∆◦ exists ν-a.s. applying theorem limt→∞ convenience implementation optimal adaboost described sections proves convenient extend last results unique input instances training whole space recall that know corresponds directly outside mistake dichotomies longer deﬁned simply vectors examples only. evaluate arbitrary must appeal hypotheses actually selected hypothesis space mistake dichotomies produced. said differently must corresponding representative hypothesis note corollary anything whether optimal adaboost maximizes margins. remind reader interest margin maximization paper reasons previously discussed introduction. siﬁer classify samples. concept representative hypothesis used implementation becomes useful next results extending previous ones demonstrate. proof arrive convergence limt→∞cη ν-a.s. limt→∞ margint limt→∞ ∑η∈e ν-a.s. convergence tion easily replace sign sign case possibly yielding non-existent limit sign hard think non-degenerate situations decision boundary measure zero. decision boundary almost classiﬁers know borel measure zero almost always correspond lower-dimensional subspace input/feature space think unless something really special ensemble weak classiﬁers output optimal adaboost likely borel measure zero optimal adaboost converge classiﬁer whose decision boundary borel measure zero. hence believe last condition reasonable. said that principle relax condition discuss posible relaxations condition appendix implicitly assume condition holds often shorten phrase p-almost surely p-a.s. stated proofs. take last condition limit classiﬁer behaves nicely. fascinating implication adaboost classiﬁer itself converging classiﬁcation almost elements instance space i.e. except subset measure respect theorem limit limt→∞ exists. ization error well. deﬁnition express /-loss function lossh binary classiﬁer output labels lossh express generalization error binary classiﬁer output labels theorem limit generalization error limt→∞ exists. proof first assume measurable probability space otherwise would make sense discuss generalization error ﬁrst place would exist classiﬁer time follows measurable limt→∞ ν-a.s. p-a.s. furthermore function lossh dominated constant function inated convergence theorem satisﬁed. limt→∞ limt→∞ limt→∞ section devoted proving proposition fundamental krylov-bogolyubov theorem listed theorem below. given dynamical system meets certain conditions krylov-bogolyubov tells system measure-preserving borel probability measure. apply theorem trapped attracting show admits invariant measure. couple concepts essential understanding theorem. first krylovbogolyubov requires deal system form formally called topological space often called state space topology furthermore needs metrizable meaning topology induced metric. topology metrizable topological space metric space metric induces simplify matters i=|w− treat directly proofs; discuss convergence sequences weights implicitly metric. convergence meaningless without metric. deﬁnition closed open sets also implicitly closed sets sets contain limit points. closed given convergent sequence {wi} limi→∞ sub-family closed sets compact sets closed sets also bounded. considering subsets subsets bounded closed subset compact. krylov-bogolyubov theorem requires state space compact. want apply theorem must scrutinize. mentioned before contained within know bounded. hence need show closed. motivation behind theorem additionally krylov-bogolyubov theorem requires continuous stated condition bounded away type- discontinuities remains show contain type- discontinuities. theorem accomplishes that. begin showing compactness ﬁrst approach proving following lemma states limit point corresponding limit point lemma {wi} arbitrary convergent sequence call limit exists second convergent sequence proof {wi} sequence described hypothesis limi→∞ compactness additionally must exist sequence composed elements. proceed consider subsets form g∩π∗. note exists contains inﬁnite elements sequence subsequence {wi} contained note sequentially compact compact subset metric space. therefore call limit claim exists convergent subsequence i.e. closure follows closed sets involved intersection closed. also note sequence therefore contained yielding either later containing weights tied another element second case impossible condition allow ties must conclude proceed show equation clear subsequence given limit point previous lemma lets construct inﬁnite orbit backwards contained entirely whereby giving compactness. next theorem formalizes this. theorem compact. proof {wi} arbitrary convergent sequence contained limi→∞ lemma exists sequence converging however notice also satisﬁes hypothesis lemma applying lemma converging therefore continue generate therefore must conclude limit arbitrary convergent sequence must case compact. must turn understanding continuity properties previously mentioned continuous points state space. condition tells bounded away type- discontinuities. continuous attracting must show contains points error zero. following lemma takes step towards goal. shows that given point error hypothesis corresponding mistake dichotomy error hypothesis inverse large. that error induced mistake dichotomy inverse also large. following lemma prove next theorem tells adaboost bounded away type- discontinuities. apply lemma recursive manner show exists weighted error hypothesis equivalently mistake dichotomy respect points/example-weights bounded away zero. weight distributions examples optimal adaboost reaches rounds starting initialization weights/distributions examples selected interior m-simplex probability distributions.) update used optimal adaboost case note properties adaboost update rearranging update equation using simple algebra yields w)−η w)η) w)))) using deﬁnitions wη). invoking weak-learning assumption yields result continue template claim cannot exist. sake contradiction suppose did. then lemma know particular principle mathη ematical induction sequence unique construction. because sequence contradiction must conclude exists selection arbitrary step construction sequence also conclude exist else would reached procedure. finally shows condition holds compact metrizable topological space. also continuous map. follows krylov-bogolyubov theorem admits invariant borel probability measure minimum ﬁnite continuous functions follows continuous well. case borel algebra continuity implies measurability. weak-learning assumption follows main objective section provide empirical evidence adaboostcycle conjecture instead favor no-ties condition empirical results context decision stumps common instantiations weak learner optimal adaboost effectively used practice ﬁrst slide breiman’s wald lecture quote beginning paper suggests. passing also provide potentially tighter data-dependent theoretical pac-style uniform-convergence generalization bounds currently known adaboost grew experimental observations. adaboosting decision stumps studied empirical behavior adaboost context called decision stumps base/weak classiﬁers/hypothesis. decision stumps simple decision tests based single attribute input; i.e. decision tree single node root corresponding attribute test. instance deﬁne following test function implementation decision stump studying decision stumps practical implications. simplicity effectiveness decision stumps arguably commonly used weakhypothesis class adaboost practice figure provides classroom example concept dominated effective hypotheses context decision stumps. table contains examples number effective/non-dominated decision stumps number unique decision stumps optimal adaboost actually uses context high-dimensional real-world datasets publicly available repository. note significant reduction effective size actual number decision stumps selected. note table like plots figure clearly suggests optimal adaboost selecting decision stump many times mean optimal adaboost cycling. fact opposite evidence cycling. relevant results rudin prior pioneering work area relate work here rest condition adaboost cycles. column; used column pair percentages respect total non-dominated columns respectively. note signiﬁcant reduction effective size actual number decision stumps selected. resulting numbers provided table robust random variations train-test validation sets size original train-test size above. refer reader back section thorough discussion.) work shows adaboost cycle carefully constructed low-dimensional synthetic datasets work also establishes adaboost cycles several conditions apply adaboost maximize minimum margin. maximizing minimum margin boosting context previously stated interest here found produce classiﬁers perform worst adaboost itself also unlike rudin provide evidence theoretical empirical high-dimensional real-world datasets extra conditions assumed. rudin present indication chaotic behavior generally inconsistent cycling behavior high-dimensional synthetic datasets. whether adaboost cycles remains open problem date found that benchmark datasets used number unique decisionstumps classiﬁers adaboost combines grows number rounds logarithmically figure illustrates variety data sets important point even though effective number decision stumps relatively small number used/selected adaboost even ﬁnal remark/clariﬁcation note particular implementation weak learner used here size effective hypothesis class number representative classiﬁers effective mistake dichotomies thus ﬁnite. hence number unique representative base-classiﬁers selected converge unique base-clasiﬁers either saturate whole representative classiﬁers plateau smaller subset. logarithmic pace growth number unique base-classiﬁers seen center plots figure continues longer runs adaboost saturation would take approximately rounds breast cancer parkinson spambase datasets respectively. needless absurdly large numbers reach test convergence. never seen adaboost plateau saturate experiments real-world high-dimensional datasets. suppose adaboost plateau saturate. then course adaboost cycle respect base classiﬁers. behavior imply adaboost cycle respect example weights type cycling interest open problem addition cycling effective hypotheses imply convergence example weights. fact adaboost example-weights update-rule weak-learning assumption sequence example weights never converge. using knowledge log-growth behavior illustrated previous subsection derived data-dependent uniform-convergence bounds. present simplest leave another bound takes account effective number original space weak-hypotheses respect adaboost appendix bounds essentially state that high probability generalization error grows logt following data-dependent pac-style bound kind typically seen computational-learning-theory literature. uniform-convergence probabilistic bound accounts logarithmic growth number unique weak hypothesis typical application optimal adaboost uses decision stumps class weak/base-classiﬁers perhaps most-commonly used instantiation practice dependence number rounds reduced number effective representative decision stumps induced data using midpoint rule. dstump recall table that practicet could considerably smaller dstump| center column figure suggest expected value loglogt considerable certainly nonyet data-dependent uniform-convergence generalization bound misclassiﬁcation error gets closer explaining general behavior test-error curves optimal adaboost practice convergence results tell even potentially tighter generalization bound still insufﬁcient. because asymptotically upper bound error differences increase preliminary experimental results support no-ties condition section discusses preliminary experimental evidence condition holds practice. recall condition requires that pair either ties limit tied effectively respect weights limit. provide empirical evidence commonly used data sets practice suggesting conditions seem satisﬁed. fact report evidence real-world datasets brevity. results presented representative observed datasets. believe also sufﬁcient purpose demonstrating empirical validity noties condition typical publicly-available real-world datasets used many researchers community adaboost literature. figure adaboost decision stumps heart-disease sonar breastcancer datasets tracking difference error best second best mistake-dichotomies round optimal mistake dichotomy round looking second best mistake dichotomy round ignore mistake dichotomies ∑iη=η recall start optimal adaboost initial weight drawn uniformly random m-simplex difference best second best mistake-dichotomy/representativehypothesis tends decrease early happens weights zero non-minimal-margin examples. refer minimalmargin examples support vectors term rudin also similar interpretation examples svms. zero-weight examples could cause certain rows become essentially equal respect weights. weights condition equate essentially satisfying part condition ignore equivalent mistake-dichotomies/hypothesis. turn causes trajectory differences best second best jump upwards. sufﬁcient number rounds support vectors manifests jumping behavior stops. point appears distance ties bounded away zero suggesting condition holds adaboosting decision stumps datasets. figure provides reasonably clear evidence convergence optimal adaboost classiﬁer boosting decision stumps cancer dataset. ﬁgure margin every example seems converging rounds little change seen clearly plot ﬁgure. figure shows convergence minimum margin; essentially complete view convergence minimum margin clearly seen histograms figure converging behavior predicted theoretical work section training examples indexed denote margint signed margin example indexed convergence results show limt→∞ minl exists. also show limt→∞ implies that training examples indexed limt→∞ implies limt→∞ limt→∞ implies limt→∞ min. also assuming training examples different outputs always exists pair different-label examples indexed limt→∞ limt→∞ turn implies limt→∞ limt→∞ leading interpretation limt→∞ min} support vectors. argue leave technical problem establishing convergence rates thus reaching ultimate goal proving validity no-ties condition future work open problem community instead address subjects within current work. present several arguments view. first maturity approach unproven conditions consistent common practices literature. example seminal work published another journal venue rudin present following begin with whether adaboost cycles open problem stated article itself evidence paper theoretical empirical justifying conditions imposed theorem quoted above. theorem important contributions paper’s title attest second clearly state reasonable conditions backed empirical evidence present here study implications conditions. approach analogous seminal work rudin partly quoted above proved no-ties condition holds many important results follow including convergence optimal adaboost classiﬁer generalization error among many results. results essentially universal sense seem largely independent hypothesis class used weak learner data distribution again remind reader provide empirical evidence favor no-ties condition several typical high-dimensional real-world data sets. therefore implications employing condition turn quite signiﬁcant establishing convergence important classiﬁer ﬁxed sample size along generalization error. relationship no-ties condition employ adaboost-cycles condition used rudin evidence suggest no-ties condition implies adaboost cycles. contrary observed cycling behavior experiments real-world datasets consistently observe no-ties condition practice; include empirical evidence ﬁgures paper. thus immediate direct connection no-ties condition possibility adaboost cycles. stated generally conditions implying other. know whether no-ties condition consistent adaboost cycling. connection would require either formal proof reasonably logical argument strong empirical evidence none have. indeed statement actually inconsistent general anecdotal observations experience practice. repeat know lack tied dichotomies imply adaboost cycle. fact informally conjecture no-ties condition imply adaboost cycles. similarly emphasize section provides considerable empirical evidence favor no-ties condition open problem whether adaboost always cycles recently stated authors whether example weights wt’s often cycle result work presented paper opposite observe practice apply adaboost real-world datasets presented discussed section indeed section rudin regards evidence chaotic non-cyclic behavior synthetic higherdimensional cases. empirical observations leads informally conjecture typical behavior adaboost large real-world datasets too. steady-state/per-round-averaged distribution limt→∞ examples indexed note result imply wt’s themselves viewed sequence indexed cycle converge. time/per-round average large class functions example weights which prove paper include margin’s converge. importantly optimal-adaboost classiﬁer generalization error converge. talking function adaboost optimizing typical results classiﬁer generalization error results unlike previous work provide strong evidence practice. know result explains adaboost’s consistent/converging test-error behavior commonly practice. found evidence no-ties condition real-word datasets evidence favor. reason believe false given evidence gathered experience running algorithm. contrary found quite sensible natural. aware reasonable strong evidence involving realistic datasets would potential validity serious doubt. future work work show no-ties condition sufﬁcient convergence many quantities objects interest occur. also necessary too. leave whether case open problem community. paper provide existential proof invariant measure supposition dynamics adaboost satisfy certain conditions standard obviously natural another present considerable empirical evidence favor. improvement result would direct proof existence measure perhaps proving adaboost always satisﬁes conditions. even stronger result would constructive proof measure. hints proof lying simple nature inverse adaboost mapping leave study open future work. stated introduction section provided proof convergence optimal adaboost conditions convergence rate. suspect rate varies signiﬁcantly datasets choice weak learner. example datasets adaboost tends overﬁt suspect rate convergence slower. hand stronger weak learner quicker rate convergence seems suggest generalization error adaboost seems quickly converge using decision trees base classiﬁers. research important also left future work. informally conjecture no-ties condition almost always holds optimal adaboost. course formal statement no-ties conjecture still escapes would venture creating formal conjecture easy might seem. believe that whatever resulting statement would likely involve minimal conditions weak-hypothesis spaces process generating training data amount training data relative characteristics generating process indirectly dichotomies induce. believe that community currently know little factors interact context optimal adaboost able formalize kind contrast optimal adaboost so-called non-optimal adaboost term also coined rudin weak hypothesis function weaklearn outputs achieves minimum weighted error among weak hypotheses respect given example weights dataset round believe convergence results extend version adaboost long ties selection; corresponding no-ties condition case would also serve purpose avoiding discontinuities example-weights update. adapting analysis non-optimal setting requires careful derivation signiﬁcant effort beyond scope paper thus left future work. regardless fascinating intriguing appealing signiﬁcant future study true role no-ties condition specially relates breiman second phase adaboost run. leave research study true fundamental implications condition relation theoretically imposed empirically studied conditions concepts adaboost literature future work. statistical perspective another question results work optimal adaboost converge bayes-risk introduce right bias deterministic selection base classiﬁers right implementation weaklearn function? perspective intended focus paper could optimal adaboost converge minimum risk/loss given amount data kind implementation conditions? finally would like something quality genexperiments involving eralization error beyond converges. decision stumps observed logarithmic growth number unique hypothesis contained combined adaboost classiﬁer function time. logarithmic growth yields tighter data-dependent bound generalization adaboost classiﬁer. believe distribution invariant measure regions important factor behavior relative frequency selecting hypothesis seems gamma distributed. based empirical evidence obtained running optimal adaboost high-dimensional real-world datasets typically used benchmarks introduced condition that roughly speaking states dynamics optimal adaboost drift away discontinuities state space consisting space example-weight distributions. supposing condition holds outlined series theorems propositions corollaries establish optimal adaboost’s weight update along functions representing various secondary quantities satisfy conditions birkhoff’s ergodic theorem using theorem main tool derived convergence results various aspects optimal adaboost showed margin examples training converge. given weak learner follows certain framework convergence results training extended whole instance space finally condition decision boundary limiting functions optimal adaboost would classiﬁcation probability conclude optimal adaboost classiﬁer along generalization error converges. decision boundary non-zero probability measure stability generalization error depends probability drawing example decision boundary converged classiﬁer. acknowledgements. work presented manuscript published conference proceedings venue except following undergraduate honor’s thesis computer science stony brook university ﬁrst author include parts work presented manuscript; arxiv technical report appears http//arxiv.org/abs/.. work supported part national science foundation’s faculty early career development program award iis-.", "year": 2012}