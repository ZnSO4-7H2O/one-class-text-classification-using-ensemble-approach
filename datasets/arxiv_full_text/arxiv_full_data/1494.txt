{"title": "Teaching Machines to Describe Images via Natural Language Feedback", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.HC"], "abstract": "Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.", "text": "robots eventually part every household. thus critical enable algorithms learn guided non-expert users. paper bring human loop enable human teacher give feedback learning agent form natural language. argue descriptive sentence provide much stronger learning signal numeric reward easily point mistakes correct them. focus problem image captioning quality output easily judged non-experts. propose hierarchical phrase-based captioning model trained policy gradients design feedback network provides reward learner conditioning human-provided feedback. show exploiting descriptive feedback model learns perform better given independently written human captions. a.i. slowly ﬁnding everyone’s lives form social bots personal assistants household robots becomes critical allow non-expert users teach guide robots example household robot keeps bringing food served ashtray thinking it’s plate ideally able educate robot mistakes possibly without needing underlying software. reinforcement learning become standard training artiﬁcial agents interact environment. signiﬁcant advances variety domains games robotics even ﬁelds like vision agents optimize action policies maximize expected reward received environment. training typically requires large number episodes particularly environments large action spaces sparse rewards. several works explored idea incorporating humans learning process order help reinforcement learning agent learn faster cases human teacher observes agent environment allowed give additional guidance learner. feedback typically comes form simple numerical reward used either shape reward directly shape policy learner paper exploit natural language guide agent. argue sentence provides much stronger learning signal numeric reward easily point mistakes occur suggests correct them. descriptive feedback thus naturally facilitate solving credit assignment problem well help guide exploration. despite clear beneﬁts approaches aimed incorporating language reinforcement learning. pioneering work translated natural language advice short program used bias action selection. possible limited domains navigating maze learning play soccer game hardly scale real scenarios large action spaces requiring versatile language feedback. figure model accepts feedback human teacher form natural language. generate captions using current snapshot model collect feedback amt. annotators requested focus feedback single word/phrase time. phrases indicated brackets example part captioning model’s output. also collect information word feedback applies suggested correction. information used train feedback network. goal allow non-expert human teacher give feedback agent form natural language would learning child. focus problem image captioning quality output easily judged non-experts. towards goal make several contributions. propose hierarchical phrase-based image captioning model naturally integrated human feedback. design interface allows collect natural language feedback human teachers snapshot model fig. show incorporate information policy gradient show improve access amount ground-truth captions. code data released facilitate human-like training captioning models. related work several works incorporate human feedback help agent learn faster. exploits humans loop teach agent cook virtual kitchen. users watch agent learn intervene time give scalar reward. reward shaping used incorporate information mdp. iterates practice agent interacts real environment critique session human labels subset chosen actions good bad. authors compare different ways incorporating human feedback including reward shaping augmentation action biasing control sharing. authors implement tamer framework real robotic platform proposes policy shaping incorporates right/wrong feedback utilizing direct policy labels. approaches mostly assume humans provide numeric reward unlike work feedback given natural language. attempts made advise agent using language. pioneering work translated advice short program implemented neural network. units network represent boolean concepts recognize whether observed state satisﬁes constraints given program. case advice network encourage policy take suggested action. incorporated natural language advice robocup simulated soccer task. translate advice formal language used bias action selection. parallel work exploits textual advice improve training time algorithm playing atari game. recently incorporates human feedback improve text-based agent. work shares similar ideas applies problem image captioning. captioning represents natural showing algorithm understands photograph non-expert observer. domain received signiﬁcant attention achieving impressive performance standard benchmarks. phrase model shares similarity differs exploits attention linguistic information train. several recent approaches trained captioning model policy gradients order directly optimize desired performance metrics follow line work. however best knowledge work ﬁrst incorporate natural language feedback captioning model. related efforts also work dialogue based visual representation learning however work tackles simpler scenario employs slightly engineered approach. figure hierarchical phrase-based captioning model composed phrase-rnn level word-level outputs sequence words phrase. useful property model directly produces output sentence segmented linguistic phrases. exploit information collecting incorporating human feedback model. model also exploits attention linguistic information please text details. stress work differs recent efforts conversation modeling visual dialog using reinforcement learning. models mimic human-to-human conversations work human converses guides artiﬁcial learning agent. approach framework consists phrase-based captioning model trained policy gradients incorporates natural language feedback provided human teacher. number captioning methods exist design phrase-based allowing natural guidance nonexpert. particular argue strongest learning signal provided feedback describes mistake time e.g. single wrong word phrase caption. example seen fig. also effectively teaches learning child. avoid parsing generated sentences test time predict phrases directly captioning model. ﬁrst describe phrase-based captioner describe feedback collection process ﬁnally propose exploit feedback guiding signal policy gradient optimization. captioning model forming base approach uses hierarchical recurrent neural network similar authors two-level lstm generate paragraphs uses generate sentences sequence phrases. latter model shares similar overall structure ours however model additionally reasons type phrases exploits attention mechanism image. structure model best explained fig. model receives image input outputs caption. composed phrase level word generates sequence words phrase. think phrase providing topic time step instructs word talk about. following convolutional neural network order extract feature vectors feature location input image. denote hidden state phrase time step denote i-th hidden state word t-th phrase. computation model expressed following equations denotes context vector obtained applying attention mechanism image. context vector essentially represents image area model looks order generate t-th phrase. information passed word-rnn well next hidden state figure caption quality evaluation human annotators. plot left shows evaluation captions generated reference model right plot shows evaluation human-corrected captions phrase-rnn. found computing different context vectors passed phrase word improves generation points mainly helping model avoid repetition words. furthermore noticed quality attention signiﬁcantly improves provide additional linguistic information. particular time step phrase also predicts phrase label following standard deﬁnition penn tree bank. phrase predict four possible phrase labels i.e. noun preposition verb conjunction phrase additional <eos> token indicate sentence. conditioning label help model look objects image focus global image information. above denotes i-th word output word-rnn t-th phrase encoded one-hot vector. note additional <eop> token word-rnn’s vocabulary signals end-of-phrase. further encodes generated phrase simple mean-pooling words provides additional word-level context next phrase. details choices functions given table. following deep output layer lstm double stochastic attention. implementation details. train hierarchical model ﬁrst process ms-coco image caption data using stanford core toolkit ﬂatten parse tree separate sentence parts label part phrase label simplify phrase structure merge previous phrase label another pre-training. pre-train model using standard cross-entropy loss. adam optimizer learning rate discuss policy gradient optimization subsec. bring human loop training captioning model. towards this create interface allows collect feedback information larger scale amt. interface akin depicted fig. provide visualizations appendix. also provide online project page. particular take snapshot model generate captions subset ms-coco images using greedy decoding. experiments take model trained objective. rounds annotation. ﬁrst round annotator shown captioned image asked assess quality caption choosing between perfect acceptable grammar mistakes only minor major errors. asked annotators choose minor major error caption contained errors semantics i.e. indicating robot understanding photo correctly. advised choose minor small errors wrong missing attributes awkward prepositions major errors whenever object action naming wrong. next round annotation select captions marked either perfect acceptable ﬁrst round. since captions contain errors annotator required provide detailed feedback mistakes. found annotators errors captions pointing annotator noise process. annotator shown generated caption delineating different phrases tokens. annotator choose type required correction write feedback natural language mark type mistake highlight word/phrase contains mistake correct chosen word/phrase evaluate quality caption correction. allow annotator submit correction even her/his evaluation still points errors. however plea good annotators continue providing feedback. latter case reset webpage replace generated caption current correction. annotator ﬁrst chooses type error i.e. something replaced missing should deleted. writes sentence providing feedback mistake corrected. require feedback provided sequentially describing single mistake time. restricting annotator select mistaken words within single phrase annotator marks details mistake indicating whether corresponds error object action attribute preposition counting grammar. annotator highlight area mistake caption replace correction. statistics data provided table examples shown table interesting fact feedback sentences cases mention wrong word caption well correction word. fig. shows evaluation caption quality reference model. captions marked containing errors randomly choose second round annotation fig. shows quality captions correction i.e. good reference captions well corrected captions submitted annotators. note paid round feedback thus captions still contained errors even correction. interestingly average annotators still rounds feedback image goal incorporate natural language feedback learning process. collected feedback contains rich information caption improved conveys location mistake typically suggests correct seen table provides strong supervisory signal want exploit framework. particular design neural network provide additional reward based feedback sentence. refer feedback network ﬁrst explain feedback network show integrate output table example classif. phrase newly sampled caption correct/wrong/not-relevant conditioned feedback sentence. notice need image judge correctness/relevance phrase. note training require generate samples model. thus training sampled captions training image change goal feedback network read newly sampled caption judge correctness phrase conditioned feedback. make depend text making learning task easier. particular performs following computation fphrase performs mean pooling words phrase represent phrase passes information -layer mlp. additionally accepts information mistake type encoded vector output layer -way classiﬁcation layer predicts whether phrase correct wrong relevant example output shown table implementation details. train ground-truth data collected. particular example wrong phrase example correct phrase treat rest relevant label. reference means generated caption collected feedback marked phrase means phrase annotator highlighted either reference corrected caption. standard cross-entropy loss train model. adam learning rate batch size reference caption several feedback sentences treat independent training data. follow directly optimize desired image captioning metrics using policy gradient technique. completeness brieﬂy summarize think caption decoder agent following parameterized policy selects action time step. action case requires choosing word vocabulary phrase label agent receives reward generating full caption i.e. reward automatic metrics weighted case also include reward feedback. objective learning parameters model expected reward received completing caption note baseline change expected gradient drastically reduce variance. reward. deﬁne different rewards sentence level phrase level. human feedback information both. ﬁrst deﬁne sentence reward reference caption weighted bleu scores particular choose reference captions compute reward either reference captions generated snapshot model evaluated minor major errors ground-truth captions. details given experimental section. weigh reward caption quality provided annotators. particular perfect acceptable grammar/ﬂuency issues only. incorporate reward provided feedback network. particular allows deﬁne reward phrase level since generated sentence segmented phrases i.e. denotes t-th phrase deﬁne combined phrase reward note produces classiﬁcation phrase. convert reward assigning correct wrong relevant. weigh reward conﬁdence network might worth exploring future. ﬁnal gradient takes following form implementation details. adam learning rate batch size follow annealing schedule. ﬁrst optimize cross entropy loss ﬁrst epochs following epochs cross entropy loss ﬁrst phrases policy gradient algorithm remaining loor phrases. choose caption multiple feedback sentences take fbn’s outputs reward phrase. sentence feedback assign zero reward. validate approach ms-coco dataset images training validation testing. particular randomly chose test images ofﬁcial validation split. collect feedback randomly chose images training well images validation. experiments report performance test set. models used pre-trained network extract image features. word vocabulary size phrase-based captioning model. analyze different instantiations phrase-based captioning table showing importance predicting phrase labels. sanity check model compare approach overall model performs slightly worse however main strength model allows natural integration feedback. note results reported models trained mle. feedback network. reported table dataset contains detailed feedback contains images. randomly select serve training feedback network test set. classiﬁcation performance reported table tried exploiting additional information network. second line reports result also exploits reference caption input represented lstm. model third line uses type error i.e. phrase missing wrong redundant. found using information kind mistake reference caption achieves best performance. model used following experiments. natural language feedback. table report performance several instantiations models. models pre-trained using cross-entropy loss full ms-coco training set. next rounds training models trained images comprise full evaluation+feedback dataset table particular separate cases. ﬁrst standard case agent access captions image. experiment different types captions e.g. ground-truth captions well feedback data. fair comparison ensure model access amount data. means count feedback sentence source information human-corrected reference caption another source. also exploit reference captions evaluated correct well corrected captions obtained annotators. particular tried types experiments. deﬁne captions captions corrected annotators evaluated containing minor major error ground-truth captions rest images. captions minor major errors rest. detailed break-down captions reported table ﬁrst test model using standard cross-entropy loss also access corrected captions addition captions. model able improve original model points. test model optimizing metric captions brings additional point achieving model. agent feedback given access captions captions feedback sentences. show model outperforms no-feedback baseline points. interestingly captions additional boost. agent access captions feedback descriptions achieve total points baseline model model. examples generated captions shown fig. also test realistic scenario models access either single caption case feedback. mimics scenario human teacher observes agent either gives feedback agent’s mistakes agent’s caption completely wrong teacher writes caption. interestingly provided corrected captions performs better given captions. overall model outperforms base points. note agents trained small subset full ms-coco training set. improvements thus possible. discussion. experiments make important point. instead giving agent completely target better strategy teach agent mistakes suggest correction. natural language thus offers rich modality providing guidance humans also artiﬁcial agents. conclusion paper enable human teacher provide feedback learning agent form natural language. focused problem image captioning. proposed hierarchical phrase-based captioning model allowed natural integration human feedback. crowd-sourced feedback snapshot model showed incorporate policy gradient optimization. showed exploiting descriptive feedback model learns perform better given independently written captions. acknowledgment gratefully acknowledge support nvidia donation gpus used research. work partially supported nserc. also thank relu patrascu infrastructure support. qualitative examples phrase-based captioning provide qualitative results phrase-based model fig. ﬁgure shows attention maps generated phrase predicted phrase label.", "year": 2017}