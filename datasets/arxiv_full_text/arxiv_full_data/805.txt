{"title": "Energy Saving Additive Neural Network", "tag": ["cs.NE", "cs.AI", "cs.LG"], "abstract": "In recent years, machine learning techniques based on neural networks for mobile computing become increasingly popular. Classical multi-layer neural networks require matrix multiplications at each stage. Multiplication operation is not an energy efficient operation and consequently it drains the battery of the mobile device. In this paper, we propose a new energy efficient neural network with the universal approximation property over space of Lebesgue integrable functions. This network, called, additive neural network, is very suitable for mobile computing. The neural structure is based on a novel vector product definition, called ef-operator, that permits a multiplier-free implementation. In ef-operation, the \"product\" of two real numbers is defined as the sum of their absolute values, with the sign determined by the sign of the product of the numbers. This \"product\" is used to construct a vector product in $R^N$. The vector product induces the $l_1$ norm. The proposed additive neural network successfully solves the XOR problem. The experiments on MNIST dataset show that the classification performances of the proposed additive neural networks are very similar to the corresponding multi-layer perceptron and convolutional neural networks (LeNet).", "text": "abstract—in recent years machine learning techniques based neural networks mobile computing become increasingly popular. classical multi-layer neural networks require matrix multiplications stage. multiplication operation energy efﬁcient operation consequently drains battery mobile device. paper propose energy efﬁcient neural network universal approximation property space lebesgue integrable functions. network called additive neural network suitable mobile computing. neural structure based novel vector product deﬁnition called ef-operator permits multiplierfree implementation. ef-operation product real numbers deﬁned absolute values sign determined sign product numbers. product used construct vector product vector product induces norm. proposed additive neural network successfully solves problem. experiments mnist dataset show classiﬁcation performances proposed additive neural networks similar corresponding multi-layer perceptron convolutional neural networks artiﬁcial neural networks shown solve many real world problems computer vision natural language processing recommendation systems many ﬁelds convolutional neural network architectures achieve human performance many computer vision problems including image classiﬁcation tasks however number parameters highperformance networks ranges millions billions require computers capable handling high computational complexity high energy memory size. consequently minimal computational environment network desktop computer powerful dedicated highend gpu. recent developments vlsi industry create powerful mobile devices used many practical recognition applications. anns already used drones unmanned aerial vehicles ﬂight control path estimation obstacle avoidance human recognition like abilities tively mobile devices high energy requirements. typical neuron needs perform three main tasks produce output inner product operation involving multiplication inputs weights addition pass result inner product activation function. according multiplication operation energy consuming operation. paper propose norm based energy efﬁcient neural network called additive neural network replaces multiplication operation energy efﬁcient operator called ef-operator. instead multiplications sign multiplications addition operations typical neuron. sign multiplication real numbers simple operation. addition consumes relatively lower energy compared regular multiplication shown processors. object recognition experiments mnist cifar datasets show able match performance state neural networks without performing changes structure. section review related work energy efﬁcient neural network design. section deﬁne vector product corresponding operator called ef-operator. section introduce additive neural network based ef-operator. section made brief analysis existence convergence problems proposed additive neural network. section provides experimental results compare performance proposed additive neural network multi-layer perceptron convolutional neural networks. finally section concludes paper. large size parameter space artiﬁcial neural networks generally computationally prohibitive become inefﬁcient terms energy consumption memory allocation. several approaches different perspectives proposed design computationally efﬁcient neural network structures handle high computational complexity. ﬁrst introduced norm based vector product image processing applications also proposed multiplication free neural network structure however recognition rate uration regions. pass region activation function approximated saturation region activation function taken dctif takes place process region. parameters transformation selected carefully balance computational complexity accuracy. shown proposed method achieve signiﬁcant decrease energy consumption keeping accuracy difference within conventional method. rastegari proposes methods provide efﬁciency cnns. ﬁrst method binary-weight-networks approximates weight values binary values network needs less memory since weight values binary convolutions estimated addition subtraction eliminates main power draining multiplication operation. therefore method provides energy efﬁciency faster computations. second method proposed called xnornetworks weights inputs convolutional fully connected layers approximated binary values. extends earlier proposed method replacing addition subtraction operations xnor bitcounting operations. method offers faster computation average. method enables cnns mobile devices costs loss accuracy average. vector product operation require multiplications. operation uses sign ordinary multiplication computes absolute values ef-operator implemented without multiplications. requires summation unary minus operation statements energy efﬁcient operations. regular neural network. article able match performance regular neural networks introducing scaling factor norm based vector product training methods. recognition rate regular neural network mnist dataset. solutions energy efﬁcient neural networks include dedicated software speciﬁc hardware i.e. neuromorphic devices although approaches reduces energy consumption memory usage require special hardware. neural network framework implemented ordinary microprocessors digital signal processors. sarwar used error resiliency property neural networks proposed approximation multiplication operation artiﬁcial neurons energy-efﬁcient neural computing approximate multiplication operation using alphabet multiplier computation sharing multiplication methods. multiplication steps replaced shift operators performed alphabet deﬁned pre-computer bank. alphabet basically subset lower order multiplies input. multiplies exist computed subset approximated rounding nearest existing multiplies. method reduces energy consumption since addition shifting operations much efﬁcient multiplication. therefore smaller sized alphabets result efﬁcient architecture. additionally deﬁne special case called multipler-less artiﬁcial neuron alphabet layer. method provides energy efﬁciency minimum accuracy loss. noted method applied test stages therefore training step still uses conventional method. proposed model reduces computational cost storage feature learning approach consists three steps. ﬁrst step train network discriminate important features redundant ones. then remove redundant weights occasionally neurons according threshold value obtain sparser network. step reduces test step’s cost. ﬁnal step retrain network tune remaining weights. state step much efﬁcient using ﬁxed network architecture. tested proposed network architecture imagenet vgg-. parameter size networks reduces without accuracy loss. abdelsalam approximate tangent activation function using discrete cosine transform interpolation filter neural networks fpga boards efﬁciently state dctif approximation reduces computational complexity activation function calculation step performing simple arithmetic operations stored samples hyperbolic tangent activation function input set. proposed dctif architecture divides activation function three regions namely pass process sativ. additive neural network ef-operator propose modiﬁcation representation neuron classical neural network replacing vector product input weight product deﬁned efoperation. modiﬁcation applied wide range artiﬁcial neural networks including multi-layer perceptrons recurrent neural networks convolutional neural networks element-wise multiplication operator rd×m weights scaling coefﬁcients biases respectively input vector. neural network neuron represented activation function deﬁned called additive neural network. comparison shows proposed additive neural networks obtained simply replacing afﬁne scoring function classical neural network scoring function function deﬁned ef-operator therefore neural networks easily converted additive network representing neurons activation functions deﬁned ef-operator without modiﬁcation topology general structure optimization algorithms network. standard back-propagation algorithm applicable proposed additive neural network small approximations. back-propagation algorithm computes derivatives respect current values parameters differentiable function update parameters. derivatives computed iteratively using previously computed derivatives upper layers chain rule. activation function excluded computations simplicity derivation depends speciﬁc activation function choice activation function affect remaining computations. hence section ﬁrst show proposed additive neural network satisﬁes universal approximation property space lebesgue integrable functions. words. exists solutions computed proposed additive network equivalent solutions obtained activation function classical vector product. then make brief analysis convergence properties back propagation algorithm vector product replaced ef-operators. universal approximation property universal approximation property suggested additive neural network proved speciﬁc form activation function. following proposition sufﬁce provide proofs universal approximation theorem linear relu activation functions only. proof general activation function requires substantial amount effort thus left future work. proposition iv.. additive neural network deﬁned neural activation function identity proof. constructing additive neural network deﬁned ef-operator enough prove lemma. construct explicitly sample network given network consists four hidden layers network easily extended higher dimensions. four hidden layers following parameters compute sign. multiplication operator. classical neural network represented activation function containing neurons dimensional input requires many multiplication operator compute hand additive neural network represented activation function w)+b) number neurons input space requires many multiplication operator compute reduction number multiplications especially important input size large hidden layer contains large number neurons. activation function taken either identity relu output layer computed without complex operations efﬁciency network substantially increased. multiplications removed entirely scaling coefﬁcients taken however networks represent functions consequently perform poorly datasets. optimization problems sign operation performed neuron ef-operator creates bunch hyperoctants cost function layer additive neural network. therefore local minima computed layer depends speciﬁc hyperoctant weights. change signs results jump hyperoctant another one. datasets local minima boundaries hyperoctants. since hyperoctants open sets leave hyperoctands nonexisting local minima. gradient based search algorithm update weights algorithm converges local minima boundary. step size number epochs increased updated weights leave current hyperoctant without converging local minima boundary weights make algorithm converge local minima another hyperoctant. however hyperoctant problem. multi-layer perceptron used measure ability proposed additive neural network machine learning problems. consists single input output layer multiple hidden layers. size number hidden layers vary great deal depending problem domain. research three hidden layers respectively different classiﬁcation problems namely problem character recognition mnist dataset. input layer receives pattern sample network. hand hidden layer contains biological inspired units called neurons learns representations input patterns. neuron consists scoring function activation function. discussed section scoring function afﬁne transform form classic neural network parameters. study call widely used classic scoring function c-operator. discussed fig. plots loss changes stochastic gradient descent algorithm training phase problem using single hidden layer mlp. figure shows changes loss network using classical score function figure shows loss changes network proposed results obtained training network times epochs shown different colors. layer using observation compute afterwards replace zeros weights introduced previous extension layer using observation replace activation function relu. works either relu relu modiﬁed network additive neural network relu activation function compute function dense easily shown sign function bounded sigmoidal function. lemma shows that activation function taken identity exist networks compute sign lemma shows equivalent networks using relu activation function compute functions. networks combined concatenation layers additive neural networks single network. also proposed architecture contains fully connected linear layer output layer compute superposition computed sign functions yielding since computable additive neural networks functions dense functions computed additive neural networks also dense computational efﬁciency proposed additive neural network contains parameters classical neuron representation architectures. however hidden layer computed using considerably less number fig. plots classiﬁcation accuracies different architectures different score functions. subplots shows results hidden layers using classic c-operator. subplots shows results hidden layers using proposed ef-operator. addition score function neuron hidden layer also activation function makes network nonlinear. several activation functions sigmoid hyperbolic tangent rectiﬁed linear unit functions used activation function. studies shown relu outperform others cases also examined sigmoid tanh following experiments. finally last layer called output layer maps ﬁnal hidden layer scores classes using score function. used classical c-operator ef-operator output layer make ﬁnal decision. optimal values parameters using backpropagation optimization algorithms stochastic gradient descent order implement network tensorﬂow python library numeric computation used. ﬁrst experiment examine ability additive neural network partition simple nonlinear space solving problem. compare classical afﬁne scoring function additive neural network ef-operator. since single hidden layer c-operator solve problem used hidden layer classical proposed architectures. mean squared error used cost function measure amount loss training phase network ﬁxed number neurons hidden layer additive neural network ef-operator could successfully solve problem reached accuracy problem. also investigate rate changes inloss changes epoch. also notable runs shown colors reach minimum values epochs. shows epochs needed runs. generally number epochs depends learning rate initialization condition ﬁnal epoch determined stopping criteria. however study interested variations cost; therefore ﬁxed number epochs using c-operator ef-operator respectively relu activation function. rerun network times epochs used k-fold cross validation specify learning-rate parameter sgd. color plots shows variations loss cost value across epochs speciﬁc network. ﬁgure shows cost value network proposed ef-operator decreases along epochs acts similar classical afﬁne operator called c-operator. second experiment classiﬁed digits mnist dataset consists handwritten examples examine proposed additive neural network multiclass classiﬁcation problem. mnist dataset consists training samples test data. example image digit one-hot code used encode class labels. example image size image concatenated single vector input network. therefore size input layer network used cross-entropy based cost function train network. used number examples iteration sgd. words batch size equal table contains classiﬁcation accuracies architecture using three activation functions relu tanh sigmoid four different learning rates. table shows additive neural network ef-operator reaches performance classic c-operator. words slightly sacriﬁcing classiﬁcation performance proposed ef-operator much energyefﬁcient. note that used regularization methods drop used krizhevsky simply show proposed ef-operator gives learning ability deep mlp. also table. shows maximum performances obtained using relu activation function. also interested variations classiﬁcation performances epochs along epochs. addition used proposed efoperator learn parameters lenet- classifying mnist dataset. table contains classiﬁcation accuracy lenet- architecture contains conventional fully connected layer. trained network cross-entropy based cost functions case. figure shows results classiﬁcation accuracies obtained based proposed ef-operator traditionally used c-operator. performances obtained successive epochs epoch network trained training examples. plots subﬁgures obtained using four different learning rates subplots left ﬁgure shows results c-operator hidden layers respectively subplots shows results proposed ef-operator. figure shows operator effectively increases classiﬁcation performance number epochs increases reaches nearly original linear function. study propose energy efﬁcient additive neural network architecture. core architecture lasso norm based ef-operator eliminates energyconsumption multiplications conventional architecture. examined universal approximation property proposed architecture space lebesgue integrable functions test real world problems. showed ef-operator successfully solve nonlinear problem. moreover observed sacriﬁcing accuracy proposed network used multilayer perceptron conventional neural network respectively classify mnist dataset. future work plan test proposed architecture state-of-the-art deep neural networks. simonyan zisserman deep convolutional networks large-scale image recognition arxiv preprint arxiv. taigman yang ranzato wolf deepface closing human-level performance face veriﬁcation proceedings ieee conference computer vision pattern recognition szegedy sermanet reed anguelov erhan vanhoucke rabinovich going deeper convolutions proceedings ieee conference computer vision pattern recognition giusti guzzi cires f.-l. rodr guez fontana faessler forster schmidhuber caro machine learning approach visual perception forest trails mobile robots ieee robotics automation letters vol. pool tran dally learning weights connections efﬁcient neural network advances neural information processing systems suhre keskin ersahin cetin-atalay ansari cetin multiplication-free framework signal processing applications biomedical image analysis ieee international conference acoustics speech signal processing ieee esser merolla arthur cassidy appuswamy andreopoulos berg mckinstry melano barch convolutional networks fast energy-efﬁcient neuromorphic computing arxiv preprint arxiv. painkras plana garside temple galluppi patterson lester brown furber spinnaker -core system-on-chip massively-parallel neural network simulation ieee journal solid-state circuits vol. moradi indiveri event-based neural network architecture asynchronous programmable synaptic memory ieee transactions biomedical circuits systems vol. park neftci cauwenberghs k-neuron -mevents/s -pj/event asynchronous micro-pipelined integrate-andﬁre array transceiver ieee biomedical circuits system conference proceedings. ieee sarwar venkataramani raghunathan multiplierless artiﬁcial neurons exploiting error resiliency energy-efﬁcient neural computing design automation test europe conference exhibition ieee abadi agarwal barham brevdo chen citro corrado davis dean devin ghemawat goodfellow harp irving isard jozefowicz kaiser kudlur levenberg monga moore murray olah schuster shlens steiner sutskever talwar tucker vanhoucke vasudevan egas vinyals warden wattenberg wicke zheng tensorflow large-scale machine learning heterogeneous systems software available tensorﬂow.org.", "year": 2017}