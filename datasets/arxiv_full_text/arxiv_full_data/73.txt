{"title": "How NOT To Evaluate Your Dialogue System: An Empirical Study of  Unsupervised Evaluation Metrics for Dialogue Response Generation", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model's generated response to a single target response. We show that these metrics correlate very weakly with human judgements in the non-technical Twitter domain, and not at all in the technical Ubuntu domain. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.", "text": "investigate evaluation metrics dialogue response generation systems supervised labels task completion available. recent works response generation adopted metrics machine translation compare model’s generated response single target response. show metrics correlate weakly human judgements non-technical twitter domain technical ubuntu domain. provide quantitative qualitative results highlighting speciﬁc weaknesses existing metrics provide recommendations future development better automatic evaluation metrics dialogue systems. important aspect dialogue response generation systems trained produce reasonable utterance given conversational context evaluate quality generated response. typically evaluation done using human-generated supervised signals task completion test user satisfaction score relevant dialogue task-focused. call models optimized supervised objectives supervised dialogue models unsupervised dialogue models. models receiving increased attention particularly using end-to-end training neural networks avoids need collect supervised labels large scale prohibitively expensive. however automatically evaluating quality models remains open question. automatic evaluation metrics would help accelerate deployment unsupervised response generation systems. faced similar challenges natural language tasks successfully developed automatic evaluation metrics. example bleu meteor standard evaluating machine translation models rouge often used automatic summarization. metrics recently adopted dialogue researchers however metrics assume valid responses signiﬁcant word overlap ground truth responses. strong assumption dialogue systems signiﬁcant diversity space valid responses given context. illustrated table reasonable responses proposed context responses share words common semantic meaning. paper investigate correlation between scores several automatic evaluation metrics human judgements dialogue response quality variety response generation models. consider statistical word-overlap similarcontext conversation speaker john want tonight? speaker don’t movie? ground-truth response hate stuff let’s something active. model response sure heard turing metrics bleu meteor rouge word embedding metrics derived word embedding models wordvec metrics show either weak correlation human judgements despite fact word overlap metrics used extensively literature evaluating dialogue response models particular show metrics small positive correlation chitchat oriented twitter dataset correlation technical ubuntu dialogue corpus. word embedding metrics show true even though metrics able signiﬁcantly distinguish baseline state-of-the-art models across multiple datasets. highlight shortcomings metrics using statistical analysis survey’s results; qualitative analysis examples data; exploration sensitivity metrics. focus metrics model-independent i.e. model generating response also evaluate quality; thus consider word perplexity although used evaluate unsupervised dialogue models computed per-response basis cannot computed retrieval models. further consider metrics used evaluate proposed responses ground-truth responses consider retrieval-based metrics recall several recent works unsupervised dialogue systems adopt bleu score evaluation. ritter formulate unsupervised learning problem translating context candidate response. statistical machine translation model generate responses various contexts using twitter data show outperforms information retrieval baselines according bleu human evaluations. sordoni extend idea using recurrent language model generate responses context-sensitive manner. also evaluate using bleu however produce multiple ground truth responses retrieving responses elsewhere corpus using simple bag-of-words model. evaluate proposed diversity-promoting objective function neural network models using bleu score single ground truth response. modiﬁed version bleu deltableu takes account several humanevaluated ground truth responses shown weak moderate correlation human judgements using twitter dialogues. however human annotation often infeasible obtain practice. galley also show that even several ground truth responses available standard bleu metric correlate strongly human judgements. signiﬁcant previous work evaluates well automatic metrics correlate human judgements machine translation natural language generation also work criticizing usefulness bleu particular machine translation many criticisms works apply dialogue generation note generating dialogue responses conditioned conversational indexes possible n-grams length number n-grams avoid drawbacks using precision score namely favours shorter sentences authors introduce brevity penalty. bleu-n maximum length n-grams considered deﬁned weighting usually uniform brevity penalty. commonly used version bleu uses modern versions bleu also sentence-level smoothing geometric mean often results scores -gram overlap note bleu usually calculated corpus-level originally designed multiple reference sentences. meteor. meteor metric introduced address several weaknesses bleu. creates explicit alignment candidate target responses. alignment based exact token matching followed wordnet synonyms stemmed tokens paraphrases. given alignments meteor score harmonic mean precision recall proposed ground truth sentence. rouge. rouge evaluation metrics used automatic summarization. consider rouge-l f-measure based longest common subsequence candidate target sentence. words occur sentences order; however unlike n-grams words contiguous i.e. words between words lcs. context fact difﬁcult problem. difﬁculty automatically evaluating language generation models lies large correct answers. dialogue response generation given solely context intuitively higher diversity translation given text source language surface realization given intermediate form given dialogue context proposed response goal automatically evaluate appropriate proposed response conversation. focus metrics compare ground truth response conversation. particular investigate approaches word based similarity metrics word-embedding based similarity metrics. ﬁrst consider metrics evaluate amount word-overlap proposed response ground-truth response. examine bleu meteor scores used machine translation rouge score used automatic summarization. metrics shown correlate human judgements target domains thoroughly investigated dialogue systems. bleu. bleu analyzes co-occurrences n-grams ground truth proposed responses. ﬁrst computes n-gram precision whole dataset twitter domain. however carried experiments different setting multiple ground truth responses rarely available practice without providing qualitative analysis results. embedding-based metrics alternative using word-overlap based metrics consider meaning word deﬁned word embedding assigns vector word. methods wordvec calculate embeddings using distributional semantics; approximate meaning word considering often co-occurs words corpus. embeddingbased metrics usually approximate sentence-level embeddings using heuristic combine vectors individual words sentence. sentence-level embeddings candidate target response compared using measure cosine distance. greedy matching. greedy matching embedding-based metric compute sentence-level embeddings. instead given sequences token greedily matched token based cosine similarity word embeddings total score averaged across words formula asymmetric thus must average greedy matching scores direction. originally introduced intelligent tutoring systems greedy approach favours responses words semantically similar ground truth response. embedding average. embedding average metric calculates sentence-level embeddings using additive composition method computing meanings phrases averaging vector representations constituent words method widely used domains example textual similarity compare ground truth response retrieved response compute cosine similarity between respective sentence level embeddings cos. vector extrema. another calculate sentence-level embeddings using vector extrema dimension word vectors take extreme value amongst word vectors sentence value sentence-level embedding similarity response vectors comintuitively apputed using cosine distance. proach prioritizes informative words common ones; words appear similar contexts close together vector space. thus common words pulled towards origin occur various contexts words carrying important semantic information away. taking extrema along dimension thus likely ignore common words. order determine correlation automatic metrics human judgements response quality obtain response diverse range response generation models recent literature including retrieval generative models. retrieval models ranking retrieval models dialogue systems typically evaluated based whether retrieve correct response corpus predeﬁned responses includes ground truth response conversation systems evaluated using recall precision metrics. however deployed real setting models access correct response given unseen conversation. thus results presented remove occurrence ground-truth response corpus model retrieve appropriate response remaining utterances. note mean correct response appear corpus all; particular exists another context dataset identical ground-truth response available selection model. evaluate model comparing retrieved response ground truth response conversation. closely imitates real-life deployment models tests ability model generalize unseen contexts. tf-idf. consider simple term frequency inverse document frequency retrieval model tf-idf statistic intends capture important given word document calculated tﬁdf |{c∈cw∈c}| contexts corpus indicates number times word appeared context total number dialogues denominator represents number dialogues word appears. order apply tf-idf retrieval model dialogue ﬁrst compute tf-idf vectors context response corpus. return response largest cosine similarity corpus either input context corpus contexts input context corpus responses dual encoder. next consider recurrent neural network based architecture called dual encoder model model consists rnns respectively compute vector representation input context response model calculates probability given response ground truth response given context taking weighted product matrix learned parameters bias. model trained using negative sampling minimize cross-entropy error pairs. knowledge application neural network models large-scale retrieval dialogue systems novel. lstm language model. baseline model lstm language model trained predict next word pair. test time model given context encodes lstm generates response using greedy beam search procedure hred. finally consider hierarchical recurrent encoder-decoder traditional encoder-decoder framework utterances context concatenated together encoding. thus information previous utterances outweighed recent utterance. hred model uses hierarchy encoders; utterance context passes ‘utterance-level’ encoder output encoders passed another ‘context-level’ encoder enables handling longer-term dependencies. conclusions incomplete analysis evaluation metrics explicitly correlated human judgement possible draw misleading conclusions examining metrics rate different models. illustrate point compare performance selected models according embedding metrics different domains ubuntu dialogue corpus contains technical vocabulary conversations often oriented towards solving particular problem non-technical twitter corpus collected following procedure ritter consider datasets since cover contrasting dialogue domains i.e. technical help casual chit-chat amongst largest publicly available corpora making good candidates building data-driven dialogue systems. results proposed embedding metrics shown table retrieval models observe model signiﬁcantly outperforms tfidf baselines metrics across datasets. further hred model signiﬁcantly outperforms basic lstm generative model domains appears similar strength model. based results might tempted conclude information captured metrics signiﬁcantly differentiates models different quality. however show next section embedding-based metrics correlate weakly human judgements twitter corpus ubuntu dialogue corpus. demonstrates metrics specifically correlated human judgements task used evaluate task. data collection. conducted human survey determine correlation human judgements quality responses score assigned metric. aimed follow procedure evaluation bleu ubuntu dialogue corpus plots represent bleu- embedding average correlation randomly selected halves human respondents volunteers computer science department author’s institution given context proposed response asked judge response quality scale indicates response appropriate sensible given context indicates response reasonable. respondents cohen’s kappa scores w.r.t. respondents standard measure inter-rater agreement respondents indicating slight agreement excluded analysis below. median score approximately roughly indicating moderate strong annotator agreement. studies asking humans evaluate text often rate different aspects separately ‘adequacy’ ‘ﬂuency’ ‘informativeness’ text evaluation focuses adequacy. consider ﬂuency proposed responses context generated human. consider informativeness because domains considered necessarily important else seems correlate highly adequacy randomly drawn elsewhere test response selected tf-idf hred models response written human annotator. chosen cover range qualities almost uniformly survey results. present correlation results between human judgements metric table compute pearson correlation estimates linear correlation spearman correlation estimates monotonic correlation. ﬁrst observation domains bleu- score previously used evaluate unsupervised dialogue systems shows weak correlation human judgement. fact found bleu- bleu- scores near-zero majority response pairs; bleu- four examples score despite this still correlate human judgements twitter corpus rate similar bleu-. smoothing constant gives tiny weight unigrams bigrams despite absence higher-order n-grams. bleu- bleu- behave scaled noisy version bleu-; thus evaluate dialogue context conversation dearest question. many thousands people panaad occupy? user panaad <number> seat capacity rizal <number> thats choose rizal think ground truth response know siting capacity thanks info user great evening. proposed response user makes sense. thanks context conversation never felt user user user long story sure wanna know bahaha thanks caring <heart> ground truth response user mind hear youre welcome <number> proposed response user know happy figure examples metrics rated response poorly humans rated highly converse responses given near-zero score bleu-n metric perform perfectly examples present examples provide intuition example-level errors become aggregated poor correlation human judgements corpus-level. responses bleu recommend choice note using test corpus larger size reported paper lead stronger correlations bleu- bleu higher number non-zero scores. interesting note that embedding metrics bleu show small positive correlation non-technical twitter domain metric signiﬁcantly correlates humans ubuntu dialogue corpus. likely correct ubuntu responses contain speciﬁc technical words less likely produced models. further possible responses ubuntu dialogue corpus intrinsically higher variability twitter conditioned context making evaluation problem signiﬁcantly difﬁcult. figure illustrates relationship metrics human judgements. include best performing metric using word-overlaps i.e. bleu- score best performing metric using word embeddings i.e. vector average plots show weak correlation cases appear random noise. seems though bleu score obtains positive correlation large number responses given score stark contrast inter-rater agreement plotted randomly sampled halves raters also calculated bleu scores removing stopwords punctuation responses. shown table weakens corfinally examined effect response length metrics considering changes scores ground truth proposed response large difference word counts. table shows bleu meteor particularly sensitive aspect compared embedding average metric human judgement. qualitative analysis. determine speciﬁcally metrics fail examine qualitative samples disagreement between metrics human rating. although show inconsistencies example-level provide intuition metrics don’t correlate human judgements corpuslevel. present figure examples embedding-based metrics bleu- score proposed response signiﬁcantly differently humans. left figure shows example embedding-based metrics score proposed response lowly humans rate highly. clear context proposed response reasonable indeed responses intend express gratitude. however proposed response different wording ground truth response therefore metrics unable separate salient words rest. suggests embedding-based metrics would benright ﬁgure shows reverse scenario embedding-based metrics score proposed response highly humans not. likely frequently occurring token fact ‘happy’ ‘welcome’ close together embedding space. however human perspective signiﬁcant semantic difference responses pertain context. metrics take account context required order differentiate responses. note responses figure overlapping n-grams greater unigrams ground truth proposed responses; thus bleu- would assign score near response. shown many metrics commonly used literature evaluating unsupervised dialogue systems correlate strongly human judgement. elaborate important issues arising analysis. constrained tasks. analysis focuses relatively unconstrained domains. work separates dialogue system dialogue planner natural language generation component applications constrained domains stronger correlations bleu metric. example propose model dialogue acts natural language sentences bleu evaluate quality generated sentences. since mapping dialogue acts natural language sentences lower diversity similar machine translation task seems likely bleu correlate better human judgements. however empirical investigation still necessary justify this. incorporating multiple responses. correlation results assume ground truth response available given context. indeed common setting recent literature training end-to-end conversation models. work using larger automatically retrieved plausible responses evaluating bleu however searching suitable metrics. provide evidence existing metrics provide good alternatives unsupervised evaluation. despite poor performance word embedding-based metrics survey believe metrics based distributed sentence representations hold promise future. word-overlap metrics simply require many ground-truth responses signiﬁcant match reasonable response high diversity dialogue responses. simple example skip-thought vectors kiros could considered. since embedding-based metrics paper consist basic averages vectors obtained distributional semantics insufﬁciently complex modeling sentence-level compositionality dialogue. instead metrics interpreted calculating topicality proposed response metrics considered paper directly compare proposed response ground-truth without considering context conversation. however metrics take account context could also considered. metrics could come form evaluation model learned data. model could either discriminative model attempts distinguish model human responses model uses data collected human survey order provide human-like scores proposed responses. finally must consider hypothesis learning models data easier solving problem dialogue response generation. hypothesis true must concede always human evaluations together metrics roughly approximate human judgements. banerjee lavie. meteor automatic metric evaluation improved correlation human judgments. proceedings workshop intrinsic extrinsic evaluation measures machine translation and/or summarization. bojar buck federmann haddow koehn leveling monz pecina post saint-amand findings workshop statistical machine translation. proceedings ninth workshop statistical machine translation pages association computational linguistics baltimore usa. cahill. correlating human automatic evaluation german surface realiser. proceedings acl-ijcnlp conference short papers pages association computational linguistics. callison-burch koehn monz peterson przybocki zaidan. findings joint workshop statistical machine translation metrics machine translation. proceedings joint fifth workshop statistical machine translation metricsmatr pages association computational linguistics. callison-burch koehn monz zaidan. findings workshop statistical machine translation. proceedings sixth workshop statistical machine translation pages association computational linguistics. espinosa rajkumar metawhite berleant. evaluation broad-coverage surface realization. proceedings conference empirical methods natural language processing pages association computational linguistics. galley brockett sordoni auli quirk dolan. deltableu discriminative metric generation tasks intrinsically diverse targets. proceedings annual meeting association computational linguistics international joint conference natural language processing galley brockett sordoni auli quirk mitchell dolan. deltableu discriminative metric generation tasks intrinsically diverse targets. arxiv preprint arxiv.. landauer dumais. solution plato’s problem latent semantic analysis theory acquisition induction representation knowledge. psychological review lasguido sakti neubig tomoki nakamura. utilizing humanto-human conversation examples multi domain chat-oriented dialog system. ieice transactions information systems schatzmann georgila young. quantitative evaluation user simulation techniques spoken dialogue systems. special interest group discourse dialogue serban sordoni bengio courville pineau. building end-toend dialogue systems using generative hierarchical aaai conference artiﬁcial neural networks. intelligence. serban sordoni lowe charlin pineau courville bengio. hierarchical latent variable encoder-decoder arxiv preprint model generating dialogues. arxiv.. sordoni galley auli brockett mitchell dolan. neural network approach contextsensitive generation conversational responses. conference north american chapter association computational linguistics stent marge singhai. evaluating evaluation methods generation presence variation. international conference intelligent text processing computational linguistics pages springer. walker litman kamm abella. paradise framework evaluproceedings ating spoken dialogue agents. eighth conference european chapter association computational linguistics pages acl. t.-h. gasic mrksic p.-h. vandyke young. semantically conditioned lstm-based natural language generarxiv preprint ation spoken dialogue systems. arxiv.. mikolov sutskever chen corrado dean. distributed representations words phrases compositionadvances neural information processing ality. systems pages m¨oller englert engelbrecht hafner jameson oulasvirta raake reithinger. memo towards automatic usability evaluation spoken dialogue services user error simulations. interspeech. papineni roukos ward zhu. bleu method automatic evaluation machine translation. proceedings annual meeting association computational linguistics papineni roukos ward henderson reeder. corpus-based comprehensive diagnostic evaluation initial arabic chinese french spanish results. proceedings second international conference human language technology research pages ritter cherry dolan. data-driven response generation social meproceedings conference empirical dia. methods natural language processing pages association computational linguistics. lintean. comparison greedy optimal assessment natural language student input using word-to-word similarity metrics. proceedings seventh workshop building educational applications using distribution kappa scores table shows full distribution scores pair human annotators. apparent scores indicating moderate agreement. suggests task reasonable well understood annotators. full scatter plots present scatterplots metrics consider correlation human judgement figures below. previously emphasized little correlation metrics bleu- bleu- scores often close zero. figure scatter plots showing correlation metrics human judgement twitter corpus ubuntu dialogue corpus plots represent vector extrema greedy matching vector averaging figure scatter plots showing correlation metrics human judgement twitter corpus ubuntu dialogue corpus plots represent bleu- bleu- bleu- bleu-", "year": 2016}