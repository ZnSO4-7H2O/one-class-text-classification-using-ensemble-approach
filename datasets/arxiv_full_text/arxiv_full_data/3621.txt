{"title": "VR-SGD: A Simple Stochastic Variance Reduction Method for Machine  Learning", "tag": ["cs.LG", "cs.CV", "math.OC", "stat.ML"], "abstract": "In this paper, we propose a simple variant of the original SVRG, called variance reduced stochastic gradient descent (VR-SGD). Unlike the choices of snapshot and starting points in SVRG and its proximal variant, Prox-SVRG, the two vectors of VR-SGD are set to the average and last iterate of the previous epoch, respectively. The settings allow us to use much larger learning rates, and also make our convergence analysis more challenging. We also design two different update rules for smooth and non-smooth objective functions, respectively, which means that VR-SGD can tackle non-smooth and/or non-strongly convex problems directly without any reduction techniques. Moreover, we analyze the convergence properties of VR-SGD for strongly convex problems, which show that VR-SGD attains linear convergence. Different from its counterparts that have no convergence guarantees for non-strongly convex problems, we also provide the convergence guarantees of VR-SGD for this case, and empirically verify that VR-SGD with varying learning rates achieves similar performance to its momentum accelerated variant that has the optimal convergence rate $\\mathcal{O}(1/T^2)$. Finally, we apply VR-SGD to solve various machine learning problems, such as convex and non-convex empirical risk minimization, leading eigenvalue computation, and neural networks. Experimental results show that VR-SGD converges significantly faster than SVRG and Prox-SVRG, and usually outperforms state-of-the-art accelerated methods, e.g., Katyusha.", "text": "abstract—in paper propose simple variant original svrg called variance reduced stochastic gradient descent unlike choices snapshot starting points svrg proximal variant prox-svrg vectors vr-sgd average last iterate previous epoch respectively. settings allow much larger learning rates also make convergence analysis challenging. also design different update rules smooth non-smooth objective functions respectively means vr-sgd tackle non-smooth and/or non-strongly convex problems directly without reduction techniques. moreover analyze convergence properties vr-sgd strongly convex problems show vr-sgd attains linear convergence. different counterparts convergence guarantees non-strongly convex problems also provide convergence guarantees vr-sgd case empirically verify vr-sgd varying learning rates achieves similar performance momentum accelerated variant optimal convergence rate finally apply vr-sgd solve various machine learning problems convex non-convex empirical risk minimization leading eigenvalue computation neural networks. experimental results show vr-sgd converges signiﬁcantly faster svrg prox-svrg usually outperforms state-of-the-art accelerated methods e.g. katyusha. index terms—stochastic optimization stochastic gradient descent variance reduction empirical risk minimization strongly convex non-strongly convex smooth non-smooth applications include deep neural networks group lasso sparse learning coding phase retrieval matrix completion conditional random ﬁelds generalized eigen-decomposition canonical correlation analysis eigenvector computation principal component analysis singular value decomposition stochastic gradient descent especially interested developing efﬁcient algorithms solve problem involving large number component functions. standard effective method solving gradient descent method including nesterov’s accelerated gradient descent accelerated proximal gradient smooth problem takes following update rule starting commonly referred learning rate machine learning step-size optimization. non-smooth -norm regularizer) typically introduce following proximal operator replace smooth functions relatively simple convex function formulation arises many places machine learning signal processing data science statistics operations research regularized empirical risk minimization instance popular choice component function binary classiﬁcation problems logistic loss i.e. log) collection training examples {±}. popular choices regularizer include -norm regularizer -norm regularizer elastic-net regularizer λx). shang zhou cheng department computer science engineering chinese university hong kong hong kong. e-mails {fhshang kwzhou jcheng}cse.cuhk.edu.hk. i.w. tsang centre artiﬁcial intelligence university technology sydney ultimo australia. e-mail ivor.tsanguts.edu.au. zhang national laboratory novel software technology nanjing university nanjing china. e-mail zljnju.edu.cn. ubtech sydney artiﬁcial intelligence centre school information technologies faculty engineering information technologies university sydney darlington australia. e-mail dacheng.taosydney.edu.au. number iterations. however per-iteration cost batch methods expensive large instead evaluating full gradient iteration efﬁcient alternative stochastic gradient descent method evaluates gradient single component function iteration much lower per-iteration cost thus successfully applied many largescale learning problems especially training deep learning models update rule index chosen uniformly random although expectation stochastic gradient estimator ∇fik unbiased estimation i.e. variance ∇fik large variance random sampling thus stochastic gradient estimators also called noisy gradients need gradually reduce step size leads slow convergence. particular even strongly convex condition standard attains slower sub-linear convergence rate accelerated recently many methods variance reduction proposed stochastic average gradient stochastic variance reduced gradient stochastic dual coordinate ascent saga stochastic primal-dual coordinate proximal variants prox-sag prox-svrg prox-sdca accelerated methods constant learning rate instead diminishing step sizes fall following three categories primal methods svrg saga dual methods sdca primal-dual methods spdc. essence many primal methods full gradient reduce variance stochastic gradient estimators well dual primal-dual methods leads revolution area ﬁrst-order optimization thus also known hybrid gradient descent method semi-stochastic gradient descent method particular strongly convex condition accelerated methods enjoy linear convergence rate oracle complexity log) obtain \u0001suboptimal solution l-smooth µ-strongly convex. oracle complexity bound shows converge faster accelerated deterministic svrg proximal variant prox-svrg particularly attractive storage requirement compared methods saga sdca require storage gradients component functions dual variables. beginning recently many acceleration techniques proposed speed stochastic variance reduced methods mentioned above. techniques mainly include nesterov’s acceleration techniques reducing number gradient calculations early iterations projection-free property conditional gradient method stochastic sufﬁcient decrease technique momentum acceleration tricks proposed accelerating catalyst framework achieved oracle complexity identical upper complexity bound hence katyusha best-known stochastic optimization method strongly convex non-strongly convex problems pointed however selecting best values parameters accelerated methods still open problem. particular accelerated stochastic variance reduction methods including katyusha require least auxiliary variable momentum parameter lead complicated algorithm design high per-iteration complexity especially high-dimensional sparse data. contributions discussions accelerated stochastic variance reduction methods applications based svrg method thus improvement svrg important research stochastic optimization. paper propose simple variant original svrg called variance reduced stochastic gradient descent snapshot point starting point epoch vrsgd average last iterate previous epoch respectively. different settings svrg prox-svrg points former last iterate latter average previous epoch. difference makes convergence analysis vr-sgd signiﬁcantly challenging svrg prox-svrg. empirical results show performance vr-sgd signiﬁcantly better counterparts svrg proxsvrg. impressively vr-sgd varying learning rates note non-smooth inequality needs revised simply replacing gradient arbitrary sub-gradient contrast non-strongly convex general convex function inequality always satisﬁed related work speed standard proximal many stochastic variance reduced methods proposed special cases problem case l-smooth µ-strongly convex roux proposed stochastic average gradient method attains linear convergence. however well incremental aggregated gradient methods saga needs store gradients memory required general similarly sdca requires storage dual variables uses memory. contrast svrg proposed johnson zhang well prox-svrg similar convergence rate sdca without memory requirements gradients dual variables. particular svrg estimator popular choice stochastic gradient estimators. update rule svrg case problem svrg prox-svrg vr-sgd strongly convex smooth linear rate unknown linear rate strongly convex non-smooth unknown linear rate linear rate unknown unknown general convex smooth general convex non-smooth unknown unknown achieves better least comparable performance accelerated methods catalyst katyusha main contributions paper summarized below. snapshot starting points vr-sgd different vectors i.e. epochs particular settings vr-sgd allow take much larger learning rates svrg e.g. thus signiﬁcantly speed convergence practice. moreover vr-sgd advantage svrg terms robustness learning rate selection. unlike proximal stochastic gradient methods e.g. prox-svrg katyusha uniﬁed update rule cases smooth nonsmooth objectives vrsgd employs different update rules cases respectively below. empirical results show gradient update rules smooth optimization problems better choices proximal update formulas provide convergence guarantees vr-sgd solving smooth/non-smooth non-strongly convex functions. comparison svrg prox-svrg convergence guarantees shown table moreover also present momentum accelerated variant vr-sgd discuss equivalent relationship empirically verify achieve similar performance variant attains optimal convergence rate finally theoretically analyze convergence properties vr-sgd option option smooth/non-smooth strongly convex functions show vr-sgd attains linear convergence. preliminary related work throughout paper denote -norm i=|xi|. denotes full gradient differentiable subgradient lipschitz continuous. epoch random inner iteration chosen index. mostly focus case problem l-smooth µ-strongly convex. common assumptions deﬁned follows. fact extend theoretical results case gradients component functions lipschitz constant general case component functions different degrees smoothness. special case problem lsmooth µ-strongly convex extend svrg proximal setting introducing proximal operator shown line algorithm based svrg estimator accelerated algorithms proposed. proximal update rules katyusha formulated follows parameters. eliminate need parameter tuning ﬁxed addition applied efﬁcient stochastic solvers compute leading eigenvectors symmetric matrix generalized eigenvectors symmetric matrices. ﬁrst method vr-pca proposed shamir convergence properties vr-pca non-convex problem also provided. garber analyzed convergence rate svrg convex function nonconvex component functions. moreover proved svrg saga minor modiﬁcations converge asymptotically stationary point non-convex functions. parallel distributed variants accelerated methods also proposed. important class stochastic methods proximal stochastic gradient method proxsvrg saga katyusha different standard variance reduction methods svrg prox-sg method uniﬁed update rule smooth non-smooth cases instance update rule prox-svrg formulated follows section propose efﬁcient variance reduced stochastic gradient descent method. different choices snapshot starting points svrg prox-svrg vectors epoch vr-sgd average last iterate previous epoch respectively. unlike existing proximal stochastic methods prox-svrg design different update rules smooth non-smooth objective functions respectively. moreover also discuss equivalent relationship vr-sgd momentum accelerated variant well extensions. snapshot starting points like svrg vr-sgd also divided epochs epoch consists stochastic gradient steps usually chosen suggested within epoch need compute full gradient option algorithm leads better robustness gradient noise also suggested fact choice also works well practice shown fig. supplementary material. therefore provide convergence guarantees algorithm either option option next section. particular effects choice option option algorithm allow taking much larger learning rates step sizes svrg practice e.g. vr-sgd svrg actually larger learning rate enjoyed vr-sgd means variance stochastic gradient estimator goes asymptotically zero faster. unlike prox-svrg whose starting point initialized average previous epoch starting point vr-sgd last iterate previous epoch. vr-sgd last iterate previous epoch becomes starting point points prox-svrg completely different thereby leading relatively slow convergence general. starting snapshot points svrg last iterate previous epoch points prox-svrg average previous epoch setting starting snapshot points vr-sgd different vectors mentioned above convergence analysis vr-sgd becomes signiﬁcantly challenging svrg prox-svrg shown section vr-sgd algorithm part propose efﬁcient vr-sgd algorithm solve problem outlined algorithm case smooth objective functions. well known original svrg works case smooth minimization problems. however many machine learning applications e.g. elastic regularized logistic regression strongly convex objective function non-smooth. solve class problems proximal variant svrg proxsvrg subsequently proposed. unlike original emphasized noise introduced random sampling inevitable generally slows convergence speed sense. however variants probably mostly used optimization algorithms deep learning particular shown adding gradient noise step noisy gradient descent escape saddle points efﬁciently converge local minimum non-convex minimization problem e.g. application deep neural networks note theoretical convergence original svrg randomly chosen however empirical results suggest option better choice option convergence guarantee svrg option strongly convex objective functions provided fig. comparison svrg vr-sgd different ﬁxed learning rates solving -norm regularized logistic regression ridge regression covtype data set. note blue lines stand results svrg lines correspond results vr-sgd learning rate algorithm ﬁxed constant. inspired existing accelerated stochastic algorithms learning rate algorithm gradually increased early iterations strongly convex non-strongly convex problems leads faster convergence different katyusha learning rate former requires gradually decayed latter needs gradually increased update rule algorithm deﬁned follows initial learning rate equivalent momentum accelerated variant inspired success momentum technique previous work present momentum accelerated variant algorithm shown algorithm unlike existing momentum techniques e.g. convex combination snapshot latest iterate xs−) wsvs algorithm option equivalent variant max{α sufﬁciently small emphasize difference options algorithm initialization theorem optimal solution problem suppose assumption holds. following inequality holds svrg vr-sgd solve smooth objective functions also directly tackle non-smooth ones. regularizer smooth e.g. -norm regularizer update rule vr-sgd unlike proximal stochastic methods proxsvrg uniﬁed update rule smooth non-smooth cases vr-sgd different update rules cases fig. demonstrates vr-sgd signiﬁcant advantage svrg terms robustness learning rate selection. vr-sgd yields good performance within range learning rate whereas performance svrg sensitive selection learning rates. thus vr-sgd convenient applied various real-world problems machine learning. fact vr-sgd much larger learning rates svrg ridge regression problems practice e.g. vrsgd svrg shown fig. although many stochastic variance reduced methods proposed them including svrg proxsvrg convergence guarantees case problem strongly convex. however non-strongly convex many machine learning applications lasso -norm regularized logistic regression. suggested class problems transformed strongly convex ones adding proximal term efﬁciently solved algorithm however reduction technique degrade performance involved algorithms theory practice thus vr-sgd directly solve non-strongly convex problems. component functions non-smooth proximal operator oracle nesterov’s smoothing homotopy smoothing techniques smoothen them thereby obtain smoothed approximations. addition directly extend vr-sgd method non-smooth setting without using smoothing techniques. considering component function different degrees smoothness picking random index non-uniform distribution much better choice commonly used uniform random sampling well without-replacement sampling withreplacement sampling done using techniques i.e. sampling probabilities proportional lipschitz constants i.e. j=lj. vr-sgd also combined accelerated techniques used svrg. instance epoch length vr-sgd automatically determined techniques instead ﬁxed epoch length. reduce number gradient calculations early iterations leads faster convergence general moreover introduce nesterov’s acceleration techniques momentum acceleration tricks improve performance vr-sgd. algorithm analysis section provide convergence guarantees vr-sgd solving smooth non-smooth general convex problems. also extend results minibatch setting. moreover study convergence properties vr-sgd solving smooth non-smooth strongly convex objective functions. convergence properties non-strongly convex part analyze convergence properties vrsgd solving general non-strongly convex problems. considering proposed algorithm different update rules smooth nonsmooth cases give convergence guarantees vrsgd cases follows. smooth objective functions ﬁrst provide convergence guarantee algorithm solving problem smooth. order simplify analysis denote lemma suppose assumption holds proofs lemma lemmas theorems included supplementary material. lemma provides upper bound expected variance variance reduced gradient estimator i.e. theorem shows oracle complexity algorithm option consistent result katyusha without reduction techniques better accelerated deterministic methods non-strongly convex functions shown fig. seen algorithm option similar performance algorithms option terms number effective passes. clearly algorithm higher complexity iteration algorithm thus report results vr-sgd section extensions vr-sgd shown mini-batching effectively decrease variance stochastic gradient estimates. therefore ﬁrst extend proposed vr-sgd method mini-batch setting well convergence results below. here denote mini-batch size selected random index outer-iteration inner-iteration m−}. deﬁnition stochastic variance reduced gradient estimator mini-batch setting deﬁned theorem proof convergence analysis different existing stochastic methods svrg prox-svrg svrg++ similarly convergence algorithm option ﬁxed learning rate guaranteed stated theorem supplementary material. results show vr-sgd attains convergence rate non-strongly convex functions. non-smooth objective functions also provide convergence guarantee algorithm option solving problem non-smooth non-strongly convex shown below. theorem hence proof omitted. hard verify based variance upper bound analyze convergence properties vr-sgd mini-batch setting shown below. theorem convex l-smooth following inequality holds convergence properties strongly convex also analyze convergence properties vr-sgd solving strongly convex problems. ﬁrst give following convergence result algorithm option also provide linear convergence guarantees algorithm option solving non-smooth strongly convex functions stated theorem supplementary material. similarly linear convergence algorithm also guaranteed smooth stronglyconvex case. results show vr-sgd attains linear convergence rate oracle complexity log) smooth non-smooth strongly convex functions. although learning rate theorem needs less much larger learning rates practice e.g. complexity analysis algorithm per-iteration cost vr-sgd dominated computation ∇fis ∇fis proximal update thus complexity svrg prox-svrg fact problems storage. result epoch requires component gradient evaluations. addition extremely sparse data introduce lazy update tricks algorithm perform update steps non-zero dimensions sample rather dimensions. words per-iteration complexity vr-sgd improved sparsity feature vectors. moreover vr-sgd much lower per-iteration complexity existing accelerated stochastic variance reduction methods katyusha updating steps additional variables shown experimental results section evaluate performance vr-sgd solving number convex non-convex problems compare performance several state-of-the-art stochastic variance reduced methods prox-svrg saga accelerated methods catalyst katyusha moreover apply vr-sgd solve machine learning problems leading eigenvalue computation neural networks. experimental setup used several publicly available data sets experiments adult covtype epsilon mnist downloaded libsvm data website. noted sample date sets normalized unit length leads upper bound lipschitz constants i.e. suggested epoch length stochastic variance reduced methods svrg proxsvrg catalyst katyusha well vrsgd. parameter tune hand learning rate speciﬁcally select learning rates since katyusha much higher periteration complexity svrg vr-sgd compare performance terms number effective passes running time computing single full gradient evaluating component gradients considered effective pass data. fair comparison implemented svrg prox-svrg saga catalyst katyusha vr-sgd matlab interface well sparse versions lazy update tricks performed experiments intel ram. source code methods available https//github.com/jnhujnhu/vr-sgd. fig. shows objective deterministic stochastic methods solving norm -norm regularized logistic regression problems seen accelerated deterministic methods similar convergence speed usually performs slightly better non-strongly convex problems. variance reduction methods signiﬁcantly outperform accelerated deterministic methods strongly nonstrongly convex cases suggesting importance variance reduction techniques. although accelerated deterministic methods faster theoretical speed svrg general convex problems discussed section converges much slower practice. vr-sgd consistently outperforms methods settings veriﬁes effectiveness vr-sgd. different choices snapshot starting points practical implementation svrg snapepoch last iterate previous epoch vectors average point previous epoch solving ridge regression lasso problems shown fig. except three different settings snapshot starting points update rules ridge regression lasso problems respectively. algorithm option consistently converges much faster algorithms options strongly convex nonstrongly convex cases. indicates setting option suggested paper better choice options stochastic optimization. fig. comparison algorithms options solving ridge regression lasso covtype. plot vertical axis shows objective value minus minimum horizontal axis denotes number effective passes. fig. comparison svrg katyusha vr-sgd proximal versions solving ridge regression problems. plot vertical axis shows objective value minus minimum horizontal axis number effective passes data. inferior performance regularization parameter relatively large shown section contrast vrsgd proximal variant similar performance former slightly outperforms latter cases suggests stochastic gradient update rules better choices proximal update rules smooth objective functions. also believe insight help design accelerated stochastic optimization methods. katyusha-i katyusha-ii usually outperform svrg proximal variant especially regularization parameter relatively small e.g. moreover seen vr-sgd proximal variant achieve much better performance methods cases also comparable katyusha-i katyusha-ii remaining cases. veriﬁes vr-sgd suitable various largescale machine learning. growing epoch size strategy early iterations subsection present general growing epoch size strategy early iterations factor otherwise ms). different doubling-epoch technique used svrg++ gradually increase epoch size early iterations. similar convergence analysis section vr-sgd growing epoch size strategy guaranteed converge. suggested svrg++ vrsgd++ vr-sgd++. note initial learning rate. compare performance solving -norm regularized logistic regression shown fig. results show vr-sgd++ converges faster vrsgd means reducing number gradient similarly also implement proximal versions original svrg vr-sgd methods denote proximal variants svrg-ii vr-sgd-ii respectively. addition original katyusha denoted katyusha-ii. fig. shows performance katyusha-i katyusha-ii solving ridge regression popular data sets adult covtype. also report results svrg vr-sgd proximal variants. clear katyusha-i usually performs better katyusha-ii converges signiﬁcantly faster case regularization parameter seems main reason katyusha proximal variant svrg different proxsvrg main difference choices snapshot point starting point. vectors former last iterate prox-svrg average point previous epoch i.e. fig. comparison saga svrg prox-svrg catalyst katyusha vr-sgd solving -norm -norm elastic regularized logistic regression problems. plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time katyusha converges much faster saga svrg prox-svrg catalyst cases regularization parameters relatively small e.g. whereas often achieves similar inferior performance parameters relatively large e.g. note implemented original algorithm option katyusha. obviously observation matches convergence properties katyusha provided mµ/l≤ katyusha attains vr-sgd converges signiﬁcantly faster saga svrg prox-svrg catalyst especially regularization parameters relatively small e.g. main reason vr-sgd much larger learning rates svrg) leads faster convergence. veriﬁes settings snapshot starting points algorithm better choices options algorithm particular vr-sgd consistently outperforms best-known stochastic method katyusha terms number passes data especially regularization parameters relatively large e.g. since vr-sgd much lower per-iteration complexity katyusha vr-sgd obvious advantage katyusha terms running time especially case sparse data shown fig. calculations early iterations lead faster convergence discussed moreover vr-sgd++ vrsgd signiﬁcantly outperform svrg++ especially regularization parameter relatively small e.g. real-world applications subsection apply vr-sgd solve number machine learning problems e.g. logistic regression nonconvex eigenvalue computation neural networks. x∈rd training examples regularization parameters. note log) formulation includes -norm -norm elastic regularized logistic regression problems. fig. shows objective decreases -norm -norm elastic regularized logistic regression problems respectively results make following observations. regularization parameters relatively large e.g. proxsvrg usually converges faster svrg strongly convex -norm regularized logistic regression) non-strongly convex -norm regularized logistic regression) cases. contrary svrg often outperforms prox-svrg regularization parameters relatively small e.g. main reason fig. comparison saga svrg svrg++ vr-sgd solving non-convex problems sigmoid loss note denotes best solution obtained running methods large number iterations multiple random initializations. moreover report classiﬁcation testing accuracies methods test sets adult mnist fig. number effective passes datasets increases. note regularization parameter seen vr-sgd method obtains higher test accuracies methods much shorter running time suggesting faster convergence. plot performance classical power iteration method vr-pca vr-sgd epsilon fig. relative error deﬁned i.e. logat x/at rd×n data matrix. note epoch length vr-pca vr-sgd suggested constant learning rate. results show stochastic variance reduced methods vr-pca vr-sgd signiﬁcantly outperform traditional method power. moreover vr-sgd method often converges much faster vr-pca. algorithms katyusha proposed learning rate katyusha least similarly learning rate used vr-sgd comparable katyusha main reason performance vr-sgd much better katyusha. also implies algorithms enjoy larger learning rates converge faster general. work shown sigmoid function usually generalizes better loss functions terms test accuracy especially outliers. here consider binary classiﬁcation four data sets adult mnist covtype rcv. note consider classifying ﬁrst class mnist. compare performance vr-sgd saga svrg svrg++ shown fig. denotes best solution obtained running methods large number iterations multiple random initializations. note saga svrg variants original saga svrg results show vr-sgd method faster convergence methods objective value much lower. implies vr-sgd yield much better solutions methods including svrg++. furthermore vr-sgd much greater advantage methods cases smaller means objective function becomes non-convex. makes vr-sgd suitable non-smooth and/or nonstrongly convex problems without using reduction techniques empirical results also showed smooth problems stochastic gradient update rules better choices proximal update formulas practical side choices snapshot starting points make vr-sgd signiﬁcantly faster counterparts svrg prox-svrg. theoretical side setting also makes convergence analysis challenging. analyzed convergence properties vrsgd strongly convex objective functions show vr-sgd attains linear convergence rate. moreover provided convergence guarantees vr-sgd non-strongly convex functions experimental results showed vr-sgd achieves similar performance momentum accelerated variant optimal convergence rate contrast svrg prox-svrg cannot directly solve non-strongly convex functions various experimental results show vr-sgd signiﬁcantly outperforms state-of-the-art variance reduction methods saga svrg prox-svrg also achieves better least comparable performance recently-proposed acceleration methods e.g. catalyst katyusha since vr-sgd much lower per-iteration complexity accelerated methods obvious advantage terms running time especially high-dimensional sparse data. veriﬁes vr-sgd suitable various large-scale machine learning. furthermore update rules vr-sgd much simpler existing accelerated stochastic variance reduction methods katyusha friendly asynchronous parallel distributed implementation similar krizhevsky sutskever hinton imagenet classiﬁcation deep convolutional neural networks proc. adv. neural inf. process. syst. zhang choromanska lecun deep learning elastic averaging proc. adv. neural inf. process. syst. used train neural networks fully-connected hidden layer nodes softmax output nodes -norm regularization). note feature mnist normalized interval -norm regularization parameter suggested implemented three algorithms best tuned polynomial learning rate adam svrg chose mini-batch size methods. note epoch length svrg vr-sgd learning rates selected ﬁxed constant. denote number nodes input output layers neural network respectively. fig. shows training error testing accuracy performance adam svrg vr-sgd number effective passes data increases. seen training loss vr-sgd much lower methods indicating faster convergence. notably vr-sgd yields much higher test accuracies methods suggesting better generalization. conclusions proposed simple variant original svrg called variance reduced stochastic gradient descent unlike choices snapshot starting points svrg prox-svrg points epoch vr-sgd average last iterate previous epoch respectively. setting allows much larger learning rates svrg e.g. vr-sgd svrg also makes vrsgd much robust learning rate selection. different existing proximal stochastic methods proxsvrg katyusha designed different update rules smooth non-smooth problems respectively schmidt babanezhad ahmed defazio clifton sarkar non-uniform stochastic average gradient method training conditional random ﬁelds proc. int. conf. artif. intell. statist. silver schrittwieser simonyan antonoglou huang guez hubert baker bolton chen lillicrap sifre driessche graepel hassabis mastering game without human knowledge nature vol. defazio bach lacoste-julien saga fast incremental gradient method support non-strongly convex composite objectives proc. adv. neural inf. process. syst. frostig kakade sidford un-regularizing approximate proximal point faster stochastic algorithms empirical risk minimization proc. int. conf. mach. learn. lian zhang zhang c.-j. hsieh zhang decentralized algorithms outperform centralized algorithms? case study decentralized parallel stochastic gradient descent proc. adv. neural inf. process. syst. flammarion bach from averaging acceleration step-size proc. conf. learn. theory qian barzilai-borwein step size stochastic gradient descent proc. adv. neural inf. process. syst. koneˇcn´y richt´arik semi-stochastic gradient descent methods optim. method softw. vol. carpenter lazy sparse stochastic gradient descent regular langford zhang sparse online learning truncated gradient mach. learn. res. vol. boyd candes differential equation modeling nesterov’s accelerated gradient method theory insights mach. learn. res. vol. fanhua shang received ph.d. degree circuits systems xidian university xi’an china currently research associate department computer science engineering chinese university hong kong. post-doctoral research fellow department computer science engineering chinese university hong kong. post-doctoral research associate department electrical computer engineering duke university durham usa. current research interests include machine learning data mining pattern recognition computer vision. james cheng assistant professor department computer science engineering chinese university hong kong hong kong. current research interests include distributed computing systems large-scale network analysis temporal networks data. ivor tsang future fellow professor artiﬁcial intelligence university technology sydney. also research director priority research centre artiﬁcial intelligence. research focuses transfer learning feature selection data analytics data trillions dimensions applications computer vision pattern recognition. research papers published top-tier journal conference papers including journal machine learning research madras journal ieee transactions pattern analysis machine intelligence ieee transactions neural networks learning systems nips icml etc. conferred natural science award ministry education china recognized contributions kernel methods. received prestigious australian research council future fellowship research regarding machine learning data. addition received prestigious ieee transactions neural networks outstanding paper award ieee transactions multimedia prize paper award number best paper awards honors reputable international conferences including best student paper award cvpr best paper award ictai also awarded eccv outstanding reviewer award. zhou kaiwen received degree computer science technology fudan university currently working toward master degree department computer science engineering chinese university hong kong hong kong. current research interests include data mining stochastic optimization machine learning large scale machine learning etc. lijun zhang received degrees software engineering computer science zhejiang university china respectively. currently associate professor department computer science technology nanjing university china. prior joining nanjing university postdoctoral researcher department computer science engineering michigan state university. research interests include machine learning optimization information redacheng professor computer science future fellow school information technologies faculty engineering information technologies inaugural director ubtech sydney artiﬁcial intelligence centre university sydney. mainly applies statistics mathematics artiﬁcial intelligence data science. research interests spread across computer vision data science image processing machine learning video surveillance. research results expounded monograph publications prestigious journals prominent conferences ieee transactions pattern analysis machine intelligence ieee transactions neural networks learning systems ieee transactions image processing journal machine learning research international journal computer vision nips cikm icml cvpr iccv eccv aistats icdm sigkdd several best paper awards best theory/algorithm paper runner award ieee icdm best student paper award ieee icdm icdm -year highest-impact paper award ieee signal processing society best paper award. received australian scopus-eureka prize gold disruptor award vice-chancellors medal exceptional research. fellow ieee iapr spie. supplementary material give detailed proofs lemmas theorems provide convergence analysis algorithm option option algorithm algorithm addition also report experimental results solving various machine learning problems real-world data sets. theorem shows algorithm option attains optimal convergence rate smooth nonstrongly convex functions. similarly convergence algorithm option guaranteed non-smooth non-strongly convex functions. appendix proofs lemma theorem proof lemma similar theorem completeness give detailed proof lemma below. proving lemma ﬁrst give prove following lemma. lemma suppose convex component function l-smooth optimal solution problem proof. order simplify notations stochastic gradient estimator deﬁned xs−) xs−). since function l-smooth implies gradient average function lipschitz similar algorithm option also analyze convergence algorithm option smooth non-strongly functions. ﬁrst give prove following lemma. lemma convex l-smooth following inequality holds corollaries viewed generalizations lemma theorem respectively hence proofs omitted. theorem corollary algorithm also achieves convergence rate non-strongly convex non-smooth functions. theorem shows mini-batch version algorithm option attains convergence rate non-strongly convex non-smooth functions. addition also provide convergence properties algorithm non-strongly convex smooth functions. assumption shows relationship gaps function values starting snapshot points epoch converge toward optimal point thus less i.e. shown fig. fact choose simple restart technique instead assumption achieve convergence guarantee. appendix convergence analysis vr-sgd++ shown section main paper vr-sgd++ variant vr-sgd uses general growing epoch size strategy early iterations factor otherwise shown algorithm note factor vr-sgd++ method constant satisfying factor svrg++ equal particular unlike doubling-epoch technique used svrg++ gradually increase epoch size early iterations epochs early iterations viewed initialization steps algorithm similar algorithm convergence algorithm also guaranteed. experimental setup paper used publicly available data sets experiments adult covtype epsilon mnist listed table fair comparison implemented state-of-the-art stochastic methods saga svrg prox-svrg catalyst katyusha vr-sgd method matlab interface conducted experiments intel ram. codes vr-sgd related methods downloaded https//github.com/jnhujnhu/vr-sgd. furthermore also implemented accelerated deterministic methods stochastic methods svrg++ comparison vr-sgd options ﬁrst compared performance vr-sgd option vr-sgd option shown fig. results show performance vr-sgd option almost identical vr-sgd option impact increasing learning rates compared performance algorithm constant varying learning rates solving -norm -norm regularized logistic regression problems shown fig. note learning rate algorithm varied according update formula initial learning rate value cases. observe algorithm varying learning rates converges much faster algorithm ﬁxed learning rates cases especially regularization parameter relatively small e.g. empirically veriﬁes importance increasing learning rate. results stochastic accelerated deterministic methods part compared well-known accelerated deterministic methods including stochastic optimization methods including stochastic variance reduced methods svrg accelerated variance reduction methods katyusha vr-sgd. note used solve smooth nonsmooth objective functions respectively. fig. shows experimental results svrg katyusha vrsgd solving -norm regularized logistic regression problems different regularization parameters. furthermore also reported experimental results solving -norm regularized logistic regression problems fig. results show stochastic variance reduction methods including svrg katyusha vr-sgd signiﬁcantly outperform accelerated deterministic methods well strongly-convex non-strongly convex cases empirically veriﬁes importance variance reduction techniques. results different choices snapshot starting points part presented experimental results algorithms three choices snapshot starting points solving ridge regression lasso problems shown fig. comparison vr-sgd method option option solving -norm regularized logistic regression ridge regression problems covtype data set. plot vertical axis shows objective value minus minimum horizontal axis number effective passes. note blue lines stand results lines correspond results results svrg++ vr-sgd++ also presented experimental results svrg++ vr-sgd++ vr-sgd shown fig. results show vr-sgd++ vr-sgd outperform svrg++ also means epoch size large svrg++ becomes slower slower later iterations. results logistic regression figs. show experimental results -norm -norm elastic regularized logistic regression different regularization parameters. results non-convex sigmoid loss finally presented experimental results figs. note denotes best solution obtained running methods large number iterations multiple random initializations. allen-zhu hazan variance reduction faster non-convex optimization proc. int. conf. mach. learn. shang cheng accelerated variance reduced stochastic admm proc. aaai conf. artif. intell. zhao arora haupt nonconvex sparse learning stochastic optimization progressive variance reduction linear convergence svrg statistical estimation arxiv.v paquette drusvyatskiy mairal harchaoui catalyst acceleration gradient-based non-convex optimization duchi ruan stochastic methods composite optimization problems arxiv. recht parallel stochastic gradient algorithms large-scale matrix completion math. prog. comp. vol. wang zhang uniﬁed variance reduction-based framework nonconvex low-rank matrix recovery proc. int. conf. shamir stochastic algorithm exponential convergence rate proc. int. conf. mach. learn. garber hazan kakade musco netrapalli sidford faster eigenvector computation shift-and-invert fig. comparison svrg katyusha vr-sgd method solving -norm regularized logistic regression problems. plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time introductory lectures convex optimization basic course. boston kluwer academic publ. tseng accelerated proximal gradient methods convex-concave optimization technical report university washington beck teboulle fast iterative shrinkage-thresholding algorithm linear inverse problems siam imaging sci. vol. robbins monro stochastic approximation method ann. math. statist. vol. zhang solving large scale linear prediction problems using stochastic gradient descent algorithms proc. int. conf. mach. learn. bubeck convex optimization algorithms complexity found. trends mach. learn. vol. silver schrittwieser simonyan antonoglou huang guez hubert baker bolton chen lillicrap sifre driessche graepel hassabis mastering game without human knowledge nature vol. fig. comparison svrg katyusha vr-sgd method solving -norm regularized logistic regression problems. plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time fig. comparison stochastic algorithms options solving ridge regression problems regularizer plot vertical axis shows objective value minus minimum horizontal axis number effective passes. fig. comparison stochastic algorithms options solving lasso problems regularizer plot vertical axis shows objective value minus minimum horizontal axis number effective passes. fig. comparison svrg katyusha vr-sgd method proximal versions solving ridge regression problems different regularization parameters adult. plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time frank wolfe algorithm quadratic programming naval res. logist. quart. vol. hazan variance-reduced projection-free stochastic optimization proc. int. conf. mach. learn. fig. comparison svrg katyusha vr-sgd method proximal versions solving ridge regression problems different regularization parameters covtype. plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time defazio simple practical accelerated method ﬁnite sums proc. adv. neural inf. process. syst. variance reduced methods non-convex composition optimization arxiv.v chen jordan nonconvex ﬁnite-sum optimization scsg methods proc. adv. neural inf. process. syst. fig. comparison svrg++ vr-sgd vr-sgd++ solving logistic regression problems regularizer plot vertical axis shows objective value minus minimum horizontal axis number effective passes. bengio learning deep architectures found. trends mach. learn. vol. huang yuan escaping saddle points online stochastic gradient tensor decomposition proc. conf. learn. flammarion bach from averaging acceleration step-size proc. conf. learn. theory qian barzilai-borwein step size stochastic gradient descent proc. adv. neural inf. process. syst. nesterov smooth minimization non-smooth functions math. program. vol. yang homotopy smoothing non-smooth problems lower complexity proc. adv. neural shamir without-replacement sampling stochastic gradient methods proc. adv. neural inf. process. syst. koneˇcn´y richt´arik semi-stochastic gradient descent methods optim. method softw. vol. carpenter lazy sparse stochastic gradient descent regularized multinomial logistic regression tech. rep. langford zhang sparse online learning truncated gradient mach. learn. res. vol. kingma adam method stochastic optimization proc. int. conf. learn. represent. glorot bengio understanding difﬁculty training deep feedforward neural networks proc. int. conf. artif. intell. statist. optimal method stochastic composite optimization math. program. vol. allen-zhu yuan improved svrg non-strongly-convex sum-of-non-convex objectives arxiv.v fig. comparison saga svrg prox-svrg catalyst katyusha vr-sgd method solving -norm regularized logistic regression problems plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time fig. comparison saga svrg prox-svrg catalyst katyusha vr-sgd method -norm regularized logistic regression problems four data sets adult covtype epsilon plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time fig. comparison saga svrg prox-svrg catalyst katyusha vr-sgd method solving elastic regularized logistic regression problems four data sets adult covtype epsilon plot vertical axis shows objective value minus minimum horizontal axis number effective passes running time fig. comparison saga svrg svrg++ vr-sgd method solving non-convex problems sigmoid loss four data sets note denotes best solution obtained running methods large number iterations multiple random initializations.", "year": 2018}