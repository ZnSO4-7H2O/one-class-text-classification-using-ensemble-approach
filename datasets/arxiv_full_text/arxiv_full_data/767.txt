{"title": "Highway and Residual Networks learn Unrolled Iterative Estimation", "tag": ["cs.NE", "cs.AI", "cs.LG", "I.2.6; I.5.1"], "abstract": "The past year saw the introduction of new architectures such as Highway networks and Residual networks which, for the first time, enabled the training of feedforward networks with dozens to hundreds of layers using simple gradient descent. While depth of representation has been posited as a primary reason for their success, there are indications that these architectures defy a popular view of deep learning as a hierarchical computation of increasingly abstract features at each layer.  In this report, we argue that this view is incomplete and does not adequately explain several recent findings. We propose an alternative viewpoint based on unrolled iterative estimation -- a group of successive layers iteratively refine their estimates of the same features instead of computing an entirely new representation. We demonstrate that this viewpoint directly leads to the construction of Highway and Residual networks. Finally we provide preliminary experiments to discuss the similarities and differences between the two architectures.", "text": "past year introduction architectures highway networks residual networks which ﬁrst time enabled training feedforward networks dozens hundreds layers using simple gradient descent. depth representation posited primary reason success indications architectures defy popular view deep learning hierarchical computation increasingly abstract features layer. report argue view incomplete adequately explain several recent ﬁndings. propose alternative viewpoint based unrolled iterative estimation—a group successive layers iteratively reﬁne estimates features instead computing entirely representation. demonstrate viewpoint directly leads construction highway residual networks. finally provide preliminary experiments discuss similarities differences architectures. deep learning thought learning many levels representation input form hierarchy concepts ﬁxed computational budget deeper architectures believed possess greater representational power consequently higher performance shallower models. intuitively layer deep neural network computes level representation. convolutional networks zeiler fergus visualized features computed layer demonstrated fact become increasingly abstract depth. refer thinking neural networks representation view probably dates back hubel wiesel representation view links layers network abstraction levels representations represents pervasive assumption many recent publications including describe success residual networks like this solely extremely deep representations obtain relative improvement coco object detection dataset. surprisingly increasing depth network beyond certain point often leads decline performance even training since adding layers cannot decrease representational power phenomenon usually attributed vanishing gradient problem therefore even though deeper models powerful principle often fall short practice. recently training feedforward networks hundreds layers become feasible invention highway networks srivastava residual networks latter widely successful computer vision advancing state many benchmarks winning several pattern recognition competitions highway networks used improve language modeling translation architectures introduced explicit goal training deeper models. however surprising ﬁndings seem contradict applicability representation view deep networks. example reported removing almost layer trained highway residual network minimal effect overall performance idea extended layerwise dropout regularizer resnets layer supposedly builds level representation previous removing layer critically disrupt input following layer. possible seems negligible effect network output? veit even demonstrated shufﬂing layers trained resnet barely affects performance. argued resnets better understood ensembles shallow networks according interpretation resnets implicitly average exponentially many subnetworks subset layers. question remains open layer subnetwork successfully operate changing input representations. this along ﬁndings begs question whether representation view appropriate understanding architectures. paper propose interpretation reconciles representation view operation highway residual networks functional blocks networks compute entirely representations; instead engage unrolled iterative estimation representations reﬁne/improve upon input representation thus preserving feature identity. transition level representation occurs dimensionality change—through projection—separates groups blocks refer stage taking perspective able explain previously elusive ﬁndings effects lesioning shufﬂing. furthermore formalize notion directly derive residual highway networks. finally present preliminary experiments compare architectures investigate relative advantages disadvantages. staying close inputs. success resnets partly attributed fact obviate need learn identity mapping difﬁcult. however learning negative identity least difﬁcult. fact residual form useful indicates residual blocks typically stay close input representation rather replacing analysis srivastava shows trained highway networks activity transform gates often sparse individual sample average activity training samples non-sparse. units learn copy inputs replace features selectively. again means features propagated unchanged rather combined refer building blocks resnet—a layers identity skip connection—as residual block analogously highway network refer collection layers gated skip connection highway block. figure illustration. figure single neural network layer directly computes desired representation. unrolled iterative estimation stage stretches computation three layers ﬁrst providing noisy estimate representation iteratively reﬁnes next layers. classic group three layers also distribute computation would produce representation layer. iterative estimation stage seen middle ground single classic neural network layer multiple classic layers lesioning. true layer computes completely features removing layer trained network would completely change input distribution next layer. would expect overall performance drop almost chance level. fact veit -layer network cifar- removing layer trained network sets classiﬁcation error around lesioning studies conducted highway networks resnets paint entirely different picture minor drop performance observed removed layer. drop pronounced early layers layers change dimensionality performance always still superior random guessing. huang take lesioning step drop entire resnet layers regularizer training. describe method training procedure enables seemingly contradictory setup train short networks deep networks test time. regularization effect procedure explained inducing implicit ensemble many shallow networks akin normal dropout. note explanation requires departure representation view layer cope possibility entire input layer removed. otherwise shallow networks ensemble would perform better chance level like lesioned net. reshufﬂing. link layers representation levels clearly challenged experiment veit layers trained -layer resnet reshufﬂed. remarkably error increases smoothly amount reshufﬂing many re-orderings result small increase error. note however layers within stage reshufﬂed since dimensionality swapped layers must match. veit take results evidence resnets behave ensembles exponentially many shallow networks. representation view guided neural networks research providing intuitions meaning computations. section augment representation view deal incongruities hopefully enable future research deep architectures reap beneﬁts. target modiﬁcation mapping layers/blocks network levels abstraction. point interesting note one-to-one mapping neural network layers levels abstraction implicit assumption rather stated part representation view. recent deep learning textbook explicitly states depth ﬂowchart computations needed compute representation concept much deeper graph concepts themselves. strict sense evidence section fact contradict representation view residual highway networks. conﬂicts idea unrolled iterative estimation. propose think blocks highway residual networks performing unrolled iterative estimation representations. mean blocks stage work together estimate iteratively reﬁne single level representation. ﬁrst layer stage already provides estimate ﬁnal representation. subsequent layer stage reﬁne estimate without changing level representation. ﬁrst layer stage detects simple shapes rest layers stage work level too. good initial estimate representation average correct even though might high variance. thus formalize notion \"preserving feature identity\" unbiased different layers estimator target representation. means units estimators latent feature refers value towards i-th feature converging. unbiased estimator condition written expected difference estimator ﬁnal feature note depend samples data-generating distribution thus random variables. fact depend also reason need keep within expectation cannot write contrasted single classic block multiple classic blocks iterative estimation case blocks within stage produce estimates representation whereas classical stage intermediate representations would different highway residual networks address problem training deep architectures improving error identity skip connections allow units copy inputs next layer unchanged. design principle originally introduced long short-term memory recurrent networks mathematically architectures correspond simpliﬁed lstm network \"unrolled\" time. highway networks unit additional gating units control much transformation applied much copy activation corresponding unit previous layer nonlinear parametric function inputs traditional feed-forward network layer written resnets simplify highway networks approach reformulating desired transformation input plus residual rationale behind easier optimize residual form original function. extreme case desired function identity amounts trivial task pushing residual zero highway networks residual networks viewed unfolded recurrent neural networks particular mathematical form lstm cell. explicitly pointed liao poggio also argue could allow residual networks emulate recurrent processing visual cortex thus adds biological plausibility. setting converts equation equation showing formulations differ precise functional form alternatively residual networks seen particular case highway networks learned. therefore residual block zero mean training equation holds said maintain feature identity. note reasonable assumption especially using batch normalization. coupled highway formula directly derived alternative ensuring equation assume estimate highway layers result optimal linearly combine former estimate minimal. minimum variance estimate i.e. requiring highway networks coupled gates mixing coefﬁcients always one. ensures expectation estimate always correct precise value mixing determine variance estimate. bound variance less equal variance previous layer restricting mixing coefﬁcients positive. highway networks done using logistic sigmoid activation function transform gate restriction equivalent assumption sign. assumption holds example error estimate independent case covariance zero thus alphas positive. using logistic sigmoid activation function transform gate means preactivation implicitly estimates log. easy logistic sigmoid figure experimental corroboration equation average estimation error empirical estimate equation block stage stays close zero stages -layer resnet trained ilsvrc- dataset. standard deviation estimation error decreases depth increases stage indicating iterative reﬁnement representations. gives another simple case independent estimates used compute empirical mean standard deviation estimation error validation subset blocks four residual stages network. finally mean empirical mean standard deviation computed three spatial dimensions. figure shows ﬁrst three stages mean estimation error indeed close zero. indicates valid interpret role residual blocks network iteratively reﬁning representation. moreover stage standard deviation estimation error decreases successive blocks indicating convergence reﬁnement procedure. note stage four appears underestimating representation values indicating probable weak link architecture. resnets many derived architectures share common characteristics divided stages residual blocks share dimensionality. stages input dimensionality changes typically down-sampling increase number channels. stages typically also increase length early stages consist fewer layers compared later ones. interpret design choices iterative estimation point view. perspective level representation stays within stage identity shortcut connections. stages level representation changed projection change dimensionality. means expect type features detected similar within stage jump abstraction stages. view also suggests ﬁrst stages shorter since level representations tend relatively simple need little figure feature visualization reproduced kind permission authors. shows response single ﬁlter evolves three blocks stage -layer resnet trained imagenet. left visualization patches imagenet validation maximally activated ﬁlter. right corresponding guided backpropagation visualizations shown. many visualization studies examined activities trained convolutional networks found evidence supporting representation view. however studies conducted networks designed iterative estimation. interpretation paints different picture networks learn unrolled iterative estimation. networks observe stages layers corresponding levels representation. indeed visualization residual network features supports iterative estimation view. figure reproduce visualizations study observe residual layers dimensionality learn features reﬁned sharpened. visualizations show response single ﬁlter changes three residual blocks within stage -layer residual network trained image classiﬁcation. note ﬁlter appears reﬁne response including surrounding context rather changing across blocks stage. ﬁrst block nine activating patches ﬁlter include three light sources specular highlights. later blocks incorporation spatial context eight nine maximally activating patches specular highlights. similar reﬁnement behavior observed throughout different stages network. another ﬁnding line implication iterative estimation view cases sharing weights residual blocks within stage doesn’t deteriorate performance much similarly renals shared weights transform carry gates thin deep highway network still achieving better performance normal deep neural networks residual networks. staying close inputs. iteratively re-estimating variable staying close value common operation changing signiﬁcantly. reason resnet formulation makes sense learning identity hard needed frequently. also explains sparse transform gate activity trained highway networks networks learn dynamically selectively update individual features keeping representation intact. lesioning. another implication iteration view processing layers incremental somewhat interchangeable. layer reﬁnes already reasonable estimate representation. follows removing layers like lesioning experiments mild effect ﬁnal result change overall representation next layer receives quality. following layer still perform mostly operation even somewhat noisy input. layer dropout ampliﬁes effect explicitly training network work variable number iterations. dropping random layers penalizes iterations relying other could another explanation regularization effect technique. without limitations. network could learn depend speciﬁc order reﬁnements would disturbed shufﬂing lesioning. expect effects moderate many cases indeed reported literature. preceding sections show construct highway residual architectures mathematically grounded learning unrolled iterative estimation. common feature architectures preserve feature identities primary difference different biases towards switching feature identities. unfortunately since current understanding computations required solve complex problems limited extremely hard priori architecture suitable type problems. therefore section perform case studies comparing contrasting behavior experimentally. studies based applications residual highway layers respectively effective. deep residual networks outperformed entries imagenet classiﬁcation challenge. study compare performance -layer convolutional highway residual networks imagenet classiﬁcation. examine importance depth task— shallower networks already outperformed deep residual networks original residual network benchmarks instead goal fairly compare architectures test following claims regarding deep convolutional highway networks train -layer convolutional highway network based -layer residual network design networks identical every convolution operation) except unlike residual blocks highway blocks sets layers learn combine using coupled highway formulation. train slight variations highway network highway design residual block addition i.e. conv-bn-relu-conv-bn-relu-conv-bn highway-full additional third relu operation added. design conv-bn-relu-conv-bn-relu-conv-bn-sigmoid. proposed initially highway layers learned using receptive ﬁelds number parameters. transform gate biases start training. fair comparison number feature maps throughout highway network reduced total number parameters close residual network. training algorithm learning rate schedule kept used residual network. plots figure show residual network data better—its ﬁnal training loss lower highway network. ﬁnal performance networks validation similar residual network producing slightly better top- classiﬁcation error highway network. highway-full network produces even closer results mean error results contradict claims above since highway networks easy train without requiring bias tuning. however support claim since highway network appears slightly underﬁt compared residual network suggesting lower capacity number parameters. importance expressive gating. mismatch results claims made explained based importance sufﬁciently expressive transform gates. experiments highway networks used convolutions transform gate instead receptive ﬁelds gates primary transformation done srivastava change design appears primary cause instabilities learning since gates longer function effectively. therefore important equally expressive transformations highway networks. role batch normalization. since architectures built-in ease optimization compared plain networks interesting investigate necessity batch normalization training networks. derivation section suggest residual networks could take role inductive bias towards iterative estimation keeping expected mean residual zero investigate role train networks without batch normalization. resulting training curves shown figure supplementary. without networks reach even lower training error performing worse validation indicating increased overﬁtting both. shows necessary training networks speed learning. interestingly effect pronounced highway network data better resnet. contradicts claim since highway network number parameters residual network demonstrates slightly higher capacity. hand networks produce higher validation error—.% highway residual network respectively—indicating clear case overﬁtting. means batch normalization provides regularization beneﬁts can’t easily explained either improved optimization inductive bias residual networks. next compare different functional forms highway network formulation case character-aware language modeling. shown utilizing highway fully connected layers instead conventional plain layers improves model performance variety languages. architecture consists stack convolutional layers followed highway layers lstm layer predicts next word based history. similar architectures since utilized obtaining substantial improvements large-scale language modeling character level machine translation highway layers coupled gates used studies. four highway layers necessary obtain signiﬁcant modeling improvements studies above. thus reasonable assume central advantage using highway layers task easing credit assignment depth improved modeling bias. test well residual variants highway networks perform compare several language models trained penn treebank dataset using setup code provided lstm-char-large model changing highway layers different variants. following variants tested full original highway formulation based lstm cell. note variant uses parameters others since changing layer size reduce parameters would affect rest network architecture well. c-only highway variant carry gate transform gate t-only highway variant transform gate carry gate residual residual form transform carry gate always one. variant four layers instead match amount computation/parameters variants. test perplexity model shown table full coupled c-only variants similar performance better t-only variant substantially better residual variant. residual variant results performance close obtained using single plain layer even though four residual layers used. learned gating identity connection crucial improving performance task. recall highway layers transform character-aware representations feeding lstm layer. thus non-contextual word-level representations resulting convolutional layers transformed representations better suited contextual language modeling. since unlikely entire representation needs change completely setting well iterative estimation perspective. interestingly table shows signiﬁcant advantage variants multiplicative gate inputs. results suggest setting crucial dynamically replace parts input representation. features need changed drastically conditioned detected features word type features need retained. result even though residual networks compatible iterative estimation best choice tasks mixing adaptive feature transform/replacement reuse required. paper offers perspective highway residual networks performing unrolled iterative estimation. extension popular representation view stands contrast optimization perspective architectures originally introduced. according view successive layers cooperate compute single level representation. therefore ﬁrst layer already computes rough estimate representation iteratively reﬁned successive layers. unlike layers conventional neural network compute representation layers therefore preserve feature identity. shown residual highway networks directly derived perspective. offers uniﬁed theory architectures understood approaches problem. view provides framework understand several surprising recent ﬁndings like resilience lesioning beneﬁts layer dropout mild negative effects layer reshufﬂing. together derivations results serve compelling evidence validity perspective. motivated conceptual similarities compare highway residual networks. preliminary experiments found give similar results networks equal size thus refuting claims highway networks would need parameters form gating impairs performance residual networks. another example found non-gated identity skip-connections perform signiﬁcantly worse offered possible explanation task requires dynamically replacing individual features gating beneﬁcial. preliminary evidence presented report meant starting point investigation. hope unrolled iterative estimation perspective provide valuable intuitions help guide research understanding improving possibly combining exciting techniques. authors wish thank faustino gomez steunebrink jonathan masci sjoerd steenkiste christian osendorfer feedback support. grateful nvidia corporation providing dgx- part pioneers research award. research supported project input srivastava rupesh greff klaus schmidhuber juergen. training deep networks. cortes lawrence sugiyama garnett advances neural information processing systems curran associates inc. szegedy christian ioffe sergey vanhoucke vincent alemi alex. inception-v inceptionresnet impact residual connections learning. arxiv. february assume random variables noisy measurements third random variable call corresponding variances looking linear estimator +qa+qb minimum variance.", "year": 2016}