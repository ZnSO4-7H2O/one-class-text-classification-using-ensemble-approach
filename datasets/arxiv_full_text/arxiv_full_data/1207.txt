{"title": "Do Deep Neural Networks Learn Facial Action Units When Doing Expression  Recognition?", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "Despite being the appearance-based classifier of choice in recent years, relatively few works have examined how much convolutional neural networks (CNNs) can improve performance on accepted expression recognition benchmarks and, more importantly, examine what it is they actually learn. In this work, not only do we show that CNNs can achieve strong performance, but we also introduce an approach to decipher which portions of the face influence the CNN's predictions. First, we train a zero-bias CNN on facial expression data and achieve, to our knowledge, state-of-the-art performance on two expression recognition benchmarks: the extended Cohn-Kanade (CK+) dataset and the Toronto Face Dataset (TFD). We then qualitatively analyze the network by visualizing the spatial patterns that maximally excite different neurons in the convolutional layers and show how they resemble Facial Action Units (FAUs). Finally, we use the FAU labels provided in the CK+ dataset to verify that the FAUs observed in our filter visualizations indeed align with the subject's facial movements.", "text": "despite appearance-based classiﬁer choice recent years relatively works examined much convolutional neural networks improve performance accepted expression recognition benchmarks importantly examine actually learn. work show cnns achieve strong performance also introduce approach decipher portions face inﬂuence cnn’s predictions. first train zero-bias facial expression data achieve knowledge state-of-the-art performance expression recognition benchmarks extended cohn-kanade dataset toronto face dataset qualitatively analyze network visualizing spatial patterns maximally excite different neurons convolutional layers show resemble facial action units finally labels provided dataset verify faus observed ﬁlter visualizations indeed align subject’s facial movements. figure visualization facial regions activate selected ﬁlters convolutional layer network trained extended cohn-kanade dataset. corresponds ﬁlter conv layer display spatial patterns images. facial expressions provide natural compact humans convey emotional state another party. therefore designing accurate facial expression recognition algorithms crucial development interactive computer systems artiﬁcial intelligence. extensive work area found small number regions change human changes expression located around subject’s eyes nose mouth. paul ekman proposed facial action coding system enumerated regions described every facial expression described combination multiple action units corresponding particular muscle group face. however computer previous work facial expression recognition split broad categories au-based/rule-based methods appearance-based methods. au-based methods would detect presence individual explicitly classify person’s emotion based combinations originally proposed friesen ekman unfortunately detector required careful hand-engineering ensure good performance. hand appearance-based methods modeled person’s expression general facial shape texture. last years many well-established problems computer vision greatly beneﬁted rise convolutional neural networks appearance-based classiﬁer. tasks object recognition object detection face recognition seen huge boosts performance several accepted benchmarks. unfortunately tasks facial expression recognition experienced performance gains magnitude. little work done much deep cnns help accepted expression recognition benchmarks. paper seek answer following questions cnns improve performance emotion recognition datasets/baselines learn? propose training established facial expression datasets analyzing learn visualizing individual ﬁlters network. work apply visualization techniques proposed zeiler fergus springenberg individual neurons network excited corresponding spatial patterns displayed pixel space using deconvolutional network. visualizing discriminative spatial patterns many ﬁlters excited regions face corresponded facial action units subset spatial patterns shown figure show cnns trained emotion recognition task learn features correspond strongly faus proposed ekman demonstrate result ﬁrst visualizing spatial patterns maximally excite different ﬁlters convolutional layers networks using ground truth labels verify faus observed ﬁlter visualizations align subject’s facial movements. also show model based works originally proposed achieve knowledge state-of-the-art performance extended cohn-kanade dataset toronto face dataset facial expression recognition systems main machinery matches quite nicely traditional machine learning pipeline. speciﬁcally face image passed classiﬁer tries categorize several expression classes anger disgust fear neutral happy surprise. cases prior passed classiﬁer face image pre-processed given feature extractor. rather recently appearance-based expression recognition techniques relied hand-crafted features speciﬁcally time systems based hand-crafted features able achieve impressive results accepted expression recognition benchmarks japanese female facial expression database extended cohn-kanade dataset multi-pie dataset however recent success deep neural networks caused many researchers explore feature representations learned data. surprisingly almost methods used form unsupervised pre-training/learning initialize models. hypothesize scarcity labeled data prevented authors training completely supervised model experience heavy overﬁtting. authors trained multi-layer boosted deep belief network achieved state-of-the-art accuracy jaffe datasets. meanwhile authors used convolutional contractive auto-encoder underlying unsupervised model. performed semi-supervised encoding function called contractive discriminant analysis separate discriminative expression features unsupervised representation. works based unsupervised deep learning also tried analyze relationship faus learned feature representations. authors learned patch-based ﬁlter bank using k-means low-level feature. features used select receptive ﬁelds corresponding speciﬁc receptive ﬁelds subsequently passed multi-layer restricted boltzmann machines classiﬁcation. receptive ﬁelds selected using mutual information criterion image feature expression label. earlier work susskind showed ﬁrst layer features deep belief network trained generate facial expression images appeared learn ﬁlters sensitive face parts. conduct similar analysis except underlying model visualize spatial patterns excite higher-level neurons network. authors’ knowledge works previously applied cnns expression data kahou jung authors developed system audio/visual emotion recognition emotion recognition wild challenge authors trained network incorporated appearance geometric features recognition. however point works dealt emotion recognition video image sequence data therefore actively incorporated temporal data computing predictions. figure network architecture network consists three convolutional layers containing ﬁlters respectively size followed relu activation functions. pooling layers ﬁrst convolutional layers quadrant pooling third. three convolutional layers followed fully-connected layer containing hidden units softmax layer. contrast work deals emotion recognition single image focus analyzing features learned network. thus demonstrate effectiveness cnns existing emotion classiﬁcation baselines also qualitatively show network able learn patterns face images correspond facial action units experiments present paper classic feed-forward convolutional neural network. networks shown visually figure consist three convolutional layers ﬁlters respectively ﬁlter sizes followed relu activation functions. pooling layers placed ﬁrst convolutional layers quadrant pooling applied third. quadrant pooling layer followed full-connected layer hidden units ﬁnally softmax layer classiﬁcation. softmax layer contains anywhere between outputs corresponding number expressions present training set. modiﬁcation introduce classical conﬁguration ignore biases convolutional layers. idea introduced ﬁrst memisevic fully-connected networks later extended paine convolutional layers. experiments found ignoring bias allowed network train quickly simultaneously reducing number parameters learn. mentum weight decay parameter constant learning rate form annealing. parameters layer randomly initialized drawing gaussian distribution zero mean standard deviation nfan number input connections layer drawn uniformly range also dropout various forms data augmentation regularize network combat overﬁtting. apply dropout fully-connected layer probability data augmentation apply random transformation input image consisting translations horizontal ﬂips rotations scaling pixel intensity augmentation. models trained using anna software library facial expression datasets experiments extended cohn-kanade database toronto face dataset database contains image sequences assigned expression labels anger contempt disgust fear happy surprise. fair comparison follow protocol used previous works ﬁrst frame sequence neutral frame addition last three expressive frames form dataset. leads total images classes total. split frames subject independent subsets manner presented perform -fold cross-validation. amalgamation several facial expression datasets. contains images annotated expression labels anger disgust fear happy neutral surprise. labeled samples divided folds containing train validation test set. train models using training fold pick best performing model using split’s validation evaluate split’s test average results folds. datasets images grayscale size pixels. case faces already detected normalized subjects’ eyes distance apart vertical coordinates. meanwhile dataset simply detect face image resize pre-processing employ patchwise mean subtraction scaling unit variance. performance toronto face database first analyze discriminative ability assessing performance dataset. table shows recognition accuracy obtained training zero-bias random initialization regularization well cnns dropout data augmentation also include recognition accuracies previous methods. results table main observations surprisingly regularization signiﬁcantly boosts performance data augmentation improves performance regular dropout furthermore dropout data augmentation used model able exceed previous state-of-the-art performance performance extended cohn-kanade present results dataset. dataset usually contains eight labels however many works ignore samples labeled neutral contempt evaluate basic emotions. therefore ensure fair comparison trained separate models. present eight class model results table class model results table eight class model conduct study observe rather similar results. again regularization appears play signiﬁcant role obtaining good performance. data augmentation gives signiﬁcant boost performance combined dropout leads increase. eight class class models achieve state-of-the-art near state-of-the-art accuracy respectively dataset. strong discriminative model hand analyze facial regions neural network identiﬁes discriminative performing classiﬁcation. this employ visualization technique presented zeiler fergus dataset consider third convolutional layer ﬁlter images chosen split’s training generated strongest magnitude response. leave strongest neuron high activations zero deconvolutional network reconstruct region pixel space. experiments chose training images. reﬁne reconstructions employing technique called guided backpropagation proposed springenberg guided backpropogation aims improve reconstructed spatial patterns solely relying masked activations given toplevel signal deconvolution also incorporating knowledge activations suppressed forward pass. therefore layer’s output deconvolution stage masked twice relu table correspondences visualization plots shown figure whose activation distribution highest divergence value. divergence values faus computed ﬁlter shown figure first analyze patterns discovered toronto face dataset figure select ﬁlters third convolutional layer ﬁlter present spatial patterns top- images training set. images reader several ﬁlters appear sensitive regions align several facial actions units corner puller brow lowerer corner depressor next display patterns discovered dataset. figure again select ﬁlters third convolutional layer ﬁlter present spatial patterns top- images training set. reader notice discriminative spatial patterns clearly deﬁned correspond nicely facial action units corner puller nose wrinkler mouth stretch addition categorical labels dataset also contains labels denote faus present image sequence. using labels present preliminary experiment verify ﬁlter activations/spatial patterns learned indeed match actual faus shown subject image. experiment aims answer following question particular ﬁlter samples whose activation values strongly differ activations given training images corresponding labels activations sample layer ﬁlter since examining convolutional layer network then ﬁlters visualized figure following figure shows charts divergences computed faus ﬁlters displayed figure largest divergence value denoted corresponding name documented table ﬁlter. results majority cases faus listed table match facial regions visualized figure means samples appear strongly inﬂuence activations particular ﬁlters indeed possess shown corresponding ﬁlter visualizations. thus show certain neurons neural network implicitly learn detect speciﬁc faus face images given relatively loose supervisory signal results appear conﬁrm intuitions cnns work appearance-based classiﬁers. instance ﬁlter appear sensitive patterns correspond surprising almost always associated smiles visualizations figure subject often shows teeth smiling highly distinctive appearance cue. similarly ﬁlter surprising different activation distributions given ﬁlter’s spatial patterns corresponded shape made mouth region surprised faces another visually salient cue. figure visualization spatial patterns activate selected ﬁlters conv layer network trained toronto face dataset corresponds ﬁlter conv layer. display images elicited maximum magnitude response. notice spatial patterns appear correspond facial action units. work showed qualitatively quantitatively cnns trained emotion recognition indeed able model high-level features strongly correspond faus. qualitatively showed portions face yielded discriminative information visualizing spatial patterns maximally excited different ﬁlters convolutional layers learned networks. meanwhile quantitatively correlated numerical activations visualized ﬁlters subject’s actual facial movements using labels given dataset. finally demonstrated zero-bias achieve state-of-the-art recognition accuracy extended cohn-kanade dataset toronto face dataset figure visualization spatial patterns activate selected ﬁlters conv layer network trained cohn-kanade dataset. corresponds ﬁlter conv layer. again display images elicited maximum magnitude response. notice spatial patterns appear clear correspondences facial action units.", "year": 2015}