{"title": "Survey of Visual Question Answering: Datasets and Techniques", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "Visual question answering (or VQA) is a new and exciting problem that combines natural language processing and computer vision techniques. We present a survey of the various datasets and models that have been used to tackle this task. The first part of the survey details the various datasets for VQA and compares them along some common factors. The second part of this survey details the different approaches for VQA, classified into four types: non-deep learning models, deep learning models without attention, deep learning models with attention, and other models which do not fit into the first three. Finally, we compare the performances of these approaches and provide some directions for future work.", "text": "visual question answering exciting problem combines natural language processing computer vision techniques. present survey various datasets models used tackle task. ﬁrst part survey details various datasets compares along common factors. second part survey details different approaches classiﬁed four types non-deep learning models deep learning models without attention deep learning models attention models ﬁrst three. finally compare performances approaches provide directions future work. visual question answering task emerged last years getting attention machine learning community task typically involves showing image computer asking question image computer must answer. answer could following forms word phrase yes/no answer choosing several possible answers blank answer. visual question answering important appealing task combines ﬁelds computer vision natural language processing. computer vision techniques must used understand image techniques must used understand question. moreover must combined effectively answer question context image. challenging survey describes prominent datasets models used tackle visual question answering task provides comparison well models perform various datasets. section covers datasets section describes models section discusses results provides possible future directions. daquar dataset question answering realworld images released ﬁrst dataset benchmark released task. takes images nyudepth dataset contains images along semantic segmentations. every pixel image labeled object class possible classes. images indoor scenes. total images authors generated question answer pairs ways automatically using question templates. authors deﬁne templates questions whose answers taken existing nyu-depth dataset annotations. example question template ‘how many using human annotations. asked participants generate questions answers constraint answers must either colors numbers classes sets these. resultant dataset contains total questionauthors propose evaluation metrics dataset simple accuracy good metric multi-word answers second wups score gives score generated answer based average match answer ground truth answers. typically wups score thresholded used generate questions. separate three workers used rate questions less positive votes discarded. multiple choice answers generated automatically workers. workers also asked draw bounding boxes objects mentioned question image ﬁrstly resolve textual ambiguity secondly enable answers visual nature dataset contains images questions. visual madlibs dataset ﬁll-in-the-blanks well multiple choice dataset. images collected ms-coco. descriptive ﬁll-in-theblank questions generated automatically using templates object information. question generated answered group workers. answer word phrase. multiple choices blanks also provided additional evaluation benchmark. dataset contains images questions. multiple choice questions evaluated accuracy metric. visualw visual dataset generated using images ms-coco dataset image captioning recognition segmentation. visualw dataset gets name generating multiple-choice questions form workers amazon mechanical turk coco-qa dataset another dataset based ms-coco. questions answers generated automatically using image captions ms-coco broadly belong four categories object number color location. question image answers single-word. dataset contains total images. evaluation done using either accuracy wups score. candidate responses. evaluated open-ended answers machine generated answer normalized evaluation system evaluated score min. answer considered completely correct matches responses least three human annotators. matches none candidate responses given score original dataset ms-coco images questions abstract images questions. iteration challenge bigger dataset total ms-coco abstract images average questions image. exact number questions mentioned challenge website. task proposed deep learning approaches already gained wide popularity state-of-the-art performance various vision tasks result almost work literature involves deep learning approaches opposed classical approaches like graphical models. couple models non-neural approach detailed ﬁrst subsection. addition several simple baselines authors involve non-neural methods also described. second sub-section describes deep learning models involve freestyle multilingual image question answering dataset takes images ms-coco dataset uses baidu crowdsourcing server workers generate questions answers. answers words phrases full sentences. question/answer pairs available chinese well english translations. dataset contains images questions. propose human evaluation visual turing test reason dataset gained much popularity. visual question answering dataset widely used dataset task. dataset released part visual question answering challenge. divided parts dataset contains real-world images ms-coco another dataset contains abstract clipart scenes created models humans animals remove need process noisy images perform high level reasoning. questions answers generated crowd-sourced workers answers obtained question unique workers. answers typically word short phrase. approximately questions answer. evaluation open-ended answer generation well multiple choice formats available. multiple choice questions non-deep learning approaches answer type prediction propose bayesian framework predict answer type question generate answer. possible answer types vary across datasets consider. instance coco-qa consider four answer types object color counting location. model three probabilities numerator three separate models. second third probabilities modeled using logistic regression. features used question skip-thought vector representation question ﬁrst probability modeled conditional multivariate gaussian similar principle quadratic discriminant analysis. original image features used model. authors also introduced simple baselines like feeding image features question features logistic regression classiﬁer feeding image question features logistic regressor feeding features multi-layer perceptron. evaluated results daquar coco-qa visualw datasets. question. world representation image. original image image along additional features obtained segmentation. evaluated using deterministic evaluation function. obtained training simple log-linear model. model called swqa authors extend swqa model multi-world scenario model uncertainty segmentation class labeling. different labelings lead different worlds probability modeled segments along distribution class labels segment. sampling distribution segment give possible world. equation becomes intractable authors sample ﬁxed number worlds model called mwqa non-attention deep learning models deep learning models typically involve convolutional neural networks embed image word embeddings wordvec along recurrent neural networks embed question. embeddings combined processed various ways obtain answer. following model descriptions assume reader familiar cnns well rnn-variants like long short term memory units gated recurrent units propose baseline model called ibowimg vqa. output later layer pre-trained google model image classiﬁcation extract image features. word embeddings word question taken text features text features simple bag-ofwords. image text features concatenated softmax regression performed across answer classes. showed model achieved performance comparable several based approaches dataset. full-cnn propose cnn-only model refer full-cnn. three different cnns image encode image question encode question join combine image question encoding together produce joint representation. image uses architecture vggnet obtains -length vector second-last layer network. passed another fully connected layer image representation vector size question involves three layers convolution pooling. size convolutional receptive ﬁeld words kernel looks word along immediate neighbors. joint call multi-modal performs convolution across question representation receptive ﬁeld size convolution operation provided full image representation. ﬁnal representation multi-modal given softmax layer predict answer. model evaluated daquar coco-qa datasets. model uses encode image obtain continuous vector representation image. question encoded using lstm network input time step word embedding question word well encoded image vector. hidden vector obtained ﬁnal time step question encoding. simple words baseline authors encode question word embeddings. answer decoded different ways either classiﬁcation different answers generation answer. classiﬁcation performed fully connected layer followed softmax possible answers. generation hand performed decoder lstm. lstm time point takes input previously generated word well question image encoding. next word predicted using softmax vocabulary. important point note model shares weights encoder decoder lstms. model evaluated daquar dataset. vis+lstm model similar model. model uses ﬁnal layer vggnet obtain image encoding. lstm encode question. contrast previous model provide encoded image ﬁrst ‘word’ lstm network question. output lstm goes fully connected followed softmax layer. call model vis+lstm. authors also propose vis+blstm model uses bidirectional lstm instead. backward lstm gets image encoding ﬁrst input well. outputs lstms concatenated passed fully connected softmax layer. authors paper argue ﬁxed parameters powerful enough task. take architecture remove ﬁnal softmax layer three fully connected layers last followed softmax possible answers. second fully connected layers ﬁxed parameters. instead parameters come network. network used encode question output network passed fully connected layer give small vector candidate parameter weights. vector mapped larger vector parameter weights required second fully connected layer above using inverse hashing function. hashing technique included authors avoid predict full parameter weights could expensive lead over-ﬁtting. dynamic parameter layer alternatively seen multiplying image representation question representation together joint representation opposed combining linear attention based techniques popular techniques used across many tasks like machine translation image captioning etc. task attention models involve focusing important parts image question order effectively give answer. look propose attention-based model henceforth referred wtl. vggnet encoding image concatenate outputs ﬁnal layers vggnet obtain image encoding. question representation obtained averaging word vectors word question. attention vector computed image features decide region image give importance vector computed following image features question embedding imporgj attention weights obtained normalising ﬁnal image representation attention weighted different regions. concatenated question embedding passed dense+softmax layer. model evaluated dataset. loss function margin based loss takes account model step previous model ways. firstly uses lstms encode question secondly computes attention image repeatedly scanning word question. concretely repeatedly compute attention weighted image features time step lstm. goes additional input next time step lstm. attention weights used obtain computed using dense+softmax layer previous hidden state lstm image itself. thus intuitively read question repeatedly decide parts image attend parts attend depend current word well previous attention weighted image ht−. evaluated visualw dataset textual answering task well pointing task softmax crossentropy loss actual predicted answer used textual answering task. pointing task likelihood candidate region obtained taking product feature representing region last state lstm. cross-entropy loss used train model. image ﬁner-grained visual information predict answer. however previous model word word model ﬁrst encodes entire question using either lstm cnn. question encoding used attend image using similar equation before. attention weighted image concatenated question encoding used compute attention original image. repeated times question ﬁnal image representation used predict answer. authors argue sort ‘stacked’ attention helps model iteratively discard unimportant regions image. authors experiment report results daquar coco-qa datasets. paper differs previous attention based methods addition modelling visual attention also models question attention part question give importance model forms coattention parallel co-attention image question attend simultaneously. done computing afﬁnity matrix tanh learnable weight matrix. represents afﬁnity word question region image. matrix used obtained image question attention vectors. alternating co-attention. iteratively attend image followed query followed image word phrase question level. question level representation obtained lstm word phrase level representation obtained cnns. present results cocoqa datasets model involves generating neural network individual image question. done choosing various sub-modules based question composing generate neural network. modules kinds attention dog) classiﬁcation reattention measurement combination decide modules compose together ﬁrst parse question using dependency parser dependency create symbolic expression based head word. example paper ‘what standing ﬁeld?’ becomes what. symbolic forms used identify modules use. whole system trained backpropagation. authors test model dataset also challenging synthetic dataset found dataset require much high level reasoning composition. present anything model tries leverage information external knowledge base help guide visual question answering. ﬁrst obtains attributes like object names properties etc. images based caption image. image captioning model trained using standard image captioning techniques ms-coco dataset. possible attributes attribute generator trained ms-coco using variation net. attributes used generate queries dbpedia database query returns text summarized using docvec summary passed additional input decoder lstm generates answer. authors show results coco-qa datasets. trend recent years deep learning models outperform earlier graphical model based approaches across datasets. however interesting note answer type prediction model performs better non-attention models proves simply introducing convolutional and/or recurrent neural networks enough identifying parts image relevant principled manner important. even competitive better attention models like look stacked attention networks signiﬁcant improvement shown hierarchical co-attention networks ﬁrst attend question addition image. helpful especially longer questions harder encode single vector representation lstms/grus ﬁrst encoding word using image attend important words helps model perform better. neural module networks uses novel interesting idea automatically composing sub-modules image/question pair performs similar coatt dataset outperforms models synthetic dataset requiring high level reasoning indicating could valuable approach real world. however investigation required judge performance model. best performing model coco-qa anything incorporates information external knowledge base possible reason improved performance knowledge base helps answer questions involve world common sense knowledge present dataset. performance model good dataset might many questions dataset require world knowledge. model naturally gives rise avenues future work. ﬁrst would recognizing external knowledge needed sort model hybrid coatt along decision maker whether access might provide best worlds. decision might even soft enable training. second direction would exploring knowledge bases like freebase nell openie extractions seen novel ways computing attention continue improve performance task. seen textual question answering task well recent models space used guide models. study providing estimated upper bound performance various datasets would valuable well idea scope possible improvement especially coco-qa automatically generated. finally tasks treat answering classiﬁcation task. dataset allows answer generation limited manner. would interesting explore answering generation task deeply dataset collection effective evaluation methodologies remain open question. deep learning methods continue models receiving attention showing state-of-the-art results. surveyed prominent models listed performance various large-scale datasets. signiﬁcant improvements performance continue seen many datasets means still plenty room future innovation task.", "year": 2017}