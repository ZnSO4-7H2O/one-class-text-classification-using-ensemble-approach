{"title": "One-Class Classification: Taxonomy of Study and Review of Techniques", "tag": ["cs.LG", "cs.AI"], "abstract": "One-class classification (OCC) algorithms aim to build classification models when the negative class is either absent, poorly sampled or not well defined. This unique situation constrains the learning of efficient classifiers by defining class boundary just with the knowledge of positive class. The OCC problem has been considered and applied under many research themes, such as outlier/novelty detection and concept learning. In this paper we present a unified view of the general problem of OCC by presenting a taxonomy of study for OCC problems, which is based on the availability of training data, algorithms used and the application domains applied. We further delve into each of the categories of the proposed taxonomy and present a comprehensive literature review of the OCC algorithms, techniques and methodologies with a focus on their significance, limitations and applications. We conclude our paper by discussing some open research problems in the field of OCC and present our vision for future research.", "text": "one-class classiﬁcation algorithms build classiﬁcation models negative class either absent poorly sampled well deﬁned. unique situation constrains learning eﬃcient classiﬁers deﬁning class boundary knowledge positive class. problem considered applied many research themes outlier/novelty detection concept learning. paper present uniﬁed view general problem presenting taxonomy study problems based availability training data algorithms used application domains applied. delve categories proposed taxonomy present comprehensive literature review algorithms techniques methodologies focus signiﬁcance limitations applications. conclude paper discussing open research problems ﬁeld present vision future research. traditional multi-class classiﬁcation paradigm aims classify unknown data object several pre-deﬁned categories problem arises unknown data object belong categories. assume training data comprising instances fruits vegetables. binary classiﬁer applied problem unknown test object given classiﬁcation. test data object entirely diﬀerent domain classiﬁer always classify either fruit vegetable wrong result cases. sometimes classiﬁcation task allocate test object predeﬁned categories decide belongs particular class not. example apple belongs class fruits not. one-class classiﬁcation classes well characterized instances training data class either instances them form statistically-representative sample negative concept. motivate importance one-class classiﬁcation consider scenarios. situation occur instance want monitor faults machine. classiﬁer detect machine showing abnormal/faulty behaviour. measurements normal operation machine easy obtain. hand possible faults would occurred reality hence little training data egative class. also want wish wait faults occur involve high cost machine malfunction risk human operators. another example automatic diagnosis disease. relatively easy compile positive data negative data diﬃcult obtain since patients database cannot assumed negative cases never tested tests expensive. alternatively disease ‘rare’ diﬃcult collect positive samples suﬃciently large group contracted disease unsatisfactory approach. another example traditional binary classiﬁer text pages requires arduous preprocessing collect negative training examples. example order construct homepage classiﬁer sample homepages sample non-homepages need gleaned. situation collection negative training examples challenging either result improper sampling positive negative classes introduce subjective biases. outline rest paper paper follows. section compares multiclass classiﬁcation discusses performance measures employed algorithms. section provides overview related reviews occ. section propose taxonomy study algorithms present comprehensive review current state signiﬁcant research contributions branches proposed taxonomy. section concludes presentation discussion open research problems vision future research occ. conventional multi-class classiﬁcation problem data classes available decision boundary supported presence data objects class. conventional classiﬁers assume less equally balanced data classes work well class severely under-sampled completely absent. appears minter ﬁrst term ‘single-class classiﬁcation’ four decades context learning bayes classiﬁer requires labelled data class interest. much later moya originate term one-class classiﬁcation research work. diﬀerent researchers used terms outlier detection novelty detection concept learning single class classiﬁcation terms originate result diﬀerent applications one-class classiﬁcation applied. juszczak deﬁnes one-class classiﬁers class descriptors able learn restricted domains multi-dimensional pattern space using primarily positive examples. observed problems encountered conventional classiﬁcation problems estimation classiﬁcation error measuring complexity solution curse dimensionality generalization classiﬁcation method also appear sometimes become even prominent. stated earlier tasks either negative data objects absent available limited amount side classiﬁcation boundary determined using positive data makes problem oneclass classiﬁcation harder problem conventional two-class classiﬁcation. task deﬁne classiﬁcation boundary around positive class accepts many objects possible positive class minimizes chance accepting outlier objects. since side boundary determined hard decide basis one-class tightly boundary directions around data. also harder decide features used best separation positive outlier class objects. readers advised refer detailed literature survey outlier detection chandola readers advised refer detailed literature survey novelty detection markou singh mentioned work confusion matrix constructed compute classiﬁcation performance one-class classiﬁers. estimate true error complete probability density classes known. case one-class classiﬁcation probability density positive class known. means number positive class objects accepted one-class classiﬁer minimized. absence examples sample distribution outlier class objects possible estimate number outliers objects accepted one-class classiﬁer furthermore noted since main complication estimated nothing known therefore limited amount outlier class data required estimate performance generalize classiﬁcation accuracy one-class classiﬁer. however testing outlier class presented reasonable proportion actual accuracy values one-class classiﬁer could manipulated true representative metric; point explored detail glavin madden imbalanced dataset scenarios several performance metrics also useful f-score geometric mean recent years considerable amount research work carried ﬁeld occ. researchers proposed several algorithms deal various classiﬁcation problems. mazhelis presents review algorithms analyzed suitability context mobile-masquerader detection. paper mazhelis proposes taxonomy one-class classiﬁers classiﬁcation techniques based mazhelis’s survey describes algorithms techniques; however covers sub-spectrum problems ﬁeld occ. describe subsequent sections one-class classiﬁcation techniques developed used various researchers diﬀerent names diﬀerent contexts. survey presented mazhelis proposes taxonomy suitable evaluate applicability speciﬁc application domain mobile-masquerader detection. brew present review several algorithms along gaussian mixture models speaker veriﬁcation problem. main work revolves around front-end processing feature extraction speech data speaker imposter modelling using framework. kennedy discuss issues related resents review several approaches includes statistical neural networks support vector machines based methods. also discuss importance including non-target data building models study developing credit-scoring system identify good creditors. bergamini present brief overview algorithms biometric applications. identiﬁed broad categories development density approaches boundary approaches. bartkowiak presents survey research anomaly outlier occ. research survey mostly focussed research applications detecting unknown behaviour. khan madden present short survey recent trends ﬁeld wherein present taxonomy study methods. taxonomy based availability training data methodology used application domains applied. publication extension work khan madden comprehensive in-depth detailed. survey publication identiﬁes important research areas raises several open questions study discusses signiﬁcant contributions made researchers work neither restrict review literature pertaining particular application domain speciﬁc algorithms dependent type data model. cover many algorithms designs contexts applications applied multiple ways publication intend duplicate re-state previous review work; little research work presented found past surveys mazhelis brew kennedy bergamini bartkowiak moreover publication encompasses broader deﬁnition many. based research work carried ﬁeld using diﬀerent algorithms methodologies application domains present uniﬁed approach proposing taxonomy study problems. taxonomy divided three broad categories proposed categories mutually exclusive overlapping among research carried categories. however cover almost major research conducted using concept various contexts application domains. contributions research fall above-mentioned categories. subsequent subsections consider categories detail. duin sch¨olkopf developed various algorithms based support vector machines tackle problem using positive examples only; detailed discussion them refer section main idea behind strategies construct decision boundary around positive data diﬀerentiate outlier/negative data. many learning tasks labelled examples rare numerous unlabeled examples easily available. problem learning help unlabeled data given small labelled examples studied blum mitchell using concept cotraining. co-training approach applied data natural separation features classiﬁers built incrementally them. blum mitchell demonstrate co-training methods train classiﬁers application text classiﬁcation. assumptions features suﬃcient classiﬁcation feature sets instance conditionally independent given class provide learning guarantees learning labelled unlabeled data prove unlabeled examples boost accuracy. denis ﬁrst conduct theoretical study learning positive unlabeled data. denis proved many concept classes speciﬁcally learnable statistical queries eﬃciently learned framework using positive unlabeled data. however trade-oﬀ considerable increase number examples needed achieve learning although remains polynomial size. comit´e give evidence theoretical empirical arguments positive examples unlabeled examples boost accuracy many achine learning algorithms. noted learning positive unlabeled data possible weight target concept known learner turn estimated small labelled examples. muggleton presents theoretical study bayesian framework distribution functions examples assumed known. extend muggleton’s result noisy case; present sample complexity results learning maximizing number unlabeled examples labelled negative constraining classiﬁer label positive examples correctly. details research carried training classiﬁers labelled positive unlabeled data presented section appear ﬁrst category presents biased view classifying algorithms according whether based osvm non-osvm. however found advancements applications signiﬁcance diﬀerence osvm based algorithms shown opens separate research area right. nonetheless non-osvm based algorithms also used tackling speciﬁc research problems; details presented following sub-sections. one-class support vector machine duin seek solve problem distinguishing positive class possible data objects pattern space. constructed hyper-sphere around positive class data encompasses almost points data minimum radius. method called support vector data description svdd classiﬁer rejects given test point outlier falls outside hypersphere. however svdd reject fraction positively labelled data volume hyper-sphere decreases. hyper-sphere model svdd made ﬂexible introducing kernel functions. considers polynomial gaussian kernel found gaussian kernel works better datasets considered uses diﬀerent values width kernel. larger width kernel fewer support vectors selected description becomes spherical. also using gaussian kernel instead polynomial kernel results tighter descriptions requires data support ﬂexible boundary. tax’s method becomes ineﬃcient data high dimension. also work well large variations density exist among positive-class objects; case starts rejecting low-density target points outliers. demonstrates usefulness approach machine fault diagnostic data handwritten digit data. duin propose sophisticated method uses artiﬁcially generated outliers optimize osvm parameters order balance over-ﬁtting under-ﬁtting. fraction outliers accepted classiﬁer estimate volume feature space covered classiﬁer. compute error without outlier examples uniformly generate artiﬁcial outliers around target class. hyper-cube used high dimensional feature space becomes infeasible. case outlier objects generated hyper-cube probability accepted classiﬁer. volume artiﬁcial outliers generated tightly possible around target class. make procedure applicable high dimensional feature spaces duin propose generate outliers uniformly hyper-sphere. done transforming objects generated figure data description trained banana-shaped data set. kernel gaussian kernel diﬀerent width sizes support vectors indicated solid circles; dashed line description boundary. source sch¨olkopf present alternative approach svdd. method construct hyper-plane instead hyper-sphere around data hyper-plane maximally distant origin separate regions contain data. propose binary function returns ‘small’ region containing data elsewhere. introduce variable controls eﬀect outliers i.e. hardness softness boundary around data. sch¨olkopf suggest diﬀerent kernels corresponding variety non-linear estimators. practical implementations method sch¨olkopf svdd method duin operate comparably perform best gaussian kernel used. mentioned campbell bennett campbell bennett origin plays crucial role methods sch¨olkopf duin drawback since origin eﬀectively acts prior abnormal class instances assumed lie; termed problem origin. sch¨olkopf tested method synthetic real-world data including postal services dataset handwritten digits. experiments show algorithm indeed extracts data objects diﬃcult assigned respective classes number outliers fact identiﬁed. manevitz yousef investigate one-class information retrieval. paper proposes diﬀerent version one-class proposed sch¨olkopf method manevitz yousef based identifying outlier data representative second class. idea methodology work ﬁrst feature space assume origin member outlier class also points close origin considered noise outliers geometrically speaking vectors lying standard sub-spaces small dimension i.e. axes faces etc. treated outliers. hence vector non-zero entries indicates data object shares items chosen feature subset database treated outlier. linear sigmoid polynomial radial basis kernels used work. manevitz yousef evaluate results reuters data using frequent categories. results generally somewhat worse osvm however observe number categories increased version osvm obtains better results. present improved version approach sch¨olkopf detecting anomaly intrusion detection system higher accuracy. idea consider points close enough origin outliers origin member second class zhao method customer churn prediction wireless industry data. investigate performance diﬀerent kernel functions version one-class show gaussian kernel function detect churners polynomial linear kernel. extension work duin sch¨olkopf proposed campbell bennett present kernel algorithm uses linear programming techniques instead quadratic programming. construct surface input space envelopes data data points within surface considered targets outside regarded outliers. feature space problem condenses ﬁnding hyper-plane pulled onto data points margin remains either positive zero. hyper-plane tightly possible mean value output function minimized. accommodate outliers soft margin introduced around hyper-plane. algorithm avoids problem origin attracting hyper-plane towards centre data distribution rather repelling away point outside data distribution. work diﬀerent kernels used create hyper-planes show radial basis function kernel produce closed boundaries input space kernels drawback method highly dependent choice kernel width parameter however data size large yang madden apply particle swarm optimization calibrate parameters osvm experimentally method either matches surpass performance osvm parameters optimized using grid search method using lower time. tian propose reﬁnement sch¨olkopf’s osvm model searching optimal parameters using particle swarm optimization algorithm improving original decision function boundary movement. experiments show adjusting threshold ﬁnal decision function gives higher detection rate lower rejection rate. extends work duin propose cost-sensitive osvm algorithm called frequency-based svdd write-related svdd intrusion detection problem. svdd method gives equal cost classiﬁcation errors whereas f-svdd gives higher cost frequent short sequences occurring system calls wssvdd gives diﬀerent costs diﬀerent system calls. experiments suggest giving diﬀerent cost importance system users processes results higher performance intrusion detection svdd. yang propose neighbourhood-based osvm method fmri data objective classify individauls schizophrenia based fmri scans brains. formulation osvm assume neighbourhood consistency hypothesis used chen integrating osvm compute primal values denote distance points hyper-plane kernel space. voxel fmri image decision value computed using primal values neighbours. decision value greater given threshold regarded activated voxel otherwise regarded non-activated. experiments various brain fmri data sets show gives stable results k-means fuzzy k-means clustering algorithms. proposes one-class classiﬁcation algorithm svms using positive unlabeled data without labelled negative data discuss limitations osvmbased algorithms assessing performance osvms scenario learning unlabeled data negative examples comments induce accurate class boundary around positive data osvm requires larger amount training data. support vectors case come positive examples cannot create proper class boundary also leads either over-ﬁtting under-ﬁtting data notes numbers support vectors osvm increased overﬁts data rather accurate. presents algorithm called mapping convergence induce accurate class boundary around positive data presence unlabeled data without negative examples. algorithm phases mapping convergence. ﬁrst phase weak classiﬁer used extract strong negatives unlabeled data. second phase base classiﬁer used iteratively maximize margin positive strong negatives better approximation class boundary. also presents another algorithm called support vector mapping convergence works faster algorithm. every iteration svmc uses minimal data accuracy class boundary degraded training time also saved. however ﬁnal class boundary slightly less accurate obtained employing show svmc perform better osvm algorithm generate accurate boundaries comparable standard fully labelled data. incorporates concept fuzzy theory osvm order deal problem standard osvm sensitive outliers noise. main idea hao’s method diﬀerent training data objects contribute diﬀerently classiﬁcation estimated using fuzzy membership. versions fuzzy one-class classiﬁer proposed crisp hyper-plane constructed separate target class origin fuzzy membership associated training data noisy outlier data given fuzzy membership values fuzzy hyper-plane constructed discriminate target class others. uses gaussian kernels diﬀerent parameter settings test method handwritten digits problem. another fuzzy osvm classiﬁer proposed choi generating visually salient semantically important video segments. algorithm gives diﬀerent weights importance measure video segments one-class classiﬁers osvms section review major algorithms based one-class ensembles neural networks decision trees nearest neighbours bayesian classiﬁers methods. one-class classiﬁer ensemble. traditional multi-class classiﬁcation problems one-class classiﬁer might capture characteristics data. however using best classiﬁer discarding classiﬁers poorer performance might waste valuable information improve performance diﬀerent classiﬁers diﬀer complexity underlying training algorithm used construct them ensemble classiﬁers viable solution. serve increase performance also robustness classiﬁcation classiﬁers commonly ensembled provide combined decision averaging estimated posterior probabilities. simple algorithm known give good results multi-class problems case one-class classiﬁers situation diﬀerent. one-class classiﬁers cannot directly provide posterior probabilities target objects accurate information distribution outlier data available discussed earlier paper. cases assuming outliers uniformly distributed posterior probability estimated. mentions methods distance estimated instead probability exists combination distance probability outputs standardized combined. done types combining rules conventional classiﬁcation ensembles used. duin investigate inﬂuence feature sets inter-dependence type one-class classiﬁers best choice combination rules. normal density mixture gaussians parzen density estimation types one-class classiﬁers. four models svdd k-means clustering k-center method auto-encoder neural network experiments parzen density estimator emerges best individual one-class classiﬁer handwritten digit pixel dataset show combining classiﬁers trained diﬀerent feature spaces useful. experiments product combination rule gives best results mean combination rule suﬀers fact area covered target tends overestimated. study combining one-class classiﬁer image database retrieval show combining svdd-based classiﬁers improve retrieval precision. juszczak duin extend combining one-class classiﬁerd classifying missing data. idea form ensemble one-class classiﬁers trained feature pre-selected group features compute dissimilarity representation features. ensemble able predict missing feature values based remaining classiﬁers. compared standard methods method ﬂexible since requires signiﬁcantly fewer classiﬁers require re-training system whenever missing feature values occur. juszczak duin also show method robust small sample size problems splitting classiﬁcation problem several smaller ones. compare performance proposed ensemble method standard methods used missing features values problem several datasets address problem building multi-class classiﬁer based ensemble one-class classiﬁers studying kinds one-class classiﬁers namely svdd kernel principal component analysis construct minimum-distance-based classiﬁer ensemble one-class classiﬁers trained class assigns test data object given class based prototype distance. method gives comparable performance svms benchmark datasets; however heavily dependent algorithm parameters. also comment process could lead faster training better generalization performance provided appropriate parameters chosen. p¸ekalska proximity target object class ‘dissimilarity representation’ show discriminative properties various enhanced combining properly. three types one-class classiﬁer namely nearest neighbour generalized mean class linear programming dissimilarity data description. make types ensembles combine diﬀerent individual one-class classiﬁers representation proper scaling using ﬁxed rules e.g. average product train single one-class classiﬁer based information combine diﬀerent training objects several base classiﬁers using majority voting rule. results show methods perform signiﬁcantly better trained single representation. nanni studies combining several one-class classiﬁers using random subspace method problem online signature veriﬁcation. nanni’s method generates training sets selecting features randomly employing one-class classiﬁcation them; ﬁnal results classiﬁers combined rule. nanni uses several one-class classiﬁers gaussian model description; mixture gaussian descriptions; nearest neighbour method description; description linear programming description svdd; parzen window classiﬁer. shown fusion various classiﬁers reduce error best fusion method combination pcad. cheplygina propose apply pruning random sub-spaces one-class classiﬁers. results show pruned ensembles give better stable performance complete ensemble. bergamini present osvm biometric fusion false acceptance rates required. suggest fusion system employed level feature selection score-matching decision-making. normalization step used combining scores diﬀerent classiﬁers. bergamini z-score normalization min-max normalization column norm normalization classiﬁers combined using various ensemble rules. min-max normalization weighted gives best results nist biometric scores set. ges`u bosco present ensemble method combining one-class fuzzy classiﬁers. classiﬁer-combining method based genetic algorithm optimization procedure using diﬀerent similarity measures. ges`u bosco test method categorical datasets show whenever optimal parameters found fuzzy combination one-class classiﬁers improve overall recognition rate. bagging ensemble method combines multiple classiﬁers re-sampled data improve classiﬁcation accuracy. extends one-class information bottleneck method information retrieval introducing bagging ensemble learning. proposed ensemble emphasizes diﬀerent parts data results diﬀerent parameter settings aggregated give ﬁnal ranking experimental results show improvements image retrieval applications. shieh kamm propose ensemble method combining osvm using bagging. however bagging useful classiﬁers unstable small changes training data cause large changes classiﬁer outputs osvm unstable classiﬁer estimated boundary always encloses positive class therefore directly applying bagging osvm useful. shieh kamm propose kernel density estimation method give weights training data objects outliers least weights positive class members higher weights creating bootstrap samples. experiments synthetic real datasets show bagging osvms achieve higher true positive rate. also mention method useful application false positive rates required disease diagnosis. boosting methods widely used traditional classiﬁcation problems high accuracy ease implementation. r¨atsch propose boosting-like one-class classiﬁcation algorithm based technique called barrier optimization also show equivalence mathematical programs support vector algorithm translated equivalent boosting-like algorithm vice versa. pointed schapire boosting svms ‘essentially same’ except measure margin optimize weight vector svms l-norm implicitly compute scalar products feature space help kernel trick whereas boosting employs l-norm perform computation explicitly feature space. schapire comment svms thought ‘boosting’ approach high dimensional feature space spanned base hypotheses. r¨atsch exemplify translation procedure algorithm called one-class leveraging. building barrier methods function returned convex combination base hypotheses leads detection outliers. comment prior knowledge used boosting algorithms choice weak learners used one-class classiﬁcation show usefulness results artiﬁcially generated data postal service database handwritten characters. neural networks. ridder conduct experimental comparison various algorithms. compare number unsupervised methods classical pattern recognition several variations standard shared weight supervised neural network show adding hidden layer radial basis function improves performance. manevitz yousef show simple neural network trained ﬁlter documents positive information available. design bottleneck ﬁlter uses basic feed-forward neural network incorporate restriction availability positive examples. chose three level network input neurons output neurons hidden neurons network trained using standard backpropagation algorithm learn identity function positive examples. idea bottleneck prevents learning full identity function m-space identity small examples fact learnable. vectors network acts identity function like sub-space similar trained set. testing given vector shown network result identity vector deemed interesting otherwise deemed outlier. manevitz yousef apply auto-associator neural network document classiﬁcation problem. training check performance values test diﬀerent levels error. training process stopped point performance starts steep decline. secondary analysis performed determine optimal threshold. manevitz yousef test method compare number competing approaches nearest neighbour; prototype algorithm) conclude outperforms them. skabar describes learn classiﬁer based feed-forward neural network using positive examples corpus unlabeled data containing positive negative examples. conventional feed-forward binary neural network classiﬁer positive examples labelled negative examples output network represents probability unknown example belongs target class threshold typically used decide class unknown sample belongs however case since unlabeled data contain unlabeled positive examples output trained neural network less equal actual probability example belongs positive class. assumed labelled positive examples adequately represent positive concept hypothesized neural network able draw class boundary negative positive decision trees. several researchers used decision tress classify positive samples corpus unlabeled examples. comit´e present experimental results showing positive examples unlabeled data eﬃciently boost accuracy statistical query learning algorithms monotone conjunctions presence classiﬁcation noise present experimental results decision tree induction. modify standard algorithm algorithm uses unlabeled positive data show relevance method datasets. letouzey design algorithm based positive statistical queries instance statistical queries algorithm guesses weight target concept i.e. ratio positive instances instance space uses hypothesis testing algorithm. show algorithm estimated polynomial time learnable positive statistical queries instance statistical queries only. then design decision tree induction algorithm called posc. using positive unlabeled data present experimental results datasets comparable algorithm. comments rule learning methods simple eﬃcient learning nominal features tricky problems continuous features high dimensions sparse instance spaces. zhang perform bagging ensemble posc. classify test samples using majority voting rule. result datasets shows classiﬁcation accuracy robustness posc. could improved applying technique. based fast decision trees posc. propose one-class vfdt data streams applications credit fraud detection intrusion detection. state using proposed algorithm even data unlabeled performance one-class vfdt close standard vfdt algorithm. d´esir propose one-class random forest algorithm internally conjoins bagging random feature selection decision trees. note number artiﬁcial outliers generated convert one-class classiﬁer binary classiﬁer exponential respect size feature space availability positive data objects. propose novel algorithm generate outliers small feature spaces combining random subspace methods. method aims generating artiﬁcial outliers regions target data objects sparsely populated less areas density target data objects high. compare method osvm gaussian estimator parzen windows mixture gaussian models image medical dataset datasets show performs equally well better algorithms. nearest neighbours. presents one-class nearest neighbour method called nearest neighbour description test object accepted member target class provided local density greater equal local density nearest neighbour training set. ﬁrst nearest neighbour used local density estimation following acceptance function used several predeﬁned choices tune various parameters. diﬀerent numbers nearest neighbours considered; however increasing number neighbours decrease local sensitivity method make method less sensitive noise. instead distance nearest neighbour average distances ﬁrst neighbours also used. value threshold changed either higher lower values change detection sensitivity classiﬁer. duin proposes nearest neighbour method capable ﬁnding data boundaries sample size low. boundary thus constructed used detect targets outliers. however method disadvantage relies individual positions objects target set. method seems useful situations data distributed subspaces. test technique real artiﬁcial data useful small amounts training data exist datta modify standard nearest neighbour algorithm appropriate learn single class positive class. modiﬁed algorithm takes examples class input. learns constant maximum distance test example learned example still considered member positive class. test data object distance greater training data object considered member positive class. variable calculated examples positive class euclidean distance dist used distance function. datta also experiment another similar modiﬁcation called involves learning vector threshold example. modiﬁcation records distance closest example example. calculated munroe madden extend idea one-class tackle recognition vehicles using features extracted frontal view present results showing high accuracy classiﬁcation. compare results multi-class classiﬁcation methods comment reasonable draw direct comparisons results multiclass single-class classiﬁers training testing data sets underlying assumptions quite diﬀerent. also note performance multi-class classiﬁer could made arbitrarily worse adding vehicle types test appear training set. since one-class classiﬁers represent concept none above performance deteriorate conditions. cabral propose one-class nearest neighbour data description using concept structural risk minimization. k-nearest neighbours suﬀers limitation store training samples prototypes would used classify unseen sample. paper based idea removing redundant samples training thereby obtaining compact representation aiming improving generalization performance classiﬁer. results artiﬁcial datasets show improved performance classiﬁers also achieved considerable reduction number stored prototypes. cabral present another approach considered k-nearest neighbours arrive decision based majority voting. experiments artiﬁcial data biomedical data data repository observe version classiﬁer outperforms better algorithms. ges´u present one-class test synthetic data simulates microarray data identiﬁcation nucleosomes linker regions across dna. decision rule presented classify unknown sample training data object representing positive instance dissimilarity function data objects means positive. meaning rule least data objects dissimilarity classiﬁed positive data object otherwise classiﬁed outlier. model depends parameters values chosen using optimization methods. results shown good recognition rate synthetic data nucleosome linker regions across dna. haro-garca one-class along one-class classiﬁers identifying plant/pathogen sequences present comparison results. methods suitable owing fact genomic sequences plant easy obtain comparison pathogens build one-class classiﬁers based information sequences plant. one-class classiﬁers used glavin madden study eﬀect unexpected outliers might arise classiﬁcation according authors unexpected outliers deﬁned outliers come distribution data postive cases outlier cases training data set. experiments show one-class method reliable task detecting outliers similar binary classiﬁer. ‘kernel approach’ used various researchers implement diﬀerent ﬂavours nn-based classiﬁer multi-class classiﬁcation. khan extends idea propose variants one-class nearest neighbour classiﬁers. variants kernel distance metric instead euclidean diistance identiﬁcation chlorinated solvents absence nonchlorinated solvents. ﬁrst method ﬁnds neighbourhood nearest neighbours second method ﬁnds localized neighbourhoods neighbours. popular kernels like polynomial kernel spectroscopic kernels weighted spectral linear kernel used experiments. kernelized methods perform better standard methods one-class classiﬁer kernel distance metric performs better compared kernels. bayesian classiﬁers. datta suggests method learn na¨ıve bayes classiﬁer samples positive class data only. traditional attempts probability class given unlabeled data object assuming attributes independent applying bayes’ theorem previous calculation proportional attribute value attribute class probabilities estimated using training examples. positive class available calculation equation cannot done correctly. therefore datta modiﬁes attribute value example probability attribute’s value. probabilities diﬀerent values attribute normalized probability frequently occurring classiﬁcation test example test example predicted member positive class. datta tests positive class algorithms various datasets taken repository conclude classiﬁcation accuracy close c.’s value although decision trees learned classes whereas one-class classiﬁers learned using class. wang stolfo simple one-class method uses positive samples masquerade detection network. idea generate user’s proﬁle using unix commands compute conditional probability user self proﬁle. non-self proﬁle assume command random probability testing sample compare ratios larger value ratio likely command come user methods. wang investigate several one-class classiﬁcation methods context human-robot interaction image classiﬁcation faces non-faces. important non-standard methods used study gaussian data description kmeans principal component analysis linear programming well svdd. study performance one-class classiﬁcation methods image recognition dataset observe svdd attains better performance comparison methods studied ﬂexibility relative methods strict models separation planar shapes. also investigate eﬀect varying number features remark features always guarantee better results increase number features training data needed reliably estimate class models. ercil buke report diﬀerent technique tackle problem based ﬁtting implicit polynomial surface point cloud features model target class order separate outliers. show utility method problem defect classiﬁcation often plentiful samples non-defective class samples various defective classes. implicit polynomial ﬁtting technique show considerable improvement classiﬁcation rate addition advantage requiring data non-defective motors learning stage. fuzzy one-class classiﬁer proposed bosco pinello identifying patterns signals embedded noisy background. employed multi-layer model data processing step proposed fuzzy one-class classiﬁer testing microarray data. results show integrating methods could improve overall classiﬁcation results. juszczak propose one-class classiﬁer built minimum spanning tree target class. method based graph representation target class capture underlying structure data. method performs distance-based classiﬁcation computes distance test object closest edge. show method performs well data size small high dimensions. segu`ı suggest performance method juszczak reduced presence outliers target class. circumvent problem present bagging ensemble approaches aimed reduce inﬂuence outliers target training data show improved performance real artiﬁcially contaminated data. hempstalk present one-class classiﬁcation method combines density estimator draw reference distribution standard model estimating class probability target class. reference distribution generate artiﬁcial data second class decompose problem standard two-class learning problem show results dataset typist dataset comparable standard osvm advantage specify target rejection rate time training. silva willett present high dimension anomaly detection method uses limited amount unlabeled data. algorithm uses variational expectation-maximization method hypergraph domains allows edges connect vertices simultaneously. resulting estimate used compute degree anomalousness based false-positive rate. proposed algorithm linear number training data objects require parameter tuning. compare method osvm another k-point entropic graph method synthetic data high dimensional enron email database conclude method outperforms methods. text document classiﬁcation traditional text classiﬁcation techniques require appropriate distribution positive negative examples build classiﬁer; thus suitable problem occ. course possible manually label negative examples though depending application domain labour-intensive time-consuming task. however core problem remains diﬃcult impossible compile negative samples provides comprehensive characterization everything ‘not’ target concept assumed conventional binary classiﬁer. common practice build text classiﬁers using positive unlabeled examples collecting unlabeled samples relatively easy fast many text page domains section discuss algorithms exploit methodology application text classiﬁcation. ability build classiﬁers without negative training data useful scenario needs extract positive documents many text collections sources. nigam show accuracy text classiﬁers improved adding small amount labelled training data large available pool unlabelled data. introduce algorithm based central idea algorithm train classiﬁer based labelled documents probabilistically label unlabeled documents repeat process till converges. improve performance algorithm propose variants give weights modulate amount unlabeled data mixture components class. show using type methodology result reduction error factor study problem learning positive unlabeled data suggest many algorithms build text classiﬁers based steps identifying reliable/strong negative documents unlabeled set. step spy-em uses technique pebl uses technique called -dnf roc-svm uses rocchio algorithm building classiﬁers iteratively applying classiﬁcation algorithm selecting good classiﬁer set. step spy-em uses expectation maximization algorithm classiﬁer pebl roc-svm svm. spy-em roc-svm methods selecting ﬁnal classiﬁer. pebl simply uses last classiﬁer convergence poor choice. steps together work iterative manner increase number unlabeled examples classiﬁed negative time maintain correct classiﬁcation positive examples. shown theoretically sample size large enough maximizing number unlabeled examples classiﬁed negative constraining positive examples correctly classiﬁed give good classiﬁer. introduce methods step step perform evaluation possible combinations methods step step discussed above. develop benchmarking system called propose approach based biased formulation allows noise positive examples. experiments reuters usenet articles suggest biased-svm approach outperforms existing two-step techniques. explore svmc performing text classiﬁcation without labelled negative data. reuters webkb corpora text classiﬁcation compare method methods simple mapping convergence osvm; standard trained positive examples unlabeled documents substituted negative documents; spy-em; negative noise; ideal trained completely labelled documents. conclude reasonable number positive documents algorithm gives best performance among methods considered. analysis show positive training data under-sampled svmc signiﬁcantly outperforms methods svmc tries exploit natural positive negative documents feature space eventually helps improve generalization performance. peng present text classiﬁer positive unlabeled documents based genetic algorithms adopting stage strategy firstly reliable negative documents identiﬁed improved -dnf algorithm. secondly classiﬁers built iteratively applying algorithm training data objects sets. discuss approach evaluate weighted vote classiﬁers generated iteration steps construct ﬁnal classiﬁer based comment evolving process discover best combination weights. experiments performed reuters data compared pebl osvm shown based classiﬁcation performs better. koppel schler study authorship veriﬁcation problem examples writings single author given task determine given piece text written author. traditional approaches text classiﬁcation cannot applied directly kind classiﬁcation problem. hence present technique called ‘unmasking’ features useful distinguishing books iteratively removed speed cross-validation accuracy degrades gauged features removed. main hypothesis books written author matter diﬀerences overall essence regularity writing style captured relatively small number features. testing algorithm consider collection twenty-one century english books written diﬀerent authors spanning variety genres obtain overall accuracy errors almost equally distributed false positives false negatives. onoda report document retrieval method using non-relevant documents. users rarely provide precise query vector retrieve desired documents ﬁrst iteration. subsequent iterations user evaluates whether retrieved documents relevant correspondingly query vector modiﬁed order reduce diﬀerence query vector documents evaluated relevant user. method called relevance feedback. relevance feedback needs relevant non-relevant documents work usefully. however sometimes initial retrieved documents presented user include relevant documents. scenario traditional approaches relevance feedback document retrieval systems work well system needs relevant relevant documents construct binary classiﬁer solve problem onoda propose feedback method using information non-relevant documents only called non-relevance feedback document retrieval. design non-relevance feedback document retrieval based osvm proposed method selects documents discriminated non-relevant near discriminant hyper-plane non-relevant document relevant documents. compare proposed approach conventional relevance feedback methods vector space model without feedback show consistently gives better performance compared methods. extend concept classifying positive examples unlabeled samples collaborative filtering application. positive data gathered based user interaction like news items recommendation bookmarking pages etc. however ambiguous interpretations limited knowledge lack interest users collection valid negative data hampered. sometime negative unlabeled positive data severely mixed becomes diﬃcult discern them. manually labelling negative data intractable considering size also poorly sampled. traditional algorithms either label negative data assume missing data negative. approaches inherent problem expensive biased recommendation results. propose approaches one-class handle negative sparse data balance extent treat missing values negative examples. ﬁrst approach based weighted rank approximation works idea providing diﬀerent weights error terms positive negative examples objective function. second approach based sampling missing values negative examples. perform experiments real world data social bookmarking site del.icio.us yahoo news data show method outperforms state algorithms. denis introduce algorithm shows feasibility learning positive unlabeled documents. step method estimation word probabilities negative class negative examples available. limitation overcome assuming estimate positive class probability practical situations positive class probability empirically estimated provided domain knowledge. results webkb data show error rates classiﬁers obtained positive data objects trained enough unlabeled examples lower error rates classiﬁers obtained labeled documents. denis consider situations small positive data available together unlabeled data. constructing accurate classiﬁer situations fail shortage properly sampled data. however learning scenario still possible using co-training framework looks feature views data. propose positive na¨ıve co-training algorithm takes small pool positive documents seed. ﬁrst incrementally builds classiﬁers positive unlabeled documents views using pnb. along co-training steps self-labelled positive examples self-labelled negative examples added training sets. base algorithm also proposed variant able self-labelled examples. experiments webkb dataset show co-training algorithms lead signiﬁcant improvement classiﬁers even initial seed composed positive documents. calvo extend classiﬁcation idea build complex bayesian classiﬁers absence negative samples positive unlabeled data present. positive tree augmented positive-unlabeled scenario proposed along beta distribution model apriori probability positive class applied experiments suggest predicting attributes conditionally independent performs better proposed bayesian approach estimating apriori probability positive class also improves performance improves selecting value prior probability positive class validation using performance measure estimated positive unlabeled examples. experiments suggest proposed algorithm performs well even without user specifying prior probability positive class. propose variants nearest neighbour classiﬁer classiﬁcation uncertain data learning positive unlabeled scenario. method outperforms method hempstalk elkan noto show classiﬁer trained using labelled unlabeled data predicted probabilities diﬀer true probabilities constant factor. test method theapplication identifying protein records show performs better comparison standard biased method zhang propose one-class classiﬁcation method classiﬁcation text streams concept drift. situations text streams withe large volumes documents arrive high speed diﬃcult label positive samples labelled. propose stacked ensemble approach compare window-based approaches demonstrate better performance. blanchard present diﬀerent outlook learning positive unlabeled data develops general solution problem surrogate problem related neymanpearson classiﬁcation binary classiﬁcation problem subject constraint falsepositive rate minimizing false-negative rate. noted problem formulation outliers assumed rare special case also discussed. perform theoretical analysis deduce generalization error bounds consistency rates convergence novelty detection show approach optimally adapts unknown novelty distributions whereas traditional methods assume ﬁxed uniform distribution. experiments compare proposed method osvm several datasets comparable performance. learning positive unlabeled data studied domains well apart text classiﬁcation facial expression recognition gene regulation networks etc. subsection highlight applications one-class classiﬁcation methods necessarily employ learning positive unlabeled data. areas handwriting detection information retrieval missing data/data correction image database retrieval face/object recognition applications remote sensing stream mining chemometrics spectroscopy biometrics assistive technologies time series analysis disease detection medical analysis bioin; steganalysis spam detection audio surveillance sound speaker classiﬁcation ship detection vehicle recognition collision detection anomaly detection intrusion detection credit scoring yeast regulation prediction customer churn detection relevant sentence extraction machine vibration analysis machine fault detection recommendation tasks compression neural networks one-class classiﬁcation used detect mineral deposits fmri analysis one-class fuzzy networks explored classify cancerous cells goal algorithms induce generalized classiﬁers class well characterized training data negative outlier class either absent poorly sampled negative concept well deﬁned. limited availability data makes problem challenging interesting. research ﬁeld encompasses several research themes developed time. paper presented uniﬁed view general problem presented taxonomy study problems. observed research carried broadly represented three diﬀerent categories areas study depends upon availability training data classiﬁcation algorithms used application domain investigated. based categories proposed taxonomy presented comprehensive literature survey current state-of-the-art signiﬁcant research work ﬁeld discussing techniques used methodologies employed focus limitations importance applications. course several years algorithms emerged application areas exploited. although ﬁeld becoming mature still several fundamental problems open research describing training classiﬁers also scaling controlling errors handling outliers using non-representative sets negative examples combining classiﬁers generating sub-spaces reducing dimensionality making fair comparison errors multi-class classiﬁcation. context believe classiﬁer ensemble methods need exploration. although exist several bagging models techniques based boosting random subspace warrant attention. random subspace methods one-class variants decision trees nearest neighbour classiﬁers interesting research direction. random oracle ensemble shown fare better standard ensembles multi-class classiﬁcation problems; however explored. believe carrying research ensemble methods within domain bring interesting results. another point note osvms kernels used mostly linear polynomial gaussian sigmoidal. suggest would fruitful investigate innovative forms kernels example genetic kernels domain speciﬁc kernels weighted linear spectral kernel shown greater potential standard classiﬁcation. moreover parameter tuning standard kernels give biased results therefore believe researchers focus eﬃciently tuning optimizing kernel parameters. kernels used distance metric shown promising results basic form one-class nearest neighbour classiﬁers. believe research direction show interesting insights problem. important issue largely ignored researchers handle missing data target class still able develop robust one-class classiﬁers. kind scenario makes problem diﬃcult. khan address issue handling missing data target class using bayesian multiple imputations propose several variants one-class classiﬁer ensemble perform better traditional method mean imputation. however recommend research involving advanced data imputation methods help building one-classiﬁers handle missingness data. feature selection one-class classiﬁers diﬃcult problem challenging model behaviour features class terms discriminatory power. studies direction however lack advanced methods techniques handle high dimensional positive class data still bottleneck learning one-class classiﬁers. case abundant unlabeled examples positive examples available researchers used many diﬀerent two-step algorithms discussed section believe bayesian network approach problems would interesting research area. learning positive data using method important problem hampers ﬂexibility model estimation prior probabilities. important research direction eﬀort estimate prior distribution minimal user intervention. possible direction attempt explore mixtures beta distribution inferring prior class probability normally employed identify anomalies outliers unusual unknown behaviours failing identify novel observations costly terms risks associated health safety money. general researchers assume equal cost errors true case occ. however traditional costsensitive learning methods neither prior probability outlier class associated cost errors known. data-dependent approaches deduce cost errors training data objects generalize cost across diﬀerent domains even application area. papers reviewed research paper discusses cost-sensitive aspect shows area research largely unexplored. believe approaches based careful application preference elicitation techniques useful deduce cost errors. terms data types research work focused numerical continuous data however much emphasis given categorical mixed data approaches. similarly adaptation development methods streaming data analysis online classiﬁcation also need research eﬀort. many algorithms perform density estimation assume outliers uniformly distributed density regions however target data also lies density region methods start rejecting positive data objects thresholds increased start accepting outliers. tackle scenarios alternate formulations required. terms applications seen explored many diverse ﬁelds encouraging results. emergence assistive technologies help people medical conditions potential application patients’ individual behaviour diﬀerent medical conditions would otherwise diﬃcult handle multiclass classiﬁcation approaches. activity recognition also central problem domains. data domains normally captured sensors prone noise miss vital recordings. multi-class classiﬁers diﬃcult employ applications detect unknown anomalous behaviours related user/patient sensor itself. believe play important role modelling kinds applications. activity recognition computer vision problems even collected positive data contain instances negative class building binary classes unclean data detrimental classiﬁcation accuracy. believe employing classiﬁers post-processing ﬁlter noise target data objects conducive build better multi-class classiﬁers. finally strongly recommend development open-source software tools benchmark datasets used researchers compare validate results coherent systematic way. survey provides uniﬁed in-depth insight current study ﬁeld occ. depending upon data availability algorithm applications domain appropriate techniques applied improved upon. hope proposed taxonomy along survey provide researchers direction formulate future novel work ﬁeld.", "year": 2013}