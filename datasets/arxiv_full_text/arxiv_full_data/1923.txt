{"title": "LipNet: End-to-End Sentence-level Lipreading", "tag": ["cs.LG", "cs.CL", "cs.CV"], "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "text": "yannis assael† brendan shillingford† shimon whiteson nando freitas department computer science university oxford oxford google deepmind london cifar canada {yannis.assaelbrendan.shillingford shimon.whitesonnando.de.freitas}cs.ox.ac.uk lipreading task decoding text movement speaker’s mouth. traditional approaches separated problem stages designing learning visual features prediction. recent deep lipreading approaches end-to-end trainable however existing work models trained end-to-end perform word classiﬁcation rather sentence-level sequence prediction. studies shown human lipreading performance increases longer words indicating importance features capturing temporal context ambiguous communication channel. motivated observation present lipnet model maps variable-length sequence video frames text making spatiotemporal convolutions recurrent network connectionist temporal classiﬁcation loss trained entirely end-to-end. best knowledge lipnet ﬁrst end-to-end sentence-level lipreading model simultaneously learns spatiotemporal visual features sequence model. grid corpus lipnet achieves accuracy sentence-level overlapped speaker split task outperforming experienced human lipreaders previous word-level state-of-the-art accuracy lipreading plays crucial role human communication speech understanding highlighted mcgurk effect phoneme’s audio dubbed video someone speaking different phoneme results third phoneme perceived. lipreading notoriously difﬁcult task humans specially absence context. lipreading actuations besides lips sometimes tongue teeth latent difﬁcult disambiguate without context example fisher gives categories visual phonemes list initial consonant phonemes commonly confused people viewing speaker’s mouth. many asymmetrically confused observations similar ﬁnal consonant phonemes. consequently human lipreading performance poor. hearing-impaired people achieve accuracy even limited subset monosyllabic words compound words important goal therefore automate lipreading. machine lipreaders enormous practical potential applications improved hearing aids silent dictation public spaces security speech recognition noisy environments biometric identiﬁcation silent-movie processing. machine lipreading difﬁcult requires extracting spatiotemporal features video recent deep learning approaches attempt extract features end-to-end. existing work however performs word classiﬁcation sentence-level sequence prediction. paper present lipnet best knowledge ﬁrst end-to-end sentence-level lipreading model. modern deep learning based automatic speech recognition lipnet trained end-to-end make sentence-level predictions. model operates character-level using spatiotemporal convolutional neural networks recurrent neural networks connectionist temporal classiﬁcation loss graves empirical results grid corpus public sentence-level datasets show lipnet attains sentence-level word accuracy overlapped speakers split popular benchmarking lipreading methods. previous best accuracy reported aligned word classiﬁcation version task furthermore lipnet generalise across unseen speakers grid corpus accuracy also compare performance lipnet hearing-impaired people lipread grid corpus task. average achieve accuracy contrast lipnet’s higher accuracy sentences. finally applying saliency visualisation techniques interpret lipnet’s learned behaviour showing model attends phonologically important regions video. furthermore computing intra-viseme inter-viseme confusion matrices phoneme level show almost lipnet’s erroneous predictions occur within visemes since context sometimes insufﬁcient disambiguation. section outline various existing approaches automated lipreading. automated lipreading existing work lipreading employ deep learning. work requires either heavy preprocessing frames extract image features temporal preprocessing frames extract video features types handcrafted vision pipelines automated lipreading literature vast adequately cover refer reader zhou extensive review. notably goldschen ﬁrst visual-only sentence-level lipreading using hidden markov models limited dataset using hand-segmented phones. later neti ﬁrst sentence-level audiovisual speech recognition using combined hand-engineered features viavoice dataset. authors improve speech recognition performance noisy environments fusing visual features audio ones. dataset contains utterances speakers training publicly available. stated visual-only results cannot interpreted visual-only recognition used rescoring noisy audio-only lattices. using similar approach potamianos report speaker independent speaker adapted dataset respectively connected digit corpus contains sentences digits. furthermore gergen speaker-dependent training lda-transformed version discrete cosine transforms mouth regions hmm/gmm system. work holds previous state-of-the-art grid corpus speaker-dependent accuracy generalisation across speakers extraction motion features considered open problem noted lipnet addresses issues. classiﬁcation deep learning recent years several attempts apply deep learning lipreading. however approaches perform word phoneme classiﬁcation whereas lipnet performs full sentence sequence prediction. approaches include learning multimodal audio-visual representations learning visual features part traditional speech-style processing pipeline classifying words and/or phonemes combinations thereof many approaches mirror early progress applying neural networks acoustic processing speech recognition chung zisserman propose spatial spatiotemporal convolutional neural networks based word classiﬁcation. architectures evaluated word-level dataset reported spatiotemporal models fall short spatial architectures average around additionally models cannot handle variable sequence lengths attempt sentence-level sequence prediction. chung zisserman train audio-visual max-margin matching model learning pretrained mouth features inputs lstm -phrase classiﬁcation ouluvs dataset well non-lipreading task. garg apply pre-trained faces classifying words phrases miracl-vc dataset words phrases. however best recurrent model trained freezing vggnet parameters training rather training jointly. best model achieves word classiﬁcation accuracy phrase classiﬁcation accuracy despite -class classiﬁcation tasks. sequence prediction speech recognition ﬁeld automatic speech recognition would state today without modern advances deep learning many occurred context connectionist temporal classiﬁcation loss graves drove movement deep learning component deep systems trained end-to-end mentioned earlier much recent lipreading progress mirrored early progress stopping short sequence prediction. lipnet ﬁrst end-to-end model performs sentence-level sequence prediction visual speech recogntion. demonstrate ﬁrst work takes input sequence images outputs distribution sequences tokens; trained end-to-end using thus also require alignments. lipreading datasets lipreading datasets plentiful contain single words small. exception grid corpus audio video recordings speakers produced sentences each total hours across sentences. table summarises state-of-the-art performance main lipreading datasets. table existing lipreading datasets state-of-the-art accuracy reported these. size column represents number utterances used authors training. although grid corpus contains entire sentences gergen consider simpler case predicting isolated words. lipnet predicts sequences hence exploit temporal context attain much higher accuracy. phrase-level approaches treated plain classiﬁcation. grid corpus evaluate lipnet sentence-level data. sentences drawn following simple grammar command color preposition letter digit adverb number denotes many word choices word categories. categories consist respectively {bin place set} {blue green white} with} z}\\{w} {zero nine} lipnet neural network architecture lipreading maps variable-length sequences video frames text sequences trained end-to-end. section describe lipnet’s building blocks architecture. convolutional neural networks containing stacked convolutions operating spatially image instrumental advancing performance computer visions tasks object recognition receive image input basic convolution layer channels channels computes input weights rc×c×kw×kh deﬁne xcij bounds. spatiotemporal convolutional neural networks process video data convolving across time well spatial dimensions hence similarly gated recurrent unit type recurrent neural network improves upon earlier rnns adding cells gates propagating information timesteps learning control information ﬂow. similar long short-term memory standard formulation input sequence denotes element-wise multiplication sigm bidirectional introduced graves schmidhuber context lstms maps another bi-gru ensures deﬁne distribution length-t sequences depends parameterise distribution sequences time-step softmax) feed-forward network weights wmlp. ≤t≤t connectionist temporal classiﬁcation loss widely used modern speech recognition eliminates need training data aligns inputs target outputs given model outputs sequence discrete distributions token classes augmented special blank token computes probability sequence marginalising sequences deﬁned equivalent sequence. simultaneously removes need alignments addresses variable-length sequences. denote tokens model classiﬁes single time-step output blank-augmented vocabulary figure lipnet architecture. sequence frames used input processed layers stcnn followed spatial max-pooling layer. features extracted processed bi-grus; time-step output processed linear layer softmax. end-to-end model trained ctc. denotes blank symbol. deﬁne function that given string deletes adjacent duplicate characters removes blank tokens. label sequence u∈b− s.t. |u|=t number time-steps sequence model. example deﬁnes probability string computed efﬁciently dynamic programming allowing perform maximum likelihood. lipnet architecture figure illustrates lipnet architecture starts subsequently features extracted followed bi-grus. bi-grus crucial efﬁcient aggregation stcnn output. finally linear transformation applied time-step followed softmax vocabulary augmented blank loss. layers rectiﬁed linear unit activation functions. details including hyperparameters found table appendix preprocessing grid corpus consists subjects narrating sentences. videos speaker missing others empty corrupt leaving usable videos. employ split holding data male speakers female speakers evaluation remainder used training also sentence-level variant split similar wand random sentences speaker used evaluation. remaining data speakers pooled together training. videos seconds long frame rate fps. videos processed dlib face detector ibug face landmark predictor landmarks coupled online kalman filter. using landmarks apply afﬁne transformation extract mouth-centred crop size pixels frame. standardise channels whole training zero mean unit variance. augmentation augment dataset simple transformations reduce overﬁtting. first train regular horizontally mirrored image sequence. second since dataset provides word start timings sentence video augment sentence-level training data video clips individual words additional training instances. instances decay rate third encourage resilience varying motion speeds deletion duplication frames performed per-frame probability augmentation methods followed proposed baselines models. evaluate lipnet compare performance three hearing-impaired people lipread well three ablation models inspired recent state-of-the-art work hearing-impaired people baseline performed three members oxford students’ disability community. introduced grammar grid corpus observed minutes annotated videos training dataset annotated random videos evaluation dataset. uncertain asked pick probable answer. baseline-lstm using sentence-level training setup lipnet replicate model architecture previous deep learning grid corpus state-of-the-art appendix implementation details. baseline-d based lipnet architecture replace stcnn spatial-only convolutions similar chung zisserman notably contrary results observe lipnet chung zisserman report poorer performance stcnns compared architectures datasets. baseline-nolm identical lipnet language model used beam search disabled. measure performance lipnet baselines compute word error rate character error rate standard metrics performance models. produce approximate maximum-probability predictions lipnet performing beam search. deﬁned minimum number word insertions substitutions deletions required transform prediction ground truth divided number words ground truth. note usually equal classiﬁcation error predicted sentence number words ground truth particularly case since almost errors substitution errors. table summarises performance lipnet compared baselines. according literature accuracy human lipreaders around expected ﬁxed sentence structure limited subset words position grid corpus facilitate context increasing performance. unseen speakers split three hearing-impaired people achieve respectively yielding average wer. table performance lipnet grid dataset compared baselines measured splits evaluating unseen speakers evaluating video subset speakers’ sentences. unseen overlapped speakers evaluation highest performance achieved architectures enhanced convolutional stacks. lipnet exhibits higher performance overlapped compared unseen speakers split. unseen speakers baseline-d lipnet achieve lower respectively hearing-impaired people. unseen speakers baseline-d whereas lipnet lower similarly error rate overlapped speakers lower lipnet compared baselined. results demonstrate importance combining stcnns rnns. performance difference conﬁrms intuition extracting spatiotemporal features using stcnn better aggregating spatial-only features. observation contrasts empirical observations chung zisserman furthermore lipnet’s stcnn rnns cleanly allow processing variable-length input variable-length output sequences whereas architectures chung zisserman chung zisserman handle former. baseline-lstm exhibits lowest performance unseen overlapped speakers respectively. interestingly although baseline-lstm replicates architecture wand despite numerous data augmentation methods model performs lower reported word-level accuracy illustrating difﬁculty sentence-level task even restricted grammar. finally disabling language model baseline-nolm exhibits approximately higher proposed model. section analyse learned representations lipnet phonological perspective. first create saliency visualisations illustrate lipnet learned attend. particular feed input model greedily decode output sequence yielding alignment respect input video frame sequence unlike simonyan guided backpropagation second train lipnet predict arpabet phonemes instead characters analyse visual phoneme similarities using intra-viseme inter-viseme confusion matrices. apply saliency visualisation techniques interpret lipnet’s learned behaviour showing model attends phonologically important regions video. particular figure analyse saliency visualisations words please speaker based ashby figure saliency maps words please produced backpropagation input showing places lipnet learned attend. pictured transcription given greedy decoding. blanks denoted production word please requires great deal articulatory movement beginning lips pressed ﬁrmly together bilabial plosive time blade tongue comes contact alveolar ridge anticipation following lateral /l/. lips part allowing compressed escape lips lips open further seen distance midpoints upper lower lips lips spread close vowel /iy/ since relatively steady-state vowel position remains unchanged rest duration attention level drops considerably. lips close slightly blade tongue needs brought close alveolar ridge attention resumes. interesting since bulk frontally visible articulatory movement involves blade tongue coming contact alveolar ridge going vowel /ey/ exactly lipnet’s attention focused little change position. according deland fisher alexander graham bell ﬁrst hypothesised multiple phonemes visually identical given speaker. later veriﬁed giving rise concept viseme visual equivalent phoneme analysis phoneme-to-viseme mapping neti clustering phonemes following categories lip-rounding based vowels alveolar-semivowels alveolar-fricatives alveolar palato-alveolar bilabial dental labio-dental velar full mapping found table appendix grid corpus contain phonemes arpabet. compute confusion matrices phonemes figure intra-viseme inter-viseme confusion matrices depicting three categories confusions well confusions viseme clusters. colours row-normalised emphasise errors. group phonemes viseme clusters following neti figure shows confusion matrices confused viseme categories well confusions viseme categories. full phoneme confusion matrix figure appendix given speakers british confusion /aa/ /ay/ probably fact ﬁrst element greater part diphthong /ay/ articulatorily identical /aa/ open back unrounded vowel confusion /ih/ /ae/ ﬁrst glance surprising fact sample /ae/ occurs word function word normally pronounced reduced weak vowel /ah/. /ah/ /ih/ frequent unstressed vowels good deal variation within them e.g. private watches confusion within categories bilabial stops alveolar stops unsurprising complete closure place articulation makes look practically identical. differences velum action vocal fold vibration unobservable front. finally quality viseme categorisation neti conﬁrmed fact matrix figure diagonal minor confusion alveolar palatoalveolar visemes. articulatorily alveolar palato-alveolar fricatives distinguished small difference tongue position palate behind alveolar ridge easily observed front. said dental /th/ alveolar /t/. proposed lipnet ﬁrst model apply deep learning end-to-end learning model maps sequences image frames speaker’s mouth entire sentences. end-to-end model eliminates need segment videos words predicting sentence. lipnet requires neither hand-engineered spatiotemporal visual features separately-trained sequence model. empirical evaluation illustrates importance spatiotemporal feature extraction efﬁcient temporal aggregation conﬁrming intuition easton basala furthermore lipnet greatly outperforms human lipreading baseline exhibiting better performance lower word-level state-of-the-art grid corpus. lipnet already empirical success deep speech recognition literature suggests performance improve data. future work hope demonstrate applying lipnet larger datasets sentence-level variant collected chung zisserman applications silent dictation demand video only. however extend range potential applications lipnet apply approach jointly trained audiovisual speech recognition model visual input assists robustness noisy environments. work supported oxford-google deepmind graduate scholarship epsrc cifar. would also like thank nvidia generous donation dgx- titan gpus used experiments; ´aine jackson brittany klug samantha pugh helping measure experienced lipreader baseline; mitko sabev phonetics guidance; odysseas votsis video production help; alex graves oiwi parker jones helpful comments. amodei anubhai battenberg case casper catanzaro chen chrzanowski coates diamos deep speech end-to-end speech recognition english mandarin. arxiv preprint arxiv. ashby. understanding phonetics. routledge chung zisserman. reading wild. asian conference computer vision chung zisserman. time automated sync wild. cruttenden. gimson’s pronunciation english. routledge dahl deng acero. context-dependent pre-trained deep neural networks largeieee transactions audio speech language processing hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups. ieee signal processing magazine hochreiter schmidhuber. long short-term memory. neural computation temporal multimodal learning audiovisual speech recognition. ieee conference karpathy toderici shetty leung sukthankar fei-fei. large-scale video classiﬁcation convolutional neural networks. proceedings ieee conference computer vision pattern recognition king. dlib-ml machine learning toolkit. jmlr kingma adam method stochastic optimization. arxiv preprint arxiv. koller bowden. deep learning mouth shapes sign language. iccv workshop ieee transactions pattern analysis machine intelligence mcgurk macdonald. hearing lips seeing voices. nature neti potamianos luettin matthews glotin vergyri sison mashari. audio visual papandreou katsamanis pitsikalis maragos. multimodal fusion learning uncertain features applied audiovisual speech recognition. workshop multimedia signal processing papandreou katsamanis pitsikalis maragos. adaptive multimodal fusion uncertainty compensation application audiovisual speech recognition. ieee transactions audio speech language processing petridis pantic. deep complementary bottleneck features visual speech recognition. ieee international conference acoustics speech signal processing ieee bennamoun togneri. listening eyes towards practical visual speech recognition system using deep boltzmann machines. ieee international conference computer vision takashima aihara takiguchi ariki mitani omori nakazono. audio-visual speech recognition using bimodal-trained bottleneck features person severe hearing loss. interspeech lipnet implemented using torch warp-ctc library stanfordctc’s decoder implementation. network parameters initialised using initialisation apart square matrices orthogonally initialised described models trained channel-wise dropout pooling layer mini-batches size used optimiser adam learning rate default hyperparameters ﬁrst-moment momentum coefﬁcient second-moment momentum coefﬁcient numerical stability parameter scores computed using beam search following parameters stanford-ctc’s decoder beam width that character -gram binarised language model suggested videos processed dlib face detector ibug face shape predictor landmarks input frames normalised using following per-channel means standard deviations table summarises lipnet architecture hyperparameters denotes time denotes channels denotes feature dimension denote height width denotes number words vocabulary including blank symbol. note spatiotemporal convolution sizes depend number channels kernel’s three dimensions. spatiotemporal kernel sizes speciﬁed order input size dimensions. input dimension orderings given parentheses input size column. baseline-lstm replicates setup wand trained lipnet. model uses lstm layers neurons. input frames converted grayscale down-sampled dropout parameters initialised uniformly values", "year": 2016}