{"title": "Obtaining Accurate Probabilistic Causal Inference by Post-Processing  Calibration", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Discovery of an accurate causal Bayesian network structure from observational data can be useful in many areas of science. Often the discoveries are made under uncertainty, which can be expressed as probabilities. To guide the use of such discoveries, including directing further investigation, it is important that those probabilities be well-calibrated. In this paper, we introduce a novel framework to derive calibrated probabilities of causal relationships from observational data. The framework consists of three components: (1) an approximate method for generating initial probability estimates of the edge types for each pair of variables, (2) the availability of a relatively small number of the causal relationships in the network for which the truth status is known, which we call a calibration training set, and (3) a calibration method for using the approximate probability estimates and the calibration training set to generate calibrated probabilities for the many remaining pairs of variables. We also introduce a new calibration method based on a shallow neural network. Our experiments on simulated data support that the proposed approach improves the calibration of causal edge predictions. The results also support that the approach often improves the precision and recall of predictions.", "text": "discovery accurate causal bayesian network structure observational data useful many areas science. often discoveries made uncertainty expressed probabilities. guide discoveries including directing investigation important probabilities well-calibrated. paper introduce novel framework derive calibrated probabilities causal relationships observational data. framework consists three components approximate method generating initial probability estimates edge types pair variables availability relatively small number causal relationships network truth status known call calibration training calibration method using approximate probability estimates calibration training generate calibrated probabilities many remaining pairs variables. also introduce calibration method based shallow neural network. experiments simulated data support proposed approach improves calibration causal edge predictions. results also support approach often improves precision recall predictions. much science consists discovering modeling causal relationships nature. increasingly scientists available multiple complex data large number samples enormous number measurements recorded thanks rapid advancements sophisticated measurement technology data often purely observational. past years tremendous progress developing computational methods discovering causal knowledge observational data primary methods analyze observational data generate novel causal hypotheses likely correct subjected experimental validation; approach signiﬁcantly increase efﬁciency causal discovery science. make informed decisions novel causal hypotheses investigate experimentally scientists need know likely hypotheses true. probabilistic terms means need probabilities hypotheses well-calibrated. informally probabilities well-calibrated events predicted occur probability occur fraction time general important calibrated probabilities making decisions using decision theory. paper focus discovery causal bayesian network structure observational data. particular focus discovery causal relationships pairs measured variables. causal novel important worthwhile experimentally investigate extent worth depends part high calibrated probability causal present. introduce method calibrate edge type probabilities cbns thousands measured variables arbitrarily many latent variables. method requires following components method generating initial probability estimates edge types pair variables; general estimates need wellcalibrated truth status small unbiased sample causal relationships network call calibration training calibration method using uncalibrated probability estimates calibration training generate calibrated probabilities large number remaining pairs variables. bootstrapping method generating probability estimates edge types. method resamples dataset times replacement learns model dataset. particular dataset really fast causal inference algorithm estimate underlying generative network allowing possibility latent confounders. given pair nodes probability given edge type estimated fraction edge type networks. previously researchers successfully applied approach estimating probabilities edge types bayesian networks bootstrap estimates guaranteed represent calibrated posterior probabilities however even large sample limit number bootstrap samples. reason heuristic search practically necessary stuck local maxima. thus need estimates calibrated probabilities focus current paper. bootstrapping approach described provides empirical estimates edge-type posterior probabilities constraint-based rfci bayesian structure learning algorithms bayesian model averaging provides alternative approach estimating edge probabilities. however bayesian methods typically applicable using datasets number random variables double digits triple digits contrast interested providing calibrated estimates edge probabilities datasets contain thousands variables typically encountered modern biological data. also note bayesian model averaging methods sensitive method applied heuristic search structure parameter priors used even non-informative. consequently generated probabilities still subject possibly uncalibrated. finally computationally tractable bayesian methods discovering cbns contain latent confounders; contrast constraint-based methods exist perform discovery cbns hundreds latent variables datasets thousands variables feasible amount time assume availability calibration training allows induce mapping bootstrap probability estimates calibrated posterior probabilities. training contain truth status subset edge types. domain biomedical applications truth status might come example results published literature. emphasize calibration training small relative number total node pairs. experiments performed consists less node pairs. using goal generate better calibrated probabilities remaining node pairs. application using biomedical data example biomedical scientist chose experimentally test causal relationships high probabilities well-calibrated could conﬁdent experiments would usually corroborate relationships. introduce neural-network-based calibration method uses calibration training construct mapping bootstrap probability estimates calibrated posterior probabilities edge types node pairs apply mapping node pairs. paper simulated data investigate main questions. first calibrated bootstrap-derived probabilities edge types? second calibrated probabilities produced neural-network-based calibration method? given ﬁnite calibration training latter method guaranteed always output perfectly calibrated probabilities either. main hypothesis paper calibration method output probabilities better calibrated bootstrap probabilities least discriminative terms measures precision recall score. colombo developed algorithm called really fast causal inference identiﬁes causal structure data-generating process presence latent variables using partial ancestral graphs representation. encodes markov equivalence class bayesian networks exhibit conditional independence relationships. rfci stages adjacency search involves selective search dependencies among measured variables orientation phase orients endpoints among pairs nodes connected according ﬁrst stage. typical constraint-based causal discovery algorithms rfci outputs single graph structure provide information uncertainty edges nodes structure. considering generated rfci possible partition pairs nodes following seven classes a··· edge directed edge means direct indirect cause similar edge type indicates either cause unmeasured confounder both; similar a◦–◦ edge type expresses cause cause unmeasured confounder unmeasured confounder causal relationships holds; bi-directed edge represents presence unmeasured confounder bootstrap rfci method apply three main steps. first performs bootstrap sampling training data times create different bootstrap training datasets. second step runs rfci datasets obtain pags. finally every pair nodes uses frequency counts edge class pair generated pags determine probability distribution seven possible edge classes. mentioned bootstrap estimates guaranteed calibrated. following section describe post-processing method bootstrap probabilities calibrated probabilities. pair nodes resulting output brfci method seven jointly exhaustive mutually exclusive class probabilities correspond seven classes described above. therefore need apply calibration method post-processes multi-class classiﬁcation score simple approach devise multi-class calibration model well-performing non-parametric binary classiﬁer calibration method isotonic regression averaging bayesian binning bayesian binning quantiles post-process corresponding output probabilities class separately. performed one-versus-remainder fashion described major drawback approach binary calibration methods histogram-based non-parametric require considerable amount data produce well-calibrated probabilities. however often expensive feasible obtain truth status large number node pairs real applications causal discovery. consequently availability small calibration training critical constraint design calibration approach. resolve problem make simple extension platt’s method parametric binary classiﬁer calibration approach. platt’s method uses sigmoid transformation output binary classiﬁer calibrated probability. uses logistic loss function learn parameters model. method advantages parameters make viable choice sample size calibration datasets method runs test time thus fast. natural extension platt’s method multi-class calibration task combination softmax transfer function cross-entropy loss function instead sigmoid function logistic loss function respectively. minimizing cross entropy equivalent minimizing empirical kullback-leibler divergence estimated probabilities observed figure structure post-processing calibration method. inputs left bootstrap probabilities seven edge types. outputs right corresponding postprocessed probabilities intended better calibrated. ones. minimum achieved true probability distribution minimizing cross entropy function result ﬁnding closest distribution parameterized model observed distribution data model uses softmax transfer function optimizes cross entropy loss function called softmax regression softmax regression-based calibration model inherits desirable properties platt’s method. however similar platt’s method mapping softmax regression-based calibration method learn restrictive since ﬁnal separating boundaries pair classes always linear. simple relaxation restriction shallow neural network hidden layer. figure shows architecture shallow neural network model post-process bootstrap-generated probabilities. experiments train different shallow neural networks setting number neurons hidden layer randomly. test time average different outputs generated models ﬁnal calibrated probability estimates. averaging helpful since reduces variance error predictions improves ﬁnal performance post-processed probabilities notation fcal denote mapping seven-element vector uncalibrated probabilities input seven-element vector calibrated probabilities output. implemented model using scikit python package uses tensorﬂow machine learning package used cross-entropy loss function adagrad optimization method learn parameters; learning rate batch size respectively. section describes experimental methods used evaluate performance calibrated network discovery method introduced above. evaluation involves following steps create random causal bayesian network real-valued nodes edges. also average number edges node construct ﬁrst ordered nodes. then randomly added edges forward direction obtaining speciﬁed mean graph density. process generates graph power-law-type distribution number parents nodes many average number parents nodes correspond continuous random variables every pair nodes parametrize relation structural equation model zero-mean gaussian noise terms linear coefﬁcient. experiments similar ramsey variances uniformly randomly chosen interval drawn uniformly randomly interval choice parameter values simulations implies that average around half variance variables error term makes structure learning difﬁcult simulate dataset size subject constraints described below. percentage variables unobserved latent variables randomly chosen confounder variables given data-generating either generate bootstrap datasets bootstrap dataset learn using rfci method; designate pags. rfci uses fisher’s test check conditional independence variables dataset. signiﬁcance level independence judgments made node pair calculate probability distribution edge types using maximum likelihood estimates counts perform stratiﬁed random sampling node pairs obtain training samples calibration rest data testing. obtaining samples used stratiﬁed random sampling select samples seven edge classes. particular ﬁrst sorted probability scores edges edge class according bootstrap probabilities. partitioned instances bins based bootstrap probabilities. finally sampled separately equal frequency. learn calibration function fcal using calibration training data. node pair test derive compare performance test pairs manner well calibrated. running evaluation procedure step time consuming part involves running rfci bootstrap datasets. however still feasible run-time efﬁciency rfci method parallel computing. simulations used tetrad open-source freely available software application coded java. steps repeated randomly generated performance results averaged. given node pair take predicted edge type pair highest probability. note although seven different edge classes consider edge types performance evaluation directed edge types partially directed edge types. ﬁrst edge-type-based evaluation measures precision recall compute measures edge type calculated four basic statistics true positives false positives true negatives false negatives types separately. precision derived ratio recall derived ratio also report f-score summary measure shows overall performance predictions terms precision recall. also evaluated edge-type-based predictions terms maximum calibration error calculated edge type partitioning output space estimated edge-type probabilities interval equal-frequency bins randomly chosen instances. estimated probability instance located bins. deﬁne associated calibration error absolute difference mean value predictions actual observed frequency positive instances. calculates maximum calibration error bins. lower value better calibration probability scores. lowest possible value highest possible value also report overall summary measure shows performance predictions terms calibration. compute measure augmented seven-element probability distribution vectors test instances form aggregated vector pall. also augmented corresponding -of-k binary labels form aggregated binary vector zall. overall deﬁned maximum calibration error calculated based pall zall. using stratiﬁed random sampling crucial severe class imbalance data running times experiments varied minutes -core compute node computationally feasible experimental results section presents results experiments evaluating performance generated probabilities edge types calibration. shallow neural network calibration method learn calibration function fcal calibration training data. since purpose paper compare calibration methods report results experiments using calibration methods rather report results calibration using neural network method found performs well relatively small calibration training sample sizes compared calibration methods tried. conﬁgurations report average results using randomly simulated cbns. tables show results cbns nodes tables boldface indicates results statistically signiﬁcantly superior based two-sided wilcoxon signed rank test signiﬁcance level. tables indicate post-processing bootstrap probabilities improve overall edge-type performance terms discrimination calibration. exception edge type lose discrimination. happening original precision recall bootstrap probabilities edge type. consequently often obtain positive instances edge type calibration training negatively affects performance predictions calibration. note no-edge type report precision recall always close figure shows calibration diagram estimated probabilities calibration calibration training instances. emphasize observing calibration instances equivalent observing less node pairs draw calibration diagrams partitioned output space estimated probabilities equal-size bins. draw average frequency positive class versus mean predictions located bin. diagrams straight dashed line connecting represents perfectly calibrated model. closer calibration curve line better calibrated prediction model. figure shows proposed shallow neural network post-processing method often improves calibration performance predictions edge-type important edge type since likely drive experimentation. particular directed edges scientist considers high probability well novel important would prime candidates experimental validation. furthermore edge-type high probability region arguably critical making decisions directed arcs investigate false positive experimental investigations minimized. also associated diagrams no-edge type figure show estimated probabilities pretty well-calibrated calibration. interesting observation considering fact precision recall also always close edge type using post-processing calibration method. results indicate calibrated probabilities indicate high probability edge pair variables nodes rarely directly causally related. result provides conﬁdence prioritizing experimental investigation node pairs direct causal relationships. another interesting observation figure bootstrap probabilities edge-type highly overestimated. results high false positive rate edge-type consequently increases false negative rate edge types. note post-processing bootstrap probabilities generate high probabilities edge-type consequently circle high-probability bins probability bins appropriate edges output rfci seldom correct. overall calibration diagrams figure show post-processing bootstrap probabilities using proposed shallow-neural network model generally improves calibration performance predictions. advantage shallow neural network approach post-processing estimated probabilities readily condition types features learning calibration mapping conditioning local global features learned graph could potentially yield improvements post-processed calibrated probabilities. area future research. table simulation results. represent number variables edges percentage hidden variables data-generating respectively. signiﬁcance level used rfci method calibration training size. boldface indicates results signiﬁcantly better based wilcoxon signed rank test signiﬁcance level. lower better. figure calibration curves edge-type probabilities calibration. closer predictions diagonal calibrated probabilities. results calibration training size percentage hidden variables signiﬁcance level test independence rfci conclusion paper introduced approach improving calibration structure discovery. used bootstrapping method obtain estimated probabilities causal relationships pair random variables. although applied bootstrapping method rfci algorithm applied type network discovery method long method sufﬁciently fast hundreds times dataset obtain bootstrap probability estimates. calibrate bootstrap probabilities devised natural extension platt’s calibration method supports multi-class calibration using shallow neural network. experiments wide range large simulated datasets show using small instances gold standards training calibration model obtain substantial improvements terms precision recall calibration relative bootstrap probabilities. future work plan expand range simulated experiments perform well evaluate method using real biomedical data truth status known literature relatively small subset variables. acknowledgement thank members center causal discovery helpful feedback. research reported publication supported grant awarded national human genome research institute funds provided trans-nih data knowledge initiative. content solely responsibility authors necessarily represent ofﬁcial views national institutes health. material also based upon work supported national science foundation grant iis-. opinions ﬁndings conclusions recommendations expressed material author necessarily reﬂect views national science foundation. references morris degroot stephen fienberg. comparison evaluation forecasters. statistician pedro domingos. useful things know machine learning. communications john duchi elad hazan yoram singer. adaptive subgradient methods online learning stochastic optimization. journal bradley efron robert tibshirani. introduction bootstrap. press friedman moises goldszmidt abraham wyner. data analysis bayesian networks bootstrap approach. proceedings clark glymour gregory cooper. computation causation discovery. press phyllis illari federica russo williamson. causality sciences. oxford mikko koivisto. advances exact bayesian structure discovery bayesian networks. arxiv. mikko koivisto kismat sood. exact bayesian structure discovery bayesian networks. journal machine learning research michael nielsen. neural networks deep learning. determination press judea pearl. causality models reasoning inference. econometric theory john platt. probabilistic outputs support vector machines comparisons regularized likelihood methods. advances large joseph ramsey. scaling greedy equivalence search continuous variables. arxiv. ricardo silva richard scheines clark glymour peter spirtes. learning structure linear latent variable models. journal peter spirtes. introduction causal inference. journal machine learning research peter spirtes clark glymour richard scheines. causation prediction search. press bianca zadrozny charles elkan. transforming classiﬁer scores accurate multiclass probability estimates. proceedings", "year": 2017}