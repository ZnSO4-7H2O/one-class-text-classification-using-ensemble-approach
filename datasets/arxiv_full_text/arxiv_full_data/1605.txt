{"title": "Learning with Memory Embeddings", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "Embedding learning, a.k.a. representation learning, has been shown to be able to model large-scale semantic knowledge graphs. A key concept is a mapping of the knowledge graph to a tensor representation whose entries are predicted by models using latent representations of generalized entities. Latent variable models are well suited to deal with the high dimensionality and sparsity of typical knowledge graphs. In recent publications the embedding models were extended to also consider time evolutions, time patterns and subsymbolic representations. In this paper we map embedding models, which were developed purely as solutions to technical problems for modelling temporal knowledge graphs, to various cognitive memory functions, in particular to semantic and concept memory, episodic memory, sensory memory, short-term memory, and working memory. We discuss learning, query answering, the path from sensory input to semantic decoding, and the relationship between episodic memory and semantic memory. We introduce a number of hypotheses on human memory that can be derived from the developed mathematical models.", "text": "embedding learning a.k.a. representation learning shown able model large-scale semantic knowledge graphs. concept mapping knowledge graph tensor representation whose entries predicted models using latent representations generalized entities. latent variable models well suited deal high dimensionality sparsity typical knowledge graphs. recent publications embedding models extended also consider time evolutions time patterns subsymbolic representations. paper embedding models developed purely solutions technical problems modelling temporal knowledge graphs various cognitive memory functions particular semantic concept memory episodic memory sensory memory short-term memory working memory. discuss learning query answering path sensory input semantic decoding relationship episodic memory semantic memory. introduce number hypotheses human memory derived developed mathematical models. four main hypotheses. ﬁrst semantic memory described triples episodic memory described triples time. second main hypothesis generalized entities unique latent representations shared across memory functions basis prediction decision support functionalities executed working memory third main hypothesis latent representation time summarizes sensory information available time basis episodic memory. finally proposed model suggests semantic memory episodic memory depend other episodic decoding depends semantic memory semantic memory developed long term store episodic memory. hand also certain independence pure storage episodic memory depend semantic memory semantic memory acquired even without functioning episodic memory. relationships semantic episodic memories found human brain. embedding learning a.k.a. representation learning essential ingredient successful natural language models deep architectures basis modelling large-scale semantic knowledge graphs concept mapping knowledge graph tensor representation whose entries predicted models using latent representations generalized entities. latent variable models well suited deal high dimensionality sparsity typical knowledge graphs. recent publications embedding models extended also consider temporal evolutions time patterns subsymbolic representations extended models used successfully predict clinical some authors make distinction latent representations application speciﬁc embeddings identical across applications might represent universal properties entities figure organization human memory paper discuss memory functions blue. sensory memory episodic memory semantic memory discussed sections. autobiographic memory topic subsection working memory short-term memory discussed sections subsection compare figures events like procedures measurements diagnoses. paper attempt embedding models developed purely solutions technical problems various cognitive memory functions. approach follows tradition latent semantic analysis classical representation learning approach hand found number technical applications hand could related cognitive semantic memories cognitive memory functions typically classiﬁed long-term short-term sensory memory long-term memory subcategories declarative memory non-declarative memory figure shows main categories ﬁner subcategories shows role working memory evidence main cognitive categories partially dissociated another brain expressed differential sensitivity brain damage however also evidence indicating different memory functions mutually independent support paper organized follows. next section introduce unique-representation hypothesis basis exchanging information different memory functions. present different tensor representations main memory functions discuss ofﬂine learning models. section introduce different representations indicator mapping function used memory models section show likely triples generated model using simulated-annealing based sampling perspective. section discuss path sensory input semantic representation scene information long-term semantic episodic memory. section explain different memory representations form basis prediction system relate working memory. section represents main results paper form discussion number postulated hypotheses human memory. section contains conclusions. figure ﬁgure shows different tensor memories models. sensory memory tensor dimensions sensory channel within buffer position time time dimension shared episodic event tensor tensor additional dimensions subject predicate object latter three shared semantic tensor right side show indicator mapping functions functions latent representations involved generalized entities. figure graphical view unique-representation hypothesis. model operate bottom down. ﬁrst case index neurons activate representation layer latent representations implemented weight vectors. ﬁgure active neurons inactive representation layer activated pattern top-down operation representation layer also activate index neurons. activation neuron inner product consider formalized neurons might actually implemented ensembles neurons form. following assume matrix stores latent representations generalized entities. context makes clear refer latent representations entities predicates time. begin considering declarative memories. prime example declarative memory semantic memory stores general world knowledge entities. second concept memory stores information concepts world hierarchical organization. contrast general setting machine learning paper entities prime focus concepts secondary interest. finally episodic memory stores information general personal events whereas semantic memory concerns information know episodic memory concerns information remember portion episodic memory concerns individual’s life involving personal experiences called autobiographic memory. semantic memories episodic memories long-term memories. contrast also consider sensory memory shortest-time element memory. ability retain impressions sensory information original stimuli ended finally working memory topic section working memory uses memories tasks like prediction decision support high-level functions. unique-representation hypothesis assumed paper entity concept predicate time step unique latent representation respectively form vector real numbers. assumption representations shared memory functions permits information exchange inference different memories. simplicity assume dimensionalities latent representations identical r˜r. figure shows simple network realization. technical realization semantic memory knowledge graph triple-oriented knowledge representation. popular large-scale dbpedia yago freebase nell google knowledge graph consider slight extension subject-predicate-object triple form adding value form value function e.g. boolean variable real number. thus states jack likes mary note represent entities subject index object index simplify notation also consider generalized entity associated predicate type index encode attributes also triples mostly simplify discussion. consider efﬁcient representation representation also possible generalize known facts facts first introduce threeway semantic adjacency tensor tensor element xspo associated value triple also deﬁne companion tensor dimensions entries θspo. contains natural parameters model connection boolean variables sigpargq expp´argqq logistic function xspo real number gaussian distribution ppxspo|θspoq npθspo unless speciﬁed otherwise assume bernoulli distribution rest paper. mentioned concept embedding learning entity ˜r-dimensional latent vector representation r˜r. particular embedding approaches used modeling assume semanticpaes aeoq. θsemantic here function semanticp¨q predicts value natural parameter. case represents conﬁdence value triple bernoulli likelihood sigpθsemantic true call function indicator mapping function discuss examples next section. latent representation approaches used successfully model large yago dbpedia parts google shown experimentally models using latent factors perform well high-dimensional highly sparse domains. since entity unique representation independent role subject object model permits propagation information across example writer born munich model infer writer also born germany probably writes german language stochastic gradient descent typically used iterative approach ﬁnding optimal latent representations optimal parameters semanticp¨q recent review please consult jackmarriedtoeq might smaller true spouse. approximation sigpθsemantic jackmarriedtoeq also approximation also permits inductive inference might large sigpθsemantic persons likely married jack sigpθsemantic general interpreted conﬁdence value triple eoq. complex queries semantic models involving existential quantiﬁer discussed concept memory would technically correspond classes hierarchical subclass structure. structure learned latent representations hierarchical clustering. hierarchical structure described type subclass relations. latent representations modeling semantic memory functions long history cognitive modeling e.g. latent semantic analysis restricted attribute-based representations. generalizations towards probabilistic models probabilistic latent semantic indexing latent dirichlet allocation latent clustering topic models extensions toward multi-relational domains discrete latent representations. also spreading activation basis teachable language comprehender network model semantic memory associate models symbolic act-r explores holographic embeddings representation learning model associative memories. attractive feature compositional representation dimensionality representation constituents. connectionists memory models described whereas semantic model reﬂects state world clinic patients observations actions describe factual knowledge discrete events which approach represented episodic event tensor. clinical setting events might prescription medication lower cholesterol level decision measure cholesterol level measurement result cholesterol level; thus events e.g. actions decisions measurements. episodic event tensor four-way tensor tensor element zspot associated value quadruple indicator mapping function added representation time event introducing generalized entity latent representation aet. latent representation compresses events happen time examples maxe θepisodic examples clinical setting would fact cholesterol blood test ordered week result blood test. note consider episodic event memory different subjects predicates objects; thus episodic event memory represent extensive event context event model related cognitive concept episodic memory episodic memory represents memory experiences speciﬁc events time serial form reconstruct actual events took place given point lives contrast semantic memory requires recollection prior experience particular instance time slice event tensor describes events typically sparse triple graph. elements triple graph affect changes example event model might record diagnosis becomes fact also common representations subject predicate object lead transfer event model semantic model applications want consider episodic information speciﬁc individual. example patient model interested happened individual time happened patients time autobiographical event tensor simply sub-tensor concerning events individual only. obtain personal time esit latent representation aesit. whereas latent representation events patients time aesit latent representation events patients time autobiographical event tensor would correspond autobiographical memory stores autobiographical events individual semantic abstraction level autobiographical event tensor related baddeley’s episodic buffer contrast tulving’s concept episodic memory temporary store considered part working memory assume sensor input consists q-channels time step buffer constructed samples channels. speciﬁes time location within buffer contrast event buffer sensory buffer operates subsymbolic level. technically might represent measurements like temperature pressure cognitive model might represent input channels senses. sensory buffer might related mini-batches spark streaming data captured buffers hold seconds minutes input streams sensory buffer described three-way tensor tensor element uqγt associated value triple generalized entity q-th sensory channel speciﬁes time location buffer generalized entity representing complete buffer time latent representations sensor channel latent representations latent components corresponds complex time patterns whose amplitudes determined components aet; thus complex sensory events sensory patterns modelled. technical application sensors measure e.g. wind speed temperature humidity location wind turbines sensory memory retains measurements human cognition sensory memory represents ability retain impressions sensory information original stimuli ended transfer sensory memory short-term memory ﬁrst step memory models particular modal theory atkinson shiffrin evidence suggests short-term memory sole gateway long-term memory sensory memory thought located brain regions responsible corresponding sensory processing. sensory memory basis sequence learning detection complex time patterns. different memories tensor representations models summarized figure unique-representation hypothesis assumed paper latent representations generalized entities central retrieval prediction memory need store facts relationships entity. also need explicitly store semantic graph explicitly. time approximation graph reconstructed latent representations. also discussion section memory function generates term cost function terms considered training adapt latent representations parameters various functional mappings. note global optimization step involving available data. general assumed unique-representation entity example assume prediction model semantic model. sometimes makes sense relax assumption assume form coupling. technically number possibilities example prediction model might trained cost function using latent representations knowledge graph initialization; alternatively different weights different cost function terms. investigators propose dimensions latent representations shared contain extensive discussions transfer latent representations. important note considering conditional probability human memory might speculate might step performed sleep. technical solutions best results focussing cost function corresponded problem solve. example prediction tasks optimized latent representations parameters using prediction cost function. using general function approximators consider semantic here indicator mapping function semanticp¨q modelled general function approximator feedforward multiway neural network index neurons representing activated input response generated output shown figure model would easy query plausibility triple valueq queries would difﬁcult handle. alternative model shown bottom figure inputs function approximator predicts latent representation vector hobject components here semantic object semantic object thus response query pjack likes obtained activating index neurons jack likes input considering index neurons outputs large values. note semantic objectp¨q function approximator produces latent representation vector activation output index neurons corresponds likelihood right answer. call modelling approach indicator mapping representation prediction. tensor decompositions also shown excellent performance modelling tensor decompositions indicator mapping function semanticp¨q implemented multilinear model. particular interest parafac model attractive feature tensor decompositions that multilinearity representation aesr aepr prediction models easily constructed parafac model hobject tucker hobject figure indicator mapping function index neurons representing activated input indicator mapping function generated output. indicator mapping prediction using representation prediction inputs latent representation vector hobject calculated activates output index neurons encoding objects. figure figure tucker model. architecture hidden layers interaction latent representations implemented product nodes. bottom drawn model three hidden layers. gp¨q-layer fully connects outputs product layer object representation layer. since tucker model symmetrical respect generalized entities following draw representations gp¨q-layer. figure nonnegative tensor models marginals conditionals easily calculated independent samples model calculated. ﬁgure shows situation tucker model. model apply vectors ones predicate object representation leads marginalization variables. subject representation acts output sample subject. center integrate object subject index input. predicate output sample predicate. finally bottom subject predicate samples inputs produce sample object. naturally subject predicate given need model bottom. many application interested retrieving triples high likelihood conditioned information thus essentially faced optimization problem. answer query form pjack likes need solve zpβq partition function normalizes distribution inverse temperature. note generated probability distribution subject predicate object random variables answer query pjack likes sample jack likes. artiﬁcial inverse temperature determine interested sampling likely response also interested responses smaller probability similarly derive models pps|p ppp|s attractive feature tensor models marginals conditionals easily obtained. here look tucker model. ppo|s equation appropriate normalization. ppp|sq equation replace ¯aobject aeo. ppsq equation replace addition ¯apredicate aep. shown architecture figure operations easily implemented. marginalization means index neurons active indicated vector ones ﬁgure. models generate samples distribution ﬁrst generating sample ppsq sample ppp|sq ﬁnally sample using ppo|s repeating process obtain independent samples note certain equivalence tensor models sum-product networks similar operations marginals conditionals deﬁned figure semantic decoding using -dimensional tucker tensor model. generated mapping sensory buffer mp¨q. sample subject given time predicate object marginalized. here marginalized samples predicate given sampling object given integrating time dimension obtain memory particular semantic memory. marginalization either input vector ones learns mean representation vector figure mapping i.e. sensory input time latent representation function mpvecputqq. sensory input signiﬁcant e.g. novel unexpected time htime attached emotions time index neuron generated stores htime latent representation aet. eventually become part long-term episodic memory. indicated mp¨q might consist several sub-functions extract different latent features. consider situation sensor input becomes available time latent representations functional mappings ﬁxed challenge calculate latent representation htime since sensory input time available information sensory buffer clear information propagation sensory input episodic memory. assume nonlinear form mp¨q function learned vecputq vectorized representations portion sensory tensor associated individual time depending application mp¨q simple linear second last layer deep neural network face recognition application deepface general assume mp¨q realized functions function focusses different aspects sensory inputs example sensory input image function might analyse color another ones shape third texture. think htime corresponds answer query. previously value conditioned subject predicate object random. appendix subsection describe samples ppsq ppp|sq ppo|s note derive equations marginalization conditioning work figure different prediction models. model assume deterministic function sensory input past latent states. time dependencies reﬂected time dependencies latent states thus future values latent states predicted using past latent states. bottom corresponds dependencies typically used recurrent neural networks state space models. past latent states causally inﬂuence future latent states. also note that pure perception learning kind needs involved. sensory input signiﬁcant e.g. novel unexpected attached emotions time index neuron generated stores htime latent representation aet. operation episode event generated. time index neuron latent representation eventually transferred long-term episodic memory section focus working memory orchestrates different memory functions e.g. prediction decision making. working memory represents intelligence memory functions links complex decision making consciousness made. focus restricted important task prediction. example clinical assume deterministic function sensory input equation past time latent representations. might time dependencies sensory input; high dimensionality input easier model dependencies latent representations instead note model used prediction htime soon sensory input available overrides prediction equation model also suitable novelty detection different htime note also include latent representation individual aeindiviual interpreted representation state individual. model interpreted autoregressive model latent representations external inputs parameter size time window might related capacity short-term memory i.e. number items working memory consider decision making. note structure recurrent neural network assumption latent state depends sensory input previous latent state. architecture shown figure bottom. section speculates relevance presented models human memory functions. particular present several concrete hypotheses. figure shows overall model explains sensory input long-term memory semantic decoding. main assumption course semantic memory described triples episodic memory described triples time i.e. quadruples. perspective paper written. arguments representation higher-order relations always reduced triples triple representations large practical signiﬁcance used large-scale kgs. unique-representation hypothesis states generalized entity represented index neuron unique latent representation stored weight patterns connecting index neurons neurons representation layer produces highly probable oq-triples given htime described figure semantic decoding also also performed episodic memory formed. form semantic memory achieved marginalizing time. even possible operate model reverse consider input let’s mary marginalize consider htime output recall mary exciting time index neuron even recall mary looked like sounded like operating mp¨q reverse. reverse direction indicated small green arrows ﬁgure. similarly time index neuron bottom excite htime past scene semantically analysed sensory impression recalled. predictp¨q predicts future htime used prediction events decisions novelty detection before predicted htime semantically decoded lead mental imagery permitting analysis expected events sensory inputs. learning model parameters adapted facilitate semantic decoding. needed representations generalized entities introduced. blue labels refer human memories naturally less speculative. note ﬁgure draw different index neurons entities roles subject object. artefact visualization sampling process. maintain hypothesis entity unique index neuron unique latent representation. shown figure note weight vectors might sparse models nonnegative. basis episodic memory semantic memory. latent representations integrate known generalized entity instrumented prediction decision support working memory. among advantages common representation would explain background information entity seemingly effortlessly integrated sensor scene understanding decision support humans least entities familiar individual. researchers reported remarkable subset medial temporal lobe neurons selectively activated strikingly different pictures given individuals landmarks objects cases even letter strings names example neurons shown selectively respond famous actors like halle berry. thus local encoding index neurons seems biologically plausible. stated before insist index neurons representing single entities exist brain rather level abstraction equivalent index neuron e.g. ensemble neurons. hypothesis supports locality globality encoding since index neurons local representations generalized entities whereas representation layers would highdimensional non-local. figure shows index layers representation layers entities relation types left. note ﬁgure draw different index neurons entities roles subject object. artefact visualization sampling process. maintain hypothesis entity unique index neuron unique latent representation. often neurons similar receptive ﬁelds clustered together sensory cortices form topographic topological maps might also organizational form neurons representing entities. thus entities similar latent representations might topographically close. detailed atlas semantic categories established extensive fmri studies showing involvement lateral temporal cortex ventral temporal cortex lateral parietal cortex medial parietal cortex medial prefrontal cortex superior prefrontal cortex inferior prefrontal cortex although established assumption neurons generated adult cortex topographic maps might change e.g. injury exhibit considerably plasticity. consequently might speculate index neurons novel entities represented cortex need integrated existing topographic organization. would contradiction model since although require representation index neurons irrelevant individual neurons represent entities. index representation neurons entities might allocated hippocampus although function later transferred cortex. discussion focussed generalized entities latent representations similarity entities expressed similarity latent representations. contrast machine learning typically concerned assignments entities concepts. concepts bring certain order example imply certain properties knowing cloe cat. concept learning main focus paper want describe simple realization. consider treat concept simply another entity latent representation e.g. introduce relation type type links entities concepts. inductive inference model learning materialize cloe also mammal living that default typical cat-attributes. proposed model treat locations entity. example would pmary observedin townhall lastfridayq. model individual hertownhall last friday triple would sufﬁcient pmelocation townhall lastfridayq model assume sensory impression decoded time latent representation m-map mp¨q actually might implemented modules responsible htime different aspects sensory input. thus htime representation shared sensory buffer episodic memory might play role phonological loop visuospatial sketchpad. mp¨q challenging component system. training mp¨q reﬁne operation would correspond perceptual learning cognition. brain mp¨q would likely implemented different sensor pathways e.g. visual pathway auditory pathway could contain internal feedback loops. note would assume connection sensory representation time-representation degree bi-directional thus time representation also feeds back sensory impressions. sensory impressions signiﬁcant time index neuron formed sensory information quickly implemented weight pattern htime shown figures time index neurons might ordered sequentially brain maintains notion temporal closeness temporal order. index neurons time i.e. might formed hippocampal region brain. evidence time cells recently found observed hippocampus becomes activated temporal order events processed model accordance concept perceived sensations decoded various sensory areas cortex combined brains hippocampus single experience. according proposed model hippocampus would need assign time neurons lifetime. fact observed adult macaque monkey forms thousand neurons daily possibly encode information neurogenesis established dentate gyrus thought contribute formation episodic memories. hippocampus might place index neurons representations generated general i.e. also places entities. certainly hippocampus involved forming spatial representations. multiple functionally specialized cell types hippocampalentorhinal circuit place grid border cells place cells selectively locations environment. place grid border cells likely interact yield global representation individuals changing position. encoded memories must consolidated. spatial memories memories thought slowly induced neocortex gradual recruitment neocortical memory circuits long-term storage hippocampal memories fast implementation weight patterns hippocampal area discussed term synaptic consolidation occurs within minutes hours considered fast type consolidation. according theory hippcampus would need well connected association areas cortex. indeed hippocampus receives inputs unimodal polymodal association areas cortex pathway involving perirhinal parahippocampal cortices project entorhinal cortex projects hippocampus. structures part mtl. perirhinal parahippocampal cortices also project back association areas cortex simple special case already semantic level. case medical application described describes procedures diagnosis think mp¨q encoder system episodicp¨q decoder complex autoencoder figure also indicates slow transfer long-term episodic memory. hypothesis index neurons latent representation form basis episodic memory biologically referred system consolidation hippocampus-dependent memories become independent hippocampus period weeks years. according standard model memory consolidation memory retained hippocampus week initial learning representing hippocampus-dependent stage. later hippocampus representations information become active explicit recall implicit recall like sleep. stage hippocampus teaching cortex information information recalled strengthens cortico-cortical connection thus making memory hippocampus-independent. therefore week beyond initial training experience memory slowly transferred neo-cortex becomes permanently stored. sense would relay station various perceptual input make memory stores whole event. occurred directs information towards neocortex provide permanent representation memory. technical model consider mechanisms transfer index neurons generated hippocampus representation pattern might become part episodic memory neurons episodic memory trained replay teaching process would performed activation time index neurons activate sketchpad trains weight patterns time index neurons long-term episodic memory. events transferred hippocampus episodic memory index neurons places entities latent representations would consolidated semantic long-term memory. consolidation memory might guided novelty attention emotional signiﬁcance. growing evidence amygdala instrumental storing emotionally signiﬁcant memories. amygdala belongs consists several nuclei considered part memory amygdala orbitofrontal cortex might also provide reward-related information hippocampus shown many studies loss function hippocampus/mtl brain region leads loss consolidation memory episodic long-term memory loss affect semantic memory. model supports hypothesis since semantic memory relies latent representation subject predicate object whereas episodic memory also relies latent representation time i.e. aet. hypothesis states semantic memory episodic memory implemented functions applied latent representations involved generalized entities include entities predicates time. thus neither knowledge graph tensors ever needs stored explicitly similarity tensor decomposition call tensor memory hypothesis. htime generated sensory input basis episodic memory. semantic interpretation sensory input recall episodic memory htime rapidly decoded semantic decoder shown center figure discussed sections model suggests decoding happens generation oq-triples stochastic sampling procedure. since sensory input general described several triples generation process repeated several times generating number oq-triples. sequential sampling triples active time ensemble triples represents query answer. sequential sampling might also inﬂuenced attention mechanisms e.g. decoding complex scenes proposed model related encoder-decoder networks produce text sequences whereas produce likely triples. mp¨q would encoder potentially internal feedback loops htime would representation shared encoder decoder semantic decoder proposed model would correspond decoder. clear indication semantic decoding happening quickly individual describe scene verbally immediately happened. past number neural winner-takes-all networks proposed neuron largest activation wins neurons driven inactivity inherent noise real spiking neurons likely winner-takes-all networks select neurons large activities necessarily largest activity. thus winner-takessampling might close sampling process speciﬁed theoretical model. might speculate winner-tales-all operation performed complex formed dentate gyrus region hippocampus proper known contains many feedback connections essential winner-takes-all computations sometimes modelled continuous attractor neural network excitatory recurrent colateral connections global inhibition sampling denoises scene interpretation. oq-sample represents sharp hypothesis; advantage sampling approach complex feedback mechanisms required generation attractors approaches. proposed sampling procedure step-wise procedure generates independent samples. alternative might gibbs sampler could implemented easily. advantage gibbs sampler require marginalization; disadvantage generated samples independent. hand correlated samples might basis free recall associative thinking chaining. association entity generate latent representation sample entity based latent representation thus explore entities similar original entity. thus barack obama might produce michelle obama. sampling roles subjects might interchanged. thus triple might produce samples describing properties relationships usa. discussed caption figure even possible operate model reverse consider person input marginalize consider htime output recall person exciting time index neuron even recall appearance operating mp¨q reverse. according model recall episodic memory would driven activation time latent representation semantically decoded elucidates sensory impressions. subjective feeling reconstruction past memory. m-mapping prediction semantic decoding fast operations possibly involving many parts cortex. semantic coding decoding proposed model might biologically located mtl. growing evidence hippocampus plays important role encoding also decoding memory involved retrieval information long-term memory binding items cortex theory states perirhinal cortex connects what pathways unimodal sensory brain regions. model information decoded oq-triples. contrast when where parts pass posterior part parahippocampal region. types information pass entorhinal cortex converge within hippocampus enables full recognition episodic event what pathway involved anterior temporal system also involving parts temporal lobe associated semantic memory. where pathway part posterior language considered simple consists triple statements. physicist eugene wigner speculated unreasonable effectiveness mathematics natural sciences words mathematics right code natural sciences. similarly semantics might considered language world humans involved might speculate unreasonable effectiveness well. discussed episodic memory implemented form time index neurons latent representations decoded using latent representations subjects predicates objects. semantic memory? section describe semantic memory implemented separate indicator mapping function also based latent representations subject predicate object. biologically might quite challenging transfer episodic memory semantic memory. alternative number interesting consequences semantic memory generated episodic memory marginalizing time shown bottom figure interpretation semantic memory long-term storage episodic memory. thus answer query what events happened time system needs retrieve perform semantic decoding oq-triples. contrast decode triple semantic memory replaced either calculated inputting vectors ones learning long-term average form semantic memory attractive since requires additional modelling effort structures needed episodic memory argued semantic memory information encountered repeatedly often actual learning episodes blurred gradual transition episodic semantic memory take place episodic memory reduces sensitivity association particular events information generalized semantic memory. without doubt semantic episodic memories support another thus theories speculate episodic memory gateway semantic memory recent overview topic. model would also support alternative view tulving episodic memory depends semantic memory i.e. representations entities predicates note studies also found independent formation semantic memories case episodic memory dysfunctional certain amnesic patients amnesic patients might learn facts without remembering episodes learned information phenomenon supported proposed model since direct path sensory input representations subject predicate object. model supports inductive inference form probabilistic materialization. certainly humans capable form logical inference might faculty working memory. approximations performed tensor models respectively multiway neural networks lead form probabilistic materialization unconscious inference example consider know lives munich. probabilistic materialization happens factorization already predict also lives bavaria germany. thus facts inductively inferred facts entity represented local environment. certain danger probabilistic materialization since might lead overgeneralizations reaching national prejudice false memories. fact many studies shown individuals produce false memories personally absolutely convinced truthfulness model assumes symmetrical connections index neurons representation neurons. biological plausibility symmetric weights discussed intensely computational neuroscience many biologically oriented models property reciprocal connectivity abundant brain perfect symmetry typically observed. interesting feature proposed model learning adaptation necessary operation long sensory information described entities predicates already known. structural adaptation happens online forming index neuron representation pattern aet. decoding successful e.g. decoded triples likelihood might consider mechanism introducing index neurons latent representations entities predicates stored memory. thus available resources insufﬁcient explaining sensory data index neurons entities predicates introduced. slower time scale might necessary ﬁne-tune parameters system possibly also latent representations entities predicates. might look model figure complex neural network inputs targets possibly recurrence prediction module. powerful learning algorithms available train system supervised might solution technical application. course biological system target information unavailable. complex system trained without clear target information? future prediction model trained lead high quality predictions future sensory inputs remaining parameters suggest form bootstrap learning model parameters adapted lead stable semantic interpretation sensory input. call semantic-attractor learning hypothesis sense semantic descriptions form attractors decoded sensory data conversely attractors adapted based sensory data. related phenomenon emergence process whereby larger patterns regularities arise interactions among smaller simpler entities exhibit properties. thus emerging semantics hypothesis semantic description emergent property sensory inputs right figure future-prediction model estimates next htime based past values based latent representation individual aes. note considered constant; example individual might diagnosed disease would reﬂected change aes. large differences predicted sensory-decoded latent representations represent novelty might component attention mechanism. discussed before novelty might important factor determines sensory information stored episodic memory speculated models supported cognitive studies interesting aspect predicted htime semantically decoded cognitive analysis predicted events lead mental imagery sensory representation predicted events. mental imagery viewed conscious explicit manipulation simulations working memory predict future events link episodic memory mental imagery studied section discussed predictive model model. human cognition might signiﬁcant would part model dynamics whereas model would purely serve predictive component. prediction events actions semantic level sometimes considered important functions cognitive working memory working memory limited-capacity store retaining information short term performing mental operations contents store. prediction model contents working memory could either originate sensory input episodic buffer semantic memory cognitive models working memory described computational models described terms predictive brain anticipating brain emphasize importance looking future namely prediction preparation anticipation prospection expectations various cognitive domains prediction central concept recent trends computational neuroscience particular recent bayesian approaches brain modelling approaches probabilistic generative models generate hypothesis observations assuming hidden causes aligned actual observations working memory brain structure involved prediction. predictive control crucial fast ballistic movements cerebellum plays crucial role implicit tasks. cerebellum involved trial-and-error learning based predictive error signals reward prediction task basal ganglia dopamine neurons encode present rewards future rewards basis reinforcement learning working memory assumed located frontal cortex representations figure many ways prediction. general working memory closely tied complex problem solving planning organizing decision support might assume important role consciousness. evidence strong working memory associated general intelligence inﬂuential cognitive model working memory baddeleys multicomponent model cognitive control executed central executive system. supported subsystems responsible maintenance rehearsal phonological loop maintains verbal information visuospatial sketchpad maintains visual spatial information. recently episodic buffer added model. episodic buffer integrates short-term longterm memory holding manipulating limited amount information multiple domains time spatially sequenced episodes emerging consensus functions working memory located prefrontal cortex number brain areas recruited precisely central executive attributed dorsolateral prefrontal cortex phonological loop left ventrolateral prefrontal cortex visuospatial sketchpad right ventrolateral prefrontal cortex function frontal lobe particular orbitofrontal cortex includes ability project future consequences resulting current actions discussed number technical memory functions realized representation learning made connection human memory. assumption knowledge graph need stored explicitly latent representations generalized entities need stored knowledge graph reconstructed inductive inference performed thus contrast knowledge graph entity represented single node graph links embedding learning entity distributed representation form latent vector i.e. form multiple latent components. unique representations lead global propagation information across memory functions learning proposed latent representation time summarizes sensory information present time basis episodic memory semantic memory depends latent representations subject predicate object. theory support semantic memory long-term aggregation episodic memory. full episodic experience depends semantic context representations hand also certain independence pure storage episodic memory depend semantic memory semantic memory acquired even without functioning episodic memory. relationships semantic episodic memories found human brain. latent representations semantic memory episodic memory sensory memory support working memory functions like prediction decision support. addition latent representations models contain parameters mapping functions memory models prediction models. make link parameters implicit skill memory reﬁning mapping sensory input latent representation corresponds perceptual learning cognition. details concrete technical solutions found also present successful applications clinical decision modeling sensor network modeling recommendation engines.", "year": 2015}