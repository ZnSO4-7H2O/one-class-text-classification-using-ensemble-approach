{"title": "Building End-To-End Dialogue Systems Using Generative Hierarchical  Neural Network Models", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "I.5.1; I.2.7"], "abstract": "We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and back-off n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.", "text": "*department computer science operations research universit´e montr´eal montreal canada †school computer science mcgill university montreal canada jpineau cs.mcgill.ca spectrum non-goaldriven systems recently sordoni shang drawn inspiration neural networks natural language modeling machine translation tasks several motivations developing nongoal-driven systems. first deployed directly tasks naturally exhibit directly measurable goal simply entertainment. second trained corpora related task goal-driven dialogue system models used train user simulator train pomdp models discussed earlier would alleviate expensive time-consuming task constructing large-scale task-speciﬁc dialogue corpus. addition this features extracted non-goal-driven systems used expand state space representation pomdp models help generalization dialogues outside annotated task-speciﬁc corpora. contribution direction end-to-end trainable non-goal-driven systems based generative probabilistic models. deﬁne generative dialogue problem modeling utterances interactive structure dialogue. such view model cognitive system carry natural language understanding reasoning decision making natural language generation order replicate emulate behavior agents training corpus. approach differs previous work learning dialogue systems interaction humans learns off-line examples human-human dialogues aims emulate dialogues training corpus instead maximize task-speciﬁc objective function. contrary explanation-based learning rule-based inference systems model require predeﬁned state action space representation. representations instead learned investigate task building open domain conversational dialogue systems based large dialogue corpora using generative models. generative models produce system responses autonomously generated word-by-word opening possibility realistic ﬂexible interactions. support goal extend recently proposed hierarchical recurrent encoder-decoder neural network dialogue domain demonstrate model competitive state-of-the-art neural language models backn-gram models. investigate limitations similar approaches show performance improved bootstrapping learning larger questionanswer pair corpus pretrained word embeddings. dialogue systems also known interactive conversational agents virtual agents sometimes chatterbots used wide applications ranging technical support services language learning tools entertainment dialogue systems divided goal-driven systems technical support services non-goal-driven systems language learning tools computer game characters. current work focuses second case availability large corpora type though model eventually prove useful goal-driven systems also. perhaps successful approach goal-driven systems view dialogue problem partially observable markov decision process unfortunately deployed dialogue systems hand-crafted features state action space representations require either large annotated task-speciﬁc corpus horde human subjects willing interact unﬁnished system. makes expensive time-consuming deploy real dialogue system also limits usage narrow domain. recent work tried push goal-driven systems towards learning examples using constraints pomdp well learning observed features neural network models conditioned previously sampled words. using standard n-grams compute joint probabilities dialogues e.g. computing probability tables token given preceding tokens suffers curse dimensionality intractable realistic vocabulary size. overcome this bengio proposed distributed vector representation words called word embeddings parameterizes smooth function using neural network. means distributed representations recurrent neural network based language model pushed state-of-the-art performance learning long ngram contexts avoiding data sparsity issues. overall rnns performed well variety tasks machine translation information retrieval called recurrent hidden state acts vector representation tokens seen position particular last state viewed order-sensitive compact summary tokens. language modeling tasks context information encoded used predict next token sentence matrix rdh×|v contains input word embeddings i.e. column vector corresponding token vocabulary size model vocabulary common approximate matrix low-rank decomposition i.e. rdh×de rde×|v approach also directly corpus examples together inference mechanisms dialogue utterances dialogue states action generation mechanisms dialogue states dialogue acts stochastically response utterances. believe training model end-to-end minimize single objective function minimum reliance hand-crafted features yield superior performance long run. furthermore focus models trained efﬁciently large datasets able maintain state long conversations. experiment well-established recurrent neural networks n-gram models. particular adopt hierarchical recurrent encoder-decoder demonstrate competitive models literature. extend model architecture better suit dialogue task. show performance substantially improved bootstrapping pretrained word embeddings pretraining model larger question-answer pair corpus. carry experiments introduce movietriples dialogue dataset based movie scripts. modeling conversations micro-blogging websites generative probabilistic models ﬁrst proposed ritter view response generation problem translation problem post needs translated response. generating responses found considerably difﬁcult translating languages likely wide range plausible responses lack phrase alignment post response. later shang proposed recurrent neural network framework generating responses micro-blogging websites. followed sordoni extended framework statusreply pairs triples three consecutive utterances. best knowledge banchs ﬁrst suggest using movie scripts build dialogue systems. conditioned utterances model searches database movie scripts retrieves appropriate response. later followed ameixa demonstrated movie subtitles could used provide responses out-of-domain questions using information retrieval system. consider dialogue sequence utterances involving interlocutors. i.e. contains sequence tokens wmnm} random variable taking values vocabulary representing token position utterance tokens represent words speech acts e.g. pause turn tokens. generative model dialogue parameterizes probability distribution governed parameters possible dialogues arbitrary lengths. probability figure computational graph hred architecture dialogue composed three turns. utterance encoded dense vector mapped dialogue context used decode tokens next utterance. encoder encodes tokens appearing within utterance context encodes temporal structure utterances appearing dialogue allowing information gradients longer time spans. decoder predicts token time using rnn. adapted sordoni advantage embedding matrix separately bootstrapped larger corpora. analogously matrix rdh×|v represents output word embeddings possible next token projected another dense vector compared hidden state probability seeing token position increases corresponding embedding vector near context vector parameter called recurrent parameter links parameters learned maximizing log-likelihood parameters training using stochastic gradient descent. hierarchical recurrent encoder-decoder work extends hierarchical recurrent encoderdecoder architecture proposed sordoni query suggestion. original framework hred predicts next query given queries already submitted user. history past submitted queries considered sequence levels sequence words query sequence queries. hred models hierarchy sequences rnns word level query level. make similar assumption namely dialogue seen sequence utterances which turn sequences tokens. representation hred given figure dialogue encoder maps utterance utterance vector. utterance vector hidden state obtained last token utterance processed. higher-level context keeps track past utterances processing iteratively utterance vector. after processing utterance hidden state context represents summary dialogue including turn used predict next utterance um+. hidden state interpreted continuous-valued state dialogue system. next utterance prediction performed means decoder takes hidden state context produces probability distribution tokens next utterance. decoder similar language model important difference prediction conditioned hidden state context rnn. interpreted response generation module dialogue system. encoder context decoder rnns make hidden unit everyelse hyperbolic tangent activation function. also possible maxout activation function hidden state projected word embeddings decoder encoder decoder parameters used every utterance dialogue. helps model generalize across utterances. details architecture described sordoni modeling dialogues expect hred model superior standard model reasons. first context allows model represent form common ground speakers e.g. represent topics concepts shared speakers using distributed vector representation hypothesize important building effective dialogue system second number computational steps utterances reduced. makes objective function stable w.r.t. model parameters helps propagate training signal ﬁrst-order optimization methods bidirectional hred hred utterance representation given last hidden state encoder rnn. architecture worked well queries insufﬁcient dialogue utterances longer contain syntactic articulations queries. long utterances last state encoder reﬂect important information seen beginning utterance. thus also experiment model utterance encoder bidirectional rnn. bidirectional rnns chains forward utterance tokens another backward i.e. reversing tokens utterance. forward hidden state position summarizes tokens preceding position backwards hidden state summarizes tokens following position obtain ﬁxed-length representation utterance summarize information forward backward hidden states either taking concatenation last state input context applying pooling temporal dimension chain taking concatenation pooled states input context rnn. bidirectional structure effectively introduce additional short term dependencies proven useful similar architectures experiments below refer variant hred-bidirectional. bootstrapping word embeddings subtitles commonsense knowledge dialogue interlocutors share difﬁcult infer dataset sufﬁciently large. therefore models improved learning word embeddings larger corpora. choose initialize word embeddings wordvec trained google news dataset containing billion words. sheer size dataset ensures embeddings contain rich semantic information word. learn good initialization point model parameters instead word embeddings pretrain model large non-dialogue corpus covers similar topics types interactions interlocutors. corpus subtle corpus containing pairs constructed movie subtitles construct artiﬁcial dialogue dataset taking pair two-turn dialogue pretrain model. make generative dialogue modeling framework. dataset available upon request. movie scripts span wide range topics contain long interactions participants relatively spelling mistakes acronyms. observed forchini movie language regarded potential source teaching learning spoken language features. therefore believe bootstrapping goal-driven spoken dialogue system based movie scripts improve performance. used python-based natural language toolkit nltk perform tokenization named-entity recognition. names numbers replaced <person> <number> tokens respectively reduce data sparsity further tokens transformed lowercase letters frequent tokens replaced generic <unk> token. generated triples i.e. dialogues three turns interlocutors emits ﬁrst utterance responds responds last utterance capture interactive dialogue structure special end-of-utterance token appended utterances continued-utterance token breaks lines speaker. avoid co-dependencies triples coming movie ﬁrst split movies training validation test construct triples. statistics reported table evaluate different variants hred model compare several alternatives including basic n-gram models standard trained concatenation utterances triple context-sensitive model recently proposed sordoni evaluation metrics accurate evaluation non-goal-driven dialogue system open problem well-established method automatic evaluation human-based evaluation expensive. nevertheless probabilistic language models word perplexity well-established performance metric suggested generative dialogue models previously best hyperparameters models chosen early stopping patience validation perplexity initialized recurrent parameter matrices orthogonal matrices parameters gaussian random distribution mean zero standard deviation baseline tested hidden state spaces hred experimented encoder decoder hidden state spaces size based preliminary results memory limitations limited size bootstrapping bootstrapping wordvec size bootstrapping subtle. preliminary experiments showed context state space performed similarly ﬁxed bootstrapping bootstrapping wordvec bootstrapping subtle. models used word embedding size bootstrapping subtle size otherwise. help generalization used maxout activation function hidden state projected word embeddings decoder bootstrapping bootstrapping wordvec. used pooling hred models except bootstrapping subtle since appeared perform slightly worse. bootstrapping word embeddings embedding matrix initialized using publicly available dim. wordvec embeddings trained google news corpus special dialogue tokens exist wordvec embeddings initialized gaussian random distribution before. training procedure carried stages. first trained neural model ﬁxed wordvec embeddings. during stage also trained speech placeholder tokens together tokens covered original wordvec embeddings. second stage trained parameters neural model convergence. bootstrapping subtle processed subtle corpus following procedure used movietriples treating last utterances empty. ﬁnal subtle corpus contained pairs total tokens. bootstrapping subtle corpus found models performed slightly better randomly initializing learning word embeddings subtle compared ﬁxing word embeddings given wordvec. reason report results combining bootstrapping subtle corpus wordvec word embeddings. hred models pretrained approximately four epochs subtle dataset performance appear improve further. then ﬁne-tuned pretrained models movietriples dataset holding word embeddings ﬁxed. model parameters dataset triples number tokens entire dataset. lower perplexity indicative better model. perplexity explicitly measures model’s ability account syntactic structure dialogue syntactic structure utterance dialogue distribution words next utterance highly multi-modal e.g. many possible answers makes perplexity particularly appropriate always measure probability regenerating exact reference utterance. also consider word classiﬁcation error deﬁned number words dataset model predicted incorrectly divided total number words dataset. word contributes either zero count means robust unlikely words. however also less ﬁne-grained word perplexity. instead measuring whole distribution measures regions high probability. ultimately care generating syntactically semantically coherent dialogues. example utterances grammatically correct reﬂect distribution topics corpus whole dialogues reﬂect interaction patterns topical evolutions dialogues corpus. despite proposed before clear well word perplexity word classiﬁcation errors correlate goal. nevertheless optimizing probabilistic models using word perplexity shown promising results several machine learning tasks including statistical machine translation speech recognition image caption generation based empirical ﬁndings expect able discriminate models based word perplexity word classiﬁcation error qualitative analysis generated dialogues understand performance models depth. training procedure train neural network models optimized loglikelihood triples using recently proposed adam optimizer implementation relied open-source python library theano able utterances outputs presented table hred-bidirectional bootstrapped subtle corpus. shown here model often produced sensible answers. however fact majority predictions generic don’t know sorry. observed phenomenon model similar observations inferred remarks recent literature best knowledge ﬁrst emphasize discuss details. several possible explanations behavior. firstly data scarcity models learned predict frequent utterances. since dialogues inherently ambiguous multi-modal predicting accurately would require data natural language processing tasks. secondly majority tokens punctuation marks pronouns. since every token weighted equally training gradient signal neural networks dominated punctuafter publishing ﬁrst draft paper investigated problem proposed change objective function test time also maximize mutual information generated utterance previous utterances. ation pronoun tokens. makes hard neural networks learn topic-speciﬁc embeddings even harder predict diverse utterances. suggests exploring neural architectures explicitly separate semantic structure syntactic structure. finally context triple short. case models beneﬁt longer contexts conditioning information sources semantic visual information. important implication observation metrics based outputs primarily favor models output number punctuation marks pronouns test utterances opposed similar semantic content would systematically biased necessarily correlate objective producing appropriate responses. therefore cannot justify metrics results known lack diversity. nevertheless also note problem occur generated stochastic samples fact stochastic samples contained large variety topic-speciﬁc words often appeared maintain topic conversation. demonstrated hierarchical recurrent neural network generative model outperform n-gram based models baseline neural network models task modeling utterances speech acts. support investigation introduced novel dataset called movietriples based movie scripts suitable modeling long open domain dialogues close human spoken language. addition recurrent hierarchical architecture found crucial ingredients improving performance large external monologue corpus initialize word embeddings large related non-dialogue corpus order pretrain recurrent net. points need larger dialogue datasets. empirical performance models measured using perplexity. established measure generative models dialogue setting utterances overwhelmed many common words especially arising colloquial informal exchanges. fruitful investigate measures performance generative dialogue systems. also considered actual responses produced model. outputs tended produce somewhat generic conversationally acceptable responses. stochastic samples model produced diverse dialogues. future work study models full length dialogues opposed triples model speech acts interlocutors entering leaving dialogue executing actions. finally analysis model outputs suggests would beneﬁcial include longer additional context including modalities audio video. authors acknowledge research nserc canada research chairs nuance foundation cifar compute canada funding resources. authors thank ryan lowe laurent charlin nissan constructive feedback. authors thank rafael banchs providing movie-dic dataset luisa coheur providing subtle dataset. authors also thank anonymous aaai reviewers helpful feedback.", "year": 2015}