{"title": "Towards End-to-End Learning for Dialog State Tracking and Management  using Deep Reinforcement Learning", "tag": ["cs.AI", "cs.CL", "cs.LG"], "abstract": "This paper presents an end-to-end framework for task-oriented dialog systems using a variant of Deep Recurrent Q-Networks (DRQN). The model is able to interface with a relational database and jointly learn policies for both language understanding and dialog strategy. Moreover, we propose a hybrid algorithm that combines the strength of reinforcement learning and supervised learning to achieve faster learning speed. We evaluated the proposed model on a 20 Question Game conversational game simulator. Results show that the proposed method outperforms the modular-based baseline and learns a distributed representation of the latent dialog state.", "text": "conventional pipeline limitations. ﬁrst issue credit assignment problem. developers usually feedback users inform system performance quality. determining source error requires tedious error analysis module because errors upstream modules propagate rest pipeline. second limitation process interdependence makes online adaptation challenging. example module retrained data others depend become sub-optimal fact trained output distributions older version module. although ideal solution retrain entire pipeline ensure global optimality requires signiﬁcant human effort. limitations goal study develop end-to-end framework taskoriented replaces important modules dialog policy single module jointly optimized. developing model task-oriented dialog syspaper presents end-to-end framework task-oriented dialog systems using variant deep recurrent qnetworks model able interface relational database jointly learn policies language understanding dialog strategy. moreover propose hybrid algorithm combines strength reinforcement learning supervised learning achieve faster learning speed. evaluated proposed model question game conversational game simulator. results show proposed method outperforms modular-based baseline learns distributed representation latent dialog state. task-oriented dialog systems important branch spoken dialog system research agent achieve predeﬁned targets natural language interaction users. typical structure task-oriented dialog system outlined figure pipeline consists several independently-developed modules natural language understanding maps user utterances semantic representation. information processed dialog state tracker accumulates input turn along dialog history. outputs current dialog state dialog policy selects next system action based dialog state. natural language generation maps selected action surface form tems faces several challenges. foremost challenge task-oriented system must learn strategic dialog policy achieve goal given task beyond ability standard supervised learning second challenge often task-oriented agent needs interface structured external databases symbolic query formats order answers users’ requests databases agent must formulate valid database query. difﬁcult conventional neural network models provide intermediate symbolic representations. paper describes deep reinforcement learning based end-to-end framework dialog state tracking dialog policy addresses above-mentioned issues. evaluated proposed approach conversational game simulator requires language understanding strategic planning. studies yield promising results jointly learning policies state tracking dialog strategies superior modular-based baseline efﬁciently incorporating various types labelled data learning dialog state representations. section paper discusses related work; section reviews basics deep reinforcement learning; section describes proposed framework; section gives experimental results model analysis; section concludes. dialog state tracking process constantly representing state dialog called dialog state tracking industrial systems rule-based heuristics update dialog state selecting high-conﬁdence output numerous advanced statistical methods proposed exploit correlation turns make system robust given uncertainty automatic speech recognition dialog state tracking challenge formalizes problem supervised sequential labelling task state tracker estimates true slot values based sequence outputs. practice output state tracker used different dialog policy distribution training data live data mismatched therefore basic assumptions dstc state tracker’s performance translate better dialog policy performance. showed positive results following assumption showing positive correlation end-to-end dialog performance state tracking performance. reinforcement learning popular approach learning optimal dialog policy task-oriented dialog system dialog policy formulated partially observable markov decision process models uncertainty existing users’ goals outputs nlu. williams showed pomdp-based systems perform signiﬁcantly better rule-based systems especially word error rate high. work explored methods improve amount training data needed pomdp-based dialog manager. gaˇsi´c utilized gaussian process algorithms greatly reduced data needed training. existing applications dialog management assume given dialog state representation. instead approach learns dialog state representation dialogs along dialog policy end-to-end fashion. end-to-end sdss many attempts develop end-to-end chat-oriented dialog systems directly history conversation next system response methods train sequence-to-sequence models large humanhuman conversation corpora. resulting models able basic chatting users. work paper differs focusing building task-oriented system interface structured databases provide real information users. introduced network-based taskedoriented dialog system. approach treated dialog system mapping problem dialog history system response. learned mapping novel variant encoder-decoder model. main differences models advantage learning strategic plan using describing proposed algorithms brieﬂy review deep reinforcement learning models based markov decision process tuple states; actions; deﬁnes transition probability deﬁnes expected immediate reward discounting factor. goal reinforcement learning optimal policy expected cumulative return maximized mdps assume full observability internal states world rarely true real-world applications. partially observable markov decision process takes uncertainty state variable account. pomdp deﬁned tuple observations deﬁnes observation probability variables ones mdps. solving pomdp usually requires computing belief state probability distribution possible states shown belief state sufﬁcient optimal control objective maximizes expected future return. deep q-network deep q-network introduced mnih uses deep neural network parametrize q-value function achieves human-level performance playing many atari games. keeps separate models target network behavior network every samples uses compute target values ydqn updates parameters every updates weights copied furthermore utilizes experience replay store previous experience tuples model update algorithm samples minibatch experiences size memory computes gradient following loss function recently hasselt leveraged overestimation problem standard q-learning introducing double schaul improves convergence speed prioritized experience replay. found modiﬁcations useful included studies. deep recurrent q-network extension deep recurrent qnetwork introduces long shortterm memory layer convolutional layer original model allows drqn solve pomdps. recurrent neural network thus viewed approximation belief state aggregate information sequence observations. hausknecht shows drqn performs signiﬁcantly better agent observes partial states. similar model proposed narasimhan kulkarni learns play multi-user dungeon games game states hidden natural language paragraphs. overview end-to-end learning refers models back-propagate error signals output inputs. prior work end-to-end state tracking learns sequential classiﬁer estimates dialog state based output without need nlu. instead treating state tracking standard supervised learning task propose unify dialog state tracking dialog policy treated actions available reinforcement learning agent. speciﬁcally learn optimal policy either generates verbal response modiﬁes current estimated dialog state based observations. formulation makes possible obtain state tracker even without labelled data required dstc long rewards users databases available. furthermore cases dialog state tracking labels available proposed model incorporate minimum modiﬁcation greatly accelerate learning speed. thus following sections describe models hybrid-rl corresponding labelling scenarios dialog success labels dialog figure network takes observation turn recurrent unit updates hidden state based history current turn embedding. model outputs q-values actions. policy network grey masked action mask pure approach described previous section could suffer slow convergence cardinality slots large. nature reinforcement learning different actions order estimate expected long-term payoff. hand supervised classiﬁer learn much efﬁciently. typical multi-class classiﬁcation loss function assumes single correct label encourages probability correct label suppresses probabilities wrong ones. modeling dialog state tracking q-value function advantages local classiﬁer. instance take situation user wants send email state tracker needs estimate user’s goal among three possible values send edit delete. classiﬁcation task incorrect labels treated equally undesirable. however cost mistakenly recognizing user goal delete much larger edit learned future rewards. order train slot-ﬁlling policy short-term long-term supervision signals decompose reward function parts figure shows overview framework. consider task-oriented dialog task slots cardinality environment consists user database edb. agent send verbal actions user user reply natural language responses rewards order interface database environment agent apply special actions modify query hypothesis hypothesis slot-ﬁlling form represents likely slot values given observed evidence. given hypothesis database perform normal query give results observations rewards rdb. lstm network dialog state tracker capable aggregating information turns generating dialog state representation approximation belief state turn finally dialog state representation lstm network input policy networks implemented multilayer perceptrons ﬁrst policy network approximates q-value function verbal actions rest estimate q-value function slot shown figure rent belief state. practice instead training separate model estimating replace sample reward label. furthermore observation although expensive collect data user easily sample trajectories interaction database since known. therefore accelerate learning generating synthetic experiences tuple )∀ah i.e. experience replay buffer. approach closely related dyna q-learning proposed difference dyna qlearning uses estimated environment dynamics generating experiences method uses known transition function generate synthetic samples. shared state tracking policies efﬁcient weights policy networks similar slots index slot input. reduce number parameters needs learned encourage shared structures. studies section illustrate example. constrained action mask constrain available actions turn force agent alternate verbal response slot-ﬁlling. deﬁne amask function takes state outputs available actions reward shaping based database reward signals users usually sparse database however provide frequent rewards agent. reward shaping technique used speed learning. showed potential-based reward shaping alter optimal solution; impacts learning speed. pseudo reward function deﬁned intuition potential function encourage agent narrow possible range valid entities quickly possible. meanwhile entities consistent current hypothesis implies mistakes previous slot ﬁlling gives potential order test proposed framework chose question game game rules follows beginning game user thinks famous person. agent asks user series yes/no questions. user honestly answers using three answers don’t know. order resemble dialog user answer natural utterance representing three intents. agent make guesses turn wrong guess results negative reward. goal guess correct person within maximum number turns least number wrong guesses. example game conversation follows person male? user think person artist? user artist. guess person bill gates. user correct. formulate game slot-ﬁlling dialog. assume system available questions select turn. answer question becomes slot slot three possible values yes/no/unknown. length limit wrong guess penalty optimal policy allow agent questions regardless context guess every person database one. selected people freebase attributes birthplace degree gender profession birthday. manually designed several yes/no questions attribute available agent. question covers different possible values given attribute thus carries different discriminative power pinpoint person user thinking result agent needs judiciously select question given context game order narrow range valid people. questions. table shows summary. beginning game simulator ﬁrst uniformly sample person database person thinking also chance simulator consider unknown attribute thus answer unknown intent question related game begins agent asks question simulator ﬁrst determines answer replies using natural language. order generate realistic natural language yes/no/unknown intent collected utterances switchboard dialog corpus table presents mapping swda dialog acts yes/no/unknown. post-processed results removed irrelevant utterances unique utterances intent respectively yes/no/unknown. keep frequency counts unique expression. thus time simulator sample response according original distribution swda game terminated four conditions fulﬁlled agent guesses correct answer people database consistent current hypothesis game length reached number guesses reached agent guesses correct answer treated game victory. loss rewards wrong guess leads penalty. training details user environment simulator accepts verbal actions either yes/no question guess replies natural language utterance. therefore contains actions ﬁrst actions questions last action makes guess given results database. database environment reads query hypothesis returns list people satisfy constraints query. size dimension three values yes/no/unknown. since cardinality slots same need slot-ﬁlling policy network q-value outputs yes/no/unknown modify value latest asked question shared policy approach mentioned section thus {yes unknown}. example considering hypothesis latest asked question applying action result hypothesis vectorial form bag-of-bigrams feature vector represent user utterance; one-hot vector represent system action single discrete number represent number people satisfying current hypothesis. hyper-parameters neural network model follows size turn embedding size lstms policy network hidden layer tanh activation. also dropout rate lstms tanh layer outputs. network total parameters. network trained hyper-parameters drqn behavior network updated every steps interval target network update \u0001greedy exploration used training linearly decreased reward shaping constant pmax discounting factor resulting network evaluated every steps model trained steps. evaluation records agent’s performance greedy policy independent episodes. compare performance three models strong modular baseline hybrid-rl. baseline independently trained state tracker dialog policy. state tracker also lstm-based classiﬁer inputs dialog history predicts slot-value latest question. dialog policy drqn assumes perfect slot-ﬁlling training simply controls next verbal action. thus essential difference baseline proposed models state tracker dialog policy trained jointly. also since hybrid-rl effectively changes reward function typical average cumulative reward metric applicable performance comparison. therefore directly compare rate average game length later discussions. table shows proposed models achieve signiﬁcantly higher rate baseline asking questions making guesses. figure illustrates learning process three models. horizontal axis total number interaction agent either user database. baseline model fastest learning speed performance saturated quickly dialog policy trained together state tracker. dialog policy aware uncertainty slotﬁlling slot-ﬁller distinguish between consequences different wrong labels hand although reaches high performance training struggles early stages suffers slow convergence. fact correct slot-ﬁlling prerequisite winning reward signal long delayed horizon approach. finally hybrid-rl approach able converge optimal solution much faster fact efﬁciently exploits information state tracking label. state tracking analysis hypotheses approach learn good state tracker using dialog success reward signals. best trained models using greedy policy collected samples. table reports precision recall slot ﬁlling trajectories. results indiferent strategy compared baseline. model aims high precision predicts unknown input ambiguous safer option predicting yes/no confusing potentially lead contradiction game failure. different baseline distinguish incorrect labels. therefore although baseline achieves better classiﬁcation metrics take account longterm payoff performs sub-optimally terms overall performance. dialog state representation analysis tracking state multiple turns crucial because agent’s optimal action depends history e.g. question already asked number guesses spent. furthermore assumptions output lstm network approximation belief state pomdp. conducted studies test hypotheses. studies hybrid-rl models saved steps simulator greedy policy recorded samples model. ﬁrst study checks whether reconnumber struct important state feature guesses agent made dialog state embedding. divide collected samples training testing. used lstm output input features linear regression model regularization. table shows correlation determination increases model trained data. second study retrieval task. latent state game true intent users’ answers questions asked far. therefore true state vector size slot four values asked unknown. therefore lstm output fact implicitly learning distribution latent state must highly correlated well-trained model. therefore nearest neighbors based cosine distance measuring record latent states compute empirical probability slot true state differs retrieved neighbors paper identiﬁes limitations conventional pipeline describes novel end-toend framework task-oriented dialog system using deep reinforcement learning. assessed model game. proposed models show superior performance natural language understanding dialog strategy. furthermore analysis conﬁrms hypotheses proposed models implicitly capture essential information latent dialog states. limitation proposed approach poor scalability large number samples needed convergence. future studies include developing full-ﬂedged task-orientated dialog systems exploring methods improve sample efﬁciency. also investigating techniques allow easy integration domain knowledge system easily debugged corrected another important direction. kurt bollacker colin evans praveen paritosh sturge jamie taylor. freebase collaboratively created graph database structuring human knowledge. proceedings sigmod international conference management data pages acm. gaˇsi´c jurˇc´ıˇcek simon keizer franc¸ois mairesse blaise thomson steve young. gaussian processes fast policy optimisation proceedings pomdp-based dialogue managers. annual meeting special interest group discourse dialogue pages association computational linguistics. matthew henderson blaise thomson steve young. word-based dialog state tracking proceedings recurrent neural networks. annual meeting special interest group discourse dialogue pages jurafsky elizabeth shriberg debra biasca. switchboard swbd-damsl shallow-discoursefunction annotation coders manual. institute cognitive science technical report pages sungjin lee. extrinsic evaluation dialog state tracking predictive metrics dialog policy opth annual meeting special timization. interest group discourse dialogue page volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature karthik narasimhan tejas kulkarni regina barzilay. language understanding textbased games using deep reinforcement learning. arxiv preprint arxiv.. iulian serban alessandro sordoni yoshua bengio aaron courville joelle pineau. building end-to-end dialogue systems using generative hierarchical neural network models. arxiv preprint arxiv.. satinder singh diane litman michael kearns marilyn walker. optimizing dialogue management reinforcement learning experiments njfun system. journal artiﬁcial intelligence research pages integrated architectures learning planning reacting based approximating dynamic programming. proceedings seventh international conference machine learning pages tijmen tieleman geoffrey hinton. lecture .-rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning tsung-hsien milica gasic nikola mrksic lina rojas-barahona pei-hao stefan ultes david vandyke steve young. network-based end-to-end trainable task-oriented dialogue system. arxiv preprint arxiv..", "year": 2016}