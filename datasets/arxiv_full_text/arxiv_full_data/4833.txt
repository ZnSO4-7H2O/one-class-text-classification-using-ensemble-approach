{"title": "Common-Description Learning: A Framework for Learning Algorithms and  Generating Subproblems from Few Examples", "tag": ["cs.AI", "cs.LG"], "abstract": "Current learning algorithms face many difficulties in learning simple patterns and using them to learn more complex ones. They also require more examples than humans do to learn the same pattern, assuming no prior knowledge. In this paper, a new learning framework is introduced that is called common-description learning (CDL). This framework has been tested on 32 small multi-task datasets, and the results show that it was able to learn complex algorithms from a few number of examples. The final model is perfectly interpretable and its depth depends on the question. What is meant by depth here is that whenever needed, the model learns to break down the problem into simpler subproblems and solves them using previously learned models. Finally, we explain the capabilities of our framework in discovering complex relations in data and how it can help in improving language understanding in machines.", "text": "current learning algorithms face many diﬃculties learning simple patterns using learn complex ones. also require examples humans learn pattern assuming prior knowledge. paper learning framework introduced called common-description learning framework tested small multi-task datasets results show able learn complex algorithms number examples. ﬁnal model perfectly interpretable depth depends question. meant depth whenever needed model learns break problem simpler subproblems solves using previously learned models. finally explain capabilities framework discovering complex relations data help improving language understanding machines. paper introduce framework called common-description learning common-description discrete nonparametric model designed capture basic relations extremely simple equality adjacency. equality relation means model check whether variables equal adjacency means model move sequence step time example reading third word sequence cannot done without passing ﬁrst second word. simple operations accompanied ability push data memory main operations used model solve problems handle memory. test model constructed small multi-task datasets containing training sequences. consider various tasks including limited addition one-digit numbers addition three one-digit numbers using learned previous task counting copying reverse distinguishing short long input sequences categories comparing digits supporting facts inductive reasoning diﬀerent kinds relations more. learned models learning complex ones. also able interact human users thus need communication algorithm make teaching machines easier also need technique tuning hyperparameters easily done non-experts. variety recent works tried learn simple patterns example zaremba used rnn-based controller trained using q-learning interacts environment interfaces selected manually task. however failed controller suitable tasks learned models overﬁt length input tasks. weston provided community great dataset tasks limited answers single word. showed standard memnns outperforms n-gram lstm failed number tasks. task used questions training testing. meanwhile largest dataset presented paper training examples. joulin mikolov used stack learn simple algorithmic patterns generated diﬀerent sequence generators. neural turing machine used modiﬁed version lstm learn tasks copying sorting data sequences overﬁt length training sequences. diﬀerent prior work learn small datasets diﬀerent ﬂavor dynamic deep architecture shaped input sequence requires less supervision learn training sequences memorize handle memory simple ﬁnally successfully learned multi-task datasets vary complexity hyperparameter settings without overﬁtting. standard deep models always require large training data costs money collecting labeling importantly leads simpler models thus small datasets could good metric comparing models used small datasets evaluating proposed model. developing powerful communication tools allow interaction human users machine necessary ﬁgure machine learns misses also complement previous idea. meaning used small datasets expected machine learn something else also solves training data. case learned interpretable provide counter examples reject learned repeatedly right model datasets presented appendix built. implemented language supporting tools written python provided help understanding analyzing interacting ﬁnal model. tool used visualize common-description model show ﬁnal model solves test sequences. visualization tools important understanding model solves problem could help discovering weaknesses correcting them. full source code visualization tools datasets found https//github.com/basemelbarashy/cdl. reader also gifs show learned models solve test sequences several tasks also visualize learned model experiments. sequence words sequence except last word last word. objective ﬁnal model predict last word sequence training data. assumptions model number tasks learned small datasets relatedness. tables examples datsets. important advantages machine learning algorithms require training dataset learn task. moreover number examples used test model evaluation based accuracy prediction steps model takes solve problem. steps easily observed visualization tools show ﬁnal model solves test questions. common-description model central part cdl. ‘common’ refer capability model produce description found applicable number training sequences makes likely generalized test sequences. solves problem sequence steps built small blocks training called nodes; node supposed simple operation point next node. section diﬀerent types nodes explained detail. describes pattern sequentially deﬁning variables comparing them using break question simpler subproblems. variables deﬁned question memory generated subproblem. variables value could small bricks gives freedom shaping construction. therefore nodes designed small possible. meaning cannot node step. table illustrates diﬀerent node types model. deﬁnes variable located directly another variable. deﬁned variable position next node sink node fails checks whether variables equal. outgoing edges true edge points next node comparison true false edge points next node comparison false points ﬁrst node apply input sequence last node deﬁnes variable output pushes word subproblem queue; generate subproblems using push node solves subproblem using ﬁnal model mgicd deﬁnes variable carrying answer discrete model consists nodes edges deﬁnes pattern sequential steps. training dataset start learning generating solve ﬁrst sequence sequences. then combined generate ﬁnal model. solution variable applying sequence however output undeﬁned sink node ﬁrst thing generate candidate solve individual sequence. figure shows learned ﬁrst fourth sequences. assignment node called positive deﬁnes variable another variable input sequence like assignment node called negative assignment variable deﬁned another like assignment node figure could learned dataset table applying ﬁrst training sequence table source node points assignment node deﬁnes ﬁrst word assignment node points conditional node compares ‘a’. equal next node node output ‘m’. otherwise next node sink node output undeﬁned. sink node represented small dash true edge conditional node green false edge connected sink node cds. applying figure training sequences. ﬁrst three sequences answer ﬁrst word right answer. since ﬁrst word fourth sequence also answer wrong. ﬁrst word ﬁfth sequence thus output undeﬁned. describe anything world either features features both. describe color could could blue yellow ...etc. easy number negative descriptions extremely higher positive ones; color value continuous positive description inﬁnite number negative ones. however cars garage description yellow meaningless negative description useful blue negation positive description blue hence refers car. important ideas learning eﬃciently reduces number negative descriptions learn positive common descriptions sequences positive negative descriptions depending value pji. example figure describes negative feature learned. therefore false edges conditional nodes must connected sink nodes except cycles covered section hybrid combination ﬁrst considered positive description rest negative descriptions. hence answer answer condition ncds undeﬁned applied sequence. valid characteristic vector contains least ‘r’. hcd. example pcds valid hcd. possible values characteristic vectors ‘u’. thus replace respectively integrated deﬁned vcds characteristic vectors all-ones vector. simply valid solve sequences dataset. enough capacity guaranteed easily proved always least solves sequences unless sequences dataset diﬀerent number possible icds grows exponentially number sequences dataset challenges discussed detail section always large number integrated need general one. minimum description length principle general method inductive inference views learning data compression. brief regularities data compressed learning models also regularities data. therefore description length criterion useful model selection compression. apply criterion need ﬁrst deﬁne concept length context cdl. candidates memory space identiﬁed number nodes computation time identiﬁed average number steps taken solve sequences training data. experiments listed section shows important former much important. moreover studying relative importance criteria required. therefore number nodes criterion used experiments. sequence words always positive deﬁnes variables checks whether variables values sequence not. also negative deﬁnes variables ensure words input sequence. resultant deﬁned input sequence exactly sequence. enough capacity assuming sequences dataset diﬀerent sequence dataset resultant hcds solve sequences example selected ﬁnal model number nodes smallest number however mgicd experiment nodes need check whether ﬁrst word not. used twice model positive description negative description hybrid nodes counted once. figure shows pipeline learning process starting learning pcds sequence individually followed merging build hcds ﬁnally building mgicd valid pcds hcds. cycle powerful components great ability capturing invariant features require adding nodes. however cycle learning computationally expensive need restrictions learning them. three experiments illustrated show importance cycles learning system achieves accuracy them. ﬁrst dataset table represents beautiful pattern shows capability learn complex pattern four sequences generalize well sequences without overﬁtting length positions equal signs. moreover pattern diﬃcult recognize even humans especially rewrote sequences small alphabets only. ﬁnal model mgicd contains shown figure cycles. second dataset represented table contains three tasks ﬁrst dataset diﬀerent letters makes unreadable humans; tasks checking whether consecutive sequences equal not. table multitask learning data set. ﬁrst four sequences table diﬀerent letters. rest model also learns decide whether sequences equal not. changing order sequences leads learned model. third dataset learn reverse sequences model output word. problem solved re-entering sequence stage. surprisinglyit easy learn task noticed training fail learn concept reverse sequences equal sign repeated words. case learns search last word sequence answer preceding one. therefore modiﬁed dataset shown table tested point repeating test sequence. knowledge classiﬁed types implicit explicit knowledge. implicit knowledge learned number examples generalized others concept animal. type classical learning models trying learn. explicit knowledge needs memorized rather learned facts stories. weston talked need memory support current neural networks question answering tasks proposed memory networks. pretty simple approach memorizing facts important sequences solving tasks. stated before starts solve question predeﬁned variables automatically deﬁned ﬁrst word last word. similarly considers sequence training data important memory memorize predeﬁned variables pointing boundaries deﬁning variables sequence. selecting sequences memorize learnable feature has. demonstrate powerful handling memory simple approach start experiment table want learn decide whether ﬁrst digit greater second not. mention three vcds mgicd. learning answer ﬁrst sequence objective able learn checking second word answer figure learns comparison false notice cannot used alone invalid used negative description short gives ‘right’. predeﬁned variables reference ﬁrst sequence dataset. compares digits searching second digit sequence searches ﬁrst digit reaching forces sink node answer undeﬁned. otherwise answer ‘wrong’. deep neural networks composed multiple layers transforms representation level abstract level deep diﬀerent ﬂavor deep models composed diﬀerent processing layers learn break input sequence simpler subproblems; solve generated subproblem mgicd; iii) return answer deep separated generation subproblem solving means deep generates subproblem solve example suppose vcds mgicd used solve input sequence. shown figure give except thus level generates subproblems solved respectively dynamic architecture mgicd constructed shaped input sequence. important advantage deep learning models input sequences length diﬀerent words produce completely diﬀerent architectures. neuropsychology working memory described cognitive system responsible manipulating information brain important reasoning guidance decision making behavior similar component model queue used save generated subproblem. nodes required deep push node used insert word queue solve node used solve subproblem queue applying mgicd deﬁning variable carries returned answer. ﬁrst push occur solve node clears queue pushing word. complex datasets used explain deep works. ‘complex’ refers diﬃculty would face humans solving numbers equal signs replaced letters. ﬁrst dataset table includes learning addition one-digit numbers three one-digit numbers. ﬁrst task learned using ﬁrst sequence shown figure simply searches second digit question ﬁrst sequence moves right number steps required ﬁrst digit question starting zero second figure valid solves second task generating related subproblems solved ﬁrst starts adding ﬁrst digits result used constructing second subproblem shows generating subproblems powerful diﬀerent standard deep models. second dataset table shows challenging problem requires variabledepth model solved recursively. mgicd experiment contains valid cds. ﬁrst three sequences solved next three regularity sequences thus require three solved. remaining sequences solved deep shown figure start symbol tells real question located similar symbols. table model learn diﬀerent tasks here ﬁrst three share pattern; next three memorized; rest solved deep generates subproblems solvable including deep one. best understand works running visualization tool shows solution steps generated problems. considering second test sequence figure screenshot visualization ends. reader notice generated problems always word aﬀect solution. domain deep figure larger added vcds learned tasks mgicd. deep works extracting problem solving using vcds mgicd including itself. meaning learned models used previously learned ones vice versa. deep general feature suggests deep make great progress transfer learning attempts develop methods transfer knowledge learned tasks improve learning related task. learning right description captures common patterns data three main stages learning pcds learning vcds iii) learning mgicd. training time always spent ﬁrst stage experiments also eﬀect stages. therefore several ideas developed optimize stage. planar directed graph composed nodes structured main path interrupted cycles. main path carries nodes passed move source node shown figure learning pcds done sequence training data separately main objectives ﬁrst reduce number repeated descriptions second avoid producing pcds survive next learning stages. characteristic vectors used stages analysis training data used ﬁrst stage. starts tmpcd initialized three nodes source sink node builds recursively. time algorithm decisions make whether save tmpcd whether continue adding nodes tmpcd. characteristic vector tmpcd plays important role making decisions. figure ﬁgure shows structure nodes main path except black ones. also shows three components cycle ﬁrst component optional conditional push nodes second conditional node third must assignment nodes update cycle variables. ﬁrst conjecture based fact next stages deal characteristic vectors consequently pcds vectors chance surviving last stage used mgicd must least number nodes. second conjecture based idea time node tmpcd domain becomes smaller. consequently number increase least change training sequences satisfy description tmpcd subsets domain. based conjectures training algorithm must stop characteristic vector tmpcd contains rest entries ‘u’s. know number decrease stayed later pcds vector nodes thus useful also changed used learn least characteristic vector. algorithm also checks whether node useful not. consider nodes useful except conditional node comparison true training sequences without characteristic vector less nodes. experiments presented paper learn least characteristic vector made learning faster aﬀect performance. however strong argument support this. example characteristic vector used negative description rejects ﬁrst sequence. assignment node step algorithm tries possible assignments. example ﬁrst call variables possible assignments. deﬁne variable using positive assignment later used negative assignments already variables deﬁned rule applied also negative assignments applied variable deﬁned cycle previous value lost. conditional node step algorithm tries possible conditional nodes. compares variables second constant variable. condition cycle false outgoing edge must connected sink node learn positive descriptions explained subsection. memorized sequences algorithm choose sequence memorize predeﬁned variables pointing boundaries deﬁning variables sequence. experiments restricted sequence ﬁrst sequence dataset means algorithm decides whether ﬁrst sequence needed memorized not. push node step algorithm tries possible push nodes condition current queue identical ﬁrst part training sequences. reason behind reduce number generated subproblems training. however testing generate subproblems exist training data. cycle shown figure cycles consist three components ﬁrst optional conditional push node second must conditional node third must assignment nodes update cycle variables. algorithm tries possible cycles. deep learning learning positive step test tmpcd training sequences deep generates subproblem needs solved mgicd learned yet. solve problem must start learning prior mgicd without push solve nodes. then learn posterior mgicd push solve nodes solve generated subproblems prior mgicd. learned posterior mgicd ﬁnal model solve input sequence generated subproblems. added restriction learning deep training sequence generated subproblems solved vcds prior mgicd solves previous sequences sequence. therefore sequences require deep solve always last part dataset like datasets tables vcds pcds hcds characteristic vectors. pcds vectors already saved previous algorithm need possible hcds valid. experiments maximum number pcds allowed hcds brute force search good algorithm complexity moreover need apply training data characteristic vector. instead vectors positive negative components calculate resultant vector based three rules table rules easily understood deﬁnition section example vector vector resultant vector valid. vcds need mgicd vcds properties characteristic vectors all-ones vector number nodes vcds minimum get. assumptions number vcds thought number tasks mgicd learned data. used positive negative vcds count nodes makes optimization problem diﬃcult. brute force solution would computationally expensive thus optimized using simple approach makes work much faster. start giving score follows ones number ones characteristic vector vcd. score deﬁned based criteria have. then select vcds highest scores vectors must all-ones vector without intersection. then calculate number nodes considered estimate number nodes mgicd. finally search mgicd condition number nodes exceed experiments identical nodes mgicd datasets. eight hyperparameters unlike machine learning algorithms represent maximum capacity models. consequently always adjust high possible considering computational cost learning. four specify maximum number nodes allowed learned type conditional assignment push solve. rest specify maximum number cycles maximum number negative used form maximum number training sequences memory lastly used allow positive assignments. future work could combine hyperparameters hyperparameter indicates maximum eﬀort spent learning data. test proposed model constructed small datasets. shown table successfully learned datasets hyperparameter settings adjusted manually provide highest capacity required learn datasets. also successfully learned datasets require higher-level used diﬀerent settings hyperparameters dataset make learning faster. datasets previously presented subsection expected failed learn datasets table nevertheless learned models captured nice patterns. failure expected require complex cycles learning algorithm provides. example ﬁrst dataset requires cycle four variables incremented time learning algorithm tries cycles variables. capacity overﬁtting. models high capacity overﬁt training sequences memorizing properties leading terrible performance test data. true learning algorithms cdl. results table show successfully learned diﬀerent datasets require diﬀerent model capacities hyperparameter settings diﬀerent number training sequences case overﬁtting. moreover datasets task vary complexity. interpretable learned mgicds perfectly interpretable ways visualized using python tool used generate ﬁgures paper; another powerful tool shows learned models solve training dataset build training sets accurate models. training time. experiments performed personal laptop intel ram. surprisingly datasets learned couple seconds others required minutes. datasets training time roughly proportional dataset complexity. ﬁrst datasets took hours next datasets took minutes last datasets took minutes. disadvantage output word; solved allowing node main path cycles. however makes analysis characteristic vectors diﬃcult. studying domain intersections vcds help better mgicds avoiding intersections lead diﬀerent answers question. future work develop approach representing vcds mgicd netwrok node shared among vcds. number input output connections part network interpreted frequency description previously learned vcds. powerful telling parts description likely occur future speeds learning algorithms produce consistent vcds. also needs support adding modifying current connections nodes. getting closer architecture function neocortex human brain explained learning algorithms still slow optimizing make possible complex structures cycles higher values hyperparameters help scaling model large databases cope real world problems. deep also need better learning algorithms allow generating subproblems exist training data training. introduced framework based ideas including dynamic deep architecture shaped input sequence deep capability breaking problem simpler subproblems positive negative descriptions relation them characteristic vectors used analysis ﬁnally simple approach handle selected memory speciﬁed training. ﬁnal model mgicd interpreted using visualization tool shows step takes solve testing sequences. approach makes easy learns something else also solves training set. case counter examples training dataset reject learned favor learn. experiments demonstrate capable learning patterns small datasets successfully learned multi-task datasets vary complexity generalizes well unseen data datasets learned hyperparameter settings. training data ahmed mona jack kareem dalia ahmed ahmed mona kareem jack ahmed jack mona jack kareem test data ahmed dalia kareem dalia kareem mona table group dataset model learn diﬀerent tasks here ﬁrst three share pattern; next three memorized; rest solved deep generates subproblems solvable including deep one.", "year": 2016}