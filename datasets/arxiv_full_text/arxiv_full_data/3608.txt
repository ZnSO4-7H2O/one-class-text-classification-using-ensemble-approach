{"title": "Spectral Normalization for Generative Adversarial Networks", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.", "text": "takeru miyato toshiki kataoka masanori koyama yuichi yoshida {miyato kataoka}preferred.jp koyama.masanorigmail.com yyoshidanii.ac.jp preferred networks inc. ritsumeikan university national institute informatics challenges study generative adversarial networks instability training. paper propose novel weight normalization technique called spectral normalization stabilize training discriminator. normalization technique computationally light easy incorporate existing implementations. tested efﬁcacy spectral normalization cifar stl- ilsvrc dataset experimentally conﬁrmed spectrally normalized gans capable generating images better equal quality relative previous training stabilization techniques. code chainer generated images pretrained models available https//github.com/pfnet-research/sngan_ projection. generative adversarial networks enjoying considerable success framework generative models recent years applied numerous types tasks datasets nutshell gans framework produce model distribution mimics given target distribution consists generator produces model distribution discriminator distinguishes model distribution target. concept consecutively train model distribution discriminator turn goal reducing difference between model distribution target distribution measured best discriminator possible step training. gans drawing attention machine learning community ability learn highly structured probability distribution also theoretically interesting aspects. example revealed training discriminator amounts training good estimator density ratio model distribution target. perspective opens door methods implicit models used carry variational optimization without direct knowledge density function. persisting challenge training gans performance control discriminator. high dimensional spaces density ratio estimation discriminator often inaccurate unstable training generator networks fail learn multimodal structure target distribution. even worse support model distribution support target distribution disjoint exists discriminator perfectly distinguish model distribution target discriminator produced situation training generator comes complete stop derivative so-produced discriminator respect input turns motivates introduce form restriction choice discriminator. paper propose novel weight normalization method called spectral normalization stabilize training discriminator networks. normalization enjoys following favorable properties. fact normalization method also functioned well even without tuning lipschitz constant hyper parameter. study provide explanations effectiveness spectral normalization gans regularization techniques weight normalization weight clipping gradient penalty also show that absence complimentary regularization techniques spectral normalization improve sheer quality generated images better weight normalization gradient penalty. learning parameters rdl×dl− r×dl element-wise non-linear activation function. omit bias term layer simplicity. ﬁnal output discriminator given taken generator discriminator functions respectively. conventional form given ex∼qdata] ex∼pg qdata data distribution generator distribution learned adversarial min-max optimization. activation function used expression continuous function range known that ﬁxed generator optimal discriminator form given machine learning community pointing recently function space discriminators selected crucially affects performance gans. number works advocate importance lipschitz continuity assuring boundedness statistics. example optimal discriminator gans standard formulation takes form particularly successful works array proposed methods control lipschitz constant discriminator adding regularization terms deﬁned input examples would follow footsteps search discriminator k-lipschitz continuous functions mean flip smallest value norm norm. input based regularizations allow relatively easy formulations based samples also suffer fact that cannot impose regularization space outside supports generator data distributions without introducing somewhat heuristic means. method would introduce paper called spectral normalization method aims skirt issue normalizing weight matrices using technique devised yoshida miyato spectral normalization controls lipschitz constant discriminator function literally constraining spectral norm layer hout. deﬁnition lipschitz norm glip equal suph spectral norm matrix equivalent largest singular value therefore linear layer norm given glip suph suph lipschitz norm activation function allip equal inequality g◦glip glip·glip observe following bound flip here would like emphasize difference spectral normalization spectral norm regularization introduced yoshida miyato unlike method spectral norm regularization penalizes spectral norm adding explicit regularization term objective function. method fundamentally different method make attempt ‘set’ spectral norm designated value. moreover reorganize derivative normalized cost function rewrite objective function method augmenting cost function sample data dependent regularization function. spectral norm regularization hand imposes sample data independent regularization cost function like regularization lasso. mentioned above spectral norm regularize layer discriminator largest singular value naively apply singular value decomposition compute round algorithm algorithm become computationally heavy. instead power iteration method estimate power iteration method estimate spectral norm small additional computational time relative full computational cost vanilla gans. please appendix detail method algorithm summary actual spectral normalization algorithm. matrix whose entry zero everywhere else respectively ﬁrst left right singular vectors hidden layer network transformed ¯wsn derivative calculated mini-batch respect discriminator given derivative weights without normalization. light second term expression seen regularization term penalizing ﬁrst singular components adaptive regularization coefﬁcient positive ¯wsnh pointing similar direction prevents column space concentrating particular direction course training. words spectral normalization prevents transformation layer becoming sensitive direction. also spectral normalization devise parametrization model. namely split layer separate trainable components spectrally normalized spectral norm constant. turns parametrization merit promotes performance gans weight normalization introduced salimans kingma method normalizes norm vector weight matrix. mathematically equivalent requiring weight weight normalization ¯wwn t-th singular value matrix therefore scaler frobenius normalization requires squared singular values normalizations however inadvertently impose much stronger constraint matrix intended. ¯wwn weight normalized matrix dimension norm ¯wwnh ﬁxed unit vector maximized ¯wwnh means ¯wwn rank one. similar thing said frobenius normalization using ¯wwn corresponds using feature discriminate model probability distribution target. order retain much norm input possible hence make discriminator sensitive would hope make norm ¯wwnh large. weight normalization however comes cost reducing rank hence number features used discriminator. thus conﬂict interests weight normalization desire many features possible distinguish generator distribution target distribution. former interest often reigns many cases inadvertently diminishing number features used discriminators. consequently algorithm would produce rather arbitrary model distribution indeed spectrum multiplicities would looking subgradients here. however probability happening zero would continue discussions without giving considerations events. spectral normalization hand suffer conﬂict interest. note lipschitz constant linear operator determined maximum singular value. words spectral norm independent rank. thus unlike weight normalization spectral normalization allows parameter matrix many features possible satisfying local -lipschitz constraint. spectral normalization leaves freedom choosing number singular components feed next layer discriminator. brock introduced orthonormal regularization weight stabilize training gans. work brock augmented adversarial objective function adding following term seems serve purpose spectral normalization orthonormal regularization mathematically quite different spectral normalization orthonormal regularization destroys information spectrum setting singular values one. hand spectral normalization scales spectrum maximum one. gulrajani used gradient penalty method combination wgan. work placed k-lipschitz constant discriminator augmenting objective function regularizer rewards function local -lipschitz constant discrete sets points form generated interpolating sample generative distribution sample data distribution. rather straightforward approach suffer problems mentioned regarding effective dimension feature space approach obvious weakness heavily dependent support current generative distribution. matter course generative distribution support gradually changes course training destabilize effect regularization. fact empirically observed high learning rate destabilize performance wgan-gp. contrary spectral normalization regularizes function operator space effect regularization stable respect choice batch. training spectral normalization easily destabilize aggressive learning rate. moreover wgan-gp requires computational cost spectral normalization single-step power iteration computation requires whole round forward backward propagation. appendix section compare computational cost methods number updates. order evaluate efﬁcacy approach investigate reason behind efﬁcacy conducted extensive experiments unsupervised image generation cifar- stl- compared method normalization techniques. method fares large dataset also applied method ilsvrc dataset well. section structured follows. first discuss objective functions used train architecture describe optimization settings used experiments. explain performance measures images evaluate images produced trained generators. finally summarize results cifar- stl- imagenet. architecture discriminator generator used convolutional neural networks. also evaluation spectral norm convolutional weight rdout×din×h×w treated operator matrix dimension dout trained parameters generator batch normalization refer readers table appendix section details architectures. latent variable standard normal distribution deterministic generator function. experiments. updates used alternate cost proposed goodfellow ez∼p))] used goodfellow warde-farley bengio updates used original cost deﬁned also tested performance algorithm so-called hinge loss given respectively discriminator generator. optimizing objectives equivalent minimizing so-called reverse divergence type loss already proposed used tran algorithm based hinge loss also showed good performance evaluated inception score fid. wasserstein gans gradient penalty used following objective function ex∼qdata]−ez∼p)]−λ ˆx∼p ˆxd−)] regularization term introduced appendix section quantitative assessment generated examples used inception score fr´echet inception distance please appendix details score. section report accuracy spectral normalization training dependence algorithm’s performance hyperparmeters optimizer. also compare performance quality algorithm regularization/normalization techniques discriminator networks including weight clipping wgan-gp batch-normalization layer normalization weight normalization orthonormal regularization order evaluate stand-alone efﬁcacy gradient penalty also applied gradient penalty term standard adversarial loss gans would refer method ‘gan-gp’. weight clipping followed original work arjovsky clipping constant convolutional weight layer. gradient penalty suggested gulrajani orthonormal initialized weight randomly selected orthonormal operator trained gans objective function augmented regularization term used brock comparative studies throughout excluded multiplier parameter weight normalization method well batch normalization layer normalization method. done order prevent methods overtly violating lipschitz condition. experimented different multiplier parameter fact able achieve improvement. optimization used adam optimizer kingma experiments. tested settings ndis number updates discriminator update generator learning rate ﬁrst second order momentum parameters adam. list details settings table appendix section. settings settings used previous representative works. purpose settings evaluate performance algorithms implemented aggressive learning rates. details architectures convolutional networks deployed generator discriminator refer readers table appendix section. number updates generator experiments unless otherwise noted. figures show inception scores method settings a–f. spectral normalization relatively robust aggressive learning rates momentum parameters. wgan-gp fails train good gans high learning rates high momentum parameters cifar- stl-. orthonormal regularization performed poorly setting stl- performed slightly better method optimal setting. results suggests method robust methods respect change setting training. also optimal performance weight normalization inferior wgan-gp spectral normalization stl- consists diverse examples cifar-. best scores spectral normalization better almost methods cifar- stl-. tables show inception scores different methods optimal settings cifar stl- dataset. sn-gans performed better almost contemporaries optimal settings. sn-gans performed even better hinge loss training number iterations sn-gans fell behind orthonormal regularization stl-. detailed comparison orthonormal regularization spectral normalization please section figure show images produced generators trained wgan-gp weight normalization spectral normalization. sn-gans consistently better gans weight normalization terms quality generated images. precise mentioned section images generated spectral normalization clearer diverse images produced weight normalization. also wgan-gp failed train good gans high learning rates high momentums generated images stl- also sn-gans twice time longer iterations seem converge. still elongated training sequence still completes wgan-gp original iteration size optimal setting sn-gans computationally light. real data -standard cnnweight clipping gan-gp wgan-gp batch norm. layer norm. weight norm. orthonormal sn-gans orthonormal sn-gans sn-gans sn-gans -resnet- orthonormal sn-gans dcgan† lr-gans‡ warde-farley al.∗ wgan-gp also compared algorithm multiple benchmark methods summarized results bottom half table also tested performance method resnet based gans used gulrajani please note methods listed thereof different optimization methods architecture model. please table appendix section detail network architectures. implementation algorithm able perform better almost predecessors performance. resnet experiments trained architecture multiple random seeds weight initialization produced models different parameters. generated images times computed average inception score model. values resnet table mean standard deviation score computed models trained different seeds. figure squared singular values weight matrices trained different methods weight clipping weight normalization spectral normalization scaled singular values largest singular values equal calculated singular values normalized weight matrices. singular values analysis weights discriminator figure show squared singular values weight matrices ﬁnal discriminator produced method using parameter yielded best inception score. predicted section singular values ﬁrst ﬁfth layers trained weight clipping weight normalization concentrate components. weight matrices layers tend rank deﬁcit. hand singular values weight matrices layers trained spectral normalization broadly distributed. goal distinguish pair probability distributions low-dimensional nonlinear data manifold embedded high dimensional space rank deﬁciencies lower layers especially fatal. outputs lower layers gone sets rectiﬁed linear transformations means tend space linear parts. marginalizing many features input distribution space result oversimpliﬁed discriminator. actually conﬁrm effect phenomenon generated images especially figure images generated spectral normalization diverse complex generated weight normalization. training time cifar- sn-gans slightly slower weight normalization signiﬁcantly faster wgan-gp. mentioned section wgan-gp slower methods wgan-gp needs calculate gradient gradient norm ∇xd. stl- computational time sn-gans almost vanilla gans relative computational cost power iteration negligible compared cost forward backward propagation cifar- please figure appendix section actual computational time. order highlight difference spectral normalization orthonormal regularization conducted additional experiments. explained section orthonormal regularization different method destroys spectral information puts equal emphasis feature dimensions including ones ’shall’ weeded training process. extent possibly detrimental effect experimented increasing difigure effect performance stl- induced change feature dimension ﬁnal layer. width highlighted region represents standard deviation results multiple seeds weight initialization. orthonormal regularization perform well large feature dimension possibly design forces discriminator dimensions including ones unnecessary. setting optimizers’ hyper-parameters used setting optimal orthonormal regularization mension feature space especially ﬁnal layer training spectral normalization prefers relatively small feature space setting training selected parameters orthonormal regularization performed optimally. ﬁgure shows result experiments. predicted performance orthonormal regularization deteriorates increase dimension feature maps ﬁnal layer. sn-gans hand falter modiﬁcation architecture. thus least perspective method robust respect change network architecture. show method remains effective large high dimensional dataset also applied method training conditional gans ilrsvrc dataset classes consisting approximately images compressed pixels. regarding adversarial loss conditional gans used practically formulation used mirza osindero except replaced standard gans loss hinge loss please appendix details experimental settings. gans without normalization gans layer normalization collapsed beginning training failed produce meaningful images. gans orthonormal normalization brock spectral normalization hand able produce images. inception score orthonormal normalization however plateaued around iterations kept improving even afterward knowledge research ﬁrst kind succeeding produce decent images imagenet dataset single pair discriminator generator measure degree mode-collapse followed footstep odena computed intra ms-ssim odena pairs independently generated gans images class. sn-gans suffering less mode-collapse ac-gans ∼.). ensure superiority method limited within speciﬁc setting also compared performance sn-gans orthonormal regularization conditional gans projection discriminator well standard gans. experiments sn-gans achieved better performance orthonormal regularization settings paper proposes spectral normalization stabilizer training gans. apply spectral normalization gans image generation tasks generated examples diverse conventional weight normalization achieve better comparative inception scores relative previous studies. method imposes global regularization discriminator opposed local regularization introduced wgan-gp possibly used combinations. future work would like investigate methods stand amongst methods theoretical basis experiment algorithm larger complex datasets. would like thank members preferred networks inc. particularly shin-ichi maeda eiichi matsumoto masaki watanabe keisuke yahata insightful comments discussions. also would like thank anonymous reviewers commenters openreview forum insightful discussions. martin heusel hubert ramsauer thomas unterthiner bernhard nessler g¨unter klambauer sepp hochreiter. gans trained time-scale update rule converge nash equilibrium. arxiv preprint arxiv. jonathan stefano ermon. generative adversarial imitation learning. nips sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing diederik kingma jimmy adam method stochastic optimization. iclr jiwei monroe tianlin alan ritter jurafsky. adversarial learning neural dialogue takeru miyato masanori koyama. cgans projection discriminator. iclr shakir mohamed balaji lakshminarayanan. learning implicit generative models. nips workshop olga russakovsky deng jonathan krause sanjeev satheesh sean zhiheng huang andrej karpathy aditya khosla michael bernstein alexander berg fei-fei. imagenet large scale visual recognition challenge. international journal computer vision seiya tokui kenta oono shohei hido justin clayton. chainer next-generation open source framework deep learning. proceedings workshop machine learning systems twentyninth annual conference neural information processing systems antonio torralba fergus william freeman. million tiny images large data nonparametric object scene recognition. ieee transactions pattern analysis machine intelligence describe shortcut section detail. begin vectors randomly initialized weight. multiplicity dominant singular values orthogonal ﬁrst left singular vectors appeal principle power method produce ﬁrst left right singular vectors following update rule updating change update would small hence change largest singular value. implementation took advantage fact reused computed step algorithm initial vector subsequent step. fact ‘recycle’ procedure round power iteration sufﬁcient actual experiment achieve satisfactory performance. algorithm appendix summarizes computation spectrally normalized weight matrix approximation. note procedure computationally cheap even comparison calculation forward backward propagations neural networks. please figure actual computational time without spectral normalization. inception score exp||p]]) approximated trained inception convolutional neural network would refer inception model short. work salimans reported score strongly correlated subjective human judgment image quality. following procedure salimans warde-farley bengio calculated score randomly generated examples trained generator evaluate ability generate natural images. repeated experiment times reported average standard deviation inception scores. mean covariance samples respectively. output ﬁnal layer inception model softmax fr´echet inception distance distributions images distance computed fr´echet inception distance true distribution generated distribution empirically samples. multiple repetition experiments exhibit notable variations score. comparative study experimented recent resnet architecture gulrajani well standard cnn. additional experiments used adam optimization used hyper parameter used gulrajani sn-gans doubled feature generator original modiﬁcation achieved better results. note doubled dimension feature wgan-gp experiment however performance deteriorated. images used experiments resized pixels. details architecture given table generator network conditional gans used conditional batch normalization namely replaced standard batch normalization layer conditional label information optimization used adam hyperparameters used resnet cifar- stl- dataset. trained networks generator updates applied linear decay learning rate iterations rate would end. image rm×m× stride= conv lrelu stride= conv lrelu stride= conv lrelu stride= conv lrelu stride= conv lrelu stride= conv lrelu stride= conv. lrelu table resnet architectures image generation imagenet dataset. generator conditional gans replaced usual batch normalization layer resblock conditional batch normalization layer. model projection discriminator used architecture used miyato koyama please paper details. figure shows spectral norm layer discriminator course training. setting optimizer table throughout training. fact deviate part. exception convolutional layers largest rank deviate beginning training norm layer stabilizes around iterations. figure shows effect ndis performance weight normalization spectral normalization. results shown figure follows setting except value ndis. performance deteriorates larger ndis amounts computing minimax better accuracy. suffer unintended effect. figure learning curves terms inception score sn-gans gans orthonormal regularization imagenet. ﬁgure shows results standard gans ﬁgure shows results conditional gans trained projection discriminator section dedicated comparative study spectral normalization regularization methods discriminators. particular show contemporary regularizations including weight normalization weight clipping implicitly impose constraints weight matrices places unnecessary restriction search space discriminator. speciﬁcally show weight normalization weight clipping unwittingly favor low-rank weight matrices. force trained discriminator largely dependent select features rendering algorithm able match model distribution target distribution dimensional feature space. originally regularization techniques invented goal improving generalization performance supervised training however recent works ﬁeld gans found another raison d’etat regularizer discriminators succeeded improving performance original. methods fact render trained discriminator k-lipschitz prescribed achieve desired effect certain extent. however weight normalization imposes following implicit restriction choice ¯wwn equation holds restriction means ¯wwn rank one. using corresponds using feature discriminate model probability distribution target. similarly frobenius normalization requires +···+σt argument follows. here critical problem regularization methods. order retain much norm input possible hence make discriminator sensitive would hope make norm ¯wwnh large. weight normalization however comes cost reducing rank hence number features used discriminator. thus conﬂict interests weight normalization desire many features possible distinguish generator distribution target distribution. former interest often reigns many cases inadvertently diminishing number features used discriminators. consequently algorithm would produce rather arbitrary model distribution matches target distribution select features. spectral normalization hand suffer conﬂict interest. note lipschitz constant linear operator determined maximum singular value. words spectral norm independent rank. thus unlike weight normalization spectral normalization allows parameter matrix many features possible satisfying local -lipschitz constraint. spectral normalization leaves freedom choosing number singular components feed next layer discriminator. figure visualization difference spectral normalization weight normalization possible sets singular values. possible sets singular values plotted increasing order weight normalization spectral normalization singular values permitted spectral normalization condition scaled ¯wwn spectral norm exactly deﬁnition weight normalization area blue curves bound note range choice weight normalization small. summary weight normalization frobenius normalization favor skewed distributions singular values making column spaces weight matrices dimensional vector spaces. hand spectral normalization compromise number feature dimensions used discriminator. fact experimentally show gans still another regularization technique weight clipping introduced arjovsky training wasserstein gans. weight clipping simply truncates element weight matrices absolute value bounded prescribed constant unfortunately weight clipping suffers problem weight normalization frobenius normalization. weight clipping truncation value value ﬁxed unit vector maximized rank training favor discriminators select features. gulrajani refers problem capacity underuse problem. also reported training wgan weight clipping slower original dcgan direct straightforward controlling spectral norm clip singular values approach however computationally heavy needs implement singular value decomposition order compute singular values. similar less obvious approach parametrize rdo×di follows get-go train discriminators constrained parametrization rdo×p rdi×p rp×p diagonal matrix. however simple task train model remaining absolutely faithful parametrization constraint. spectral normalization hand carry updates relatively computational cost without compromising normalization constraint. recently gulrajani introduced technique enhance stability training wasserstein gans work endeavored place k-lipschitz constraint discriminator augmenting adversarial loss function following regularizer function using augmented objective function gulrajani succeeded training based resnet impressive performance. advantage method comparison spectral normalization impose local -lipschitz constraint directly discriminator function without rather round-about layer-wise normalization. suggest method less likely underuse capacity network structure. time type method penalizes gradients sample points suffers obvious problem able regularize function points outside support current generative distribution. fact generative distribution support gradually changes course training destabilize effect regularization itself. contrary spectral normalization regularizes function itself effect regularization stable respect choice batch. fact observed experiment high learning rate destabilize performance wgan-gp. training spectral normalization falter aggressive learning rate. moreover wgan-gp requires computational cost spectral normalization single-step power iteration computation requires whole round forward backward propagation. figure compare computational cost methods number updates. said that shall rule possibility gradient penalty compliment spectral normalization vice versa. methods regularizes discriminators completely different means experiment section actually conﬁrmed combination wgan-gp reparametrization spectral normalization improves quality generated examples baseline take advantage regularization effect spectral normalization develop another algorithm. consider another parametrization weight matrix discriminator given scalar variable learned. parametrization compromises -lipschitz constraint layer interest gives freedom model keeping model becoming degenerate. reparametrization need control lipschitz condition means gradient penalty indeed think analogous versions reparametrization replacing ¯wsn normalized criterions. extension form new. salimans kingma originally introduced weight normalization order derive reparametrization form ¯wsn replaced vectorized part addendum experimentally compare reparametrizations derived different normalization methods tested reprametrization methods training discriminator wgan-gp. architecture network wgan-gp used used previous section. resnet-based used architecture provided tables summarize result. method signiﬁcantly improves inception score baseline regular slightly improves score resnet based cnn. figure shows learning curves critic losses train validation sets inception scores different reparametrization methods. beneﬁcial effect spectral normalization learning curve discriminator well. verify ﬁgure discriminator spectral normalization overﬁts less training dataset discriminator without reparametrization weight normalization effect overﬁtting observed inception score well ﬁnal score spectral normalization better others. best inception score achieved course training spectral normalization achieved whereas spectral normalization vanilla normalization achieved respectively. table inception scores different reparametrization mehtods cifar without label supervisions. reported inception score frobenius normalization training collapsed early stage. figure learning curves critic loss inception score different reparametrization method cifar- weight normalization spectral normalization parametrization free", "year": 2018}