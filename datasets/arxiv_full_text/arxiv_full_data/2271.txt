{"title": "Extreme Dimension Reduction for Handling Covariate Shift", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "In the covariate shift learning scenario, the training and test covariate distributions differ, so that a predictor's average loss over the training and test distributions also differ. In this work, we explore the potential of extreme dimension reduction, i.e. to very low dimensions, in improving the performance of importance weighting methods for handling covariate shift, which fail in high dimensions due to potentially high train/test covariate divergence and the inability to accurately estimate the requisite density ratios. We first formulate and solve a problem optimizing over linear subspaces a combination of their predictive utility and train/test divergence within. Applying it to simulated and real data, we show extreme dimension reduction helps sometimes but not always, due to a bias introduced by dimension reduction.", "text": "covariate shift learning scenario training test covariate distributions differ predictor’s average loss training test distributions also differ. work explore potential extreme dimension reduction i.e. dimensions improving performance importance weighting methods handling covariate shift fail high dimensions potentially high train/test covariate divergence inability accurately estimate requisite density ratios. ﬁrst formulate solve problem optimizing linear subspaces combination predictive utility train/test divergence within. applying simulated real data show extreme dimension reduction helps sometimes always bias introduced dimension reduction. often population labelled training data differs population needs make predictions for. handling discrepancy often assumes training test domain identical conditional out|x different marginal come distributions popular importance weighting approach solving covariate shift problem fails high dimensions density ratios used reweight training loss unreliably estimated concentrated small number training samples leading high variance estimates predictor’s expected test loss quantity minimized predictors. work propose extreme linear dimension reduction preprocessing step i.e. dimensions improve methods’ performance high dimensional data. propose dimension reduction extreme maximally reduce estimator variance density ratio estimation error. propose dimension reduction linear downstream method assume learn linear models would still learn models linear original covariates thus retaining interpretability. propose dimension reduction done might uncover subspace within train test covariate distributions different. would afford downstream estimator larger effective sample size thus lower variance. clear priori extreme dimension reduction would computationally experimentally favorable. firstly given extreme dimension subspaces consider selecting wish estimate subspaces’ true predictive utility minimum achievable test domain loss predictor acting however estimating utility single candidate subspace requires solving convex optimization problem. must confront potentically impractical computational problem; cheaper less accurate proxy utility unlikely sufﬁce extremely dimensions. secondly point dimension reduction introduces bias downstream estimator extreme dimension reduction might much bias actually help. test utility extreme dimension reduction ﬁrst develop procedure maximizes projection matrices estimates resulting subspace’s aforementioned true predictive utility regularized penalty subspaces downstream estimator would high variance. procedure using techniques bilevel optimization veriﬁed computationally feasible. study real data whether identify subspaces downstream method reliably test loss. some real data extreme dimension reduction facilitate learning accurate predictors test domain. hypothesize success cases procedure returns sufﬁciently predictive subspaces within reduction downstream regularization constant vector estimated ratio function element-wise positive constraint ensuring positive. ulsif simply removes positivity constraint obtaining longer guranteed positive ulsif learned ratio function dimension reduction approach motivated following suppose apply linear projection prior method handling covariate shift. furthermore suppose model class method linear models. ﬁrst choose subspace dimension projection matrix rk×d minimize projection potentially reduce variance downstream loss estimator choose satisfy criteria. firstly dimension subspace projects greatly reduced density ratio estimates lower variance lower dimensional space. secondly choose effective sample size estimator following projection course projection might hurt performance criteria satisﬁed. want projection introduce much bias downstream estimator want low-dimensional. course want high predictive utility; argminb∈rk constructively obtain satisfying combination criteria minimizing projection matrices discourages subspaces downstream estimator would high variance. estimate estimator variance offsets potential estimator bias. better understand phenomena construct simulated data example extreme dimension reduction helps another introduced bias crippling. finally reformulating problem learning subgroup-speciﬁc model covariate shift problem demonstrate beneﬁt extreme dimension reduction learning depression classiﬁer college-aged population subgroup. covariate shift problem given labelled training samples {xtr training distribution unlabelled test samx test distribution ples {xte importantly makes covariate shift assumption given model class covariate shift problem seeks loss function argminf∈f assume convex minimize expected test loss predictors noting past work constructs unbiased estimator test loss forming empirical expectation version latter expectation minimizes predictors adding regularization carry importance weighting requires estimating density ratios variety methods exist least squares importance ﬁtting computationally efﬁcient variant unconstrained lsif stands involves quadratic program admits cross validation scheme. lsif assumes estimated ratio function written {φm} basis functions choose gaussian kernels centered test points parameter aspire identify minimizing expected squared error true ratios sion reduction projection matrix rk×d tr/i) effective sample size downstream loss estimator equation density ratio estimates replaced true density ratios larger effective sample size loss estimator dimension reduction performed tr/i) lemma rk×d s.t. used fact ﬁxed justify equation follows denoting pearson divergence distributions always least veriﬁes intuition projecting onto subspace idealized circumstances increase effective sample size. though effective sample size following projection larger expectation realized effective sample size might still small projections thus need explicitly regularize cases. dimension reduction reduces downstream estimator variance introduces bias estimator. covariate shift assumption estimator equation true density ratios replacing estimates thereof unbiasedly estimate test loss. lemma lemma i.e. linear predictor out-of-sample test loss obtained loss minimization regularized tradeoff constant complete problem still need construct projected density ratio estimates notably depend constraining equal projected density ratio estimates would returned ratio estimation method i.e. lsif ulsif projected covariates formulation project covariates prior application method tackling covariate shift problem described section dimension desired subspace. particular assume given linear model using importance weighting i.e. minimizing estimator equation regularization ulsif used density ratio estimation. ﬁrst term objective predictive utility estimate section interpretation ﬁrstly constraint equation secondly equations estimates returned running ulsif training test covariates pro}{at jected again note thus depend optimization problem running ulsif. ulsif instead lsif ratio estimation computational beneﬁts relatively strong performance. interpretation second term objective regularizer encourages effective high. regarding hyperparameter selection update throughout individual runs gradient descent procedure in-line cross-validation choose controlling lower bound effective sample size using weighted cross-validation technique sufﬁces both argmin variables solutions unconstrained problems. please calculate lsif used given constrained optimization problem. calculating ﬁrst describe explicitly form naive calculate simply form matrix multiply. since minimizes convex function satisﬁes length stationarity condition vector zeros notation suggests depends depends differentiating respect gives desired jacobian matrix rearranging hessian matrix obtain multiple linear system solve however solving multiple means requisite hessian detrimental. ﬁrst bias nonexistent. second bias crippling. generated -dimensional covariates real valued labels. labels depend ﬁrst covariates particular figure shows samples training samples test samples test. thus absolute bias estimator. ﬁrst term bound measures roughly speaking much training test distributions differ subspace orthogonal subspace parameterizes. second measures much variance ﬁxed averaged test distribution i.e. represents well loss predicted given bias upper bound high terms simultaneously high. analysis studies bias variance downstream estimator assumption ﬁxed. course ﬁxed furthermore selection uses estimator obtaining subspace’s estimated predictive utility equation therefore bias variance also affects selection particular total variance procedure contains variability selecting experiments loss variance two-step procedure sometimes higher even conditional chosen would expect downstream method lower variance. also keen interest would uniform bound bias objective optimization problem refer depends projection matrix several intermediate variables. thus solve optimization problem gradient descent projection matrices using pymanopt package. challenge calda intermediate variables culating gradient depend variables analytically solution convex optimization problems parameterized dependent variables. call argmin variables. fortunately apply existing work efﬁciently calculate used autograd calculate gradients involving argmin variables. problem convex utilize multiple random restarts. differentiation argmin variables reverse mode differentiation generic task recursively given objective form problem reverse mode differentiation compute length vector weights concreteness illustrate latter calculation; table shows mean standard deviation out-ofsample test loss replicates methods number generated training test samples changes. elaborate generate samples aside test data model using labelled training unlabelled test samples note performance often worse difﬁculty estimating weights small sample size. furthermore sliced inverse regression methods account covariate shift estimating projection subspace particularly informative thus random projection methods. hand method able useful test domain downstream method applied correct -dimensional subspace. note variance loss method slightly increased variance selection subspace within apply dimension reduction method projects covariates retain ﬁnds able accurately estimate despite fact estimation uses estimators predictor test loss potentially biased lemma reason designed data distributions estimators true density ratios replacing estimates actually unbiased. show case since function calculating expectation respect distribution marginalize variables speciﬁcally calculating invoke lemma obtain note model class contains linear models trained tested model uses comparable performance compared model uses even though appears informative model class contain v-shaped predictor mean follows. contrarily trained tested model uses superior performance uses covariate shift test covariates support arms -dimensional projection contain equal contributions unchanged examples remains fact ideal projection apply learning model would retain projection would suboptimal. reﬂected table performance method degraded approach baseline methods ﬁrst example. verify reason degradation examining component -dimensional projection vector corresponding examples. i.e. fairly large. average value components example respectively. corresponding values example respectively reﬂecting example method ﬁnds equally predictive. took several datasets each introduced covariate shift creating sampling scheme repeatedly generate training test data samples. dataset ﬁrst identify single predictive vector covariate space generating random vectors measuring predictive utility vector projecting covariates onto running kernel density regression examining in-sample squared error. ﬁrst sample dataset form training data. generate test data subsample remaining dataset according projections along vector. denote projected covariate value dataset standard deviation projected values. select data probability proportional density distribution based projected value. values close small values lead small effective sample size; choose achieved. given vector multiple training test datasets sampled. slightly modify data distribution previous example obtain example previously unbiased estimators suffer crippling bias. particular make single change letting figure several datasets mean loss downstream effective sample size loss standard deviation method shown black subspace dimension varies. respective values unweighted importance weighting baseline shown blue respectively. dataset dimension parentheses. model remaining data. experimental results figure dataset subplots plot black respective quantities method dimension projection changes loss average loss replicates n_eff average effective sample size enjoyed downstream estimator loss standard deviation loss replicates. comparison also indicate horizontal lines values quantities unweighted baseline blue naive importance weighting baseline red. normalize results average loss average effective sample size thus loss n_eff subplots always horizontal blue line height parentheses dataset dimension whether classiﬁcation regression absolute prediction error loss regression problems before loss classiﬁcation problems. extreme dimension reduction always help; whether depends dataset dimension subspace. however many datasets subspace dimension dimension reduction procedure help. given dataset loss increase decrease reduction subspace dimension depending whether reduction variance estimator used selecting downstream enough offset potential increase estimator bias. dimension reduction offers covariate shift problem navigate tradeoff. also note alluded section simulation study variance two-step procedure higher also caution quantities report method directly comparable different subspace dimensions value hyperparameter chosen cross-validation also differs dimensions. means cannot make claims like loss standard deviation lowered reducing subspace dimension solely decreased density ratio estimation variance. despite caveat still identify phenomena paradoxical ﬁrst going dimensions utilized effective sample size n_eff tends increase. coupled accompanying decrease density ratio estimation variance would expect loss standard deviation decrease change. however observe exactly opposite. phenomena likely increased number local optima non-convex optimization problem subspace dimension extremely i.e. anecdotally observed phenomena simulation study loss standard deviation sensitive many random restarts used optimization. troubles likely single physically active average lower weight poverty levels. differences open possibility best model subgroup different general population. furthermore general population falls subgroup approaches beyond simply ﬁtting model data subgroup needed handle data scarcity. regarding features easily obtained aforementioned well income pulse years education excluding require medical tests urine sugar level closely correlated depression i.e. often individual considers general mental health bad. utility classiﬁer would identify risk depression using easily obtainable features proxies depression. following guideline using features removing features available retain data individuals labelled depressed compare performance method naive importance weighting unweighted methods learning classiﬁer depression subgroup using covariate shift reformulation well baseline ﬁtting classiﬁer using data subgroup perform comparison number labelled data varies various uniformly subsample subset size train model using given method obtain predictions remaining unsampled data. times method report average -auc figure well average effective sample size ﬁtting models figure method dataset sizes able consider subspaces effective sample size loss estimator close entire labelled data without hurting performance labelled data scarce resulting variance loss estimator offsets bias results estimating density ratios within subspace compared whose effective sample size always smaller. data plentiful i.e. relative advantage method utilizing data diminished compared baselines utilize less biased estimators. similar relationship holds biased though dataset size relative performances change order occurs finally whose estimator unbiased enjoys lower effective sample sizes worse performance covariate shift approaches utilize labelled data outside subgroup. recent work shown real world models lower prediction performance minority subgroups majority group. theory problem could alleviated learning subgroup model using data subgroup. however data scarce performance subgroup model could improved utilizing appropriate data majority group. correcting covariate shift model learned minimize average prediction error respect population given labelled samples drawn another population. motivates recast subgroup model learning problem covariate shift correction problem. describe formulation method learn classiﬁcation model depression subgroup. formally given pxyz feature vectors indicator denoting membership subgroup interest given loss function model class subgroup model learning problem seeks argminf∈f epxy reformulate covariate shift problem covariate shift assumption |xz= apply covariate shift correction method national health nutrition examination survey dataset downloaded eponymous package dataset contains demographic physical health lifestyle variables individuals united states obtained surveys importantly individuals sampled representative united states general population whole subgroups underrepresented population also dataset. given recent rise mental health issues college students method learn classiﬁer predict presence depression subgroup persons college i.e. years age. persons according nhanes dataset average different general population. example tend less likely smoke sleep work studies beneﬁt dimension reduction prior applying importance weighting approaches solving covariate shift problem particular dimension reduction potentially reduce otherwise high variance importance weighting approaches pointed past work addressing issue used various forms regularization discourage exceedingly large weights cost increased bias. another line work eschews weight estimation builds predictors robust potential shifts covariate conditional outcome distribution. separate work reduces variance using predictor minimizing training loss prior minimizing reweighted loss. work apply dimension reduction estimating density ratios searching subspace training test densities maximally different estimating ratios within however chosen subspace construction would result small underlying effective sample size relative subspaces. approach similar supervised representation learning methods representations predictive similar sense domains; penalizing subspaces small effective sample sizes equivalent penalizing subspaces pearson divergence high. however none approaches perform importance weighting evaluating predictive utility representation advantageous many situations downstream model misspeciﬁed importance weighting actually necessary begin with. example ﬁnds linear subspace minimizing additive combination maximum mean discrepancy lowest possible unweighted squared loss linear model acting recent work used neural networks representations unweighted training domain loss achieved similar domains accomplished using closed form discrepancy measures adversarial training however learned model linear interpretable. address high variance importance weighting approaches handling covariate shift explored beneﬁt extreme dimension reduction i.e. dimensions prior applying approaches. presented solved bilevel optimization problem searches subspace predictive downstream procedure would enjoy large effective sample size. illustrated lemmas simulated real data dimension reduction helps sometimes always bias introduced loss estimator fact incur additional variance selection subspace. study alleviate issues well avoid local optima optimization problem potential future work. also formulated problem learning subgroup-speciﬁc model covariate shift problem demonstrate advantage dimension reduction learning classiﬁer depression college-aged subgroup. finally believe work implications causal inference formulated covariate shift problem propensity score essentially density ratio used widely unreliably estimated higher dimensions", "year": 2017}