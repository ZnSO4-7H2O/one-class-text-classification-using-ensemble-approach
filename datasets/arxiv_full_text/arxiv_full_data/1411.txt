{"title": "Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on  Graphs", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "A number of problems can be formulated as prediction on graph-structured data. In this work, we generalize the convolution operator from regular grids to arbitrary graphs while avoiding the spectral domain, which allows us to handle graphs of varying size and connectivity. To move beyond a simple diffusion, filter weights are conditioned on the specific edge labels in the neighborhood of a vertex. Together with the proper choice of graph coarsening, we explore constructing deep neural networks for graph classification. In particular, we demonstrate the generality of our formulation in point cloud classification, where we set the new state of the art, and on a graph classification dataset, where we outperform other deep learning approaches. The source code is available at https://github.com/mys007/ecc", "text": "bels results overly homogeneous view local graph neighborhoods effect similar enforcing rotational invariance ﬁlters regular convolutions images. hence work propose convolution operation make information channel show leads improved graph classiﬁcation performance. novel formulation also opens broader range applications; concentrate point clouds speciﬁcally. point clouds mostly ignored deep learning voxelization trend best knowledge offer competitive alternative different advantages disadvantages construct graphs euclidean space point clouds work demonstrate state performance sydney dataset lidar scans contributions follows formulate convolution-like operation graph signals performed spatial domain ﬁlter weights conditioned edge labels dynamically generated speciﬁc input sample. networks work graphs arbitrary varying structure throughout dataset. ﬁrst apply graph convolutions point cloud classiﬁcation. method outperforms volumetric approaches attains state performance sydney dataset beneﬁt preserving sparsity presumably details. ﬁrst formulation convolutional network analogy irregular domains modeled graphs introduced bruna looked spatial spectral domain representation performing localized ﬁltering. number problems formulated prediction graph-structured data. work generalize convolution operator regular grids arbitrary graphs avoiding spectral domain allows handle graphs varying size connectivity. move beyond simple diffusion ﬁlter weights conditioned speciﬁc edge labels neighborhood vertex. together proper choice graph coarsening explore constructing deep neural networks graph classiﬁcation. particular demonstrate generality formulation point cloud classiﬁcation state graph classiﬁcation dataset outperform deep learning approaches. source code available https//github.com/mys/ecc. convolutional neural networks gained massive popularity tasks underlying data representation grid structure speech processing natural language understanding image classiﬁcation segmentation video parsing hand many tasks data naturally irregular generally non-euclidean domains structured graphs many cases. include problems modeling computational chemistry biology geospatial analysis social networks natural language semantics knowledge bases name few. assuming locality stationarity composionality principles representation hold least level data meaningful consider hierarchical cnn-like architecture processing however generalization cnns grids general graphs straightforward recently become topic increased interest. identify current formulations graph convolution exploit edge lafigure illustration edge-conditioned convolution directed subgraph. feature vertex l-th network layer computed weighted features predecessor vertices assuming self-loops. particular weight matrices dynamically generated ﬁlter-generating network based corresponding edge labels visualized colors. spectral methods. mathematically sound deﬁnition convolution operator makes spectral analysis theory corresponds multiplication signal vertices transformed spectral domain graph fourier transform. spatial locality ﬁlters given smoothness spectral ﬁlters case modeled b-splines. transform involves expensive multiplications eigenvector matrix. however parameterization ﬁlters chebyshev polynomials eigenvalues approximate evaluation computationally efﬁcient localized ﬁltering recently achieved defferrard nevertheless ﬁlters still learned context spectrum graph laplacian therefore graphs dataset. means graph structure ﬁxed signal deﬁned vertices differ. precludes applications problems graph structure varies dataset meshes point clouds diverse biochemical datasets. cover important cases formulate ﬁltering approach spatial domain limited complexity evaluation localization property provided construction. main challenge dealing weight sharing among local neighborhoods number vertices adjacent particular vertex varies ordering often well deﬁnable. problem. duvenaud signal neighboring vertices followed weight matrix multiplication effectively sharing weights among edges. atwood towsley share weights based number hops vertices. kipf welling further approximate spectral method weaken dependency laplacian ultimately arrive center-surround weighting neighborhoods. none methods captures ﬁner structure neighborhood thus generalize standard convolution grids. contrast method make possible edge labels shown generalize regular convolution approach niepert introduces heuristic linearizing selected graph neighborhoods conventional used. share goal capturing structure neighborhoods approach different way. finally graph neural networks propagate features across graph convergence exploit edge labels sources information however system quite different current multilayer feed-forward architectures making reuse today’s common building blocks straightforward. cnns point clouds meshes. little work deep learning point clouds meshes. masci deﬁne convolution patch descriptors around every vertex mesh using geodesic distances formulated deep learning architecture. processing point clouds using deep learning ﬁrst propose method performing convolutions local graph neighborhoods exploiting edge labels show generalize regular convolutions afterwards present deep networks convolution operator case point clouds general graphs complexity. computing vertices requires most evaluations matrixvector multiplications directed resp. undirected graphs. operations carried efﬁciently batch-mode. formulation convolution graph neighborhoods retains properties standard convolution regular grids useful context cnns weight sharing locality. consider directed undirected graph ﬁnite vertices edges lmax} layer index feed-forward neural network. assume graph vertexedgelabeled i.e. exists function assigning labels vertex assigning labels edge. functions regarded matrices rn×dl rm×s input signal. neighborhood e}∪{i} vertex deﬁned contain adjacent vertices including approach computes ﬁltered signal vertex weighted signals rdl− neighborhood commutative aggregation solves problem undeﬁned vertex ordering varying neighborhood sizes also smooths structural information. retain propose condition ﬁltering weight respective edge label. borrow idea dynamic ﬁlter networks deﬁne ﬁlter-generating network rdl×dl− given edge label outputs edge-speciﬁc weight matrix learnable bias parameterized learnable network weights clarity model parameters updated training dynamically generated parameters edge label particular input graph. ﬁlter-generating network importantly standard discrete convolution grids special case demonstrate clarity. consider ordered vertices forming path graph obtain convolution centered kernel size form vertex connected spatially nearest neighbors including self directed edge labeled one-hot encoding neighbor’s discrete offset figure taking singlelayer perceptron without bias denotes respective reshaped column parameter matrix r×s. slight abuse notation arrive equivalence ignoring normalization factor principle applicable vertex classiﬁcation graph classiﬁcation tasks paper restrict latter i.e. predicting class whole input graph. hence follow common architectural pattern feed-forward networks inproblems self-edges always present. since architecture designed process graphs variable deal varying vertex count lowest graph resolution global average/max pooling. point clouds important data modality arising many acquisition techniques laser scanning multi-view reconstruction. natural irregularity sparsity processing point clouds using deep learning ﬁrst voxelize feeding classiﬁcation segmentation purposes. dense representation hardware friendly simple handle current deep learning frameworks. hand several disadvantages too. first voxel representation tends much expensive terms memory usually sparse point clouds second necessity ﬁxed size grid brings discretization artifacts loss metric scale possibly details. work would like offer competitive alternative mainstream performing deep learning point clouds directly. know ﬁrst demonstrate result. graph construction. given point cloud point features build directed graph labels follows. first create vertex every point assign respective signal connect vertex vertices spatial neighborhood directed edge experiments neighborhoods ﬁxed metric radius worked better ﬁxed number neighbors. offset points corresponding vertices represented cartesian spherical coordinates edge label vector graph coarsening. single input point cloud pyramid downsampled point clouds obtained voxelgrid algorithm overlays grid resolution point cloud replaces points within voxel centroid resulting point clouds independently converted graph labeling neighborhood radius described above. pooling deﬁned point assigned spatially nearest point subsampled point cloud figure construction directed graph one-hot edge labeling proposed edge-conditioned convolution equivalent regular convolution centered ﬁlter size terlaced convolutions poolings topped global pooling fully-connected layers figure illustration. information local neighborhoods gets combined successive layers gain context edge labels ﬁxed particular graph interpretation means ﬁlter generating networks change layer layer therefore restriction -hop neighborhoods constraint akin using small ﬁlters normal cnns exchange deeper networks known beneﬁcial batch normalization convolution necessary learning converge. interestingly success feature normalization techniques data-dependent initialization layer normalization pooling. convolutional layers point-wise layers change underlying graph evolve signal vertices pooling layers deﬁned output aggregated signal vertices coarsened graph. therefore pyramid hmax progressively coarser graphs constructed input graph. extend notation additional superscript hmax} distinguish among different graphs pyramid necessary. also associated labels signal coarsening typically consists three steps subsampling merging vertices creating edge structure labeling mapping vertices original graph coarsened different algorithm depending whether work general graphs graphs euclidean space therefore postpone discussing details applications. finally pooling layer index aggregates lower dimensional based figure example using introduced notation. figure illustration deep network three edge-conditioned convolutions pooling last convolution executed structurally different graph related input graph coarsening signal aggregation pooling step according mapping section details. data augmentation. order reduce overﬁtting small datasets perform online data augmentation. particular randomly rotate point clouds upaxis jitter scale perform mirroring delete random points. many problems modeled directly graphs. cases graph dataset already given appropriate graph coarsening scheme needs chosen. means trivial exists large body literature problem without concept spatial localization vertices resort established graph coarsening algorithms utilize multiresolution framework shuman works repeated downsampling graph reduction input graph. downsampling step based splitting graph components sign largest eigenvector laplacian. followed kron reduction also deﬁnes edge labeling enhanced spectral sparsiﬁcation edges note algorithm regards graphs unweighted purpose coarsening. method attractive reasons. downsampling step removes approximately half vertices guaranteeing certain level pooling strength sparsiﬁcation step randomized. latter property exploited useful data augmentation technique since several different graph pyramids generated single input graph. spirit similar effect fractional max-pooling perform point cloud dataset consists objects categories manually extracted lidar scans figure demonstrates non-ideal sensing conditions occlusions large variability viewpoint makes object classiﬁcation challenging task. following protocol employed dataset authors report mean score weighted class frequency dataset imbalanced. score aggregated four standard training/testing splits. network conﬁguration. ecc-network parametric layers levels graph resolution. conﬁguration described c-c-mpc-c-mp-c-mp-gapfc-d-fc denotes output channels followed afﬁne batch normalization relu activation stands max-pooling grid resolution meters neighborhood radius meters global average pooling fullyconnected layer output channels dropout probability ﬁlter-generating networks conﬁguration fc-fc-fc orthogonal weight initialization relus between. input graphs created meters break overly dense point clusters. networks trained cross-entropy loss epochs batch size learning rate step-wise decreasing epochs. vertex signal scalar laser return intensity representing depth. results. table compares result methods based volumetric cnns evaluated voxelized occupancy grids size orion outperform small margin state result dataset. table also study dependence convolution radii increasing convolutional layers leads drop performance would correspond preference using smaller ﬁlters regular cnns. average neighborhood size roughly vertices best-performing network. hypothesize larger radii smooth information central vertex. investigate this increased importance self-loop adding identity skip-connection retrained networks. achieved mean respectively. stronger identity connection allowed successful integration larger context limit indeed suggests information aggregated neither much little. network conﬁguration. ecc-network modelnet parametric layers levels graph resolution conﬁguration c-c-mpc-c-mp-c-gmp-fcd-fc global pooling. deﬁnitions ﬁlter-generating networks section input graphs created units mimicking typical grid resolution voxel-based methods. network trained cross-entropy loss epochs batch size learning rate step-wise decreasing every epochs. vertex signal i.e. zero. modelnet network wider trained epochs learning rate decreasing epochs. results. table compares result several recent works based either volumetric rendered image representation test sets expanded include orientations also evaluate voting orientations slightly improves results likely rotational variance voxelgrid algorithm. fully reaching state believe method remains competitive fairer comparison leading volumetric method retrained voxelized synthetic point clouds. evaluate graph classiﬁcation benchmark frequently used community consisting datasets mutag enzymes d&d. properties found table indicating variability dataset sizes graph sizes availability labels. following perform -fold cross-validation consist graph representations chemical compounds screened activity nonsmall cell lung cancer ovarian cancer cell lines respectively. mutag dataset nitro compounds labeled according whether mutagenic effect bacterium. enzymes contains representations tertiary structure classes enzymes. database protein structures classiﬁed enzymes non-enzymes. network conﬁguration. ecc-network parametric layers levels graph resolution. conﬁguration described c-c-c-mpc-c-mp-c-gap-fc-d-fc denotes output channels followed afﬁne batch normalization relu activation dropout stands max-pooling onto coarser graph global average pooling fullyconnected layer output channels dropout probability ﬁlter-generating networks conﬁguration fc-fc orthogonal weight initialization relu between. labels encoded one-hot vectors networks trained cross-entropy loss epochs batch size learning rate step-wise decreasing epochs. dataset expanded times randomized sparsiﬁcation small deviations description four datasets mentioned supplementary. results. table conveys clear winning algorithm method performs level state edge-labeled datasets results demonstrate importance exploiting edge labels convolution-based methods performance dcnn without edge labels distinctly worse justifying motivation behind paper. averaging random sparsiﬁcations test time improves accuracy small amount. results datasets withedge labels somewhat state still reasonable level though improvement case work. indicates research needed adaptation cnns general graphs. detailed discussion dataset available supplementary. baselines. compare method state weisfeiler-lehman graph kernel four approaches using deep learning least components randomized sparsiﬁcation used training time also exploited test time network prediction scores validate method applied mnist classiﬁcation problem dataset greyscale images handwritten digits represented grid size regard image point cloud points signal representing pixel edge labeling graph coarsening performed explained section mainly interested questions able reach standard performance classic baseline? kind representation learn? network conﬁguration. ecc-network parametric layers conﬁguration c-mp-cmp-c-mp-c-d-fc; notation ﬁlter-generating network section last convolution stride thus maps points single point. input graphs created model exactly corresponds regular three convolutions ﬁlters size interlaced max-poolings size ﬁnished fully connected layers. networks trained cross-entropy loss epochs batch size learning rate step-wise decreasing epochs. results. table proves network achieve level quality comparable good standard community exactly accuracy reported defferrard better offered spectral-based approaches note aiming becoming state mnist work. next investigate effect regular grid irregular mesh. discard black points point clouds corresponding data retrain network exactly test performance obtained indicating method stable respect graph structure changing sample sample. furthermore check quality learned ﬁlter generating networks compare conﬁgured mimic regular convolution using single-layer ﬁlter networks one-hot encoding offsets described section conﬁguration reaches accuracy implying perfect still perform well learning proper partitioning edge labels. last explore generated ﬁlters visually case sparse input ecc. ﬁlters continuous function edge label visualize change values dimension images sampling labels grids resolutions. coarser figure integer steps corresponding offsets shows ﬁlters exhibiting structured patterns typically found ﬁrst layer cnns. ﬁner resolution figure reveals ﬁlters fact smooth contain discontinuities apart angular artifact periodicity azimuth. interestingly artifact distinct ﬁlters suggesting network learn overcome necessary. introduced edge-conditioned convolution operation graph signal performed spatial domain ﬁlter weights conditioned edge labels dynamically generated speciﬁc input sample. shown formulation generalizes standard convolution graphs edge labels chosen properly experimentally validated assertion mnist. applied approach point cloud classiﬁcation novel setting state performance sydney dataset. furthermore outperformed deep learning-based approaches graph classiﬁcation dataset nci. source code available https//github.com/mys/ecc. feature work would like treat meshes graphs rather point clouds. moreover plan address currently higher level memory consumption case large graphs continuous edge labels example randomized clustering could also serve additional regularization data augmentation.", "year": 2017}