{"title": "Bridging LSTM Architecture and the Neural Dynamics during Reading", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "abstract": "Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its internal architecture with the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activities, indicating the correlation between LSTM architecture and the cognitive process of story reading.", "text": "fmri experiment train lstm neural network generate sequential representation story. looking potential alignment representations produced lstm neural activities recorded fmri time able explore cognitive plausibility lstm architecture although previous works tried computational models decode human brain activity associated meaning words focused isolate words. recently studied alignment latent vectors used neural networks brain activity observed magnetoencephalography subjects read story. work focused alignment word-by-word vectors produced neural networks word-by-word neural activity recorded meg. main contributions summarized follows. first show might possible brain data understand interpret illustrate encoded lstm architecture drawing parallels model components brain processes; second perform empirical study gating mechanisms demonstrate superior power gates except forget gates. long short-term memory neural network recurrent neural network able process sequence arbitrary length recursively applying transition function internal hidden state vector input sequence. activation hidden state time-step computed function current input symbol previous hidden state classic recurrent neural network gradient blow decay exponentially time. therefore lstm proposed solution vanishing gradient problem. basic unit lstm consists three gates memory cell designed analogy psychological foundation recently long short-term memory neural network attracted wide interest success many tasks. lstm architecture consists memory cell three gates looks similar neuronal networks brain. however still lacks evidence cognitive plausibility lstm architecture well working mechanism. paper study cognitive plausibility lstm aligning internal architecture brain activity observed fmri subjects read story. experiment results show artiﬁcial memory vector lstm accurately predict observed sequential brain activities indicating correlation lstm architecture cognitive process story reading. introduction recent years biologically-inspired artiﬁcial neural networks become focused topic ﬁeld computer science among various network architectures long short-term memory neural network attracted recent interest gives state-of-the-art results many tasks time series prediction adaptive robotics control connected handwriting recognition image classiﬁcation speech recognition machine translation sequence learning problems lstm extension simple recurrent neural network employs three gate vectors ﬁlter information memory vector store history information. mechanism help encode longterm information better simple rnn. despite biological inspiration architecture desgin lstm efforts understanding lstm memory cell still lacks evidence cognitive plausibility lstm architecture well working mechanism. lstm unit memory cell three gates input gate output gate forget gate. intuitively time step input gate controls much unit updated output gate controls exposure internal memory state forget gate controls amount unit memory cell erased. memory cell keeps useful history information used next process. lstm model linguistic input sentences documents ﬁrst step represent symbolic data distributed vectors also called embeddings formally lookup table word realvalued vector. unseen words regarded special symbol mapped vector. methodology complexity lstm always clear assess compare performances might useful task other. also easy interpret dense distributed representations. order explore correlation lstm architecture human cognitive process employ paradigm mapping artiﬁcial representation linguistic stimuli real observed neural activity explained figure hand stimulated series linguistic input neural response measured brain imaging techniques hand given series linguistic stimuli input information artiﬁcial model also generates abstract continuous vector representation correspondence real-time brain state. would attractive whether exists linear mapping relationship model-based representation brain activity. would guide brain imaging data brain imaging data originally acquired recorded brain activities subjects read ninth chapter famous novel harry porter philosopher’s stone chapter segmented words presented subject center screen staying seconds each. since chapter quite long complicated whole chapter divided four sections. subjects short breaks presentation different sections. section started ﬁxation period seconds subjects stared cross middle screen. total length four sections minutes. words presented subject story reading task. brain activity data collected functional magnetic resonance imaging popular brain imaging technique used cognitive neuroscience research. fmri displays poor temporal resolution brain activity acquired every seconds namely every words. details data acquisition referred course potential limitations fmri. temporal resolution compared eeg. fmri bold therefore avoid possible supervision fmri data training lstm language model. evaluation metric regarding evaluation metric evaluate model computing average cosine distance predicted functional brain image true observed brain activity certain time step story reading process. activations lstm time step test cases compute predicted brain activity calculate cosine distance since cosine distance lies normalise cosine distance accuracy test case. train linear model brain imaging data test model remaining apply -folds cross-validation order average performance model. experiment experiments dimensionality word embeddings hidden state size also initial learning rate parameters initialized randomly sampling uniform distribution based experience recurrent neural network. lstm trained according procedure neural language model predicts incoming word given history context. since chapter harry porter philosopher’s stone involved original story reading experiment remaining chapters book used training data. dent) signal indirect measurement neural activities. however high spatial resolution non-invasive characteristic made successful tool cognitive neuroscience especially human subjects. thus think appropriate measure neural dynamics fmri since widely accepted academic community. figure summarize basic information experiment setting story reading task. taking time step whole story time line example previous words ‘end’ ‘with’ ‘him’ ‘narrowly’ appeared screen one. bold signal recorded brain imaging machine after presentation words. similar arrangements carried part reading process. preprocess fmri data training model remove noise data much possible. compute default brain activity selecting fmri recording default state averaging fmri data. subtract observed brain activity default brain activity fmri data time step used experiments. alignment lstm hidden vector representations keep history information used next time step memory vector summarizes history previous words; hidden state vector used predict probability incoming word. paradigm brain-lstm mapping experiment brain activity t-th time step story reading process viewed vector continuous high-dimensional space. dimension reﬂects measured value bold signal certain tiny area brain also called voxel mathematically t-th time step vector represent activations internal neurons lstm. paper memory vector hidden state vector figure similarity real brain activities ones predicted memory vector hidden state vector lstm different model conﬁgurations. x-axis sub-ﬁgure represents window size training data. y-axis sub-ﬁgure represents window size test data. memory vector three different experimental settings eight subjects figure much between-subject variance. therefore data point figure computed averaging accuracy test cases effect long-term memory lstm explicitly differentiate short-term memory long-term memory general view unit architecture. therefore order clearly explore lstm unit learns encode long-term short-term information interaction types working memory deliberately text data different window size training corpus test stimuli. window size sentence-length training data document-length test data visualized figure training lstm neural network generating vector representation every time step choose data different window size lstm neural network. experiment results presented figure suggests memory vector lstm neural language model generally performs signiﬁcantly better hidden state vector neural network brain mapping task given hyper-parameter conﬁguration. besides accuracy predicted brain image lstm memory vector reach best performance highest accuracy predicted brain image lstm hidden state vector reach supports cognitive plausibility lstm memory cell architecture. regarding inﬂuence window size training data window size test data model performance accuracy increases large test window size general. hidden state vector concerned accuracy also increases small window size training data large window size test data. however surprised memory vector lstm architecture achieves best performance lstm model trained text data exactly word window size generate brain activity representation word sequence input document-wise test window size shown figure accuracy sharply decreases model generates representation predicts fmri signals previous words limited small test window size. accuracy memory vector test window size small matter decrease increase window size training data. indicates long-term memory plays important role constructing artiﬁcial memory. order take look role gate general lstm architecture remove input gates forget gates output gates respectively setting vector permanent all-one vector respectively. train models data different window size. results presented figure obvious dropping gates brings fatal inﬂuence performance brain image prediction task. negative impact drop input output gates performances seem even improved dropping forget gates. looking brief history lstm architecture original model input gate output gate. added forget gate lstm unit suggesting help improve model dealing continual input stream. might case story reading experiment tough prediction task since size input stream limited part novel chapter. addition reading involve processing documentwise information means setting forget gates all-one aveembedding average word embedding words read certain time step generate representation brain state. public turian word embedding dataset applying evaluation metric found aveembed heuristic model performs well achieving similarity lstm memory vector signiﬁcantly better heuristic method. hidden vectors however give poor performance. reason captures short-term information therefore fails modelling reading process involves strong integration long-term information. lstm ability encode semantics story memory vector stored information used predict brain activity similairty. compared simple overall architecture lstm cognitively plausible. gating mechanisms effective lstm ﬁlter valuable information except forget gates also consistent adaptive gating mechanism working memory system visualization analysis addition quantitative analysis above visualize part brain state dynamics subject figure including true signal sequence randomly-selected voxels signal sequences reconstructed lstm memory vector lstm hidden state vector. x-axis sub-ﬁgure represents time steps reading process colour indicates activation level certain brain region certain time step. worth noticing performance falls sharply output gates removed. technical perspective probably output gate close hidden state means output gate receives back-propagating gradient earlier gates. error correctly updated. therefore removing output gate certainly interfere normal back propagation process leading model training process towards wrong direction. comparison models compare lstm model vanilla heuristic models. vanilla hidden train vanilla language model generate series representation story hidden state vector. experiment conﬁguration training testing data best lstm model. figure real brain activity reconstructed brain activities subject four sections experiment concatenated show complete time line story reading process. pictures constructed true brain data lstm memory lstm hidden state. brain activities measuring pearson correlation coefﬁcient voxel within subject. compute averaged correlation speciﬁc anatomical region deﬁned atlas visualize correlation strength subject figure notice interesting phenomena might reﬂect association lstm memory vector predictability brain regions involved language processing semantic working memory. indicates prefrontal cortex associated semantic working memory. summarizes frontal superior gyrus associated processing word meaning temporal pole area associated sentence reading. analysis also reﬂects strong correlation reconstructed brain activity lstm memory vector observed brain activity prefrontal cortex temporal cortex especially inferior anterior part gyrus. also found reconstructed brain activity lingual gyrus fusiform highly correlated real observed activities. previous neuroscience research reported brain regions play important role word recognition. similar patterns also found subjects. connection cognitive neuroscience explore whether predictability neural activity varies among different brain regions lstm memory vector compute correlation predicted real conclusion paper explore lstm architecture sequential brain signal story reading. experiment results suggest correlation lstm memory cell cognitive process story reading. future work continue investigate effectiveness different lstm variants relating representation generated models neural dynamics. would also design reasonable artiﬁcial memory architecture better approximation working memory system language cognition. besides investigate non-linear mapping function artiﬁcial brain memories. references yoshua bengio r´ejean ducharme pascal vincent christian janvin. neural probabilistic language model. journal machine learning research barry devereux colin kelly anna korhonen. using fmri activation conceptual stimuli evaluate methods extracting conceptual representations corpora. proceedings naacl workshop computational neurolinguistics felix gers j¨urgen schmidhuber fred cummins. learning forget continual prediction lstm. neural computation alex graves. generating sequences recurrent neural networks. arxiv preprint arxiv. glyn humphreys kate mayall andrew olson cathy price. differential effects word length visual contrast fusiform lingual gyri during. proceedings royal society london biological sciences svetlana shinkareva andrew carlson kai-min chang vicente malave robert mason marcel adam just. predicting human brain activity associated meanings nouns. science francisco pereira matthew botvinick greg detre. using wikipedia learn semantic feature representations concrete concepts neuroimaging experiments. artiﬁcial intelligence leila wehbe brian murphy partha talukdar alona fyshe aaditya ramdas mitchell. simultaneously uncovering patterns brain regions involved different story reading subprocesses. plos leila wehbe ashish vaswani kevin knight mitchell. aligning context-based statistical models language brain activity reading. proceedings emnlp daniel yamins hong charles cadieu ethan solomon darren seibert james dicarlo. performance-optimized hierarchical models predict neural responses higher visual cortex. proceedings national academy sciences", "year": 2016}