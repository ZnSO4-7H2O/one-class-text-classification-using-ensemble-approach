{"title": "Texture Synthesis with Spatial Generative Adversarial Networks", "tag": ["cs.CV", "stat.ML"], "abstract": "Generative adversarial networks (GANs) are a recent approach to train generative models of data, which have been shown to work particularly well on image data. In the current paper we introduce a new model for texture synthesis based on GAN learning. By extending the input noise distribution space from a single vector to a whole spatial tensor, we create an architecture with properties well suited to the task of texture synthesis, which we call spatial GAN (SGAN). To our knowledge, this is the first successful completely data-driven texture synthesis method based on GANs.  Our method has the following features which make it a state of the art algorithm for texture synthesis: high image quality of the generated textures, very high scalability w.r.t. the output texture size, fast real-time forward generation, the ability to fuse multiple diverse source images in complex textures. To illustrate these capabilities we present multiple experiments with different classes of texture images and use cases. We also discuss some limitations of our method with respect to the types of texture images it can synthesize, and compare it to other neural techniques for texture generation.", "text": "generative adversarial networks recent approach train generative models data shown work particularly well image data. current paper introduce model texture synthesis based learning. extending input noise distribution space single vector whole spatial tensor create architecture properties well suited task texture synthesis call spatial knowledge ﬁrst successful completely data-driven texture synthesis method based gans. method following features make state algorithm texture synthesis high image quality generated textures high scalability w.r.t. output texture size fast real-time forward generation ability fuse multiple diverse source images complex textures. illustrate capabilities present multiple experiments different classes texture images cases. also discuss limitations method respect types texture images synthesize compare neural techniques texture generation. texture deﬁned image containing repeating patterns amount randomness. formally texture realization stationary ergodic stochastic process goal visual texture analysis infer generating process example texture allows generate arbitrarily many samples texture hence performing texture synthesis. success task judged primarily visual quality closeness original texture estimated human observers also criteria application speciﬁc e.g. speed analysis synthesis ability generate diverse textures arbitrary size ability create smoothly morphing textures. approaches fall broad categories. non-parametric techniques resample pixels whole patches example textures effectively randomizing input texture ways preserve visual perceptual properties. produce high quality images drawbacks \"learn\" models textures interest reorder input texture using local similarity time-consuming large textures synthesized search routines involved. methods accelerate examplebased techniques requires complicated algorithms. figure learning texture satellite image barcelona size pixels. visualize pixel subset training generated images comparison draw pixel sgan receptive ﬁeld adversarial approach texture synthesis sgan generates city texture higher visual quality output method gatys second category texture synthesis methods based matching statistical properties descriptors images. texture synthesis equivalent ﬁnding image similar descriptors usually solving optimization problem space image pixels. work portilla simoncelli notable example approach yields good image quality textures. carefully designed descriptors spatial locations orientations scales used represent statistics target textures. gatys present data driven parametric approach allow generation high quality textures variety natural images. using ﬁlter correlations different layers convolutional networks trained discriminatively large natural image collections results powerful technique nicely captures expressive image statistics. however creating single output texture requires solving optimization problem iterative backpropagation costly time memory. recent papers deal problem train feed-forward convolutional networks order speed texture synthesis approach instead costly optimization output image pixels utilize powerful deep learning networks trained produce images minimizing loss. separate network trained texture interest quickly create image desired statistics forward pass. generative approach texture synthesis uses recurrent neural network learn pixel probabilities statistical dependencies natural images. obtain good texture quality many image types method computationally expensive makes less practical texture generation cases size speed matter. present novel class generative parametric models texture synthesis using fully convolutional architecture trained employing adversarial criterion. introduced gans train generative model captures data distribution discriminator attempts separate generated training data. radford improved architecture using deep convolutional layers stride batch normalization overall gans powerful enough generate natural looking images high quality confuse even human observers however gans size output image hard coded network architecture. limitation texture synthesis much larger textures multiple sizes required. laplacian pyramids used generate images increasing size gradually adding details images stacked conditional gans. however technique figure spatial model generator transforms spatial noise array rl×m×d image rh×w× stack fractionally strided convolution layers. discriminator either generated image rectangular patch extracted image database case uses stack convolutional layers output ﬁeld probabilities fake/real data. detailed architecture generator discriminator akin varies exclusively convolutional layers potentially different number hidden layers. subnetwork projects single vector array i.e. generated output image equivalent standard however non-overlapping vectors overlapping projective ﬁelds output. view sgan convolutional roll-out gans. still limited output image sizes handle needs train models increasing complexity scale image pyramid. scale levels must also speciﬁed advance method cannot create output arbitrary size. work input images textures data distribution must learn. however modify dcgan architecture allow scalability ability create desired output texture size employing purely convolutional architecture without fully connected layers. sgan architecture especially well suited texture synthesis arbitrary large size novel application adversarial methods. experiments section examine points detail. next section describe spatial architecture. idea behind generative adversarial networks simultaneously learn generator network discriminator network task randomly sampled vector prior distribution sample rh×w× image data space. discriminator outputs scalar representing probability real training data generator learning motivated game theory generator tries fool discriminator classifying generated data real discriminator tries discriminate real generated data. adapt time generates data gets close input data distribution. sgan generalizes generator tensor rl×m×d image rh×w× figure call spatial dimensions number channels. like gans sampled prior distribution restricted experiments slice position i.e. independently sampled note architecture puts constraint dimensions network convolution layers stride zero padding similarly extended generator discriminator maps two-dimensional ﬁeld size containing probabilities indicate input real generated. order apply sgan target texture need deﬁne true data distribution pdata. extract rectangular patches rh×w× image random positions. chose size samples generator otherwise training failed experiments. reason chose symmetric architectures i.e. spatial dimensions generator discriminator derived architecture contrast original architecture however spatial gans forgo fully connected layers networks purely convolutional. allows manipulation spatial dimensions without changes weights. hence network trained generate small images able generate much larger image deployment matches local statistics training data. optimize discriminator simultaneously spatial dimensions formula ﬁrst corresponds figure second figure practice helpful apply trick minimize log)) instead log)). note model describes stochastic process image space. particular generator purely convolutional identically distributed independent location generated data translation-invariant. hence process stationary respect translations. show process also strong mixing ﬁrst need deﬁne projective ﬁeld spatial patch smallest patch image contains affected pixels possible changes full analogy refer receptive ﬁeld patch corresponding minimal patch affects assume non-overlapping patches generated data additionally take respective projective ﬁelds non-overlapping always achieved projective ﬁelds ﬁnite array made arbitrarily large. generated data independently generated. process hence strong mixing implies also ergodic. following experiments used architecture close dcgan setup convolutional layers stride generator convolutional layers stride discriminator kernels size zero padding. used uniform distribution support depending size structure texture learned used networks different complexity table used networks identical depths sizes ﬁlter banks chosen reverse yielding channels smaller spatial table time required generating output texture certain size. sgan architecture faster texturenet gatys time costs calculated pixel scale sublinearly. representations. denote sganx layers depth applied batch normalization layers except output layer input output layers network weights initialized -mean gaussians tried different sizes image patches note spatial dimensions dependent setting adjusting respective depending variables worked similarly well despite different relative impact zero padded boundaries. code implemented theano tested nvidia tesla gpu. texture generation speeds trained sgan different architectures image sizes shown table generation fast expected single forward pass. forward pass generation texturenet signiﬁcantly slower sgan pixel resolution despite fact texturenet uses fewer ﬁlters simpler sgan architecture avoids multiple scales join operations texturenet rendering computationally efﬁcient. expected method gatys orders magnitude slower iterative optimization required. initial time costs training sgan target textures. optimization used adam parameters samples minibatch. simple textures section subjective assessment indicates generated images close ﬁnal quality roughly minutes training. complex textures sections required around minutes training. training times texturenet requires hours texture. could compare directly sgan machine textures assume sgan trains efﬁciently texturenet simpler architecture. exact time number iterations required training sgans depends structure target texture. general problem training often required monitor results stop training occasionally overtraining lead degeneracy image quality degradation. common measure quality texture synthesis visually evaluate generated samples. setup small input textures allows direct comparison sgan method gatys examples used sgan layers. figure shows results textures coming stationary ergodic process. textures radishes stones figure texture synthesis small texture images different size results sgan comparable gatys left corner images indicate receptive ﬁeld size pixels sgan visually illustrate size relative texture image sizes. satellite images provide interesting examples macroscopic structures texture-like properties. city blocks particular resemble realizations stationary stochastic process different city blocks similar visual properties mixing occurs characteristic length scale given major streets. figure shows method works better satellite images concretely single image barcelona. sgan creates city-like structure whereas gatys’ method generates less structure detail. interpret following sgan trained speciﬁcally input image utilizes model power learn statistics particular texture. gatys relies pretrained ﬁlters learned imagenet dataset generalize well large images fails model salient features city image particular street grid orientation. indicate superior quality sgan texture spatial auto-correlation original synthesized textures figure shown figure calculate whole images size pixels. original sgan generation similar another show clearly directions street grid. contrast gatys’ texture looks isotropic original indicating loss visually important information. figure illustrates effects different network depths sgan generated outputs. layers larger receptive ﬁelds sgan allow larger structures learned longer streets emerge i.e. less mixing regularity given scale. figure spatial autocorrelation barcelona city example figure preferred directions city streets clearly visible centre input texture. sgan texture reﬂects structure much better result gatys approach texture synthesis combine multiple example texture images natural way. experimented ﬂowers dataset containing images various ﬂowers figure examples. resized image pixels h-dimension rescaling w-dimension preserve aspect ratio original image. trained sgan layers; minibatch training contained pixel patches extracted random positions randomly selected input ﬂower images. example non-ergodic stochastic process since input images quite different another. sample generated composite texture shown figure algorithm generates variety natural looking ﬂowers cannot blend smoothly since trained dataset single ﬂower images. still ﬁnal result looks aesthetic fusion large image datasets texture learning great potential photo mosaic applications. another experiment learned texture representing satellite images shown figure input images depict areas city amsterdam different prevailing orientations. figure demonstrates generated texture orientations inputs. although data augmentation training angled segments join smoothly model learns spatial transitions different input textures. overall amsterdam city segments come ergodic process ﬂowers example less ergodic barcelona example. based methods fuse several images naturally generative models capture statistics image patches they’re trained with. contrast methods speciﬁed image statistical descriptors generate textures match single target image closely descriptors. extending methods several images straight-forward. spatial dimensions locally independent output image pixels depend subset input noise tensor property sgan allows practical tricks creation output textures special properties. illustrate tricks brieﬂy appendix details. seamless textures important computer graphics arbitrarily large surfaces covered them. suppose want synthesize seamless texture desired size generating would require spatial dimensions given sgan model. ˆh/ˆl ratio spatial dimensions notation python slicing notation indicates indices array dimension indicates elements ﬁrst along second dimension elements along last explicitly indexed dimension. sample slightly bigger noise tensor r××d edges repeat calculate crop pixels border resulting image size tiled rectangular grid shown figure addition sgan generator create textures memory efﬁcient splitting calculation independent chunks less memory. approach allows straightforward parallelization. potential application would real-time engines sgans could produce currently visible parts arbitrary large textures. suppose rˆl× ˆm×d. split along dimension call generator twice producing call using approximately half memory call whole create desired large output concatenate partially generated images cropping edges approach limitation number pixels stored memory footprint constant. appendix precise analysis procedure. sgan synthesizes textures learning generate locally consistent image patches thus making repeating structures present textures. mixing length scale generation depends projective ﬁeld sizes. choosing best architecture depends speciﬁc texture image. holds also algorithm gatys parametric expressiveness model depends network layers used descriptive statistics calculation. figure pixels texture tiled times vertically times horizontally. straightforward create seamless textures sgan framework enforcing periodic boundary conditions rather using handcrafted features priors specify desired properties output adversarial framework learns relevant statistics information contained training texture. contrast parametric methods specify statistical descriptors priori seeing image includes models using wavelet transform properties ﬁlters neural network trained large image dataset models generalize many textures universal sometimes better train generative model single texture examples section show. interesting case describes generative model takes inputs noise vectors produces images desired statistics. analogous generator gan. however features extracted pre-trained discriminative networks play role discriminator function contrast learned discriminators adversarial frameworks. examples sections show method deals well texture images corresponding realizations stationary ergodic stochastic processes mix. possible learn statistical dependencies exceed projective ﬁeld size sgans. synthesizing regular non-mixing textures problematic methods sgan cannot learn cases distant pixels output image independent another strong mixing property sgan generation discussed section example model learns basic letter shapes text image figure fails align \"letters\" globally straight rows text. example shows approach gatys similar problems synthesizing regular textures. contrast parametric method non-parametric instance-based methods work well regular patterns. summarize capabilities method real-time generation high quality textures single forward pass generation images desired size processing time requirements scale linearly number output pixels combination separate source images complex textures seamless texture tiles next steps would like examine modiﬁcations allowing learning images longer spatial correlations strong regularity. conditional architectures help that well allow precise control generated content. conditioning used train single network variety textures simultaneously blend novel textures. future work plan examine applications sgan style transfer mosaic rendering image completion in-painting modeling data. particular data offers intriguing possibilities combination texture modeling e.g. machine generated fashion designs. sgan approach also applied audio data represented time series experiment novel methods audio waveform generation. alexei efros william freeman. image quilting texture synthesis transfer. proceedings annual conference computer graphics interactive techniques siggraph arthur szlam emily denton soumith chintala fergus. deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems goodfellow jean pouget-abadie mehdi mirza bing david warde-farley sherjil ozair aaron courville yoshua bengio. generative adversarial nets. advances neural information processing systems sergey ioffe christian szegedy. batch normalization accelerating deep network training reducing internal covariate shift. proceedings international conference machine learning dmitry ulyanov vadim lebedev andrea vedaldi victor lempitsky. texture networks feed-forward synthesis textures stylized images. international conference machine learning li-yi sylvain lefebvre vivek kwatra greg turk. state example-based texture synthesis. pauly greiner editors eurographics state reports. eurographics association examine detail projective ﬁelds inputs output pixels indicate range starting index inclusive ending index exclusive also call left right border. convenience notation write indices square e.g. refer square ﬁeld python slicing notation. simplicity express formulas valid architecture usually used sgan kernels convolutional layers stride start examining recursive relation input output fractionally strided convolutional layer. proposition input output applying convolutional layer. holds relation holds implement convolutional layer stride theano like dcgan does. note exactly relationship input output sizes transposed convolution. rewrite recursive relations function initial size count layers proposition input output convolutional. layers. holds particular size single denote table pf/rf. relations show split without loss information calculation array smaller volumes section proposition rˆl× ˆm×d. deﬁne holds python slicing notation rightmost pixel index kˆl/ left border image k−ˆl calculation pixel inﬂuenced elements i.e. holds exactly equal left half desired image would mean similar argument left-most pixel equal right half desired image proves smallest required calculate left border k−ˆl right border pixel inside similar proof made seamless i.e. periodic texture case section sketch here. making periodic spacial dimensions makes output periodic well. previous case need border overlap hence need make elements along dimension identical i.e. periodic structure needed would redundant. output image easily shown leftmost rightmost pixels together required seamless texture. pixels information elements equal numerically relative positions pixel inside volume offset exactly pixel compared position inside", "year": 2016}