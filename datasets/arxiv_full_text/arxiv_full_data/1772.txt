{"title": "Quantifying and Reducing Stereotypes in Word Embeddings", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Machine learning algorithms are optimized to model statistical properties of the training data. If the input data reflects stereotypes and biases of the broader society, then the output of the learning algorithm also captures these stereotypes. In this paper, we initiate the study of gender stereotypes in {\\em word embedding}, a popular framework to represent text data. As their use becomes increasingly common, applications can inadvertently amplify unwanted stereotypes. We show across multiple datasets that the embeddings contain significant gender stereotypes, especially with regard to professions. We created a novel gender analogy task and combined it with crowdsourcing to systematically quantify the gender bias in a given embedding. We developed an efficient algorithm that reduces gender stereotype using just a handful of training examples while preserving the useful geometric properties of the embedding. We evaluated our algorithm on several metrics. While we focus on male/female stereotypes, our framework may be applicable to other types of embedding biases.", "text": "tolga bolukbasi kai-wei chang james venkatesh saligrama adam kalai boston university saint mary’s street boston microsoft research england memorial drive cambridge machine learning algorithms optimized model statistical properties training data. input data reﬂects stereotypes biases broader society output learning algorithm also captures stereotypes. paper initiate study gender stereotypes word embedding popular framework represent text data. becomes increasingly common applications inadvertently amplify unwanted stereotypes. show across multiple datasets embeddings contain signiﬁcant gender stereotypes especially regard professions. created novel gender analogy task combined crowdsourcing systematically quantify gender bias given embedding. developed efﬁcient algorithm reduces gender stereotype using handful training examples preserving useful geometric properties embedding. evaluated algorithm several metrics. focus male/female stereotypes framework applicable types embedding biases. word embeddings trained word co-occurrence text corpora capture rich semantic information words meanings word encoded d-dimensional word vector using simple vector arithmetic embeddings capable answering analogy puzzles. instance manking woman returns queen answer similarly japan returned parisfrance tokyojapan number embeddings made publicly available including popular wordvec embedding trained million words dimensions refer wvnews embedding trained corpus text google news. word embeddings used variety downstream applications sentiment analysis question retrieval word-embeddings encode semantic information also exhibit hidden biases inherent dataset trained instance word embeddings based wvnews return biased solutions analogy puzzles fatherdoctor mothernurse mancomputer programmer womanhomemaker. publicly available embeddings produce similar results exhibiting gender stereotypes. moreover closest word query black male returns assaulted response white male entitled raises serious concerns widespread use. prejudices stereotypes embeddings reﬂect biases implicit data trained. embedding word typically optimized predict cooccuring words corpus. therefore mother nurse frequently co-occur vectors vmother vnurse also tend similar encode gender stereotypes. embeddings applications amplify biases. illustrate point consider search where example recent project shown that carefully combined existing approaches word vectors signiﬁcantly improve page relevance results consider researcher seeking summer intern work machine learning project deep learning searches linkedin graduate student machine learning neural networks. word embedding’s semantic knowledge improve relevance sense linkedin page containing terms student embeddings deep learning related different query terms ranked highly results. however word embeddings also rank research related terms closer male names female names. consequence would pages differed names mary john otherwise identical search engine would rank john’s higher mary. hypothetical example usage word embedding makes even harder women recognized computer scientists would contribute widening existing gender computer science. focus gender bias speciﬁcally male/female approach applied types biases. propose methods systematically quantify gender bias word embeddings. first quantify words corresponding professions distributed along direction embeddings she. second design algorithm generating analogy pairs embedding given seed words crowdworkers quantify whether embedding analogies reﬂect stereotypes. analogies reﬂect stereotypes hejanitor shehousekeeper healcoholism sheeating disorders. finally others provoke interesting discussions herealist shefeminist heinjured shevictim. since biases cultural enlist u.s.-based crowdworkers identify analogies judge whether analogies reﬂect stereotypes nonsensical ﬁrst establish biases indeed exist embeddings. show that surprisingly information distinguish stereotypical associations like femalehomemaker deﬁnitional associations like femalesister often removed. propose approach that given embedding handful words reduce amount bias present embedding without signiﬁcantly reducing performance benchmarks. contributions. initiate study stereotypes biases word embeddings. work follows large body literature bias language word embeddings speciﬁc interest commonly used machine learning simple geometric structures quantiﬁed mathematically. develop metrics quantify gender stereotypes word embeddings based words associated professions together automatically generated analogies scored crowd. develop algorithm reduces gender stereotypes embedding using handful training examples preserving useful properties embedding. figure comparison gender bias profession words across embeddings wordvec trained googlenews glove trained web-crawl texts. axes show projections onto he-she direction embeddings. common profession words. words closest closest colored shown plot. prior work. body prior work bias language prejudice machine learning algorithms large fully cover here. note gender stereotypes shown develop children young years statistical analyses language shown interesting contrasts language used describe women e.g. recommendation letters number online systems shown exhibit various biases racial discrimination presented users approaches modify classiﬁcation algorithms deﬁne achieve various notions fairness described number works e.g. recent survey implicit stereotypes word embedding stereotyped words. simple approach explore gender stereotypes manifest embeddings quantify words closer versus embedding space used list common profession names removing names associated gender deﬁnition name computed projection onto gender axis /||vhe vshe||. figure shows projection professions wvnews embedding different embedding trained glove dataset web-crawled texts several professions closer vector consistent across embeddings suggesting embeddings encode gender stereotypes. stereotyped analogies. professions give easilyinterpretable insights embedding stereotypes developed general method automatically detect quantify gender bias word embedding. embeddings shown perform well analogy tasks. motivated this embedding generate analogous word pairs crowd-sourcing evaluate degree stereotype pair. desired analogy heshe following properties direction align he-she; semantically similar i.e. large. based this given word embedding proposed score analogous pairs following formulation /||vhe vshe|| gender direction threshold similarity. observe setting often works well practice; corresponds requiring words forming analogy signiﬁcantly closer together random embedding vectors. embedding generated analogous pairs largest scores. avoid redundancies multiple pairs share kept pair. employed amazon mechanical turk evaluate analogies. analogy manwoman doctornurse turkers yes/no questions verify pairing makes sense analogy whether exhibits gender stereotype. every word pair judged turkers used number turkers rated pair stereotyped quantify degree bias analogy. table shows least stereotypical analogies generated wordvec googlenews. overall analogy judgments stereotypical nonsensical respectively turkers. reducing stereotypes word embedding demonstrated word embeddings contain substantial stereotypes professions analogies developed method reduce stereotypes preserving desirable geometry embedding. result impractical even impossible reduce stereotypes training word vectors. therefore assume given word vectors remove stereotypes post-processing step. approach takes following inputs word embedding stored matrix number words dimension latent matrix rnbr column space. vector representing direction stereotype. paper vshe general contain multiple stereotypes including gender racism etc. matrix rnpr whose columns correspond seed words want debias. example seed word gender manager. matrix whose columns represent background words. want algorithm preserve pairwise distances. goal generate transformation matrix following properties transformed embeddings stereotypical-free. every column vectors perpendicular column vectors transformed embeddings preserve distances vectors matrix frobenius norm ﬁrst term ensures pairwise distances preserved second term induces biases small seed words. user-speciﬁed parameter balances terms. directly solving optimization problem challenging. practice dimension matrix scale dimensions matrices axat causing computational memory issues. conduct singular value decomposition orthogonal matrices diagonal matrix. table sample analogies generated heshe wawb wvnews ordered number workers judged reﬂect stereotypes. analogies rated stereotypical workers shown random sample twelve analogies rated stereotypical workers shown. overall analogies rated reﬂecting gender stereotypes. experimental validation. validate debiasing algorithm asked turkers suggest words likely reﬂect gender stereotype collected words random setup used training columns matrix. remaining used testing. figure illustrates results algorithm. blue circles gender-stereotype words suggested turkers form held-out test set. green crosses random sample background words suggested stereotype. stereotype words close line consistent lies near midpoint she. contrast background points substantially less affected debiasing transformation. variances quantify result. test word project onto direction. compute variance projections original embedding debiasing transformation. gender-stereotype test words variance original embedding variance transformation background words variance transformation respectively. demonstrates transformation able reduce gender stereotype. figure changes word vectors gender direction. axes show absolute values projections onto he-she direction debasing. solid line diagonal. blue gender-stereotypical words test green randomly selected words suggested gender related. lastly verify debiasing transformation preserves desirable geometric structure embedding tested transformed embedding several standard benchmarks measure whether related words similar embeddings well well embedding performs analogy tasks. table shows results original transformed embeddings transformation negatively impact performance. turner patricia gervai judit. multidimensional study gender typing preschool children parents personality attitudes preferences behavior cultural differences. developmental psychology finkelstein gabrilovich evgeniy matias yossi rivlin ehud solan zach wolfman gadi ruppin eytan. placing search context concept revisited. www. joshi hrishikesh barzilay regina jaakkola tommi katerina tymoshenko alessandro moschitti marquez liuis. semi-supervised question retrieval gated convolutions. naacl.", "year": 2016}