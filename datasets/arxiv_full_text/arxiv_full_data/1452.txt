{"title": "Modeling Semantic Expectation: Using Script Knowledge for Referent  Prediction", "tag": ["cs.CL", "cs.AI", "stat.ML"], "abstract": "Recent research in psycholinguistics has provided increasing evidence that humans predict upcoming content. Prediction also affects perception and might be a key to robustness in human language processing. In this paper, we investigate the factors that affect human prediction by building a computational model that can predict upcoming discourse referents based on linguistic knowledge alone vs. linguistic knowledge jointly with common-sense knowledge in the form of scripts. We find that script knowledge significantly improves model estimates of human predictions. In a second study, we test the highly controversial hypothesis that predictability influences referring expression type but do not find evidence for such an effect.", "text": "recent research psycholinguistics provided increasing evidence humans predict upcoming content. prediction also affects perception might robustness human language processing. paper investigate factors affect human prediction building computational model predict upcoming discourse referents based linguistic knowledge alone linguistic knowledge jointly common-sense knowledge form scripts. script knowledge signiﬁcantly improves model estimates human predictions. second study test highly controversial hypothesis predictability inﬂuences referring expression type evidence effect. able anticipate upcoming content core property human language processing received attention psycholinguistic literature recent years. expectations upcoming words help humans comprehend language noisy settings deal ungrammatical input. paper computational model address question different layers knowledge inﬂuence human anticipation. focus attention semantic predictions discourse referents upcoming noun phrases. task particularly interesting allows separate semantic task anticipating intended referent processing actual surface form. example context ordered medium sirloin steak fries. later waiter brought strong expectation speciﬁc discourse referent i.e. referent introduced object preceding sentence possible referring expression could either steak ordered steak food existing models human prediction usually formulated using informationtheoretic concept surprisal. recent work however surprisal usually computed represent relevant semantic unit surface form referring expressions even though increasing amount literature suggesting human expectations different levels representation separable effects prediction consequence modelling level insufﬁcient present model addresses shortcoming explicitly modelling representing common-sense knowledge conceptually separating semantic surface level expectations. discourse referent prediction task related task coreference resolution substantially differs task following ways incrementally available left context coreference resolution uses full text; coreference resolution tries identify given target context look expectations based context distinction referent prediction prediction referring expressions also allows study closely related question natural language generation choice type referring expression based predictability intended speaker. part work inspired referent guessing experiment tily piantadosi showed highly predictable referents likely realized pronoun unpredictable referents likely realized using full effect observe consistent gricean point view principle uniform information density however tily piantadosi provide computational model estimating referent predictability. also include selectional preference common-sense knowledge effects analysis. believe script knowledge i.e. commonsense knowledge everyday event sequences represents good starting point modelling conversational anticipation. type common-sense knowledge includes temporal structure particularly relevant anticipation continuous language processing. furthermore approach build progress made recent years methods acquiring large-scale script knowledge; section hypothesis script knowledge signiﬁcant factor human anticipation discourse referents. explicitly modelling knowledge thus allow produce human-like predictions. script knowledge enables model generate anticipations discourse referents already mentioned text well anticipations textually discourse referents activated script knowledge. modelling event sequences event participants model captures many long-range dependencies normal language models able example consider following alternative text passages tion waiter brought instances food menu bill participant types. context alternative preceding sentences strong expectation instances menu food participant respectively. paper represents foundational research investigating human language processing. however also potential application assistant technology embodied agents. goal achieve human-level language comprehension realistic settings particular achieve robustness face errors noise. explicitly modelling expectations driven common-sense knowledge important step direction. order able investigate inﬂuence script knowledge discourse referent expectations corpus contains frequent reference script knowledge provides annotations coreference information script events participants section present large-scale experiment empirically assessing human expectations upcoming referents allows quantify points text humans clear anticipations not. goal model human expectations even turn incorrect speciﬁc instance. experiment conducted mechanical turk follows methodology tily piantadosi section describe computational model represents script knowledge. model trained gold standard annotations corpus assume human comprehenders usually analysis preceding discourse closely corresponds gold standard. compare prediction accuracy model human predictions well baseline models section uses structural linguistic features predicting referents; uses general script-independent selectional preference features. section test whether surprisal predict type referring expression used original texts corpus experiment also wider implications respect on-going discussion whether referring expression choice dependent predictability predicted uniform information density hyfigure excerpt story inscript corpus. referring expressions parentheses corresponding discourse referent label given superscript. referring expressions discourse referent color superscript number. script-relevant events square brackets colored orange. event type indicated corresponding subscript. scripts represent knowledge typical event sequences example sequence events happening eating restaurant. script knowledge thereby includes events like order bring well participants events e.g. menu waiter food guest. existing methods acquiring script knowledge based extracting narrative chains text eliciting script knowledge crowdsourcing mechanical turk modelling anticipated events participants motivated evidence showing event representations humans contain information current event also previous future states humans generate anticipations event sequences normal language ordinary texts including narratives encode script structure complex implicit time enable systematic study script-based expectation. contain interleaved references many different scripts usually refer single scripts point-wise fashion only relying ability reader infer full event chain using background knowledge. inscript corpus study predictive effect script knowledge. inscript crowdsourced corpus simple narrative texts. participants asked write speciﬁc activity personally experienced instructed tell story explaining activity child. resulted stories centered around speciﬁc scenario explicitly mention mundane details. thus generally realize longer event chains associated single script makes particularly appropriate purpose. inscript corpus labelled event-type participant-type coreference information. full verbs labeled event type information heads noun phrases participant types using scenario-speciﬁc lists event types participant types average template offers choice event types participant figure illustration mechanical turk experiment referent cloze task. workers supposed guess upcoming referent either choose previously activated referents write something new. figure response workers corresponding story fig. workers guessed already activated discourse referents workers also chose option wrote different lexical variants bathtub drain corresponding participant type drain. inscript corpus consists stories addressing scenarios corpus words verb instances event labels head nouns participant instances. modi report inter-annotator agreement event types participant types gold-standard eventparticipant-type annotation study inﬂuence script knowledge expectation discourse referents. addition inscript provides coreference annotation makes possible keep track mentioned discourse referents point story. information computational model prediction guessing experiment described next section. example annotated inscript story shown figure ents evaluate prediction accuracy. done testing often models manage reproduce original discourse referent tests whether verb together role correctly guessed model). however want predict correct text also model human expectation context. empirically assess human expectation created additional database crowdsourced human predictions discourse referents context using amazon mechanical turk. design experiment closely resembles guessing game extends substantial way. workers read stories inscript corpus guess upcoming participants target workers shown story excluding itself asked guess next person object likely referred case decided favour discourse referent already mentioned choose among available discourse referents clicking preceding text i.e. noun speciﬁc coreference-indicating color; figure otherwise would click button would turn asked give short description person object expected mentioned. percentage guesses agree actually referred entity taken basis estimating surprisal. experiment done stories test stories inscript corpus evenly taken scenarios. since focus effect script knowledge considered targets direct dependents script-related events. guessing started third sentence order ensure minimum context information available. keep complexity context manageable restricted guessing maximum targets skipped rest story collected guesses noun phrase instances amounts total around guesses. workers selected context cases cases. leading hypothesis script knowledge substantially inﬂuences human expectation discourse referents. guessing experiment provides basis estimate human expectation already mentioned however expect script knowledge particularly strong inﬂuence case ﬁrst mentions. script evoked text assume full script structure including participants activated available reader. tily piantadosi interested second mentions therefore make worker-generated noun phrases classiﬁed new. study effect activated explicitly mentioned participants carried subsequent annotation step worker-generated noun phrases classiﬁed new. presented annotators noun phrases contexts addition displayed participant types relevant script annotators correct target asked annotators either select participant type instantiated label unrelated script link overt antecedent text case actually second mention erroneously labeled worker. option provides basis ﬁne-grained estimation ﬁrst-mention drs. option added noticed considerable number overlooked antecedents serves correction results m-turk experiment. annotated cases identiﬁed second mentions linked participant type classiﬁed really novel. cloze task figure also reserve probability mass ‘new’ i.e. activated script context completely novel ones belonging script. principle different variants activation mechanism must distinguished. many participant types single participant belonging speciﬁc semantic class expected contrast towel participant type activate objects elements referred towel another towel. bath means participant type even activate group belonging different semantic classes since feasible enumerate potential participants ‘new’ predict participant type words number categories model equal number previously introduced plus number participant types script plus reserved corresponding script participant follows slightly abuse terminology refer categories discourse referents. unlike standard co-reference models predict co-reference chains relying entire document model incremental predicting discourse referent given position look history excluding referring expression predicted also assume past correctly resolved assigned correct participant types typical applications automatic coreference resolution systems since want model human behavior might inappropriate since automated system would underestimate human performance. strong assumption reasons explained above gold standard past res. sequently dozen parameters need estimated scenario-speciﬁc data. second order test hypothesis script information beneﬁcial prediction task need disentangle inﬂuence script information general linguistic knowledge. address carefully splitting features apart even prevents modeling interplay sources information. describe classes features below; also summary table features based tily piantadosi addition consider selectional preference feature. recency feature. feature captures distance position last occurrence candidate distance measure number sentences last mention exponentiate number make dependence extreme; recent receive noticeable weight exp). feature drs. frequency. frequency feature indicates number times candidate discourse referent mentioned far. perform bucketing. grammatical function. feature encodes dependency relation assigned head word last mention special none label new. previous subject indicator. binary feature indicates whether candidate coreferential subject previous verbal predicate. previous object indicator. object position. previous type. three-valued feature indicates whether previous mention candidate pronoun non-pronominal noun phrase never observed before. selectional preference feature captures well candidate given syntactic position given verbal predicate computed cosine similarity simcos vector-space representation structured vector-space representation predfeatures included function predicate syntactically governing unobservable target however incremental setting predicate available history subject nps. case additional probabilistic model estimates probability predicate given context marginalize predictions predicate probabilities computed based sequence preceding predicates using recurrent neural network language model estimated training set. expression denotes feature function computed referent given history composed predicate features features encode properties well characterize compatibility context. face challenges designing features. first although sizes datasets respectable script annotation perspective small learn richly parameterized model. many features address challenge using external word embeddings associate parameters simple similarity measures computed using embeddings. conicate xvr. similarities calculated using distributional memory approach similar baroni lenci structured vector space representation shown work well tasks evaluate correlation human thematic estimates thus suited task. representation computed average head word representations previous mentions word vectors obtained typedm model baroni lenci count-based third-order cooccurrence tensor whose indices word second word complex syntactic relation used stand-in semantic link. values cell tensor local mutual information estimates obtained dependency-parsed combination large corpora procedure differences baroni lenci. example estimating alternative average head words training null referent. calculated average r-ﬁllers typedm; words prototypical instrument represented summing vectors like towel soap eraser coin. predicate encountered scores scenario-relevant verbs emitted marginalization. script features section describe features rely script information. goal show common-sense information beneﬁcial performing prediction. consider script features. participant type feature characterizes well participant type candidate speciﬁc syntactic role governing predicate regarded generalization selectional preference feature participant types also specialisation considered scenario. given candidate participant type syntactic decided take yesterday afternoon working getting ready needed cleaned went decided take ﬁlled warm added undressed stepped grabbed rubbed rinsed xxxxxx relation collect predicates training participant type position embedding given average embedding predicates. feature computed product word embedding predicate predicate schemas following feature captures speciﬁc aspect knowledge prototypical sequences events. knowledge called predicate schemas recent co-reference modeling work peng predicate schemas goal model pairs events participated ﬁrst event likely participate second event example restaurant scenario observes phrase john ordered likely john waited somewhere later document. speciﬁc arguments important important argument reused across predicates. would correspond rule x-subject-of-order x-subject-of-eat. unlike previous work dataset small cannot induce rules directly rules model would generalize data well enough. instead encode intuition using similarities real-valued embedding space. remaining questions deﬁne joint embeddings estimate parameter vector joint embedding predicates principle composition function embeddings example component-wise product. inspired bordes difference word embeddings external embeddings corresponding verbs. encoding succession relation translation embedding space desirable property scoring function largely agnostic morphological form predicates. example difference embeddings rinsed rubbed similar rinse corresponding rules receive similar scores. rewrite equation parameter vector number potential ways estimated. example train discriminative classiﬁer estimate parameters. however opted simpler approach—we equal empirical estimate expected feature vector training this essentially corresponds using naive bayes model simplistic assumption score differences normally distributed spherical covariance matrices. model asked predict marked xxxxxx figure predicate-schema rules yield previously introduced score soap example previously introduced feature computed. order choose inference rules applied yield soap inspect figure preceding predicates soap object resulting potential rules x-object-of-grabbed x-object-of-rinsed x-object-of-rubbed x-object-of-rinsed. deﬁne score average rule scores. formally write table accuracies perplexities different models scenarios. script model substantially outperforms linguistic base models expected human prediction model outperforms script model irrelevant current prediction x-object-ofplugged x-object-of-ﬁlling figure even encode valid regularities xobject-of-got x-object-of-scrubbed suggest feature contaminated noise informative making predictions. however recall independent random vectors high dimensions alorthogonal assuming bounded products close zero. consequently products relevant terms example likely overcome noise. ablation studies predicateschema feature indeed predictive contributes performance full model. experiments order able evaluate effect script knowledge referent predictability compare three models full script model uses linguistic features introduced section model relies ‘linguistic features’ script-speciﬁc ones; base model includes shallow linguistic features. base model differs linguistic model model selectional preferences. table summarizes features used different models. illustrate computation example. imagine training consists document figure trained model used predict upcoming referent cloze example training document includes pair x-object-of-scrubbed x-object-of-rinsing corresponding term participates summation αobj. rely external embeddings encode semantic similarities lexical items product term high. consequently expected positive soap thus predicting soap likely forthcoming unfortunately terms expression αobj expression terms score would even higher predicate morphological form rinsing rather rinsed. however embeddings rinsing rinsed would still sufﬁciently close argument hold. narios) test sets. feature weights learned using l-bfgs optimize loglikelihood. evaluation original referents. calculated percentage correct predictions. table averages across scenarios. task appears hard humans average performance reaches accuracy. expected base model weakest system modeling selectional preferences yields extra accuracy ﬁnding incorporation script knowledge increases accuracy although still behind human performance besides accuracy perplexity computed models also human predictions. possible task solved multiple humans. used unsmoothed normalized guess frequencies probabilities. table perplexity scores consistent accuracies script model outperforms methods expected models weaker humans. used sets script features capturing different aspects script knowledge performed extra ablation studies experiments conﬁrm feature sets beneﬁcial. evaluation human expectations. previous subsection demonstrated incorporation selectional preferences perhaps interestingly integration automatically acquired script knowledge lead improved accuracy predicting discourse referents. turn another question raised introduction incorporation knowledge make predictions human-like? words able accurately estimate human expectations? includes sufﬁciently accurate also making kind incorrect predictions. evaluation therefore human guesses collected referent cloze task target. calculate relative accuracy computational model. seen figure script model approx. accuracy accurate predicting human guesses linguistic model base model. also observe margin script model linguistic model larger evaluation base model linguistic model. indicates model access script knowledge much similar human prediction behavior terms guesses script-agnostic models. would like assess predictions similar distributions rather yielding similar predictions. order compare distributions jensen-shannon divergence symmetrized version kullbackleibler divergence. intuitively measures distance probability distributions. smaller value indicative similar distributions. figure shows probability distributions resulting script model similar human predictions linguistic base models. experiments shown script knowledge improves predictions upcoming referents script model best among models approximating human referent predictions. uniform information density hypothesis suggests speakers tend convey information uniform rate applied choice referring expression type would predict highly predictable referent encoded using short code unpredictable referent encoded using longer form information density measured using information-theoretic measure surprisal message successful explaining variety linguistic phenomena; jaeger however controversy whether affects pronominalization. tily piantadosi report evidence writers likely refer using pronoun proper name referent easy guess full readers less certainty upcoming referent; also arnold experiments failed effect predictability pronominalization present study hence contributes debate whether affects referring expression choice. model referring expression choice goal determine whether referent predictability correlated type referring expression used text. focus distinction between pronouns full noun phrases. data also contains small percentage proper names small class size earlier ﬁndings proper nouns behave much like pronouns combined pronouns proper names single class short encodings. referring expression type prediction task estimate surprisal referent computational models section well human cloze task. surprisal upcoming discourse referent based previous context order determine whether referent predictability effect referring expression type factors known affect choice referring expression train logistic regression model referring expression type response variable discourse referent predictability well large linguistic factors explanatory variables. model deﬁned follows four different logistic regression models. models contained exactly linguistic predictors differed estimates used referent type surprisal residual entropy. logistic regression model used surprisal estimates based human referent cloze task three models used estimates based three computational models experiment interested choice referring expression type occurrences references real choice possible. therefore exclude analysis reported ﬁrst mentions well ﬁrst second person pronouns subset contains data points. results results four logistic regression models shown table ﬁrst take look results linguistic features. variability terms exact coefﬁcient estimates models effect features largely consistent across models. instance positive coefﬁcients recency feature means previous mention happened table coefﬁcients obtained regression analysis different models. types considered full pronoun/propernoun base class full signiﬁcance ‘***’ ‘**’ coefﬁcients surprisal estimates different models however signiﬁcantly different zero. model comparison shows improve model also used estimated models predict referring expression type data found surprisal estimates models improve prediction accuracy. effect even holds human cloze data. hence cannot interpreted problem models—even human predictability estimates dataset predictive referring expression type. also calculated regression models full dataset including ﬁrst second person pronouns well ﬁrst mentions results full dataset fully consistent ﬁndings shown table signiﬁcant effect surprisal referring expression type. result contrasts ﬁndings tily piantadosi reported signiﬁcant effect surprisal type data. order replicate settings closely possible also included residualentropy predictor model however change results. study incrementally predicting discourse referents showed script knowledge highly important factor determining human discourse expectations. crucially computational modelling approach allowed tease apart different factors affect human prediction cannot manipulate humans directly modelling common-sense knowledge terms event sequences event participants model captures many long-range dependencies normal language models. script knowledge automatically induced model crowdsourced scenario-speciﬁc text collections. second study test hypothesis uniform information density affects referring expression type. question highly controversial literature tily piantadosi signiﬁcant effect surprisal referring expression type corpus study similar ours studies tightly controlled experimental approach found effect predictability type present study replicating exactly setting terms features analysis support effect type. difference results results could different corpora text sorts used; speciﬁcally would expect larger predictability effects might observable script boundaries rather within script case stories. next step moving participant prediction model towards applications would replicate modelling results automatic textto-script mapping instead gold-standard data done furthermore move complex text types include reference several scripts. plan consider recently published stories corpus large crowdsourced collection topically unrestricted short simple narratives basis next steps research. thank editors anonymous reviewers insightful suggestions. would like thank florian pusse helping amazon mechanical turk experiment. would also like thank simon ostermann tatjana anikina helping inscript corpus. research partially supported german research foundation part ‘information density linguistic encoding’ european research council part starting grant broadsem dutch national science foundation part vidi part mmci cluster excellence marco baroni georgiana dinu germ´an kruszewski. don’t count predict systematic comparison context-counting context-predicting semantic vectors. proceedings acl. antoine bordes nicolas usunier alberto garcia-duran jason weston oksana yakhnenko. translating embeddings modeling multi-relational data. proceedings nips. marta kutas katherine delong nathaniel smith. look around lies ahead prediction predictability language processing. predictions brain using past generate future. tomas mikolov martin karaﬁ´at lukas burget cernock`y sanjeev khudanpur. recurrent neural network based language model. proceedings interspeech. tomas mikolov stefan kombrink anoop deoras lukar burget cernocky. rnnlm-recurrent proneural network language modeling toolkit. ceedings asru workshop. nasrin mostafazadeh nathanael chambers xiaodong devi parikh dhruv batra lucy vanderwende pushmeet kohli james allen. corpus cloze evaluation deeper understanding commonsense stories. proceedings naacl. rachel rudinger vera demberg ashutosh modi benjamin durme manfred pinkal. learning predict script events domain-speciﬁc text. proceedings international conference lexical computational semantics asad sayeed clayton greenberg vera demberg. thematic evaluation aspect selectional preferences. proceedings workshop evaluating vector space representations harry tily steven piantadosi. refer efﬁciently less informative expressions predictable meanings. proceedings workshop production referring expressions bridging computational empirical approaches reference. alessandra zarcone marten schijndel jorrig vogels vera demberg. salience attention surprisal-based accounts language processing. frontiers psychology", "year": 2017}