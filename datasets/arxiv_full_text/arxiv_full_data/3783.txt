{"title": "How many faces can be recognized? Performance extrapolation for  multi-class classification", "tag": ["stat.ML", "cs.CV", "cs.IT", "cs.LG", "math.IT"], "abstract": "The difficulty of multi-class classification generally increases with the number of classes. Using data from a subset of the classes, can we predict how well a classifier will scale with an increased number of classes? Under the assumption that the classes are sampled exchangeably, and under the assumption that the classifier is generative (e.g. QDA or Naive Bayes), we show that the expected accuracy when the classifier is trained on $k$ classes is the $k-1$st moment of a \\emph{conditional accuracy distribution}, which can be estimated from data. This provides the theoretical foundation for performance extrapolation based on pseudolikelihood, unbiased estimation, and high-dimensional asymptotics. We investigate the robustness of our methods to non-generative classifiers in simulations and one optical character recognition example.", "text": "difﬁculty multi-class classiﬁcation generally increases number classes. using data subset classes predict well classiﬁer scale increased number classes? assumption classes sampled exchangeably assumption classiﬁer generative show expected accuracy classiﬁer trained classes moment conditional accuracy distribution estimated data. provides theoretical foundation performance extrapolation based pseudolikelihood unbiased estimation high-dimensional asymptotics. investigate robustness methods non-generative classiﬁers simulations optical character recognition example. multi-class classiﬁcation observes pairs feature vectors unknown labels countable label goal construct classiﬁcation rule predicting label data point; generally classiﬁcation rule learned previously observed data points. many applications multi-class classiﬁcation face recognition image recognition space potential labels practically inﬁnite. setting might consider sequence classiﬁcation problems ﬁnite label subsets i-th problem constructs classiﬁcation rule supposing joint distribution deﬁne accuracy i-th problem zi]. using data predict accuracy achieved larger label problem performance extrapolation. practical instance performance extrapolation occurs neuroimaging studies number classes limited experimental considerations. obtained fmri brain scans record single subject’s visual cortex responds natural images. label corresponds space grayscale photographs natural images subset photographs used experiment. construct classiﬁer achieves accuracy classifying photographs; based exponential extrapolation estimate would take order photographs accuracy model drops directly validating estimate would take immense resources would useful develop theory needed understand compute extrapolations principled way. however fully general setting impossible construct non-trivial bounds accuracy achieved classes based knowledge could consist entirely well-separated classes classes consist entirely highly inseparable classes vice-versa. thus important assumption theory exchangeable sampling. labels assumed exchangeable sample condition exchangeability ensures separability random subsets inferred looking empirical distributions therefore estimate achievable accuracy obtained. assumption exchangeability greatly limits scope application methods. many multi-class classiﬁcation problems hierarchical structure class labels distributed according non-uniform discrete distributions e.g. power laws either case exchangeability violated. would interesting extend theory hierarchical setting handle non-hierarchical settings non-uniform prior class probabilities leave subject future work. addition assumption exchangeability consider restricted classiﬁers. focus generative classiﬁers classiﬁers work training model separately class. convenient property allows characterize accuracy classiﬁer selectively conditioning class time. section technique reveal equivalence expected accuracies moments common distribution. moment equivalence result allows standard approaches statistics u-statistics nonparametric pseudolikelilood directly applied extrapolation problem discuss section non-generative classiﬁers classiﬁcation rule joint dependence entire classes cannot analyzed conditioning individual classes. section empirically study performance classiﬁers. since generative classiﬁers comprise minority classiﬁers used practice applied methods variety generative non-generative classiﬁers simulations dataset. methods varying success generative non-generative classiﬁers seem work badly neural networks. contribution. knowledge ﬁrst formalize problem prediction extrapolation. introduce three methods prediction extrapolation method extended unbiased estimation constrained pseudolikelihood method novel. third method based asymptotics application recently proposed method estimating mutual information motivated problem performance extrapolation reformulate problem notational theoretical convenience. instead requiring random subset section take size without losing generality since monotonic sequence ﬁnite subsets embedded sequence |zk| addition rather randomizing labels randomize marginal distribution label; towards space feature vectors measurable space probability distributions probability measure inﬁnite sequence i.i.d. draws refer probability measure probability measures meta-distribution. distributions marginal distributions ﬁrst classes. assuming labels equiprobable rewrite accuracy order construct classiﬁcation rule need data classes instances multi-class classiﬁcation observes independent observations used construct classiﬁer. since order observations generally matter sufﬁcient statistic training data t-th classiﬁcation problem collection empirical distributions class. henceforth make simplifying assumption training data i-th class remains ﬁxed drop superscript write conditional distribution given also write marginal distribution example suppose every class number training examples empirical distribution i.i.d. observations empirical meta-distribution meanwhile true meta-distribution empirical distribution i.i.d. draws random extending formalism tewari bartlett deﬁne classiﬁer collection mappings called classiﬁcation functions. intuitively speaking classiﬁcation function learns model ﬁrst arguments empirical marginals classes ˆfk. class classiﬁer assigns real-valued classiﬁcation score query point higher score indicates higher estimated probability belongs k-th class. therefore classiﬁcation rule corresponding classiﬁer assigns class maximum classiﬁcation score called single-class classiﬁcation function generative classiﬁer. quadratic discriminant analysis naive bayes examples generative classiﬁers. generative property allows prove strong results accuracy classiﬁer exchangeable sampling assumption section performance extrapolation generative classiﬁers specialize case generative classiﬁer classiﬁcation function consider estimating expected accuracy k-th classiﬁcation problem framework deﬁne classiﬁer vector-valued function. however introduce notion classiﬁer multiple-argument functional empirical distributions echoes functional formulation estimators common statistical literature. deﬁne conditional accuracy function maps distribution test observation real number conditional accuracy gives probability independently drawn greater deﬁne conditional accuracy distribution generated follows true distribution drawn empirical distribution drawn query drawn independent signiﬁcance conditional accuracy distribution expected accuracy written terms moments. theorem single-distribution classiﬁcation function distribution assume jointly satisfy tie-breaking property properties conditional accuracy distribution conditional accuracy distribution determined conditional accuracy distribution without making assumptions either answer much. arbitrary probability measure construct conditional accuracy distribution even makes perfect sampling assumption theorem deﬁned theorem denote then probability distribution construct meta-distribution classiﬁcation function conditional accuracy distribution perfect sampling uniform deﬁne also straightforward calculation yields hand obtain positive result assume classiﬁer approximates bayes classiﬁer. assuming absolutely continuous respect lebesgue measure probability bayes classiﬁer results assuming perfect sampling taking theorem states bayes classiﬁer measure density monotonically increasing. since ‘good’ classiﬁer approximates bayes classiﬁer intuitively expect monotonically increasing density good model conditional accuracy distribution ‘good’ classiﬁer. theorem absolutely continuous respect probability regular conditional probability distribution. denote density monotonic proof. sufﬁces prove coincides deﬁnition special case generative. high level hierarchical model drawn distribution binomial. assume density marginal distribution written however observed {vij} comprise i.i.d. sample. discuss following three approaches estimating based vij. ﬁrst extension unbiased estimation based binomial u-statistics discussed section second pseudolikelihood approach. problems marginal distributions known dependence structure variables unknown pseudolikelihood deﬁned product marginal distributions. certain problems time series analysis spatial statistics maximum pseudolikelihood estimator proved consistent discuss pseudolikelihood-based approaches section thirdly note high-dimensional theory anon applied prediction accuracy discuss section theorem motivates monotonicity constraint second constraint restrict k-th moment match unbiased estimate. addition constraints yields constrained pmle ˆηcon obtained solving discretizing maximization problems solved using general-purpose convex solver. added constraints guarantee unique solution improve estimation thus improve moment estimation high-dimensional asymptotics number conditions distribution including large dimension anon relate accuracy bayes classiﬁer mutual information label response found disciplined convex programming language using ecos second-order cone programming solver succeeds optimizing problems dimension discretized large applied methods described section simulated gaussian mixture telugu character classiﬁcation task simulated gaussian mixture vary size initial subset classes classes extrapolate performance gaussian mixture model multinomial logistic one-layer neural network figure shows predicted k-class accuracy changes varied. predicted accuracy curves logistic similar behavior even though generative multinomial logistic not. three methods perform better logistic classiﬁers neural network fact neural network test accuracy initial becomes better estimator three proposed methods curve. also exponential extrapolation method ˆpexp variable constrained pseudolikelihood ˆpcon high-dimensional estimator ˆphd. additional simulation results found supplement. character classiﬁcation task predict -class accuracy naive bayes multinomial logistic regression \u0001-nearest neighbors deep neural networks using -class data training examples class taking test accuracy classes proxy compare performance three extrapolation methods; benchmark also consider using test accuracy classes estimate. exponential extrapolation method performs well deep neural network. meanwhile constrained pmle achieves accurate extrapolation four classiﬁers logistic failed converge deep neural network highdimensional estimator ˆphd performs well multinomial logistic deep neural network classiﬁers. three methods beat benchmark ﬁrst four classiﬁers; however benchmark best estimator deep neural network similarly observe simulation k-nearest neighbors ﬁxed network architecture follows x-c-mp-c-c-mp-c-c-mp-c-sm. binary input image convolutional layer output maps max-pooling layer softmax output layer classes. table performance extrapolation predicting accuracy classes using data classes telugu character dataset. indicates failure converge. \u0001-nearest neighbors. empirical results indicate methods generalize beyond generative classiﬁers. possible explanation since bayes classiﬁer generative classiﬁer approximates bayes classiﬁer also ‘approximately generative.’ however important caveat classiﬁer must already attain close bayes accuracy smaller subset classes. classiﬁer initially bayes classiﬁer becomes accurate classes added theory could underestimate accuracy larger subset. non-issue generative classiﬁers training data class ﬁxed since generative classiﬁer approximates bayes rule single-class classiﬁcation function approximates bayes optimal single-class classiﬁcation function. hand classiﬁers built-in model selection representation learning expected classiﬁcation functions become accurate sense better approximate monotonic function bayes classiﬁcation functions data classes added. results still inconclusive recommend estimators practice. theoretically still remains derive conﬁdence bounds generative case; practically additional experiments needed establish reliability estimators speciﬁc applications. also remains plenty room improved estimators area instance ﬁxing instability constrained pseudolikelihood estimator test accuracy high. thank john duchi steve mussmann qingyun jonathan taylor trevor hastie robert tibshirani useful discussion. supported graduate research fellowship.", "year": 2016}