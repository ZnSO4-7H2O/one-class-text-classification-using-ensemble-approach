{"title": "Word Representation Models for Morphologically Rich Languages in Neural  Machine Translation", "tag": ["cs.NE", "cs.CL"], "abstract": "Dealing with the complex word forms in morphologically rich languages is an open problem in language processing, and is particularly important in translation. In contrast to most modern neural systems of translation, which discard the identity for rare words, in this paper we propose several architectures for learning word representations from character and morpheme level word decompositions. We incorporate these representations in a novel machine translation model which jointly learns word alignments and translations via a hard attention mechanism. Evaluating on translating from several morphologically rich languages into English, we show consistent improvements over strong baseline methods, of between 1 and 1.5 BLEU points.", "text": "dealing mplex word forms morphologically rich languages open problem language processing particularly important translation. contrast modern neural systems translation discard identity rare words paper propose several architectures learning word representations character morpheme level word decompositions. incorporate representations novel machine translation model jointly learns word alignments translations hard attention mechanism. evaluating translating several morphologically rich languages english show consistent improvements strong baseline methods bleu points. models end-to-end machine translation based neural networks shown produce excellent translations rivalling surpassing traditional statistical machine translation systems central challenge neural handling rare uncommon words. conventional neural models ﬁxed modest-size vocabulary identity rare words lost makes translation exceedingly difﬁcult. accordingly sentences containing rare words tend translated much poorly containing common words rare word problem particularly exacerbated translating morphology rich languages several morphological variants words result huge vocabulary heavy tail distribution. example russian least words encoding case gender number sentiment semantic connotations. many words share common lemma contain regular morphological afﬁxation; consequently much information required translation present accessible form models neural paper propose solution problem constructing word representations compositionally smaller sub-word units occur frequently words themselves. show representations effective handling rare words increase generalisation capabilities neural beyond vocabulary observed training set. propose several neural architectures compositional word representations systematically compare methods integrated novel neural model. speciﬁcally make character sequences morpheme sequences building word representations. sub-word units combined using recurrent neural networks convolutional neural networks simple bag-ofunits. work inspired research compositional word approaches proposed language modelling notable exceptions approaches applied challenging problem translation. integrate word representations novel neural model build robust word representations source language. feng cohn considers translation sequential decision process. decisions involved generating target word decomposed separate translation alignment factors factor modelled separately conditioned rich history recent translation decisions. considered form attentional encoder-decoder bahdanau hard attention decision contextualised source word contrasting soft attention bahdanau integrating word models neural provide ﬁrst time comprehensive systematic evaluation resulting word representations translating english several morphologically rich languages russian estonian romanian. evaluation includes intrinsic extrinsic metrics compare approaches based translation performance well ability recover synonyms rare words. show morpheme character representation words leads much better heldout perplexity although improvement translation bleu scores modest. intrinsic analysis shows recurrent encoder tends capture morphosyntactic information words whereas convolutional network better encodes lemma. factors provide different strengths part translation model might lemmas generalise words sharing translations morphosyntax guide reordering contextualise subsequent translation decisisions. factors also likely important language processing applications. neural models rely words basic units consequently face problem handle tokens test out-ofvocabulary i.e. appear training often words either assigned special token allows application data however comes expense modelling accuracy especially structured problems like language modelling translation identity word paramount making next decision. word units using model word composite morphemes. luong proposed recursive combination morphs using afﬁne transformation however unable differentiate compositional non-compositional cases. botha blunsom address problem forming word representations adding word’s morpheme embeddings word embedding. morpheme based methods rely good morphological analysers however available limited languages. unsupervised analysers prone segmentation errors particularly fusional polysynthetic languages. settings character-level word representations appropriate. several authors proposed convolutional neural networks character sequences part models part speech tagging language models machine translation models able capture orthographic similarity also semantics. another strand research looked recurrent architectures using long-short term memory units capture long orthographic patterns character sequence well non-compositionality. aforementioned models shown consistently outperform standard word-embedding approaches. systematic investigation various modelling architectures comparison characters versus morpheme atomic units word composition. work consider morpheme character levels study wether character-based approaches outperform morpheme-based importantly linguistic lexical aspects best encoded type architecture efﬁcacy part machine translation model translating morphologically rich languages. ﬁrst contribution paper neural network variant operational sequence model translation modelled sequential decision process. words target sentence generated time left-to right order similar decoding strategy traditional phrase-based smt. decisions involved generating target word decomposed number separate factors factor modelled separately conditioned rich history recent translation decisions. previous work sequence operations modelled markov chain bounded history translation decision conditioned ﬁnite history past decisions. using deep neural architectures model sequence translation decisions non-markovian chain i.e. unbounded history. therefore approach able capture long-range dependencies commonplace translation missed previous approaches. speciﬁcally operations generation target word jumps source sentence capture re-ordering aligning null capture gappy phrases ﬁnishing translation process. probability sequence operations generate target translation given source sentence figure model architecture several approaches learning word representations showing left bagof-morphs bilstm morphs character convolution. note bilstm also applied character level. input word täppi-de-ga estonian speckled bearing plural comitative sufﬁxes. right order) -to- correspondence alignment left-hand-side. model generates target sentence sequence operations recurrent neural network stage state function previous state previously generated target word aligned source word using single layer permlp ceptron applies afﬁne transformation concatentated input vectors followed tanh activation function rvs×es word embedding matrices size source vocabulary size target vocabulary word embedding sizes target source languages respectively. jump action moving source sentence ﬁnishing translation process τ|t|+ finish. worth noting sequence operations generating target translation captures important aspects candidate position next alignment current alignment position; reminiscent features captured alignment model. feature vector composed parts ﬁrst part one-hot vector activating proper feature depending whether equal action null finish second part consists features ij+−ij note neural considered hard attentional model opposed soft attentional neural translation model soft attentional model dynamic summary source sentence used context translation decision formulated weighted average encoding source positions. hard attentional model context comes encoding single ﬁxed source position. beneﬁt allowing external information included model predicted alignments high quality word alignment tools complementary strengths compared neural network translation models. turn problem learning word representations. outlined above translating morphologically rich languages treating word types unique discrete atoms highly naive compromise translation quality. better accuracy would need characterise words subword units order capture lemma morphological afﬁxes thereby allowing better generalisation similar word forms. more generally capture aspects past alignment decisions hence used impose structural biases constrain alignment space neural e.g. symmetry fertility position bias. compare baseline word embedding approach. type sub-word encoder learn word representations estimated sub-units word embedding. pooling embeddings obtain word representation embedding word sub-word encoding. pooling operation captures non-compostionality semantic meaning word relative sub-parts. assume model would favour unit-based embeddings rare words word-based common ones. vocabulary sub-word units i.e. morphemes characters dimensionality unit embeddings reu×|u| matrix unit embeddings. suppose word source dictionary made sequence units stands number constituent units word. combine representation sub-word units using lstm recurrent neural networks convolutional neural network simple bag-of-units resulting word representations neural source word embeddings. encoding word formulated using pair lstms operating leftto-right input sequence another operating right-to-left lstm single hidden layer tanh activation function form word representation convolutional encoder last word encoder consider convolutional neural network inspired similar approach language modelling reu×|u|w denote unit-level representation column corresponds unit embedding idea unitlevel apply kernel reu×kl width obtain feature r|u|w−kl+. formally element feature convolutional representation tanhuwj ql+b) reu×kl slice spans representations unit preceding units frobenius inner product. example suppose input size kernel size sliding step then obtain feature map. process implements character n-gram equal width ﬁlter. word representation derived pooling feature maps kernels order capture interactions character n-grams obtained ﬁlters highway network applied pooling layer mlpσ sigmoid gating function modulates tanh transformation input preserving input setup. compare different word representation models based three morphologically rich languages using exterinsic intrinsic evaluations. exterinsic evaluation investigate effects translating english estonian romanian russian using neural osm. intrinsic evaluation investigate accurately models recover semantically/syntactically related words given words. datasets. parallel bilingual data europarl estonian-english romanian-english web-crawled parallel data russian-english preprocessing tokenize lower-case ﬁlter sentences longer words. furthermore apply frequency threshold replace low-frequency words special token. split corpora three partitions training development test; table provides datasets statistics. morfessor training. morfessor catmap perform morphological analysis needed morph-based neural models. morfessor rely linguistic knowledge instead relays minimum description length principle construct stems afﬁxes paradigms explains data. word form represented ∗+∗. morfessor entire initial datasets ﬁltering long sentences. word perplexity morfessor parameter adjusted. parameter depends vocabulary size larger vocabulary requires higher perplexity number; setting perplexity threshold small value results over-splitting. experimented various thresholds tuned yield reasonable morpheme inventories. table presents percentage unknown words test source language reconstruction considered words native alphabet only. recovering rate depends model. characters words could easily rebuilt. case morpheme-based approach quality mainly depends morfessor output level word segmentation. terms morphemes estonian presents highest reconstruction rate therefore expect beneﬁt morpheme-based models. romanian hand presents lowest unknown words rate morphologically simple three languages. morfessor quality russian table corpus statistics parallel data russian/romanian/estonian english. rate fraction word types source language test frequency cut-off unseen training. extrinsic evaluation training. annotate training sentence-pairs sequence operations training neural model. ﬁrst word aligner align target word source word. read sequence operations scanning target words left-to-right order. result training objective consists maximising joint probability target words alignments performed stochastic gradient descent training stops likelihood objective development starts decreasing. re-ranker standard features generated moses underlying phrase-based system plus additional features coming neural model. neural features based generated alignment translation probabilities correspond ﬁrst second terms respectively. train reranker using mert restarts. translation metrics. bleu meteor measure translation quality reference. bleu purely based exact match n-grams generated reference translation whereas meteor takes account matches based stem synonym paraphrases well. particularly suitable morphology representation learning methods since result using translation paraphrases. train paraphrase table meteor using entire initial bilingual corpora based pivoting results. table shows translation alignment perplexities development sets models trained. seen cnnchar model leads lower word alignment perplexities almost cases. interesting shows power model ﬁtting morphologically complex languages using characters. table presents bleu meteor score results re-ranker optimised meteor bleu reporting corresponding score. seen re-ranking based neural models’ scores outperforms phrase-based baseline. furthermore translation quality bilstmmorph model outperforms others romanian estonian whereas cnnchar model outperforms others russian consistent expectations. assume replacing morfessor real morphology analyser language improve performance morpheme-based models leave future research. however translation quality neural models signiﬁcantly different convoluted contributions high frequency words bleu meteor. therefore investigate representation learning models intrinsically next section. semantic morphological information nearest neighbour words. learning representations frequency words harder highfrequency words since cannot capitalise reliably contexts. therefore split test lexicon subsets according frequency training since word frequency threshold training words appearing frequency band fact oovs test set. word test take top- nearest neighbours whole training lexicon using cosine metric. semantic evaluation. investigate well nearest neighbours interchangable query word translation process. formalise notion semantics source words based translations target language. pivoting deﬁne probability candidate word synonym query word pp|f target language word translation probabilities inside summation estimated using wordbased translation model trained entire bilingual corpora take top- probable words gold synonyms query word test set. measure quality predicted nearest neighbours using multi-label accuracy ∩n=∅] sets gold standard synonyms nearest neighbors respectively; function condition true zero otherwise. words fraction words whose nearest neighbours gold standard synonyms non-empty overlap. table presents semantic evaluation results. seen words frequency cnnchar model performs best across three languages. superiority particularly interesting words model cooked representations commorphological evaluation. turn evaluating morphological component. evaluation focus russian since notoriously hard morphology. another morphological analyser mystem generate linguistically tagged morphological analyses word e.g. tags case person plurality etc. represent morphological analysis vector showing presence grammatical features. word assigned vectors corresponding morphological analyses. morphology similarity words take minimum hamming similarity corresponding sets vectors. table shows average morphology similarity words nearest neighbours across frequency bands. likewise represent words based lemma features; table shows average lemma similarity. character-based models capture morphology better morpheme-based ones especially cases words. also clear tends outperform bi-lstm case compare lemmas bi-lstm seems better capturing afﬁxes. take closer look character-based models. manually created non-existing russian words three types. words ﬁrst consist known root afﬁxes combination atypical although might guess meaning. second type corresponds words non-existing root meaningful afﬁxes might guess part speech properties e.g. gender plurality case. finally third type comprises words known root morphemes combination absolutely possible language meaning hard guess. table shows strongly biased towards longest substring matching beginning word yields better recall retrieving words sharing lemma. bi-lstm hand mainly focused matching patterns ends regardless middle word. results higher recall words sharing grammar features. paper proposes novel translation model incorporating hard attentional mechanism. context compared four different models morphemecharacter-level word representations source language. models lead robust encodings words morphological rich languages overall better translations simple word embeddings. detailed analyses shown word-embeddings superior frequent words whereas convolutional method bilstmchar ne+blagodar+n+os´t ingratitude ne+forma´l +n+os´t informality ne+mysl+im+os´t unthinkableness ne+kompeten+t+n+os´t incompetence ne+gramot+n+os´t illiteracy table analysis similar words nonsense russian words cnnchar bilstmchar word encodings based cosine similarity. diacritic indicates softness. tags s-noun a-adjective v-verb; gender m-masculine -feminine nneuter; number sg-singular pl-plural; case nom-nominative gengenitive dat-dative acc-accusative ins-instrumental abl-prepositional loc-locative; tense praes-present inpraes-continuous praet-past perfect -imperfect; indic-indicative; transitivity trans-transitive intr-intransitive; adjective form br-brevity plen-full form posspossessive; comparative supr-superlative comp-comparative; noun person p-ﬁrst p-second p-third;other geo-geolocation nonsnonsense inc-incorrect spelling famn-family name praed-predicative best handling rare words. comparison convolutional recurrent methods character sequences shown convolutional method better captures lemma critical importance translating out-of-vocabulary words would also many semantic applications.", "year": 2016}