{"title": "The Modular Audio Recognition Framework (MARF) and its Applications:  Scientific and Software Engineering Notes", "tag": ["cs.SD", "cs.CL", "cs.CV", "cs.MM", "cs.NE", "I.5; I.2.6; D.2.10; D.2.11; D.2.5; D.2.2; I.2.7"], "abstract": "MARF is an open-source research platform and a collection of voice/sound/speech/text and natural language processing (NLP) algorithms written in Java and arranged into a modular and extensible framework facilitating addition of new algorithms. MARF can run distributively over the network and may act as a library in applications or be used as a source for learning and extension. A few example applications are provided to show how to use the framework. There is an API reference in the Javadoc format as well as this set of accompanying notes with the detailed description of the architectural design, algorithms, and applications. MARF and its applications are released under a BSD-style license and is hosted at SourceForge.net. This document provides the details and the insight on the internals of MARF and some of the mentioned applications.", "text": "marf stands modular audio recognition framework. contains collection algorithms sound speech natural language processing arranged uniform framework facilitate addition algorithms preprocessing feature extraction classiﬁcation parsing etc. implemented java. marf also research platform various performance metrics implemented algorithms. main goal build general open-source framework allow developers audio-recognition industry choose apply various methods contrast compare them applications. proof concept user frontend application textindependent speaker identiﬁcation created framework variety testing applications applications show various aspects marf also present. recent addition experimental support also included marf ..-devel- information applications employ marf chapter chosen implement project using java programming language. choice justiﬁed binary portability java applications well facilitating memory management tasks issues concentrate algorithms instead. java also provides built-in types data-structures manage collections eﬃciently term marf refer software accompanies documentation. application programmer could anyone using wants part marf system. marf developer core member marf hacking away marf system. permission copy modify distribute software documentation purpose without without written agreement hereby granted provided copyright notice paragraph following paragraphs appear copies. event shall concordia university authors liable party direct indirect special incidental consequential damages including lost profits arising software documentation even concordia university authors advised possibility damage. concordia university authors specifically disclaim warranties including limited implied warranties merchantability fitness particular purpose. software provided hereunder as-is basis concordia university authors obligations provide maintenance support updates enhancements modifications. cl´ement iclementusers.sourceforge.net serguei mokhov mokhovcs.concordia.ca a.k.a serge dimitrios nicolacopoulos pwrslaveusers.sourceforge.net a.k.a jimmy suggestions contributions make reports don’t hesitate contact marf-related issues please contact marf-devellists.sf.net. please report bugs marf-bugs lists.sf.net. marf project initiated september four students concordia university montr´eal canada course project pattern recognition guidance c.y. suen. included cl´ement stephen sinclair jimmy nicolacopoulos serguei mokhov. jimmy focused implementation wave format loader storage issues. serguei designed entire marf framework architecture originally implemented general distance classiﬁer chebyshev minkowski mahalanobis incarnations along normalization sample data. serguei designed exceptions framework marf involved integration modules testing applications marf. shuxin ‘susan’ contributed development maintenance test applications initial adoption junit framework within marf. also ﬁnalized utility modules till completion performed marf code auditing inventory. shuxin also added netbeans project ﬁles build system marf. it’s project maintained developed spare time course over serguei mokhov primary maintainer project. rewrote storage support polished marf years added various utility modules support implementation algorithms applications. serguei maintains manual site sample database collection. also made releases project follows ..-devel- sunday february ..-devel- wednesday august ..-devel- saturday july ..-devel- monday june ..-devel- monday june monday february monday february december final project deliverable december demo project since inception always open-source project. releases including current time accessible <http//marf.sourceforge.net> provided sourceforge.net. complete documentation well manual sources available download page. series four digit version number introduced like ﬁrst digit indicates major version. typically indicates signiﬁcant coverage implementations major milestones improvements testing quality validation veriﬁcation justify major release. minor version smaller milestones achieved throughout development cycles. subjective minor major version bumps todo list appendix sets tentative milestones completed minor major versions. revision third digit typically applied stable releases number critical ﬁxes minor release it’s revision bumped. last digit represents minor revision code release. typically used throughout development releases make code available sooner testing. notion ﬁrst introduced count sort increments marf development included ﬁxes past increment chunk material. bump major minor versions revision resets minor revision back zero. ..-devel release series minor revisions publicly displayed dates particular release made. version programmatically queried validated against. version class introduced encapsulate versioning information validation used applications. marf.version class provides validate version application report mismatches convenience. reference details. also query full marf.jar marf-debug.jar release version four components displayed typing need java compiler. recent versions javac recommended. need least installed jdks lower longer supported additionally would need javadoc want build appropriate documentation .jar ﬁles. linux unix make required; make programs work. make often installed name gmake; systems make default tool name make. document always refer name make. make version running type make http//marf.sf.net download appropriate marf-<ver>.jar there. install downloaded .jar somewhere within reach classpath java extensions directory extdirs. voila since write mini mind-blowing apps based marf. also demo applications speakeridentapp exact site out. install platform-independent. marf several .jar ﬁles released. number increase based demand. diﬀerent jars contain various subsets marf’s code addition full complete code always released past. description currently released jars marf-<ver>.jar contains complete marf’s code excluding junit tests debug infor marf-util-<ver>.jar contains subset marf’s code corresponding primarily contents marf.util package optimized. binary release useful apps rely quite comprehensive general-purpose utility modules nothing else. quite small size. marf-storage-<ver>.jar contains subset marf’s code corresponding primarily contents marf.storage marf.util packages optimized. binary release useful apps rely general-purpose utility storage modules. marf-math-<ver>.jar contains subset marf’s code corresponding primarily contents marf.math marf.util packages optimized. binary release useful apps rely general-purpose utility math modules. marf-utilimathstor-<ver>.jar contains subset marf’s code corresponding primarily contents marf.util marf.math marf.stats marf.storage packages optimized. binary release useful apps rely general-purpose modules packages. grab latest tarball current pre-packaged -src release compile producing .jar need install described section marf sources obtained http//marf.sf.net. extract also used jbuilder version project marf.jpx directory. jbuilder project build marf.jar. also eclipse netbeans project ﬁles rooted .project .classpath eclipse import build.xml nbproject/*.* netbeans. otherwise stuck javac/java/jar command-line tools moment. follow pretty much steps unixen build above. might need hack makeﬁles corresponding environment variable support directory separator quoting instead typically marf really need much conﬁguration tweaking installed applications marf however able marf.jar either setting classpath environment variable point mention explicitly command line -classpath options. similarly commonly-used path variable classpath tells .class .jar ﬁles present standard known default directories. colon-separated linux/windows semicolon-separated windows/cygwin list directories .class ﬁles explicitly mentioned .jar .war .zip archives containing .class ﬁles. marf depend non-default classes java archives requires classpath junit.jar must somewhere classpath). however applications wishing marf point classpath unless default known places. reason decide marf’s jars servlet/jsp container tomcat capable setting classpath long shared/lib webapp/<your application here>/web-inf/lib directories. important note marf’s still stabilizing especially newer modules even older modules still aﬀected marf still ﬂexible that. thus changes fact happen every release insofar better this however also means serialized version serializable classes also change corresponding data needs retrained upgrade utility available. therefore please check versions changelog class revisions talk unsure aﬀect you. great deal eﬀort went versioning public class interface marf check running want test newly built marf deploy regression tests. regression tests test suite verify marf runs platform developers expected option manually tests located marf.junit package well executing test* applications described later applications chapter. regression testing application comprehensive testing grouping test applications well junit tests. application still development .... begin understand basic marf system architecture. understanding parts marf interact make follow sections somewhat clearer. document presents architecture marf system including layout physical directory structure java packages. let’s take look general marf structure figure marf class central server conﬁguration placeholder contains major methods core pipeline typical pattern recognition process. ﬁgure presents basic abstract modules architecture. developer needs module derive generic ones. core pipeline sequence diagram application result presented figure includes major participants well basic operations. participants modules responsible typical general pattern recognition pipeline. conceptual data-ﬂow diagram pipeline figure grey areas indicate stub modules implemented. application using framework choose concrete conﬁguration submodules preprocessing feature extraction classiﬁcation stages. application deﬁned module marf. also pipeline often assumes whole sample loaded anything instead sending parts sample time. perhaps simpliﬁes things won’t allow deal large samples moment. however it’s problem framework application samples small enough memory cheap. additionally streaming support already wavloader modules support ﬁnal conversion streaming happen yet. possible pass modulespeciﬁc arguments problems like number parameters mismatch feature extraction classiﬁcation tracked. also instance moduleparams exists marf limiting combination non-default feature extraction modules. section presents methods algorithms implemented used project. overview storage issues ﬁrst preprocessing methods followed feature extraction ended classiﬁcation. deﬁned standard storagemanager interface modules use. that’s part storagemanager interface module override module know serialize itself applications using marf care. thus storagemanager base class abstract methods dump restore. methods would generalize model’s serialization sense somehow read written. store data used training later classiﬁcation process. pass feature vectors trainingset/trainingsample class pair which result store mean vectors training models. neural network classiﬁcation using speakers. thought speaker would ideal we’ll lose much discrimination power. means need complete re-training training utterance mapping. record speakers speakers. neuralnetwork stochastic methods long neuralnetwork return proper number clusters stochastic module proper labeling. retraining speaker would involve phases appending features ﬁle/dir re-training models. classification modules aware scheme re-train data required. given feature vectors n-dimensional space want represent items make groupings vectors center point suen discussed iterative algorithm optimal groupings anyway don’t believe suen’s clustering stuﬀ useful know info training speaker associated feature vector create clusters information. neuralnetwork clusters regular training. stochastic clusters believe need represent gaussian curve mean vector co-variance matrix. created feature vectors speaker. again know optimal clustering business useless. marf.storage.trainingset.* represent training sets used training diﬀerent preprocessing feature extraction methods; either gzipped binary text diﬀerent methods diﬀerent feature vector sizes. depends kind precision desires. case result features array doubles corresponding frequency range. generic moduleparams container class created application able pass modulespeciﬁc parameters specifying model ﬁles training data amount coeﬃcients window size logging/stats ﬁles etc. classiﬁcation over result stored somehow retrieval application. deﬁned result object carry task. contains subject identiﬁed well additional statistics training testing samples recorded external sound recording program using standard microphone. sample saved properties stored appropriate folders would loaded within main application. audio format refers digital encoding audio sample contained format used ﬁles. encoding analog signal represented sequence amplitude values. range amplitude value given audio sample size represents number bits value consists case audio sample size -bit means value range since using pcm-signed format gives amplitude range amplitude values recorded sample vary within range. also sample size gives greater range thus provides better accuracy representing audio signal using sample size -bit limited range therefore -bit audio sample size used experiments order provide best possible results. sampling rate refers number amplitude values taken second audio digitization. according nyquist theorem rate must least twice maximum rate analog signal wish digitize otherwise signal cannot properly regenerated digitized form. since using sampling rate means actual analog frequency sample limited khz. however limitation pose hindrance since diﬀerence sound quality negligible number channels refers output sound experiment single channel format used avoid complexity sample loading process. read audio information saved voice sample special sample-loading component implemented order load sample internal data structure processing. this certain sound libraries provided java programming language enabled stream audio data sample ﬁle. however data captured converted readable amplitude values since library routines provide values function reads values sample stream byte array twice length audiodata; array hold converted amplitude values values read audiobuffer high order bytes make amplitude value extracted according type representation deﬁned sample’s audio format. data representation endian high order byte value located every even-numbered position audiobuffer. high order byte ﬁrst value found position second value third forth. similarly order byte value located every odd-numbered position words data representation endian bytes code read left right audiobuffer. data representation endian high order bytes inversed. high order byte ﬁrst value array position order byte position high order bytes properly extracted bytes merged form -bit double value. value scaled represent amplitude within unit range resulting value stored audiodata array passed calling routine available audio data entered array. additional routine also required write audio data array wave ﬁle. routine involved inverse reading audio data sample stream. speciﬁcally amplitude values inside array converted back codes stored inside array bytes following illustrates works decided stick mono-hz-bit ﬁles. -bit might okay could retain precision -bit ﬁles. supposed need contain frequencies vocal spectrum we’ll wasting space computation time. also thinking think made mistake downsampling hz... saying voice ranges that’s need samples realized sample actually represents would account diﬀerence noticed.. maybe using samples. hand even voice still perfectly distinguishable... tried waveloader samples provided stephen nice results graphed obtained getaudiodata function noticed quite diﬀerence graph obtained test.wav. test.wav getting unexpected results graph didn’t resemble wave form. lead believe needed convert data order represent wave form function). tested routine jimmy.wav beautiful wave-like graph data makes sense since represents amplitude values reason test.wav sample actually -bit mono rather -bit mono stephen’s samples. basically don’t need conversion -bit mono samples scrap getwaveform function. come wave class sometime week take care loading wave ﬁles windowing audio data. also read audio data actual sound meaning silence discarded sample extracting audio data. thinking loud basic pass-everything-through method doesn’t actually preprocessing. originally developed within framework meant base line method gives better results many conﬁgurations. method however fair others doesn’t normalization samples compared original data coming likewise silence noise removal done here. since voices recorded exactly level important normalize amplitude sample order ensure features comparable. audio normalization analogous image normalization. since samples loaded ﬂoating point values range ensured every sample actually cover entire range. vocal sample taken less-than-perfect environment experience certain amount room noise. since background noise exhibits certain frequency characteristic noise loud enough inhibit good recognition voice voice later tested diﬀerent environment. therefore necessary remove much environmental interference possible. remove room noise ﬁrst necessary sample room noise itself. sample usually least seconds long provide general frequency characteristics noise subjected analysis. using technique similar overlap-add ﬁltering room noise removed vocal sample simply subtracting noise’s frequency characteristics vocal sample question. silence removal implemented marf’s preprocessing.removesilence methods better results removesilence executed normalization default threshold works well cases. silence removal performed time domain amplitudes threshold discarded sample. also makes sample smaller less resemblent samples improving overall recognition performance. endpointing algorithm implemented marf follows. end-points mean local minimums maximums amplitude changes. variation whether consider sample edges continuous data points end-points. marf four cases considered end-points default option enable disable latter cases setters moduleparams facility. endpointing algorithm implemented endpoint marf.preprocessing.endpoint package appeared .... ﬁlter used modify frequency domain input sample order better measure distinct frequencies interested ﬁlters useful speech analysis high frequency boost low-pass ﬁlter speech tends fall rate octave therefore high frequencies boosted introduce precision analysis. speech still characteristic speaker high frequencies even though lower amplitude. ideally boost performed compression automatically boosts quieter sounds maintaining amplitude louder sounds. however simply done using positive value ﬁlter’s frequency response. low-pass ﬁlter used simpliﬁed noise reducer simply cutting frequencies certain point. human voice generate sounds maximum frequency test samples therefore since range ﬁlled noise better out. essentially ﬁlter implementation overlap-add method ﬁlter design process simple perform fast convolution converting input frequency domain manipulating frequencies according desired frequency response using inversefft convert back time domain. figure demonstrates normalized incoming wave form translated frequency domain. code applies square root hamming window input windows applies multiplies results desired frequency response applies inverse-fft applies square root hamming window again produce undistorted output. low-pass ﬁlter realized filter setting frequency response zero frequencies past certain threshold chosen heuristically based window size ﬁltered frequencies past figure realized filter fact opposite low-pass ﬁlter ﬁlters frequencies implementation high-pass ﬁlter found marf.preprocessing.fftfilter.highpassfilter. ﬁlter also implemented ﬁlter boost high-end frequencies. frequencies boosted approx. factor heuristically determined renormalized. figure implementation high-frequency boost preprocessor found marf.preprocessing.fftfilter.highfrequencyboost. experimentation said would useful test high-pass ﬁlter along high-frequency boost. immediate class this marf chains former latter addition preprocessing framework constructor preprocessing module takes another allowing preprocessing pipeline own. results experiment found consolidated results section. yield better recognition performance good see. tweaking trying required make ﬁnal decision approach issue re-normalization entire input instead boosted part. section outlines concrete implementations feature extraction methods marf project. first present structure followed description methods. class diagram module figure many techniques necessary consider smaller portion entire speech sample rather attempting process entire sample once. technique cutting sample smaller pieces considered individually called windowing. simplest kind window rectangle simply unmodiﬁed larger sample. unfortunately rectangular windows introduce errors near edges window potentially sudden drop high amplitude nothing produce false pops clicks analysis. better window sample slowly fade toward edges multiplying points window window function. take successive windows side side edges faded distort analysis sample modiﬁed window function. avoid this necessary overlap windows points sample considered equally. ideally avoid distortion overlapped window functions constant. exactly hamming window does. deﬁned fast fourier transform algorithm used feature extraction basis ﬁlter algorithm used preprocessing. although complete discussion algorithm beyond scope document short description implementation provided here. essentially optimized version discrete fourier transform. takes window size returns complex array coeﬃcients corresponding frequency curve. feature extraction magnitudes complex values used ﬁlter operates directly complex results. implementation involves steps first shuﬄing input positions binary reversion process combining results butterﬂy decimation time produce ﬁnal frequency coeﬃcients. ﬁrst step corresponds breaking time-domain sample size frequencydomain samples size second step re-combines samples size n-sized frequencydomain sample. frequency-domain view window time-domain sample gives frequency characteristics window. feature identiﬁcation frequency characteristics voice considered list features voice. combine windows vocal sample taking average them average frequency characteristics sample. subsequently average frequency characteristics samples speaker essentially ﬁnding center cluster speaker’s samples. speakers cluster centers recorded training speaker input sample identiﬁable comparing frequency analysis cluster center classiﬁcation method. characteristic worry window used input. using normal rectangular window result glitches frequency analysis sudden cutoﬀ high frequency distort results. therefore necessary apply hamming window input sample overlap windows half. since hamming window adds constant overlapped distortion introduced. comparing phonemes window size appropriate comparing whole words window size likely useful. larger window size produces higher resolution frequency analysis. method feature extraction used marf project linear predictive coding analysis. evaluates windowed sections input speech waveforms determines coeﬃcients approximating amplitude frequency function. approximation aims replicate results fast fourier transform store limited amount information valuable analysis speech. method based formation spectral shaping ﬁlter that applied input excitation source yields speech sample similar initial signal. excitation source assumed spectrum leaving useful information model shaping ﬁlter used implementation called all-pole model follows coeﬃcients ﬁnal representation speech waveform. obtain coeﬃcients least-square autocorrelation method used. method requires autocorrelation signal deﬁned coeﬃcients evaluated windowed iteration yielding vector coeﬃcient size coeﬃcients averaged across whole signal give mean coeﬃcient vector representing utterance. thus sized vector used training testing. value chosen based tests given speed accuracy. value around observed accurate computationally feasible. text doesn’t much detail gives techniques. seem involve another preprocessing remove high frequencies estimation postprocessing correction. another detailed source needed. min/max amplitudes extraction simply involves picking maximums minimums sample features. length sample less diﬀerence ﬁlled middle element sample. todo feature extraction perform well conﬁguration simplistic implementation sample amplitudes sorted minimums maximums picked ends array. samples usually large values group really close identical making hard classiﬁers properly discriminate subjects. future improvements include attempts pick values distinct enough features samples smaller increments diﬀerence smallest maximum largest minimum divided among missing elements middle instead value ﬁlling space method appeared marf .... class feature extraction instead allows concatenation results several actual feature extractors combined single result. give classiﬁcation modules discriminatory power featureextractionaggregator still implements featureextraction order used main pipeline marf. aggregator expects moduleparams enumeration constants module invoked followed module’s enclosed instance moduleparams. implementation enclosed instance moduleparams isn’t really used main limitation aggregator aggregated feature extractors default settings. happen pipeline re-designed include capability. aggregator clones incoming preprocessed sample feature extractor runs module separate thread. results tread collected order speciﬁed initial moduleparams returned concatenated feature vector. meta-information available needed. default given window size samples picks random number gaussian distribution multiplies incoming sample frequencies. adds feature vector end. bottom line performance feature extraction methods. also used relatively fast testing module. section outlines classiﬁcation methods marf project. first present overall structure followed description methods. overall structure modules figure chebyshev distance used along distance classiﬁers comparison. chebyshev distance also known city-block manhattan distance. here’s mathematical representation implementation marf.classification.distance.mahalanobisdistance depends marf.classification.distance.distance used test marf.marf speakeridentapp distance classiﬁcation meant able detect features tend vary together cluster linear transformations applied them becomes invariant transformations unlike other previously seen distance classiﬁers. release namely ..-devel covariance matrix identity matrix making mahalanobis distance euclidean one. need complete learning covariance matrix complete classiﬁer. serguei mokhov invented classiﬁer original idea based diff unix utility works. later performance enhancements modiﬁed. essence distance count input vector diﬀerent terms elements correspondence. chebyshev distance corresponding elements greater error distance accounted plus additional distance penalty added. factors vary depending desired conﬁguration. elements equal pretty close small bonus subtracted distance. method classiﬁcation used artiﬁcial neural network. network meant represent neuronal organization organisms. classiﬁcation method lies training network output certain value given particular input neuron consists inputs associated weights threshold activation function output value. output value propagate neurons case neuron part output layer network. relation inputs activation function follows output structure network used feed-forward neural network. implies neurons organized sets representing layers neuron layer inputs layer output layer only. structure facilitates evaluation training network. instance evaluation network input vector output neuron ﬁrst layer calculated followed second layer training feed-forward neural network done algorithm called back-propagation learning. based error ﬁnal result network. error propagated backward throughout network based amount neuron contributed error. deﬁned follows parameters used avoid local minima training optimization process. weight combination weight addition change. usual values determined experimentally. back-propagation training method used conjunction epoch training. given training input vectors back-propagation training done run. however weight vectors neuron vector stored used. inputs trained weights committed test input vectors mean error calculated. mean error determines whether continue epoch training not. classiﬁer neural network used feature vectors speaker identiﬁers. neurons input layer correspond feature feature vector. output network binary interpretation output layer. therefore neural network input layer size size feature vectors output layer size maximum speaker identiﬁer. network structure trained input vectors corresponding training samples speaker. network epoch trained optimize results. fully trained network used classiﬁcation recognition process. might sound strange random classiﬁer marf. less testing module quickly test pipeline. picks pseudo-random manner list trained subjects classiﬁcation. also serves bottom-line performance other slightly sophisticated classiﬁcation methods meaning performance aforementioned methods must better random; otherwise problem. even though section entitled don’t really much couple things marf.gui package occasionally eventually expand real classes. tiny package figure sometimes useful visualize data playing with. typical thing dealing sounds speciﬁcally voice people interested spectrograms frequency distributions. spectrogram class designed handle produce spectrograms algorithms simply draw them. manage make true component instead made dump spectrograms ppm-format image ﬁles looked using graphical package. examples spectrograms appendix taking output spectrogram. it’s supposed half took hamming window waveform intervals samples half intervals mean second half window ﬁrst half next. o’shaughnessy says good window. thus streaming waveform must consider this. spectrogram determination multiply signal window dofft dolpc coeﬃcient determination resulting array windowed samples. gave approximation stable signal course experiment windows better deﬁnitive best. wavegrapher another class designed name suggests draw wave form incoming/preprocessed signal. well doesn’t actually draw thing dumps sample points tab-delimited text loaded plotting software gnuplot excel. also produce graphs signal frequency domain instead time domain. examples graphs data obtained class preprocessing section female male speakers ranging college student university professor. table list people contributed voice samples project want thank helping out. main idea compare combinations diﬀerent methods variations within terms recognition rate performance. means several preprocessing modules several feature extraction modules several classiﬁcation modules possible combinations. purpose written speakeridentapp command-line application speaker identiﬁcation. every possible conﬁguration following shell script namely testing.sh script linux/unix environments. similar script windows testing.bat classiﬁcation retrain shortcut re-training classiﬁcation. completed development series. module used generate mean vector coeﬃcients utterance. used average fundamental frequency utterance. results concatenated form output vector particular order. classiﬁer would take account weighting features neural network would implicitly beneﬁts speaker matching stochastic modiﬁed give weight vice versa depending best increasing number samples results better; exceptions however. explained diversity recording equipment less uniform number samples speaker absence noise removal. samples recorded environments. results start averaging awhile. fun. interesting note also tried take samples music bands feed application along speakers application’s performance didn’t suﬀer even improved samples treated manner. groups mentioned table name here halen chili peppers samples used). ultimate results conﬁgurations samples we’ve below. looks like best results -endp -lpc -cheb -raw -aggr -eucl -norm -aggr -diﬀ -norm -aggr -cheb -raw -aggr -mah -raw -mah -raw -eucl -norm -diﬀ result around second-best around -endp -lpc -cheb -raw -aggr -eucl -norm -aggr -diﬀ -norm -aggr -cheb -raw -aggr -mah -raw -mah -raw -eucl -norm -diﬀ -norm -cheb -raw -aggr -cheb -endp -lpc -mah -endp -lpc -eucl -raw -mink -norm -mah -norm -eucl -norm -aggr -eucl -low -aggr -diﬀ -norm -aggr -mah -raw -cheb -raw -aggr -mink -norm -aggr -mink -low -cheb -raw -lpc -mink -raw -lpc -diﬀ -raw -lpc -eucl -raw -lpc -mah -raw -lpc -cheb -low -aggr -eucl -norm -lpc -mah -norm -lpc -mink -norm -lpc -diﬀ -norm -lpc -eucl -low -aggr -cheb -norm -lpc -cheb -low -aggr -mah -low -mah -norm -mink -low -diﬀ -low -eucl -raw -aggr -diﬀ -high -aggr -mink -high -aggr -eucl -endp -lpc -mink -high -aggr -mah -raw -diﬀ -high -aggr -cheb -low -aggr -mink -low -mink -high -cheb -high -mah -low -lpc -cheb -high -mink -high -eucl -low -lpc -eucl -low -lpc -mah -low -lpc -mink -low -lpc -diﬀ -high -lpc -cheb -raw -lpc -band -aggr -diﬀ -norm -lpc -band -diﬀ -high -lpc -eucl -high -aggr -diﬀ -endp -diﬀ -endp -eucl -band -lpc -mink -band -lpc -mah -band -lpc -eucl -endp -cheb -band -lpc -cheb -endp -mah -high -lpc -mah -endp -aggr -diﬀ -endp -aggr -eucl -endp -aggr -mah -endp -aggr -cheb -high -lpc -diﬀ -band -aggr -eucl -endp -mink -band -aggr -cheb -band -lpc -diﬀ -band -aggr -mah -band -mah -band -eucl -endp -aggr -mink -high -diﬀ -high -lpc -mink -raw -minmax -mink -raw -minmax -eucl -band -aggr -mink -raw -minmax -mah -endp -minmax -eucl -endp -minmax -cheb -endp -minmax -mah -band -cheb -raw -minmax -cheb -endp -lpc -endp -lpc -diﬀ -endp -minmax -mink -endp -randfe -diﬀ -endp -randfe -cheb -endp -minmax -diﬀ -endp -minmax -band -mink -norm -randfe -eucl -raw -minmax -diﬀ -endp -randfe -eucl -endp -randfe -mah -low -randfe -mink -norm -randfe -mah -norm -minmax -eucl -norm -minmax -cheb -low -minmax -mink -norm -minmax -mah -norm -randfe -mink -endp -randfe -mink -low -minmax -mah -low -randfe -eucl -high -minmax -mah -raw -randfe -mink -low -randfe -mah -low -minmax -diﬀ -high -minmax -diﬀ -high -minmax -eucl -low -minmax -eucl -low -minmax -cheb -norm -randfe -diﬀ -norm -randfe -cheb -high -randfe -mink -low -randfe -diﬀ -high -randfe -diﬀ -high -randfe -eucl -high -randfe -cheb -low -randfe -cheb -norm -minmax -mink -raw -randfe -eucl -band -lpc -raw -randfe -mah -high -minmax -mink -raw -minmax -high -randfe -mah -high -minmax -cheb -high -minmax -endp -randfe -randcl -band -minmax -mink -band -minmax -diﬀ -band -minmax -eucl -band -minmax -mah -raw -minmax -randcl -band -minmax -cheb -low -lpc -raw -randfe -diﬀ -norm -minmax -diﬀ -boost -lpc -randcl -raw -randfe -cheb -boost -minmax -highpassboost -lpc -norm -minmax -highpassboost -minmax -boost -minmax -randcl -boost -lpc -raw -aggr -randcl -band -randfe -mah -highpassboost -lpc -randcl -band -randfe -diﬀ -band -randfe -eucl -low -minmax -boost -randfe -randcl -band -randfe -cheb -raw -lpc -randcl -highpassboost -aggr -randcl -boost -randcl -highpassboost -minmax -diﬀ -boost -randfe -eucl -highpassboost -minmax -eucl -boost -lpc -mink -boost -lpc -diﬀ -boost -mah -boost -lpc -eucl -low -randcl -low -minmax -randcl -boost -minmax -mah -highpassboost -minmax -mah -boost -randfe -cheb -high -randfe -randcl -highpassboost -minmax -cheb -highpassboost -mah -boost -lpc -cheb -boost -aggr -mah -highpassboost -lpc -mink -highpassboost -lpc -diﬀ -endp -lpc -randcl -highpassboost -lpc -eucl -high -minmax -randcl -highpassboost -lpc -cheb -norm -randcl -band -aggr -randcl -low -randfe -randcl -boost -aggr -mink -boost -aggr -diﬀ -endp -aggr -randcl -boost -aggr -eucl -boost -mink -boost -randfe -mah -boost -diﬀ -boost -eucl -highpassboost -randfe -mink -highpassboost -randfe -diﬀ -boost -minmax -mink -boost -minmax -diﬀ -highpassboost -randfe -eucl -boost -minmax -eucl -low -aggr -randcl -band -randcl -boost -aggr -cheb -band -randfe -randcl -boost -cheb -highpassboost -aggr -mink -highpassboost -aggr -diﬀ -highpassboost -mink -endp -minmax -randcl -highpassboost -diﬀ -highpassboost -aggr -eucl -highpassboost -randfe -cheb -high -lpc -boost -minmax -cheb -highpassboost -eucl -boost -lpc -mah -norm -randfe -randcl -highpassboost -aggr -cheb -highpassboost -cheb -band -minmax -randcl -boost -aggr -randcl -highpassboost -lpc -mah -highpassboost -aggr -mah -high -lpc -randcl -highpassboost -randfe -mah -boost -randfe -mink -boost -randfe -diﬀ -highpassboost -minmax -mink -raw -randfe -randcl -highpassboost -randcl -band -lpc -randcl -endp -randcl -raw -randcl -norm -lpc -randcl -highpassboost -randfe -randcl -high -aggr -randcl -band -randfe -mink -low -lpc -randcl -highpassboost -minmax -randcl -norm -aggr -randcl -high -randcl -band -minmax -norm -minmax -randcl -endp -lpc -cheb -raw -aggr -eucl -norm -aggr -diﬀ -norm -aggr -cheb -raw -aggr -mah -raw -mah -raw -eucl -norm -diﬀ -norm -cheb -raw -aggr -cheb -endp -lpc -mah -endp -lpc -eucl -raw -mink -norm -mah -norm -eucl -norm -aggr -eucl -low -aggr -diﬀ -norm -aggr -mah -raw -cheb -raw -aggr -mink -norm -aggr -mink -low -cheb -raw -lpc -mink -raw -lpc -diﬀ -raw -lpc -eucl -raw -lpc -mah -raw -lpc -cheb -low -aggr -eucl -norm -lpc -mah -norm -lpc -mink -norm -lpc -diﬀ -norm -lpc -eucl -low -aggr -cheb -norm -lpc -cheb -low -aggr -mah -low -mah -norm -mink -low -diﬀ -low -eucl -raw -aggr -diﬀ -high -aggr -mink -high -aggr -eucl -endp -lpc -mink -high -aggr -mah -raw -diﬀ -high -aggr -cheb -low -aggr -mink -low -mink -high -cheb -high -mah -low -lpc -cheb -high -mink -high -eucl -low -lpc -eucl -low -lpc -mah -low -lpc -mink -low -lpc -diﬀ -high -lpc -cheb -raw -lpc -band -aggr -diﬀ -norm -lpc -band -diﬀ -high -lpc -eucl -high -aggr -diﬀ -endp -diﬀ -endp -eucl -band -lpc -mink -band -lpc -mah -band -lpc -eucl -endp -cheb -band -lpc -cheb -endp -mah -high -lpc -mah -endp -aggr -diﬀ -endp -aggr -eucl -endp -aggr -mah -endp -aggr -cheb -high -lpc -diﬀ -band -aggr -eucl -endp -mink -band -aggr -cheb -band -lpc -diﬀ -band -aggr -mah -band -mah -band -eucl -endp -aggr -mink -high -diﬀ -high -lpc -mink -raw -minmax -mink -raw -minmax -eucl -band -aggr -mink -raw -minmax -mah -endp -minmax -eucl -endp -minmax -cheb -endp -minmax -mah -band -cheb -raw -minmax -cheb -endp -lpc -endp -lpc -diﬀ -endp -minmax -mink -endp -randfe -diﬀ -endp -randfe -cheb -endp -minmax -diﬀ -endp -minmax -band -mink -norm -randfe -eucl -raw -minmax -diﬀ -endp -randfe -eucl -endp -randfe -mah -low -randfe -mink -norm -randfe -mah -norm -minmax -eucl -norm -minmax -cheb -low -minmax -mink -norm -minmax -mah -norm -randfe -mink -endp -randfe -mink -low -minmax -mah -low -randfe -eucl -high -minmax -mah -raw -randfe -mink -low -randfe -mah -low -minmax -diﬀ -high -minmax -diﬀ -high -minmax -eucl -low -minmax -eucl -low -minmax -cheb -norm -randfe -diﬀ -norm -randfe -cheb -high -randfe -mink -low -randfe -diﬀ -high -randfe -diﬀ -high -randfe -eucl -high -randfe -cheb -low -randfe -cheb -norm -minmax -mink -raw -randfe -eucl -band -lpc -raw -randfe -mah -high -minmax -mink -raw -minmax -high -randfe -mah -high -minmax -cheb -high -minmax -endp -randfe -randcl -band -minmax -mink -band -minmax -diﬀ -band -minmax -eucl -band -minmax -mah -raw -minmax -randcl -band -minmax -cheb -low -lpc -raw -randfe -diﬀ -norm -minmax -diﬀ -boost -lpc -randcl -raw -randfe -cheb -boost -minmax -highpassboost -lpc -norm -minmax -highpassboost -minmax -boost -minmax -randcl -boost -lpc -raw -aggr -randcl -band -randfe -mah -highpassboost -lpc -randcl -band -randfe -diﬀ -band -randfe -eucl -low -minmax -boost -randfe -randcl -band -randfe -cheb -raw -lpc -randcl -highpassboost -aggr -randcl -boost -randcl -highpassboost -minmax -diﬀ -boost -randfe -eucl -highpassboost -minmax -eucl -boost -lpc -mink -boost -lpc -diﬀ -boost -mah -boost -lpc -eucl -low -randcl -low -minmax -randcl -boost -minmax -mah -highpassboost -minmax -mah -boost -randfe -cheb -high -randfe -randcl -highpassboost -minmax -cheb -highpassboost -mah -boost -lpc -cheb -boost -aggr -mah -highpassboost -lpc -mink -highpassboost -lpc -diﬀ -endp -lpc -randcl -highpassboost -lpc -eucl -high -minmax -randcl -highpassboost -lpc -cheb -norm -randcl -band -aggr -randcl -low -randfe -randcl -boost -aggr -mink -boost -aggr -diﬀ -endp -aggr -randcl -boost -aggr -eucl -boost -mink -boost -randfe -mah -boost -diﬀ -boost -eucl -highpassboost -randfe -mink -highpassboost -randfe -diﬀ -boost -minmax -mink -boost -minmax -diﬀ -highpassboost -randfe -eucl -boost -minmax -eucl -low -aggr -randcl -band -randcl -boost -aggr -cheb -band -randfe -randcl -boost -cheb -highpassboost -aggr -mink -highpassboost -aggr -diﬀ -highpassboost -mink -endp -minmax -randcl -highpassboost -diﬀ -highpassboost -aggr -eucl -highpassboost -randfe -cheb -high -lpc -boost -minmax -cheb -highpassboost -eucl -boost -lpc -mah -norm -randfe -randcl -highpassboost -aggr -cheb -highpassboost -cheb -band -minmax -randcl -boost -aggr -randcl -highpassboost -lpc -mah -highpassboost -aggr -mah -high -lpc -randcl -highpassboost -randfe -mah -boost -randfe -mink -boost -randfe -diﬀ -highpassboost -minmax -mink -raw -randfe -randcl -highpassboost -randcl -band -lpc -randcl -endp -randcl -raw -randcl -norm -lpc -randcl -highpassboost -randfe -randcl -high -aggr -randcl -band -randfe -mink -low -lpc -randcl -highpassboost -minmax -randcl -norm -aggr -randcl chapter describes applications employ marf purpose another. applications demonstrate various marf’s features. others either research internal projects own. marf derivative application would like listed here please know marf-develsf.lists.net. .... data structures main statistics place-holder ostats hashtable nicely provided java hashes words keys data structures called wordstats. wordstats data structure contains word’s spelling called lexeme word’s frequency word’s rank wordstats provides typical variety methods access alter three values. array called osortedstatrefs eventually hold references wordstats objects ostats hashtable sorted frequency. .... system requirements program mostly developed linux there’s makefile testing shell script simplify routine tasks. bash would nice able batch script. since application written java it’s bound speciﬁc architecture thus compiled without makeﬁles scripts virtually operating system. .... using testing.sh script script written using bash syntax; hence bash present. script ensures program compiled ﬁrst invoking make loop feeds *.txt ﬁles along zipflaw.java program executable redirects standard output ﬁles <corpus-name>.log current directory. ﬁles contain grouping frequent words every words well frequency-of-frequency counts end. .... running zipflaw application application without wrapping scripts provide options command-line application associated yet. application compile ﬁrst. either make arguments compile standard java compiler. compiled thing jvm. required argument either corpus analyze --help. it’s corpus accompanied options overriding default settings. options application’s output ﬁlename isn’t speciﬁed stated usage instructions displayed. output ﬁlename generated input name options pre-pended ends extension .csv directly opened openoﬃce calc microsoft excel. .... default setup simplistic approach application everything letter blank discarded. words folded lower case well. default setup overridden specifying command-line options described above. failed prove zipf wrong. corpora settings. further log/log graphs frequency rank default extreme setup provided. graphs don’t change much shape. option combinations graphs provided since don’t vary much. turned capitalization sentence symbols numbers treated tokens don’t make much diﬀerence simply aren’t there. distribution archive *.log *.csv ﬁles test runs contain described statistics. welcome make clean re-run tests own. note default output goes standard output it’s good idea redirect especially corpus large one. zipf’s holds far. however experimentation required example punctuation characters ending sentence considered. languages english good area explore well. .... sophisticated statistical estimators work better good statistical estimators presented chapter give better results simpler ones. simpler mean add-* family. sophisticated mean witten-bell good-turing combination statistical estimators. english english language corpora biggest one. simplify training process combined en.txt. includes additionally chapters little prince. total size combined arabic compiled surah transliterated quran well couple texts transcribed michelle khalif´e proverb passage newspaper arabic total size ar.txt. c/c++ various .cpp ﬁles used variety projects examples comp system software design comp computer graphics comp comp system software courses. total size cpp.txt. perl perl used many serguei’s scripts written help marking assignments accept electronic submissions well couple tools written perl internet size perl.txt. .... bag-of-languages alphabets point alphabet document mean something people understand language alphabet. case alphabet include characters letters numbers punctuation even blank sometimes derived training corpus. initially attempted treat programming languages natural ones. developer’s standpoint deal uniformly. assumption could viewed cheating extent however programming languages larger alphabets necessary lexical parts language addition statements written using ascii letters. therefore gives discriminatory power compared characters inputted user. treating using ascii latin base lead confusion english keywords english words addition literal text strings comments present within code. among transcribed transliterated latin alphabetical diﬀerences. instance arabic three h-like sounds english equivalent sometimes numbers used purpose capitals used instead. fair others numerals part alphabet well. analogous problem existed russian bulgarian symbols bulgarian) represent certain letters sounds; hence always part alphabet. caused problems again thought another separation needed latin-based cyrillic-based semitic-based languages bag-of-languages approach might well. .... alphabet normalization diﬀerent alphabet sizes corpora language. alphabets derived corpora themselves depending size characters appear corpora might appear another. thus perform classiﬁcation task given sentence models compared diﬀerently-sized alphabets thereby returning probability n-gram’s characters present given trained model. viewed non-uniform smoothing models implies necessity normalization alphabets language models accounting n-grams made smooth. language model normalization implemented yet. normalization however provoke problem data sparseness similar described below. presents problem smoothing techniques counts either n-grams -grams values division become problem. .... n-grams implemented maximum general problem problem stems excessive data sparseness models taking example won’t able cope properly without special care two-dimensional matrix easily places division zero unavoidable. analogous problem exists good-turing smoothing. solve said make maybe created trouble design decision ﬁrst implementation. .... bag-of-languages language split came testing sentences/statements languages then based random observations conducted guided experiments. note sentences necessarily convey meaning information interest; testing purposes including esta noche salimos juntos fiesta. abolish microprocessor use. shla sasha shosse sosala sushku amour manques tellement beaucoup. buduchi chelovekom borodoi misha slegka posgatyvalsya posle burnoi p’yanki druz’yami quiero muchisimo troyer krekhtst harts burattino pinocchio chou habbibi gara nepoznato lampi mqtna svetlina class extends include <foo.h> cout makhn neshome farvundert public interface strict; wa-innaka laaaala khuluqin aaatheemin etim letom mimiletom lyudi edut kuda avant avoir capote ordi devenu vraiment ishtara jeha aacharat hamir assignment tomorrow n’est-ce pas? note sentences necessarily convey meaning information interest; testing purposes including esta noche salimos juntos fiesta. abolish microprocessor use. amour manques tellement beaucoup. quiero muchisimo burattino pinocchio avant avoir capote ordi devenu vraiment assignment tomorrow n’est-ce pas? shla sasha shosse sosala sushku buduchi chelovekom borodoi misha slegka posgatyvalsya posle burnoi p’yanki druz’yami troyer krekhtst harts chou habbibi gara nepoznato lampi mqtna svetlina makhn neshome farvundert wa-innaka laaaala khuluqin aaatheemin etim letom mimiletom lyudi edut kuda ishtara jeha aacharat hamir .... tokenization used types tokenizer restricted unrestricted experiment diverse alphabets. restricted tokenizer means lowercase-folded ascii characters numbers unrestricted tokenizer means additional characters allowed case-sensitive. tokenizers blanks discarded. implementation tokenizer settings command-line options still todo simply changing code recompiling. code unrestricted tokenizer detailed results observed appendix .... numbers recognition rate sentences presented earlier every language model. note numbers convey enough information look detailed results actually realize number samples debatable good training corpora uniform. might also want look languages confused other. witten-bell good-turing performed rather poorly tests general. think need larger corpora test witten-bell good-turing smoothing thoroughly. conﬁrmed results good-turing gave english english biggest corpus we’ve got. recognition programming languages according conducted experiments qualiﬁed so-so compared only. recognized slightly better bag-of-languages approach .... system requirements application primarily developed linux there’s makefile testing shell script simplify routine tasks. tcsh would nice able batch script. since application written java it’s bound speciﬁc architecture thus compiled without makeﬁles scripts virtually operating system. .... using shell scripts scripts training.sh testing.sh. former used batch training languages models latter perform batch-testing models. hide complexity typing many options users. ever them tweak appropriately speciﬁc languages n-gram models don’t all-or-nothing testing. scripts written using tcsh syntax; hence tcsh present. scripts ensure program compiled ﬁrst invoking make several loops feed options ﬁlenames application. .... running langidentapp application application without wrapping scripts provide options command-line application associated application compile ﬁrst. either make arguments compile standard java compiler. -char makes sure deal characters instead strings characters part n-gram statistical estimators none present won’t pick default language parameter thus language typically two-to-four letter abbrieviation language trying train test.langs bag-of-languages test.latin.langs english french spanish italian sentences test.non-latin.langs arabic hebrew russian bulgarian sentences test.pls.langs programming languages section describes implementation probabilistic parsing algorithm implemented java discusses experiments grammars used results diﬃculties encountered. .... manual source code mini user manual along instructions application provided section .... source code provided electronic form extracts presented document. .... grammar file format serguei developed grammar compiler compiler design course adapted accept probabilistic grammars. grammar compiler reads source grammar text compiles result successfully compiled grammar object terminals non-terminals rules stored binary ﬁle. parsers re-load compiled grammar main parsing supposed parse. grammar grammar format presented figure description elements below. example grammar rules figure whenever changes grammar recompiled take eﬀect. <lhs> single non-terminal left-hand side rule. rule operator separating rhs. probability ﬂoating-point number indicating rule’s probability. particular assignment i.e. either terminal non-terminals enclosed within angle brackets grammar rules terminated %eol acts semicolon c/c++ java. indicates stop processing current rule look next amount white space grammar elements doesn’t matter much. grammar also notion comments. grammar compiler accepts shell-like single line comments lines start well comments like eﬀect c++. algorithm’s data structures back arrays represented -dimensional array doubles vectors back-indices respectively double oparsematrix vector aoback marf/nlp/parsing/probabilisticparser.java. also vector words incoming sentence parsed vector owords. experimented three grammars basic extended realistic. description grammars obtained presented below. testing sentences initially based given grammar whether algorithm indeed parses syntactically valid sentences rejects rest. sentence augmented various sources finally original requirements attempted used source grammar. .... extended grammar basic grammar extended rules probability scores artiﬁcial adapted basic grammar book’s grammar recomputed approximately hand. extended grammar figure basic extended grammars used testing sentences presented figure number sentences never come rules initially intended parses output seen todo. .... realistic grammar without completed extended grammar jumped develop something realistic realistic grammar. since previous grammars quite artiﬁcial test realistic data came best grammar could sentences requirements sentences expanded figure grammar figure many rules form a->bc still truly correct probabilities corresponding paper reasons. .... grammar restrictions incoming sentences though case-sensitive required lower-case unless given word proper noun pronoun current system doesn’t deal punctuation either complex sentences commas semicolons punctuation characters parsed correctly learning probabilistic grammars major problem type grammars learn them. requires least decent tagger decent knowledge english grammar sitting computing probabilities rules manually. time-consuming unjustiﬁed enormous eﬀort documents relatively small size hence need automatic tools and/or pre-existing requires. problem human maintainer number rules grows becomes less obvious initial rule like there’s always possibility create undesired parses way. extended realistic grammars based grammar rules book exponential growth storage requirements time playing basic extended grammars didn’t much attention run-time aspect algorithm second less sentence figure became problem however came realistic grammar. run-time grammar jumped minutes average sentence figure rather discouraging problem stems algorithm’s complexity huge data sparseness large grammars. major reasons data sparseness problem requirement described above number non-terminals grows rapidly largely increasing dimensions back arrays causing number iteration parsing algorithm increase exponentially time spent there’s parse sentence data sparseness smoothing bigger grammar severe data sparseness arrays algorithm causing problem run-time storage requirements. smoothing techniques previously implemented langidentapp applied least zero-probabilities array believe smoothing might hurt rather improve parsing performance; haven’t tested claim better could smooth grammar rules’ probabilities. .... generalities algorithm indeed seem work accept syntactically-valid sentences rejecting ungrammatical ones. though exhaust possible grammatical ungrammatical sentences testing would require time. also disappointment respect running time increase grammar grows problems mentioned before. .... system requirements program mostly developed linux there’s makefile testing shell script simplify routine tasks. bash would nice able batch script. since application written java it’s bound speciﬁc architecture thus compiled without makeﬁles scripts virtually operating system. using shell script script testing.sh. simply compilation batch processing three grammars sets test sentences run. script written using bash syntax; hence bash present. running probabilisitcparsingapp application without wrapping scripts provide options command-line application associated yet. application compile ﬁrst. either make arguments compile standard java compiler. --train <grammar-file> compile grammar speciﬁed text ﬁle. ﬁrst thing need trying parse sentences. compiled don’t need recompile time parser unless made fresh copy application made changes grammar plan grammar another ﬁle. entire source code provided electronic form provide methods extracted marf/nlp/parsing/probabilisticparser.java actual implementation algorithm build_tree function named parse dumpparsetree respectively. code experienced minor clean-ups report version debug error handling information removed. unaltered code please refer mentioned itself. testfilters testing applications marf. tests four types fft-ﬁlter-based preprocessors work simulated real wave type sound samples. user’s point view testfilters provides usage preprocessors loaders command-line options. entering --help arguments usage information displayed. explains arguments used testfilters. ﬁrst argument type preprocessor/ﬁlter. high-pass ﬁlter low-pass ﬁlter band-pass ﬁlter high frequency boost preprocessor chosen. next argument type loader load initial testing sample. testfilters uses types loaders sineloader wavloader. users enter either --sine --wave second argument specify desired loader. argument --sine uses sineloader generate plain sine wave selected preprocessor. --wave argument uses wavloader load real wave sound sample. latter case users need input third argument sample format feed wavloader. selecting necessary arguments user output testfilters within seconds. main marf module enumerates preprocessing modules high frequency boost filter bandpass filter pass filter high pass filter incoming sample format sine. optionprocessor helps maintaining validating command-line options. sample maintains incoming processed sample data. sampleloader provides sample loading interface marf loaders. must overridden concrete sample loader sineloader wavloader. preprocessing general preprocessing preprocess normalize removesilence removenoise application former three used. modules work together test work ﬁlters produce output stdout. output testfilters ﬁltered data original signal preprocessors. provides users programmers internal information eﬀect marf preprocessors compared expected output expected folder detect errors underlying algorithm changed. testlpc application targets unit testing preprocessing module. number options also allows choosing implemented loaders wavloader sineloader. faciliate option processing marf.util.optionprocessor used provides ability maintaining validating valid/active option sets. application also utilizes dummy preprocessing module perform normalization incoming sample. user’s point view testfft provides usage loaders command-line options. entering --help even arguments usage information displayed. explains arguments used testfft. another argument type loader. testfft uses types loaders sineloader wavloader. users enter --sine --wave second argument. argument --sine uses sineloader generate plain sine wave generic preprocessor. argument --wave speciﬁed application uses wavloader load wave sample ﬁle. latter case users need supply argument sample format *.wav loaded wavloader. selecting necessary arguments user output testfft within seconds. complete usage information main marf module enumerates incoming sample format sine. optionprocessor helps maintain validate command-line options. sample maintains processed incoming sample data. sampleloader provides sample loading interface marf loaders. must overridden concrete sample loader sineloader wavloader. preprocessing general preprocessing preprocess normalize removesilence removenoise application former three used. then processed data serve parameter featureextraction.fft.fft extract sample’s features. modules work together test work algorithm produce output stdout. know output testfft extracts features data feature extraction algorithm. gives users programmers direct information eﬀect marf feature extraction compared expected output expected folder detect errors underlying algorithm changed. user’s point view testwaveloader provides usage input wave sample output wave sample output textual command-line options. entering --help arguments usage information displayed. explains arguments used testwaveloader. ﬁrst argument input wave sample whose name mandatory argument. second third arguments output wave sample output textual sample loaded input sample. names output ﬁles optional arguments. user provide last arguments output ﬁles saved ﬁles provided testwaveloader. application made exercise following marf modules. main module wavloader marf.storage.loaders package. sample sampleloader modules marf.storage help wavloader prepare loading wave ﬁles. sample maintains processes incoming sample data. sampleloader provides sample loading interface marf loaders. must overridden concrete sample loader. loading wave samples sampleloader needs wavloader implementation. three modules work together load write back wave sample whose name provided users ﬁrst argument save loaded sample newly-named wave save loaded input data data referenced odatfile output sample’s duration size stdout. know output testwaveloader saves loaded wave sample newly named output wave ﬁle. output also saves input data textual ﬁle. testwaveloader gives users programmers direct information results marf wave loader. output sample compared expected output expected folder detect errors. user’s point view testloaders provides usage loader types input sample output sample output textual command-line options. entering --help arguments usage information displayed. entering --version testloaders’ version marf’s version displayed. usage info explains arguments used testloaders. ﬁrst argument type loader mandatory argument. second argument input name mandatory loaders except sineloader. third forth arguments output wave sample output textual sample names loaded input sample. names output ﬁles optional arguments. user provide last arguments output ﬁles saved names derived original. application made exercise following marf modules. main modules testing marf.storage.loaders package. optionprocessor module marf.util helps handling diﬀerent loader types according users input argument. sample sampleloader modules marf.storage help wavloader prepare loading input ﬁles. sample maintains processes incoming sample data. sampleloader provides sample loading interface marf loaders. must overridden concrete sample loader. modules work together load write back sample save loaded sample save loaded input data data referenced odatfile output sample’s duration size stdout. loading sine samples needs sineloader implementation instead saving data saves referenced odatfile. output testloaders saves loaded wave sample newly named output wave ﬁle. output also saves input data textual ﬁle. testloaders gives users programmers direct information results marf loaders. input sample compared expected output expected folder detect errors. mathtestapp application targets testing math-related implementations marf.math package. writing application mainy exercises matrix class used mahalanobisdistance classiﬁer. also necessary testing vector class well. running tests application validates marf version produces output stdout various linear matrix operations inversion identity multiplication transposal scaling rotation translation shear. stored output expected folder math.out contains expected output authors believe correct used regression testing. ifeatureextraction iclassification along marf itself. interfaces implemented variety ways. sample loader nearly identical functionality wavloader except yanks checks sample format making unrestricted random preprocessing used multiply incoming amplitudes pseudo-random gausian distribution. feature extraction performed convering incoming variable-length input chunks ﬁxed size elements added pairwise. finally classiﬁcation summing logical hashing applying modulus number speakers have. actual samples either real wave samples generated sine wave used. application made exercise marf’s implementation artiﬁcial neural network algorithm located marf.classification.neuralnetwork package. additionally following marf modules utilized gipsy stands general intensional programming system distributed research development investigation platform intensional hybrid programming languages. gipsy world developed concordia university information system please gipsy makes variety marf’s utility storage modules writing includes basethread threading part gipsy multithreaded compilation execution. expandedthreadgroup group number compiler executor threads group control debug well debugging logger compiler executor activities storagemanager binary serialization optionprocessor option processing core gipsy utilities arrays common arrays functionality private little application developed engineering computer science faculty concordia university’s faculty information systems team. application synchronize inventory data palm device database relational database powering online inventory application. application primarily exercises basethread threading part gipsy multithreaded compilation execution. expandedthreadgroup group number compiler executor threads group control main reasons recognition rate could non-uniform sample taking lack preprocessing techniques optimizations parameter tweaking lack noise removal lack incomplete sophisticated classiﬁcation modules lack samples train test welcome contribute areas make framework better. even though commercial university-level research standards recognition rate considered opposed required minimum above think still reasonably well demonstrate framework operation. sergej timofeevich aksakov. zapiski ruzhejnogo ohotnika orenburgskoj gubernii. moskva izdatel’stvo pravda http//lib.ru/lat/litra/aksakow/rasskazy_ohotnika. txt. peter grogono. latexe gallimaufry. techniques tips traps. department computer science software engineering concordia university montreal canada march http//www.cse.concordia.ca/~grogono/writings/gallimaufry.pdf last viewed serguei mokhov. towards hybrid intensional programming jlucid objective lucid general imperative compiler framework gipsy. master’s thesis department computer science software engineering concordia university montreal canada october isbn david probst. united states needs scalable shared-memory multiprocessor might white paper. concordia university montreal canada http//www.cs.concordia.ca/~grynbaum/. jean-jacques rousseau; translated conyngham mallory. confessions jean-jacques rousseau. http//www.knuten.liu.se/~bjoch/works/ rousseau/confessions.txt. gipsy research development group. general intensional programming system project. department computer science software engineering concordia university montreal canada http//newton.cs.concordia.ca/~gipsy/ last viewed april documentation html format found documentation distribution latest version please consult <http//marf.sourceforge.net/api/>. want participate development developers version <http//marf.sourceforge.net/api-dev/> includes private constructs docs well. adddelta addone arrays bandpass filter bandpassfilter basethread byteutils classiﬁcation debug dofft dolpc dummy endpoint expandedthreadgroup featureextraction featureextraction.fft.fft featureextractionaggregator goodturing grammar hashtable high frequency boost filter high pass filter highfrequencyboost highpassfilter iclassiﬁcation ifeatureextraction ipreprocessing isampleloader langidentapp logger pass filter lowpassfilter mahalanobisdistance marf marf-debug.jar marf.classiﬁcation.distance.diﬀdistance marf.classiﬁcation.distance.distance marf.classiﬁcation.distance.mahalanobisdistance marf.featureextraction.fft marf.gui marf.jar marf.junit marf.marf marf.marf.major version marf.marf.minor revision marf.marf.minor version marf.marf.revision marf.math marf.math.algortithms.hamming marf.nlp marf.preprocessing.dummy marf.preprocessing.dummy.dummy marf.preprocessing.dummy.raw marf.preprocessing.endpoint marf.preprocessing.fftfilter.* marf.preprocessing.fftfilter.highfrequencyboost marf.preprocessing.fftfilter.highpassfilter marf.preprocessing.preprocessing marf.stats marf.storage marf.storage.loaders marf.storage.sample marf.storage.sampleloader marf.storage.wavloader marf.util marf.util.arrays marf.util.debug marf.util.optionprocessor marf.version marf.version.major version marf.version.minor revision marf.version.minor version marf.version.revision mathtestapp matrix moduleparams neuralnetwork normalize optionprocessor osortedstatrefs ostats parse preprocess preprocessing preprocessing.removesilence probabilisticparsingapp recognize regression removenoise removesilence sample sampleloader serializable sine speakersidentdb spectrogram stochastic storage storagemanager test testfft testfilters testloaders testlpc testnn testplugin testwaveloader train vector version wavegrapher wavloader wittenbell wordstats zipflaw applications probabilistic parsing encsassets encsassetscron encsassetsdesktop gipsy language identiﬁcation mathtestapp speakeridentapp testfft testfilters testloaders testlpc testnn ../../api ../../api-dev .class .classpath .cpp .csv .jar .project .war .zip /usr/lib/marf /.emacs /.vimrc ar.txt bg.txt build.xml changelog cpp.txt data/asmt-sentences.txt data/test-sentences.txt dumpparsetree en.txt es.txt expected expected/ expected/sine.out fr.txt gmake graham.wav grammar.asmt.txt grammars/grammar.asmt.txt grammars/grammar.extended.txt grammars/grammar.original.txt grfst.txt he.txt hwswc.txt index/ install it.txt java.txt junit.jar makeﬁle marf*.jar marf-<ver> marf-<ver>.jar marf-debug*.jar marf-debug-<ver>.jar marf-debug-*.jar marf-math-<ver>.jar marf-storage-<ver>.jar marf-util-<ver>.jar marf-utilimathstor-<ver>.jar marf.jar marf.jpx marf/nlp marf/nlp/ marf/nlp/parsing/ marf/nlp/parsing/grammarcompiler/ marf/nlp/parsing/grammarcompiler/nonterminal.java marf/nlp/util math.out multiprocessor.txt nbproject/*.* nlpstreamtokenizer.java perl.txt ru.txt shared/lib test* test.*.langs test.langs test.latin.langs application point view applications architecture authors authors emeritus coding conventions contact contributors copyright core pipeline current limitaions current maintainers external applications history installation introduction packages project location purpose research applications source code source code formatting testing testing applications versioning methodology", "year": 2009}