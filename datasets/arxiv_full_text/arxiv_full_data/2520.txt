{"title": "Semi-described and semi-supervised learning with Gaussian processes", "tag": ["stat.ML", "cs.AI", "cs.LG", "math.PR", "60G15, 58E30", "G.3; I.2.6"], "abstract": "Propagating input uncertainty through non-linear Gaussian process (GP) mappings is intractable. This hinders the task of training GPs using uncertain and partially observed inputs. In this paper we refer to this task as \"semi-described learning\". We then introduce a GP framework that solves both, the semi-described and the semi-supervised learning problems (where missing values occur in the outputs). Auto-regressive state space simulation is also recognised as a special case of semi-described learning. To achieve our goal we develop variational methods for handling semi-described inputs in GPs, and couple them with algorithms that allow for imputing the missing values while treating the uncertainty in a principled, Bayesian manner. Extensive experiments on simulated and real-world data study the problems of iterative forecasting and regression/classification with missing values. The results suggest that the principled propagation of uncertainty stemming from our framework can significantly improve performance in these tasks.", "text": "propagating input uncertainty non-linear gaussian process mappings intractable. hinders task training using uncertain partially observed inputs. paper refer task semi-described learning. introduce framework solves both semi-described semi-supervised learning problems auto-regressive state space simulation also recognised special case semi-described learning. achieve goal develop variational methods handling semi-described inputs couple algorithms allow imputing missing values treating uncertainty principled bayesian manner. extensive experiments simulated real-world data study problems iterative forecasting regression/classiﬁcation missing values. results suggest principled propagation uncertainty stemming framework signiﬁcantly improve performance tasks. many real-world applications missing values occur data example measurements come unreliable sensors. correctly accounting partially observed instances important order exploit available information increase strength inference model. focus paper gaussian process models allow bayesian non-parametric inference. missing values occur outputs corresponding learning task known semi-supervised learning. example consider task learning classify images labelled much smaller total set. bootstrapping potential solution problem according model trained fully observed data imputes missing outputs. previous work semi-supervised learning involved cluster assumption classiﬁcation. consider approach uses manifold assumption assumes observed complex data really generated compressed less-noisy latent space. often encountered missing data problem unobserved input features statistics popular approach impute missing inputs using combination different educated guesses machine learning ghahramani jordan learn joint density input output data integrate missing values. gaussian process models missing input case received little attention challenge propagating input uncertainty non-linear mapping. paper introduce notion semidescribed learning generalise scenario. speciﬁcally deﬁne semi-described learning task learning inputs missing uncertain values. approach dealing missing inputs semi-described learning algorithmically closer data imputation methods. however contrast past approaches missing values imputed fully probabilistic manner considering explicit distributions input space. paper develop general framework solves semi-supervised semi-described learning. also consider related forecasting regression problem seen pipeline predictions obtained iteratively auto-regressive manner propagating uncertainty across predictive sequence here cast auto-regressive learning particular type semi-described learning. seek solve tasks within single coherent framework preserves fully bayesian property methodology. outputs non-linear model. this build variational approach titsias lawrence allows approximately propagating densities throughout nodes gp-based directed graphical models. resulting representation particularly advantageous whole input domain coherently associated posterior distributions. sample input space principled manner populate small initial labelled sets semi-supervised learning scenarios. avoid heuristic selftraining methods rely bootstrapping present problems over-conﬁdence. previously suggested approaches modelling input uncertainty also lack feature considering explicit input distribution training test instances. speciﬁcally consider case input uncertainty test time. propagating test input uncertainty non-linear results non-gaussian predictive density girard qui˜nonero-candela qui˜nonero-candela rely moment matching obtain predictive mean covariance. hand oakley o’hagan derive analytic expressions rather develop scheme based simulations. mchutchon rasmussen rely local approximations inside latent mapping function rather modelling approximate posterior densities directly. dallaire propagate uncertainty inputs mapping rather amend kernel computations account input uncertainty. seen special case developed framework data imputation performed using standard gp-lvm another advantage framework allows consider different levels input uncertainty point dimension without principle increasing danger under/overﬁtting since input uncertainty modelled variational rather model parameters. second methodological tool needed achieve goals need incorporate partial uncertain observations variational framework. this develop variational constraint mechanism constrains distribution input space given observed noisy values. approach fast whole framework incorporated parallel inference algorithm contrast damianou consider separate process modelling input distribution. however approach cannot easily extended data imputation purposes concern since cannot consider different uncertainty levels input dimension additionally computation scales cubicly number datapoints even within sparse frameworks. constraints framework propose interesting inference tool also modelling approach inputs constrained outputs obtain bayesian version back-constraints framework lawrence qui˜nonero candela however contrast approaches constraint deﬁned variational operates upon distribution rather single points. also follow idea constraining posterior distribution rich side information albeit completely different application. contrast osborne roberts handle partially missing sensor inputs modelling correlations input space special covariance functions. thirdly variational methods developed need encapsulated algorithms perform data imputation correctly accounting introduced uncertainty. develop algorithms showing considered applications cast learning pipelines rely correct propagation uncertainty stage. summary contributions paper following; ﬁrstly building bayesian gp-lvm developing variational constraint mechanism demonstrate uncertain inputs explicitly represented distributions training test time. secondly couple variational methodology algorithms allow solve problems associated partial uncertain observations semi-supervised learning auto-regressive iterative forecasting ﬁnally newly studied type learning refer semi-described learning. solve applications within single framework allowing handling uncertainty semi-supervised semidescribed problems coherent way. software accompanying paper found http//git.io/atn. paper extends previous workshop paper assume dataset input–output pairs stored rows matrices respectively. throughpaper denote rows matrices columns single elements denoted double subscript. ﬁrst outline standard formulation variables fully observed. assuming outputs corrupted zero-mean gaussian noise denoted obtain following generative model treated using local approximations. however paper consider explicit input distribution. achieve treat unobserved true inputs latent variables estimated marginal likeformulation corresponds graphical model figure however approach needs additionally estimate noise parameters might challenging given large number interplay variational noise parameters {si}n therefore considered alternative solution found result better performance. figure incorporating uncertain inputs intermediate input space considering gaussian prior centered variational constraint approximate posterior. figure represents two-stage approach dealing missing outputs classiﬁcation dotted line represents discriminative mapping. alternative relating true noisy inputs obtained focusing posterior rather prior distribution. start with re-express variational lower bound equation lower bound becomes exact variational distribution matches true posterior distribution noise-free latent inputs given observed inputs outputs. allow approximation introduce simple variational constraint operates factorised distribution written simplest case highlight dependency spectrum gp-lvm inputs fully unobserved corresponds unsupervised setting. absence observed inputs likelihood takes form equation inputs need recovered outputs maximum likelihood. bayesian gp-lvm proceeds additionally placing gaussian prior latent space approximately integrating constructing variational lower bound denotes expectation respect since appears non-linearly inside ﬁrst term variational bound intractable. however follow approximate intractable expectation analytically. paper wish deﬁne general framework operates whole range aforementioned extrema i.e. fully observed fully unobserved inputs case. ﬁrst step obtaining framework allow uncertainty inputs. assume inputs observed directly rather access noisy versions {zi}n n×q. relationship noisy true inputs given assuming gaussian noise obviously distribution collapses delta function recover standard case given recover gp-lvm. problem modelling assumption equation cannot equation since inputs available. hand replace equation effectively ignore input noise. mchutchon rasmussen proceed combining equations obtain mapping fji) inputs observed uncertain constraint consists replacing variational means factor corresponding observed input variational parameters account uncertainty. similarly back-constraint lawrence qui˜nonero candela variational constraint constitute probabilistic mapping. however allows encode input noise directly approximate posterior without specify additional noise parameters sacriﬁce scalability. next elaborate exact form constraint. general case namely inputs partially observed deﬁne similar constraint speciﬁes variational distribution gaussian dirac delta distributions. notationally consider data split fully partially observed subsets e.g. denote fully partially observed sets respectively. features missing appear different dimension individual point notational clarity index rows containing least missing dimension. case variational distribution constrained form corresponding distributions approximate dirac delta. notice partially observed still replace observed dimension corresponding observation second factors equation i.e. given above well spherical gaussian prior required intractable density approximated variational lower bound clarity dropped dependency expressions. since dirac functions approximated sharply peaked gaussians inside posterior variational bound computed manner bayesian gp-lvm bound equation speciﬁcally term tractable since involves gaussians. ﬁrst term equation follow bayesian gp-lvm methodology augment probability space extra samples {ui}m latent function evaluated pseudo-inputs m×q. consistency gaussian distribution. omit dependence expressions. likelihood becomes ppp. term equation approxmated ˆfq. however approximation still intractable since problematic term still appears inside contains inverse covariance matrix thus rendering expectation intractable. trick titsias lawrence deﬁne variational distribution form augmentation trick decouples latent function values given inducing points uncertainty inputs propagated nested integral. operation inducing outputs marginalised out. therefore integral analytically tractable since nested integral tractable results gaussian longer appears inverse covariance matrix. ﬁnal lower bound objective function thus obtained using partial bound place ﬁrst term equation thus obtaining ﬁnal bound summarise variational methodology seeks approximate true posterior variational distribution qqq. achieve this constrained take exact form term eliminated giving tractability effect re-introduced variational distribution contrast variational constraint approximate posterior factor constrained according effect considered comparison gives insight conceptual similarity variational approach followed obtain tractability followed handling partially observed inputs. variationally constrained model shown total parameters optimised objective function equation model parameters denotes hyper-parameters covariance function variational parameters optimally eliminated appendix). depending application corresponding learning algorithm certain dimensions treated observed. algorithms discussed following sections. formulate semi-described semi-supervised learning particular instances learning mapping function inputs associated uncertainty. cases devise two-step strategy based uncertain inputs framework allows efﬁciently take account partial information given datasets improve predictive performance. brevity refer framework described previous section variationally constrained semi-described auto-regressive semi-supervised approach obtained special cases given algorithms explained section. assume observed outputs correspond fully observed inputs partially observed inputs make correspondence clearer also split observed outputs according sets note output sets fully observed. interested learning regression function using available information. since variationally constrained inputs replaced distributions uncertainty taken account naturally variational distribution. context formulate data imputation-based approach inspired self-training methods; nevertheless principled handling uncertainty. speciﬁcally algorithm stages; ﬁrst step fully observed data subset train initial variationally constrained model encapsulates sharply peaked variational distribution given equation given model estimate predictive posterior missing locations essentially replace missing locations variational means variances predictive mean variance obtained self-training step. selection constitutes nevertheless initialisation. next step parameters optimised together fully observed data. speciﬁcally initializing explained step proceed train variationally constrained model full training contains fully partially observed inputs. algorithm outlines approach detail. formulation deﬁnes semi-described approach naturally incorporates fully partially observed examples communicating uncertainty throughout relevant parts model principled way. indeed predictive uncertainty obtained imputing missing values ﬁrst step pipeline incorporated input uncertainty second step pipeline. extreme cases resulting non-conﬁdent predictions example presence outliers corresponding locations simply ignored automatically large uncertainty. mechanism together subsequent optimisation parameters stage guards reinforcing predictions imputing missing values based smaller training set. model includes regression gp-lvm special cases. particular limit observed values semidescribed equivalent gp-lvm missing values equivalent regression. similarities traditional self-training straightforward mechanisms propagate uncertainty domain typically rely boot-strapping followed thresholding samples prevent model over-conﬁdence. framework predictions made initial model constitute initialisations later optimised along model parameters hence refer step partial self-training. further predictive uncertainty used hard measure discarding unconﬁdent predictions; instead allow values contribute according optimised uncertainty measure input variances therefore uncertainty handled makes self-training part algorithm principled compared many bootstrap-based approaches. considered simulated real-world data demonstrate semi-described algorithm. simulated data created sampling inputs giving samples input another unknown obtain corresponding outputs real-world data demonstration considered motion capture dataset taken subject motion capture database. selected subset walk motions human body represented joint locations. formulated regression problem ﬁrst dimensions original data used targets remaining algorithm semi-described learning uncertain input gps. given fully partially observed inputs respectively corresponding fully observed outputs inputs. given partial joint representation human body task infer rest representation. datasets simulated motion capture selected portion training inputs denoted randomly missing features. extended dataset used train method referred semi-described multiple linear regression regression performing nearest neighbour search test training instances observed input locations performing data imputation using standard gp-lvm. taking account predictive uncertainty imputation found catastrophic results simulated data training robust predictions. therefore gp-lvm variant considered real data experiment. also considered standard cannot handle missing inputs straightforwardly trained observed data goal reconstruct test outputs given fully observed test inputs simulated data used following sizes |zo| |zu| |z∗| dimensionality inputs outputs motion capture data used |zo| |zu| |z∗| plot obtained competing methods varying percentage offv missing features simulated data experiment points plot average runs considered different random seeds. clarity y−axis limit ﬁxed ﬁgure methods produced huge errors. full picture ﬁgure seen ﬁgures semi-described able handle extra data make much better predictions even large portion missing. indeed performance starts converge standard missing values performs identically standard values missing. found large compared data imputation step problematic percentage missing features approaches i.e. method reliant covariates available. appendix discusses behaviour systematic investigation left future work. method implicitly models uncertainty inputs also allows predictions autoregressive manner propagating uncertainty predictive sequence speciﬁcally assuming given data constitute multivariate timeseries observed time vector equally spaced given time-window length reformat input-output collections pairs follows ﬁrst input model given stacked vector ﬁrst output given similarly data that perform extrapolation ﬁrst train model modiﬁed dataset referring semidescribed formulation described section assign training inputs observed training perform iterative prediction future sequence where similarly approach taken girard predictive variance figure predictions obtained different methods semi-described learning. cannot handle partial observations thus uncertainty constant; clarity errorbar plotted separately right dashed vertical line results simulated data obtained trials. clarity limits y−axis ﬁxed errors become certain methods chart. errorbars gplvm-based approach also large plotted. full picture given ﬁgure step accounted propagated subsequent predictions. algorithm makes iterative -step predictions future; initially output predicted predictive variance ˆs∗;. next step observations augmented include distribution predictions simulation process seen constructing predictive sequence step step i.e. newly inserted input points constitute parts predictive sequence training points. therefore procedure seen iterative version semi-described learning. note straightforward extend model applying auto-regressive mechanism latent space stacked model generally deep additionally introducing functions latent space nonlinearly observation space obtain fully nonlinear state space model manner deisenroth model uncertainty encoded states nonlinear transition functions. correct propagation uncertainty vital well calibrated models future system behavior automatic determination structure model informative describing order underlying dynamical system. demonstrate framework simulation state space model. consider mackey-glass chaotic time series standard benchmark also considered girard data one-dimensional timeseries represented pairs values simulates process obviously generating process non-linear rendering dataset challenging. trained autoregressive model data series modiﬁed dataset created used ﬁrst points train model predicted subsequent points iterative free simulation. compared method naive autoregressive model input-output pairs given autoregressive modiﬁcation dataset ˆy}. model predictions made iteratively predicted values predictive step added observation set. however standard model straight forward incorporating/propagating uncertainty therefore input uncertainty zero every step iterative predictions. also compared method girard denoted plots gpuncert. figure shows results last steps full free simulation ﬁgure gives complete picture. seen variances plot method gpuncert robust handling uncertainty throughout predictions; naive method underestimates uncertainty. consequently seen ﬁgure ﬁrst predictions methods give answer. however predictions naive method diverge little true values error carried ampliﬁed underestimating uncertainty. hand gpuncert perhaps overestimates uncertainty therefore conservative predictions resulting higher errors. quantiﬁcation error shown table implemented basic moment matching approach although original paper authors additional approximations namely taylor expansion around predictive moments. figure chaotic timeseries forecasting steps ahead iterative prediction. ﬁrst steps shown here ﬁgure gives complete picture. comparing naive autoregressive propagate uncertainties; method girard referred gpuncert; approach closely tracks true test sequence last steps extrapolation. comparative depiction predictions split plots left center. rightmost plot shows predictive uncertainties x−axis prediction step y−axis function value section study semi-supervised learning which contrast semi-described learning handling missing values outputs. scenario typically encountered classiﬁcation settings. therefore introduce sets index respectively labelled missing rows outputs accordingly full dataset split fully observed. task devise method improves classiﬁcation performance using labelled unlabelled data. inspired kingma deﬁne semisupervised framework features extracted available information subsequently given inputs discriminative classiﬁer. speciﬁcally using whole input space learn low-dimensional latent space approximate posterior obviously speciﬁc case input space uncertain totally unobserved reduces bayesian gp-lvm model. notice posterior longer constrained rather directly approximates since forward probabilistic mapping treated random variable gaussian distribution i.e. exactly setting used gplvm. since one-to-one correspondence notationally write further since assume factorised across datapoints write second step semi-supervised algorithm train discriminative classiﬁer observed labelled space main idea that including inputs ﬁrst learning step manage deﬁne better latent embedding extract useful features discriminative classiﬁer. notice would ideally input discriminative classiﬁer whole distribution rather single point estimates. therefore wish take advantage associated uncertainty; speciﬁcally populate labelled sampling distribution example latent point corresponds input-output pair sample assigned label inference steps described graphically depicted figure exactly setting suggested kingma wish investigate applicability non-parametric gaussian process based framework. encouraging results reported point towards future direction applying technique framework deep gaussian processes able compare considered deep generative models. evaluated semi-supervised algorithm datasets ﬁrstly considered examples usps handwritten digit database examples contained digits split instances used test set. remaining instances selected various portions labelled rest unlabelled. experiment repeated times include errorbars plots. secondly considered data consist dimenfigure plots number incorrectly classiﬁed test points function |zl|. multiple trials performed resulting errorbars shown standard deviation. small training sets large errorbars expected because occasionally challenging instances/outliers included result high error rates affect overall standard deviation. bayesian gp-lvm baseline struggled small training sets performed badly dataset; thus plotted clarity. sional observations belonging three known classes corresponding different phases ﬂow. performed trials instances used test whereas rest split different proportions labelled/unlabelled sets. multi-label data also handled method case considered here. method learned low-dimensional embedding available inputs logistic regression classiﬁer trained relevant parts embedding corresponding class space. experimented taking different numbers samples populating initial labelled set; difference increasing samples minimal. also using mean obtained worse results method still outperformed baselines. compared training classiﬁer features learned standard bayesian gp-lvm applied baselines take account populate small training sets using sampling. figure presents results suggesting approach manages effectively take account unlabelled data. gain performance signiﬁcant method copes well even labelled data extremely scarce. notice methods would perform better robust classiﬁer used logistic regression convenient choice performing multiple trials fast. therefore conclusions safely drawn obtained relative errors since methods compared equal footing. considered semi-described problems part general class missing value problems also includes semisupervised learning auto-regressive future state simulation. principled method including input uncertainty partial inputs gaussian process models also introduced solve problems within single coherent framework. explicitly represent uncertainty approximate posterior distributions variationally constrained. allowed deﬁne algorithms casting missing value problems particular instances learning pipelines variationally constrained formulation building block. algorithms resulted signiﬁcant performance improvement forecasting regression classiﬁcation. believe contribution paves building powerful models representation learning real-world heterogenous data. particular achieved combining method deep gaussian process models relevance determination techniques consolidate semi-described hierarchies features gradually abstracted concepts. plan investigate application models settings control robotic systems learn simulating future states auto-regressive manner using incomplete data miminal human intervention. transfer learning another promising direction applying models. references bishop james. analysis multiphase ﬂows using dual-energy gamma densitometry neural networks. nuclear instruments methods physics research chapelle sch¨olkopf zien editors. semisupervised learning. press cambridge damianou hensman lawrence. gaussian process models parallelization acceleration. arxiv preprint arxiv. deisenroth rasmussen. gaussian processes data-efﬁcient learning robotics control. ieee transactions pattern analysis machine intelligence issn rihan torr rogez lawrence. ambiguity modeling latent spaces. popescubelis stiefelhagen editors machine learning multimodal interaction lncs pages springer-verlag june wilk rasmussen. distributed variational inference sparse gaussian process regression latent variable models. arxiv. ghahramani jordan. learning incomplete data. technical report cbcl massachusetts institute technology girard rasmussen qui˜nonero candela murray-smith. gaussian process priors uncertain inputs—application multiple-step ahead time series forecasting. advances neural information processing systems pages lawrence jordan. semi-supervised learning gaussian processes. saul weiss bouttou editors advances neural information processing systems volume pages cambridge press. lawrence qui˜nonero candela. local distance preservation gp-lvm back constraints. cohen moore editors proceedings international conference machine learning volume pages omnipress isbn ---. ./.. qui˜nonero-candela girard larsen rasmussen. propagation uncertainty bayesian kernel models-application multiple-step ahead forecasting. acoustics speech signal processing proceedings.. ieee international conference volume pages ii–. ieee rosenberg hebert schneiderman. semisupervised self-training object detection modapplication computer vision els. wacv/motions volume volume pages ./acvmot... point variational bound similar equation ﬁrst term denoted refers expanded probability space thus involves inducing inputs additional variational distribution since second term tractable going focus term. breaking logarithm again write term covariance matrix constructed evaluating covariance function training inputs full expression found taking appropriate product respect dimensions; indeed since joint probability factorises respect output dimensions bound logarithm marginal likelihood written terms every term considers single dimension notice obtain tractable bound explicitly make assumption equation form variational distribution. however assumption still made implicitly equivalence derivations rather instructive respect effect variational constraint. also notice expression covariance matrix longer inverted. therefore writting term form manage obtain expression allows uncertainty propagated mapping. possible also obtain tighter variational bound depend need collect terms contain equation stationary point respect distribution setting zero). able replace optimal value back variational bound. titsias lawrence explain trick. marginal prior inducing variables. expressions denotes covariance matrix constructed evaluating covariance function inducing points cross-covariance inducing latent points order perform variational inference expanded probability model introduce variational distributions taken gaussian. convenience drop inducing points expressions remainder appendix convenience. have figure predictions obtained different methods semi-described learning comparing method standard method multiple linear regression nearest neighbour regression input space data-imputation method based gp-lvm mean predictor results simulated data obtained trials. method cannot handle partial observations thus uncertainty constant; clarity errorbar plotted separately right dashed vertical line gp-lvm method produced huge errorbars thus don’t plot here clarity. section looked performing predictions gaussian processes trained partially observed inputs. method compared approaches ﬁgure limit y−axis ﬁxed smaller value show comparison standard method clearly. reason methods produced large errors omitted. appendix show full ﬁgure obtained results ﬁgure conclusion drawn ﬁgure method efﬁcient taking account extra partially observed input true even extra small proportion features observed. hand nearest neighbour runs difﬁculties real data considered even worse produces huge errors features missing finally baseline uses standard gp-lvm means imputing missing values produces results fact worse compared extra ignored baseline using gplvm treats input space single point estimates; incorporating uncertainty input location model ignoring imputed values. appendix refers auto-regressive gaussian process model developed section ﬁgure showed results last steps iterative forecasting task. show rest predictive sequence obtained extrapolating steps. corresponding quantiﬁcation error shown table table mean squared mean absolute error obtained extrapolating chaotic time-series data. gpuncert refers basic method girard naive autoregressive approach propagate uncertainties. algorithm problematic percentage missing features approaches somehow corner-case still shows method reliant covariates available. investigate issue created simulated data explained section time multiple datasets generated different input output dimensions respectively. ﬁgure show comparison sd-gp standard different selections percentage missing features explain challenge handling missing values sd-gp consider separate variational parameter exists every input namely parameters ...n step algorithm extreme cases mentioned previous paragraph number variational parameters remains large available covariates learn few. renders optimisation parameters difﬁcult. even features missing using sd-gp still advantageous compared using standard sd-gp utilise extra information fully observed outputs correspond fully missing figure full predictions obtained competing methods chaotic time-series data. plots show values obtained predictive step compared methods; plot bottom shows corresponding predictive uncertainties gpuncert refers basic method girard naive autoregressive propagate uncertainties.", "year": 2015}