{"title": "Document Context Language Models", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "Text documents are structured on multiple levels of detail: individual words are related by syntax, but larger units of text are related by discourse structure. Existing language models generally fail to account for discourse structure, but it is crucial if we are to have language models that reward coherence and generate coherent texts. We present and empirically evaluate a set of multi-level recurrent neural network language models, called Document-Context Language Models (DCLM), which incorporate contextual information both within and beyond the sentence. In comparison with word-level recurrent neural network language models, the DCLM models obtain slightly better predictive likelihoods, and considerably better assessments of document coherence.", "text": "yangfeng trevor cohn lingpeng kong chris dyer jacob eisenstein school interactive computing georgia institute technology department computing information systems university melbourne school computer science carnegie mellon university text documents structured multiple levels detail individual words related syntax larger units text related discourse structure. existing language models generally fail account discourse structure crucial language models reward coherence generate coherent texts. present empirically evaluate multi-level recurrent neural network language models called document-context language models incorporate contextual information within beyond sentence. comparison sentence-level recurrent neural network language models dclms obtain slightly better predictive likelihoods considerably better assessments document coherence. statistical language models essential components natural language processing systems machine translation automatic speech recognition text generation information retrieval language models estimate probability word given context. conventional language models context represented n-grams models condition ﬁxed number preceding words. recurrent neural network language models dense vector representation summarize context across preceding words within sentence. context operates multiple levels detail syntactic level word’s immediate neighbors predictive; level discourse topic words document lend contextual information. recent research developed variety ways incorporate document-level contextual information. example mikolov zweig mikolov topic information extracted entire document help predict words sentence; propose construct contextual information predicting bag-of-words representation previous sentence separate model; wang build bag-of-words context previous sentence integrate long short-term memory generating current sentence. models hybrid architectures recurrent sentence level different architecture summarize context outside sentence. paper explore multi-level recurrent architectures combining local global information language modeling. simplest model would train single ignoring sentence boundaries shown figure last hidden state previous sentence used initialize ﬁrst hidden state sentence architecture length equal number tokens document; typical genres news texts means training rnns sequences several hundred tokens introduces problems information decay sentence thirty tokens contextual information previous sentence must propagated recurrent dynamics thirty times reach last token current sentence. meaningful documentlevel information unlikely survive long pipeline. figure fragment document-level recurrent neural network language model also extension sentence-level rnnlm document level ignoring sentence boundaries. paper multi-level recurrent structures solve problems thereby successfully efﬁciently leveraging document-level context language modeling. present several variant document-context language models evaluate predictive likelihood ability capture document coherence. core modeling idea work integrate contextual information language model previous sentence language model current sentence. present three alternative models various practical theoretical merits evaluate section distributed representation n-th word corresponding hidden state computed word representation previous hidden state bias term. input hidden dimension respectively. original rnnlm prediction word sequence. transition function could nonlinear function used neural networks elementwise sigmoid function complex recurrent functions lstm work lstm consistently gives best performance experiments. stacking lstm together able obtain even powerful transition function called multi-layer lstm multi-layer lstm hidden state lower-layer lstm cell used input upperlayer hidden state ﬁnal-layer used prediction. following models number layers two. rest section consider different ways employ contextual information document-level language modeling. models obtain contextual representation hidden states previous sentence information different ways. underlying assumption work contextual information previous sentences needs able short-circuit standard directly impact generation words across longer spans text. ﬁrst consider relevant contextual information ﬁnal hidden representation previous sentence that length sentence create additional paths information impact hidden representation current sentence writing word representation n-th word t-th sentence have activation function parameterized function combines context vector input hidden state. future work consider variety forms function simply concatenate representations emission probability computed standard rnnlm underlying assumption model contextual information impact generation word current sentence. model therefore introduces computational short-circuits cross-sentence information illustrated figure information ﬂows hidden vector another call context-to-context document context language model abbreviated ccdclm. speciﬁc architecture number parameters size hidden representation size word representation vocabulary size. constant factors come weight matrices within two-layer lstm unit. complexity class standard rnnlm. special handling necessary ﬁrst sentence document. inspired idea sentence-level language modeling introduce dummy contextual representation start symbol document. another parameter learned jointly parameters model. training procedure ccdclm similar conventional rnnlm move left right document compute softmax loss output ytn. backpropagate loss entire sequences. rather incorporating document context recurrent deﬁnition hidden state push directly output illustrated figure hidden state conventional rnnlm sentence deﬁned equation document context impacts output directly call model context-to-output dclm modiﬁcation model architecture ccdclm codclm leads notable change number parameters. total number parameters codclm difference parameter numbers codclm ccdclm recall vocabulary size size latent representation cases therefore reasonable cases codclm includes parameters ccdclm general. codclm parameters must learned potentially important computational advantage. shifting hidden layer output layer relationship hidden vectors different sentences decoupled computed isolation. guided language generation scenario machine translation speech recognition common case neural language models means decoding decisions pairwise dependent across sentences. contrast ccdclm tying means decoding decisions jointly dependent across entire document. joint dependence important advantages propagates contextual information across document; ccdclm codclm thereby offer points tradeoff accuracy decoding complexity. potential shortcoming ccdclm codclm limited capacity context vector ﬁxed dimensional representation context. might sufﬁce short sentences sentences grow longer amount information needing carried forward also grow therefore ﬁxed size embedding insufﬁcient. reason consider attentional mechanism based conditional language models translation allows dynamic capacity representation context. ct−n formulated weighted linear combination hidden states previous sentence weights constrained simplex using softmax transformation. weight encodes importance context position generating current word deﬁned neural network hidden layer single scalar output. consequently position generated output ‘attend’ different elements context sentence would arguably useful shift focus make best context vector generation. revised deﬁnition context equation requires minor changes generating components. include additional input recurrent function output generating function follows output uses single hidden layer network merge local state context expanding dimensionality size output vocabulary using extended model named attentional dclm evaluate models perplexity document-level coherence assessment. ﬁrst data used evaluation penn treebank corpus standard data used evaluating language models standard split sections training development test keep words construct vocabulary replace lower frequency words special token unknown. vocabulary also includes special tokens start indicate beginning sentence. total vocabulary size investigate capacity modeling documents larger context subset north american news text corpus construct another evaluation data set. shown table average length training documents sentences. follow procedure preprocess dataset corpus keep words training vocabulary. basic statistics data sets listed table two-layer lstm build recurrent architecture document language models implement package rest section includes additional details implementation available online https//github.com/jiyfeng/dclm. initialization parameters initialized random values drawn range rameter matrix respectively suggested glorot bengio learning online learning performed using adagrad initial learning avoid exploding gradient problem used norm clipping trick proposed pascanu ﬁxed norm threshold hyper-parameters models include tunable hyper-parameters dimension word representation hidden dimension lstm unit consider values best combination model selected development sets grid search. experiments hidden dimension attentional component adclm document length shown table average length documents tokens extreme cases tokens. practice noticed training long documents leads slow convergence. therefore segment documents several non-overlapping shorter documents sentences preserving original sentence order. value used experiments although compare subsection recurrent neural network language model model trained individual sentences without contextual information comparison models baseline system highlights contribution contextual information. rnnlm sentence boundary straightforward extension sentence-level rnnlm document-level illustrated figure also viewed conventional rnnlm without considering sentence boundaries. difference rnnlm drnnlm drnnlm able consider extra-sentential context. hierarchical rnnlm also adopt model architecture hrnnlm another baseline system reimplemented several modiﬁcations fair comparison. comparing original implementation ﬁrst replace sigmoid recurrence function long short-term memory used dclms. furthermore instead using pretrained word embedding update word representation training. finally jointly train language models sentencelevel document-level. changes resulted substantial improvements make fair comparison across different models follow conventional compute perplexity. particularly start tokens used notational convenience. token previous sentence never used predict start token current sentence. therefore computation procedure perplexity models without contextual information. table present results language modeling perplexity. best perplexities given context-to-context dclm data attentional dclm nant data dclm-based models achieve better perplexity prior work. improvements dataset small absolute sense consistently point value including multi-level context information language modeling. value context information veriﬁed model performance nant dataset. interest behavior attentional dclm data sets. model combines context-tocontext context-to-output mechanisms. theoretically adclmis considerably expressive codclm ccdclm. hand also complex learn innately favors large data sets. results reported table document length threshold ﬁxed meaning documents partitioned subsequences sentences. interested know whether results depended parameter. taking would identical standard rnnlm separately sentence. test effect increasing also empirical comparison ccdclm. figure shows curves development set. x-axis number updates ccdclm training set. y-axis mean per-token log-likelihood given equation development set. shown ﬁgure seems learn quickly iteration beginning although iteration time-consuming need backpropagate longer documents. however sufﬁcient number updates ﬁnal performance results nearly identical slight advantage setting. suggests tradeoff amount contextual information ease learning. long-term goal coherence evaluation predict texts coherent optimize criterion multi-sentence generation tasks summarization machine translation. well-known proxy task automatically distinguish original document alternative form sentences scrambled multi-sentence language models applied task directly determining whether original document higher likelihood; supervised training necessary. adopt speciﬁc experimental setup proposed barzilay lapata give robust model comparison limited number documents available test employ bootstrapping first test generated sampling documents original test replacement. then shufﬂed sentences document pseudo-document combination single test example. repeated procedure produce test sets test includes pairs document test set. since test instance pairwise choice random baseline expected accuracy evaluate models proposed paper conﬁguration best development perplexity shown table results accuracy standard deviation calculated resampled test sets. shown table best accuracy given ccdclm also gives smallest standard deviation furthermore dclm-based models signiﬁcantly outperform rnnlm given two-sample one-side z-test bootstrap samples. addition ccdclm codclm outperform hrnnlm statistic respectively. addition also evaluated models trained nant dataset coherence evaluation task. test sets best accuracy number across different models obtained codclm. compare results table believe performance drop domain mismatch. even though nant corpora collecting news articles totally different distributions words sentence lengths even document lengths shown table unlike prior work coherence evaluation approach trained supervised data. supervised training might therefore improve performance further. however emphasize real goal make automatically-generated translations summaries coherent therefore avoid overﬁtting artiﬁcial proxy task. neural language models learn distributed representations words together probability function word sequences. proposed bengio feedforward neural network single hidden layer used calculate language model probabilities. limitation model ﬁxed-length context used. recurrent neural network language models avoid problem recurrently updating hidden state thus enabling condition arbitrarily long histories. work make extension include context recurrent architecture allowing multiple pathways historical information affect current word. comprehensive review recurrent neural networks language models offered mulder model baselines rnnlm sentence boundary hierarchical rnnlm models attentional dclm context-to-output dclm context-to-context dclm signiﬁcantly better drnnlm p-value signiﬁcantly better hrnnlm p-value conventional language models including models recurrent structures limit context scope within sentence. ignores potentially important information preceding text example previous sentence. targeting speech recognition contextual information especially important mikolov zweig introduce topic-conditioned rnnlm incorporates separately-trained latent dirichlet allocation topic model capture broad themes preceding text. focus discriminatively-trained end-to-end models. recently introduced document-level language model called hierarchical recurrent neural network language model approach channels information modeling words sentence another recurrent model modeling sentences based bag-of-words representation sentence. also construct bag-of-words representation previous sentences insert sentence-level lstm.) modeling approach uniﬁed compact employing single recurrent neural network architecture multiple channels information feed forward prediction word. also prior work exploring attentional architecture moving away speciﬁc problem language modeling brieﬂy consider approaches modeling document content. hovy propose convolution kernel summarize sentence-level representations modeling document. model coherence evaluation parameters learned supervised training. related convolutional architectures document modeling considered denil tang encoder-decoder architectures provide alternative perspective compressing information sequence single vector attempting decode target information vector; idea notably applied machine translation also employed coherence modeling hierarchical sequence-to-sequence model conditions start word sentence contextual information provided encoder apply idea language modeling. different models hierarchical structures paragraph vector encodes document numeric vector discarding document structure retaining topic information. contextual information beyond sentence boundary essential document-level text generation coherence evaluation. propose document-context language models provide various approaches incorporate contextual information preceding texts. empirical evaluation perplexity shows dclms give better word prediction language models comparison conventional rnnlms; performance also good unsupervised coherence assessment. future work includes testing applicability models downstream applications summarization translation. acknowledgments work initiated jelinek memorial summer workshop speech language technologies university washington seattle supported johns hopkins university grant darpa lorelei contract hr--- gifts google microsoft research amazon mitsubishi electric research laboratory. also supported google faculty research award kyunghyun bart merri¨enboer caglar gulcehre dzmitry bahdanau fethi bougares holger schwenk yoshua bengio. learning phrase representations using encoder-decoder statistical machine translation. emnlp mulder steven bethard marie-francine moens. survey application recurrent neural networks statistical language modeling. computer speech language misha denil alban demiraj kalchbrenner phil blunsom nando freitas. modelling visualising summarising documents single convolutional neural network. arxiv preprint arxiv. xavier glorot yoshua bengio. understanding difﬁculty training deep feedforward neural networks. international conference artiﬁcial intelligence statistics shujie muyun yang ming zhou sheng hierarchical recurrent neural network document modeling. emnlp lisbon portugal september association computational linguistics. alessandro sordoni michel galley michael auli chris brockett yangfeng margaret mitchell jian-yun jianfeng bill dolan. neural network approach context-sensitive generation conversational responses. naacl", "year": 2015}