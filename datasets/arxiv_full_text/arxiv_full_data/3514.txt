{"title": "Supervised Classification: Quite a Brief Overview", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "The original problem of supervised classification considers the task of automatically assigning objects to their respective classes on the basis of numerical measurements derived from these objects. Classifiers are the tools that implement the actual functional mapping from these measurements---also called features or inputs---to the so-called class label---or output. The fields of pattern recognition and machine learning study ways of constructing such classifiers. The main idea behind supervised methods is that of learning from examples: given a number of example input-output relations, to what extent can the general mapping be learned that takes any new and unseen feature vector to its correct class? This chapter provides a basic introduction to the underlying ideas of how to come to a supervised classification problem. In addition, it provides an overview of some specific classification techniques, delves into the issues of object representation and classifier evaluation, and (very) briefly covers some variations on the basic supervised classification task that may also be of interest to the practitioner.", "text": "original problem supervised classiﬁcation considers task automatically assigning objects respective classes basis numerical measurements derived objects. classiﬁers tools implement actual functional mapping measurements—also called features inputs—to so-called class label—or output. ﬁelds pattern recognition machine learning study ways constructing classiﬁers. main idea behind supervised methods learning examples given number example input-output relations extent general mapping learned takes unseen feature vector correct class? chapter provides basic introduction underlying ideas come supervised classiﬁcation problem. addition provides overview speciﬁc classiﬁcation techniques delves issues object representation classiﬁer evaluation brieﬂy covers variations basic supervised classiﬁcation task also interest practitioner. preliminaries bayes classiﬁer generative probabilistic classiﬁers discriminative probabilistic classiﬁers losses hypothesis spaces loss convex surrogate losses particular surrogate losses neural networks neighbors trees ensembles decision trees multiple classiﬁer systems apparent error holdout resampling techniques leave-one-out k-fold cross validation bootstrap estimators tests signiﬁcance learning curves single best classiﬁer words realistic scenarios consider playful realistic problem comparing apples oranges. precisely consider goal telling apart automated e.g. means machine speciﬁcally computer. building machine? approach could construct accurate physiological model types fruit. gather known appearances cell structure chemical compounds etc. together type physiological laws relate quantities inner processes. presented piece fruit measure quantities think matter check models. piece fruit want predict whether apple orange best assigned class model best. clearly performance approach critically depends issues well build models good deciding quantities matter accurate measure these actually decide whether good model model instance adequate dealing cases pathological physiological point view e.g. pieces fruit suﬀer deformations rot. nevertheless enough understanding problem hand able tackle sketched. consider even challenging problem comparing foos bars. again consider goal telling apart automated basis particular measurements taken individual foos bars. going problem now? might even know exactly dealing here. foos? bars? fruits could build instance physiological model even know whether physiology applies. physics need case describe objects? chemistry maybe? economics? linguistics? absence precise knowledge problem hand another approach construct asked-for machine tells object classes apart means learning examples. this move away precisely interpretable model based less factual knowledge object classes dealing with. substitute consider diﬀerent type model—in sense general purpose—that learn examples. premier learning settings studied unfortunately people often able recognize absence. work tversky kahneman others tells instance people suﬀer systematic deviations rationality good judgment i.e. cognitive biases ﬁelds pattern recognition machine learning. particular task referred supervised classiﬁcation task given number example input-output relations general mapping learned takes unseen feature vector correct class? generalize ﬁnite examples? next section starts basic introduction classiﬁcation technology underlying ideas. classiﬁers aforementioned mappings functions take measurements made objects need assigning values correct corresponding output label often referred section discusses matter typically warrants consideration prior even actual learning training classiﬁer measurements classiﬁer construct actually going rely predicting labels unseen examples? section broadly cover issue object representation generally considers present objects classiﬁer. point build various classiﬁers diﬀerent possibilities represent objects really good actual classiﬁer built prediction task? section delves question classiﬁer evaluation providing tools insight behavior performance machines constructed. chapter’s main conclusions covered section single best classiﬁer inherent tradeoﬀ number examples generalize classiﬁer complexity. section quickly introduces elementary regularization i.e. another controlling classiﬁer complexity. conclusion section mentions variations basic supervised classiﬁcation. selection classiﬁers introduce notation make mathematically precise scope within operate describe ultimate classiﬁer equally unambiguous terms. general classiﬁcation setting considered total labeled objects learn from. every object represented d-dimensional feature vector corresponding labels assume actually already numerical representation makes measurements every vector addition choice limit two-class classiﬁcation problems simplicity exposition. although typically stated explicitly aware pairs assumed random i.i.d. draws underlying true distribution notational convenience also deﬁne number positive negative samples respectively. general shorthand rather slightly elaborate e.g. classiﬁer function mapping space feature vectors labels input space deﬁned taken smaller instance know measurements take continuous values integer. least input space typically larger training points classiﬁer generalize unseen input vectors. generally consider possible inputs. other classiﬁer splits space sets. points assigned class assignment boundary sets generally referred classiﬁcation decision boundary finally deﬁne objective every classiﬁer sets optimize. sense also ultimate measure decide classiﬁers performs best. facto standard measure determines fraction using misclassiﬁed objects applying classiﬁer problem described iverson brackets measure goes various names like error rate classiﬁcation error risk probability misclassiﬁcation. again rather refer terms accuracy equals minus error rate. lower particular classiﬁer better would particular problem perform better every two-class classiﬁer however less principled extended general multi-class case. combining multiple two-class classiﬁers classiﬁers particular probabilistic ones adapt naturally multi-class case. understand know done. case construct optimal classiﬁer attains minimum risk classiﬁer? refer optimal classiﬁer feature vector want label consider corresponding term within integral equation assign choice adds least integral feature vector value assignment class largest. reach overall minimum stick optimal choice every location case actually decision boundary matter decision classiﬁer makes induce equally large error. words deﬁne follows possibly instructive reformulation considering conditional probabilities often referred posterior probabilities simply posteriors instead full probabilities. equivalent checking verify whether vein equation decide assign expanding footnote remark important settings another performance measure another evaluating appropriate case classiﬁcation cost class diﬀerent. equation tacitly assumes predicting wrong class incurs cost predicting right class comes cost. many real-world settings however making error costly making other. instance building rotten fruit detector classifying fresh piece fruit rotten could turn less costly classifying piece fruit good. building actual classiﬁer life often even worse even know cost really incurred misclassiﬁcation. reason resort analysis so-called receiver operating characteristic curve related measure area curve curve related area provide sense tools study behavior classiﬁers possible misclassiﬁcation costs simultaneously. another important classiﬁcation setting strong imbalance class sizes e.g. expect class occur signiﬁcantly much often class—a situation easily imagined various applications. also analyses related techniques advisable. topic reader kindly referred related work. indeed case assign otherwise. latter basically states that given observations made assign corresponding object class largest probability conditioned observations. especially formulated like this seems like obviously optimal assignment strategy. theoretical constructs referred bayes error rate bayes classiﬁer respectively. former gives lower bound best error could ever achieve problem hand. latter shows make optimal decisions known. quantities merely theoretical importance indeed. reality hope approximate them exact never available objects work draws distribution. based examples build classiﬁer generalizes well follows chapter setting considered. previous subsection showed compare perform optimal class assignment. hold making assumptions form underlying class distributions px|y estimate free parameters training data provided. class-conditional probabilities describe distribution underlying individual classes i.e. consider distribution given subsequently px|y combined estimate prior probability every class—i.e. estimate often every class anyway occurs—to come overall estimate px|y models called generative because ﬁtted data allow generate data class-conditional distributions though accuracy generative process course heavily depends accuracy model common simple instantiations generative model classical linear discriminant analysis assumes classes normally distributed diﬀerent means equal covariance matrices. denoting normal distribution mean covariance full probability model written merely speciﬁes class models consider tell data have. classical come parameters determine maximum likelihood estimates. approaches like maximum posteriori proper bayesian estimation possible well. relying log-likelihood some authors take term linear discriminant analysis refer broadly deﬁned class classiﬁers. normality-based classiﬁer discussed also referred fisher’s linear discriminant classical discriminant analysis. note that matter estimates chooses abbreviation refers fact decision boundary forms -dimensional hyperplane checked explicitly solving ﬁnding takes form linear equation again estimates specify free parameters model. instance could also estimated estimator could applied laplace smoothing prior estimates instance consider instead n+/n. course probably dramatic inﬂuence actual choice normal class-conditional models realistic. even though misspeciﬁed model such mean classiﬁer perform well advanced complex choices class-conditional distributions introduced potentially improve upon normal distributions also refer already subsection important notes regarding classiﬁer complexity relation training size). example substitute relatively rigid parametric choice highly ﬂexible nonparametric kernel density estimator class leads classiﬁer often referred parzen classiﬁer. previous subsection decided model based come decision whether assign class considering corresponding feature vector subsection however showed might well model reach decision. course full model conditional going direction possible. classiﬁcation however merely need know save trouble building full model. fact unsure true form underlying class-conditionals marginal describes feature vector distribution directly modeling wise avoid potential problems model misspeciﬁcation. hand full model accurate enough positive eﬀect classiﬁer’s performance approaches directly model called discriminative straightaway information matters tell class apart other. classical model setting sense counterpart called logistic regression. model assume logarithm so-called posterior odds ratio takes linear form i.e. parameters typically estimated maximizing log-likelihood. formally consider likelihood full model posterior choice necessary additional marginal model inﬂuence optimum parameters interested consider generative models discriminative probabilistic ones come diﬀerent kinds ﬂavors. particularly popular fairly general form turning linear classiﬁers nonlinear ones discussed subsection variations found among others made explicit equations logistic regression lead linear decision boundaries i.e. -dimensional hyperplanes d-dimensional feature space. functional form decision boundaries indirectly ﬁxed choice probabilistic model. ultimately seen mapping feature vector linear value real line. subsequent decision assign point class depends whether obtained value smaller larger last operation basically turns linear mapping actual classiﬁer essence carried applying sign function following linear transform. maps every number point except points decision boundary mapped main diﬀerence logistic regression free parameters respective linear transformations obtained. especially within areas machine learning common altogether forget potentially probabilistic interpretation classiﬁer. instead explicitly deﬁnes functions transform feature vector single number subsequently turned actual classiﬁers signing them. next that deﬁnes measure good particular function point training data. functions often referred hypothesis space measure typically referred loss speciﬁcally loss function takes predicted value decides good value matches desired output once ﬁxed search best ﬁtting typically measure zero decision boundary gets mapped really inﬂuence performance classiﬁer. nevertheless course decide boundary points mapped rather obvious choice take loss actually interested fraction misclassiﬁed observations. equation deﬁnes fraction i.e. classiﬁcation error risk true distribution. considering ﬁnite number training data best count number incorrectly assigned samples major problem loss ﬁnding optimal many cases computationally hard. take example linear functions ﬁnding turns np-hard even settling approximate solutions necessarily help broadly used approach around problem computational complexity loss poses consider relaxation original problem turns convex optimization problem relatively easy solve e.g. gradient descent variations technique. achieve this ﬁrst chooses convex. classical choice would space linear functions complex choices possible later. second step relaxation optimization problem turn so-called surrogate losses. losses approximate well possible additional beneﬁts. instance could choose loss everywhere diﬀerentiable something hold loss. turn optimization convex optimization function equation needs convex well. generally approximate equation convex function upper-bounds loss everywhere. idea upper bound minimizer know loss would achieve training certainly larger surrogate risk minimizer attains. figure plot loss terms times predicted output three examples surrogate losses upperbound loss. logistic loss solid light gray hinge loss dashed dark gray squared loss dotted black. would decide label sample based output trained classiﬁer provides typically needs consider loss function value instance rewrite achieve loss. figure shape loss plotted terms. ﬁgure shows various widely-used upper bounds ﬁgure displays solid light gray curve. using loss combination linear hypothesis class leads standard logistic regression introduced subsection case probabilistic view resulting classiﬁer well interpretation logistic regression minimizer speciﬁc surrogate loss. second well-known classiﬁer least basic form obtained using so-called hinge loss using linear hypotheses basically among others referred linear regression classiﬁer least squares classiﬁer least squares support vector machine fisher classiﬁer fisher’s linear discriminant indeed classiﬁer reinterpretation classical decision function introduced fisher language losses hypotheses. truncated squared loss absolute loss |−ba|. subsection introduce ways designing nonlinear classiﬁers often rely formalism presented subsection. artiﬁcial neural networks supervised learning traced back least year perceptron introduced providing linear classiﬁer could trained using basic iterative updating scheme parameters. currently neural networks dominant technique many applications application related areas massively sized data sets train available. even though original formulation perceptron give direct interpretation terms solution optimizing particular loss within hypothesis space linear classiﬁers formulations possible neural networks employed nowadays readily follow earlier sketched loss-hypothesis space paradigm. possibly characteristic feature neural networks hypotheses considered built relatively simple computational units called neurons nodes. unit function takes inputs maps single numerical output. typically takes form linear mapping followed nonlinear transfer activation function together underlying theory svms caused furore late early many development still prime achievements mathematical ﬁeld statistical learning theory started vapnik chervonenkis early least svms still widely known used classiﬁers within ﬁelds pattern recognition machine learning. possibly main feats statistical learning theory broke statistical tradition merely studying theory asymptotic behavior estimators. statistical learning theory also concerned ﬁnite sample setting makes instance statements expected performance unseen data classiﬁers trained limited examples smooth threshold function. choices possible however. choice popularized recently clearly diﬀerent characteristic so-called rectiﬁed linear unit deﬁned various previously mentioned classiﬁers free parameters tuned measuring well given training data. widely used choice squared loss also likelihood based methods considered links probabilistic models studied realize that whatever choice activation function long monotonic using classiﬁcation lead linear classiﬁer. nonlinear classiﬁers constructed combining various parallel sequence. build arbitrarily large networks perform arbitrarily complex input-output mappings. means dealing large diverse hypothesis classes general construction multiple nodes connected parallel provide inputs subsequent nodes. consider instance nonlinear extension where instead single node ﬁrst step multiple nodes receive feature vectors input. second step outputs collected another node transformed similar kind way. function form course stop steps. network arbitrary number steps layers typically referred nowadays called deep networks employed hundreds layers millions parameters. addition this generally many diﬀerent variations basic scheme sketched making diﬀerent choices transfer function using multiple transfer functions changing structure network number nodes layer etc. basically changes hypothesis class considered. addition subsection choice would typically convex optimization problem using neural networks typically move away optimization problems reasonably expect able global optimum. result fully deﬁne classiﬁer specify loss hypothesis class also exact optimization procedure employed. many choices possible carry optimization approaches rely gradient descent variations basic scheme there course clear-cut deﬁnition number layers makes network deep. similar wondering amount data makes number features makes problem high-dimensional. nearest neighbor rule maybe classiﬁer intuitive appeal. widely used classical decision rule earliest nonparametric classiﬁers proposed. order classify unseen object simply determines distances describing feature vector feature vectors training decides assign object class closest feature vector training has. often euclidean distance used determine nearest neighbor training data principle other possibly even expert-designed learned distance measure employed. direct worthwhile extension consider closest sample training i.e. ﬁrst nearest neighbor consider closest assign unseen object class occurs often among nearest neighbors. nearest neighbor classiﬁer various nice interesting properties interesting ones result roughly states increasing numbers training data nearest neighbor classiﬁer converges bayes classiﬁer given increases appropriate rate. classiﬁcation trees decision trees classiﬁers visualized hierarchical tree structure every observation classiﬁed traverses tree’s nodes arrive leave contains ﬁnal decision i.e. assign label every node basic operation decides next node level deeper visited. probably simplest extensively used form decisions made every node check whether single particular feature value larger chosen threshold value. outcomes possible every step nodes available every next level reached. using training data various ways come automated construction exact tree hierarchy together decisions make diﬀerent nodes. possible beneﬁts kinds classiﬁers especially last mentioned type decision tree given tree deep easily trace back interpret decision assign sample class reached. retrace steps tree subsequent decisions lead class label provided. terms multiple classiﬁer systems classiﬁer combining ensemble methods refer roughly idea potentially powerful classiﬁers built combining latter often referred base classiﬁers. techniques classiﬁers such ways compile base classiﬁers classiﬁers sense beﬁt data better. various reasons combine classiﬁers. sometimes classiﬁer turns overly ﬂexible wish stabilize base classiﬁer well-known combining technique called bagging trains various classiﬁers based bootstrap samples data assigns sample based average output often large base classiﬁers. another construct diﬀerent base classiﬁers consider random subspaces sampling diﬀerent features every base learner. technique extensively exploited random forests like combining classiﬁers also exploited dealing problem where sense essentially diﬀerent sets features play role. instance analysis patient data might want diﬀerent classiﬁers high-dimensional genetic measurements low-dimensional clinical data sets behave rather diﬀerently other. specialized classiﬁers trained various forms socalled ﬁxed trained combiners applied come ﬁnal decision rule times base classiﬁers already quite complex possibly multiple classiﬁer system itself. nice examples available medical image analysis recommender systems cases advanced systems developed independently other. result fair chance every systems strengths weaknesses even best performing individual system cannot expected perform best every part feature space. hence combining systems result signiﬁcantly improved overall performance. another reason employ classiﬁer combining integrate contextual features classiﬁcation process. approaches especially beneﬁcial integrating contextual information image signal analysis tasks techniques seen speciﬁc form stacked generalization stacking becoming relevant days connection deep learning finally mention boosting approaches multiple classiﬁer systems particular adaboost boosting initially studied theoretical setting show so-called weak learners i.e. classiﬁers barely reach better performance error rate equal priori probability largest class could combined strong learner signiﬁcantly improve performance weak ones. research culminated development combining technique sequentially adds base classiﬁers ensemble already constructed next base classiﬁer focuses especially samples previous base learners unable correctly classify. last feature adaptive characteristic particular combining scheme warrants preﬁx ada-. foregoing tacitly assumed already decided feature every object generally represent every object constructing classiﬁcation rule. actual applications however choices made. start with lucky even inﬂuence kind measurements going taken. making color images standard camera oranges? make scan? measure weight diameter plasticity? sequence tissue samples? clearly actual measurements informative depend classiﬁcation task hand. importantly realize measurements omitted never recover lost information building classiﬁer matter advanced clever classiﬁer designs reality practitioner often little inﬂuence precise measurements going made s/he work predetermined initial features. even setting considerations made. going features? using construct derivative features. decisions hinge one’s insight problem possible external expert knowledge acquired. really need values every pixel describing oranges average value channel enough? need measure orangeness suﬃcient? need weight measurement consulting expert suggest contain relevant information without? give brief overview ways represent objects tools partially automate process selecting features create derivatives. addition discuss the—initially maybe counterintuitive—eﬀects using measurements roughly relates complexity classiﬁer. also introduce tool possibly valuable analysis eﬀect number features so-called feature curve. such situation course arise fact particular measurements simply cannot realized number reasons. instance measurements expensive consider part realistic solution invasive unacceptable degree. limitation actually easily removed. nonlinear classiﬁers created including nonlinear transformations original features original feature vector subsequently train linear classiﬁer extended space. essentially extends hypothesis space example form nonlinear features transforming original individual features e.g. transform ﬁrst feature combining individual features e.g. form product feature xixi. clearly possibilities endless. transformation maps original feature vector extended representation. learned classiﬁer extended space induces classiﬁer original space simply take every feature vector classiﬁed transform apply classiﬁer trained higher-dimensional space. using construct even would limit linear classiﬁers would typically nonlinear decision boundaries original d-dimensional feature space. svms received attention result statistical learning theory literature also introduced become widely known kernel trick kernel method roots kernel trick allows extend many inherently linear approaches nonlinear ones computationally simple way. basis that—following representer theorem —many solutions type optimization problems linear classiﬁers considered subsection expressed terms weighted combination inner products training feature vectors classiﬁed i.e. becomes apparent thing matters settings know compute inner products mapped feature vectors function also referred kernel function simply kernel. course explicitly deﬁned always construct corresponding kernel functions power kernel trick many settings avoided. interesting least ways. feature space take inner product grows direct calculation however show inner product larger space expressed terms much simpler case particularly that imagine moving nonlinearities even higher degree eﬀect becomes pronounced. point explicitly expanding feature vector nonlinearly becomes prohibitive calculating induced inner product still easy extreme example radial basis function gaussian kernel deﬁned second reason formulation terms inner products interest principle allows forget explicit feature representation altogether. going back original objects construct function takes objects fulﬁls criteria kernel directly substitute equation kernel function constructed—whether explicit feature space build classiﬁers. kernel methods deﬁne general powerful ﬂexible formalism allows design problem speciﬁc kernels. research direction spawned massive amount publications approaches kernel provides similarity measure feature observations larger value similar observations are. like inner product that least implicitly corresponds underlying feature space limitations apply. many settings might actually idea proper measure similarity sense equivalent dissimilarity objects. possibly measure provided this demonstrated explicitly writing sides equation. note ﬁrst degree monomials also included either explicitly including additional feature original feature vector constant implicitly deﬁning inner product expert working ﬁeld asked build classiﬁer for. therefore expected well thought-through quantity captures essential resemblance diﬀerence objects. dissimilarity approach allows build classiﬁers similar kernelbased classiﬁers without restrictions. core ideas every objects represented absolute measurements performed every individual object rather relative measurements tells similar object interest representative objects. representative objects also referred prototypes. particular prototypes favorite dissimilarity measure every object represented d-dimensional dissimilarity vector measuring features every object seems imply gather useful information them. worst happen measure features partly completely redundant e.g. measuring density already measured mass volume. information present features cannot vanish anymore. sense indeed true question whether still extract information relevant growing feature numbers. classiﬁers rely form estimation basically train classiﬁer estimation typically becomes less less reliable space carry grows. result that typically would improved performance every additional feature beginning eﬀect gradually wear long even leads deterioration performance soon estimates become unreliable enough. behavior often referred curse dimensionality curve plots performance classiﬁer increasing number features called feature curve used simple analytic tool idea sensitive classiﬁer number measurements object described with. possibly equal importance curves used compare classiﬁers other. forms feature curves take depends heavily speciﬁc problem dealing with complexity classiﬁcation method complexity relates speciﬁc problem number training samples train classiﬁer. possible exact mathematical quantiﬁcation quantities real challenge. roughly state complex classiﬁer quicker performance starts deteriorating increasing number features. hand training data available later deterioration performance sets also classiﬁcation technique complex possible decision boundaries former model ﬂexible similarly less smooth. another think hypothesis former classiﬁcation method larger latter one. curse dimensionality indicates particular cases beneﬁcial performance classiﬁer lower feature dimensionality. applicable instance little insight classiﬁcation problem hand case tends deﬁne lots potentially useful features and/or dissimilarities hope least pick important discriminate classes. carrying less systematic reduction dimensionality deﬁning large class features lead acceptable classiﬁcation results. roughly speaking main approaches ﬁrst feature selection second feature extraction. former reduces dimensionality picking subset original feature latter allows combination features fewer features. combination often restricted linear transformations i.e. weighted sums original features meaning considers linear subspaces original feature space. principle however feature extraction also encompasses nonlinear dimensionality reductions. feature selection construction linear possible subspace even limited linear spaces parallel feature space axes. lowering feature dimensionality feature selection also interpreting classiﬁcation results. least shed light features seem matter mostly possibly gain insight interdependencies instance studying coeﬃcients trained linear classiﬁer. aiming interpretable classiﬁer might even sacriﬁce performance sake really limited feature size. feature selection also used select right prototypes employing dissimilarity approach case every feature basically corresponds distances particular prototype. good classiﬁcation approaches really? decide feature other? compare classiﬁers? prototypes work not? section already stated error rate deﬁned equation context ultimate measure decides procedure best lower error rate better. ﬁrst problem face encountered dealing bayes error classiﬁer—ε∗ available unable determine exact classiﬁcation error procedure. like building classiﬁer rely samples merely come estimated error rate. situation worse even. give estimate performance measure based samples—the simplest ways would count number misclassiﬁcations divide number typically also samples train classiﬁer. many real-world settings simply data available time money left gather more. consider alternatives estimate error rate introduce so-called learning curves give basic insight classiﬁer behavior mention overtraining sense single best classiﬁer oﬀer considerations comes developing complete classiﬁer system. major mistake still made among users practitioners pattern recognition machine learning simply uses available samples build classiﬁer estimates error samples. estimate called resubstitution apparent error denoted problem approach gets overly optimistic estimate. classiﬁer adapting speciﬁc points speciﬁc labels therefore performs particularly well set. faithfully estimate actual generalization performance classiﬁer would need training train classiﬁer completely independent so-called test estimate performance. latter also referred holdout set. reality often single disposal case construct training holdout splitting initial two. decide sizes sets? dealing conﬂicting goals here. would like train much data possible would typically give best performing classiﬁer. ﬁnal classiﬁer would deliver client would trained particular settings dealing transductive learning instance inputs available exploited training phase. case training still performed independent labels test primary requirement. related setting semi-supervised learning brieﬂy covered subsection full initial available. idea true performance classiﬁer—a possible selling point least need independent samples. smaller take however less trustworthy estimate extreme case test sample instance estimate error rate always equal adding data test reduce amount data training removes setting train ﬁnal classiﬁer full set. following approaches relying resampling training data provide resolution. cross validation evaluation technique oﬀers best worlds allows train test large data sets. moreover comes estimation accuracy so-called leave-one-out cross validation probably best options have. latter approach loops whole data step pair used evaluate classiﬁer trained examples full except single sample training size test size given want least data test best training size have. test size almost worst have single step loop. going data available every single sample point test giving estimated error rates subsequently average better overall estimate computational reasons e.g. dealing rather large data sets classiﬁers take long train consider settle so-called k-fold cross validation instead leave-one-out variant. case original data split preferably equal sized sets folds. this procedure basically leave-one-out loop folds consecutively leave training test leave-one-out n-fold cross validation. simple approaches proceeds follows data training samples generate bootstrap samples size calculate corresponding apparent error rates particular choice classiﬁer. using every time classiﬁer also calculate error rate total data set. estimate bias given averaged diﬀerence. various improvements upon alternatives scheme suggested studied possibly best-known estimator .εo. ﬁrst term right-hand side apparent error second term outof-bootstrap error. latter determined counting samples original data misclassiﬁed part current bootstrap sample based classiﬁer built. adding mistakes rounds dividing number total number out-of-bootstrap samples gives foregoing course important remember estimates based random samples observe instantiation random variable. result decide anything like signiﬁcant diﬀerence performances classiﬁers resort statistical tests rough idea standard deviation error estimates based test size size ntest instance simple estimator derived result averaging ntest bernoulli trials subsection brieﬂy introduced feature curves give impression error rate evolves increasing numbers features. discussed curse dimensionality context. clear also feature curves like error rate estimated would typically apply estimation techniques described foregoing. another maybe important curve provides insight behavior classiﬁcation method so-called learning curve learning curve plots true error rate figure stylized plots learning curves diﬀerent classiﬁers. solid lines true errors diﬀerent training sizes dashed lines sketch apparent errors. black lines classiﬁer relatively complexity gray lines illustrate behavior complex classiﬁer. light gray dotted horizontal line bayes error problem considered. figure displays stylized learning curves classiﬁers diﬀerent complexity. various characteristics interest observe plots reﬂect typical behavior many classiﬁer many classiﬁcation problems. start with growing sample size classiﬁer expected perform better terms error rate. addition apparent error would typically observe opposite behavior curve increases becomes diﬃcult solve classiﬁcation problem growing training set. limit inﬁnite amount data points curves come together training data better describes general data encounter test time closer true error apparent error get. fact indication trained classiﬁer focuses much speciﬁcs training test set. called overtraining overﬁtting. larger true apparent error overtraining occurred. learning curves classiﬁer come together also glean insight. classiﬁers less complex typically drop quickly also level earlier complex ones. addition former converges error rate often limiting error rate latter given enough data closer bayes error employing complex classiﬁer. result often case classiﬁer uniformly better another even consider classiﬁcation problem. really matters training size dealing benchmarking classiﬁcation method other really taken account. generally smaller training data better stick simple classiﬁers e.g. using linear hypothesis class features. fact best choice classiﬁer depend type classiﬁcation problem need tackle also number training samples disposal lead wonder generally said superiority classiﬁer other. wolpert made question mathematically precise demonstrated several variations question answer that maybe surprisingly exist distinctions learning algorithms. so-called free lunch theorem states roughly averaged classiﬁcation problems possible classiﬁcation method outperforms other. though result certainly gripping interpret care. realized instance among possible classiﬁcation problems construct probably many reﬂect kind realistic setting. nevertheless generally single best classiﬁer. finally learning curve give idea whether gathering training data improve performance. figure classiﬁer corresponding black curves hardly improved even enormous amounts additional data. classiﬁer gray curves probably improve reaching slightly lower error rate enlarging traning set. real-world applications designing building full classiﬁer system often process consider many feature representations various feature reduction schemes compare many diﬀerent types classiﬁers. that might kinds preprocessing steps applied data working images signals instance perform various types enhancement smoothing normalization techniques positive negative eﬀect performance ﬁnal classiﬁer. data also going used evaluation leaks training phase. estimated test errors therefore overly optimistic complex classiﬁers simple ones. result wrongly trained classiﬁer together overly optimistic estimate performance. simple instance decided point nearest neighbor classiﬁer. thing remains done ﬁnding best value decides determine basis performance every test set. seem like minor oﬀense often many choices best number features number nodes layer neural network free parameters kernels etc. particular point list). example maybe diﬃcult gone wrong. decide everything seemingly clean way. preﬁx classiﬁers want study feature selection schemes want decide beforehand kernels want consider classiﬁer combining schemes want employ. gives ﬁnite number diﬀerent classiﬁcation schemes compare based cross validation. pick scheme provides best performance. even though approach actually fairly standard something wrong here. number diﬀerent classiﬁcation schemes gets hand easily does still risk pick overtrained solution badly biased estimate true error rate especially dealing small training sets even complicated issues arise multiple groups work large challenging classiﬁcation task. nowadays various research initiatives labeled data provided publicly third party researchers work simultaneously collaboratively also competition other. information particular possibly indirect leakage test data becomes diﬃcult oversee alone easily correct providing error estimates corresponding conﬁdence intervals like. instance correct fact one’s method inspired results another group read research literature? though statistical approaches available alleviate particular problems safe currently generally applicable solution—if exists. primarily pertains evaluation. real scenarios course also worry reproducibility replicability ﬁndings. otherwise kind science would clearly issues also play signiﬁcant role areas research. general turns however diﬃcult control aspects mistakes made mostly unwittingly case possibly even knowingly. potential less dramatic consequences refer following good reads main idea regularization means performing complexity control. seen already classiﬁer complexity controlled number features used complexity hypothesis class regularization related these. well-known ways regularizing linear classiﬁer constraining already limited hypothesis space further. typically done restricting admissible weights linear classiﬁer sphere radius around origin hypothesis space means solve constraint optimization problem procedure used classical ridge regression eﬀectively stabilizes solution obtained. eﬀect regularization bias classiﬁcation method increases cannot reach certain linear classiﬁers anymore added constraint. time variance classiﬁer estimates decreases constraint average small moderate parameter worsening performance increased bias amply compensated improvement performance reduced variance case regularization lead improved classiﬁer. however regularize strongly bias start dominate pull model away reasonable solution point true error rate start increase again. seen various guises importance acknowledged early statistics data analysis explicit dissection bias-variance tradeoﬀ context learning methods published complex classiﬁer higher variance faced training model important form regularization becomes. equations consider basic form regularization. many variations theme. among others regularizers built-in feature selectors regularizers deep connections earlier discussed kernels introduced discussed main aspects supervised classiﬁcation. last section like review slight variations basic learning problem. though basic many decision problems actually cast classiﬁcation problem. nevertheless reality often confronted problems still completely restricted setting. cover here. particular settings appropriate simply easier describe every object single feature vector feature vectors. approach example common various image analysis tasks so-called descriptors i.e. feature vectors capture local image content various locations image representation image. every image training test represented descriptors goal construct classiﬁer sets. research area studies approaches applicable setting every object described sets feature vectors diﬀerent sizes feature vectors measurement space called multiple instance learning. large number classiﬁcation routines developed speciﬁc problem range basic extensions classiﬁers supervised classiﬁcation domain means combining techniques dissimilarity-based approaches approaches speciﬁcally designed purpose classiﬁcation classical reference initial problem formalized various problems diﬃcult suﬃcient examples classes diﬃcult simply occur seldom. case one-class classiﬁcation might use. instead trying solve two-class problem straightaway aims model distribution support oft-occurring class accurately based decides points really belong class therefore assigned class little known techniques direct relations approaches perform outlier novelty detection data data streams aims identify objects sense away bulk data. test data point outlier less training data present vicinity therefore less certain classiﬁer assigning corresponding object class. consequently outlier detection related techniques also used implement so-called reject options identify points which small automated decision classiﬁer hand probably unreliable. case ultimate decision better left human expert. might instance dealing sample third class; something classiﬁer never examples kind rejection also referred distance reject option second option ambiguity rejection case classiﬁer rather looks leaves ﬁnal decision human expert posteriors close other i.e. approximately ambiguity distance rejection realize erroneous decision classiﬁer deploying human expert come costs. main challenges reject option trade costs optimal way. contextual classiﬁcation already mentioned subsection multiple classiﬁer systems. contextual approaches samples classiﬁed isolation various types neighborhood relations exploited improve overall performance. classical approach employs markov random ﬁelds speciﬁc variations techniques like conditional random ﬁelds earlier mentioned methods using classiﬁer combining techniques often easily applicable leverage full potential general classiﬁcation methodologies. already indicated subsection well latter class techniques seems become relevant context nowadays deep learning approaches. many real-world setting missing data considerable reoccurring problem. classiﬁcation setting means particular features and/or class labels observed. missing features occur failure measurement apparatus human non-response data recorded accidentally erased. various ways deal deletions topic thoroughly studied statistics case missing labels additional causes. simply expensive label data additional input data collected afterwards extend already available data collector specialist provide necessary annotation. case missing label data known within pattern recognition machine learning semi-supervised learning also problem studied years already many diﬀerent techniques developed. though maybe theoretical sense still completely satisfactory practicable solution problem. major issues question extent guarantee supervised classiﬁer indeed improved taking unlabeled data account well various kinds reasons distribution data training time rather diﬀerent test time. examples medical devices trained samples country machine also deployed another country. generally machines sensors suﬀer wear tear result measurement statistics later point time really match distribution time training. depending assumed diﬀerence domains often referred speciﬁcally source target particular approaches employed alleviate discrepancy areas domain adaptation transfer learning study techniques challenging settings. depending actual transfer learned less successful approaches identiﬁed tackle problems. ﬁnal variation supervised classiﬁcation actually concerned regular supervised classiﬁcation. diﬀerence however main setting discussed throughout chapter active learning sets improve data collection process. tries answer various related questions follows. given large number unlabeled samples budget label samples instances consider labeling enable train better classiﬁer would able case would rely random sampling? systematic collect data labeled quicker come well-trained classiﬁer? problem formulation direct relations sequential analysis optimal experimental design overviews current techniques found major issues active learning systematic collection labeled training data typically leads systematic bias well. correcting seems essential points problem generally encounter practical settings directly relates issues indicated subsection assumptions supervised classiﬁcation training test consist i.i.d. samples underlying problem deﬁned density reality assumption probably violated care taken. many thanks mariusz flasiński second anonymous reviewer critical encouraging appraisal overview. comments helped improve exposition chapter drastically reduce number spelling grammar glitches.", "year": 2017}