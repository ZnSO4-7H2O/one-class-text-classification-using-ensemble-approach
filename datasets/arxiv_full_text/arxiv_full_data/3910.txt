{"title": "Multidimensional data classification with artificial neural networks", "tag": ["cs.NE", "cs.AI", "F.1.1; K.3.2; I.2.6"], "abstract": "Multi-dimensional data classification is an important and challenging problem in many astro-particle experiments. Neural networks have proved to be versatile and robust in multi-dimensional data classification. In this article we shall study the classification of gamma from the hadrons for the MAGIC Experiment. Two neural networks have been used for the classification task. One is Multi-Layer Perceptron based on supervised learning and other is Self-Organising Map (SOM), which is based on unsupervised learning technique. The results have been shown and the possible ways of combining these networks have been proposed to yield better and faster classification results.", "text": "multi-dimensional data classiﬁcation important challenging problem many astro-particle experiments. neural networks proved versatile robust multi-dimensional data classiﬁcation. article shall study classiﬁcation gamma hadrons magic experiment. neural networks used classiﬁcation task. multi-layer perceptron based supervised learning self-organising based unsupervised learning technique. results shown possible ways combining networks proposed yield better faster classiﬁcation results. many high-energy gamma experiments deal problem separating gammas hadrons experiments usually generate large data sets many attributes them. multi-dimensional data classiﬁcation problem oﬀers daunting challenge extracting small number interesting events overwhelming background many techniques active research addressing problem. list includes classical statistical techniques sophisticated techniques like neural networks classiﬁcation trees kernel functions. class neural networks provides automated technique classiﬁcation data given number classes active research artiﬁcial intelligence machine learning communities. several neural network models developed address classiﬁcation problem. usually makes distinction supervised unsupervised classiﬁers supervised classiﬁer used analyst examples correct classiﬁcation known. done example problems related particle physics accelerators generally good knowledge detectors underlying physics good simulations available. whereas unsupervised technique events partitioned classes similar elements without using additional information. case especially ﬁelds operating discovery regime e.g. astroparticle physics mathematical perspective neural network simply mapping input data dimension output dimension neural network network typically divided various layers; layer neurons also called nodes information units connected together links. artiﬁcial neural networks able classify data learning discriminate patterns features associated data. neural network learns data data vector input subjected learning information gain stored links associated neurons. output generated network depends problem network type. gamma/hadron separation problem supervised network maps input vector onto interval whereas unsupervised networks nodes adapted input vector output network represents natural groups exist data set. visualization technique used view groups discovered network. section describes data sets used classiﬁcation. section deals multilayer perceptron network classiﬁcation results. section deals self-organizing maps variant along classiﬁcation results. conclusions future perspectives discussed section data sets generated montecarlo program corsika contain gammas ’on’ events hadron events. events stored diﬀerent ﬁles. ﬁles contain event parameters ascii format line numbers event parameters deﬁned below approach used root analysis package particular multilayer perceptron class implements generic layered network. since supervised network took half gamma data train network remaining data test code root package ﬂexible simple use. allowed create network nodes input layer hidden layer number nodes output layer single neuron return data represent hadrons they’re gammas. weights randomly beginning training session adjusted following runs order minimize errors errors cycle deﬁned erri error output node. data input output nodes transferred linearly hidden layers sigmoid /)). tested network using diﬀerent learning methods proposed code authors example called stochastic minimization based robbins-monro stochastic approximation default broyden fletcher goldfarb shanno method proved quickest better error approximation. figures represent possible output using root package data. ﬁrst depicts error function network comparing training test data. note greater number runs better network behaves. second shows distributions output nodes many times network decides give value near used classom based unsupervised learning technique. siﬁcation data sets labels. consists information units also called neurons arranged two-dimensional grid every neuron associated n-dimensional reference vector denotes dimension input vectors. neurons connected adjacent neurons neighbourhood relation dictates topology structure map. common topologies rectangular hexagonal. learning process follows data groups visualisation steps repeated selected number trials epochs. trails completed unfolds distribution data ﬁnding number natural groups exist data set. output reference vectors associated units. termed codebook. view groups outliers discovered visualize codebook. u-matrix technique typically used purpose. events data directly used som. prior training required. unsupervised behavior discovered groups data automatic way. worked kernel neighbourhoods described below. kernel neighbourhood deﬁned gaussian function results classiﬁcation shown ﬁgure network trained epochs. increase size epochs shown improved results.the cutgaussian kernel shown better performance gaussian kernel. proposal future work combining techniques. combination techniques could yield better results. first train data yields clustered data data train label groups. signiﬁcantly decrease training period thus makes network perform faster.", "year": 2004}