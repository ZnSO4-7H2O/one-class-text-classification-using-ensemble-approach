{"title": "Optimal Sparse Linear Auto-Encoders and Sparse PCA", "tag": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "stat.CO", "stat.ML"], "abstract": "Principal components analysis (PCA) is the optimal linear auto-encoder of data, and it is often used to construct features. Enforcing sparsity on the principal components can promote better generalization, while improving the interpretability of the features. We study the problem of constructing optimal sparse linear auto-encoders. Two natural questions in such a setting are: i) Given a level of sparsity, what is the best approximation to PCA that can be achieved? ii) Are there low-order polynomial-time algorithms which can asymptotically achieve this optimal tradeoff between the sparsity and the approximation quality?  In this work, we answer both questions by giving efficient low-order polynomial-time algorithms for constructing asymptotically \\emph{optimal} linear auto-encoders (in particular, sparse features with near-PCA reconstruction error) and demonstrate the performance of our algorithms on real data.", "text": "singular value decomposition allows write uσvt columns rn×ρ left singular vectors columns rd×ρ right singular vectors rρ×ρ diagonal matrix positive singular values orthonormal golub loan integer rn×k ﬁrst left singular vectors rk×k diagonal matrix corresponding top-k singular values. view matrix columns. columns features reserve data points matrix -sparse kaik moreover equal matrix r-sparse. k-sparse unit vector magdon-ismail sparse special case generalized eigenvalue problem maximize subject generalized eigenvalue problem known np-hard moghaddam reduction sparse regression points factors correlated variance straightforward deﬁne optipistic estimate explainied variance. computes variance decorrelation quantify quality sparse pca. solution completely simplest algorithms greedy forward backward subset selection. example moghaddam develop greedy branch bound algorithm based spectral bounds running time forward selection running time backward selection. alternative view problem sparse problem considered additional non-negativity constraint; authors give algorithm takes input parameter running time constructs sparse solution -factor optimal. running time practical large approximation guarantee non-trivial spectrum rapidly decaying. modiﬁed accomodate case output fewer factors output encoding. step even though matrix rank hence dimensions depend lemma encoder produced algorithm r-sparse. also observe zeros located coordinates. compute running time algorithm. ﬁrst steps algorithm compute take time last step involves matrix multiplications done additional time also affect asymptotic running time or). show encoder produced algorithm good columns result good rank-k approximation xck. remains sampling matrix gives good columns xckk small. main tool obtain developed boutsidis gave constant factor deterministic approximation algorithm relative-error randomized approximation expectation respect random choices algorithm. using application markov’s inequality positive random variable xckk boosted high-probability least additional factor increase running time. entire algorithm de-randomized give deterministic algorithm using deterministic approximation ghashami phillips derandomization adaptive sampling step appeared recent result boutsidis woodruff tradeoff optimal construction given boutsidis woodruff constructs columns rn×r rows sampling matrices uφσφvt then encoder o-sparse decoder σφvt reconstruction xωuφσφvt contains rows reconstruction based linear combinations rows. theorem holds general linear auto-encoders lower bound also applies symmetric auto-encoder traditional formulation sparse pca. case r-sparse unit norm bvvtk explained variance btbv upper-bounded iterative algorithm control sparsity factor independently achieve desired approximation bound. recall encoder -sparse khik iterative algorithm summarized below. hand batch algorithm uses sparsity encoder achieves reconstruction error iterative algorithm uses sparser features pays little reconstruction error. additive term small depends practice smaller practice though theoretical bound iterative algorithm slightly worse batch algorithm guarantee iterative algorithm performs algorithm every encoder vector sparsity parameter reconstruction error means number non-zeros k/ε. iterative encoder ﬁrst encoder vectors sparse getting denser encoder vectors used iterated expectation. used lemma take expectation hℓ+. used deﬁnition qℓ+/qℓ induction hypothesis also observed that since xhℓ†x rank-k approximation bound follows eckart-young theorem xhℓ†xk hence inequality also holds expectation. used deﬁnition bound ˚eqproof compare empirical performance algorithms existing state-of-the-art sparse methods. inputs rn×d number components sparsity parameter output sparse encoder rn×k khik used project onto figure performance sparse encoder algorithms pitprops data lymphoma data colon data data ﬁgures show information loss symmetric explained variance observe algorithms give best information loss appears decreasing inversely theory predicts. existing sparse algorithms maximize symmetric explained variance surprisingly perform better respect symmetric explained variance. ﬁgures highlight information loss symmetric explained variance quite different metrics. argue information loss meaningful criterion optimize. observations measured variables. original dataset described jeffers colon gene-expression dataset alon here lymphoma gene-expression dataset alizadeh here matrices hence", "year": 2015}