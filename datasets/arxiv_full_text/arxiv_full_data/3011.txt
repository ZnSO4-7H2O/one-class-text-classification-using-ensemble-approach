{"title": "Interpretable Structure-Evolving LSTM", "tag": ["cs.CV", "cs.AI", "cs.LG"], "abstract": "This paper develops a general framework for learning interpretable data representation via Long Short-Term Memory (LSTM) recurrent neural networks over hierarchal graph structures. Instead of learning LSTM models over the pre-fixed structures, we propose to further learn the intermediate interpretable multi-level graph structures in a progressive and stochastic way from data during the LSTM network optimization. We thus call this model the structure-evolving LSTM. In particular, starting with an initial element-level graph representation where each node is a small data element, the structure-evolving LSTM gradually evolves the multi-level graph representations by stochastically merging the graph nodes with high compatibilities along the stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two connected nodes from their corresponding LSTM gate outputs, which is used to generate a merging probability. The candidate graph structures are accordingly generated where the nodes are grouped into cliques with their merging probabilities. We then produce the new graph structure with a Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in local optimums by stochastic sampling with an acceptance probability. Once a graph structure is accepted, a higher-level graph is then constructed by taking the partitioned cliques as its nodes. During the evolving process, representation becomes more abstracted in higher-levels where redundant information is filtered out, allowing more efficient propagation of long-range data dependencies. We evaluate the effectiveness of structure-evolving LSTM in the application of semantic object parsing and demonstrate its advantage over state-of-the-art LSTM models on standard benchmarks.", "text": "paper develops general framework learning interpretable data representation long short-term memory recurrent neural networks hierarchal graph structures. instead learning lstm models pre-ﬁxed structures propose learn intermediate interpretable multi-level graph structures progressive stochastic data lstm network optimization. thus call model structure-evolving lstm. particular starting initial element-level graph representation node small data element structure-evolving lstm gradually evolves multi-level graph representations stochastically merging graph nodes high compatibilities along stacked lstm layers. lstm layer estimate compatibility connected nodes corresponding lstm gate outputs used generate merging probability. candidate graph structures accordingly generated nodes grouped cliques merging probabilities. produce graph structure metropolis-hasting algorithm alleviates risk getting stuck local optimums stochastic sampling acceptance probability. graph structure accepted higher-level graph constructed taking partitioned cliques nodes. evolving process representation becomes abstracted higher-levels redundant information ﬁltered allowing efﬁcient propagation long-range data dependencies. evaluate effectiveness structure-evolving lstm application semantic object parsing demonstrate advantage state-of-the-art lstm models standard benchmarks. recently surge interest developing various kinds long short-term memory neural networks modeling complex dependencies within sequential multi-dimensional data advandespite remarkable success existing lstm models chain-structured tree-structured lstm models graph-structured lstm process data pre-ﬁxed structures terms internal information propagation route. therefore limited dealing data containing complex multilevel correlations. example structure human social network inherently hierarchical individual member several communities ranging small large semantic object parsing image another example beneﬁt modeling contextual dependencies among regions different levels lower-level graph representation small regions preserve local object boundaries higher-level graph larger coherent regions captures semantic interactions. thus order well abstract multi-level representations data desirable integrate data structure evolving lstm parameter learning. work seek general interpretable framework representing data lstm networks dynamically learned multi-level data structures hierarchical intrinsic representations simultaneously learned data along encoding longterm dependencies lstm units. since numerous important problems framed learning graph data structure-evolving directly investigates hierarchical representation learning initial arbitrary graph structures. however learning dynamic hierarchical graphs much challenging convenient hierarchical convolution neural networks arbitrary number nodes orderless node layouts diverse probabilistic graph edges. learn intermediate interpretable graph structures data alleviate over-ﬁtting problem design stochastic algorithm sample graph figure illustration structure evolving process proposed structure-evolving lstm model. starting initial graph structure-evolving lstm learns evolve hierarchical graph structures stochastic bottom-up node merging process propagates information generated multi-level graph topologies following stochastic node updating scheme. structure lstm layer gradually build multi-level graph representations bottom-up manner. thus name model structure-evolving lstm. compared existing lstm structures pre-ﬁxed chain tree graph topologies structure-evolving lstm capability modeling long-range interactions using dynamically evolved hierarchical graph topologies capture multi-level inherent correlations embedded data. illustrated fig. structure-evolving lstm gradually evolves multi-level graph representations stochastic bottom-up node merging process starting initial graph node indicates data element every neighboring nodes linked edge. enable learn interpretable hierarchical representation propose progressively merge different graph nodes guided global advantage reward step. graph composed merged graph nodes updated graph edges thus generated stochastic policy ensures less overhead graph transition previous graph graph advantage discriminative capability brought graph. merging probability estimated adaptive forget gate outputs lstm unit indicating likely nodes tend merged clique graph structure generated designing metropolis-hasting algorithm speciﬁcally algorithm stochastically merging graph nodes sampling merging probabilities produces graph structure structure examined determined according global reward deﬁned acceptance probability. stochastic sampling paradigm acceptance probability involves terms state transition probability posterior probability representing compatibility generated graph structure task-speciﬁc observations. intuitively global reward thus encourages structureevolving step better leads hugh graph shift also help boost target-speciﬁc performance. lstm layer broadcasts information along generated graph topology following stochastic updating scheme order enable global reasoning nodes. turn updated lstm gate outputs induce merging probability graph nodes subsequent graph structure evolving. instead inﬂuenced equally neighboring nodes lstm unit model learns adaptive forget gates neighboring node updating hidden states certain node. adaptive scheme advantage conveying semantically meaningful interactions graph nodes. network parameters updated back-propagation end-to-end way. leverage structure-evolving lstm model address fundamental semantic object parsing task experimentally show structure-evolving lstm outperforms state-of-the-art lstm structures three object parsing datasets. long short-term memory recurrent networks ﬁrst introduced address sequential prediction tasks extended multidimensional image processing tasks image generation person detection scene labeling object parsing keep long-term memory training proper gating weights practically showed effectiveness range problems image processing lstm unit prediction pixel designed affected ﬁxed factorization diagonal neighborhood recently tree-lstm introduces structure tree-structured topologies predicting semantic representations sentences. graph figure illustration stochastic structure-evolving step evolving lower-level graph higher-level one. given computed merging probabilities nodes structure-evolving step takes several trials evolve graph till graph accepted evaluated acceptance probability. graph generated stochastically merging nodes high predicted merging probabilities thus edges produced. acceptance probabilities computed considering graph transition cost advantage discriminative capability brought graph. lstm proposed propagate information basic pre-deﬁned graph topology capture diverse natural visual correlations however complex patterns different modalities often embed hierarchal structures representing different levels correlations nodes. different using pre-deﬁned data structures previous lstms proposed structureevolving lstm targets automatically learning hierarchical graph representations evolving initial graph structure. intrinsic multi-level semantic abstractions learned used boost multi-scale reasoning lstm units. structure-evolving lstm superior related graph lstm aspects structure-evolving lstm learns powerful representations progressively exploits hierarchical information along stacked lstm layers; later layers structure-evolving lstm captures inherent structure desired output beneﬁting higher-level graph topologies. superiorities bring signiﬁcant improvements several semantic parsing datasets gives apple-to-apple comparison work aims develop general principled graph evolving based learning method learn powerful graph lstm models. devising graph-lstm unit within scope paper. graph-lstm running example means implies method limited graph lstm. fig. illustrates proposed structure-evolving lstm network architecture. suppose initialized graph data denoted corresponding graph nodes ments) edges. node represented deep features learned underlying model dimensions. based lstm gate outputs graph previous t-th lstm layer structure-evolving lstm learns higherlevel graph structure information propagation lstm layer. learning graph structures updating lstm parameters thus alternatively performed network parameters trained end-to-end way. given dynamically constructed graph structure t-th structure-evolving lstm layer determines states node comprise hidden states memory states node edge probability nodes evolving graph structure. state node inﬂuenced previous states states connected graph nodes order propagate information nodes. thus inputs lstm units consist previous hidden input states states hidden memory states neighboring nodes note ﬂexibility order node updating structure-evolving lstm layers. following randomly specify node updating sequence propagate information nodes order increase model diversity learning lstm network parameters. structure-evolving lstm follows graph lstm units generate hidden memory cells show inject edge merging probabilities nodes lstm units. thus ﬁrst introduce generation hidden memory cells make paper self-contained. operating speciﬁc node neighboring nodes already updated others not. therefore visit indicate whether graph node updated otherwise updated hidden states visited nodes previous states logistic sigmoid function indicates point-wise product. denote concatenation weight matrices {zjt}j∈ng represent related information neighboring nodes. mechanism acts memory system information written memory states sequentially recorded graph node used communicate hidden states subsequent graph nodes previous lstm layer. merging probabilities {pij} conveniently learned used generating higher-level graph structure layer detailed section training merging probabilities graph edges supervised approximating ﬁnal graph structure speciﬁc task connections ﬁnal semantic regions image parsing. back propagation used train weight metrics. given graph structure merging probabilities {pij} higherlevel graph structure evolved stochastically merging graph nodes examined acceptance probability shown fig. speciﬁcally graph node constructed merging graph nodes merging probability. deterministic graph transition path initial graph ﬁnal intractable enumerate possible evaluation within large search space. thus stochastic mechanism rather deterministic good graph transition. stochastic searching scheme also effective alleviating risk getting trapped local optimum. better graph transition between graphs acceptance rate transition graph graph deﬁned metropolis-hastings method denote graph state transition graph another denote posterior probability graph structure respectively. typically assumed follow gibbs distribution partition function network prediction task-speciﬁc target corresponding loss function. example segmentation groundtruth pixel-wise cross-entropy loss unvisited nodes. note nodes graph arbitrary number neighboring nodes. denote number neighboring graph nodes node obtain ﬁxed feature dimension inputs graph lstm unit network training hidden states ¯ht− used computing lstm gates node obtained averaging hidden states neighboring nodes computed structure-evolving lstm. structure-evolving input gate forlstm consists gates gate adaptive forget gate memory gate output gate edge gate indicator function. indicates recurrent edge gate weight parameters. recurrent gate weight matrices speciﬁed input features hidden states node. weight parameters speciﬁed states neighboring nodes. structureevolving lstm unit speciﬁes different forget gates different neighboring nodes functioning input states current node hidden states deﬁned results different inﬂuences neighboring nodes updated memory states hidden states merging probability pair graph nodes calculated weighting adaptive forget gates weight matrix intuitively adaptive forget gates identify distinguished correlations different node pairs e.g. nodes stronger correlations others. merging probability pair thus estimated adaptive forget gates graph evolving. hidden states memory states edge gates graph calculated follows figure overview segmentation network architecture employs structure-evolving lstm layer semantic object parsing image domain. based basic convolutional feature maps structure-evolving lstm layers stacked propagate information stochastically generated multi-level graph structures constructed superpixel neighborhood graph. convolutional layers appended lstm layers produce multi-scale predictions combined generate ﬁnal result. image parsing task. model likely accept graph structure bring signiﬁcant performance improvement indicated |i;wu) |i;wu) graph state transition probability ratio computed state transition probability thus calculated multiplying merging probabilities eliminated edges implies graph nodes larger merging probabilities encouraged merged testing acceptance rate determined graph state transition probability eqn. enable ﬁnish graph structure exploration within speciﬁed time schedule step empirically upper bound sampling trials experiments. structure-evolving lstm layer information propagation performed nodes stochastic node updating sequence along graph topology input states produced averaging corresponding merged nodes similarly hidden memory states averaged used updating. weight matrices structureevolving lstm units shared stacked layers generated hierarchical graph representations helps improve capability network parameters sensing multi-level semantic abstractions. ﬁnal loss training structure-evolving lstm includes ﬁnal task-related proposed structure-evolving lstm aims provide principled framework dynamically learn hierarchal data structures applicable kinds tasks however among applications semantic object parsing task requires produce pixel-wise labeling considering complex interactions different pixels superpixels parts perfect match better evaluate structure generation capability structure-evolving lstm. dynamically evolved hierarchical graph structures effectively capture multi-level diverse contextual dependencies. thus evaluate effectiveness proposed structureevolving lstm model semantic object parsing task exploiting multi-level graph representations image content natural useful ﬁnal parsing result. semantic object parsing task take object parsing task application scenario aims generate pixel-wise semantic part segmentation image shown fig. initial graph constructed superpixels obtained image over-segmentation using slic following superpixel indicates graph node graph edge connects spatially neighboring superpixel nodes. input image ﬁrst passes stack convolutional layers generatt convolutional feature maps. input features graph node computed averaging convolutional features table comparison semantic object parsing performance several state-of-the-art methods pascal-person-part dataset variants structure-evolving lstm model including using different lstm structures extracted multi-scale superpixel maps deterministic policy different thresholds graph transition respectively. pixels belonging superpixel node five structure-evolving lstm layers stacked learn multi-level graph representations stochastically grouping nodes large node coherent semantic meanings bottom-up process. make sure number input states ﬁrst lstm layer compatible following layers dimensions hidden memory states lstm layers feature dimension last convolutional layer lstm stack. that prediction layer several convolution ﬁlters produces conﬁdence maps labels. training groundtruth semantic edge deﬁned superpixels supervise prediction merging probabilities edges lstm layer. specifically ground-truth merging probability graph nodes belong semantic label. l-norm loss employed back-propagation. cross-entropy loss employed predictions layers produce ﬁnal parsing result. dataset validate effectiveness structureevolving lstm three challenging image parsing datasets. pascal-person-part dataset concentrates human part segmentation images pascal semantic labels consist head torso upper/lower arms upper/lower legs background class. images used training testing. horse-cow parsing dataset part segmentation benchmark introduced includes training images testing images pixel labeled head tail body. third task human parsing aims predict every pixel labels face sunglass scarf hair upper-clothes left-arm right-arm belt pants left-leg right-leg skirt left-shoe right-shoe dress null. originally images included dataset training testing validation. images collected cover images challenging poses clothes variations. evaluation metric standard intersection union criterion pixel-wise accuracy adopted evaluation pascal-person-part dataset horsecow parsing dataset following evaluation metrics evaluation human parsing dataset including accuracy average precision average recall average score. network architecture fair comparison network based publicly available model deeplab-crf-largefov pascal-personpart horse-cow parsing dataset slightly modiﬁes vgg- co-cnn structure used compare human parsing datasets fair comparison. training slic over-segmentation method generates superpixels average image. learning rate newly added layers pre-trained models initialized previously learned layers initialized weight matrices used structure-evolving lstm units randomly initialized uniform distribution lstm layers models since slight improvements observed using lstm layers also consumes computation resources. weights convolutional layers initialized gaussian distribution standard deviation train models using stochastic gradient descent batch size image momentum weight decay ﬁne-tune networks deeplabcrf-largefov train networks based cocnn scratch roughly epochs. structureevolving lstm implemented extending caffe framework networks trained single nvidia geforce titan memory. testing stage extracting superpixels takes method takes image total. comparisons state-of-the-art methods. report result comparisons recent state-of-the-art methods pascal-person-part dataset horse-cow parsing dataset dataset table table table respectively. proposed structure-evolving lstm structure substantially outperforms baselines terms metrics especially small semantic parts. superior performance achieved structureevolving lstm demonstrates effectiveness capturing multi-scale context propagating information gentable performance comparison state-of-the-art methods evaluating dataset following also take additional images extra training images denoted ours comparison human parsing performance seven state-of-the-art methods evaluating dataset. method comparisons existing lstm structures. table gives performance comparison among different lstm structures including lstm diagonal bilstm lg-lstm grid lstm graph lstm network architecture number lstm layers. particular lstm diagonal bilstm lg-lstm grid lstm lg-lstm ﬁxed locally factorized topology images graph lstm propagates information ﬁxed superpixel graph. seen exploiting multi-level graph representations different lstm layers leads improvement pre-deﬁned lstm structures average iou. discussion using stochastic policy. note structure-evolving lstm stochastically merges graph nodes employs acceptance rate determine whether graph structure accepted. alternative deterministically merging graph nodes hardthresholding nodes merged merging probability larger ﬁxed threshold experiment three thresholds tested table using smaller threshold likely obtain aggressive graph transitions merging nodes larger threshold would prevent graph changing structure. shown using figure comparison parsing results structure-evolving lstm graph lstm dataset visualization corresponding generated multi-level graph structures. better viewed zoomed-in color pdf. threshold deterministic policy obtains best performance still inferior proposed stochastic policy. additionally slight performance differences obtained running feed-forward prediction using structure-evolving lstm times veriﬁes robustness structure-evolving lstm. comparisons using pre-deﬁned graph structures. optional strategy capture multi-scale context utilize pre-computed multi-scale superpixel maps intermediate graph structures reported graph lstm table five predeﬁned graph structures lstm layers constructed superpixel maps superpixels respectively. superpixel numbers consistent averaged node number learned graph structures training images. superiority structure-evolving lstm demonstrates exploiting adaptive graph structures makes structure consistent high-level semantic representation instead relying bottom-up oversegmentation. discussion predictions different levels graphs. performance using different numbers structure-evolving lstm layers reported table demonstrates exploiting levels graph structures makes network parameters learn different levels semantic abstraction leading better parsing results whereas previous lstm model reported performance gain achieved lstm layers. note parsing prediction produced lstm layer predictions element-wisely summed generate ﬁnal result. individual parsing performance using graph structure reported table higher-level graph structure wrongly merge bottom-up graph nodes thus lead deteriorated performance. however combining predictions structure-evolving lstm layers largely boost prediction beneﬁted incorporating multiscale semantical context. visualization. qualitative comparisons parsing results dataset graph structures exploited structure-evolving lstm layers visualized fig. structure-evolving lstm outputs reasonable results confusing labels effectively exploiting multi-scale context generated multi-level graph structures. presented novel interpretable structure-evolving graph lstm simultaneously learns multi-level graph representations data lstm network parameters end-to-end way. following line graph-based rnns work signiﬁcantly improves network learning allowing underlying multi-level graph structures evolve along parameter learning. network thus learn representations better hidden structure data. moreover propose principled approach evolve graph structures stochastically straightforward could potential impact application graph-based rnns multiple domains. demonstrated effectiveness object parsing task image. structure-evolving lstm extended enable reversible graph transition lstm network optimization. also evaluate performance tasks modalities social networks.", "year": 2017}