{"title": "On Local Optima in Learning Bayesian Networks", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "This paper proposes and evaluates the k-greedy equivalence search algorithm (KES) for learning Bayesian networks (BNs) from complete data. The main characteristic of KES is that it allows a trade-off between greediness and randomness, thus exploring different good local optima. When greediness is set at maximum, KES corresponds to the greedy equivalence search algorithm (GES). When greediness is kept at minimum, we prove that under mild assumptions KES asymptotically returns any inclusion optimal BN with nonzero probability. Experimental results for both synthetic and real data are reported showing that KES often finds a better local optima than GES. Moreover, we use KES to experimentally confirm that the number of different local optima is often huge.", "text": "paper proposes evaluates k-greedy learning bayesian networks complete data. main characteristic allows trade-off greediness different edly. greediness corresponds search algorithm kept minimum prove mild conditions inclusion synthetic ity. experimental results real data reported showing finds better local optimum con­ siderably illustrate optima usually huge. learning data widely studied last years. approaches learning independence constraints searches space models using score. frame­ algorithms works asymptotically developed assumption algorithm based approach greedy equivalence gorithm search algorithm neighbor terion asymptotically ness assumption. several assumptions obtain optimality strictly neighborhood faith­ fulness assumption tion namely composition equivalent reversals inclusion tion includes model strictly included throughout paper symbol denotes nonempty finite discrete variables iden­ tified nodes graphs. directed nodes specified i.e. directed cycle formed arcs directed node parent parents denoted graph clear context only. union node parents called family ={x} x--> covered every order nodes exists causal precedes denote nodes precede node causal order. joint probability satisfy certain conditional straints means d-separation globally definition butions satisfy pendence enforced called faithful tional independencies nodes enforced causal order joint probabil­ distribution pendencies property included conditional bility distribution iality decomposition .llyuiz jlyizu contraction jlyizu yuiz. note contraction weak union decomposition block independence lemma yuiz. addition tribution distributions) satisfies jlyiz jluiz .llyuiz. uses data select among different scoring criterion model sometimes score representative model too. scoring criterion value dags representing scoring criterion lent. scoring assigned joint probability ways increases removal adds conditional inclusion boundary union lower upper inclusion models strictly model strictly strictly models strictly include including strictly model strictly characterized models represented obtained adding removing single equivalent plex characterization model enables efficient generation inclusion boundary chickering meek undirected graphical distributions satisfy property exhibit inclusion els. extend particular parameterization asymptotically totically version paper considers step whole inclusion boundary current best model described two-phase upper inclusion lower inclusion versions algorithms usually proceed exactly practice. thus conceptually spite original performs slightly original first phase. whole upper inclusion single pair non-adjacent model inclusion opti­ follows node condi­ tional independence enforced hold moreover pre\\paa thus holds enforced d-separation too. note causal order used causal order well. possible then pre\\pac proof high level idea behind proof the­ orem another model shown thus algorithm increases score step stops model higher score current best model always finds model holds finite number different score step prevents models increasing algorithm model twice. obvious proof applies equally well first derived. mentioned implementation described model iteration. risk draw specified models step parameter ternally translated mean percentage distinct sampled ·iib)i note order guarantee behaves practice i.e. best scoring model selected itera­ tion must equal infinity course imprac­ tical. checked experiments used sufficiently exact value iib)i arcs added empty number graph proximation fact connectivity components graphs representing recall section first generates models better then samples random subset defined size. cannot produce effi­ ciently random sample defined size then select models better sample i.e. swap practice first steps kes. working impact implementation stopping cri­ terion instead halting model higher score stops practice sufficiently fortunately member nonzero proba­ bility covered reverse repeat covered reversal several times then choose random pair nodes either remove without creating descendant need generate aware using essential models search space avoid many con­ implementation sequently kes. however approach discarded higher complexity. paper effectiveness kes. finally good idea implementation learning purpose avoiding computing once. particularly plementation anism afford generating many times occur implementation data first time repeated computations cache. specifically tree-like family i.e. take advantage posability cache tree stores score node specified first level branch given parents specified branch. mentioned arbitrary parents always ordered access family unique entry cache. relates cache tree vice versa. structure cache described r-tree index fixed reasonably cache tree needed allocated mem­ ory. cache provides computing score data. acute myeloid leukemia finally databases cases measurements sponding patients databases tively tioned cases treated i.i.d. thus cases correlated. analysis sults. fact approach commonly taken gene expression first synthetic widely studied alarm database sample characterized genes follows. discretized theory based method then dis­ cretized containing acute lymphoblastic containing figure illustrates data different values parameter considered. sorted ascending independent corresponds first conclusion performs poorly. reason somewhat deceptive exist many locally hand making reaching runs tween runs identify model selected results suggest probability locally returned considerable specially beats converge different models. therefore misses best model four five databases involved evaluation several models superior servations greediness able suboptimal performance again many locally optimal exist even data faithful going back alarm database surpris­ effective faithful alarm database asymptotically results suggest cases alarm enough guarantee matter fact returned equiva­ lent generative supported trap database outperform database generated superior incorporating learning returned ges. specifically runs data identify different locally make graph left figure increasing graph right graph left shows true extent outperforms significantly runs respectively. greediness result gain effectiveness based solely space restrictions effectiveness rest databases shown. instead five databases ered. unless otherwise stated discussion table reports highest value scored indepen­ induced algorithm dent runs best number runs alternatively worse <ges terms values induced models. additionally include parenthesis identified runs. table outperforms databases ularly encouraging relative regarding databases reader evidence independent distinct databases coil trap. obviously figures number local optima therefore models actually moreover amount available number locally results large sample assumption unbeatable different locally optimal models", "year": 2012}