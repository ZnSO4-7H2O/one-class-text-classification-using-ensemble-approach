{"title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement  Learning", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.", "text": "abstract—the recent advances deep neural networks effective vision-based reinforcement learning methods employed obtain human-level controllers atari games pixel data. atari games however resemble real-world tasks since involve non-realistic environments third-person perspective. here propose novel test-bed platform reinforcement learning research visual information employs ﬁrstperson perspective semi-realistic world. software called vizdoom based classical ﬁrst-person shooter video game doom. allows developing bots play game using screen buffer. vizdoom lightweight fast highly customizable convenient mechanism user scenarios. experimental part test environment trying learn bots scenarios basic move-and-shoot task complex maze-navigation problem. using convolutional deep neural networks q-learning experience replay scenarios able train competent bots exhibit human-like behaviors. results conﬁrm utility vizdoom research platform imply visual reinforcement learning realistic ﬁrst-person perspective environments feasible. keywords video games visual-based reinforcement learning deep reinforcement learning ﬁrst-person perspective games visual learning neural networks visual signals primary sources information surrounding environment living artiﬁcial beings. computers already exceeded humans terms data processing still match ability interact complex realistic environments. recent increase computing power advances visual learning enabled signiﬁcant progress area. possible thanks renaissance neural networks deep architectures particular. deep learning applied many supervised machine learning tasks performed spectacularly well especially ﬁeld image classiﬁcation recently deep architectures also successfully employed reinforcement learning domain train human-level agents play atari games pixel information thanks high recognizability easy-to-use software toolkit atari games widely adopted benchmark visual learning algorithms. atari games have however several drawbacks research perspective. first involve environments. second environments hardly resemble world live third third-person perspective games match real-world mobile-robot scenario. last least although atari games human players still ahead bots trained scratch best deep reinforcement learning algorithms already ahead average. therefore need challenging reinforcement learning problems involving ﬁrst-person-perspective realistic worlds. paper propose software platform vizdoom machine learning research visual information. environment based doom famous ﬁrst-person shooter video game. allows developing bots play doom using screen buffer. environment involves world signiﬁcantly real-world-like atari games. also provides relatively realistic physics model. agent vizdoom effectively perceive interpret learn world order make tactical strategic decisions act. strength environment research platform also lies customization capabilities. platform makes easy deﬁne custom scenarios differ maps environment elements non-player characters rewards goals actions available agent. also lightweight modern computers play game nearly frames second using single core particular importance learning involved. order demonstrate usability platform perform vizdoom experiments deep q-learning ﬁrst involves somewhat limited d-like environment optimal rate agents make decisions. second experiment agent navigate maze collecting object omitting others. results experiments indicate deep reinforcement learning capable tackling ﬁrst-person perspective environments. games especially popular ones unreal tournament counter-strike quake arena already used research. however studies agents acted upon high-level information like positions walls enemies locations items etc. usually inaccessible human players. supplying visual information might relieve researchers burden providing high-level information handcrafted features. also hypothesize could make agents behave believable studies reinforcement learning visual information obtained games. date fps-based environments allow research agents relying exclusively visual information. could serious factor impeding progress vision-based reinforcement learning since engaging requires large amount programming work. existence ready-to-use tool facilitates conducting experiments focusing goal research. earliest works visual-based reinforcement learning asada trained robots various elementary soccer-playing skills. works area include teaching mobile robots visual-based qlearning learning policies deep auto-encoders batch-mode algorithms neuroevolution vision-based version mountain problem compressed neuroevolution recurrent neural networks vision-based simulator recently mnih shown deep q-learning method learning atari games visual input different ﬁrst-person shooter video games already used either research platforms application domains. ﬁrst academic work games geisler concerned modeling player behavior soldier fortune cole used genetic algorithms tune bots counter strike dawes identiﬁed unreal tournament potential research test-bed. rhalib studied weapon selection quake arena smith devised retaliate reinforcement learning algorithm optimizing team tactics unreal tournament sarsa another reinforcement learning method subject research games recently continuous reinforcement learning techniques applied learn behavior tanks game bzflag studies employed genre-classical doom fps. also previous study used visual information develop bots ﬁrst-person perspective games notable exception abel’s work minecraft creating another ﬁrst-person perspective environment scratch solely research purposes would somewhat wasteful popularity ﬁrstperson shooter genre decided existing game engine base environment. concluded meet following requirements multi-platform. order make decision according above-listed criteria analyzed seven recognizable games quake arena doom half-life unreal tournament unreal tournament cube. comparison shown table features listed table objective others subjective brand recognition estimated number google results phrases game <gamename> <gamename> ‘doom’ ‘quake’ ‘half-life’ ‘unreal tournament’ ‘cube’. game considered low-resolution capable possible resolution values smaller games rejected right away spite high general appeal. unreal tournament engine accessible software development lacks support controlling speed execution direct screen buffer access. game prepared heavily modiﬁed. similar problems shared half-life despite fact source engine widely known modding capabilities. also lacks direct multiplayer support. although source engine offers multiplayer support involves client-server architecture makes synchronization direct interaction engine problematic also great capabilities. despite active community availability source code rejected high system requirements. games e.g. off-screen rendering custom rewards. game highly recognizable runs three major operating systems. also designed work resolution despite fact modern implementations allow bigger resolutions still utilizes low-resolution textures. moreover source code easy-to-understand. unique feature doom software renderer. because that could without desktop environment accessing screen buffer require transferring graphics card. vizdoom ﬂexible easy-to-use. designed reinforcement apprenticeship learning mind therefore provides full control underlying doom process. particular allows retrieving game’s screen buffer make actions correspond keyboard buttons mouse actions. game state variables player’s health ammunition available directly. vizdoom’s written c++. offers myriad conﬁguration options control modes rendering options. addition support bindings python java provided. python example shown fig. getting hurt picking object). mechanism opens endless experimentation possibilities. particular allows creating scenario difﬁculty capabilities assessed learning algorithms. creation scenarios possible thanks easy-to-use software tools developed doom community. recommended free tools include doom builder slade visual editors allow deﬁning custom maps coding game mechanics action code script. also enable conveniently test scenario without leaving editor. depth buffer access vizdoom provides access renderer’s depth buffer help agent understand received visual information. feature gives opportunity test whether learning algorithms autonomously learn whereabouts objects environment. depth information also used simulate distance sensors common mobile robots. off-screen rendering frame skipping facilitate computationally heavy machine learning experiments equipped vizdoom off-screen rendering frame skipping features. off-screen rendering lessens performance burden actually showing game screen makes possible experiments servers frame skipping hand allows omitting rendering selected frames all. intuitively effective every single frame. explore issue experimentally section main factors affecting vizdoom performance number actors rendering resolution computing depth buffer. fig. shows number frames second depends factors. tests made synchronous player mode linux running intel core i-k. vizdoom uses single core. screen buffer game variables game.get_state s.image_buffer misc s.game_variables perform random action action choice reward game.make_action something reward... vizdoom provides features exploited different kinds experiments. main features include different control modes custom scenarios access depth buffer off-screen rendering eliminating need using graphical interface. control modes vizdoom implements four control modes synchronous player synchronous spectator iii) asynchronous player asynchronous spectator. asynchronous modes game runs constant frames second agent reacts slowly miss frames. conversely makes decision quickly blocked next frame arrives engine. thus reinforcement learning research useful synchronous modes game engine waits decision maker. learning system learn pace limited temporal constraints. scenarios important features vizdoom ability custom scenarios. includes creating appropriate maps programming environment mechanics deﬁning terminal conditions rewards deep q-learning learning procedure similar deep q-learning introduced atari problem modeled markov decision process qlearning used learn policy. action selected \u0001-greedy policy linear decay. q-function approximated convolutional neural network trained stochastic gradient decent. also used experience replay target network freezing neural network architecture network used experiment consists convolutional layers square ﬁlters pixels wide respectively convolution layer followed max-pooling layer pooling size rectiﬁed linear units activation next fully-connected layer leaky rectiﬁed linear units output layer linear units corresponding combinations available actions game settings state represented recent frame -channel image. number skipped frames controlled skipcount parameter. experimented skipcounts important note agent repeats last decision skipped frames. learning settings arbitrarily discount factor learning rate replay memory capacity elements mini-batch size initial starts decay learning steps ﬁnishing decay learning steps. every agent learned steps consisting performing action observing transition updating network. monitor learning progress testing episodes played learning steps. final controllers evaluated episodes. experiment performed intel core geforce handled neural network. results figure shows learning dynamics selected skipcounts. demonstrates although agents improve time skips inﬂuence learning speed smoothness well ﬁnal performance. agent skip frames learning slowest. generally larger skipcount faster smoother learning also observed agents learning higher skipcounts less prone irrational behaviors like staying idle going direction opposite monster results lower variance plots. hand large skipcounts make agent ‘clumsy’ performance test shows vizdoom render nearly low-resolution frames second. rendering resolution proves important factor inﬂuencing processing speed. case resolutions time needed render frame negligible compared backpropagation time reasonably complex neural network. primary purpose experiment show reinforcement learning visual input feasible vizdoom. additionally experiment investigates number skipped frames inﬂuences learning process. scenario simple scenario takes place rectangular chamber agent spawned center room’s longer wall. stationary monster spawned random position along opposite wall. agent strafe left right shoot. single enough kill detailed results shown table indicate optimal skipcount scenario however higher values close maximum. also checked robust skipcounts agents are. purpose evaluated using skipcounts different ones trained with. agents performed worse native skipcounts. least robust agents trained skipcounts less larger skipcounts resulted robust agents. interestingly skipcounts greater equal agents score better skipcounts lower native ones. best agent trained skipcount also best executed skipcount also worth showing increasing skipcount inﬂuences total learning time slightly. learning takes longer primarily higher total overhead associated episode restarts since higher skipcounts result greater number episodes. skipcounts range provide best balance learning speed ﬁnal performance. results also indicate would profitable start learning high skipcounts exploit steepest learning curve gradually decrease ﬁne-tune performance. previous experiment conducted simple scenario closer arcade game rather true virtual world. decided test similar deep reinforcement learning methods would work involved scenario requiring substantial spatial reasoning. scenario scenario agent spawned random spot maze acid surface slowly constantly takes away agent’s life survive agent needs collect medikits avoid blue vials poison. items types appear random places episode. agent allowed move turn scores point tick punished points dying. thus motivated survive long possible. facilitate learning also introduced shaping rewards points collecting medikit vial respectively. shaping rewards count ﬁnal score used agent’s training helping ‘understand’ goal. episode ends ticks agent dies maximum achievable score. idle results scoring points. neural network architecture employed network similar used previous experiment. differences follows. involves three convolutional layers square ﬁlters pixels wide respectively. fullyconnected layer uses leaky rectiﬁed linear units output layer linear units corresponding combination available actions. game settings game’s state represented -channel image health points current tick number additionally kind memory implemented making agent last states neural network’s input. nonvisual inputs directly ﬁrst fully-connected layer. skipcount used. learning settings discount factor learning rate replay memory capacity elements mini-batch size initial started decay learning steps ﬁnishing decay episodes. agent learn steps. monitor learning progress testing episodes played learning steps. whole learning process including testing episodes lasted hours. results learning dynamics shown fig. observed agents fairly quickly learns perfect score time time. average score however improves slowly reaching learning. trend might however suggest improvement still possible given training time. plots suggest even learning agent initial states fails live random player. watching agent play revealed developed policy consistent expectations. navigates towards medikits actively although deftly avoids poison vials push walls corners. also backpedals reaching dead poison vial. however often hesitates choosing direction results turning left right alternately spot. quirky behavior probable direct cause fully satisfactory performance. vizdoom doom-based platform research visionbased reinforcement learning. easy-to-use highly ﬂexible multi-platform lightweight efﬁcient. contrast popular visual learning environments atari vizdoom provides semi-realistic ﬁrst-person perspective virtual world. vizdoom’s gives user full control environment. multiple modes operation facilitate experimentation different learning paradigms reinforcement learning apprenticeship learning learning demonstration even ‘ordinary’ supervised learning. strength versatility environment customizability mechanism scenarios conveniently programmed open-source tools. also demonstrated visual reinforcement learning possible virtual environment vizdoom performing experiments deep q-learning scenarios. results simple move-and-shoot scenario indicate speed learning system highly depends number frames agent allowed skip learning. found proﬁtable skip frames. used knowledge second involved scenario agent navigate hostile maze collect items avoid others. although agent able perfect strategy learned navigate maze surprisingly well exhibiting evidence human-like behavior. vizdoom recently reached stable version potential extended many interesting directions. first would like implement synchronous multiplayer mode would convenient self-learning multiplayer settings. second bots deaf thus plan allow bots access sound buffer. lastly interesting supervised learning experiments could conducted vizdoom automatically labeled objects scene. work supported part polish national science centre grant dec-//d/st/. kempka acknowledges support ministry science higher education grant //dspb/. david abel alekh agarwal fernando diaz akshay krishnamurthy robert schapire. exploratory gradient boosting reinforcement learning complex domains. corr abs/. minoru asada shoichi noda sukoya tawaratsumida hosoda. purposive behavior acquisition real robot vision-based reinforcement learning. recent advances robot learning pages springer minoru asada eiji uchibe shoichi noda sukoya tawaratsumida hosoda. vision-based reinforcement learning coordination soccer playing behaviors. proceedings aaai- workshop a-life entertainment pages nicholas cole sushil louis chris miles. using genetic algoevolutionary computation rithm tune ﬁrst-person shooter bots. cec. congress volume pages ieee giuseppe cuccu matthew luciw j¨urgen schmidhuber faustino gomez. intrinsically motivated neuroevolution vision-based reinforcement learning. development learning ieee international conference volume pages ieee mark dawes richard hall. towards using ﬁrst-person shooter knowledgecomputer games artiﬁcial intelligence testbed. based intelligent information engineering systems pages springer esparcia-alcazar martinez-garcia mora merelo garcia-sanchez. controlling bots first person shooter game evolutionary computation using genetic algorithms. ieee congress pages chris gaskett luke fletcher alexander zelinsky. reinforcement intelligent robots learning vision based mobile robot. systems proceedings. ieee/rsj international conference volume pages ieee benjamin geisler. empirical study machine learning algorithms applied modeling player behavior ﬁrst person shooter video game. thesis university wisconsin-madison glavin madden. dre-bot hierarchical first person shooter using multiple sarsa reinforcement learners. computer games international conference pages glavin madden. adaptive shooting bots first person shooter games using reinforcement learning. computational intelligence games ieee transactions xavier glorot antoine bordes yoshua bengio. deep sparse rectiﬁer neural networks. geoffrey gordon david dunson editors proceedings fourteenth international conference artiﬁcial intelligence statistics volume pages journal machine learning research workshop conference proceedings hladky bulitko. evaluation models predicting opponent positions ﬁrst-person shooter video games. computational intelligence games ieee symposium pages koutn´ık j¨urgen schmidhuber faustino gomez. evolving deep unsupervised convolutional networks vision-based reinforcement conference genetic learning. evolutionary computation pages imagenet classiﬁcation deep convolutional neural networks. pereira c.j.c. burges bottou k.q. weinberger editors advances neural information processing systems pages curran associates inc. volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski stig petersen charles beattie amir sadik ioannis antonoglou helen king dharshan kumaran daan wierstra shane legg demis hassabis. human-level control deep reinforcement learning. nature megan smith stephen lee-urban h´ector mu˜noz-avila. retaliate learning winning policies ﬁrst-person shooter games. proceedings national conference artiﬁcial intelligence volume page menlo park cambridge london; aaai press; press;", "year": 2016}