{"title": "A Note on Information-Directed Sampling and Thompson Sampling", "tag": ["cs.LG", "cs.AI"], "abstract": "This note introduce three Bayesian style Multi-armed bandit algorithms: Information-directed sampling, Thompson Sampling and Generalized Thompson Sampling. The goal is to give an intuitive explanation for these three algorithms and their regret bounds, and provide some derivations that are omitted in the original papers.", "text": "sampling thompson sampling generalized thompson sampling. goal give intuitive explanation three algorithms regret bounds provide derivations omitted original papers. multi-armed bandit problem sequential decision making problem. time learner selects action based current knowledge arm-selection policy receives reward action selected. since rewards actions selected unknown learner needs balance exploit current knowledge select best explore potential best arms. note describe three bayesian style multi-armed bandit algorithms information-directed sampling thompson sampling generalized thompson sampling. three algorithms maintains posterior distribution indicating probability arm/policy optimal. however diﬀerent rules update posterior distribution based observed rewards. problem. setting actions time decision-maker assume chooses action action draws reward reward distribution rewards i.i.d distributed reward distribution stationary respect time formulate multi-armed bandit bayesian denote maxa∈a era∼pa means highest expected reward respect distribution also denote reward drawn decision-maker know real original paper assume arms ﬁrst draw outcome outcome distribution ﬁxed known function maps outcomes rewards. however sake simplicity assume outcome equal reward. reward distribution estimate distributions time step denote ˆpat. uncertainly action time decision-maker believe whether action highest expected reward. denote believe instead sampling actions directly based posterior distribution sample actions based distribution also distribution actions constructed based posterior distribution interested following expected regret instead pure exploitation using immediate regret would want exploration seek potential best arms. this deﬁned term information gain denoted idea that already posterior distribution hope pull arms entropy distribution decreases gain certain amount information denote entropy equation above ˆpat reward posterior distribution time ˆpat reward posterior distribution conditioned highest mean reward. condition reward posterior distribution shift satisfy constrain. example figure show arms mean reward gaussian distribution suppose want calculate reward posterior distribution conditioned highest mean reward. examine point mean reward mean reward cannot greater probability mass arms greater remaining normalized. author stated sparse non-zero elements possible combinations arms gives lowest given sample pull arm. omit detail since it’s well described paper. k-armed bernoulli bandit problem arms reward i-th follows bernoulli distribution mean bayesian style learning algorithm standard model mean reward using beta distribution showed strong empirical results however several potential problems. think main problem algorithm time consuming reason integral calculate evaluate integrand discrete grid points. another problem paper didn’t mention choose format trade-oﬀ version. again assume action time step thompson sampling select action reward rat. also assume reward follows parametric distribution mean parameter. deﬁne past observations consists arms pulled rewards observed. beginning thompson sampling assumes prior distribution parameters time step update posterior distribution based past observations. similar goal minimize regret essential ids. however calculating time consuming since thompson sampling need explicitly need samples suﬃces draw random parameter posterior distribution. algorithm describes procedure thompson sampling bernoulli bandit problem. binomial distribution parameters deﬁne beta beta distribution parameters denote played time denotes number plays time denote number successes among plays bernoulli bandit case denote empirical mean denote sample mean reward time assume ﬁrst unique optimal i.e˙ choose thresholds diﬀerent choices problem dependent problem independent bound respectively. also deﬁne event finally reduced problem bounding problem bounding summation series random variables involving binomial distribution. provide details bound quite complicated. framework include thompson sampling special case. context arms |a|. time step decision-maker observes context selects receives reward expectation reward binary easy generalize continuous space. diﬀerent classic thompson sampling algorithm generalized thompson sampling allows decision-maker ‘generalized thompson sampling’ means diﬀerent types loss functions updating described loss functions logarithmic loss square loss. logarithmic loss deﬁned square loss deﬁned next section show loss function logarithmic loss generalized thompson sampling format expert exponential weighting however also thompson sampling framework ways this ways need assume loss loss expert predicts probability ﬁrst this think generalized thompson sampling maintaining posterior distribution weight expert denoted posterior distribution interpreted posterior probability reward-maximizing expert. update rule step average shifted loss ertathpi witˆlii assume constant kp¯lt. also make self-boundedness property loss function erhˆlii kerhˆlii means second moment", "year": 2015}