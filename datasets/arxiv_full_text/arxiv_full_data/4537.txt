{"title": "ABC-LogitBoost for Multi-class Classification", "tag": ["cs.LG", "cs.AI"], "abstract": "We develop abc-logitboost, based on the prior work on abc-boost and robust logitboost. Our extensive experiments on a variety of datasets demonstrate the considerable improvement of abc-logitboost over logitboost and abc-mart.", "text": "develop abc-logitboost based prior work abc-boost robust logitboost. extensive experiments variety datasets demonstrate considerable improvement abc-logitboost logitboost abc-mart. boosting algorithms become successful machine learning. study revisits logitboost framework adaptive base class boost multi-class classiﬁcation. described alg. builds additive model greedy stage-wise procedure using second-order approximation requires knowing ﬁrst derivatives loss function respective derivatives derived assuming relations among however used throughout paper provided alternative explanation. showed conditioning base class noticed resultant derivatives independent particular choice base class. stage logitboost individual regression function separately class. analogous popular individualized regression approach multinomial logistic regression known result loss statistical efﬁciency compared full maximum likelihood approach. base class must identiﬁed boosting iteration training. suggested exhaustive procedure adaptively best base class minimize training loss iteration. propose abc-logitboost combining abc-boost robust logitboost. extensive experiments demonstrate abc-logitboost considerably improve logitboost abc-mart variety datasets. note operations applied individual sample. goal ensure response |zik| large hand hope larger |zik| better capture data variation. therefore thresholding occurs frequently expected useful information lost. consider weights response values assumed ordered according sorted order corresponding feature values. tree-splitting procedure index weighted mean square error reduced split seek maximize number terminal nodes determines capacity base learner. suggested commented unlikely. experience large datasets often reasonable choice; also combined abc-boost mart develop abc-mart. demonstrated good performance abcmart compared mart. study illustrate abc-logitboost combination abc-boost logitboost reduce test errors least variety datasets. table letter pendigits zipcode optdigits isolet used standard training test sets. covertype split mnistk used original test samples original mnist dataset training original training samples testing. also explained letterk used last samples letter training remaining testing original letter dataset. note zipcode otpdigits isolet small datasets necessarily provide reliable comparison different algorithms. since popular datasets nevertheless include experiments. recall logitboost three main parameters since overﬁtting largely avoided simply unless machine zero reached. performance sensitive performance also sensitive good range. ideally would like show that every reasonable combination abc-logitboost exhibits consistent improvement logitboost. datasets experimented every combination provide summary experiments presenting detailed results mnistk. dataset experimented every combination trained till loss reached machine zero exhaust capacity learner could provide reliable comparison iterations. interestingly abc-logitboost sometimes needed iterations reach machine zero logitboost. explained part fact logitboost precisely abc-logitboost. also would like experiment range values. table summarizes smallest test mis-classiﬁcation errors along relative improvements abc-logitboost logitboost. abc-logitboost exhibits rerr smaller test mis-classiﬁcation errors logitboost. -values range although reported table table mnistk. test mis-classiﬁcation errors logitboost abc-logitboost along relative improvement rerr report smallest values figures cell contains three numbers logitboost error abc-logitboost error relative improvement rerr original abc-boost paper include experiments mnistk. thus study table summarizes smallest test mis-classiﬁcation errors mart abc-mart. again consistent considerable improvement abc-mart mart. also comparing tables abclogitboost also signiﬁcantly improves abc-mart. table mnistk. test mis-classiﬁcation errors mart abc-mart along relative improvement rerr report smallest values figures cell contains three numbers mart error abc-mart error relative improvement rerr table summarizes test errors overall best test mis-classiﬁcation errors. table rerr relative improvement test performance. -values tested statistical signiﬁcance abc-logitboost achieved smaller error rates logitboost. compare abc-logitboost abc-mart table also includes test errors abc-mart -values testing statistical signiﬁcance abc-logitboost achieved smaller error rates abcmart. comparisons indicate clear performance abc-logitboost abc-mart especially large datasets. table summarizes smallest test mis-classiﬁcation errors logitboost abc-logitboost along relative improvements since fairly large dataset experimented results covertype reported differently datasets. covertype fairly large. building large model would expensive. testing large model run-time costly infeasible certain applications therefore often important examine performance algorithm much earlier boosting iterations. table shows abc-logitboost improve logitboost much rerr opposed reported rerr table table letterk. test mis-classiﬁcation errors logitboost abc-logitboost along relative improvement rerr cell contains three numbers logitboost error abc-logitboost error relative improvement rerr multi-class classiﬁcation fundamental task machine learning. paper presents abc-logitboost algorithm demonstrates considerable improvements logitboost abc-mart variety datasets. interesting dataset named poker training samples million testing samples. experiments showed abc-boost could achieve accuracy interestingly using libsvm accuracy obtained. report results separate paper.", "year": 2009}