{"title": "LightNet: A Versatile, Standalone Matlab-based Environment for Deep  Learning", "tag": ["cs.LG", "cs.CV", "cs.NE"], "abstract": "LightNet is a lightweight, versatile and purely Matlab-based deep learning framework. The idea underlying its design is to provide an easy-to-understand, easy-to-use and efficient computational platform for deep learning research. The implemented framework supports major deep learning architectures such as Multilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). The framework also supports both CPU and GPU computation, and the switch between them is straightforward. Different applications in computer vision, natural language processing and robotics are demonstrated as experiments.", "text": "present lightnet lightweight versatile purely matlab-based implementation modern deep neural network models. succinct eﬃcient matlab programming techniques used implement computational modules. many popular types neural networks multilayer perceptrons convolutional neural networks recurrent neural networks implemented lightnet together several variations stochastic gradient descent based optimization algorithms. since lightnet implemented solely matlab major computations vectorized implemented hundreds lines code orders magnitude succinct existing pipelines. fundamental operations easily customized basic knowledge matlab programming required. mathematically oriented researchers focus mathematical modeling part rather engineering part. application oriented users easily understand modify part framework develop network architectures adapt applications. aside simplicity lightnet following features lightnet contains modern network architectures. applications computer vision natural language processing reinforcement learning demonstrated. lightnet provides comprehensive collection optimization algorithms. lightnet supports straightforward switching computing. fast fourier transforms used eﬃciently compute convolutions thus large convolution kernels supported. lightnet automates hyper-parameter tuning novel selective-sgd algorithm. example using lightnet found simple template provided start training process. user required critical training parameters number training epochs training method. selective-sgd algorithm provided facilitate selection optimal learning rate. learning rate lightnet lightweight versatile purely matlabbased deep learning framework. idea underlying design provide easy-to-understand easy-to-use eﬃcient computational platform deep learning research. implemented framework supports major deep learning architectures multilayer perceptron networks convolutional neural networks recurrent neural networks framework also supports computation switch straightforward. diﬀerent applications computer vision natural language processing robotics demonstrated experiments. computer vision; natural language processing; image understanding; machine learning; deep learning; convolutional neural networks; multilayer perceptrons; recurrent neural networks; reinforcement learning given rise major advancements many problems machine intelligence. current implementations neural network models primarily emphasize eﬃciency. pipelines consist quarter half million lines code often involve multiple programming languages requires extensive eﬀorts thoroughly understand modify models. straightforward self-explanatory deep learning framework highly anticipated accelerate understanding application deep neural network models. permission make digital hard copies part work personal classroom granted without provided copies made distributed proﬁt commercial advantage copies bear notice full citation ﬁrst page. copyrights third-party components work must honored. uses contact owner/author. october amsterdam netherlands copyright held owner/author. isbn ----//. http//dx.doi.org/./. convolutional layer maps nmap input feature maps nmap output feature maps multidimensional ﬁlter bank kio. input feature convolved corresponding ﬁlter bank kio. convolution results summed bias value added generate o-th large convolution kernels fast fourier transforms used computing convolutions according convolution theorem convolution spatial domain equivalent point-wise multiplication imcol function implemented convert stridden pooling patches column vectors vectorize pooling computation matlab. built-in function called column vectors return pooling result indices maximum values. then indices selected automatically optionally adjusted during training. framework supports computation opts.use option. additional functions provided prepare training data initialize network structure. every experiment paper reproduced running related script ﬁle. details found project webpage. primary computational module includes feed forward process backward/back propagation process. feed forward process evaluates model back propagation reports network gradients. stochastic gradient descent based algorithms used optimize model parameters. lightnet allows focus mathematical modeling network rather low-level engineering details. make paper self-contained explain main computational modules lightnet. networks paper built modules. notations chosen simplicity. readers easily extend derivations mini-batch setting. mapping input linear perceptron ﬁnal network output expressed non-linear function represents network’s computation deeper layers network output usually loss value. without loss generality mapping maxpooling layer input ﬁnal network output expressed selection matrix column vector denotes input data layer. pooling range less equal stride size calculated simple matrix indexing techniques matlab. speciﬁcally empty tensor dzdx size input data created. dzdx dzdy pooling indices dzdy tensor recording pooling results. pooling range larger stride size entry pooled multiple times back propagation gradients need accumulated multiple-pooled entries. case calculated using matlab function accumarray. rectiﬁed linear unit implemented major non-linear mapping function functions including sigmoid tanh omitted discussion here. relu function identity function input usually loss function connected outputs deepest core computation module. currently lightnet supports softmax log-loss function classiﬁcation tasks. stochastic gradient descent algorithm based optimization algorithms primary tools train deep neural networks. standard algorithm several popular variants adagrad rmsprop adam also implemented deep learning research. worth mentioning implement novel selective-sgd algorithm facilitate selection hyperparameters especially learning rate. algorithm selects eﬃcient learning rate running process iterations using learning rate discrete candidate set. middle neural training selective-sgd algorithm also applied select diﬀerent learning rates accelerate energy decay. lightnet supports using state-of-the-art convolutional network models pretrained imagenet dataset. also supports training novel network models scratch. convolutional network convolution layers constructed test performance lightnet cifar- data relu functions applied convolution layer non-linear mapping function. lightnet automatically selects adjusts learning rate achieve stateof-the-art accuracy architecture. selective-sgd leads better accuracy compared standard ﬁxed learning rate. importantly using selective-sgd avoids manual tuning learning rate. fig. experiment results. computations carried desktop computer intel nvidia titan memory. current version lightnet process images second popular recurrent neural network model. lightnet’s versatility lstm network implemented lightnet package particular application. notably core computational modules lightnet used perform time domain forward process back propagation lstm. lightnet provides easy-to-expand ecosystem understanding development deep neural network models. thanks user-friendly matlab based environment whole computational process easily tracked visualized. main features provide unique convenience deep learning research community. gate time denotes distorted input memory cell time denotes content memory cell time denotes hidden node value. maps hidden nodes network loss time full network loss calculated summing loss individual time frame lstm network tested character language modeling task. dataset consists sentences selected works shakespeare. sentence broken characters lstm model deployed predict next character based characters before. hidden nodes used network model rmsprop used training. epochs prediction accuracy next character improved application reinforcement learning created q-network network. q-network applied classic cart-pole problem dynamics cart-pole system learned twolayer network hundreds iterations. iteration update process q-network action randomly selected probability epsilon otherwise action leading highest score selected. desired network output qnew calculated using observed reward discounted value resulting state predicted current network", "year": 2016}