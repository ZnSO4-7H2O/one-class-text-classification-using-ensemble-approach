{"title": "Stacked Kernel Network", "tag": ["stat.ML", "cs.CV", "cs.LG"], "abstract": "Kernel methods are powerful tools to capture nonlinear patterns behind data. They implicitly learn high (even infinite) dimensional nonlinear features in the Reproducing Kernel Hilbert Space (RKHS) while making the computation tractable by leveraging the kernel trick. Classic kernel methods learn a single layer of nonlinear features, whose representational power may be limited. Motivated by recent success of deep neural networks (DNNs) that learn multi-layer hierarchical representations, we propose a Stacked Kernel Network (SKN) that learns a hierarchy of RKHS-based nonlinear features. SKN interleaves several layers of nonlinear transformations (from a linear space to a RKHS) and linear transformations (from a RKHS to a linear space). Similar to DNNs, a SKN is composed of multiple layers of hidden units, but each parameterized by a RKHS function rather than a finite-dimensional vector. We propose three ways to represent the RKHS functions in SKN: (1)nonparametric representation, (2)parametric representation and (3)random Fourier feature representation. Furthermore, we expand SKN into CNN architecture called Stacked Kernel Convolutional Network (SKCN). SKCN learning a hierarchy of RKHS-based nonlinear features by convolutional operation with each filter also parameterized by a RKHS function rather than a finite-dimensional matrix in CNN, which is suitable for image inputs. Experiments on various datasets demonstrate the effectiveness of SKN and SKCN, which outperform the competitive methods.", "text": "kernel methods powerful tools capture nonlinear patterns behind data. implicitly learn high dimensional nonlinear features reproducing kernel hilbert space making computation tractable leveraging kernel trick. classic kernel methods learn single layer nonlinear features whose representational power limited. motivated recent success deep neural networks learn multi-layer hierarchical representations propose stacked kernel network learns hierarchy rkhs-based nonlinear features. interleaves several layers nonlinear transformations linear transformations similar dnns composed multiple layers hidden units parameterized rkhs function rather ﬁnite-dimensional vector. propose three ways represent rkhs functions nonparametric representation parametric representation random fourier feature representation. furthermore expand architecture called stacked kernel convolutional network skcn learning hierarchy rkhs-based nonlinear features convolutional operation ﬁlter also parameterized rkhs function rather ﬁnitedimensional matrix suitable image inputs. experiments various datasets demonstrate effectiveness skcn outperform competitive methods. kernel methods represent well-established learning paradigm able capture nonlinear complex patterns underlying data. kernel methods learning implicitly performed high-dimensional nonlinear feature space kernel trick vector input space projected rkhs using nonlinear mapping linear learning performed nonlinear features implicclassical kernel methods perform single-layer feature learning input transformed used ﬁnal features learning carried out. motivated recent success deep neural networks perform hierarchical representation learning able capture low-level middle-level high-level features study deep kernel methods learn several layers stacked nonlinear transformations. speciﬁcally propose stacked kernel network interleaves several layers nonlinear transformations linear mappings. starting input nonlinear feature applied project -dimensional could inﬁnite) rkhs linear mapping project -dimensional linear space k-th vector according deﬁnition rkhs computed function rkhs. representation written k-th element treating input apply procedure again projecting another rkhs followed linear projection another linear space getting representation repeating process times obtain hidden layers. contains multiple layers nonlinear representations could inﬁnite-dimensional. grants vast representation power capture complex patterns behind data. hand nonlinear layer linear mapping applied conﬁne size model model capacity control. figure shows architecture skn. similar deep neural network contains multiple hidden layers. striking difference hidden unit parameterized rkhs function units parametrized vectors. rkhs functions could inﬁnite-dimensional arguably expressive ﬁnite-dimensional vectors. result could possess representational power dnn. given multiple layers rkhs functions learn challenging. ﬁrst need seek explicit representations functions. propose three ways. first motivated representer theorem parameterize rkhs function linear combination kernel functions anchored training data {xi}n αik. second shrink domain functions entire rkhs image nonlinear feature function parameterized learnable vector third gaining insight random fourier features rkhs function approximated random fourier feature transformation parameter vector. backpropogation algorithm learn skns three representations. evaluations various datasets demonstrate effectiveness skn. rest paper organized follows. section introduces stack kernel network section presents experimental results. section reviews related works section concludes paper. kernel methods perform learning reproducing kernel hilbert space functions. rkhs represents high-dimensional feature space expressive input space since able capture non-linear patterns behind data. rkhs associated kernel function inner product rkhs computed evaluating lower-dimensional input space well established kernel methods include support vector machine kernel principal component analysis kernel independent component analysis gaussian process name few. kernel methods featured three prominent concepts feature maps data point input space element inner product space kernel takes data points returns real number; rkhs hilbert space functions relations follows. first feature deﬁnes kernel. feature def= kernel. second kernel deﬁnes feature maps. every kernel exists hilbert space feature third rkhs deﬁnes kernel. every rkhs functions deﬁnes unique kernel called reproducing kernel fourth kernel deﬁnes rkhs. every kernel exists unique rkhs reproducing kernel brieﬂy review deep neural networks inspire construct stacked kernel network. contains input layer several hidden layers output layer. layer units. units adjacent layers inter-connected connection associated weight parameter. achieve non-linearity nonlinear activation function applied hidden unit. commonly used activation functions include sigmoid tanh rectiﬁer linear. classic kernel methods learn single layer nonlinear features expressive enough accommodate complex data. inspired hierarchical representation learning deep neural networks propose stacked kernel network learns multiple layers nonlinear features based rkhs. figure shows architecture urally represented ﬁnite-dimensional vectors rkhs functions inﬁnite dimension whose storage computation troublesome. address issue ﬁrst seek explicit representations functions facilitate learning. investigate three ways data-dependent nonparametric representation data-independent parametric representation approximate representation based random fourier features. kernel methods common represent rkhs function based representer theorem given regularized risk functional training data denotes hilbert norm increasing function minimizer functional admits following form skn. similar consists input layer output layer hidden layers. hidden layer associated rkhs hidden units therein parameterized functions rkhs. difference hidden units parameterized weight vectors. next present detailed construction procedure skn. start deﬁning ﬁrst hidden layer. input feature vector. pick functions rkhs -dimensional latent space i-th dimension ﬁrst hidden layer functions another rkhs deﬁne second hidden layer. repeating process times hidden layers characterized functions rkhs obtained. l-th hidden layer utilized produce outputs. stacking multiple rkhs together highly expressive. better understand this present equivalent architecture figure recall rkhs function equivalently deﬁned follows given input vector ﬁrst transform using reproducing kernel feature deﬁne contains linear coefﬁcients. based upon deﬁnition represented interleaving nonlinear projections linear projections given latent representation layer ﬁrst transform using nonlinear feature linear project matrix latent representation layer wφ). nonlinear features could inﬁnite-dimensional. contains layers features lead substantial representational power. adjacent layers nonlinear features layer ﬁnite-dimensional linear features placed. ensures size properly controlled. light large computational complexity nonparametric representation investigate parametric counterpart. basic idea instead searching optimal solution entire rkhs restrict learning subset rkhs functions subset nice parametric forms. speciﬁcally choose subset image reproducing kernel kernel function associated rkhs learnable parameter vector initialized using k-means clustering input samples trained gradient decent. speciﬁcally rkhs function associated j-th hidden unit layer addition nonparametric parametric representations also investigate another approximated representation based random fourier features given shift-invariant kernel approximated rffs transformation. generated following compute fourier transform kernel draw i.i.d samples i.i.d samples uniform distribution qx+bq)]⊤. given rkhs function approximate using figure shows architecture representation multiple layers transformations linear projections interleaved. weight parameters transformation sampled fourier transform ﬁxed training. learnable parameters weights table presents comparison three representations. functions nonparametric representation contain global optimal function rkhs. functions random fourier feature representation approximate rkhs. nonparametric representation contains global optimal solution rkhs parameters grow training data size parametric representations computationally efﬁcient however rare figure equal parameterize hidden units stacked kernel convolutional network. left parameterize hidden units interleave nonlinear linear mapping. right parameterize hidden units rkhs function mapping. chance reach global optimal. number parameters three representations grows number layers number units layer. hidden unit parametric representation parameters representation parameters much smaller order apply visual recognition problems aiming extend plain stacked kernel network certain advanced architecture strong representation ability visual recognition tasks. inspired recent success convolutional neural network extract features patch-wisely input feature space convolution operation strong representation features corresponding local area design convolutional architecture called stacked kernel convolution network speciﬁcally shown figure starting ﬁrst kernel convolutional layer rd×d×h input feature space patch extracted denoted rd×d×h. similar nonlinear feature ﬁrst applied project -dimensional rkhs using linear mapping project dimensional linear space according deﬁnition rkhs pick functions rkhs replace parameterize hidden units representation written ﬁrst kernel convolutional layer functions another rkhs deﬁne second layer. repeating process times skcn kernel convolutional layers characterized functions rkhs obtained. l-th kernel convolutional layer utilized produce outputs. following experiments applied parametric representation random fourier feature representation skcn architecture named p-skcn rff-skcn respectively. mnist contains images handwritten digits represented -dimensional vectors pixels. training images test images. imagenet-. dataset contains categories imagenet images category images. images represented -dimensional convolutional neural network features extracted pre-trained convnet model pendigits. multi-class dataset integer attributes classes. created collecting samples writers. satimage. generated landsat multi-spectral scanner image data containing training samples testing samples belonging classes. feature dimension segment. data examples drawn randomly database outdoor image categories. images hand-segmented create label every pixel. vowel. dataset consists classes class -dimensional samples. mnist. mnist dataset mentioned above. reshape -dimensional vectors images grayscale channel. cifar- consists images color channels images classes. train test contain images respectively. experimented three representations rkhs function nonparametric representation parametric representation random fourier feature representation kernel functions radial basis function kernel polynomial kernel polynomial kernel applicable rffskn since shift-invariant. scale parameter kernel tuned range using -fold cross validation. parameters polynomial kernel tuned range compare deep neural networks activation function rectiﬁed linear hyperparameters time. trained stochastic gradient decent using cross-entropy loss. batch size learning rate rff-skn number random fourier feature implemented using tensorflow experiments performed linux machines .ghz cores ram. skcn polynomial kernel function. rffskcn kernel function. trade-off runtime accuracy sample number rff-skcn parameters polynomial kernel parameter kernel tuned p-skn. compare methods using hyperparameter setting time. instance table skcn network architecture conv-poolnorm conv-norm-pool conv-normpool conv-norm-pool; kernel size strides layers; padding same-padding; pooling max-pooling; dropout without; data augmentation none. different setting rff-skcn activation functions rff-skcn contrast experiment stochastic gradient decent minimize cross-entropy loss batch size adaptive learning rate initialized staircase like decayed every iterations. local response normalization provided tensorﬂow normalize outputs layer. experiments performed linux machines tesla gpus ram. table shows classiﬁcation accuracy different representation rkhs functions kernel functions. accuracy np-skn layers available since model large takes much time converge. table make following observations. p-skn outperforms dnn. instance vowel p-skn kernel achieves accuracy accuracy achieved hidden units parameterized inﬁnite-dimensional rkhs functions expressive ﬁnite-dimensional weight vectors dnn. thus capable capture complex patterns behind data achieves higher classiﬁcation accuracy. cases hidden layers outperforms single hidden layer. example segment dataset p-skn layers achieve accuracy layer achieves demonstrates deep better single-layer kernel method. stacking multiple layers rkhs functions together improves representation power kernel methods results better performance. however number layers cannot large otherwise leads overﬁtting hurts generalization performance test set. instance satimage dataset p-skn layers performs worse layers. performance p-skn better np-skn rff-skn. np-skn number parameters grows linearly data size easily leads overﬁtting. rff-skn rff-represented functions true functions rkhs rather approximations suffers approximation error contrast functions parametric representations exactly rkhs parameters depend data size. besides design experiment sensitiveness dnns. figure shows accunumber hidden units racy changes w.r.t. mnist dataset. experimented different acivation functions including relu relu softplus softsign tanh sigmoid. seen overall skns sensitive accuracy remains stable increases contrast dnns sensitive method best accuracy achieved middle ground. smaller expressive enough larger leads overﬁtting. representation used deﬁning kernel rather making predictions unclear deal propose local deep kernel learning local feature space represented hierarchy nonlinear transformations guided tree structure. deep neural network deﬁne kernel function given input data ﬁrst generating latent representations kernel function overall deﬁned major difference works utilize nested nonlinear transformations deﬁne single kernel function work deﬁnes network multiple layers rkhs functions. propose generative model stacks multiple layers gaussian processes work discriminative model stacks multiple layers rkhs functions. convolutional neural network approximate kernel work uses random fourier features building blocks deep networks. paper propose deep kernel method stacked kernel network learns hierarchy rkhs functions. consists multiple layers interleaving nonlinear linear projections. nonlinear projection carried reproducing kernel feature associated rkhs resultant features could inﬁnite-dimensional. equipped multiple feature maps bring high representation power. avoid model size control immediately nonlinear transformation linear projection applied inﬁnite-dimensional nonlinear space ﬁnite-dimensional linear space. composed multiple hidden layers associated rkhs unit therein parameterized function rkhs. investigate three ways represent rkhs functions make learning tractable datadependent nonparametric representation based representer theorem data-independent parametric representation approximate representation based random fourier features. experiments various datasets demonstrate effectiveness skn. table listed experiment results skcn. major observation table rff-skcn outperforms cnn. skcn achieve higher accuracy dataset layer number. instance rff-skcn achieves dropout without dropout cifar- dataset -layer respectively. cases skcn layers outperforms single layer. next show detail performance rffskcn different layer cifar- dataset figure figure first figure figure observe rff-skcn exhibits considerably higher testing accuracy generalizable baseline architecture indicates inﬁnite-dimensional rkhs functions expressive ﬁnite-dimensional ﬁlters cnn. second although rff-skcn outperforms figure figure observe method rff-skcn take nearly twice long single iteration. mainly representation need train larger result this learned parameters rffskcn expressive cnn. several studies performed bridge deﬁne methods deep learning. deep kernel ﬁrst successively composing nonlinear transformation multiple times input deﬁning φ)). using kernel trick replaced kernel functions. however", "year": 2017}