{"title": "Emergence of grid-like representations by training recurrent neural  networks to perform spatial localization", "tag": ["q-bio.NC", "cs.AI", "cs.NE", "stat.ML"], "abstract": "Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.", "text": "decades research neural code underlying spatial navigation revealed diverse neural response properties. entorhinal cortex mammalian brain contains rich spatial correlates including grid cells encode space using tessellating patterns. however mechanisms functional signiﬁcance spatial representations remain largely mysterious. understand neural representations trained recurrent neural networks perform navigation tasks arenas based velocity inputs. surprisingly grid-like spatial response patterns emerge trained networks along units exhibit spatial correlates including border cells band-like cells. different functional types neurons observed experimentally. order emergence grid-like border cells also consistent observations developmental studies. together results suggest grid cells border cells others observed natural solution representing space efﬁciently given predominant recurrent connections neural circuits. understanding neural code brain long driven studying feed-forward architectures starting hubel wiesel’s famous proposal origin orientation selectivity primary visual cortex inspired recent development deep learning burst interest applying deep feedforward models particular convolutional neural networks study sensory systems hierarchically extract useful features sensory inputs kriegeskorte kietzmann yamins dicarlo cognitive tasks neural systems often need maintain certain internal representations relevant variables absence external stimuliprocess requires feature extraction. focus spatial navigation typically requires brain maintain representation self-location update according animal’s movements landmarks environment. physiological studies done rodents mammals revealed variety neural correlates space hippocampus entorhinal cortex including place cells grid cells along border cells band-like cells others particular grid cell ﬁres animal occupies distinct physical locations strikingly locations lattice. study neural underpinning spatial cognition provided important window high-level cognitive functions supported brain might spatial navigation task solved using network neurons? recurrent neural networks seem particularly useful tasks. indeed recurrent-based continuous attractor networks popular type models proposed formation grid cells place cells models provided valuable insights possible mechanisms could support formation grids. however models typically rely ﬁne-tuned connectivity patterns particular models need subtle systematic asymmetry connectivity pattern move attractor state according animal’s movement. existence speciﬁc connectivity rodent remains unclear. additionally previous models mainly focused grid cells types responses co-exist entorhinal cortex largely ignored. would useful uniﬁed model simultaneously explain different types neural responses motivated considerations present alternative modeling approach understanding representation space neural system. speciﬁcally trained perform spatial navigation tasks. leveraging recent development training knowledge navigation system brain show training biologically relevant constraints naturally gives rise variety spatial response proﬁles observed including grid-like responses. knowledge ﬁrst study show grid-like responses could emerge training perform navigation. result implies neural representation seen natural brain solve navigation task efﬁciently generally suggests rnns powerful tool understanding neural mechanisms certain high-level cognitive functions. figure example neural data showing different kinds neural correlates underlying spatial navigation ﬁgures replotted previous publications. left right grid cell recorded animal navigates square environment replotted krupic heat representing ﬁring rate neuron function animal’s location band-like cell krupic border cell solstad irregular spatially tuned cell diehl speed cell kropff exhibits roughly linear dependence rodent’s running speed; heading direction cell sargolini shows systematic change ﬁring rate depending animal’s heading direction. network consists recurrently connected units receive external inputs representing animal’s speed heading direction. outputs linearly weight neurons rnn. goal training make responses output neurons accurately represent animal’s physical location. typical trajectory training. shown output accurately though perfectly track animal’s location navigation. activity unit related activation unit nonlinearity study take tanh). unit receives input units recurrent weight matrix also receives external input enters network weight matrix unit sources bias learned represents noise intrinsic network taken gaussian zero mean constant variance. network simulated using euler method timesteps duration perform navigation task linearly combine ﬁring rates units network estimate current location animal. responses linear readout neurons given following equation network inputs outputs inspired simple spatial navigation tasks open environments. task resembles dead-reckoning ethologically relevant many animal species speciﬁc inputs network animal’s speed direction time step. experimentally shown velocity signals exist also evidence signals necessary grid formation throughout paper adopt common assumption head direction animal coincides actual moving direction. outputs xy-coordinates integrated position. direction animal modeled modiﬁed brownian motion increase probability straight-runs order consistent typical rodent’s behavior open environment. usage simple movement statistics advantage full control simulated trajectories. however future work would interesting test model using different animals’ real movement trajectories results might change. special care taken animal close boundary. boundary environment affect statistics movement animal cannot cross boundary. fact reﬂected model re-sampling angular input variable input angle lead animal outside boundary. simulations shown below animal always starts center arena veriﬁed results insensitive starting locations. optimized network parameters minimize squared error equation target xy-coordinates dimensional navigation task network outputs generated according equation regularized input output weights according equation squared ﬁring rates units according equation training aims minimize loss function consists error animal metabolic cost penalty large network parameters. results qualitatively insensitive initialization schemes used recurrent weight matrix rec. results presented paper simulations hexagonal environment obtained initializing elements zero mean gaussian random variables variance simulations square triangular environments initialized orthogonal initialized bias output weights zero. elements zero mean gaussian variables variance /nin. figure different types spatial selective responses units trained rnn. example simulation results three different environments presented. blue represents activity. grid-like responses. band-like responses; borderrelated responses; spatially irregular responses. responses spatially selective form regular pattern deﬁned conventional sense. simulation experiments arenas different boundary shapes including square triangular hexagonal. figure shows typical example model performance training; network accurately tracks animal’s actual path test whether trained developed location-selective representations plot individual neurons’ mean activity level function animal’s location spatial exploration. note average response proﬁles confused linear ﬁlters typically shown feedforward networks. surprisingly neurons trained show range interesting spatial response proﬁles. examination response proﬁles suggests classiﬁed distinct functional types. importantly show distinct spatial response proﬁles mapped naturally known physiology spatial responses units trained networks shown appendix. grid-like responses interestingly units exhibit clear grid-like responses ﬁring patterns typically exhibit multiple ﬁring ﬁelds ﬁring ﬁeld exhibiting roughly circular symmetric ellipse shape. furthermore ﬁring ﬁelds highly structured i.e. combined arranged regular lattice. furthermore structure response lattice depends shape boundary. particular training network perform self-localization square environment tends give rectangular grids. hexagonal triangular environments grids closer triangular. experimentally shown contains so-called grid cells exhibit multiple ﬁring ﬁelds regular grid grid-like ﬁring patterns simulation reminiscent grid cells rodents mammals. however also notice grid-like model responses typically exhibit periods many experimental data possible using larger network might reveal ﬁner grid-patterns model. nonetheless surprising gird-like spatial representations develop model given periodicity input. another potential concern that experimentally reported grids often corners triangular lattice even square environments though grids somewhat inﬂuenced shape environment. however rats experiments presumable spatial experience environments various boundary shapes. experimentally would interesting grid cells would square lattice instead rats raised single square environment situation simulating here. border responses many neurons exhibit selectivity boundary typically encode portion boundary e.g. piece wall square shaped environment. properties similar border cells discovered rodent experimentally border cells mainly along piece wall although observed along multiple borders along whole boundary environment; interestingly multi-border responses also observed models. currently unclear boundary-like response proﬁles emerge model points possibility border cells emerge without presence tactile cues. furthermore suggests border cell formation related movement statistics animals i.e. asymmetry movement statistics along boundary. band-like responses interestingly neurons exhibit band-like responses simulations bands tend parallel boundaries. units bands overlaps boundary others case. experimentally neurons periodic-like ﬁring patterns recently reported rodent study reported substantial portion cells exhibit band-like ﬁring characteristics however note based reported data krupic band pattern clear model. spatially-stable non-regular responses besides units described above remaining units also exhibit stable spatial responses belong categories. response proﬁles exhibit either large irregular ﬁring ﬁeld; multiple circular ﬁring ﬁelds ﬁring ﬁelds show regular pattern. experimentally types cells also observed. fact recently reported non-grid spatial cells constitute large portion neurons layer rodent speed tuning next neurons tuned inputs. many model neurons exhibit linear responses running speed animal neurons show selectivity speed suggested near-ﬂat response functions. example response proﬁles figure direction tuning speed tuning nine example units trained triangular arena. unit show spatial tuning directional tuning speed tuning respectively left right. abc) three model neurons show strong directional tuning spatial tuning weak irregular. three neurons also exhibit linear speed tuning. def) three neurons exhibit grid-like ﬁring patterns clear speed tuning. strength direction tuning differ. border cells exhibit weak complex directional tuning almost speed tuning. band cell shows weak directional tuning strong speed tuning. shown figure interestingly observe model border cells tend almost zero speed-tuning head direction tuning substantial portion model neurons show direction tuning. diversity direction tuning proﬁles terms strength tuning preferred direction. example tuning curves shown figure direction tuning curves complete population shown appendix. interestingly general model neurons show strongest head direction tuning show clear spatial ﬁring pattern suggests group neurons mostly responsible encoding direction. also notice neurons clear grid-like ﬁring exhibit variety direction tuning strengths weak strong appendix quantify relation different tuning properties whole population level show somewhat complex dependence. experimentally heading direction tuning well-known e.g. sargolini grid non-grid cells exhibit head direction tuning furthermore linear speed dependence model neurons similar properties speed cells reported recently result also consistent another recent study reporting majority neurons exhibit amount speed tuning remains open question experimentally population level different types tuning characteristics relate other. next investigate spatial response proﬁles evolve learning/training progresses. report main observations. first neurons selectively along boundary typically emerge ﬁrst. second grid-like responses ﬁner spatial tuning patterns emerge later training. visualization perform dimensionality reduction using t-sne algorithm algorithm embeds model neurons three phases training two-dimensional space according similarity temporal responses. similarity metric taken ﬁring rate correlation. space shown figure border cell representations appear early stably persist training. furthermore early training responses similar border related responses. contrast grid-like cells typically undergo substantial change ﬁring pattern training settling ﬁnal grid-like representation developmental time line grid-like cells border cells roughly consistent developmental studies rodents. experimentally known border cells emerge earlier development exist weeks born grid cells mature weeks birth furthermore simulations suggest reason border cells emerge earlier development computationally easier wire-up network gives rise border cell responses. figure development border cells grid-like cells. early training responses similar border related responses training continues grid-like cells emerge. perform dimensionality reduction using t-sne algorithm ﬁring rates neurons. represents neuron color represents different training stages line shows trajectory single highlighted neuron ﬁring responses evolve training. panel highlight border representation. appears four clusters border cells responding wall square environment cells’ response proﬁles appear early stably persist training illustrated short distance travel space. show neurons eventually become grid cells initially tuning proﬁles similar border cells change tuning substantially learning. natural consequence need travel long distance space early late phase training. spatial responses shown four grid-like cells late phase training. appropriate regularizations crucial emergence grid-like representations. observed grid-like representations network encouraged store information perturbed noise. accomplished setting speed input zero e.g. zero speed time adding gaussian noise network equation precise method setting speed input zero value noise variance crucial simulations develop grid-like representations. cost function aims capture penalization metabolic cost neural activity also acts important regularization. simulations show grid-like representation emerge without metabolic cost. figure show typical simulation results square environment without proper metabolic regularization. appendix illustrate effect regularization further particular role injecting noise units. results consistent general notion importance incorporating proper constraint learning useful representations neural networks furthermore suggests that learn model response properties similar neural systems necessary incorporate relevant constraints e.g. noise metabolic cost. figure complete spatial response proﬁles neurons trained square environment. without proper regularization complex periodic spatial response patterns emerge. proper regularization rich periodic response patterns emerge including grid-like responses. regularization also adjusted achieve spatial proﬁles intermediate examples. figure error-correction happens boundary error stable time. boundary direction resampled avoid input velocities lead path extending beyond boundary environment. changing input statistics boundary termed boundary interaction receives boundary. uses boundary interactions correct accumulated error true integrated input prediction based linear readout equation panel mean squared error increases boundary interactions decreases boundary interaction boundary interactions leading greater error reduction. absence boundary interaction squared error would gradually increase roughly constant rate. network trained using mini-batches timesteps stable error duration least four orders magnitude larger. error output compared error would achieved outputting best constant values network would gradually accumulate leading decrease localization performance. test simulating paths several orders magnitude longer. somewhat surprisingly rnns still perform well fact squared error stable. spatial response proﬁles individual units also remain stable. implies rnns acquired intrinsic error-correction mechanisms training. boundary interactions enable error-correction signals based boundary-related activities. indeed boundary interactions dramatically reduce accumulated error figure shows that without boundary interactions average squared error grows roughly linearly expected however interactions boundaries substantially reduce error frequent boundary interactions reduce error further. error-correction grid cells boundary interactions proposed however emphasize model proposed develops grid-like responses boundary responses error-correction mechanisms within neural network thus potentially providing unifying account diverse phenomena. paper trained rnns perform path integration arenas. found training rnns appropriate regularization model neurons exhibit variety spatial velocity tuning proﬁles match neurophysiology what’s more also similarity terms distinct neuron types emerge training/development. long thought involved path integration localization animal’s location general agreement different response properties model neurophysiology provide strong evidence supporting hypothesis neural population provide efﬁcient code representation self-locations based velocity input. recently increased interest using complex neural network models understand neural code. focus using feedforward architectures particular cnns given abundant recurrent connections brain seems particularly fruitful avenue take advantage recent development rnns help neuroscience questions here show instance following approach. however insight work could general potentially useful cognitive functions well. ﬁnding metabolic constraints lead emergence grid-like responses seen conceptually related efﬁcient coding hypothesis visual processing particular seminal work emergence v-like gabor ﬁlters sparse coding model olshausen field indeed work partly inspired results. conceptual similarities however also note differences sparse coding work ours. first sparsity constraint sparse coding naturally viewed particular prior context recurrent network difﬁcult interpret way. second grid-like responses sparse solution could imagine. fact still quite dense compared spatially localized representation. third grid-like patterns emerged network ﬁlters based input rather velocity inputs need integrated ﬁrst order encode spatial locations. work also inspired recent work using efﬁcient coding idea explain functional architecture grid cells shown efﬁcient coding considerations could explain particular grid scales observed rodents however work ﬁring patterns neurons assumed lattice structure start with. furthermore work related study sussillo others show regularization models important generating solutions similar neural activity observed motor cortex. sussillo smoothness constraint together others lead simple oscillatory neural dynamics well matches neural data. incorporated smoothness constraint network. additionally note recent studies place cells input generate grid cells fundamentally different work. feedforward network models grid cells essentially perform dimensionality reduction based spatial input place cells. however main issue models that unclear place cells acquire spatial tuning ﬁrst place. contrary model takes animal’s velocity input addresses question spatial tuning generated input known exist another related study authors train although model shows qualitative match neural responses observed nonetheless several major limitations offering interesting future research directions. first learning rule seems biologically implausible. interested exploring biologically plausible learning rule could give rise similar results second simulation results show variety spatial scales grid-like cells. experimentally known grid cells multiple spatial scales scale geometrically ratio particular scale ratio predicted efﬁcient coding space investigating modify model hierarchy spatial scales perhaps incorporating neurons modifying regularization. last least focused representation produced trained rnn. equally important questions concern networks actually support generation representation. preliminary effort examined connectivity patterns trained network seem resemble connectivity patterns required standard attractor network models. maybe seen surprising. trained networks produce diverse neural responses previous models grid responses. would interesting future work systematically examine questions related underlying mechanisms. thank members center theoretical neuroscience columbia university useful discussions three anonymous reviewers constructive feedback. research supported neuronex award dbi- training grant yoshua bengio aaron courville pascal vincent. representation learning review perspectives. ieee transactions pattern analysis machine intelligence jonathan couey aree witoelar sheng-jia zhang kang zheng jing benjamin dunn rafal czajkowski may-britt moser edvard moser yasser roudi recurrent inhibitory circuitry mechanism grid formation. nature neuroscience geoffrey diehl olivia stefan leutgeb jill leutgeb. grid nongrid cells medial entorhinal cortex represent spatial location environmental features complementary coding schemes. neuron yedidyah dordek daniel soudry meir dori derdikman. extracting grid cell characteristics place cell inputs using non-negative principal component analysis. elife alex graves abdel-rahman mohamed geoffrey hinton. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee joshua jacobs christoph weidemann jonathan miller alec solway john burke xue-xin nanthia suthana michael sperling ashwini sharan itzhak fried direct recordings grid-like neuronal activity human spatial navigation. nature neuroscience alex krizhevsky ilya sutskever geoffrey hinton. imagenet classiﬁcation deep convolutional neural networks. advances neural information processing systems rosamund langston james ainge jonathan couey cathrin canto tale bjerknes menno witter edvard moser may-britt moser. development spatial representation system rat. science timothy lillicrap daniel cownden douglas tweed colin akerman. random synaptic feedback weights support error backpropagation deep learning. nature communications bruce mcnaughton francesco battaglia jensen edvard moser may-britt moser. path integration neural basis ’cognitive map’. nature reviews neuroscience volodymyr mnih koray kavukcuoglu david silver andrei rusu joel veness marc bellemare alex graves martin riedmiller andreas fidjeland georg ostrovski human-level control deep reinforcement learning. nature francesca sargolini marianne fyhn torkel hafting bruce mcnaughton menno witter maybritt moser edvard moser. conjunctive representation position direction velocity entorhinal cortex. science francis song guangyu yang xiao-jing wang. training excitatory-inhibitory recurrent neural networks cognitive tasks simple ﬂexible framework. plos comput biol david sussillo mark churchland matthew kaufman krishna shenoy. neural network ﬁnds naturalistic solution production muscle activity. nature neuroscience daniel yamins hong charles cadieu ethan solomon darren seibert james dicarlo. performance-optimized hierarchical models predict neural responses higher visual cortex. proceedings national academy sciences noise metabolic cost important grid-like representations. ﬁgure left shows spatial responses network trained noise metabolic cost. ﬁgure right shows spatial responses network trained noise metabolic cost. quantify speed selectivity unit ﬁrst line tuning curve unit activity function speed. speed selectivity absolute value slope. unit activity modulated speed speed selectivity quantify direction selectivity unit calculated average unit activity function direction input took maximum minus minimum tuning curve. unit activity modulated direction direction selectivity quantify spatial selectivity used lifetime sparseness unit activity modulated spatial location spatial selectivity ﬁgures show selectivity single unit. training tried balance three terms minimizing single term neglected dominated. beginning training weighted regularization term equal error function decreased weighting according schedule used martens sutskever adaptively adjusted weighting starting initial value enforcing upper bound training progressed. found training procedure improved training performance interesting representations.", "year": 2018}