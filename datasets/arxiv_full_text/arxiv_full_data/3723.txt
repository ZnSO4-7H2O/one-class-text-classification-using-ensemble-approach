{"title": "Vulnerability of Deep Learning", "tag": ["stat.ML", "cond-mat.dis-nn", "cond-mat.stat-mech", "cs.AI", "cs.LG"], "abstract": "The Renormalisation Group (RG) provides a framework in which it is possible to assess whether a deep-learning network is sensitive to small changes in the input data and hence prone to error, or susceptible to adversarial attack. Distinct classification outputs are associated with different RG fixed points and sensitivity to small changes in the input data is due to the presence of relevant operators at a fixed point. A numerical scheme, based on Monte Carlo RG ideas, is proposed for identifying the existence of relevant operators and the corresponding directions of greatest sensitivity in the input data. Thus, a trained deep-learning network may be tested for its robustness and, if it is vulnerable to attack, dangerous perturbations of the input data identified.", "text": "renormalisation group provides framework possible assess whether deep-learning network sensitive small changes input data hence prone error susceptible adversarial attack. distinct classiﬁcation outputs associated diﬀerent ﬁxed points sensitivity small changes input data presence relevant operators ﬁxed point. numerical scheme based monte carlo ideas proposed identifying existence relevant operators corresponding directions greatest sensitivity input data. thus trained deep-learning network tested robustness vulnerable attack dangerous perturbations input data identiﬁed. despite many successful applications deep learning poorly understood. absence underlying theory means cannot certain circumstances given trained network operate correctly uncertainty calls question networks safety-critical applications. also possibility slightly perturbed input data constructed would fool network human making network susceptible adversarial attack simply cause misclassify thereby conceal information data important distinguish ambiguous data system machine human would diﬃcult classify data imperceptibly altered cause deep network misclassify paper introduces theoretical framework based renormalisation group speciﬁes conditions latter occur proposes calculational method testing trained network vulnerability. further determines directions space input data along small perturbations ampliﬁed vulnerable deep network causing generate qualitatively incorrect output. theoretical framework captures salient features deep learning multi-layer structure trained layer time. demonstrated speciﬁc case layer restricted boltzmann machine network trained classify data terms outputs whose exact conditional probability distribution however approach extended straightforwardly deep networks data generation well classiﬁcation. ﬁrst applied critical phenomena describes features microscopic degrees freedom system depend length scale successively eliminating shortdistance ﬂuctuations generating sequence eﬀective hamiltonians describe surviving features larger larger length scales. critical point associated scale-invariant ﬂuctuations convergence sequence eﬀective hamiltonians ﬁxed-point hamiltonian describes them. although many applications deep learning involve data exhibit structure diﬀerent length scales e.g. data drawn physical world appears captured deep learning cheap need case. second transformation eﬀected layer network diﬀerent reﬂecting emergence diﬀerent types features data whereas critical phenomena application transformation changes length scale ﬁxed amount. assume training deep network converges conditional probability distribution good approximation provided suﬃcient number layers. thus sequence transformations corresponding applying layer output hidden nodes previous layer starting input data converges. presumably generalised form scaling takes place deep learning assume eﬀect successively scaling smaller values ﬁnite number eigenvalues fisher information matrix trained deep network spectrum sloppy section context deep network used classiﬁcation particular deﬁne sequence eﬀective hamiltonians whose ﬁxed points universality classes deﬁne distinct outputs. section develop computational scheme based monte carlo estimating stability ﬁxed points respect depth network. section relates instability particular ﬁxed point direction input data space along small changes data cause major changes output probability distribution hence sources vulnerability. consider layered network rbms input nodes layers hidden nodes output nodes example hidden nodes binary vectors dimension input data outputs joint probability distribution layer form parametrise space eﬀective hamiltonians terms complete operators which case binary variables consist possible products components couplings thus eﬀective hamiltonian layer next consider coupling-constant space close ﬁxed point particular stability properties ﬁxed point number layers increased. assume deeper network accurately able learn classiﬁcation problem. close ﬁxed point {g∗} relationship between couplings adjacent layers leading order stability matrix ﬁxed point. stability properties ﬁxed point determine whether becomes sensitive small changes input data number layers increased. hidden nodes output nodes given input don’t enable construct sequence eﬀective hamiltonians ﬁxed-point hamiltonian. next section explain monte carlo methods used estimate stability matrix deﬁned eigenvalues behave deep networks close ﬁxed point. suﬃcient determine whether particular trained deep network ﬁxed points relevant operators. worth noting existence multiple ﬁxed points imply existence relevant operators ﬁxed points. robust situation ﬁxed point relevant direction data close boundary universality classes simply ambiguous represent source adversarial attack trained deep network rbms classiﬁcation problem whose exact solution available sequence rbms given example enables conditional probability distributions sample hidden nodes layer output layer. using capability apply methods developed monte carlo estimate stability matrix ﬁxed point space eﬀective hamiltonians. language relevant irrelevant. context deep learning existence relevant operator ﬁxed point creates sensitivity input data generate non-zero coupling operator. data lead probability distribution output increasingly deviates ﬁxed-point distribution network made deeper source vulnerability deep networks tiny changes input data excite relevant operator leading sequence eﬀective hamiltonians converging wrong ﬁxed point i.e. misclassifying data. stands provides explanation potential vulnerability deep networks doesn’t help diagnose whether particular network susceptible. because practice access weights e.g. enable sample chosen subspace operators {oα} express eﬀective hamiltonians layer network suﬃciently precise estimates largest eigenvalues stability matrix associated layer obtained method section compute using chain rule last derivative computed explicit form ﬁrst layer e.g. expressing terms {oα} determine alternatively solve system linear equations ﬁrst layer thus suitable choice operators given speciﬁc input sampling hidden nodes upper layers network computing expectation values operators construct linear equations elements stability matrix hence obtain successive approximations eigenvalues eigenvectors varying subset operators used test convergence estimates largest eigenvalues stability matrix associated given layer network computing eigenvalues successive layers investigate whether increasing depth network associated scaling behaviour corresponding operators scaling behaviour eigenvalue magnitude greater even tiny component initial data creates non-zero coupling corresponding eigenvector cause instability ﬁxed point network deep. next section explain identify speciﬁc perturbation data this. consider exact conditional probability distribution hence trained network parametrised input data ﬁxed-point hamiltonian fisher information matrix metric measures probability distribution changes along diﬀerent directions parameter space. ﬁxed-point hamiltonian unstable direction relevant operator correspond direction parameter space along probability distribution changes fastest i.e. stiﬀest direction. hence generate adversarial data need compute eigenvector associated largest eigenvalue fisher information matrix ﬁxed-point probability distribution. tiny ﬁrst layer taking product stability matrices associated layer computing expectation values operators subspace using full network. fact built product stability matrices connects scaling behaviour sloppiness spectrum observed i.e. ﬁxed number eigenvalues scaled small values resulting ﬁxed number stiﬀ directions many sloppy directions. obtain eigenvector corresponding largest eigenvalue fim. deep networks unstable ﬁxed point small perturbations corresponding input data direction eigenvector likely result data misclassiﬁed. paper extended formal analogy between deep learning enabled interpret classiﬁcation problem terms sequence eﬀective hamiltonians associated layers network whose couplings determined input data convergence sequence distinct ﬁxed point distinct output classiﬁcation problem. input data classiﬁcation result coupling-constant space eﬀective hamiltonians converges ﬁxed exposed possibility ﬁxed point might unstable direction associated relevant operator language small perturbations input data create non-zero overlap relevant operator tend ampliﬁed exponentially successive layers network diverges correct ﬁxed point data misclassiﬁed. fixed points relevant operators prone sort error misclassify data ambiguous even human. convergence sequence eﬀective hamiltonians ﬁxed point given subset input data guaranteed training existence relevant direction ﬁxed point associated sensitivity perturbations take data outside subset. size ampliﬁcation factor perturbation greater greater depth network. hence vulnerability misclassiﬁcation directly depth network. result paper method diagnosing whether given trained network vulnerable adversarial attack simply misclassiﬁcation noise data based computing expectation values operators hidden output layers. computations fairly demanding justiﬁed network intended safety-critical applications.", "year": 2018}