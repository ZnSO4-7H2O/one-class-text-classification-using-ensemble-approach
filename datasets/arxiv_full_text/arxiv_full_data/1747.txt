{"title": "A 'Gibbs-Newton' Technique for Enhanced Inference of Multivariate Polya  Parameters and Topic Models", "tag": ["cs.LG", "cs.CL", "stat.ML"], "abstract": "Hyper-parameters play a major role in the learning and inference process of latent Dirichlet allocation (LDA). In order to begin the LDA latent variables learning process, these hyper-parameters values need to be pre-determined. We propose an extension for LDA that we call 'Latent Dirichlet allocation Gibbs Newton' (LDA-GN), which places non-informative priors over these hyper-parameters and uses Gibbs sampling to learn appropriate values for them. At the heart of LDA-GN is our proposed 'Gibbs-Newton' algorithm, which is a new technique for learning the parameters of multivariate Polya distributions. We report Gibbs-Newton performance results compared with two prominent existing approaches to the latter task: Minka's fixed-point iteration method and the Moments method. We then evaluate LDA-GN in two ways: (i) by comparing it with standard LDA in terms of the ability of the resulting topic models to generalize to unseen documents; (ii) by comparing it with standard LDA in its performance on a binary classification task.", "text": "hyper-parameters play major role learning inference process latent dirichlet allocation order begin latent variables learning process hyperparameters values need pre-determined. propose extension call ‘latent dirichlet allocation gibbs newton’ places non-informative priors hyper-parameters uses gibbs sampling learn appropriate values them. heart lda-gn proposed ‘gibbs-newton’ algorithm technique learning parameters multivariate polya distributions. report gibbs-newton performance results compared prominent existing approaches latter task minka’s ﬁxed-point iteration method moments method. evaluate lda-gn ways comparing standard terms ability resulting topic models generalize unseen documents; comparing standard performance binary classiﬁcation task. keywords topic modelling gibbs sampling perplexity dirichlet multivariate polya large text corpora increasingly abundant result ever-speedier computational processing capabilities ever-cheaper means data storage. increased interest automated extraction useful information corpora particularly task automated characterization and/or summarization document corpus well corpus whole. generally tacitly understood ﬁrst step characterizing describing individual document identify topics covered document. thus much current research topic modelling methods algorithms extract structured semantic topics collection documents. algorithms many applications various ﬁelds genetics image analysis survey data processing social media analysis current topic modelling methods based well-known ‘bag-of-words’ representation; approach document represented simply words words counts preserved order original document ignored. current topic modelling methods also tend probabilistic models involving many observed hidden variables need learned training data. latent dirichlet allocation —the springboard many topic modelling methods—is simplest topic modelling approach common use. however pre-determined hyper-parameters play major role lda’s learning inference process; authors whether algorithms ﬁxed hyper-parameter values. paper extension proposed removes need predetermine hyper-parameters. basic idea behind version call ‘latent dirichlet allocation gibbs newton’ place non-informative uniform priors hyper-parameters component sampled uniform distribution. non-informative prior used since generally prior information parameters. evaluate lda-gn comparing standard using recommended settings described tested comparison based evaluation metrics. firstly perplexity inferred topic model measured unseen test documents secondly test inferred topic model’s performance supervised task spam ﬁltering. heart lda-gn call ’gibbs-newton’ approach learning hyper-parameters multivariate polya distribution. within lda-gn role learn parameters amounts combination distinct multivariate polya distributed data streams assumed model. however also extract standalone method—since able learn parameters data distributed multivariate polya distribution—and compare prominent methods task moments method suggested ronning minka’s ﬁxedpoint iteration method enhanced wallach java implementation lda-gn also standalone provided http//is.gd/gntmod. rest paper organised follows firstly completeness provide brief description standard lda; includes elaboration inference learning algorithms involved standard discussion eﬀect hyperparameters. following that brieﬂy review current algorithms learning parameters multivariate polya distributions. succeeding section present proposed algorithm evaluate comparison methods terms accuracy speed. afterwards proposed extension lda-gn detailed. move discussion evaluation metrics followed evaluation lda-gn concluding discussion. latent dirichlet allocation unsupervised generative model discover hidden topics collection documents corpus total number documents corpus document corpus. model treats words observed variables topics unobserved latent variables. basic idea consider document generated sampling mixture latent topics topic multinomial distribution full vocabulary terms corpus. ‘vocabulary terms’ mean list unique word tokens appear corpus. dirichlet distributions used model variables respectively. where concentration parameters dirichlet distributions model’s hyper-parameters. figure shows graphical representation model using plate notation total number latent topics topic assignments word document. thus model’s generative process described follows beginning process topic distributions sampled dirichlet distribution vocabulary terms parameter topics used represents documents corpus. document corpus topic mixture sampled dirichlet distribution parameter then order sample word document topic sampled multinomial distribution parameter that word sampled multinomial distribution parameter ϕzdt word distribution topic zdt. consequently dirichlet distributions ﬁrst distribution dimensional simplex used model topic mixtures. hand second distribution dimensional simplex used model topic distributions; total number corpus vocabulary terms. assumes samples speciﬁc variable considered i.i.d. where vector length component value represents number words document assigned topic hand vector length component value represents number instances term whole corpus assigned topic dirichlet distribution’s normalization constant multivariate version bivariate beta function normalizing constant beta distribution. dirichlet distribution’s normalization constant given following formula unfortunately exact calculation posterior distribution generally intractable denominator. calculation involves summing possible settings topic assignment variable number exponential value given total number corpus words. however several approximation algorithms sample posterior distribution used variational inference methods expectation propagation gibbs sampling variational methods markovchain monte carlo methods gibbs sampling widely used literature. gibbs sampling despite slowest widely considered provide accurate results however asuncion show appropriate values hyper-parameters methods provide almost level accuracy. paper gibbs sampling used experiments. gibbs sampling markov-chain monte carlo algorithm seen special case metropolis–hastings algorithm used obtain observation sequence high-dimensional multivariate probability distribution. consequently sequence used approximate marginal distribution subset model’s variables. addition used compute integral hidden variables consequently compute expected value. thanks conjugacy multinomial dirichlet distributions ‘collapsed’ gibbs sampler implemented variables analytically integrated carrying gibbs sampling process. allows sample directly distribution instead distribution turns reduces number hidden variables model makes inference learning faster. required samples posterior distribution thus full conditional distributions deﬁned where represents topic assignments values corpus words excluding word document assuming word document word instance term then where wd¬t words document excluding word zd¬t wd¬t word’s topic assignments. also number words document assigned topic excluding document’s word whereas number word instances term assigned topic corpus documents excluding word document finally need construct values correspond setting deﬁnition variables distributed multinomially dirichlet priors. thus distributed dirichlet-multinomial distribution follows where vector topics observation counts document vector term observation counts topic therefore using expectation dirichlet distribution corresponding setting given hyper-parameters play large role learning building high-quality topic models typically symmetric values used literature. using symmetric values means topics chance assigned ﬁxed number documents. symmetric values mean terms—frequent infrequent ones—have chance assigned ﬁxed number topics. however according wallach using asymmetric symmetric tends give best performance results terms inferred model’s ability generalise unseen documents. hyper-parameters generally smoothing eﬀect multinomial variables control sparsity respectively. sparsity controlled hence smaller values make model prefer describe document using smaller number topics. sparsity controlled hence smaller values makes model reluctant assign corresponding terms multiple topics. consequently similar words similar small values tend assigned subset topics. multivariate polya distribution also known dirichlet-multinomial distribution compound distribution. sampling multivariate polya distribution involves sampling vector dimensional dirichlet distribution parameter drawing discrete samples categorical distribution parameter process corresponds ’polya urn’ comprises sampling replacement inspection model reveals model comprises multivariate polya distributions model data. ﬁrst distribution used model distribution documents topics given multinomial counts. counts represent numbers words assigned topic document. second distribution models distribution topics vocabulary terms given multinomial counts word instances assigned diﬀerent topics corpus whole. thus accurate methods learn multivariate polya distribution parameters enhance quality topic modelling level documents topics well level topics vocabulary terms. number times outcome sample. assuming data distributed according multivariate polya distribution parameter basic idea behind learning parameter data maximize likelihood research literature replete methods estimate multivariate polya parameters; however exact closed form solution available accurate methods minka’s ﬁxed-point iteration method hand fastest methods moments method brieﬂy described reviewed. moments method approximate maximum likelihood technique useful initialization step methods. provides fast learn approximations dirichlet multivariate polya distribution parameters directly data. moments method uses known formulae ﬁrst second moments distribution’s density function calculate parameter. ﬁrst moment multivariate polya density function given following formula easy calculate empirical mean value data counts. consequently required calculate value order ﬁgure value parameter done using second moment value. variance dimension enough calculate basic idea behind minka’s ﬁxed-point iteration method maximizing likelihood follows starting initial guess multivariate polya distribution parameter simple lower bound likelihood tight constructed. maximum value lower bound calculated closed form becomes estimate process repeated convergence. thus objective maximise likelihood function multivariate polya distribution real values close original values respectively. values used previous guess constant comprises terms involve thus taking logarithm sides equation leads wallach provides faster version algorithm using digamma function recurrence relation. done representing data counts samples histograms. words number samples dimensional multivariate polya distribution. then eﬃcient representation would vectors counts elements cell vector represents number times count observed values related dimension value represented where histogram vector counts samples associated dimension histogram vector counts sums dimensions samples. numbers elements vectors respectively. formula speeds computation extent depends many frequent count values spotted dimension frequent values faster computation unfortunately digamma function call time-consuming practice; however wallach suggests room improving performance getting digamma function call completely. done taking consideration digamma recurrence relation parameters multivariate polya distribution dirichlet distribution learned data using standard bayesian methods. paper focus multivariate polya distribution plays major role lda. thus given samples multivariate polya distribution data modelled using generative model shown figure generative process case amounts ﬁrst sampling value components uniform distribution parameters then vector dimension sampled dirichlet distribution parameter eventually multinomial variable sampled multinomial distribution parameter non-informative uniform prior placed component parameter vector prior knowledge values. model’s joint probability where represents count sample dimensions. order learn values hidden variable gibbs sampler needs designed. goal gibbs sampling approximate distribution takes place calculating distribution sample value separately. unfortunately trivial solution previous equation; newton’s method root. although previous equation multiple roots interested positive real root. order apply newton’s method need second derivative second derivative loggamma function called trigamma function important solution high precision beginning coeﬃcient accurate ﬁrst iteration gibbs sampling represents guessed values. value updated full iteration gibbs sampler; words processing values. thus iteration newton’s method used section main experiments designed assess performance method moments method minka’s ﬁxed-point iteration. ﬁrst experiment intended evaluate accuracy whereas second experiment aimed assessing eﬃciency. artiﬁcial data used section allowing compare methods wide variety conditions. number multivariate polya samples used ranges number elements used generate sample falls range order assess accuracy proposed method categories data sets considered. categories designed dimensional multivariate polya distribution known parameters ﬁrst category small component values real numbers sampled uniformly range second category relatively large component values range sampled uniformly. category contain multinomial count vectors multivariate polya samples. figure diﬀerences actual learned values parameter components small values smaller diﬀerence better. proposed method minka’s ﬁxed-point iteration method moments method figure diﬀerences actual learned values parameter components large values smaller diﬀerence better. proposed method minka’s ﬁxed-point iteration method moments method moments method minka’s ﬁxed-point iteration method proposed method used learn parameter vectors data. given resulting vector diﬀerence component actual value calculated registered. experiments done category data allowing methods evaluated highly varied settings terms data sparsity number samples needed. figure displays diﬀerences small components actual values using ﬁrst data. figure shows diﬀerences relatively large values second category data. ﬁgure indicates minka’s ﬁxed-point iteration method record similar levels accuracy clearly better moments method respect. surprising minka’s ﬁxed-point iteration method method eventually maximizing log-likelihood function. wallach benchmarks minka’s ﬁxed-point iteration method alongside methods involving minka’s newton iteration evidence ﬁxed-point iteration leave-one-out evidence ﬁxed-point iteration evidence introduced mackay peto ﬁnds eﬃcient implementation minka’s ﬁxed-point iteration fastest accurate. paper comparing proposed method wallach’s eﬃcient implementation minka’s ﬁxed-point iteration method moments method mallet implementation moments method used. seen figure execution time minka’s ﬁxed-point iteration dimensional multivariate polya distribution using diﬀerent values number samples diﬀerent values number elements used generate sample another data sets generated purpose. ﬁrst generated using dimensional multivariate polya distribution whereas second generated using dimensional multivariate polya distribution. datasets used test performance proposed algorithm minka’s ﬁxed-point iteration relatively high dimensional cases respectively. distributions known parameter vector components sets number multinomial counts vectors number samples falls range starting increasing steps total number elements used generate sample value starting increasing step size using ﬁrst data combination number samples number elements data generated given random values time taken estimation method measured. solution considered converged maximum value among diﬀerences previous guesses alpha components values current estimates less .e-. process repeated times mean time plotted surface shown figure whole process repeated times again time using higher dimensional samples. corresponding surface high-dimensional trials shown figure figure execution time minka’s ﬁxed-point iteration dimensional multivariate polya distribution using diﬀerent values number samples diﬀerent values number elements used generate sample iteration alpha values requires less half number iterations required minka’s ﬁxed-point iteration method convergence. speedup pronounced case lower-dimensional dataset however number iterations needed less required minka’s ﬁxed-point iteration algorithm settings. lda-gn variant incorporates proposed method using learn variables main idea behind lda-gn allow similar words similar beta values consequently distributed similarly topics. thus asymmetric beta prior used case. order learn beta values model extended placing non-informative prior beta variables shown figure gives corpus words ability distributed diﬀerently topics. useful necessary terms need participating higher number topics compared terms. hand symmetric beta used words participate roughly number topics seen limitation original model. further argued topics bounded number documents distributed over. thus asymmetric alpha prior advisable well. technique applied alpha words placing non-informative prior alpha variables also shown figure generative process associated lda-gn described algorithm lda-gn generative process similar standard generative process extra pair steps. ﬁrst step sampling vector component value uniform distribution parameters second step sampling vector component value uniform distribution parameters give total number words assigned topic whole corpus total number words document samples multivariate polya distribution equation equation still used calculate variables respectively. calculation take place gibbs sampling convergence using good sample. consequently lda-gn collapsed gibbs sampling algorithm given algorithm unsupervised nature lda-based topic modelling algorithms evaluation inferred topic models diﬃcult task. however popular methods literature attempt evaluation. topic model’s perplexity hold-out test documents usually used standard evaluation metric. hand topic model’s performance supervised task also used assess performance models. methods described next. common evaluate topic model calculate perplexity unseen test documents. perplexity measure benchmark topic model’s ability generalize unseen documents. words provides numerical value indicating eﬀect much topic model ’surprised’ data. higher probability test document words given model smaller perplexity value becomes. consequently model smaller perplexity value considered better ability generalize unseen documents. unseen test corpus contains documents. perplexity calculated exponentiating negative mean log-likelihood value whole document words given model. perplexity given following formula number words test document ˜wj. unfortunately exact value marginal distributions intractable need diﬀerent settings however multiple methods approximate marginal probability literature annealed importance sampling harmonic mean method chib-style estimation best methods literature left-to-right algorithm gives topic assignments test document ˜wj. seen previous equation involves marginalizing variable ˜zj; intractable large test documents high number topics luckily previous possible value settings ˜zjt approximated using sequential monte carlo techniques another evaluate topic model model supervised task classiﬁcation spam ﬁltering. topic model’s performance tested models’ performance. paper chose spam ﬁltering task compare lda-gn standard lda. multiple ways topic model spam ﬁltering. example treating spam ﬁltering task binary classiﬁcation problem topic model used document dimensionality reduction technique choose features carry classiﬁcation using standard methods however multi-corpus approach distinct models inferred using vocabulary words. first model inferred collection spam documents topics whereas second model inferred collection non-spam documents topics. consequently words distributions topics learned. idea mc-lda merge previous models create uniﬁed model topics. done simply encoding topic identiﬁcation numbers spam topic model begin instead beginning thus unseen document inference uniﬁed model made using following formula ˜nk¬ topic excluding word document. however count represents number word instances vocabulary term form documents assigned topic unknown. thus previous multi-corpus inference formula’s second factor value. word test document approximated using then result inference process suﬃcient number iterations words topic assignment calculated. consequently document topic distribution calculated using section evaluation results using methods detailed previous section displayed. mainly involves evaluating lda-gn using perplexity metric using performance spam ﬁltering task. cases lda-gn compared standard model suggested according wallach standard model asymmetric dirichlet prior documents-overtopics distributions symmetric dirichlet prior topics-over-words distributions using algorithm lda-gn model gibbs sampler implemented. hand mallet implementation used standard lda. implemented using java. recommended settings suggested wallach used standard model asymmetric dirichlet prior documents-overtopics distributions symmetric dirichlet prior topics-over-words distributions. order train evaluate models corpora used. first corpus epsrc corpus comprises summaries projects information communication technology funded engineering physical sciences research council second corpus news corpus subset associated press data first text retrieval conference corpora provided http//is.gd/gntmod. standard english stop words removed corpora learning inference application. corpus divided parts ﬁrst part used training whereas second part used evaluation purposes. ﬁrst part comprises corpus documents used train lda-gn models. remaining part used calculate perplexity scores using equation order calculate probabilities java implementation left-to-right algorithm used test document index test document total number test documents. thus better model higher probability value consequently lower perplexity score. initial values variable topics variable values initialized vocabulary terms initial values recommended mallet package documentation that standard model’s mallet implementation using training corpus input. ﬁrst iterations values kept ﬁxed. burn-in period minka’s ﬁxed-point iteration used learn values sampler’s histograms. values learning process repeated every iterations. iterations model considered fully trained. hand lda-gn model trained using training corpus used standard lda. asymmetric values used model. similarly standard model lda-gn model considered fully trained iterations. performance standard lda-gn tested range scenarios. approaches times following settings number topics topics. following every individual runa fresh split used generate training testing corpora. figure figure show perplexity values unseen test data models inferred lda-gn epsrc news corpora respectively. error bars drawn point ﬁgures. figure figure show lda-gn outperforms standard settings another evaluate topic model check performance supervised task spam ﬁltering. thus spam ﬁlters built using mc-lda method elaborated before. ﬁrst built using standard whereas second built using lda-gn. three spam corpora used evaluation purposes enron corpus comprises subset enron emails period corpus contains legitimate message spam; lingspam corpus contains legitimate message spam; collection contains legitimate messages spam messages. standard english stop words removed three corpora. corpus split parts ﬁrst part comprises corpus used training whereas remaining used testing purposes. using training part mc-lda models built using standard lda-gn respectively. ﬁrst mc-lda model built using standard comprises models combined. ﬁrst trained using legitimate messages topics whereas second trained using spam messages topics. hand second mc-lda model built using lda-gn comprises lda-gn models combined. again ﬁrst trained using legitimate messages topics whereas second trained using spam messages topics. given fully trained mc-lda models inference performed test documents. order fully test models’ classiﬁcation abilities multiple thresholds used. thresholds values used threshold given trained mc-lda models inference applied three times model. mean values accuracy f-measure calculated points registered graph. standard deviation standard error values accuracy f-measure calculated well shown error bars. whole process repeated times every time fresh train/test split. eventually median points associated threshold value calculated curve drawn. figure figure figure show accuracy scores lda-gn standard models enron lingspam collection corpora respectively. moreover figure figure figure show f-measure scores lda-gn standard models enron lingspam collection corpora respectively. perusal ﬁgures shows models inferred lda-gn lead results less sensitive threshold value. however right threshold value chosen both models able provide almost level accuracy. since lda-gn models provide less sensitivity threshold values argued topic models inferred lda-gn higher discrimination inferred standard lda. mc-lda approach document classiﬁed spam score larger speciﬁc threshold value. hand threshold value less optimal value spam ﬁlter tends become strict. means legitimate documents classiﬁed spam. hand threshold value larger optimal value spam ﬁlter tends become tolerant consequently classifying spam documents legitimate. mc-lda based lda-gn able achieve higher accuracy f-measure scores mc-lda based model threshold less optimal value. speciﬁcally figure shows lda-gn enron case better precision threshold values less optimal better recall scores threshold values optimal. thus legitimate document’s score lda-gn spam topics always less score standard spam topics. contrary spam document’s score lda-gn spam topics always higher score standard spam topics. topic model inferred lda-gn provides better representation corpus inferred standard lda. paper main contributions oﬀered firstly algorithm learn multivariate polya distribution parameters named ‘gn’ described evaluated. secondly based extension dubbed ‘lda-gn’ proposed evaluated. order assess performance compared appropriate methods moments method—a quick approximate approach—and minka’s ﬁxed-point iteration method—a accurate slower method. able infer accurate values moments method able provide level accuracy provided minka’s ﬁxed-point iteration method. however time taken compute results invariably less time consumed minka’s ﬁxed-point iteration method accuracy. algorithm used applications dirichlet distribution multivariate polya distributions learn parameters data itself. extension lda-gn shows better performance compared standard lda. experiments using corpora suggest ability generalise unseen documents greater since shows lower perplexity values unseen documents. measure models perform supervised task standard lda-gn used context mc-lda method spam classiﬁcation task. generally lda-gn showed better performance task multiple choices threshold value. however models able provide levels accuracy given judicious choices threshold value. lower sensitivity threshold spam classiﬁcation tasks—as shown models inferred using lda-gn—suggests lda-gn able infer higher quality topic models better representations discriminatory legitimate spam parts corpora. recommended settings described mainly using asymmetric alpha symmetric beta priors lead diﬀerent words generally constrained contribute number topics. symmetric beta used betav relatively large value words really appear small number topics encouraged spread topics. hand betav relatively small value words tend distributed small number topics despite fact words could legitimately appear many topics. consequently topic models built constraints typically contain many irrelevant words among topics. contrast lda-gn every vocabulary term freedom distributed number topics restriction. however restriction stop words encouraged distributed topics evenly. important remove stop words lda-gn model learnt. stop words removed advance lda-gn paper. meanwhile quality topic models learned lda-gn seems augur well supervised learning tasks; spam classiﬁcation example believe tasks general area supervised document classiﬁcation beneﬁt lda-gn context mc-lda approach. especially fruitful case discrimination tasks involve ’close’ categories", "year": 2015}