{"title": "Discrete Energy Minimization, beyond Submodularity: Applications and  Approximations", "tag": ["cs.CV", "cs.LG", "math.OC", "stat.ML"], "abstract": "In this thesis I explore challenging discrete energy minimization problems that arise mainly in the context of computer vision tasks. This work motivates the use of such \"hard-to-optimize\" non-submodular functionals, and proposes methods and algorithms to cope with the NP-hardness of their optimization. Consequently, this thesis revolves around two axes: applications and approximations. The applications axis motivates the use of such \"hard-to-optimize\" energies by introducing new tasks. As the energies become less constrained and structured one gains more expressive power for the objective function achieving more accurate models. Results show how challenging, hard-to-optimize, energies are more adequate for certain computer vision applications. To overcome the resulting challenging optimization tasks the second axis of this thesis proposes approximation algorithms to cope with the NP-hardness of the optimization. Experiments show that these new methods yield good results for representative challenging problems.", "text": "discrete pair-wise energies review pair-wise energy function minimization discrete pair-wise energies binary problems multilabel problems relation linear programming discrete optimization algorithms correlation clustering optimization introduction optimization continuous perspective perspective large scale optimization experimental results conclusion discrete multiscale optimization introduction multiscale energy pyramid energy-aware interpolation uniﬁed discrete multiscale framework experimental results conclusion derivation general energy functions thesis explore challenging discrete energy minimization problems arise mainly context computer vision tasks. work motivates hard-to-optimize non-submodular functionals proposes methods algorithms cope np-hardness optimization. consequently thesis revolves around axes applications approximations. applications axis motivates hardto-optimize energies introducing tasks. energies become less constrained structured gains expressive power objective function achieving accurate models. results show challenging hard-to-optimize energies adequate certain computer vision applications. overcome resulting challenging optimization tasks second axis thesis proposes approximation algorithms cope np-hardness optimization. experiments show methods yield good results representative challenging problems. thesis explore challenging discrete energy minimization problems arise mainly context computer vision. binary energies ﬁgure-ground segmentations multi-label semantic segmentation stereo denoising inpainting image editing pritch bagon energies usually involve thousands variables dozens discrete states. moreover energies domain pair-wise energies involve interactions pairs neighboring variables. optimization discrete energies known np-hard cases still despite theoretical hardness instances energies special properties give rise polynomial time global optimal algorithms. instances slightly diﬀerent properties allow practice good eﬃcient approximation schemes. next chapter reviews previous work related discrete pairwise energy functions optimization. outlines properties conditions global optimization feasible conditions required successful practical approximations. chapter also surveys several approximation algorithms. provides brief outline properties energy must order algorithm succeed. conclusion survey discrete pair-wise energies broadly classify categories smoothness-encouraging energies energies favor conﬁgurations neighboring variables taking discrete state. contrast-enhancing energies energies encourage solutions neighboring variables take diﬀerent states. energies mainly used computer vision tasks ﬁrst category smoothness-encouraging smoothness-encouraging energies allow eﬃcient approximation schemes. hand contrast-enhancing energies challenging comes optimization indeed less popular practice. applications ﬁrst motivates hard-to-optimize functionals introducing applications. energies become less constrained structured gains expressive power objective function achieving accurate models. results show contrast-enhancing hard-to-optimize functionals adequate certain computer vision tasks. approximations overcome resulting challenging optimization tasks second axis thesis proposes methods algorithms cope np-hardness optimization. experiments show methods yield good results representative challenging problems. discrete energy minimization ubiquitous task computer vision. binary energies ﬁgure-ground segmentations multi-label semantic segmentation stereo denoising inpainting image editing pritch bagon thesis focus various types minimization problems pair-wise energies arise diﬀerent computer vision applications. discrete optimization problems general np-hard. cases minimization pair-wise energy solved exactly polynomial time. introductory chapter survey diﬀerent properties discrete pair-wise energies. show properties energies relate inherent diﬃculty optimization task. properties entail exact optimization algorithms properties admit eﬃcient approximations. important property smoothness-encouraging energy prefers labels neighboring variables same. smoothness-encouraging energies exist eﬃcient approximate minimization algorithms. contrast energies encourage neighboring variables diﬀerent labels much challenging minimize. contrast-enhancing energies existing algorithms provide poor approximations. thesis focuses optimization challenging contrast-enhancing energies. diving minimization task section presents discrete pair-wise energy function notations used thesis. also provides insights motivation energies. discard pair-wise term minimizing energy simply choosing label best variable separately. however presence pair-wise term introduces dependencies diﬀerent variables turns optimization much complicated process. despite local nature pair-wise term binding values neighboring variables introduces global eﬀects overall optimization. propagating information local pair-wise terms form global solution major challenge optimization following example shows discrete functional arise well studied computer vision application. example also illustrates relation discrete optimization inference graphical models. given rectiﬁed stereo pair images goal disparity pixel reference image. true disparity pixel random variable denoted pixel location variable take discrete states represent possible disparities point. possible disparity value cost associated matching pixel reference image corresponding pixel image disparity value. typically cost based intensity diﬀerences pixels observed quantity. denote cost relates compatible disparity value observed intensity diﬀerence second function expresses disparity compatibility neighboring pixels. function usually expresses prior assumption disparity ﬁeld smooth. examples prior commonly used potts model resulting graphical model known pairwise markov random field. although compatibility functions consider adjacent variables variable still able inﬂuence every variable ﬁeld pairwise connections. energy functions form arise many graphical models references therein). however restricted domain also encountered variety domains structural learning inference part discriminative models tsochantaridis thesis explores challenging instances energies explores methods improved minimization approaches hard-to-optimize energies. energy function presented previous section used many applications evaluate compatible certain discrete solution given problem. desired best solution given problem ﬁnding solution lowest energy. solving optimization problem general optimization problem np-hard. known hard combinatorial problems max-cut multi-way many others formulated form special instances problem optimized exactly polynomial time. main factors aﬀect diﬃculty optimization problem ture) optimization fairly straight-forward propagating information back forth leafs root convergence global minimum attained. information propagation often referred beliefpropagation koller friedman cycle-free graph crucial rapid polynomial time convergence messagepassing scheme every path root leaf unique therefore consensus along path attained single forward backward pass. however instances problem still eﬃciently minimized pair-wise terms meet certain conditions. next sections explore conditions detail providing pointers existing optimization methods succeed exploiting special structures suggest methods guarantees optimization process. simplicity start sec. special case discrete binary variables submodularity important property since exact optimization binary submodular functions done polynomial time regardless graph structure optimization algorithm identiﬁes mapping binary assignments cuts specially constructed graph. careful choice weights edges special graph gives rise mapping weight appropriate binary assignment construction choice weights illustrated fig. appropriate correspondences assignments graph-cuts weight energy illustrated fig. mapping established optimizing simply ﬁnding minimum constructed graph. done polynomial time provided weights edges nonnegative examining details construction reveals edge weights non-negative function submodular. details construction found e.g. greig boykov detail kolmogorov zabih however submodular hold) optimization becomes np-hard case exact minimum cannot guaranteed polynomial time approximation sought. notable approach approximating non-submodular binary optimization problems extension min-cut approach. method called qpbo proposed hammer later extended rother main idea behind approximation scheme energy function non-submodular derived graph edges negative weights. therefore propose construct redundant graph variable represented nodes node represents case variable assigned node represents case assigning variable. redundant representation eliminates need negative edge weights thus min-cut graph computed polynomial time. looking resulting min-cut discern cases variable ﬁrst separates complementary nodes representing variable. case clearly deﬁnes optimal state variable however second case complementary nodes fall side cut. case fig. graph construction binary energy weighted graph constructed following variable corresponding node addition special nodes added. edges connect node special nodes addition edge added interacting pair variables weights edges deﬁned according parameters energy illustrated ﬁgure. note weight edge exactly weight non-negative fig. binary energies graph-cuts four possible cuts graph constructed fig. separates special nodes edges marked dash lines. first rows show mapping assignment last rows show mapping weight energy appropriate assignment. therefore conclude qpbo extends min-cut approach handle non-submodular binary energies. recovery global minimum longer guaranteed algorithm recover partial labeling guaranteed part globally optimal solution. however extreme case happen qpbo unable label variable. form deﬁned ﬁnite discrete vector l}n. shown previous section properties pair-wise terms crucial eﬀect computational complexity optimization problem. binary case distinctive property submodularity however discrete variables states considered subtle types pair-wise interactions aﬀect ability optimize least eﬃciently approximate section describes various types eﬀect discrete optimization task. important result regarding minimization multilabel submodular functions presented schlesinger flach reduction made submodular minimization st-mincut specially constructed graph. shown original energy multilabel submodular weights resulting graph non-negative hence global minimum found polynomial time. construction generalizes construction ishikawa speciﬁc convex pair-wise functions. however submodularity restrictive property. well known potts term many pair-wise interactions submodular. still important properties non-submodular functions boykov derived important approximations rely properties properties relaxed kolmogorov zabih condition def. resembles triangle inequality metric functions case potts model robust interaction mentioned earlier section examples relaxed-metric pair-wise interaction. note property relaxed metric diﬀerent convexity def. another property less restrictive relaxed metric examples relaxed semi-metric functions truncated clearly relaxed metric function also relaxed semi-metric. point useful intuition meaning semi-metric property according def. function semi-metric cost assigning neighboring variables label never greater cost assigning diﬀerent labels. property implies encourages smoothness solution figure shows relation diﬀerent types functions ϕij. restrictive type convex subset submodular regarding relaxed metric submodular submodular functions relaxed metric relaxed metric submodular table shows examples popular pair-wise functions properties. unfortunately promising results schlesinger flach regarding global minimization submodular functions hold general functions. longer submodular longer hope achieve global optimality polynomial time. however relaxed metric relaxed semi-metric functions boykov showed large move making approximate algorithms performs quite well practice large move making algorithms iteratively seek improve energy current solution updating large number variables once. large step carried solving simple fig. diﬀerent types multilabel hierarchy relations between diﬀerent types multilabel energies. green indicates existence global minimization algorithms. energies good approximation algorithms. binary submodular minimization st-mincut. relaxed metric functions large move α-expansion. iteration binary problem solved variable either retain current label switch label relaxed metric property ensures resulting binary problem submodular thus solved globally polynomial time. α-expansion algorithm iterates labels converges. convergence ﬁnite number iterations guaranteed certain cases theoretical bounds proven quality approximation details). relaxed semi-metric functions slightly diﬀerent large move devised. pair labels large move called αβ-swap. iteration binary problem solved variables currently labeled either variable pick either relaxed semi-metric property ensures resulting binary problem submodular thus solved globally polynomial time. αβswap algorithm iterates pairs labels converges. convergence ﬁnite number iterations guaranteed however theoretical bounds approximation longer exists details). table shows resulting types pair-wise energies current results minimization. thesis focuses hard optimization contrast-enhancing functionals deﬁned cyclic graphs. part shows introducing energies contain contrast-enhancing terms gives rise applications. part deals methods approximating section establishes connection pair-wise energy minimization problem ﬁeld convex optimization particular linear programming following discussion useful introduce notations deﬁnitions. ﬁrst useful representation overcomplete representation solution vector representation deﬁned follows wainwright wainwright jordan unlike marginal polytop deﬁned using polynomial numschemes. fact ﬁrst order approximation aﬀected number deﬁning interacting pairs variables. polytops aﬀected parameters ϕij. vector entails translating hyperplane normal tangent constraint set. point tangency cost vector occurs integral vertex. point tangency cost vector occurs fractional vertex outside case integrality contain vertices correspond discrete solution refer vertices fractional solutions. fig. provides illustration marginal local polytops relation them impact optimal solution ﬁgure also distinguishes integral fractional vertices polytops. describes detail case fractional solution optimal. important note parameter vector fractional solution optimal example encourages relation discrete energy minimization convex presented section lies foundation popular optimization algorithms tree-reweighted relation also important illustrate challenging task optimizing represents energy function smoothness-encouraging optimal previous sections outlined properties energy function eﬀect minimization process. also demonstrated speciﬁc optimization algorithms take advantage properties. section present several prominent optimization algorithms refer later thesis. selected representative approaches sketches main directions current discrete optimization research mainly focused. simple basic iterative optimization algorithm proposed besag approximate method acting locally variables suitable multilabel functions arbitrary underlying graph arbitrary ϕij. iteration visits variables sequentially choose variable best state given current states variables. process viewed greedy coordinate descend algorithm bears analogy gauss-seidel relaxations continuous domain belief-propagation optimization algorithm based local updates. however contrast hard assignment performs update maintains soft beliefs variable passes messages neighboring variables according current belief. message variable neighbor mi→j vector length message vector encodes feels assigning state originally used inference algorithm tree-structured graphical models koller friedman messages initialized zero. messages passed leafs root back leafs. forward-backward message passing converges global converge. proposed cyclic graphs variant called loopybp. loopy case however clear schedule messages determine number iterations perform. even loopy converges ﬁxed point usually local optimum guarantees global optimality wainwright jordan signiﬁcant development presented works wainwright kolmogorov werner wainwright jordan komodakis works proposed interpretation basic message passing operation conducts. relaxed discrete minimization form continuous linear programming manner presented sec. related message passing optimization resulting shown relaxation forms speciﬁc structure. special structure turn exploited devise specially tailored optimization scheme uses message passing basic operation. tree-reweighted approach establishes relation discrete optimization continuous convex optimization relation brings forward interesting results properties continuous optimization domain discrete one. instance allows lagrangian multipliers formulate lagrangian dual original problem. dual representation provides lower bound sought optimal solution. solution found energy equals lower bound certiﬁcate provided global minimum. shown practice many computer vision application able recover globally optimal solutions. results dealt mainly relaxed metric energies however general guarantee cases involving challenging energies beyond relaxed metric shown integrality exists longer provide tight approximation bagon galun opposed local methods approach boykov proposes discrete methods based combinatorial principles. basic observation lies heart large-move algorithms instead treating variables locally time aﬀect labeling many variables performing large moves. large moves formulated binary step diﬀerence diﬀerent ﬂavors large-move algorithms formulation binary steps. makes large move eﬀective eﬃcient fact binary submodular sub-problems solved globally eﬃciently. basic large move algorithms α-expand αβ-swap already described sec. context relaxed metric relaxed semi metric energies. recently another large move making algorithm called fusion-moves proposed iteration fusion algorithm discrete solution proposed. proposed solution fused current solution binary optimization variable retain current label switch respective label proposed solution however unlike swap expand algorithms resulting binary optimization fusion step longer guaranteed submodular highly depends types proposed solutions. therefore often case qpbo non-submodular binary approximation algorithm used perform binary steps fusion algorithm. summarize brief outline existing approximation algorithms notice eﬀort recent years developing improving approximate optimization algorithms. research providing better practical results exploring theoretic aspects problem. approximation methods derived inspired continuous optimization domain discrete domain however results mainly focus minimization functions structure them either relaxed metric relaxed semi-metric smoothness-encouraging functions current algorithms succeed providing good approximations practice despite theoretical np-hardness. contrast comes arbitrary contrast-enhancing functions little known terms approximation method currently exists provides satisfying approximations. thesis focuses optimization arbitrary contrast-enhancing functions. discrete energy minimization ubiquitous task computer vision scientiﬁc domains. however previous chapter optimization discrete energies known np-hard cases despite theoretical hardness many smoothness encouraging energies approximate optimization algorithms exist provide remarkable approximations practice. however tasks become sophisticated models grow complex tree structured cyclic graphs grids simple smoothing priors complex arbitrary pair-wise interactions. energies become less constrained structured gains expressive power objective function cost signiﬁcantly challenging optimization task. work would like step outside comfort-zone smoothness-encouraging energies explore challenging discrete energies. step gives rise important questions case decide embark challenging task approximating arbitrary discrete energy tackle problem? propose approaches directions diﬃcult approximation tasks discrete energies beyond semi-metric? ﬁrst axis thesis involves exploring applications require arbitrary contrast-enhancing energies beyond semi-metrics. examples demonstrate additional expressive power arbitrary energies crucial derive applications. show utilizing arbitrary energies gives rise interesting desirable behaviors diﬀerent applications. present applications part thesis. chapter shows image sketching application provides binary sketch small collection images similar objects. application binary sketch described interactions neighboring pixels corresponding images. neighboring sketch bits corresponding similar image pixels encourage similar value contrast neighboring sketch bits corresponding dissimilar image pixels encourage diﬀerent value binary sketch output resulting non-submodular energy minimization. sketching application thought special case binary image segmentation. considering contrast-enhancing objective function task unsupervised segmentation clustering introduce solution clustering problem also help determining underlying number clusters. clustering objective function commonly known correlation clustering chapter explores correlation clustering functional underlying model-selection capability. image segmentation clustering examples contrastenhancing energies. chapter describes surface reconstruction multiple images diﬀerent known lighting. reconstruction takes account changes appearance surface change lighting directions. changes amounts implicit partial diﬀerential equation describes unknown surface. work propose pose solution resulting discrete optimization task. incorporating integrability prior unknown surface resulting discrete energy contrast-enhancing terms. modeling prior knowledge sources contrastenhancing terms energies. many cases exact parameters energy learned training data. chapter presents decision tree fields example learning scheme. learns principled manner pair-wise energy labeled training examples. since learning process constraint smoothness-encouraging energies often case resulting energy contrast-enhancing terms. training phase seeks parameters maximize likelihood data. therefore resulting contrast-enhancing terms better suited describe underlying behavior data. restricting model smoothness-encouraging terms would prohibit accurately predicting results test time. enhancement descriptive power gained considering arbitrary energies comes price longer good approximation algorithms hand. therefore second axis thesis explores possible directions approximating resulting challenging arbitrary energies. part work propose practical methods approaches approximate resulting np-hard optimization problems. particular chapter propose discrete optimization approach aforementioned correlation clustering optimization. approach scales gracefully number variables better existing approaches chapter concludes part general perspective discrete optimization. perspective inspired multiscale approaches suggests cope np-hardness discrete optimization using multiscale landscape energy function. deﬁning observing multiscale landscape energy propose methods explore exploit derive coarse-to-ﬁne optimization framework. perspective gives rise uniﬁed multiscale framework discrete optimization. proposed multiscale approach applicable diversity discrete energies smoothness-encouraging well arbitrary contrast-enhancing functions. part concentrate ﬁrst axis thesis. direction explores applications require arbitrary energies. start unsupervised clustering objective function correlation clustering chapters show several applications revolving around correlation clustering energy. energy hard optimize smoothness-encouraging well contrastive pair-wise terms; data term guide optimization process number discrete labels known a-priori. analyze interesting property correlation clustering functional ability recover underlying number clusters. interesting property mainly usage terms enhance contrast rather smoothness solution another application requires arbitrary energy reconstruction surfaces. show chapter certain conditions reconstruction posed solution partial diﬀerential equation solving recover surface done discretization solution space. discrete version yields pair-wise terms beyond semi-metric. finally parameters energy function deﬁning terms ﬁxed a-priori. happen would like learn energy function training data various applications. chapter shows example energy learning framework. resulting learned energy longer guaranteed well-behaved. fact experiments show learning procedure constrained often case resulting energy arbitrary yield known structure. given images containing common object interest severe variations appearance detect common object provide compact visual representation object depicted binary sketch. algorithm composed stages detect mutually common ensemble ‘self-similarity descriptors’ shared input images. found mutually common ensemble ‘invert’ generate compact sketch best represents ensemble. provides simple compact visual representation common object eliminating background clutter query images. obtained query images. clean sketches useful detection retrieval recognition co-segmentation artistic graphical purposes. given images containing common object interest possibly severe appearance changes detect common object provide simple compact visual representation object depicted binary sketch input images contain additional distracting objects clutter object interest unknown image locations appearance signiﬁcantly vary across images assume however diﬀerent instances object share rough need extract common images occurs various application areas including object detection large digital libraries. example user provide example images containing method based densely computed local self-similarity descriptors shechtman irani algorithm composed main steps identify common object detecting similar ensemble self-similarity descriptors shared input images. corresponding descriptors common object across diﬀerent images similar descriptor values well relative positions within ensemble. found mutually common ensemble descriptors method inverts generate compact binary sketch best represents ensemble. shown shechtman irani given single query image object interest possible detect instances object images densely computing matching local self-similarity descriptors. query image real synthetic image even hand-drawn sketch object. centered around object interest contain also objects signiﬁcant background clutter. goal detect least trivial common part query images generate clean possible sketch eliminating background clutter query images. clean sketches obtained query images useful detection retrieval recognition artistic graphical purposes. applications illustrated experiments. moreover shechtman irani received input clean hand-drawn sketch object interest produce sketch outputs thereby also solving inverse problem namely given several images object generate sketch using self-similarity descriptor. closely related research area problem address ’learning appearance models’ object category area recently received growing attention chum ferrari winn jojic karlinsky grauman nguyen name few). goal methods discover common object shapes within collections images. methods assume single object category ferrari karlinsky winn jojic nguyen others assume multiple object categories grauman methods rely weakly supervised learning techniques typically require tens images order learn detect represent object category. unique problem pose method ability depict common object images despite large variability appearance. scenario method able address. small number images provide enough ’statistical samples’ methods. method cannot compete performance methods many example images provided outperforms existing methods images large variability available. attribute strength method densely computed region-based information opposed commonly used sparse spurious edge-based information moreover sketching step algorithm provides additional global constraint. image region. results ‘correlation’ surface correlation surface quantized compact log-polar representation bins achieve invariance small local aﬃne non-rigid deformations. maximum value constitutes value corresponding descriptor entry co-segmentation segment object common images seeking segments diﬀerent images share common properties common properties shared remaining backgrounds diﬀerent images. co-segmentation methods extract common object images usually assume much higher degree similarity appearance diﬀerent instances object assumed rest paper organized follows sec. formulates problem gives overview approach. sec. describes component algorithm detects ‘least trivial’ common part collection images whereas sec. describes sketching component algorithm. experimental results presented sec. input images containing common object widely diﬀerent appearances. object appear diﬀerent colors diﬀerent textures small non-rigid deformations. backgrounds arbitrary contain distracting clutter. images diﬀerent sizes image locations common object unknown. assume however diﬀerent instances object share rough common geometric shape roughly scale orientation. output sketch captures rough common shape. approach thus based detecting ’common regions’ using densely computed local self-similarity descriptors shechtman irani descriptor captures local shape information image vicinity computed invariant photometric properties log-polar representation makes descriptor insensitive small shown h¨orster local self-similarity descriptor strong descriptive power local selfsimilarity descriptors allows method handle much stronger variations appearance handled previous methods. densely compute self-similarity descriptors images ‘common’ image parts across images similar arrangements self similarity descriptors. best captures rough characteristic shape common object shared ˜ik. formally seek binary image whose local self-similarity descriptors match best possible local self-similarity descriptors edge-based detection and/or sketching grauman ferrari requires many input images regionbased detection sketching recovered images. edges tend spurious prone clutter fig. ..b). edge-based approaches thus require considerable number images allow consistent edge/gradient features object stand inconsistent background clutter. contrast region-based information much less sparse less aﬀected clutter misalignments sensitive existence strong clear boundaries. much larger image oﬀsets required push corresponding regions alignment misalign thin edges. thus region-based cues require fewer images detect represent common object. indeed method provide good sketches images. fact cases method produces meaningful sketch even single image edge-based sketching impossible interpret example fig. general case however locations object within input images unknown. seek binary image sketches ‘least trivial’ object ‘most common’ images. ‘most common’ constraint obvious image high edge generated method maire binary sketch generated method applied single input image illustrates concept region-based information much richer sparse edge-based information therefore appears powerful detection sketching. subimage centered ck). however many image regions trivially shared many natural images. example uniform regions occur abundantly natural images. regions share similar self-similarity descriptors even underlying textures colors diﬀerent similarly strong vertical horizontal edges occur abundantly images. wish identify trivial common regions images ‘common object’. luckily since regions good image matches lots locations statistical signiﬁcance good matches tends contrast nontrivial common part least good match input image matches would ‘statistically signiﬁcant’ optimization algorithm iterate constraints least trivial common image part sketch common object given image locations overall process results sketch image provides simple compact visual representation common object interest query images eliminating distracting background clutter found images. wish detect image locations corresponding subimages centered locations share many self-similarity descriptors possible matches non-trivial ﬁnal sketch obtained subimages ﬁnal sketch subimage extracted input images iterative process). wish check good match input images also check statistical signiﬁcance matches. ‘correlate’ input images image highest match value maxm atch. higher value stronger match. however every high match value statistically signiﬁcant. statistical signiﬁcance maxm atch measured many standard deviations away mean match value entire collection images i.e. algorithm employed shechtman irani match ensembles self-similarity descriptors modiﬁed version eﬃcient ensemble matching algorithm boiman irani algorithm employs simple probabilistic star graph model capture relative geometric relations large number local descriptors small non-rigid deformations. ‘signiﬁcantly common’ subimage matched locations images. assign signiﬁcance score pixel according ‘signiﬁcance’ surrounding subimage signif icance accordingly improved sketch algorithm iterated several times. practice experiments good sketch recovered already ﬁrst iteration. additional iteration sometimes useful improving detection. fig. shows iterations process applied input images. results detection seen fig. computational complexity detection algorithm implemented coarse-to-ﬁne. ﬁrst step algorithm described quadratic size input images. however since number images fig. iterations detection sketching left input images. right ﬁrst iteration detection algorithm results detected image regions correct outlier resulting sketch produced regions reasonably good used reﬁning detection input images. results correct detections second iteration improved sketch. best captures rough characteristic shape object shared posed namely whose ensemble selfsimilarity descriptors similar possible ensembles descriptors extracted ˜ik. neglect binary constraint requirement consistency similarity descriptors {di}w·h because inherent robustness median operator outliers descriptors recovered collection descriptors proceed solve inverse problem i.e. generate image descripi= generated ‘median’ ‘average’ operations longer guaranteed valid collection self-similarity descriptors real image thus proceed recover simplest possible image whose self-similarity obtained self-similarity descriptors cover large image regions high overlaps. such similarity dissimilarity image locations implicitly captured multiple self-similarity descriptors different descriptor entries. self-similarity descriptor deﬁned shechtman irani values range indicates high resemblance central patch patches corresponding logpolar indicates high dissimilarity central patch corresponding log-polar bin. purposes stretch descriptor every image locations induced collection ‘combined’ entry matrix degree attraction/repulsion image locations determined self-similarity descriptors centered points. value containing location descriptor similarly value containing location descriptor entry gets following value fig. computing attraction/repulsion matrix log-polar selfsimilarity descriptor located white bins signify image areas high similarity central patch dark bins signify image areas dissimilarity central patch. point center descriptor falls white contains types values belong belong diﬀerent regions points distant general case however entries span range stands strong attraction strong repulsion means don’t care. closer value lower attraction/repulsion conﬁdence; closer note diﬀerent classical aﬃnity matrix used spectral clustering min-cut non-negative aﬃnities value ambiguous signiﬁes high-dissimilarity well low-conﬁdence. distinction ‘attraction’ ‘repulsion’ ‘low-conﬁdence’ critical case thus cannot resort max-ﬂow algorithm spectral clustering order solve problem. aﬃnity matrix positive negative values used context normalized-cut functional. however functional appropriate problem therefore deﬁne diﬀerent functional optimization algorithm order solve binary sketch functional aﬀected values shown ‘ideal’ case i.e. generated binary image global minimum obtained solving constrained optimization problem min-cut problem non-negative values allowed solved maxﬂow algorithm polynomial time. however weights functional obtain positive negative values turning ‘cut’ problem posed np-hard problem. therefore approximate reposing quadratic programming problem relaxing binary constraints. since necessarily positive semi-deﬁnite guarantee regarding approximation quality still empirical tests demonstrate good performance approximation. matlab’s optimization toolbox solve optimization problem obtain sketch principle yield binary image. however practice resulting sketches look close binary images capture well rough geometric shape common objects. sketching algorithm quite robust outliers obtains good sketches images. moreover constructing attraction/repulsion matrix replace ‘combined’ descriptors self-similarity descriptors single image algorithm produce ‘binary’ sketches single image example sketch obtained single image found fig. conducted empirical evaluations algorithm using ethz shape dataset ferrari dataset consists object categories large variability appearance applelogos bottles giraﬀes fig. sketching presence outliers corrupt inlier randomly chosen natural graph shows mean values quality function percent outlier images input i.e. mugs swans around images ground-truth information regarding location object image along single hand-drawn ground truth shape category. order assess quality algorithm range multiply sketches pixel-wise. sketch quality score quality places sketches agree sign pixel-wise product positive places sketches disagree product negaﬁrst assessed quality algorithm identify sketch common object correctly function number input images randomly sampled images object category applied detection sketching algorithm subset compared resulting sketch ground-truth repeated experiment times computed mean sketch quality scores. fig. displays plots mean quality score categories. seen relatively images already achieve sketches good quality even challenging sets giraﬀes examples sketching results experiments seen fig. next evaluated robustness sketching component algorithm outliers. robustness important since detection algorithm often produces outlier detections used inlier images alone generate good sketch high sketch quality score. added outlier images every image generated sketch compared ground-truth. experiment repeated times. fig. displays plots sketch quality percent outliers sketching method relatively robust outliers performs quite well even presence outliers falls farther away width height bounding-box ground-truth center. repeated experiment times plotted average detection rates fig. apples bottles swans high detection rates however detection rates good giraﬀe since giraﬀes undergo strong non-rigid deformations current algorithm cannot handle strong non-rigid deformations. clustering fundamental task unsupervised learning. focus chapter correlation clustering functional combines positive negative aﬃnities pairs data points. chapter provide theoretical analysis functional. analysis suggests probabilistic generative interpretation functional justiﬁes intrinsic model-selection capability. addition suggest applications utilize model-selection capability unsupervised face identiﬁcation interactive multi-object segmentation rough boundary delineation. resulting energy arbitrary diﬃcult approximate. defer discussion approximate minimization algorithms energy chapter part deals approximation schemes arbitrary energies. fundamental tasks unsupervised learning clustering grouping data points coherent clusters. clustering data points aspects pair-wise aﬃnities measured attraction i.e. likely points cluster repulsion i.e. likely points diﬀerent clusters. indeed approaches clustering recently presented bansal suggest combine attraction repulsion information. normalized cuts extended allow negative aﬃnities. however resulting functional provides sub-optimal clustering results sense lead fragmentation large homogeneous clusters. correlation clustering functional proposed bansal tries maximize intra-cluster agreement inter-cluster disagreement contrary many clustering objecsec. focuses theoretical probabilistic interpretation functional. subsequent sections present applications. applications build upon integrating attraction repulsion information large number points require robust recovery underlying number clusters chapter focuses functional properties derived applications. defer chapter novel approach optimization. experimental results presented chapter produced using algorithms described detail chapter consider following probabilistic generative model matrix true unobserved partition points clusters. assume pairs points observe pairwise similarity values sij. values random realizations either distribution depending whether points cluster not. namely uniformly distributed induced prior takes non-trivial shape assigns probability trivial solutions time gives preference partitions non-trivial mode prior roughly showed generative model underlying functional single model partitions regardless therefore optimizing functional need select diﬀerent generative models decide optimal comparing partitions diﬀerent therefore straight forward require additional model complexity term described previous section functional assumes uniform prior partitions. uniform prior induces prior number clusters i.e. a-priori probability clusters stirling numbers second kind compute induced prior shows non-trivial shape induced prior number clusters fig. interactive multi-object segmentation user provides crude partial indications locations boundaries relevant objects image output algorithm correctly segments image multiple segments. image taken alpert fig. boundary scribbles aﬃnity matrix boundary scribble drawn user inducing ﬁgure/ground regions opposite sides scribble method levin generate soft segmentation image segments pixel values soft segmentation range segment belong pixel described using segmentation membership vector entry corresponding assignment soft segmentation non-zero entry sparse aﬃnity matrix correlation normalized vectors across scribble. segment pixels opposite sides boundary likely diﬀerent segments observation design novel approach interactive multi-object image segmentation. instead using diﬀerent strokes diﬀerent objects user applies single brush indicate parts boundaries diﬀerent objects. using sparse incomplete boundary hints correctly complete boundaries extract desired number segments. although user provide stage number objects method able automatically detect number segments using incomplete boundary cues. fig. provides example novel interactive multi-object segmentation approach. computing aﬃnities fig. illustrates sporadic user-provided boundary cues compute sparse aﬃnity matrix positive negative entries. note modiﬁcation aﬃnity computation presented stein interactive boundary cues drive computation rather boundaries computed unsupervised technique. compute small fraction entries matrix opposed full matrix stein importantly positive negative aﬃnities contrast stein positive aﬃnities. methods optimizing correlation clustering functional unable handle size matrix. chapter describes detail novel approach optimization enables optimize large scale problems. applied swap-and-explore algorithm problem provides good looking results several minutes processing image. interface allows user segment image several coherent segments without changing brushes without explicitly enumerate number desired segments algorithm. application shows detecting underlying number clusters important task own. given collection face images expect diﬀerent clusters correspond diﬀerent persons. identifying diﬀerent people requires high purity resulting clusters importantly correctly discover appropriate number clusters. experiment extension existing work problem same/not-same learning. following recent metric learning approach learn single classiﬁer assigns probability pair faces likely pair person. then using classiﬁer able determine number persons cluster faces unseen people. given face images several unseen people clustering approach able automatically cluster identify many diﬀerent people face images never seen people. experiment face dataset kasinski dataset consists images people images taken partially controlled illumination conditions uniform background. main sources face appearance variations changes head pose facial expression. method guillaumin describe face. sift descriptors computed ﬁxed points face multiple scales. annotations provided dataset generate state-ofthe-art method guillaumin learn mahalanobis distance sigmoid function. experiment chose people test used images people training. learned distance used compute probability faces belong person pairs face images people test set. aﬃnities apply clustering −pij algorithm search optimal partition report identiﬁed number people purity resulting clusters. experimented repeated experiments several diﬀerent choices diﬀerent persons. settings algorithms described sec. performed roughly terms recovering purity resulting clustering. however terms running time adaptive-label completed task signiﬁcantly faster methods. compare swap-and-explore diﬀerent approaches connected components nected components. component correspond diﬀerent person. experiment tried threshold values reported best result. spectral treating probabilities matrix positive aﬃnity matrix ncuts malik cluster faces. method number clusters determined according spectral largest eigenvalue normalized laplacian matrix number clusters maxi fig. shows cluster purity number diﬀerent persons identiﬁed function actual number people diﬀerent methods. method succeeds identify roughly correct number people sizes test sets maintain relatively high purity values. chapter provides generative probabilistic interpretation correlation clustering functional justifying intrinsic model selection capability. using generative probabilistic formulation allows better understanding functional underlying assumptions makes including prior imposes solution. chapter consider problem reconstructing shape moving object accounting change intensities change orientation respect light sources. assume lighting motion parameters given. methods presented. first lambertian objects illuminated point source derive quasilinear implicit surface shape. propose solve continuation extending existing method allow large motion. secondly formulate reconstruction problem solve using discrete optimization techniques. latter method works fairly general reﬂectance functions applied sequences images. also incorporate prior information boundary conditions. discuss method extracting boundary conditions. demonstrate performance algorithms showing reconstructions smooth shaped objects comparing reconstructions reconstructions laser scans. moving objects change appearance change orientation respect light source. changes make diﬃcult identify corresponding points across images result complicate task motion recovery subsequently also reconstruction. existing approaches motion analysis largely assume either brightness constancy rely extracting distinctive features. approaches successfully handle polygonal shapes objects noticeable surface markings less suitable handling objects smooth shapes gradual albedo variations. paper propose framework reconstructing shape objects explicitly takes lighting eﬀects account. work based observation changes location intensity moving object across images coupled motion. therefore sources information combined constrain matching points across images allowing achieve veridical reconstruction. number previous studies used observation propose algorithms shape recovery either multi-view settings restricted settings views available. study propose method work images deal general lighting conditions fairly large motion. consider case object moves rigidly respect camera light source. assume object lambertian lighting conditions motion parameters known. present algorithms. first address problem shape reconstruction views lighting composed single directional source derive partial diﬀerential equation solved recover shape object using continuation show derive boundary conditions method. formulation extends work basri frolova objects undergoing large motion. second algorithm uses formulation construct cost function graph representing domain reconstructed. cost function takes form markov random field oﬀ-the-shelf algorithms solve sought shape. formulation oﬀers several advantages previous work. work fairly general reﬂectance functions. applied pairs images well sequences three images. robust errors continuation solutions accumulate errors integration. finally prior information incorporated; particular method boundary conditions show experiments boundary conditions essential. experiments real smooth objects demonstrate utility formulation. paper divided follows. section provides brief summary related work. section deﬁnes reconstruction problem derives continuation solution case large motion. section casts problem optimization discusses solution discrete techniques. section explains compute boundary conditions section shows results experiments. majority reconstruction techniques either motion shading techniques rarely combine cues. shape shading horn brooks photometric stereo woodham utilize shading cues reconstruct objects single multiple images static objects. motion often handled assumption brightness constancy horn schunk lucas kanade matching sparse feature points authors proposed reconstruct static objects combining shading stereo cues cryer leclerc note stereo setup lambertian objects retain intensities across images. another studies seek generalize brightness constancy assumption account local lighting variations black negahdaripour haussecker fleet studies motion shading cues simultaneously carceroni kutulakos maki mukawa simakov weber zhang joshi kriegman chen moses shimshoni typically require multi-frame setting studies also estimate light source direction along shape object. exception basri frolova requires images assumes small motion. work improves methods proposing reconstruction methods handle large motion work image pairs larger sequences images. addition second algorithm allows general lighting settings include point extended light sources spherical harmonic representations r.ramamoorthi p.hanrahan consider pair images taken stationary camera. images depict lambertian object moving rigidly space illuminated constant lighting. denote images point object’s surface described coordinate frame denote projections onto assuming weak perspective projection scale function surface normal. expression used model variety materials. particular model lambertian objects illuminated directional source setting max. direction point source given intensity likebasri frolova made equation explicit using taylor approximation used obtained expression recover shape lambertian objects illuminated single directional source. method approximates ﬁrst order handle objects undergoing small motion. speciﬁcally assuming rectiﬁed epipolar lines horizontal showed written explicit quasilinear tion vertical axis. described solution using method continuation showed method extract dirichlet boundary conditions along bounding contour object. ﬁrst contribution note basri frolova’s algorithm extended also handle large motion. quasi-linear even without taylor approximation solved method continuation. note that lies tangent vectors starting points given dirichlet boundary conditions. unfortunately implicit extracting boundary conditions complicated; suggest method section general unless points direction axis rotation parallel planes perpendicular independent note however generally planes contain characteristics coincide epipolar planes except axis rotation parallel image plane. case coincides becomes degenerate since right hand side consequently methods assume constant brightness applied. figure shows reconstruction obtained suggested method continuation. model rendered using point source light rotated angle continuation method applied exact boundary condition values. continuation method suﬀers several shortcomings. first method accumulates errors integration. moreover quasi-linear lighting composed single directional light source non-linear realistic lighting models used. continuation principle applied also non-linear equations signiﬁcantly less robust. addition method relies boundary conditions diﬃcult compute time allow inclusion prior information. finally provide integrate information images images object available. optimization solve using discrete optimization techniques. objective valid surface satisﬁes therefore deﬁne cost function composed unary binary terms. deﬁned grid overlaid ﬁrst image grid point associated state vector determining parameters surface element observed pixel. parameters include depth value surface normal i.e. surfel. note state vectors cannot represent points along silhouette contour derivatives points diverge. cost function combines unary binary terms grid points. unary term function residual speciﬁcally general images used reconstruction need supply boundary conditions form along curve section describe technique estimate depths near bounding contours object. procedure provides depth values normals along contour given boundary conditions simply modify unary cost along contour vanish states coincide depth values normals predicted boundary conditions. extraction boundary conditions noisy optimization modify depth values. optimization applied three images additional images constrain solution boundary conditions necessary. algorithm rother iteration solves binary decision problem variable allowed either change current state ﬁxed state retain current state. submodular energies iteration implemented using st-mincut algorithm kolmogorov zabih boykov qpbo algorithm extends st-mincut algorithm handle negative edge capacities arising case non-submodular pairwise terms. algorithm constructs graph complimentary vertices variable explicitly representing possible binary states. using redundant variable representation resulting graph constructed positive capacities edges therefore st-mincut algorithm applied following rule used label variables st-mincut step. variable complimentary vertices fall diﬀerent sides labeled according assignment induced cut. variables complimentary vertices fall side remain unlabeled. minimizing non-submodular energies generally nphard qbpo guaranteed label variables st-mincut step variables remain unlabeled. unlabeled variables retain original label previous iteration. procedure repeated every possible choice label entire batch iterations repeated convergence. optimization initialized setting initial state every point state minimizes unary term discrete optimization solver followed continuous quadratic optimization order relax quantized labeling surface. solve following quadratic optimization functional maintaining integrability constraint remaining close quantized solution required along characteristic curve. essential continuation method useful discrete optimization framework constrain solution. obtaining depth values complicated require ﬁnding correspondence pixels smooth objects. bypassed basri frolova since small motion taylor expansion becomes explicit allowing compute depths silhouette contours object intensities along contour. describe method computes depth values near silhouette contours implicit. denote object silhouette respectively denote ∂vωi portion silhouette contour remains visible likewise ∂vωj portion silhouette contour remains visible point coordinate frame projection onto denoted lies ∂vωi. since lies bounding contour surface normal parallel normal occluding contour derived image. plugging normal gives scalar right hand side using known value compute anticipated intensity corresponding point intensity search along epipolar line determine complicated since multiple points along epipolar line anticipated intensity addition face problems intensity values along bounding contour unreliable neither continuation discrete optimization scheme normals parallel image plane. normal point inside assume surface normal expressed unknown angle point along epipolar line denoted satisﬁes normal. equation unknowns corresponding point image therefore regularize problem requiring curve near ∂vωi mapped continuous curve euclidean distance point point along closes boundary intensity anticipated heuristic useful reasonable rotations. solve minimization substitute resulting system equations non-linear solve alternate minimization. tested algorithm sets real images compared reconstructions laser scans. image taken dark background allow segmentation foreground object. image estimated motion parameters manually marking points image mesh object. objects smooth contain almost clear surface markings motion estimates perfect. subsequently using mesh estimated environment lighting conditions calculating coeﬃcients ﬁrst order harmonic representation lighting representing ambient lighting directional source. obtained motion lighting parameters used input software. ﬁgures show reconstructions obtained optimization algorithm measure quality matches obtained using rmse error pixels. ﬁgures show input images bear elephant toys reconstructions pairs images without boundary conditions reconstructions multiple images. experiments cover range rotations seen tested reconstructions achieve rmse values. interestingly noticeable advantage using boundary conditions. described methods reconstructing shape moving object accounts intensities changes change orientation respect light sources. particular presented continuation method method based discrete optimization. experiments demonstrates utility methods. setting requires knowledge motion lighting parameters. plan future seek ways relax limiting assumptions. fig. reconstructions image pairs. shows left right laser scan reconstruction without boundary conditions. shows surface bottom shows surface painted intensity values. refig. reconstructions image pairs. shows left right laser scan reconstruction without boundary conditions. shows surface bottom shows surface painted intensity chapter introduces formulation discrete image labeling tasks decision tree field combines generalizes random forests conditional random ﬁelds widely used computer vision. typical model unary potentials derived sophisticated random forest boosting based classiﬁers however pairwise potentials assumed simple parametric form pre-speciﬁed ﬁxed dependence image data deﬁned basis small ﬁxed neighborhood. contrast local interactions multiple variables determined means decision trees evaluated image data allowing interactions adapted image content. results powerful graphical models able represent complex label structure. technical contribution show model trained eﬃciently jointly using convex approximate likelihood function enabling learn million free model parameters. show experimentally applications rich complex label structure model outperforms state-of-the-art approaches. last decade seen meteoric rise random ﬁeld models computer vision szeliski random ﬁelds used model many problems including foreground-background segmentation blake boykov semantic segmentation shotton winn shotton stereo boykov optical baker roth black horn schunk reconstruction snow vogiatzis many problems cast image labeling problem given image need predict labels random ﬁelds provide factorizing joint distribution posterior distribution consistency potentials ensuring smooth solution geman geman major advance ﬁeld make smoothness cost dependent local image structure boykov conditioning parts model input data. last decade conditional random ﬁeld models laﬀerty sutton mccallum become popular improved ability capture relationship labels image. research eﬀort devoted development eﬃcient algorithms estimating maximum posteriori solution models felzenszwalb huttenlocher szeliski kolmogorov komodakis tziritas true algorithms probabilistic inference wainwright jordan koller friedman further large number methods proposed learn parameters random ﬁeld models anguelov kumar sutton mccallum szummer zhang seitz number higher order random ﬁeld formulations also proposed able encode interactions groups pixels shown produce much better results kohli roth black however despite rapid developments state-of-the-art random ﬁeld models continue suﬀer following limitations deﬁned basis ﬁxed neighborhood structure roth black potentials assumed simple parametric form pre-speciﬁed ﬁxed dependence image data. relatively easy think various ways overcome limitations research challenge suggest model eﬃcient high-quality training still tractable. paper introduces graphical model decision tree field overcomes above-mentioned limitations existing models. take simple radical view every interaction model depends image further dependence non-parametric. easy even representing model extremely challenging since numerous ways deﬁning mapping image parameters unary pairwise interaction graphical model. model uses decision trees image content interaction values. every node every decision tree model associated parameters used deﬁne potential functions graphical model. making predictions novel test instance leaf number important reasons choice decision trees specify dependence potentials image content. firstly decision trees non-parametric represent rich functional relationships suﬃcient training data available. secondly training decision trees scalable training size approach parallelized; recent advances even allows training graphics processors sharp since computer vision applications well known obtaining high predictive performance amount training data many recent works decision trees related variant extremely randomized trees geurts semantic texton forest shotton context decision trees give another advantage allow eﬃciently jointly learn parameters model. achieve using logconcave pseudo-likelihood objective function known work well given enough training data consistent estimator koller friedman contributions best knowledge propose ﬁrst graphical model image labeling problems allows potential functions arbitrary dependence image data. show dependence potential functions image data expressed decision trees. show training model involves learning large number parameters performed eﬃciently. empirically demonstrate dtfs superior existing models random forest common crfs applications complex label interactions large neighborhood structures. relatively little work learning image-dependent potential functions i.e. conditional part random ﬁeld. algorithms learning parameters random ﬁeld learn class-to-class energy table depend image content anguelov batra nowozin lampert szummer taskar however attempts learning parameters conditional potentials prasad gould recently gould gould used multiclass logistic regression classiﬁer manually selected features length orientation region boundaries obtain image-dependent learned model pairwise interactions. even recently proposed model image restoration whose interactions dependent semantic meaning local image content predicted classiﬁer. unlike work above-mentioned models either target speciﬁc tasks assume particular form dependence potentials image content. neither above-mentioned approaches able learn dependency model thousands even millions parameters model achieve. decision trees popularly used model unary interactions e.g. shotton exceptions used pairwise higher-order interactions. ﬁrst exception paper glesner koller glesner koller decision trees used model conditional probability tables many discrete variables bayesian network. diﬀerence work decision trees evaluate given image content state random variable thus requiring change inference procedure used. second exception random forest random ﬁeld payet todorovic despite similarity name approach fundamentally diﬀerent ours. instead deﬁning explicit model payet todorovic payet todorovic deﬁne model distribution implicitly equilibrium distribution learned metropolishastings markov chain. metropolis-hastings ratio estimated classiﬁcation trees. clever idea comes number limitations test-time choice diﬀerent inference methods bound using ineﬃcient markov chain monte carlo payet todorovic superpixel graphs hundred regions used inference takes seconds despite using advanced swendsen-wang cuts model remains implicit inspecting learned interactions section possible. broader view model richer representation complex label structure. deep architectures latent variable crfs schnitzspan goal hidden variables representing presence larger entities object parts. models successful representing structure generally diﬃcult train negative log-likelihood function longer convex. contrast learning powerful non-parametric conditional interactions achieve similar expressive power retain convexity training problem. describe details model. throughout refer given observed image possible images goal infer discrete labeling labeling per-pixel i.e. variables label denote collection likewise write denote parts contained many diﬀerent subsets assume distinct types denote type factor function factors type variables image content acts upon diﬀers. furthermore function parameterized means weight vector discussed below. visualization small factor graph model shown figure three pairwise factor types unary factor types factors depend image data figure shows unrolled factor graph image size -by- pixels start root tree perform sequence tests image content traversing tree left right. process illustrated figure leaf node reached collect path traversed nodes root node leaf node. node tree associate table energy values depending number variables energy function acts table vector matrix general k-dimensional array denotes nodes taken evaluating tree. using weights node regularize nodes root tree exert stronger inﬂuence aﬀecting large number leaves; test-time precompute summation along root-to-leaf path store result leaf. proposed generalizes number popular existing image labeling methods. ignore pairwise higher-order interactions variables independent making predictions pixel evaluating random forests used e.g. shotton interestingly show experiments even setting still slightly outperform standard random forests since learn weights decision node instead using empirical histograms; novel modiﬁcation improves predictive performance without testtime overhead compared random forests. pairwise interactions generalize simple crfs contrast-sensitive pairwise potentials popular grabcut system rother textonboost shotton finally pairwise interactions decision trees depth interactions depend image content model becomes classic markov random ﬁeld prior learning parameters model need elaborate parameters deﬁne energy. important observation ﬁxed decision trees energy function represented linear parameters this consider single function deﬁne binary indicator function show eﬃcient approximate likelihood methods learn parameters associated decision trees training data. assume given ﬁxed factor types including decision trees learn weights/energies associated nodes trees. discuss learn trees later. pseudolikelihood besag deﬁnes surrogate likelihood function maximized. contrast true likelihood function computing pseudolikelihood tractable eﬃcient. pseudolikelihood v\\{i} v\\{i} write regularized negative multivariate normal distributions constant omitted form optimization depend factor type prior hyperparameter controls overall inﬂuence factor need select suitable value means model selection procedure cross validation. denote subset involves variable likewise restricted factors matching type i.e. summing pixels images weights zero approximately ∇wnpl optimization stop ∇wnpl case around ideally would like learn neighborhood structure decision trees jointly weights using single objective function. however whereas weights continuous decision trees large combinatorial set. therefore propose simple two-step heuristic determine decision tree structure learn classiﬁcation tree using training samples information gain splitting criterion. greedy tree construction popular known work well image labeling problems shotton parameters maximum depth tree minimum number samples required keep growing tree type number split features used. settings diﬀer application application describe experimental section. unlike normal classiﬁcation tree store weights every decision node initialize zero instead storing histograms classes leaf nodes only. procedure easily understood unary interactions show extended straightforward manner learn decision trees pairwise factors well. pairwise factor consider product labels treat label pair single class. training pair labels becomes single class tree classes using information gain criterion. instead storing procedure extends higher-order factors straightforward trees obtained weights zero optimize optimization interaction diﬀerent decision trees taken account. important tree structures determined independently optimize weight independently well would suﬀer overcounting labels training. overcounting problem would occur would want class histograms leaf nodes directly example taking negative log-probability energy. label number factors neighborhood structure. note linear quantities independent order factors. possible pseudolikelihood approximation. moreover even eﬃcient performing single sweep message first observe training procedure parallelizes every step train classiﬁcation trees parallel sharp likewise evaluating gradient large summation independent terms compute parallel communication overhead. second observation every step training procedure carried subsampled training set. classiﬁcation trees process subset pixels shotton less obvious thing optimizing objective ﬁrst term equation takes form empirical expectation ei∼u approximated deterministically means stochastic approximation. deterministic approximation selecting ﬁxed subset evaluating select large enough computation remains eﬃcient; typically million elements. diﬀerent inference methods test-time. making maximum posterior marginal predictions eﬃcient gibbs sampler. gibbs sampling updates quantities used computing unroll graph. obtaining approximate predictions gibbs sampler simulated annealing exploiting model structure. gibbs sampler minimization explained supplementary materials. baseline comparison also minimize using tree-reweighted message passing unrolling factor graph using implementation kolmogorov considered broad range applications report experiments three data sets. experiment reported supplementary materials. show enables improved performance challenging tasks large number interactions parameters need considered cannot manually tuned. moreover show conditional pairwise interactions better represent data lead improved performance. three datasets quite diverse also show broad applicability system. experiment construct task weak local evidence particular label structural information needs propagated test-time order make correct predictions. moreover structure given needs learned training data. consider figure right illustrating task. snake shown input image sequence adjacent pixels color input image encodes direction next pixel means north yellow means sampling uniformly random replacement large east blue means west green means south. background pixel reached snake ends. snake pixels long pixel assigned label starting head tail knowing rules labeling perfectly reconstructed. here however rules need learned training instances. course real system unary interactions typically provide strong cues shotton batra believe task distills limitations noisy unary interactions task making perfect predictions unary would need learn possible snakes length clearly many. standard -neighborhood models. unary decision trees allowed look every pixel input image therefore could remember entire training image. experimental details please supplementary materials. training images test images. results obtained shown table figure random forests trained unary potentials learned markov random ﬁeld perform equally well around upon examining performance further discovered head tail labels labeled perfect accuracy towards middle segments snakes labeling error highest table plausible labels local evidence weakest. using conditional pairwise interactions performance improves almost perfect makes sense pairwise conditional interactions allowed peek color-codes neighbors determining directionality snake. predictions illustrated single test instance figure makes perfect prediction. show uncertainty unary model visualize samples model. learning calligraphy chinese characters previous experiment used standard -connected neighborhood structure. experiment show using larger conditional neighborhoods able represent shape. kaist hanja database handwritten chinese characters. occlude character grey centered image random width height. details please supplementary materials. shown leftmost column figure consider datasets small occlusion large occlusion box. note characters test never observed training model learned shape structure chinese characters still plausible completions input image. unary factor decision tree depth additionally dense pairwise neighborhood structure -connected neighbors pixels distance plus sparse neighbors results large occlusion task shown figure qualitatively show diﬀerence rich connectivity structure conditional interactions. observe example essentially performs smoothing results respecting local stroke-width constraints apparent prediction ﬁrst figure contrast predictions hallucinate meaningful structure quite diﬀerent ground truth bears similarity chinese characters. note achieve rich structure without latent variables. task inpainting task quantitative assessment diﬃcult since task truly ambiguous. therefore report accuracies small-occlusion case reasonable reconstruction ground truth seems feasible. measure per-pixel accuracy occluded area test set. random forest baseline obtain dense neighborhood improves obtains example structure model able learn consider visualization pairwise interactions shown figure ﬁgure shows pairwise interaction learned diagonal energies minus cross-diagonal entries. value negative interaction encouraging pixels take value. marks interactions encourage pixels take diﬀerent values. plot shows strong local smoothing term interestingly symmetric. explained fact horizontal strokes chinese characters typically slanted slightly upwards chen note consider task body part classiﬁcation depth images recently proposed shotton given depth image foreground mask task label pixel belonging diﬀerent body parts shown figure despite variations pose body sizes shotton obtains high-quality recognition results evaluating random forest pixel testing local global depth disparities. task label large amount structure clear suﬃciently complex unary classiﬁer given image cannot implicitly represent structure reasonably well. show adding pairwise interactions fact improve recognition accuracy. moreover make interactions conditional accuracy improves even further. experimental setup follows. subset data used shotton depth images training images testing. train unary decision trees models. pairwise models following neighborhood sizes adding neighborhood pixel away -neighborhood pixels away adding -neighborhood twenty pixels away. conﬁguration variable neighbors. pairwise interactions train trees depth six. detailed description exact experimental setup found supplementary materials. measure results using mean perresults training images shown table instance shown figure even without adding pairwise interactions learned unary weights already outperform random forest classiﬁer shotton adding interactions performance increases dense pairwise interactions represent implicit size preferences body parts. likewise adding conditionality performance improves. best performing model large structure almost million free parameters. trained minutes achieves mean perclass accuracy. setup images original work shotton reports mean class accuracy achieving impressive scaling training images trained core cluster. example learned pairwise interaction shown figure demonstrating improved performance directly attributed powerful interactions allowed take image account. report results supplementary materials. introduced decision tree fields ﬂexible accurate models image labeling tasks. accuracy achieved able represent complex image-dependent structure labels. importantly expressiveness achieved without latent variables therefore learn parameters model eﬃciently minimizing convex function. fig. learned horizontal interactions left ﬁgure shows average depth-normalized silhouette reaching leaf nodes learned decision tree. select leaf show learned weights along path root leaf node. conditional interaction understood visualizing attractive repulsive elements matrix. superimpose arrows attractive repulsive interactions test images ﬁrst second pose exemplify left right upper parts legs appear right matches pattern leaf. conﬁguration like shown third fourth pose plausible leaf pattern thus interaction active. previous part several diﬀerent applications domain computer vision presented demonstrate enhanced descriptive power gained considering arbitrary energies. however gain comes price existing optimization algorithms longer provide good approximations practice. part work addresses issue. concentrates around second axis thesis focuses practical methods approaches approximating minimization challenging arbitrary discrete energies. particular chapter proposes discrete optimization approach correlation clustering functional presented chapters approach scales gracefully number variables better existing approaches fact show discrete approach optimization handle energies deﬁned hundreds thousands variables arise e.g. image segmentation variables existing method handle. chapter concludes part general perspective discrete optimization. perspective inspired multiscale approaches suggests cope np-hardness discrete optimization using multiscale landscape energy function. deﬁning observing multiscale landscape energy propose methods explore exploit derive coarse-to-ﬁne optimization framework. perspective gives rise uniﬁed multiscale framework discrete optimization. proposed multiscale approach applicable diversity discrete energies smoothness-encouraging well arbitrary contrast-enhancing functions. focus chapter optimization correlation clustering functional combines positive negative aﬃnities data points. main contribution chapter optimization algorithms cope large scale problems infeasible using existing methods. draw analogy optimizing functional well known potts energy minimization. analogy allows suggest several optimization algorithms exploit intrinsic model-selection capability functional automatically recover underlying number clusters. compare algorithms existing methods synthetic real data. optimizing tightly related many graph partitioning formulations however known np-hard existing methods derive convex continuous relaxations approximately optimize functional. however algorithms scale beyond thousands variables. example works nowozin jegelka bagon vitaladevuni basri glasner work suggests perspective functional showing analogy known potts model. perspective allows leverage recent advances discrete optimization propose optimization algorithms. show algorithms scale large number variables fact tackle tasks infeasible past e.g. applying pixel-level image segmentation. addition provide rigorous statistical interpretation functional justify intrinsic model selection capability. algorithms exploit model selection property automatically recover underlying number clusters optimizing correlation clustering functional np-hard instead solving directly partition existing methods optimize indirectly binary adjacency matrix i.e. belong cluster. introducing binary beij wijxij. connected components proper rounding resulting clusters number clusters naturally emerges. indirect optimization methods must ascertain feasible consists decomposable achieved either posing semi-deﬁnite constraints introducing large number linear inequalities vitaladevuni basri methods take continuous convex relaxation approach approximate resulting functional. approach allows nice theoretical properties convex optimization cost restricted scalability. directly therefore methods scale poorly number variables fact cannot handle hundreds variables. summary methods suﬀer drawbacks recovering highly susceptible noise importantly infeasible solve large scale problems methods. existing methods view optimization context convex relaxation build upon methods approaches common practice ﬁeld continuous optimization. propose alternative perspective optimization viewing discrete energy minimization. perspective allows build upon recent advances discrete optimization propose eﬃcient direct optimization algorithms. importantly resulting algorithms solve directly thus scales signiﬁcantly better number variables. challenge optimization process non-submodular energy non-submodular. notion submodularity discrete analogue convexity continuous optimization optimizing non-submodular energy np-hard even binary case unknown number labels energies deﬁned ﬁxed known number labels. thus search space restricted larger complicated. unary term unary term energy. unary term plays important role guiding optimization process moreover strong unary term crucial energy non-submodular exist examples crfs literature share characteristics kolmogorov wainwright unknown number labels isack boykov bleyer best knowledge existing exhibits three challenges once. speciﬁcally ﬁrst handle non-submodular energy unary term. therefore cannot oﬀ-the-shelf potts optimization algorithms rather modify improve cope three challenges posed energy. section adapt known discrete energy minimization algorithms cope three challenges posed energy. derive three optimization algorithms stem either large move making algorithms iterated conditional modes besag resulting algorithms scale gracefully number variables solve optimization problems infeasible past. furthermore algorithms take advantage intrinsic model selection capability functional robustly recover underlying number clusters. boykov introduced eﬀective method multi-label energy minimization makes large search steps iteratively solving binary subproblems. large move making algorithms α-expand αβswap diﬀer binary sub-problem solve. α-expand consider variable whether better retain current label label binary step αβ-swap involves variables currently assigned labels consider whether better retain current label switch either deﬁned submodular energies binary step algorithms solved using graph-cut. propose optimization algorithms expand-and-explore swapand-explore cope challenges energy. binary step solver handles non-submodular energies. incorporate model selection iterative search recover underlying number clusters absence unary term good initial labeling provided non-submodular binary solver. binary non-submodular energies approximated extension graph-cuts qpbo binary energy nonsubmodular qpbo guaranteed provide labeling variables. instead outputs partial labeling. many variables labeled depends amount non-submodular pairs relative strength unary term speciﬁc energy. unary term exists energy qpbo leaves variables unlabeled. circumvent behavior improve extension qpbo extension capable improving initial labeling labeling lower energy context expand swap algorithms natural initial labeling binary steps current labels variables qpboi improve ensuring energy increase iterations. overcome problem ﬁnding number clusters algorithms iterate ﬁxed number labels explore empty cluster addition existing clusters current solution. exploring extra empty cluster allows algorithms optimize solutions number clusters fact unary term energy makes straight forward perform. alg. alg. presents expand-and-explore swap-and-explore algorithms detail. another discrete energy minimization method modiﬁed cope three challenges optimization point-wise greedy search algorithm. iteratively variable assigned label minimizes energy conditioned current labels variables. commonly used estimation energies ﬁxed number labels. present adaptive-label using conditional iterations adaptively determine number labels conditioned current labeling assign point cluster attracted singleton cluster repelled all. capable dealing large number variables suitable pixel-level image segmentation. algorithms solve directly cluster membership point thus need rounding scheme extract adjacency matrix number clusters optimally determined algorithm externally supplied like many clustering/segmentation methods. work elsner schudy proposed greedy algorithm optimize functional complete graphs. algorithm fact method presented outside proper context energy minimization thus allow generalize concept discrete optimization recent optimization methods. section evaluates performance proposed optimization algorithms using synthetic real data. compare existing discrete optimization algorithms handle multi-label non-submodular energies pearl existing state-of-the-art optimization method vitaladevuni basri since existing optimization methods scale beyond several hundreds variables extremely small matrices used following experiments. interactive segmentation results evaluated method large scale problems. fig. synthetic results graphs comparing energy convergence. recovered number clusters. purity resulting clusters. time algorithms trw-s almost indistinguishable swap expand plots. experiment uses synthetic aﬃnity matrices compare algorithms existing potts optimization algorithms. synthetic data variables randomly assigned clusters diﬀerent sizes overall resulting percent positive connections purity resulting clusters recovered number clusters rity incorrect recovery demonstrates diﬃculty energy minimization problem unary term many non-submodular pair-wise terms. results accordance observations kolmogorov wainwright energy hard optimize. large move making algorithms expand-and-explore provides marginally better clustering results swap-and-explore. however relatively slow running time makes infeasible large problems. somewhat surprising result experiments shows matrices sparse adaptive-label performs surprisingly well. fact signiﬁcantly faster methods manages converge correct number clusters high purity energy. experiments conclude swap-and-explore good choice optimization algorithm functional. however aﬃnity matrix sparse worth giving adaptive-label shot. following experiment compares algorithms state-of-the-art optimization method r-lp vitaladevuni basri comparison aﬃnity matrices computed co-segmentation. cosegmentation problem formulated correlation clustering problem super pixels variables obtained aﬃnity matrices courtesy glasner used experiments. number variables matrix ranges sparsity ranges roughly number positive negative entries. table shows ratio energy energy r-lp method. table also shows percent matrices algorithms found solution lower energy r-lp. results show superiority algorithms existing multi-label energy minimization approaches furthermore shown methods existing state-of-the-art optimization method small problems. however unlike existing methods algorithms applied problems orders magnitude larger. optimizing directly diﬀerence time expand swap explained looking number variables involved binary steps carried expand algorithm binary step involves almost variables binary swap move considers small subset variables. tab. comparison glasner ratio energy glasner since energies negative higher ratio means lower energy. ratio higher means energy better glasner al.. bottom shows percentage cases method strictly lower energy glasner al.. work suggests perspective functional viewing discrete potts energy. resulting energy minimization presents three challenges energy submodular number clusters known advance unary term. proposed energy minimization algorithms successfully cope challenges. chapter presents uniﬁed multiscale framework discrete energy minimization directly acts energy. approach utilizes algebraic multiscale principles eﬃciently explore discrete solution space. main goal framework improve optimization performance challenging non-submodular energies current methods provide unsatisfactory approximations. furthermore ability derive multiscale pyramid directly energy makes framework application independent. important implications rise independence longer needs tailor multiscale scheme suit diﬀerent application. framework makes trivial turn existing single scale optimization algorithms powerful multiscale methods. framework gives rise complementary energy coarsening strategies coarser scales involve fewer variables revolutionary coarser scales involve fewer discrete labels. empirically evaluated uniﬁed framework variety non-submodular submodular energies including energies middlebury benchmark. discrete energy minimization ubiquitous computer vision spans variety problems segmentation denoising stereo etc. unfortunately apart submodular binary case minimizing energies known np-hard. eﬀort recently developing algorithms approximate discrete optimization ever challenging energies multi-label non-submodular etc. kolmogorov bagon galun discrete energies grossly divided categories submodular energies non-submodular energies. submodular energies characterized smoothness-encouraging pair-wise terms. energies reﬂect piece-wise constant prior popular fig. uniﬁed multiscale framework derive multiscale representation energy energy pyramid. multiscale framework uniﬁed sense diﬀerent problems diﬀerent energies share multiscale scheme making framework widely applicable general. common computer vision applications. reason eﬀort research regarding discrete optimization context computer vision focuses energies encouraging results. practice despite np-hardness energies algorithms developed provide solutions energies close global optimum boykov therefore consider type energies easy optimize. pair wise terms. energies encountered parameters energy learned diﬀerent functionals used glasner comes optimization generally considered challenging task optimize non-submodular energies. since examples nonsubmodular energies recently explored optimization receives less attention consequently existing optimization methods provide approximations quite unsatisfactory. consider energies hard optimize. algorithms discrete energy minimization also classiﬁed categories primal methods dual methods. primal methods directly discrete variables label space minimize energy boykov contrast dual methods formulate dual problem energy maximize lower bound sought energy dual methods recently considered favorable since provide approximate solution also provide lower bound solution global optimum. furthermore labeling found energy equals lower bound certiﬁcate provided global optimum found. since relevant discrete optimization problems np-hard provide empirical evaluation well given algorithm approximates representative instances energies. submodular easy optimize energies shown dual methods tend provide better approximations tight lower bounds. makes discrete energy minimization challenging endeavor? fact minimization implies exploration exponentially large search space makes challenging task. alleviate diﬃculty multiscale search. illustration right shows energy diﬀerent scales detail. considering original scale diﬃcult suggest effective exploration/optimization method. however looking coarser scales energy interesting phenomenon revealed. coarsest scale large basin attraction emerges accuracy. scales become ﬁner loses sight large basin sense local properties higher accuracy. term well known phenomenon multiscale landscape energy. multiscale landscape phenomenon encourages coarse-toﬁne exploration strategies starting large basins apparent coarse scales gradually locally reﬁning search ﬁner scales. three decades vision community focused multiscale pyramid image burt adelson almost experience methods apply multiscale scheme directly discrete energy. paper present novel uniﬁed discrete multiscale optimization scheme acts directly energy approach allows eﬃcient exploration discrete solution space construction energy pyramid. moreover multiscale framework application independent diﬀerent problems diﬀerent energies share multiscale tion lead conclude comes hard optimize non-submodular energies primal methods tend provide better approximations dual methods. motivated observation formulate multiscale framework primal space multiscale framework becomes core optimization process allowing existing oﬀ-the-shelf primal optimization algorithms eﬃciently exploit multiscale landscape energy achieves signiﬁcantly lower energies faster. work makes several contributions novel uniﬁed multiscale framework discrete optimization. wide variety optimization problems including segmentation stereo denoising correlation-clustering others share multiscale framework. energy-aware coarsening scheme. variable aggregation takes account underlying structure energy itself thus eﬃciently directly exposes multiscale landscape. integrating existing single-scale optimization algorithms multiscale framework. achieve signiﬁcantly lower energy assignments diverse computer vision energies including challenging non-submodular examples. optimizing hard non-submodular energies. using several classes non-submodular energies empirically exemplify superiority primal methods. show combining multiscale framework single-scale primal optimization methods achieve increased optimization performance challenging problems. works apply multiscale schemes directly energy. prominent example approach felzenszwalb huttenlocher provide coarse-to-ﬁne belief propagation scheme restricted regular diadic pyramid. recent work komodakis provides algebraic multigrid formulation discrete optimization dual space. however despite general formulation komodakis provides examples using regular diadic grids easy optimize submodular energies. work proposes two-scales scheme mainly aimed improving run-time optimization process. proposed coarsening strategies interpreted special cases uniﬁed framework. analyze underlying assumptions suggest better methods eﬃcient exploration multiscale landscape energy. diﬀerent approach discrete optimization suggests large move making algorithms swendsen wang experimentally show plugging methods within multiscale framework improves optimization results. methods scale gracefully number labels. lempitsky proposed method exploit known properties metric labels allow faster minimization energy. however method restricted energies clear known label metrics requires training. contrast framework addresses issue principled scheme builds energy pyramid decreasing number labels without prior training fewer assumptions labels interactions. build energy pyramid decreasing number degrees freedom. component constructing pyramid interpolation method. interpolation maps solutions levels pyramid deﬁnes approximate original energy fewer degrees freedom. propose novel principled energy aware interpolation method. resulting energy pyramid exposes multiscale landscape energy making energy assignments apparent coarse levels. tute assignment binary matrix }n×l. rows correspond variables columns corresponds labels variable labeled representation allows interpolate discrete solutions shown subsequent sections. energy variables labels parameterized ﬁrst describe energy pyramid construction given interpolation matrix defer detailed description novel interpolation sec. generated coarse energy parameterized approximates energy coarse energy form original energy allowing apply coarsening procedure recursively construct energy pyramid. explored reduction number degrees freedom reducing number variables. however well look problem diﬀerent perspective reducing search space decreasing number labels well known fact optimization algorithms suﬀer signiﬁcant degradation performance number labels increases propose novel principled general framework reducing number labels scale. interpolation matrix time interpolation matrix acts labels i.e. columns coarse labeling matrix number rows fewer columns notation emphasize coarsening aﬀects labels rather variables. fig. interpolation soft variable aggregation variable aggregated coarse variable variables aggregated coarse variable soft aggregation allows variables inﬂuenced coarse variables e.g. variable convex combination hard aggregation special case binary matrix. case variable inﬂuenced exactly coarse variable. main theoretical contribution work encapsulated multiscale trick equations formulation forms basis uniﬁed framework allowing coarsen energy directly exploits multiscale landscape eﬃcient exploration solution space. scheme moves multiscale completely optimization side makes independent speciﬁc application. practically approach wide diverse family energies using multiscale implementation. eﬀectiveness multiscale approximation heavily depends interpolation matrix poorly constructed interpolation matrices fail expose multiscale landscape functional. subsequent section describe principled energy-aware method computing energy pyramid approximates original energy using decreasing number degrees freedom thus excluding solutions original search space coarser scales. solutions excluded determined interpolation matrix desired interpolation exclude energy assignments coarse levels. yields energy. however variables strongly correlated energy aggregating together eﬃciently allows exploration energy assignments. desired interpolation aggregates strongly correlated energy. provide correlations measures used computing variable coarsening used label coarsening energy-aware correlations variables reliable estimation correlations variables allows construct desirable aggregates strongly correlated variables. na¨ıve approach would assume neighboring variables correlated assumption clearly hold general lead undesired interpolation matrix proposed several closed form formulas energy-aware variable grouping. however formulas take account either unary term pair-wise term. indeed diﬃcult decide term dominates fuse terms together. therefore closed form method successfully integrates them. opposed closed form methods propose novel empirical scheme correlation estimation. empirical estimation correlations naturally accounts integrates inﬂuence unary pair-wise terms. moreover method inspired livne brandt extends energies submodular nonsubmodular metric arbitrary arbitrary energies deﬁned regular grids arbitrary graphs. variables correlated energy yields relatively energy value. estimate correlations empirically generate several locally energy assignments measure label agreement neighboring variables iterated conditional modes besag obtain locally energy assignments starting random assignment chooses iteration variable label yielding largest decrease energy function conditioned labels assigned neighbors. interesting note strong correlation variables usually implies pair-wise term binding together smoothness-preserving type relation. assume even challenging energies many contrast-enhancing pair-wise terms still signiﬁcant amount smoothness-preserving terms allow eﬀective coarsening. energy-aware correlations labels correlations labels easier estimate since information explicit matrix using measure variable correlations follow algebraic multigrid method brandt compute interpolation matrix softly aggregates strongly correlated variables. begin selecting coarse representative variables every variable strongly correlated every variable either strongly correlated variables cij. aﬀects coarsening rate i.e. ratio nc/nf smaller starting adding strongly correlated given selected coarse variables maps indices variables typically coarsest scale less variables. result exploring energy scale robust initial assignment single-scale method used. ﬁner scale ﬁner scale serves good initialization computationally intensive modules framework empirical estimation variable correlations single-scale optimization used reﬁne interpolated solutions. complexity easy framework generalizes felzenszwalb huttenlocher komodakis restricted hard aggregation felzenszwalb huttenlocher komodakis multiscale pyramid however variable aggregation energy-aware restricted diadic pyramids. hand limited energy-aware aggregation applied level pyramid. optimize coarse scale cannot reﬁne solution scale. experiments main goals ﬁrst stress diﬃculty approximating non-submodular energies show advantages primal methods type minimization problems. goal demonstrate uniﬁed multiscale framework improved performance existing single-scale primal methods. evaluated multiscale framework diversity discrete optimization tasks ranging challenging non-submodular synthetic co-clustering energies low-level submodular vision energies denoising stereo. addition provide comparison diﬀerent methods measuring variable correlations presented sec. conclude label coarsening experiment. experiments minimize given publicly available benchmark energy attempt improve energy formulation itself. αβ-swap α-expansion representative single-scale oﬀthe-shelf primal optimization algorithms. help large move making algorithms overcome non-submodularity energies augment qpbo rother follow protocol szeliski uses lower bound trw-s baseline comparing performance diﬀerent optimization methods diﬀerent energies. report close results lower bound closer better. show remarkable improvement combined multiscale framework compared single-scale scheme. large move making algorithms smaller consistent improvement multiscale single scale scheme. trw-s dual method considered stateof-the-art discrete energy minimization szeliski however show comes non-submodular energies struggles behind resulting synthetic energies non-submodular challenging energies state-of-the-art dual method performs rather poorly signiﬁcant lower bound energy actual primal solution provided. among primal methods used results motivate focusing primal methods especially αβ-swap. ﬁned -connected grid. energies designed trained perform task learning chinese calligraphy represented complex non-local binary pattern. despite large number parameters fig. chinese characters inpainting visualizing instances used experiments. columns original character used testing. input partially occluded character. qpbo results multiscale single scale results. results trw-s results nowozin obtained long simulated annealing tab. energies chinese characters inpainting table showing mean energies inpainting experiment relative baseline nowozin percent instances strictly lower energy achieved. fig. energies chinese characters inpainting plot showing median resulting energies relative reference energies nowozin multiscale approach combined qpbo achieves consistently better energies baseline variance. trw-s improves instances high variance results. involved representing complex energies learning conducted eﬃciently using decision tree field main challenge models becomes inference test time. experiments show approaching challenging energies using uniﬁed multiscale framework allows better approximations. table fig. compare multiscale framework single-scale methods acting primal binary variables. since energies binary qpbo instead large move making algorithms. also provide evaluation dual method energies. addition quantitative results fig. provides visualization instances restored chinese characters. real world energies highlight advantage primal methods dual ones comes challenging non-submodular energies. clear signiﬁcant improvement made multiscale framework. potts energy. obtained co-clustering energies courtesy glasner used experiments. number variables energy ranges sparsity report results percent baseline smaller better lower even outperforms state-of-the-art. also report fraction energies multiscale framework outperform state-of-the-art. table compares discrete multiscale framework combined αβ-swap. energies diﬀerent baseline state-of-theart results glasner obtained applying specially tailored convex relaxation method multiscale framework improves state-of-the-art family challenging energies. semi-metric energies trw-s performs quite well fact enough iterations allowed lower bound converges global optimum. opposed trw-s large move making always converge global optimum. able show signiﬁcant improvement primal optimization algorithms used within multiscale framework. tables figs. show multiscale results diﬀerent submodular energies. fig. stereo note multiscale framework drastically improves results. visible improvement αβ-swap also seen middle numerical results examples shown table fig. denoising inpainting single scale unable cope inpainting performing local steps unable propagate information enough missing regions images. hand multiscale framework allows perform large steps coarse scales successfully gaps. numerical results examples shown table explained sec. correlations variables crucial component constructing eﬀective multiscale scheme. experiment compare energy-aware correlation measure three methods proposed unary-diﬀ min-unary-diﬀ mean-compat. methods estimate correlations based either unary term pair-wise term both. also compare fig. shows percent lower bound diﬀerent energies. measure consistently outperforms methods successfully balances inﬂuence unary pair-wise terms. αβ-swap scale gracefully number labels. coarsening energy labels domain proves signiﬁcantly improve performance αβ-swap shown table examples constructing energy pyramid took milliseconds closed form formula estimating label correlations. work presents uniﬁed multiscale framework discrete energy minimization allows eﬃcient direct exploration multiscale landscape energy. propose paths expose multiscale landscape energy coarser scales involve fewer coarser variables another coarser levels involve fewer labels. also propose adaptive methods energy-aware interpolation scales. multiscale framework signiﬁcantly improves optimization results challenging energies. framework provides mathematical formulation bridges relates multiscale discrete optimization algebraic multiscale methods used solvers connection allows methods practices developed numerical solvers applied multiscale discrete optimization well. note diagonal assumed zero reasonable assumption represents interaction variable itself. type interaction well represented unary term coarsening energy happen longer zeros diagonal. case arise single coarse variable represents neighboring scale variables. case scale pair-wise interaction absorbed coarse scale unary term. easy term wiivαα added unary term whenever zeros diagonal. rectiﬁcation non-zeros entries diagonal zero. pair-wise energies. used slightly restricted form throughout paper since emphasizes simplicity algebraic derivation multiscale framework. nevertheless multiscale framework easily applied general energies. coarsening variables computation interpolation matrix unchanged. applied energies form estimate agreement neighboring variables. agreements used compute interpolation matrix order write coarsening pair-wise term need introduce notations variables scale denote variables coarse scale. entry interpolation matrix indicates scale variable aﬀected coarse scale variable given interpolation matrix coarsen energy coarsening labels case computation interpolation matrix trivial since longer single matrix derive agreements from. leave issue deriving eﬃcient interpolation matrix general case future work. however given matrix coarsening labels done easily discrete energies involve terms beyond pair-wise describe interaction sets variables. energies often referred high-order energies. examples energies found e.g. kohli rother coarsening variables computation interpolation matrix unchanged. applied energies form estimate agreement neighboring variables. agreements used compute interpolation matrix ﬁne-scale variable represented coarse variable size reduced relative size coarsening labels case computation interpolation matrix trivial since longer clear representation interactions diﬀerent labels. leave issue deriving eﬃcient interpolation matrix general case future work. however given matrix coarsening labels done easily. using denote coarse scale labels work motivated challenging discrete energies beyond semi-metric diﬀerent tasks computer vision. conclude despite computational challenges posed stepping away form well studied smoothness-encouraging energies gain terms expressiveness quality solution worth while. applications energies describe better underlying process therefore better suited modeling. moreover practical algorithms approaches proposed work cope resulting challenging optimization tasks. considering task unsupervised clustering example using arbitrary potts energy correlation clustering functional allows model partition data diﬀerent clusters also recover underlying number clusters. recovery number cluster issue usually evaded many clustering algorithms assume information somehow given input. thesis showed challenging optimization functional lies core several diverse applications sketching common unsupervised face identiﬁcation interactive segmentation. focusing optimization example work also proposed several practical approximation algorithms. functional casted special case arbitrary potts energy minimization. discrete formulation functional allows adapt utilize known methods discrete optimization resulting optimization algorithms capable handling large number variables another example challenging arbitrary energy task shape reconstruction. problem reconstruction images common lighting conditions known object motion respect static camera discretized form challenging energy. energy emerges induced integrality constraints recovered surface. deﬁned large label space despite challenging pair-wise terms large label space successful approximation scheme proposed adapted task. comes learning parameters discrete energies various tasks computer vision happens underlying data better explained contrast enhancing terms chinese characters body parts. fact learning procedure restricted prefer contrast-enhancing functions better depictions underlying statistical behavior training set. therefore restricting learned model submodular semi-metric energies might severely compromise ability generalize novel exemplars. diﬃcult task. work proposes several methods provide good empirical approximations variety practical energies. uniﬁed discrete multiscale framework succeeds providing good approximations practice variety challenging energies. multiscale framework exploits underlying multiscale landscape given energy exposes allow eﬃcient eﬀective coarse-to-ﬁne optimization process. formulating coarsening procedure clear algebraic formulation allows propose paths expose multiscale landscape energy coarser scales involve fewer coarser variables another coarser levels involve fewer labels. also propose adaptive methods energy-aware interpolation scales. approach exploits underlying multiscale landscape given energy exposes allow eﬃcient eﬀective coarse-to-ﬁne optimization process. framework provides mathematical formulation bridges relates multiscale discrete optimization algebraic multiscale methods used solvers connection allows methods practices developed numerical solvers applied multiscale discrete optimization well. c.-p. chen c.-s. chen y.-p. hung. pixel-based correspondence shape reconstruction moving objects. workshop color reﬂectance imaging computer vision demaine immorlica. correlation clustering partial information. approximation randomization combinatorial optimization. algorithms techniques pages shotton winn rother criminisi. textonboost image understanding multi-class object recognition segmentation jointly modeling texture layout context. ijcv szeliski zabih scharstein veksler kolmogorov agarwala tappen rother. comparative study energy minimization methods markov random ﬁelds smoothness-based priors. pami", "year": 2012}