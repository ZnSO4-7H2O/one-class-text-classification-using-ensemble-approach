{"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB  model size", "tag": ["cs.CV", "cs.AI"], "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).  The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet", "text": "forrest iandola song matthew moskewicz khalid ashraf william dally kurt keutzer deepscale∗ berkeley {forresti moskewcz kashraf keutzer}eecs.berkeley.edu {songhan dally}stanford.edu recent research deep convolutional neural networks focused primarily improving accuracy. given accuracy level typically possible identify multiple architectures achieve accuracy level. equivalent accuracy smaller architectures offer least three advantages smaller cnns require less communication across servers distributed training. smaller cnns require less bandwidth export model cloud autonomous car. smaller cnns feasible deploy fpgas hardware limited memory. provide advantages propose small architecture called squeezenet. squeezenet achieves alexnet-level accuracy imagenet fewer parameters. additionally model compression techniques able compress squeezenet less https//github.com/deepscale/squeezenet much recent research deep convolutional neural networks focused increasing accuracy computer vision datasets. given accuracy level typically exist multiple architectures achieve accuracy level. given equivalent accuracy architecture fewer parameters several advantages efﬁcient distributed training. communication among servers limiting factor scalability distributed training. distributed data-parallel training communication overhead directly proportional number parameters model short small models train faster requiring less communication. less overhead exporting models clients. autonomous driving companies tesla periodically copy models servers customers’ cars. practice often referred over-the-air update. consumer reports found safety tesla’s autopilot semi-autonomous driving functionality incrementally improved recent over-the-air updates however over-theair updates today’s typical cnn/dnn models require large data transfers. alexnet would require communication server car. smaller models require less communication making frequent updates feasible. feasible fpga embedded deployment. fpgas often less onchip memory off-chip memory storage. inference sufﬁciently small model could stored directly fpga instead bottlenecked memory bandwidth video frames stream fpga real time. further deploying cnns application-speciﬁc integrated circuits sufﬁciently small model could stored directly on-chip smaller models enable asic smaller die. several advantages smaller architectures. mind focus directly problem identifying architecture fewer parameters equivalent accuracy compared well-known model. discovered architecture call squeezenet. addition present attempt disciplined approach searching design space novel architectures. rest paper organized follows. section review related work. then sections describe evaluate squeezenet architecture. that turn attention understanding architectural design choices impact model size accuracy. gain understanding exploring design space squeezenet-like architectures. section design space exploration microarchitecture deﬁne organization dimensionality individual layers modules. section design space exploration macroarchitecture deﬁne high-level organization layers cnn. finally conclude section short sections useful researchers well practitioners simply want apply squeezenet application. remaining sections aimed advanced researchers intend design architectures. related work model compression overarching goal work identify model parameters preserving accuracy. address problem sensible approach take existing model compress lossy fashion. fact research community emerged around topic model compression several approaches reported. fairly straightforward approach denton apply singular value decomposition pretrained model developed network pruning begins pretrained model replaces parameters certain threshold zeros form sparse matrix ﬁnally performs iterations training sparse recently extended work combining network pruning quantization huffman encoding create approach called deep compression designed hardware accelerator called operates directly compressed model achieving substantial speedups energy savings. microarchitecture convolutions used artiﬁcial neural networks least years; lecun helped popularize cnns digit recognition applications late neural networks convolution ﬁlters typically height width channels dimensions. applied images ﬁlters typically channels ﬁrst layer subsequent layer ﬁlters number channels ﬁlters. early work lecun uses xxchannels ﬁlters recent architectures extensively ﬁlters. models network-in-network googlenet family architectures ﬁlters layers. trend designing deep cnns becomes cumbersome manually select ﬁlter dimensions layer. address this various higher level building blocks modules comprised multiple convolution layers speciﬁc ﬁxed organization proposed. example googlenet papers propose inception modules comprised number different dimensionalities ﬁlters usually including plus sometimes sometimes many modules combined perhaps additional ad-hoc layers form complete network. term microarchitecture refer particular organization dimensions individual modules. macroarchitecture microarchitecture refers individual layers modules deﬁne macroarchitecture system-level organization multiple modules end-to-end architecture. perhaps mostly widely studied macroarchitecture topic recent literature impact depth networks. simoyan zisserman proposed family cnns layers reported deeper networks produce higher accuracy imagenet-k dataset proposed deeper cnns layers deliver even higher imagenet accuracy choice connections across multiple layers modules emerging area macroarchitectural research. residual networks highway networks propose connections skip multiple layers example additively connecting activations layer activations layer refer connections bypass connections. authors resnet provide comparison -layer without bypass connections; adding bypass connections delivers percentage-point improvement top- imagenet accuracy. neural network design space exploration neural networks large design space numerous options microarchitectures macroarchitectures solvers hyperparameters. seems natural community would want gain intuition factors impact nn’s accuracy much work design space exploration focused developing automated approaches ﬁnding architectures deliver higher accuracy. automated approaches include bayesian optimization simulated annealing randomized search genetic algorithms credit papers provides case proposed approach produces architecture achieves higher accuracy compared representative baseline. however papers make attempt provide intuition shape design space. later paper eschew automated approaches instead refactor cnns principled comparisons investigate architectural decisions inﬂuence model size accuracy. following sections ﬁrst propose evaluate squeezenet architecture withmodel compression. then explore impact design choices microarchitecture macroarchitecture squeezenet-like architectures. squeezenet preserving accuracy parameters section begin outlining design strategies architectures parameters. then introduce fire module building block build architectures. finally design strategies construct squeezenet comprised mainly fire modules. architectural design strategies overarching objective paper identify architectures parameters maintaining competitive accuracy. achieve this employ three main strategies designing architectures strategy replace ﬁlters ﬁlters. given budget certain number convolution ﬁlters choose make majority ﬁlters since ﬁlter fewer parameters ﬁlter. strategy decrease number input channels ﬁlters. consider convolution layer comprised entirely ﬁlters. total quantity parameters layer maintain small total number parameters important decrease number ﬁlters also decrease number input channels ﬁlters. decrease number input channels ﬁlters using squeeze layers describe next section. strategy downsample late network convolution layers large activation maps. convolutional network convolution layer produces output activation spatial resolution least often much larger height width activation maps controlled size input data choice layers downsample architecture. commonly downsampling engineered architectures setting convolution pooling layers early layers network large strides layers small activation maps. conversely layers network stride strides greater concentrated toward network many layers network large activation maps. intuition large activation maps lead higher classiﬁcation accuracy else held equal. indeed applied delayed downsampling four different architectures case delayed downsampling higher classiﬁcation accuracy strategies judiciously decreasing quantity parameters attempting preserve accuracy. strategy maximizing accuracy limited budget parameters. next describe fire module building block architectures enables successfully employ strategies fire module deﬁne fire module follows. fire module comprised squeeze convolution layer feeding expand layer convolution ﬁlters; illustrate figure liberal ﬁlters fire modules application strategy section expose three tunable dimensions fire module fire module number ﬁlters squeeze layer number ﬁlters expand layer number ﬁlters expand layer. fire modules less squeeze layer helps limit number input channels ﬁlters strategy section squeezenet architecture describe squeezenet architecture. illustrate figure squeezenet begins standalone convolution layer followed fire modules ending ﬁnal conv layer gradually increase number ﬁlters module beginning network. squeezenet performs max-pooling stride layers conv conv; relatively late placements pooling strategy section present full squeezenet architecture table squeezenet details brevity omitted number details design choices squeezenet table figure provide design choices following. intuition behind choices found papers cited below. -pixel border zero-padding input data ﬁlters expand modules. relu applied activations squeeze expand layers. dropout ratio applied module. note lack fully-connected layers squeezenet; design choice inspired training squeezenet begin learning rate linearly decrease learning rate throughout training described details training protocol please refer caffe-compatible conﬁguration ﬁles located here https//github.com/deepscale/squeezenet. caffe framework natively support convolution layer contains multiple ﬁlter resolutions around this implement expand layer separate convolution layers layer ﬁlters layer ﬁlters. then concatenate outputs layers together channel dimension. numerically equivalent implementing layer contains ﬁlters. released squeezenet conﬁguration ﬁles format deﬁned caffe framework. however addition caffe several frameworks emerged including mxnet chainer keras torch native format representing architecture. said libraries underlying computational back-ends cudnn mkl-dnn research community evaluation squeezenet turn attention evaluating squeezenet. model compression papers reviewed section goal compress alexnet model trained classify images using imagenet dataset. therefore alexnet associated model compression results basis comparison evaluating squeezenet. table review squeezenet context recent model compression results. svdbased approach able compress pretrained alexnet model factor diminishing top- accuracy network pruning achieves reduction model size maintaining baseline top- top- accuracy imagenet deep compression achieves reduction model size still maintaining baseline accuracy level squeezenet achieve reduction model size compared alexnet meeting exceeding top- top- accuracy alexnet. summarize aforementioned results table appears surpassed state-of-the-art results model compression community even using uncompressed -bit values represent model squeezenet smaller model size best efforts model compression community maintaining exceeding baseline accuracy. open question been small models amenable compression small models need representational power afforded dense ﬂoating-point values? applied deep compression table comparing squeezenet model compression approaches. model size mean number bytes required store parameters trained model. reduction model size alexnet squeezenet using sparsity -bit quantization. yields model equivalent accuracy alexnet. further applying deep compression -bit quantization sparsity squeezenet produce model equivalent accuracy. small model indeed amenable compression. addition results demonstrate deep compression works well architectures many parameters also able compress already compact fully convolutional squeezenet architecture. deep compression compressed squeezenet preserving baseline accuracy. summary combining architectural innovation state-of-the-art compression techniques achieved reduction model size decrease accuracy compared baseline. finally note deep compression uses codebook part scheme quantizing parameters -bits precision. therefore commodity processors trivial achieve speedup -bit quantization using scheme developed deep compression. however developed custom hardware efﬁcient inference engine compute codebook-quantized cnns efﬁciently addition months since released squeezenet gysel developed strategy called ristretto linearly quantizing squeezenet bits speciﬁcally ristretto computation bits stores parameters activations -bit data types. using ristretto strategy -bit computation squeezenet inference gysel observed less percentage-point drop accuracy using -bit instead -bit data types. microarchitecture design space exploration proposed architectural design strategies small models followed principles create squeezenet discovered squeezenet smaller alexnet equivalent accuracy. however squeezenet models reside broad largely unexplored design space architectures. sections explore several aspects design space. divide architectural exploration main topics microarchitectural exploration macroarchitectural exploration section design execute experiments goal providing intuition shape microarchitectural design space respect design strategies proposed section note goal maximize accuracy every experiment rather understand impact architectural choices model size accuracy. microarchitecture metaparameters squeezenet fire module three dimensional hyperparameters deﬁned section squeezenet fire modules total dimensional hyperparameters. broad sweeps design space squeezenet-like architectures deﬁne following higher level metaparameters control dimensions fire modules cnn. deﬁne basee number expand ﬁlters ﬁrst fire module cnn. every fire modules increase number expand ﬁlters incre. expand layer fire module ﬁlters deﬁne pctx shared fire modules) percentage expand ﬁlters words pctx finally deﬁne number ﬁlters squeeze layer fire module using metaparameter called squeeze ratio shared fire modules) squeezenet example architecture generated aforementioned metaparameters. speciﬁcally squeezenet following metaparameters basee incre pctx squeeze ratio section proposed decreasing number parameters using squeeze layers decrease number input channels seen ﬁlters. deﬁned squeeze ratio ratio number ﬁlters squeeze layers number ﬁlters expand layers. design experiment investigate effect squeeze ratio model size accuracy. experiments squeezenet starting point. squeezenet experiments following metaparameters basee incre pctx train multiple models model different squeeze ratio range figure show results experiment point graph independent model trained scratch. squeezenet sr=. point ﬁgure. ﬁgure learn increasing beyond increase imagenet top- accuracy model model. accuracy plateaus sr=. setting sr=. increases model size without improving accuracy. architectures spatial resolution layers’ ﬁlters; googlenet network-in-network ﬁlters layers. googlenet authors simply propose speciﬁc quantity ﬁlters without analysis. here attempt shed light proportion ﬁlters affects model size accuracy. following metaparameters experiment basee incre vary pctx words fire module’s expand layer predeﬁned number ﬁlters partitioned turn knob ﬁlters mostly mostly previous experiment models fire modules following organization layers figure show results experiment figure note models figure figure architecture pctx figure top- accuracy plateaus using ﬁlters increasing percentage ﬁlters leads larger model size provides improvement accuracy imagenet. macroarchitecture design space exploration explored design space microarchitecture level i.e. contents individual modules cnn. explore design decisions macroarchitecture level concerning high-level connections among fire modules. inspired resnet explored three different architectures illustrate three variants squeezenet figure simple bypass architecture adds bypass connections around fire modules requiring modules learn residual function input output. resnet implement bypass connection around fire input fire equal operator elementwise addition. changes regularization applied parameters fire modules resnet improve ﬁnal accuracy and/or ability train full model. limitation that straightforward case number input channels number output channels same; result half fire modules simple bypass connections shown middle diagram same number channels requirement can’t complex bypass connection illustrated right figure simple bypass just wire deﬁne complex bypass bypass includes convolution layer number ﬁlters equal number output channels needed. note complex bypass connections extra parameters model simple bypass connections not. addition changing regularization intuitive adding bypass connections would help alleviate representational bottleneck introduced squeeze layers. squeezenet squeeze ratio meaning every squeeze layer fewer output channels accompanying expand layer. severe dimensionality reduction limited amount information pass squeeze layers. however adding bypass connections squeezenet open avenues information around squeeze layers. trained squeezenet three macroarchitectures figure compared accuracy model size table ﬁxed microarchitecture match squeezenet described table throughout macroarchitecture exploration. complex simple bypass connections yielded accuracy improvement vanilla squeezenet architecture. interestingly simple bypass enabled higher accuracy accuracy improvement complex bypass. adding conclusions paper proposed steps toward disciplined approach design-space exploration convolutional neural networks. toward goal presented squeezenet architecture fewer parameters alexnet maintains alexnet-level accuracy imagenet. also compressed squeezenet less smaller alexnet without compression. since released paper technical report song collaborators experimented squeezenet model compression. using approach called dense-sparse-dense model compression training regularizer improve accuracy producing compressed squeezenet parameters percentage-points accurate imagenet-k also producing uncompressed squeezenet parameters percentage-points accurate compared results table mentioned near beginning paper small models amenable on-chip implementations fpgas. since released squeezenet model gschwend developed variant squeezenet implemented fpga anticipated gschwend able able store parameters squeezenet-like model entirely within fpga eliminate need off-chip memory accesses load model parameters. context paper focused imagenet target dataset. however become common practice apply imagenet-trained representations variety applications ﬁne-grained object recognition logo identiﬁcation images generating sentences images imagenettrained cnns also applied number applications pertaining autonomous driving including pedestrian vehicle detection images videos well segmenting shape road think squeezenet good candidate architecture variety applications especially small model size importance. squeezenet several cnns discovered broadly exploring design space architectures. hope squeezenet inspire reader consider explore broad range possibilities design space architectures perform exploration systematic manner. tianqi chen yutian naiyan wang minjie wang tianjun xiao bing chiyuan zhang zheng zhang. mxnet ﬂexible efﬁcient machine learning library heterogeneous distributed systems. arxiv. sharan chetlur cliff woolley philippe vandermersch jonathan cohen john tran bryan catanzaro evan shelhamer. cudnn efﬁcient primitives deep learning. arxiv. dipankar sasikanth avancha dheevatsa mudigere karthikeyan vaidyanathan srinivas sridharan dhiraj kalamkar bharat kaul pradeep dubey. distributed deep learning using synchronous stochastic gradient descent. arxiv. jeff donahue yangqing oriol vinyals judy hoffman ning zhang eric tzeng trevor darrell. decaf deep convolutional activation feature generic visual recognition. arxiv. fang saurabh gupta forrest iandola rupesh srivastava deng piotr dollar jianfeng xiaodong margaret mitchell john platt lawrence zitnick geoffrey zweig. captions visual concepts back. cvpr song xingyu huizi jing ardavan pedram mark horowitz william dally. efﬁcient inference engine compressed deep neural network. international symposium computer architecture song jeff pool sharan narang huizi shijian tang erich elsen bryan catanzaro john tran william dally. regularizing deep neural networks dense-sparse-dense training ﬂow. arxiv. forrest iandola matthew moskewicz sergey karayev ross girshick trevor darrell kurt keutzer. densenet implementing efﬁcient convnet descriptor pyramids. arxiv. yangqing evan shelhamer jeff donahue sergey karayev jonathan long ross girshick sergio guadarrama trevor darrell. caffe convolutional architecture fast feature embedding. arxiv. lecun b.boser j.s. denker henderson r.e. howard hubbard l.d. jackel. backpropagation applied handwritten code recognition. neural computation jiantao wang song kaiyuan boxun erjin zhou jincheng tianqi tang ningyi song wang huazhong yang. going deeper embedded fpga platform convolutional neural network. international symposium fpga christian szegedy yangqing pierre sermanet scott reed dragomir anguelov dumitru erhan vincent vanhoucke andrew rabinovich. going deeper convolutions. arxiv.", "year": 2016}