{"title": "Borrowing Treasures from the Wealthy: Deep Transfer Learning through  Selective Joint Fine-tuning", "tag": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "Deep neural networks require a large amount of labeled training data during supervised learning. However, collecting and labeling so much data might be infeasible in many cases. In this paper, we introduce a source-target selective joint fine-tuning scheme for improving the performance of deep learning tasks with insufficient training data. In this scheme, a target learning task with insufficient training data is carried out simultaneously with another source learning task with abundant training data. However, the source learning task does not use all existing training data. Our core idea is to identify and use a subset of training images from the original source learning task whose low-level characteristics are similar to those from the target learning task, and jointly fine-tune shared convolutional layers for both tasks. Specifically, we compute descriptors from linear or nonlinear filter bank responses on training images from both tasks, and use such descriptors to search for a desired subset of training samples for the source learning task.  Experiments demonstrate that our selective joint fine-tuning scheme achieves state-of-the-art performance on multiple visual classification tasks with insufficient training data for deep learning. Such tasks include Caltech 256, MIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to fine-tuning without a source domain, the proposed method can improve the classification accuracy by 2% - 10% using a single model.", "text": "large-scale image datasets imagenet ilsvrc dataset places coco series breakthroughs visual recognition including image classiﬁcation object detection semantic segmentation many related visual tasks beneﬁted breakthroughs. nonetheless researchers face dilemma using deep convolutional neural networks perform visual tasks sufﬁcient training data. training deep network insufﬁcient data might even give rise inferior performance comparison traditional classiﬁers handcrafted features. fine-grained classiﬁcation problems oxford flowers stanford dogs examples. number training samples datasets enough training large-scale deep neural networks networks would become overﬁt quickly. solving overﬁtting problem deep convolutional neural networks learning tasks without sufﬁcient training data challenging transfer learning techniques apply knowledge learnt task related tasks proven helpful context deep learning ﬁne-tuning deep network pre-trained imagenet places dataset common strategy learn taskspeciﬁc deep features.this strategy considered simple transfer learning technique deep learning. however since ratio number learnable parameters number training samples still remains same ﬁne-tuning needs terminated relatively small number iterations; otherwise overﬁtting still occurs. paper attempt tackle problem training deep neural networks learning tasks insufﬁcient training data. adopt source-target joint training methodology ﬁne-tuning deep neural networks. original learning task without sufﬁcient training data called target learning task boost performance target learning task teamed another learning task rich training data. latter called source learning task suppose source learning task large-scale training target learning task small-scale training since target deep neural networks require large amount labeled training data supervised learning. however collecting labeling much data might infeasible many cases. paper introduce deep transfer learning scheme called selective joint ﬁne-tuning improving performance deep learning tasks insufﬁcient training data. scheme target learning task insufﬁcient training data carried simultaneously another source learning task abundant training data. however source learning task existing training data. core idea identify subset training images original source learning task whose low-level characteristics similar target learning task jointly ﬁne-tune shared convolutional layers tasks. speciﬁcally compute descriptors linear nonlinear ﬁlter bank responses training images tasks descriptors search desired subset training samples source learning task. experiments demonstrate deep transfer learning scheme achieves state-of-the-art performance multiple visual classiﬁcation tasks insufﬁcient training data deep learning. tasks include caltech indoor ﬁne-grained classiﬁcation problems comparison ﬁne-tuning without source domain proposed method improve classiﬁcation accuracy using single model. codes models available https//github.com/zyyszj/ selective-joint-fine-tuning. convolutional neural networks become deeper larger pursue increasingly better performance classiﬁcation recognition tasks looking successes deep learning computer vison large amount training pretraining data essential training deep neural networks. learning task likely specialized task envisage image signals dataset possess certain unique low-level characteristics learned kernels convolutional layers deep network need grasp characteristics order generate highly discriminative features. thus supplying sufﬁcient training images similar low-level characteristics becomes important mission source learning task. core idea identify subset training images whose low-level characteristics similar jointly ﬁne-tune shared convolutional layers source target learning tasks. source learning task ﬁne-tuned using selected training images only. hence process called selective joint ﬁne-tuning. rationale behind unique low-level characteristics images might overwhelmed images taken training samples source learning task. select images share similar low-level characteristics since kernels followed nonlinear activation deep convolutional neural network actually nonlinear spatial ﬁlters sufﬁcient data training high-quality kernels responses existing linear nonlinear ﬁlter banks deﬁne similarity low-level characteristics. gabor ﬁlters form example linear ﬁlter bank complete kernels certain layers pretrained form example nonlinear ﬁlter bank. histograms ﬁlter bank responses image descriptors search images similar low-level characteristics. motivation behind selecting images according low-level characteristics fold. first low-level characteristics extracted kernels lower convolutional layers deep network. lower convolutional layers form foundation entire network quality features extracted layers determines quality features higher levels deep network. sufﬁcient training images sharing similar low-level characteristics could strength kernels layers. second images similar low-level characteristics could different high-level semantic contents. therefore searching images using low-level characteristics less restrictions return much training images using high-level semantic contents. source-target selective joint ﬁne-tuning scheme expected beneﬁt target learning task different ways. first since convolutional layers shared learning tasks selected training samples source learning task prevent deep network overﬁtting quickly. second since selected training samples source learning task share similar lowlevel characteristics target learning task kernels shared convolutional layers trained proposed source-target selective joint ﬁne-tuning scheme easy implement. experimental results demonstrate state-of-the-art performance multiple visual classiﬁcation tasks much less training samples required recent deep learning architectures. visual classiﬁcation tasks include ﬁne-grained classiﬁcation stanford dogs oxford flowers image classiﬁcation caltech scene classiﬁcation indoor summary paper following contributions introduce deep transfer learning scheme called selective joint ﬁne-tuning improving performance deep learning tasks insufﬁcient training data. important step forward context widely adopted strategy ﬁne-tuning pre-trained deep neural network. develop novel pipeline implementing deep transfer learning scheme. speciﬁcally compute descriptors linear nonlinear ﬁlter bank responses training images tasks descriptors search desired subset training samples source learning task. experiments demonstrate deep transfer learning scheme achieves state-of-the-art performance multiple visual classiﬁcation tasks insufﬁcient training data deep learning. related work multi-task learning. multi-task learning obtains shared feature representations classiﬁers related tasks comparison learning individual tasks independently features classiﬁers learned often better generalization capability. deep learning faster rcnn jointly learns object locations labels using shared convolutional layers different loss functions tasks. multi-scale convolutional architecture used predict depth surface normals semantic labels. indicates convolutional neural networks adapted different tasks easily. previous work attempts shared feature space beneﬁts multiple learning tasks proposed joint training scheme paper focuses learning shared feature space improves performance target learning task only. feature extraction fine-tuning. off-the-shelf features proven powerful various computer vision problems. pre-training convolutional neural networks imagenet places standard practice vision problems. however features learnt pre-trained models tailored figure pipeline proposed selective joint ﬁne-tuning. left right datasets source domain target domain. select nearest neighbors target domain training sample source domain low-level feature space. deep convolutional neural network initialized weights pre-trained imagenet places. jointly optimize source target cost functions label spaces. target learning task. fine-tuning pre-trained models become commonly used method learn task-speciﬁc features. transfer ability different convolutional layers cnns investigated however tasks sufﬁcient training data overﬁtting occurs quickly ﬁne-tuning. proposed pipeline paper alleviates overﬁtting also attempts discriminative feature space target learning task. transfer learning. different transfer learning applies knowledge learnt domain related tasks. domain adaptation algorithms divided three categories including instance adaption feature adaption model adaption hong transferred rich semantic information source categories target categories attention model. tzeng performed feature adaptation using shared convolutional neural network transferring class relationship source domain target domain. make pipeline ﬂexible paper assume source target label spaces different work randomly resamples training classes images source domain paper conducts special type transfer learning selecting source training samples nearest neighbors samples target domain space certain low-level image descriptor. form training set. method search nearest neighbors large-scale labeled dataset using lowlevel features instead high-level semantic information. shown low-level features computed bottom layers encode rich information completely reconstruct original image. experimental results show nearest neighbor search using low-level features outperform using high-level semantic information shows overall pipeline proposed source-target selective joint ﬁne-tuning scheme. given target learning task insufﬁcient training data perform selective joint ﬁne-tuning follows. entire training dataset associated target learning task called target domain. source domain deﬁned similarly. source domain minimum requirement number images source domain large enough train deep convolutional neural network scratch. ideally training images present diversiﬁed low-level characteristics. running ﬁlter bank give rise diversiﬁed responses possible. exist large-scale visual recognition datasets serve source domain including imagenet ilsvrc dataset places coco filter bank responses ﬁlter bank describe low-level characteristics image. ﬁrst ﬁlter bank gabor ﬁlter bank. gabor ﬁlters commonly used feature description especially texture description gabor ﬁlter responses powerful lowlevel features image pattern analysis. parameter setting reference. real imaginary parts convolutional kernels scales orientations. thus gabor ﬁlters total. kernels deep convolutional neural network actually spatial ﬁlters. nonlinear activation following kernel combination kernel nonlinear activation essentially nonlinear ﬁlter. deep extract low/middle/high level features different convolutional layers convolutional layers close input data focus extract low-level features away input extract middlehigh-level features. fact subset kernels ﬁrst convolutional layer alexnet trained imagenet exhibit oriented stripes similar gabor ﬁlters trained large-scale diverse dataset imagenet kernels used describing generic low-level image characteristics. practice kernels ﬁrst second convolutional layers alexnet pre-trained imagenet second choice ﬁlter bank. image descriptor denote response i-th convolutional kernel gabor ﬁlter ﬁlter bank histogram. obtain discriminative histogram features ﬁrst obtain upper bound i-th response scanning lower bound entire target domain interval divided small bins. adaptively width every histogram contains roughly equal percentage pixels. manner avoid large percentage pixels falling bin. concatenate histograms ﬁlter response maps form nearest neighbor ranking given histogram-based descriptor training image target domain search nearest neighbors source domain note number kernels different convolutional layers alexnet might different. ensure equal weighting among different convolutional layers during nearest neighbor search histogram kernel responses normalized total number kernels corresponding layer. thus distance descriptor source image computed follows. source domain training images selective joint ﬁne-tuning images source domain training images. instead image target domain search certain number images similar low-level characteristics source domain. images returned searches used training images source learning task selective joint ﬁnetuning. apply ﬁlter bank images source domain target domain. histograms ﬁlter bank responses used image descriptors search. associate adaptive number source domain images target domain image. hard training samples target domain might associated larger number source domain images. ﬁlter banks used experiments. gabor ﬁlter bank consists kernels convolutional layers alexnet pre-trained imagenet architecture almost existing deep convolutional neural network alexnet vggnet residualnet used selective joint ﬁnetuning. -layer residual network identity mappings architecture experiments. entire residual network shared source target learning tasks. extra output layer added residual network learning tasks. output layer shared learning tasks share label space. residual network pre-trained either imagenet places. source-target joint fine-tuning task uses cost function selective joint ﬁne-tuning every training image contributes cost function corresponding domain comes from. source domain images selected aforementioned searches used training images source learning task entire target domain used training target learning task only. since residual network shared learning tasks ﬁne-tuned training sets. output layers residual network ﬁne-tuned corresponding training only. thus conduct end-to-end joint ﬁne-tuning minimize original loss functions source learning task target learning task simultaneously. unique step pipeline. image target domain search certain number images similar low-level characteristics source domain. images returned searches used training images source learning task selective joint ﬁne-tuning. elaborate image search step below. number convolutional kerh nels corresponding layer klth histogram images divergence. hard samples target domain labels training samples target domain varying degrees difﬁculty satisfy. intuitively would like seek extra help hard training samples target domain searching nearest neighbors source domain. propose iterative scheme purpose. calculate information entropy measure classiﬁcation uncertainty training samples target domain m-th iteration follows. training samples high classiﬁcation uncernext tainty considered hard training samples. iteration increase number nearest neighbors hard training samples continue ﬁnetuning model trained current iteration. training sample target domain number nearest neighbors next iteration deﬁned follows. predicted label number nearest neighbors iteration. changing number nearest neighbors samples target domain subset source domain used training data evolves iterations turn gradually changes feature representation learned deep network. equation typically initial number nearest neighbors samples target domain. experiments stop iterations. table compare effectiveness gabor ﬁlters various combinations kernels alexnet selective joint ﬁne-tuning. experiment -layer residual network half convolutional kernels original architecture. experiments -layer residual network identity mappings deep convolutional architecture conventional ﬁne-tuning performed pretrained network architecture without using source datasets baseline. note network architecture different used published methods datasets experiments many existing methods adopt sophisticated parts models feature encodings. performance methods still included paper indicate simple holistic method without incorporating parts models feature encodings capable achieving state-of-the-art performance. pre-trained model released initialize residual network. selective joint ﬁne-turning source target samples mixed together minibatch. data passed average pooling layer residual network split source target samples send corresponding softmax classiﬁer layer respectively. source target classiﬁers initialized randomly. experiments titan memory. training data augmented ﬁrst follow training testing settings every mini-batch include images using modiﬁed implementation residual network. include randomly chosen samples target domain mini-batch. chosen target sample include retrieved nearest neighbors source domain mini-batch. iter size iteration caffe momentum parameter weight decay sgd. selective joint ﬁne-tuning learning rate starts divided every iterations experiments. experiments ﬁnish iterations. source image retrieval imagenet ilsvrc training source domain stanford dogs oxford flowers caltech combination imagenet places training sets source domain indoor fig. shows retrieved nearest neighbors imagenet places observed corresponding source target images share similar colors figure images source domain similar low-level characteristics target images. ﬁrst column shows target images stanford dogs oxford flowers caltech indoor following columns rows corresponding nearest images imagenet following columns images retrieved places local patterns global structures. since low-level ﬁlter bank responses encode strong semantic information nearest neighbors target domain include images various sometimes completely unrelated categories. experimentally least retrieved images source domain. source images give rise overﬁtting quickly. therefore initial number retrieved nearest neighbors target training sample meet requirement. hand surprising result setting large would make performance target learning task drop signiﬁcantly. experiments different values stanford dogs oxford flowers caltech indoor since exists much overlap among nearest neighbors different target samples retrieved images typically cover entire imagenet places datasets. shown table mean class accuracy achieved ﬁne-tuning residual network using training samples dataset without source domain shows -layer residual network pre-trained imagenet dataset strong generalization capability ﬁne-grained classiﬁcation task. using entire imagenet dataset regular joint ﬁnetuning improve performance ﬁnally perform proposed selective joint ﬁne-tuning using subset source domain images retrieved using histograms low-level convolutional features performance improved higher performance conventional ﬁne-tuning without source domain higher result reported expands original target training using google image search. comparison demonstrates selective joint ﬁne-tuning signiﬁcantly outperform conventional ﬁne-tuning. oxford flowers oxford flowers consists ﬂower categories. images used training validation images used testing. training scratch using target domain selective joint training scratch fine-tuning source domain joint ﬁne-tuning source samples selective joint random source samples selective joint iterative retrieval selective joint gabor ﬁlter bank selective joint ﬁne-tuning selective joint model fusion shown table mean class accuracy achieved conventional ﬁne-tuning using training samples dataset without source domain selective joint ﬁne-tuning improves performance higher previous best result single network compare previous state-of-the-art results obtained using ensemble different networks also average performance multiple models obtained iterative source image retrieval hard training samples target domain. experiments show performance ensemble model higher previous best ensemble performance reported note training scratch using target domain selective joint training scratch fine-tuning source domain joint ﬁne-tuning source samples selective joint random source samples selective joint iterative retrieval selective joint gabor ﬁlter bank selective joint ﬁne-tuning selective joint model fusion simon used validation dataset additional training data. verify effectiveness joint ﬁne-tuning strategy also conducted experiments using training setting result single network outperforms general object recognition caltech caltech object categories background cluster class. every category least images used training validation testing. researchers typically report results number training samples class falling follow testing procedure compare stateof-the-art results. conduct four experiments number training samples class respectively. according table comparison conventional ﬁnetuning without using source domain selective joint ﬁnetuning improves classiﬁcation accuracy four experiments degree improvement varies performance improvement selective joint ﬁne-tuning obvious smaller number target training image class used. limited diversity target training data imposes greater need seek help source domain. experiments classiﬁcation performance selective joint ﬁne-tuning also signiﬁcantly better previous stateof-the-art results. scene classiﬁcation indoor indoor scene categories. category images training images testing. since indoor scene dataset addition imagenet ilsvrc training places- training also potential source domain. compare three settings slective joint ﬁne-tuning imagenet source domain places source domain combination imagenet places source domain. shown table mean class accuracy selective joint ﬁne-tuning imagenet source domain higher conventional ﬁnetuning without using source domain. since imagenet object-centric dataset indoor scene dataset hard training images target domain retrieve source domain images similar low-level characteristics. source images retrieved imagenet still prevent network overﬁtting heavily help achieve performance gain. places dataset serves source domain mean class accuracy reaches higher performance ﬁnetuning without source domain higher previous best result single network hybrid source domain based imagenet places improve performance. averaging output networks jointly ﬁne-tuned places hybrid source domain obtain classiﬁcation accuracy higher previous best result ensemble model perform ablation study stanford dogs oxford flowers replacing removing single component pipeline. first instead ﬁne-tuning perform training scratch settings using target domain using selective joint training. tables show selective joint training obviously improves performance still inferior ﬁne-tuning pretrained networks. subsample relatively small percentage source data still insufﬁcient train deep networks scratch. second instead using subset retrieved training images source domain simply training images source domain. joint ﬁne-tuning entire source domain decrease performance respectively. demonstrates using training data source domain always better. contrary using less relevant data source domain actually helpful. third instead using subset retrieved training images number randomly chosen training images source domain. again performance drops respectively. fourth validate effectiveness iteratively increasing number retrieved images hard training samples target domain turn feature number retrieved images training samples target domain. performance drops respectively. indicates adaptive scheme hard samples useful improving performance. fifth convolutional kernels bottom layers pre-trained alexnet ﬁlter bank. replace ﬁlter bank gabor ﬁlter bank overall performance drops respectively indicates ﬁlter bank learned diverse dataset could powerful analytically deﬁned one. finally perform conventional ﬁne-tuning without using source domain performance drop becomes quite signiﬁcant reaches respectively. conclusions paper address deep learning tasks insufﬁcient training data introducing deep transfer learning scheme called selective joint ﬁne-tuning performs target learning task insufﬁcient training data simultaneously another source learning task abundant training data. different previous work directly adds extra training data target learning task scheme borrows samples large-scale labeled dataset source learning task require additional labeling effort beyond existing datasets. experiments show deep transfer learning scheme achieves state-of-the-art performance multiple visual classiﬁcation tasks insufﬁcient training data deep networks. nevertheless suitable source domain speciﬁc target learning task remains open problem future investigation.", "year": 2017}