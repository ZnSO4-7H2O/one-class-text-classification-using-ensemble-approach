{"title": "Neural Discourse Modeling of Conversations", "tag": ["cs.CL", "cs.NE"], "abstract": "Deep neural networks have shown recent promise in many language-related tasks such as the modeling of conversations. We extend RNN-based sequence to sequence models to capture the long range discourse across many turns of conversation. We perform a sensitivity analysis on how much additional context affects performance, and provide quantitative and qualitative evidence that these models are able to capture discourse relationships across multiple utterances. Our results quantifies how adding an additional RNN layer for modeling discourse improves the quality of output utterances and providing more of the previous conversation as input also improves performance. By searching the generated outputs for specific discourse markers we show how neural discourse models can exhibit increased coherence and cohesion in conversations.", "text": "deep neural networks shown recent promise many language-related tasks modeling conversations. extend rnn-based sequence sequence models capture long range discourse across many turns conversation. perform sensitivity analysis much additional context affects performance provide quantitative qualitative evidence models able capture discourse relationships across multiple utterances. results quantiﬁes adding additional layer modeling discourse improves quality output utterances providing previous conversation input also improves performance. searching generated outputs speciﬁc discourse markers show neural discourse models exhibit increased coherence cohesion conversations. deep neural networks successful modeling many aspects natural language including word meanings machine translation syntactic parsing language modeling image captioning given suﬃcient training data dnns highly accurate trained end-to-end withneed intermediate knowledge representations explicit feature extraction. recent interest conversational user interfaces virtual assistants chatbots application dnns facilitate meaningful conversations area progress needed. sequence sequence models based recurrent neural networks shown initial promise creating intelligible conversations noted work needed models fully capture larger aspects human communication including conversational goals personas consistency context word knowledge. since discourse analysis considers language conversation-level including social psychological context useful framework guiding extension end-to-end neural conversational models. drawing concepts discourse analysis coherence cohesion codify makes conversations intelligent order design powerful neural models reach beyond sentence utterance level. example looking features indicate deixis anaphora logical consequence machine-generated utterances benchmark level coherence cohesion rest conversation make improvements models accordingly. long neural models encode long-range structure conversations able express conversational discourse similar human brain does without need explicitly building formal representations discourse theory model. explore rnn-based sequence sequence architectures capture long-range relationships multiple utterances conversations look ability exhibit discourse relationships. speciﬁcally look baseline encoder-decoder attention mechanism model additional discourse encodes sequence multiple utterances. building work done machine translation sequence sequence models based encoderdecoders initially applied generate conversational outputs given single previous message utterance input. several models presented included context vector combined message utterance various encoding strategies initialize bias single decoder rnn. models also included additional tier capture context conversations. example includes hierarchical context layer summarize state dialog includes intension network model conversation intension dialogs involving participants speaking turn. modeling persona participants conversation embedding speaker k-dimensional embedding shown increase consistency conversations formal representations rhetorical structure theory developed identify discourse structures written text. discourse parsing phrases coherence modeling based coreference resolution named-entities applied tasks summarization text generation. lexical chains narrative event chains provide directed graph models text coherence looking thesaurus relationships subject-verbtemporal relationships respectively. recurrent convolutional neural networks used classify utterances discourse speech-act labels hierarchical lstm models evaluated generating coherent paragraphs text documents models since conversations sequences utterances utterances sequences words natural models based encoder-decoder predict next utterance conversation given previous utterances source input. compare types models seqseq+a applies attention mechanism directly encoder hidden states nseqseq+a adds additional tier attention mechanism model discourse relationships input utterances. vector matrices learned parameters. decoder state time concatenated make predictions inform next time step. seqseq+a hidden states encoder nseqseq+a hidden states discourse therefore seqseq+a attention mechanism applied word-level nseqseq+a attention applied utterance-level. seqseq+a baseline starting point attention mechanism help model discourse straightforward adaptation encoderdecoder conversational model discussed join multiple source utterances using symbol delimiter feed encoder single input sequence. reversed order tokens individual utterances preserved order conversation turns. attention mechanism able make connections words used earlier utterances decoder generates word output response. nseqseq+a since conversational threads ordered sequences utterances makes sense extend encoder-decoder adding another tier model discourse turns conversation progress. given input utterances encoder applied utterance time shown fig. output encoder input utterances forms time step inputs discourse rnn. attention mechanism applied hidden states discourse decoder rnn. also considered model output encoder also combined output discourse attention decoder found purely hierarchical architecture performed better. learning model chose identical optimizers hyperparameters etc. experiments order isolate impact speciﬁc diﬀerences network architecture also taking computation times figure schematic seqseq+a nseqseq+a models multiple turns conversation. attention mechanism applied either directly encoder intermediate discourse rnn. available resources account. would straightforward perform grid search tune hyperparameters lstm cells increase layers etc. improve performance individually model beyond report here. layer gated recurrent units hidden cells. separate embeddings encoder decoder dimension vocabulary size trained on-the-ﬂy without using predeﬁned word vectors. stochastic gradient descent optimizer norms clipped initial learning rate learning rate decay factor applied needed. trained mini-batches randomly selected examples training approximately epochs validation loss converged. ﬁrst present results comparing neural discourse models trained large conversation threads based opensubtitles dataset examine models able produce outputs indicate enhanced coherence searching discourse markers. opensubtitles dataset large-scale dataset important want model variations nuances human language. opensubtitles corpus created training validation conversation fragments respectively. conversation fragment consists utterances previous lines movie dialog leading target utterance. main limitation opensubtitles dataset derived closed caption style subtitles noisy include labels actors speaking turn show conversation boundaries diﬀerent scenes. considered cleaner datasets ubuntu dialog corpus movie-dic dialog corpus subtle corpus found contained orders magnitude fewer conversations and/or many fewer turns conversation average. therefore found size opensubtitles dataset outweighed beneﬁts cleaner smaller datasets. echoes trend neural networks large noisy datasets tend perform better small clean datasets. lack large-scale clean dataset conversations open problem ﬁeld. table report average perplexity validation convergence model. found nseqseq+a shows modest signiﬁcant performance improvement baseline seqseq+a. nseqseq+a larger values assuming would continue outperform. enough discourse dataset tagged crisp discourse relationships currently available seek quantitatively compare relative levels coherence cohesion. alternative human-rated evaluation performed simple text analysis search speciﬁc discourse markers indicate enhanced coherence decoder output follows table show number previous conversation turns used input aﬀects likelihood discourse markers appear decoder output. percentage output utterances containing discourse markers related deixis anaphora logical consequence reported sample validation examples. general context leads higher likelihood discourse markers indicating long-range discourse relationships indeed being modeled. results show potentially interesting sensitivity value require study likely dependent diﬀerent conversational styles domains. examples table show examples comparing decoder outputs nseqseq+a model using either previous conversation turns input. qualitatively neural discourse model capable producing increased cohesion provided context. studied neural discourse models capture long distance relationships features found diﬀerent utterances conversation. found model additional discourse outperforms baseline encoder-decoder attention mechanism. results indicate providing context previous utterances improves model performance point. qualitative examples illustrate discourse produces increased coherence cohesion rest conversation quantitative results based text mining discourse markers show amount deixis anaphora logical consequence found decoder output sensitive size context window. future work interesting train discourse models even larger corpora compare conversations diﬀerent domains. examining attention weights possible study discourse markers models paying attention possibly provide powerful tool analyzing discourse relationships. applying multi-task sequence sequence learning techniques able combine conversational modeling task tasks discourse parsing and/or world knowledge modeling achieve better overall model performance. conversations neural discourse modeling could also applied written text documents domains strong patterns discourse news legal healthcare.", "year": 2016}