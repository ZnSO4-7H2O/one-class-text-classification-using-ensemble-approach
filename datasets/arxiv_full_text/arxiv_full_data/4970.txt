{"title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement  Learning", "tag": ["cs.AI", "cs.LG"], "abstract": "As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.", "text": "figure example world instructions. agent tasked execute longer sequences instructions correct order training short sequences instructions; addition previously unseen instructions given evaluation additional reward available randomly appearing boxes regardless instructions tery arrivals reward sources) executing instructions. thus agent blindly execute instructions sequentially sometimes deviate instructions depending circumstances requires balancing between different objectives. problem. develop capabilities paper introduces instruction execution problem agent’s overall task execute given list instructions described simple form natural language dealing unexpected events illustrated figure speciﬁcally assume instruction executed performing high-level subtasks sequence. even though agent pre-learn skills perform subtasks figure instructions easily translated subtasks problem difﬁcult following challenges. generalization pre-training skills done subset subtasks agent required perform previously unseen subtasks order execute unseen instructions testing. thus agent learn generalize subtasks skill learning stage. furthermore agent required execute previously unseen longer sequences instructions evaluation. delayed reward agent told instruction execute point time environment given full list instructions input. addition agent receive signal completing instep towards developing zero-shot task generalization capabilities reinforcement learning introduce problem agent learn execute sequences instructions learning useful skills solve subtasks. problem consider types generalizations previously unseen instructions longer sequences instructions. generalization unseen instructions propose objective encourages learning correspondences similar subtasks making analogies. generalization sequential instructions present hierarchical architecture meta controller learns acquired skills executing instructions. deal delayed reward propose neural architecture meta controller learns update subtask makes learning efﬁcient. experimental results stochastic domain show proposed ideas crucial generalization longer instructions well unseen instructions. introduction ability understand follow instructions allows perform large number complex sequential tasks even without additional learning. example make dish following recipe explore city following guidebook. developing ability execute instructions potentially allow reinforcement learning agents generalize quickly tasks instructions available. example factory-trained household robots could execute novel tasks house following human user’s instructions addition generalization instructions intelligent agent also able handle unexpected events emergencies better agent interrupt ongoing subtask ﬁnished perform different subtask deal events resume executing interrupted subtask instructions that. thus agent achieve balance executing instructions dealing events. memory loop instructions require agent perform subtask multiple times take account history observations subtasks order decide move next instruction correctly. challenges agent able execute novel subtask keep track done monitor observations interrupt ongoing subtasks depending circumstances switch next instruction precisely current instruction ﬁnished. approach technical contributions. address aforementioned challenges divide learning problem stages learning skills perform subtasks generalizing unseen subtasks learning execute instructions using learned skills. speciﬁcally assume subtasks deﬁned several disentangled parameters. thus ﬁrst stage architecture learns parameterized skill perform different subtasks depending input parameters. order generalize unseen parameters propose objective function encourages making analogies similar subtasks underlying manifold entire subtask space learned without experiencing subtasks. second stage architecture learns meta controller parameterized skill read instructions decide subtask perform. overall hierarchical architecture shown figure deal delayed reward well interruption propose novel neural network learns update subtask meta controller. allows learning efﬁcient delayed reward operating larger time-scale also allows interruptions ongoing subtasks unexpected event observed. main results. developed visual environment using minecraft based agent interact many objects. results multiple sets parameterized subtasks show proposed analogy-making objective generalize successfully. results multiple instruction execution problems show meta controller’s ability learn update subtask plays role solving overproblem outperforms several hierarchical baselines. demo videos available following website https//sites.google.com/a/umich. edu/junhyuk-oh/task-generalization. rest sections organized follows. section presents related work. section presents analogymaking objective generalization parameterized tasks demonstrates application different generalization scenarios. section presents hierarchical architecture instruction execution problem neural network learns operate large time-scale. addition demonstrate agent’s ability generalize sequences instructions well provide comparison several alternative approaches. related work hierarchical number hierarchical approaches designed deal sequential tasks. typically form meta controller lower-level controllers subtasks however much previous work assumes overall task ﬁxed words optimal sequence subtasks ﬁxed evaluation makes hard evaluate agent’s ability compose pre-learned policies solve previously unseen sequential tasks zero-shot fashion unless re-train agent tasks transfer learning setting work also closely related programmable hams pham designed execute given program. however program explicitly speciﬁes policy phams effectively reduces state-action search space. contrast instructions description task work means agent learn instructions maximize reward. hierarchical deep hierarhical recently combined deep learning. kulkarni proposed hierarchical deep q-learning demonstrated improved exploration challenging atari game. tessler proposed similar architecture highlevel controller allowed choose primitive actions directly. bacon proposed option-critic architecture learns options without pseudo reward demonstrated learn distinct options atari games. heess formulated actions meta controller continuous variables used modulate behavior low-level controller. florensa trained stochastic neural network mutual information regularization discover skills. approaches build open-loop policy highlevel controller waits previous subtask ﬁnished chosen. approach able interrupt ongoing subtasks principle architecture switch subtask time. zero-shot task generalization. papers zero-shot generalization tasks. example silva introduced parameterized skills sets task descriptions policies. isele achieved zero-shot task generalization dictionary learning sparsity constraints. schaul proposed universal value function approximators learn value functions state goal pairs. devin proposed composing sub-networks shared across tasks robots order achieve generalization unseen conﬁgurations them. unlike prior work propose ﬂexible metric learning method applied various generalization scenarios. andreas proposed framework learn underlying subtasks policy sketch speciﬁes sequence subtasks agent generalize sequences principle. contrast work aims generalize unseen subtasks well unseen sequences them. addition agent handle unexpected events problem described instructions interrupting subtasks appropriately. instruction execution. line work building agents execute natural language instructions tellex robotics macmahon chen mooney simulated environment. however approaches focus natural language understanding instructions actions groundings supervised setting. contrast focus generalization sequences instructions without supervision language understanding actions. although branavan also tackle similar problem agent given single instruction time agent needs learn align instructions state given full list instructions. learning parameterized skill paper parameterized skill multi-task policy corresponding multiple tasks deﬁned categorical input task parameters e.g. formally deﬁne parameterized skill mapping observations task parameters primitive actions indicates whether task ﬁnished not. space tasks deﬁned using cartesian product task parameters i-th parameters given observation time task parameters policy optimized task termination function probability state terminal time given task parameterized skill represented nonlinear function approximator neural network paper. neural network architecture parameterized skill illustrated figure network maps input task parameters task embedding space combined observation followed output layers. details described appendix. learning generalize analogy-making subset tasks available training order generalize unseen tasks evaluation network needs learn knowledge relationship different task parameters learning task embedding propose analogy-making objective inspired reed main idea learn correspondences tasks. example target objects ‘visit/pick actions independent enforce analogy embedding space means difference ‘visit’ ‘pick consistent regardless target objects vice versa. allows agent generalize unseen combinations actions target objects performing learned perform difference vector between tasks embedding space τdis τdif constant threshold distances. intuitively ﬁrst constraint enforces analogy reed constraints prevent trivial solutions. incorporate constraints following objectives based contrastive loss lsim ega...d∼gsim ldis ega...d∼gdis ldif egab∼gdif gsimgdisgdif sets task parameters satisfy corresponding conditions three constraints. ﬁnal analogy-making objective weighted three objectives. training parameterized skill trained tasks actor-critic method generalized advantage estimation also found pre-training policy distillation gives slightly better results discussed tessler throughout training parameterized skill also made predict whether current state terminal binary classiﬁcation objective analogy-making objective applied task embedding separately. full details learning objectives described appendix. experiments environment. developed visual environment using minecraft based shown figure observation represented pixel image. different types objects sheep greenbot horse ice. topology world objects randomly generated every episode. agent actions look move pick transform operation. pick removes object front agent transform changes object front agent implementation details. network architecture parameterized skill consists convolution layers lstm layer. conducted curriculum training changing size world density object walls according agent’s success rate. implemented actor-critic method threads based sukhbaatar parameters updated episodes thread. details architectures hyperparameters described appendix. table performance parameterized tasks. entry shows ‘average reward assume episode successful agent successfully ﬁnishes task termination predictions correct throughout whole episode. results. useful analogy-making generalization unseen parameterized tasks trained evaluated parameterized skill three different sets parameterized tasks deﬁned below. independent task space deﬁned {visit pick transform} object types. agent move target object given ‘visit’ task perform corresponding actions front target given ‘pick ‘transform’ tasks. subset tasks encountered training agent generalize unseen conﬁgurations task parameters. object-dependent task space deﬁned {interact with}. divided objects groups either picked transformed given ‘interact with’ task. subset target object types encountered training chance agent generalize without knowledge group object. applied analogy-making analogies made within group. allows agent perform object-dependent actions even unseen objects. interpolation/extrapolation task space deﬁned agent perform task given number times given training agent generalize unseen numbers note optimal policy task derived predicting termination requires generalization unseen numbers. applied analogymaking based arithmetic summarized table parameterized skill analogy-making objective successfully generalize unseen tasks generalization scenarios. suggests learning representation task parameters possible inject prior knowledge form analogy-making objective agent learn consider instruction execution problem agent given sequence simple natural language instructions illustrated figure assume already trained parameterized skill described section thus main remaining problem parameterized skill execute instructions. although requirement instructions executed sequentially makes problem easier agent still needs make complex decisions deviate instructions deal unexpected events remember done deal loop instructions discussed section address challenges hierarchical architecture consists modules meta controller parameterized skill. speciﬁcally meta controller reads instructions passes subtask parameters parameterized skill executes given subtask provides termination signal back meta controller. section describes overall architecture meta controller dealing instructions. section describes novel neural architecture learns update subtask order better deal delayed reward signal well unexpected events. meta controller architecture illustrated figure meta controller mapping list instructions. intuitively meta controller decides subtask parameters conditioned observation list instructions previously selected subtask termination signal contrast recent hierarchical deep approaches meta controller update subtask previous terminates ﬁxed number steps meta controller update subtask time takes termination signal additional input. gives ﬂexibility meta controller order keep track agent’s progress instruction execution meta controller maintains internal state computing context vector determines subtask execute focusing instruction time list instructions context given sentence embedding retrieved previous time-step instructions previously selected subtask subtask termination troller computes context vector follows neural network. intuitively provide information subtask solved parameterized skill whether ﬁnished not. thus summary current observation ongoing subtask. takes history account lstm used subtask updater. subtask updater subtask updater constructs memory structure list instructions retrieves instruction maintaining pointer memory computes subtask parameters. instruction memory. given instructions list sentences sentence conupdater constructs memory blocks re×k subtask updater maintains instruction pointer non-negative sums indicating instruction meta controller executing. memory construction retrieval written figure unrolled illustration meta controller learned time-scale. internal states subtask updated meta controller continues previous subtask without updating internal states. embedding i-th sentence retrieved sentence embedding used computing subtask parameters. intuitively one-hot vector indicates single instruction whole list instructions. meta controller learn manage focus correct instruction time-step. address dilemma propose learn time-scale meta controller introducing internal binary decision indicates whether invoke subtask updater update subtask illustrated figure sigmoid function. meta controller continues current subtask without updating subtask updater. otherwise subtask updater updates internal states subtask parameters. allows subtask updater operate large time-scale decision made subtask updater results multiple actions depending values. overall meta controller architecture update scheme illustrated figure sampling idea take weighted ‘update’ ‘copy’ scenarios using weight. method described algorithm found training meta controller using soft-update followed ﬁne-tuning sampling crucial training meta controller. note soft-update rule reduces original formulation sample bernoulli multinomial distributions justiﬁes initialization trick. since instructions executed sequentially location-based memory addressing mechanism manage instruction pointer. speciﬁcally subtask updater shifts instruction pointer follows convolution operator ϕshif neural network soft-attention vector three shift operations optimal policy keep instruction pointer unchanged executing instruction increase pointer precisely current instruction ﬁnished. learning operate large time-scale although meta controller learn optimal policy updating subtask time-step principle making decision every time-step inefﬁcient subtasks change frequently. instead temporallyextended actions useful dealing delayed reward operating larger time-scale reasonable subtask termination signal deﬁne temporal scale meta controller many recent hierarchical deep approaches approach would result mostly open-loop integrating hierarchical rnn. idea learning time-scale recurrent neural network closely related hierarchical approaches different groups recurrent hidden units operate different time-scales capture long-term short-term temporal information. idea naturally integrated hierarchical rnns applying update decision subset recurrent units instead units. speciﬁcally divide context vector groups low-level units high-level units updated depending value simple modiﬁcation leads form hierarchical low-level units focus shortterm temporal information high-level units capture long-term dependencies. training meta controller trained training lists instructions. given pre-trained ﬁxed parameterized skill actor-critic method used update parameters meta controller. since meta controller also learns subtask embedding deal unseen subtasks evaluation analogy-making objective also applied. details objective function provided appendix. experiments experiments designed explore following questions proposed hierarchical architecture outperform non-hierarchical baseline? beneﬁcial meta controller’s ability learn update subtask? also interested understanding qualitative properties agent’s behavior. environment. used minecraft domain used section agent receives time penalty step receives reward ﬁnishes entire list instructions correct order. throughout episode randomly appears probability transforming gives reward. subtask space deﬁned semantics subtask ‘independent’ case section used best-performing parameterized skill throughout experiment. types instructions {visit pick transform pick transform pick transform target object type. note parameterized skill used experiment trained loop instructions last four instructions require meta controller learn repeat corresponding subtask given number times. agent generalizes previously unseen instructions subset instructions subtasks presented training. implementation details. meta controller consists convolution layers lstm layer. also conducted curriculum training changing size world density object walls number instructions according agent’s success rate. used table performance instruction execution. entry shows average reward success rate. ‘hierarchical-dynamic’ approach learns update subtask. episode successful agent solves instructions correctly. actor-critic implementation described section baselines. understand advantage using hierarchical structure beneﬁt meta controller’s ability learn update subtask trained three baselines follows. flat identical meta controller except directly chooses primitive actions without using parameterized skill. also pre-trained training subtasks. hierarchical-long identical architecture except meta controller update subtask current subtask ﬁnished. approach similar recent hierarchical deep methods overall performance. results instruction execution summarized table figure shows architecture handle relatively long list seen unseen instructions length reasonably high success rates even though trained short instructions length although performance degrades number instructions increases architecture ﬁnishes seen instructions unseen instructions average. results show agent able generalize longer compositions seen/unseen instructions learning solve short sequences subset instructions. flat hierarchy. table shows baseline completely fails even training instructions. controller tends struggle loop instructions learned sub-optimal policy moves next instruction small probability step regardless progress. implies hard controller detect precisely subtask ﬁnished whereas hierarchical architectures easily detect subtask done parameterized skill provides termination signal meta controller. figure performance number instructions. left right plots show reward success rate number steps average number instructions completed respectively. figure analysis learned policy. ‘update’ shows agent’s internal update decision. ‘shift’ shows agent’s instruction-shift decision bottom text shows instruction indicated instruction pointer text shows subtask chosen meta controller. agent picks ﬁnish instruction moves next instruction. agent observes randomly appeared executing ‘pick pig’ instruction immediately changes subtask dealing event agent resumes executing instruction agent ﬁnishes ﬁnal instruction. subtask ﬁnished baseline puts high probability switch regardless existence transforming gives bonus reward exists chance. however leads wasting much time ﬁnding appears results poor success rate time limit. result implies open-loop policy wait subtask ﬁnishes confused uncertain event cannot interrupt ongoing subtasks termination. hand observed ‘hierarchical-short’ often fails loop instructions moving next instruction ﬁnishes instructions. baseline repeat subtask changing instruction pointer long time reward even delayed given loop instructions. contrast subtask updater architecture makes fewer decisions operating large time-scale direct feedback long-term future. conjecture architecture performs better baseline. result shows learning update subtask using neural network beneﬁcial dealing delayed reward without compromising ability interrupt. analysis learned policy. visualized agent’s behavior given long list instructions figure interestingly agent sees meta controller immediately changes subtask positive reward even though instruction pointer indicating ‘pick pig’ resumes executing instruction dealing box. throughevent loop instruction meta controller keeps instruction pointer unchanged illustrated figure addition agent learned update instruction pointer subtask almost needed provides subtask updater temporally-extended actions. computationally efﬁcient also useful learning better policy. conclusion paper explored type zero-shot task generalization problem agent required execute generalize sequences instructions. proposed analogy-making objective enables generalization unseen parameterized tasks various scenarios. also proposed novel learn time-scale meta controller proved efﬁcient ﬂexible alternative approaches interrupting subtasks dealing delayed sequential decision problems. empirical results stochastic domain showed architecture generalizes well longer sequences instructions well unseen instructions. although hierarchical architecture demonstrated simple setting instructions executed sequentially believe ideas limited setting extended richer forms instructions. kulkarni narasimhan saeedi tenenbaum. hierarchical deep reinforcement learning integrating temporal abstraction intrinsic motivation. arxiv preprint arxiv. episode terminates steps ‘independent’ ‘object-dependent’ cases steps ‘inter/extrapolation’ case. agent receives positive reward successfully ﬁnishes given task receives time penalty step. details generalization scenario described below. independent. semantics tasks consistent across types target objects. training tasks shown table examples analogies used experiment object type object-dependent. divided objects groups group group given ‘interact with’ action group picked whereas group transformed agent. training tasks groups object shown table examples analogies used experiment note second example implies sheep greenbot treated different belong different groups. given analogies agent learn interact interacts sheep interact greenbot interacts horse. inter/extrapolation. experiment task deﬁned three parameters action object number. agent repeat subtask given number times. agent trained conﬁgurations actions target objects. however subset numbers used training. order interpolate extrapolate deﬁne analogies based simple arithmetic episode terminates steps training meta controller. appears probability disappears steps. evaluation longer instructions episode terminates steps. constructed training instructions unseen instructions described table generated different sequences instructions training evaluation sampling instructions sets instructions. experiment grid-world environment. approach generally applied different domains developed grid-world based mazebase agent interact many objects illustrated figure unlike original mazebase observation represented binary tensor number object types size grid world. channel binary mask indicating presence object type. agent blocks water types objects agent interact randomly placed episode. figure example grid-world object speciﬁcation. arrows represent outcome object transformation. objects without arrows disappear transformed. agent allowed blocks gets penalty going water. agent primitive actions no-operation move pick transform move actions move agent cell speciﬁed direction. pick actions remove adjacent object corresponding relative position transform actions either remove transform another object depending object type shown figure agent receives time penalty time-step. water cells obstacles give agent visits them. agent receives reward ﬁnishes instructions correct order. throughout episode enemy randomly appears moves disappears steps. transforming enemy gives reward. evaluation parameterized tasks. main experiment evaluated parameterized skill seen unseen parameterized tasks separately using generalization scenarios. shown table parameterized skill analogy-making successfully generalize unseen tasks independent object-dependent scenarios. table performance parameterized tasks. entry shows ‘average reward assume episode successful agent successfully ﬁnishes task termination predictions correct throughout whole episode. visualized value function learned critic network parameterized skill figure expected generalization performance parameterized skill trained analogy-making objective learned high values around target objects given unseen tasks. figure value function visualization given unseen tasks. visualizes learned values position agent grid world agent estimates high values around target object world. evaluation instruction execution. types instructions visit pick transform pick transform target object type. ﬁrst three instructions require agent perform corresponding subtask last instructions require agent repeat subtask target objects completely disappear world. overall result consistent result environment shown figure table baseline learned sub-optimal policy transforms picks target objects world even ‘all’ adverb instructions. sub-optimal policy unnecessarily removes objects potentially target objects future instructions. performance baseline drastically decreases number instructions increases figure architecture learned time-scale performs much better ‘hierarchicalshort’ baseline updates subtask every time-step. also suggests learning operate large-time scale crucial dealing delayed reward. also observed agent learned deal enemies whenever appear thus outperforms ‘shortest path’ method near-optimal executing instructions ignoring enemies. table performance meta controller. entry table represents reward success rate parentheses averaged -best runs among independent runs. ‘shortest path’ hand-designed policy executes instructions optimally based shortest path ignores enemies. ‘near-optimal’ near-optimal policy executes instructions based shortest path transforms enemies close agent. parameterized skill ﬁrst trained policy distillation ﬁne-tuned using actor-critic method generalized advantage estimation parameterized skill also trained predict whether current state terminal binary classiﬁcation objective. training tasks. lterm cross-entropy loss termination prediction. intuitively sample mini-batch tasks parameterized skill generate episodes train predict teachers’ actions. method shown efﬁcient multi-task learning. architectures hyperparameters background multiplicative interaction combining condition variables neural network used form multiplicative interaction instead concatenating variables suggested also related parameter prediction approaches parameters neural network produced condition variables approach shown effective achieving zero-shot one-shot generalization image classiﬁcation problems formally given input output convolution fully-connected layer parameters predicted condition variable written embedding condition variable learned multi-layer perceptron note matrix factorization reduce number parameters fully-connected layer. intuitively condition variable converted weight convolution fully-connected layer multiplicative interactions. used approach parameterized skill meta controller. details described below. parameterized skill. teacher architecture used policy distillation conv-convlstm. network fully-connected output layers actions value respectively. parameterized skill architecture consists conv-conv-conv-conv-lstm. relug compute subtask embedding. linearly transformed weights conv weight lstm multiplicative interaction described above. finally network three fully-connected output layers actions termination probability baseline respectively. used rmsprop optimizer smoothing parameter epsilon training teacher policy actor-critic used learning rate training parameterized skill used learning rate policy distillation actor-critic ﬁne-tuning respectively. used τdis τdif analogy-making regularization termination prediction objective respectively. used discount factor balancing weight gae. threads batch size used episodes parallel parameter updated better exploration applied entropy regularization weight linearly decreased zero ﬁrst iterations. total number iterations policy distillation actor-critic ﬁne-tuning. meta controller. meta controller consists conv-conv-conv-poollstm. embedding previously selected subtask previously retrieved instruction subtask termination concatenated given input one-layer compute -dimensional joint embedding. linearly transformed weights conv lstm multiplicative interaction. output used context vector used bag-of-words representation sentence emj= wmwj word embedding matrix -dimensional. hidden layer units ϕshif fullyconnected layer used ϕupdate. ϕgoal hidden layer units takes concatenation input computes probabilities subtask parameters outputs. baseline network linear regression concatenation memory pointer binary mask indicating presence given instruction ﬁnal hidden layer used hyperparameters used parameterized skill except batch size trained soft-architecture learning rate using curriculum learning iterations weight entropy regularization ﬁne-tuned learning rate without curriculum learning iterations. finally initialized hard-architecture flat controller. controller architecture consists layers used meta controller following differences. previously retrieved instruction transformed hidden layers compute weight conv lstm. output probabilities primitive actions. curriculum learning. training architectures randomly sampled size world density walls sampled density objects sampled training parameterized skill training meta controller. sampled number instructions training meta controller. sampling range determined based success rate agent.", "year": 2017}