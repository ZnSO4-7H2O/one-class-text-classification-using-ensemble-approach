{"title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to  Action Sequences", "tag": ["cs.CL", "cs.AI", "cs.LG", "cs.NE", "cs.RO"], "abstract": "We propose a neural sequence-to-sequence model for direction following, a task that is essential to realizing effective autonomous agents. Our alignment-based encoder-decoder model with long short-term memory recurrent neural networks (LSTM-RNN) translates natural language instructions to action sequences based upon a representation of the observable world state. We introduce a multi-level aligner that empowers our model to focus on sentence \"regions\" salient to the current world state by using multiple abstractions of the input sentence. In contrast to existing methods, our model uses no specialized linguistic resources (e.g., parsers) or task-specific annotations (e.g., seed lexicons). It is therefore generalizable, yet still achieves the best results reported to-date on a benchmark single-sentence dataset and competitive results for the limited-training multi-sentence setting. We analyze our model through a series of ablations that elucidate the contributions of the primary components of our model.", "text": "place back wall intersection. forward segment intersection blue-tiled hall. interesction contains chair. turn left. forward hall. turn left. forward segment intersection wooden-ﬂoored hall. intersection conatains easel. turn right. forward segments hall. turn left. forward segment intersection containing lamp. turn right. forward segment empty corner. figure example route instruction-path pair virtual worlds macmahon stankiewicz kuipers colors indicate ﬂoor patterns wall paintings letters indicate different objects. method successfully infers correct path instruction. propose recurrent neural network long shortterm memory encode navigational instruction sequence bidirectionally decode representation action sequence based representation current world state. lstms well-suited task shown effective learning temporal depropose neural sequence-to-sequence model direction following task essential realizing effective autonomous agents. alignment-based encoder-decoder model long short-term memory recurrent neural networks translates natural language instructions action sequences based upon representation observable world state. introduce multi-level aligner empowers model focus sentence regions salient current world state using multiple abstractions input sentence. contrast existing methods model uses specialized linguistic resources taskspeciﬁc annotations therefore generalizable still achieves best results reported to-date benchmark single-sentence dataset competitive results limited-training multi-sentence setting. analyze model series ablations elucidate contributions primary components model. robots must able understand successfully execute natural language navigational instructions work seamlessly alongside people. example someone using voice-commandable wheelchair might direct take room across kitchen soldier command micro aerial vehicle hallway second room right. however interpreting free-form instructions challenging ambiguity complexity uncertainty interpretation long-term dependencies among instructions actions differences amount detail given diverse ways language composed. figure presents example instruction method successfully follows. previous work domain largely requires specialized resources like semantic parsers seed lexicons re-rankers interpret ambiguous free-form natural language instructions. contrast goal work learn instructions actions end-to-end fashion assumes prior linguistic knowledge. instead model learns meaning words spatial relations pendencies exist sequences especially tasks image captioning machine translation natural language generation additionally learn correspondences words input navigational instruction actions output sequence using alignment-based lstm standard alignment methods consider high-level abstractions input sacriﬁces information important identifying correspondences. instead introduce multi-level aligner empowers model highlow-level input representations turn improves accuracy inferred directions. evaluate model benchmark navigation dataset achieve best results reported to-date singlesentence task without using additional resources semantic parsers seed lexicons rerankers used previous work. multi-sentence task executing full paragraph amount training pairs even smaller model performs better several existing methods competitive state-of-the-art specialized linguistic resources extra annotation reranking. perform series ablation studies order analyze primary components model including encoder multi-level representations alignment bidirectionality. great deal attention paid late algorithms allow robots autonomous agents follow free-form navigational route instructions methods solve harnad refers symbol grounding problem associating linguistic elements corresponding manifestation external world. initial research natural language symbol grounding focused manually-prescribed mappings language sets predeﬁned environment features actions recent work statistical language understanding learns convert free-form instructions referent symbols observing language perceptual context methods represent natural language grounding terms manually deﬁned linguistic spatial semantic features learn model parameters natural language corpora often requiring expensive annotation pair phrase corresponding grounding. guage equivalent. example matuszek koscher assume prior linguistic knowledge employ general-purpose supervised semantic parser learner. alternatively chen mooney parse free-form route instructions formal action speciﬁcations robot control process execute. learn parser weakly supervised manner natural language instruction action sequence pairs together corresponding world representation. alternatively mooney frame grounded language learning probabilistic context free grammar induction learned lexicons control space production rules allows scale pcfgs navigation domain. mooney improve upon accuracy adding subsequent re-ranking step uses weakly supervised discriminative classiﬁer. meanwhile artzi zettlemoyer learn combinatory categorical grammar -based semantic parser convert freeform navigational instructions manifestation lambda-calculus representation. mooney improve upon accuracy re-ranking. artzi petrov extend parser learning using statistics corpus control size lexicon resulting improved multi-sentence accuracy. second class grounded language learning techniques function mapping free-form utterances corresponding object location action referents agent’s world model. methods learn probabilistic model expresses association word instruction matching referent world model. problem interpreting instruction becomes inference learned model. kollar build generative model assumed sequential structure language includes combination pre-speciﬁed learned models spatial relations adverbs verbs. tellex later propose discriminative model expresses hierarchical compositional structure language. factor probability distribution according parse structure free-form command employ log-linear factor graph express learned correspondence linguistic elements space groundings adopt alternative formulation treat problem interpreting route instructions sequence-to-sequence learning problem. learn mapping end-to-end fashion using neural network without using prior linguistic structure resources annotation improves generalizability. method inspired recent success sequence-to-sequence methods machine translation image video caption synthesis natural language generation similarly adopt encoderdecoder approach. model encodes input free-form route instruction decodes embedding identify corresponding output action sequence based upon lotors asked give written commands describe navigate location another without subsequent access map. instruction given several human followers tasked navigating virtual world without paths recorded. many instructions include spelling grammatical errors others incorrect approximately single sentences associated action feasible path. formulate problem interpreting natural language route instructions inference probabilistic model action sequence world state time natural language instruction problem viewed mapping given instruction sequence action sequence effective means learning sequence-to-sequence mapping neural encoder-decoder architecture. ﬁrst bidirectional recurrent neural network model encode input sentence word nonlinear functions deﬁne shortly. next context vector encodes language instruction time next another decodes context vector arrive desired likelihood observable world state moreover decoder also includes alignment focus portions sentence relevant current action technique proven effective machine translation machine vision however unlike standard alignment techniques model learns align based high-level input abstraction also low-level representation input instruction improves performance. recently andreas klein conditional random ﬁeld model learn alignment instructions actions; lstm-based aligner performs substantially better approach. consider problem mapping natural language navigational instructions action sequences based knowledge local observable environment. instructions take form isolated sentences full paragraphs interested learning mapping corpora training data form variable length natural language instruction corresponding action sequence observable environment representation. model learns produce correct action sequence given previously unseen pair. challenges arise fact instructions free-form complex contain numerous spelling grammatical errors ambiguous meaning. further model aware local environment agent’s line-of-sight. paper consider route instruction dataset generated macmahon stankiewicz kuipers data includes free-form route instructions corresponding action sequences within three different virtual worlds. environments consist interconnected hallways pattern hallway ﬂoor painting walls objects intersections. explored environment instrucmulti-level aligner context representation instruction computed weighted word vectors encoder states whereas previous work align based hidden annotations found also including original input word aligner improves performance. multi-level representation allows decoder reason high-level context-based representation input sentence also consider original low-level word representation adding model offsets information lost high-level abstraction instruction. intuitively model able better match salient words input sentence directly corresponding landmarks current world state used decoder. context vector takes form decoder architecture uses lstm decoder takes input current world state context instruction lstm’s previous hidden state st−. output conditional probability distribution next action represented deep output layer model employs lstms nonlinear functions ability learn long-term dependencies exist instruction action sequences without suffering exploding vanishing gradients. model also integrates multi-level alignment focus parts instruction salient current action multiple levels abstraction. next describe component network detail. encoder encoder takes input language sequence ﬁrst last words sentence respectively. treat word k-dimensional one-hot vector vocabulary size. feed sequence lstm-rnn summarizes temporal relationships previous words returns sequence hidden annotations annotation summarizes words including afﬁne transformation logistic sigmoid restricts input input output forget gates lstm respectively memory cell activation vector. memory cell summarizes lstm’s previous memory current input modulated forget input gates respectively. forget input gates enable lstm regulate extent forgets previous memory input output gate regulates degree memory affects hidden state. encoder employs bidirectionality encoding sentences forward backward directions approach found successful speech recognition machine translation hidden annotations inference trained model generate action sequences ﬁnding maximum posteriori actions under learned model action sequence completed stop action emitted. single-sentence task perform search using standard beam search maintain list current best hypotheses. iteratively consider k-best sequences time candidates generate sequences size keep resulting best them. multi-sentence perform search sentence-by-sentence initialize beam next sentence list previous best hypotheses. common denoising method deep learning perform inference ensemble randomly initialized models. dataset train evaluate model using publicly available sail route instruction dataset collected macmahon stankiewicz kuipers data original form dataset contains non-trivial navigational instruction paragraphs produced instructors unique start position pairs spread evenly across three virtual worlds. instructions segmented individual sentences paired action sequence. corpus statistics chen mooney world state world state encodes local observable world time make standard assumption agent able observe elements environment within line-of-sight. speciﬁc domain consider evaluation elements include ﬂoor patterns wall paintings objects occluded walls. represent world state concatenation simple bag-of-words vector direction choice bag-of-words representation avoids manual domain-speciﬁc feature-engineering combinatoriality modeling exact world conﬁgurations. evaluation metrics evaluate end-to-end model single-sentence multi-sentence versions corpus. single-sentence task following previous work strict evaluation metric deems trial successful ﬁnal position orientation exactly match original demonstration. multi-sentence disregard ﬁnal orientation previous work does. however training details follow procedure chen mooney training segmented data testing singlemulti-sentence versions. train models using three-fold cross-validation based three maps. fold retain test partition two-map training data training validation sets latter used tune hyperparameters. repeat process three folds report average test results folds. later refer training procedure vdev. additionally previous methods adopted slightly different training strategy whereby fold trains maps uses test decide stopping iteration. order compare methods also train separate version model refer vtest. optimization found adam effective training dataset. training usually converges within epochs. performed early stopping based validation task metric. similar previous work found validation log-likelihood well correlated task metric. primary result ﬁrst investigate ability navigate intended destination given natural language instruction. figure illustrates output example model successfully executes input natural language instruction. table reports overall accuracy model singlemulti-sentence settings. report statistics model order directly compare existing work. table surpass state-of-the-art results single-sentence route instruction task despite using linguistic knowledge resources. multi-sentence accuracy working really small amount training data competitive state-of-theart outperforms several previous methods additional specialized resources form semantic parsers logical-form lexicons re-rankers. note model yields good results using greedy search vdev achieve singlesentence multi-sentence vtest single-sentence multi-sentence. distance evaluation evaluation required action sequence reach exact desired destination. interest consider close model gets destination reached. table displays fraction test results reach within nodes destination. often method produces action sequences reach points close desired destination. multi-level aligner ablation unlike existing methods align based hidden annotations adopt different approach also including original input word shown table multi-level representation signiﬁcantly improves performance standard aligner figure visualizes alignment words actions environment several sentences instruction paragraph depicted figure aligner ablation model utilizes alignment decoder means focusing word regions salient current world state. analyze effect learned alignment training alternative model note ensemble still state-of-the-art single-sentence better comparable approaches multi-sentence artzi extra annotations logical-form lexicon mooney discriminative reranking techniques orthogonal approach likely improve results well. context vector unweighted average shown table learning alignment improve accuracy resulting action sequence. note aligner model still maintains connections instruction actions using non-learned uniform weights. bidirectionality ablation train alternative model uses unidirectional encoder. shown table bidirectional encoder signiﬁcantly improves accuracy. encoder ablation evaluate beneﬁt encoding input sentence consider alternative model directly feeds word vectors randomly initialized embeddings decoder relies alignment model choose salient words. table presents results without encoder demonstrates substantial gain encoding input sentence context representation. believe difference rnn’s ability incorporate sentence-level information word’s representation processes sentence sequentially advantage helps resolve ambiguities turn right versus turn right presented end-to-end sequence-to-sequence approach mapping natural language navigational instructions action plans given local observable world state using bidirectional lstm-rnn model multilevel aligner. evaluated model benchmark route instruction dataset demonstrates achieves state-of-the-art single-sentence execution yields competitive results challenging multi-sentence domain despite working small training datasets using specialized linguistic knowledge resources. performed number ablation studies elucidate contributions primary model components. thank yoav artzi david chen oriol vinyals kelvin helpful comments. work supported part robotics consortium u.s. army research laboratory collaborative technology alliance program cooperative agreement wnf-- faculty award.", "year": 2015}