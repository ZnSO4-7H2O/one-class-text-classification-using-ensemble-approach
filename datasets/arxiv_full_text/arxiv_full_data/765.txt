{"title": "Summary - TerpreT: A Probabilistic Programming Language for Program  Induction", "tag": ["cs.LG", "cs.AI", "cs.NE"], "abstract": "We study machine learning formulations of inductive program synthesis; that is, given input-output examples, synthesize source code that maps inputs to corresponding outputs. Our key contribution is TerpreT, a domain-specific language for expressing program synthesis problems. A TerpreT model is composed of a specification of a program representation and an interpreter that describes how programs map inputs to outputs. The inference task is to observe a set of input-output examples and infer the underlying program. From a TerpreT model we automatically perform inference using four different back-ends: gradient descent (thus each TerpreT model can be seen as defining a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper comparisons between different approaches to inference.  We illustrate the value of TerpreT by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models. To our knowledge, this is the first work to compare gradient-based search over program space to traditional search-based alternatives. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations.  This is a workshop summary of a longer report at arXiv:1608.04428", "text": "study machine learning formulations inductive program synthesis; given input-output examples synthesize source code maps inputs corresponding outputs. contribution terpret domain-speciﬁc language expressing program synthesis problems. terpret model composed speciﬁcation program representation interpreter describes programs inputs outputs. inference task observe inputoutput examples infer underlying program. terpret model automatically perform inference using four different back-ends gradient descent linear program relaxations graphical models discrete satisﬁability solving sketch program synthesis system. terpret main beneﬁts. first enables rapid exploration range domains program representations interpreter models. second separates model speciﬁcation inference algorithm allowing proper comparisons different approaches inference. illustrate value terpret developing several interpreter models performing extensive empirical comparison alternative inference algorithms variety program models. knowledge ﬁrst work compare gradient-based search program space traditional search-based alternatives. empirical ﬁnding constraint solvers dominate gradient descent lp-based formulations. learning computer programs input-output examples inductive program synthesis fundamental problem computer science dating back least summers biermann ﬁeld produced many successes perhaps visible example flashfill system microsoft excel also signiﬁcant recent interest neural network-based models components resemble computer programs models combine neural networks external memory external computational primitives and/or built-in structure reﬂects desired algorithmic structure execution. however none produce programs output. instead program hidden inside controllers composed neural networks decide operations perform learned program understood terms executions produces speciﬁc inputs. ∗work done author microsoft research. figure high level view program synthesis task. forward execution traditional interpreter. forward execution terpret model. inference terpret model. translation problem ﬁrst-order logical formula existential constraints. cast terpret model partial program containing holes inferred speciﬁcation work focus models represent programs simple natural source code i.e. kind source code people write. main advantages representing programs source code rather weights neural network controller. first source code interpretable; resulting models inspected human debugged modiﬁed. second programming languages designed make easy express algorithms people want write. using languages model representation inherit inductive biases lead strong generalization course natural source code likely best representation cases. example programming languages designed writing programs classify images reason expect natural source code would impart favorable inductive bias case. optimization program space known difﬁcult problem. however recent progress neural networks showing possible learn models differentiable computer architectures along success gradient descent-based optimization raises question whether gradient descent could powerful technique searching program space. issues motivate main questions work. whether models designed speciﬁcally synthesize interpretable source code contain looping branching structures searching program space using gradient descent compares combinatorial search methods traditional ips. address ﬁrst question develop models inspired intermediate representations used compilers like llvm trained gradient descent. models interact external storage handle non-trivial control explicit statements loops appropriately discretized learned model expressed interpretable source code. note concurrent works adaptive neural compilation differentiable forth implement similar models. address second question concerning efﬁcacy gradient descent need specifying many problems gradient based approach compared alternative approaches like-for-like manner across variety domains. alternatives originate rich histories programming languages inference discrete graphical models. knowledge comparison previously performed. beneﬁt formulated context terpret. terpret provides means describing execution model deﬁning program representation interpreter maps inputs outputs using parametrized program. terpret description independent particular inference algorithm. task infer execution model parameters given execution model pairs inputs outputs. overview synthesis task appears fig. perform inference terpret automatically compiled intermediate representation particular inference algorithm. table describes inference algorithms currently supported terpret. interpretable source code obtained directly inferred model parameters. driving design principle terpret strike subtle balance breadth expression needed precisely capture range execution models restriction expression needed ensure automatic compilation range different back-ends tractable. space give simple example terpret models written. full grammar language several longer examples long version gaunt terpret obeys python syntax python library parse compile terpret models. model composed param variables deﬁne program variables represent intermediate state computation inputs outputs. currently variables must discrete constant sizes. terpret supports loops ranges deﬁned constants statements arrays array indexing user-deﬁned functions discrete inputs discrete output. total constraints imply model converted gated factor graph details works longer version. simple terpret model appears fig. model binary tape program writes program rule table speciﬁes state write current position tape conditional pattern appeared previous tape locations. ﬁrst tape positions initialized inputs ﬁnal tape position program output. used terpret build much complicated models including turing machine boolean circuits basic block model similar intermediate representation used llvm assembly-like model. these constants deﬁne maximum quantities like maximum number timesteps program execute maximum number instructions program. variable-length behavior achieved deﬁning absorbing end-state allowing no-op instruction etc. table gives overview benchmark programs attempt synthesize. created terpret descriptions execution model designed three synthesis tasks model speciﬁed input-output examples. measure time taken inference techniques listed table synthesize program task. results summarized table exception fmgd algorithm perform single timeout hours task. fmgd algorithm vanilla form optimized form additional heuristics gradient clipping gradient noise entropy bonus convergence. even heuristics observe several random initializations fmgd algorithm stall uninterpretable local optimum rather ﬁnding interpretable discrete program global optimum. vanilla case report fraction different random initializations lead globally optimal solution consistent input-output speciﬁcation also wall clock time epochs gradient descent algorithm optimized fmgd case randomly draw sets hyperparameters manually chosen distribution learning different random initializations setting. report success rate best hyperparameters found also average across runs random search. results show traditional techniques employing constraint solvers outperform methods sketch system able solve benchmarks timeout. furthermore table highlights precise formulation interpreter model affect speed synthesis. basic block assembly models equally expressive assembly model biased towards producing straight line code minimal branching. cases synthesis successful assembly representation seen outperform basic block model terms synthesis time. addition performed separate experiment investigate local optima arising fmgd formulation. using terpret describe task inferring values string length observation parity neighboring variables. possible show analytically number local optima grow exponentially table provides empirical evidence minima encountered practice hinder convergence fmgd algorithm. additional processing performed sketch makes slower solver smaller conclusion presented terpret probabilistic programming language specifying problems. ﬂexibility terpret language combination four inference backends allows likefor-like comparison gradient-based constraint-based techniques inductive program synthesis. primary take-away experiments constraint solvers outperform approaches cases studied. however work measuring ability efﬁciently search program space. remain optimistic extensions terpret framework allow differentiable interpreters handle problems involving perceptual data using machine learning techniques guide search-based techniques alexander gaunt marc brockschmidt rishabh singh nate kushman pushmeet kohli jonathan taylor daniel tarlow. terpret probabilistic programming language program induction. arxiv preprint arxiv. giles guo-zheng hsing-hen chen yee-chun dong chen. higher order recurrent networks grammatical inference. advances neural information processing systems pages table benchmark results. fmgd present time seconds epochs success rate random restarts {vanilla best hypers average hypers} columns respectively. back-ends present time seconds produce synthesized program. symbol indicates timeout failure random restart converge. number provided input-output examples used specify task case. chris lattner vikram adve. llvm compilation framework lifelong program analysis transformation. code generation optimization international symposium pages ieee arvind neelakantan quoc ilya sutskever. neural programmer inducing latent programs gradient descent. proceedings international conference learning representations jason weston sumit chopra antoine bordes. memory networks. proceedings international conference learning representations http//arxiv.org/ abs/..", "year": 2016}