{"title": "Increasing the Interpretability of Recurrent Neural Networks Using  Hidden Markov Models", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "As deep neural networks continue to revolutionize various application domains, there is increasing interest in making these powerful models more understandable and interpretable, and narrowing down the causes of good and bad predictions. We focus on recurrent neural networks (RNNs), state of the art models in speech recognition and translation. Our approach to increasing interpretability is by combining an RNN with a hidden Markov model (HMM), a simpler and more transparent model. We explore various combinations of RNNs and HMMs: an HMM trained on LSTM states; a hybrid model where an HMM is trained first, then a small LSTM is given HMM state distributions and trained to fill in gaps in the HMM's performance; and a jointly trained hybrid model. We find that the LSTM and HMM learn complementary information about the features in the text.", "text": "deep neural networks continue revolutionize various application domains increasing interest making powerful models understandable interpretable narrowing causes good predictions. focus recurrent neural networks state models speech recognition translation. approach increasing interpretability combining hidden markov model simpler transparent model. explore various combinations rnns hmms trained lstm states; hybrid model trained ﬁrst small lstm given state distributions trained gaps hmm’s performance; jointly trained hybrid model. lstm learn complementary information features text. following recent progress deep learning researchers practitioners machine learning recognizing importance understanding interpreting goes inside black models. recurrent neural networks recently revolutionized speech recognition translation powerful models could useful applications involving sequential data. however adoption slow applications health care practitioners reluctant opaque expert system make crucial decisions. make inner workings rnns interpretable applications beneﬁt power. several aspects makes model algorithm understandable humans. aspect model complexity parsimony. another aspect ability trace back prediction model component particularly inﬂuential features data could useful understanding mistakes made neural networks humanlevel performance time perform poorly seemingly easy cases. instance convolutional networks misclassify adversarial examples high conﬁdence made headlines image tagging algorithm google photos mislabeled african americans gorillas. it’s reasonable expect recurrent networks fail similar ways well. would thus useful visibility sorts errors come from i.e. groups features contribute ﬂawed predictions. several promising approaches interpreting rnns developed recently. approached using gradient boosting trees predict lstm output probabilities explain features played part prediction. model internal structure lstm instead approximate entire architecture black box. karpathy showed lstm language models around memory state dimensions interpreted naked color-coding text data state values; track quotes brackets clearly identiﬁable aspects text. building results take somewhat systematic approach looking interpretable hidden state dimensions using decision trees predict individual hidden state dimensions visualize overall dynamics hidden states coloring training data k-means clusters state vectors evaluate methods well predict next observation validation set. models forward pass validation compute state distribution vector time step compute predictive likelihood next observation follows literature mostly focuses methods speciﬁcally train predict states posteriors referred hybrid tandem methods respectively. ﬁrst investigate approach require modiﬁed order make understandable interpretation happens fact. here model picture state changes lstm extracting hidden states approximating continuous emission hidden markov model take reverse approach state probabilities added output layer lstm lstm model make information gaps performing well resulting lstm smaller number hidden state dimensions could interpreted individually character-level lstm layer dropout based element-research library. train lstm epochs starting learning rate learning rate halved whenever likelihood score epoch l-norm parameter gradient vector clipped threshold figure decision tree predicting individual hidden state dimension hybrid algorithm based preceding characters linux data. hidden state dimensions -state hybrid mostly track comment characters. main hybrid model together sequentially shown figure ﬁrst discrete data outputting hidden state distributions obtained hmm’s forward pass information architecture parallel -layer lstm. linear layer lstm prediction layer augmented extra column state. lstm component architecture smaller standalone lstm since needs gaps hmm’s predictions. written python rest architecture torch. also build joint hybrid model lstm simultaneously trained torch. implemented torch module optimized using stochastic gradient descent rather ffbs. similarly sequential hybrid model concatenate lstm outputs state probabilities. test models several text data sets character level penn tree bank data sets used karpathy tiny shakespeare linux kernel chose continuous based analysis lstm states ﬁrst components captured almost variance. text character method. text data sets hybrid algorithm performs better standalone lstm lstm state dimension. effect gets smaller increase lstm size makes less difference prediction hybrid algorithm states better states. joint hybrid algorithm outperforms sequential hybrid shakespeare data worse linux data suggests joint hybrid helpful smaller data sets. joint hybrid order magnitude slower sequential hybrid sgd-based slower train ffbs-based hmm. interpret lstm states hybrid algorithm lstm state dimensions states figures showing features identiﬁed lstm components. figures color-code training data states. figures apply k-means clustering lstm state vectors color-code training data clusters. lstm states pick spaces indentation special characters data examples lstm complement other learning different things spaces comments linux data punctuation shakespeare data. figure individual lstm hidden state dimensions identify similar features comment symbols linux data. hybrid component colors correspond states. blue cluster identiﬁes spaces. green cluster identiﬁes punctuation ends words. purple cluster picks vowels. hybrid lstm component colors correspond k-means clusters hidden state vectors. yellow cluster identiﬁes spaces. grey cluster identiﬁes punctuation purple cluster ﬁnds letters. figure visualizing lstm states shakespeare data hybrid lstm state dimensions states. lstm components learn complementary features text learn identify spaces lstm completely identify punctuation pick vowels already done. hybrid component colors correspond states. distinguishes comments indentation spaces spaces cluster identiﬁes punctuation brackets. green cluster also ﬁnds capitalized variable names. hybrid lstm component colors correspond k-means clusters hidden state vectors. distinguishes comments spaces beginnings lines spaces words indentation spaces opening brackets closing brackets green figure visualizing lstm states linux data hybrid lstm state dimensions states. lstm components learn complementary features text related spaces comments. hybrid hmm-rnn approaches combine interpretability hmms predictive power rnns. sometimes small hybrid model perform better standalone lstm size. visualizations show lstm components hybrid algorithm complement terms features learned data. connectionist speech recognition hybrid approach volume kluwer international series engineering computer science. kluwer academic publishers boston zhengping purushotham sanjay yan. distilling knowledge deep networks applications healthcare domain. neural information processing systems workshop machine learning healthcare been shah julie doshi-velez finale. mind generative approach interpretable feature selection extraction. cortes corinna lawrence neil daniel sugiyama masashi garnett roman neural information processing systems nguyen yosinski jason clune jeff. high deep neural networks easily fooled conﬁdence predictions unrecognizable images. ieee conference computer vision pattern recognition cvpr boston june continuous discrete discrete lstm hybrid hybrid lstm joint hybrid hybrid hybrid hybrid joint hybrid lstm hybrid hybrid joint hybrid lstm hybrid joint hybrid discrete discrete lstm joint hybrid hybrid hybrid joint hybrid lstm hybrid hybrid lstm joint hybrid hybrid hybrid joint hybrid lstm hybrid hybrid continuous discrete discrete lstm hybrid joint hybrid hybrid lstm hybrid joint hybrid hybrid lstm hybrid hybrid joint hybrid lstm hybrid hybrid", "year": 2016}