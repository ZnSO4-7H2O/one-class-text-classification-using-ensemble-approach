{"title": "Learning A Task-Specific Deep Architecture For Clustering", "tag": ["cs.LG", "cs.CV", "stat.ML"], "abstract": "While sparse coding-based clustering methods have shown to be successful, their bottlenecks in both efficiency and scalability limit the practical usage. In recent years, deep learning has been proved to be a highly effective, efficient and scalable feature learning tool. In this paper, we propose to emulate the sparse coding-based clustering pipeline in the context of deep learning, leading to a carefully crafted deep model benefiting from both. A feed-forward network structure, named TAGnet, is constructed based on a graph-regularized sparse coding algorithm. It is then trained with task-specific loss functions from end to end. We discover that connecting deep learning to sparse coding benefits not only the model performance, but also its initialization and interpretation. Moreover, by introducing auxiliary clustering tasks to the intermediate feature hierarchy, we formulate DTAGnet and obtain a further performance boost. Extensive experiments demonstrate that the proposed model gains remarkable margins over several state-of-the-art methods.", "text": "jointly optimize unsupervised feature learning supervised task-driven steps joint optimization usually rely solving complex bi-level optimization constitutes another efﬁciency bottleneck. more effectively model represent datasets growing sizes sparse coding needs refer larger dictionaries since inference complexity sparse coding increases linearly respect dictionary size scalability sparse coding-based clustering work turns quite limited. conquer limitations motivated introduce tool deep learning clustering lack attention paid. advantages deep learning achieved large learning capacity linear scalability stochastic gradient descent inference complexity feed-forward networks could naturally tuned jointly task-driven loss functions. hand generic deep architectures largely ignore problem-speciﬁc formulations prior knowledge. result encounter difﬁculties choosing optimal architectures interpreting working mechanisms initializing parameters. paper demonstrate combine sparse coding-based pipeline deep learning models clustering. proposed framework takes advantage sparse coding deep learning. speciﬁcally feature learning layers inspired graph-regularized sparse coding inference process reformulating iterative algorithms feed-forward network named tagnet. layers jointly optimized taskspeciﬁc loss functions end. technical novelty merits summarized three-folds deep feed-forward model proposed framework provides extremely efﬁcient inference process high scalability large scale data. allows learn descriptive features conventional sparse codes. discover incorporating expertise sparse code-based clustering pipelines improves performances signiﬁcantly. moreover greatly facilitates model initialization interpretation. abstract sparse coding-based clustering methods shown successful bottlenecks efﬁciency scalability limit practical usage. recent years deep learning proved highly effective efﬁcient scalable feature learning tool. paper propose emulate sparse coding-based clustering pipeline context deep learning leading carefully crafted deep model beneﬁting both. feed-forward network structure named tagnet constructed based graphregularized sparse coding algorithm. trained taskspeciﬁc loss functions end. discover connecting deep learning sparse coding beneﬁts model performance also initialization interpretation. moreover introducing auxiliary clustering tasks intermediate feature hierarchy formulate dtagnet obtain performance boost. extensive experiments demonstrate proposed model gains remarkable margins several state-of-the-art methods. introduction clustering aims learn hidden data patterns group similar structures unsupervised way. many classical clustering algorithms proposed k-means gaussian mixture model clustering maximum-margin clustering information theoretic clustering work well data dimensionality low. since high-dimensional data exhibits dense grouping low-dimensional embeddings researchers motivated ﬁrst project original data low-dimensional subspace clustering feature embeddings. among many feature embedding learning methods sparse codes proven robust efﬁcient features clustering veriﬁed many effectiveness scalability major concerns designing clustering algorithm data scenarios conventional sparse coding models rely iterative approximation algorithms whose inherently sequential structure well data-dependent complexity latency often constitute major bottleneck computational efﬁciency also results difﬁculty tries figure proposed pipeline consisting tagnet network feature learning followed clusteringoriented loss functions. parameters learnt end-to-end training data. block diagram solving less effective terms learning discriminative features clustering. authors extended semi non-negative matrix factorization model deep semi-nmf model whose architecture resembles stacked aes. proposed model substantially different previous approaches unique task-speciﬁc architecture derived sparse coding domain expertise well joint optimization clusteringoriented loss functions. model formulation proposed pipeline consists blocks. depicted fig. trained end-to-end unsupervised way. includes feed-forward architecture termed task-speciﬁc graph-regularized network learn discriminative features clustering-oriented loss function. tagnet task-speciﬁc graph-regularized network different generic deep architectures tagnet designed take advantage successful sparse code-based clustering pipelines aims learn features optimized clustering criteria encoding graph constraints regularize target solution. tagnet derived following theorem related work sparse coding clustering assuming data samples encoded sparse codes using learned dictionary learned atoms. sparse codes obtained solving following convex optimization authors suggested sparse codes used construct similarity graph spectral clustering furthermore capture geometric structure local data manifolds graph regularized sparse codes suggested solving da|| graph laplacian matrix constructed pre-chosen pairwise similarity matrix recently authors suggested simultaneously learn feature extraction discriminative clustering formulating task-driven sparse coding model proved joint methods consistently outperformed non-joint counterparts. deep learning clustering authors explored possibility employing deep learning graph clustering. ﬁrst learned nonlinear embedding original graph auto encoder followed kmeans algorithm embedding obtain ﬁnal clustering result. however neither exploits adapted deep architectures performs task-speciﬁc joint optimization. deep belief network nonparametric clustering presented. generative graphical model provides faster feature learning solve quite mild conditions initialized repeat shrinkage thresholding process convergence. moreover iterative algorithm could alternatively expressed block diagram fig. time-unfolding truncating fig. ﬁxed number iterations obtain tagnet form fig. learnt jointly data. tied weights stages. important note output tagnet necessarily identical predicted sparse codes solving instead goal tagnet learn discriminative embedding optimal clustering. eqn. indicates original neuron trainable thresholds decomposed linear scaling layers plus unit-threshold neuron. weights scaling layers diagonal matrices deﬁned elementwise reciprocal respectively. notable component tagnet branch stage. graph laplacian could computed advance. feed-forward process branch takes intermediate input applies operator deﬁned above. output aggregated output learnable layer. back propagation altered. graph regularization effectively encoded tagnet structure prior. appealing highlight tagnet lies effective straightforward initialization strategy. sufﬁcient data many latest deep networks train well random initializations without pre-training. however discovered poor initializations hamper effectiveness ﬁrst-order methods certain cases tagnet however much easier initialize model right regime. beneﬁts analytical relationships sparse coding network hyperparameters deﬁned could initialize deep models corresponding sparse coding components latter easier obtain. advantage becomes much important training data limited clustering-oriented loss functions assuming clusters parameters loss function corresponds i-th cluster paper adopt following forms clustering-oriented loss functions. maximum margin clustering approach proposed ﬁnds label samples running implicitly margin obtained would maximized possible labels referring deﬁnition authors designed max-margin loss prototype j-th cluster. testing predicted cluster label input determined weight vector achieves maximum model complexity proposed framework handle large-scale high-dimensional data effectively stochastic gradient descent algorithm. step back propagation procedure requires operations order training algorithm takes time addition easy parallelized thus could efﬁciently trained using gpus. connections existing models close connection sparse coding neural network. feed-forward neural network named lista proposed efﬁciently approximate sparse code input signal obtained solving advance. lista network learns hyperparameters general regression model training data pre-solved sparse codes using back-propagation. lista overlooks useful geometric information among data points therefore could viewed special case tagnet fig. moreover lista aims approximate optimal sparse codes pre-obtained therefore requires estimation tedious pre-computation authors exploit potential supervised task-speciﬁc feature learning. deeper look hierarchical clustering dtagnet deep networks well known capabilities learn semantically rich representations hidden layers section investigate intermediate features tagnet interpreted utilized improve model speciﬁc clustering tasks. compared related non-deep models hierarchical clustering property another unique advantage deep. strategy mainly inspired algorithmic framework deeply supervised nets fig. proposed deeply-task-speciﬁc graph-regularized network brings additional deep feedbacks associating clustering-oriented local auxiliary loss stage. auxiliary loss takes form overall except expected cluster number different depending auxiliary clustering task performed. dtagnet backpropagates errors overall loss layer also simultaneously auxiliary losses. seeking optimal performance target clustering dtagnet also driven auxiliary tasks explicitly targeted clustering speciﬁc attributes. enforcrs constraint hidden representation directly making good cluster prediction. addition overloss introduction auxiliary losses gives another strong push obtain discriminative sensible features individual stage. discovered classiﬁcation experiments auxiliary loss acts feature regularization reduce generalization errors results faster convergence. also section every indeed suited targeted task. deep semi-nmf model proposed learn hidden representations grant interpretation clustering according different attributes. authors considered problem mapping facial images identities. face image also contains attributes like pose expression help identify person depicted. experiments authors found factorizing mapping factor adds extra layer abstraction deep model could automatically learn latent intermediate representations implied clustering identity-related attributes. although clustering interpretation hidden representations figure dtagnet architecture taking multipie dataset example. model able simultaneously learn features pose clustering expression clustering identity clustering ﬁrst attributes related helpful last task. part image sources referred speciﬁcally optimized clustering sense. instead entire model trained overall reconstruction loss clustering performed using k-means learnt features. consequently clustering performance satisfactory. study shares similar observation motivation task-speciﬁc manner performing optimizations auxiliary clustering tasks jointly overall task. multipie contains around images subjects captured varied laboratory conditions. unique property multipie lies image comes labels identity illumination pose expression attributes. multipie chosen learn multiattribute features hierarchical clustering. experiments follow adopt subset images subjects different poses different emotions. notably pre-process images using piece-wise afﬁne warping utilized align images. setting learning rate experiments workstation intel xeon .ghz cpus gpu. training takes approximately hour mnist dataset. also observed training efﬁciency model scales approximately linearly data. experiments default value chosen crossvalidation. dictionary ﬁrst learned ksvd initialized based also pre-calculated formulated gaussian kernel obtaining output initial tagnet models could initialized based minimizing beneﬁts task-speciﬁc deep architecture denote proposed model tagnet plus entropyminimization loss tagnet-eml plus maximum-margin loss tagnet-mml respectively. include following comparison methods refer initializations proposed joint models non-joint counterparts denoted nj-tagnet-eml nj-tagnet-mml respectively. design baseline encoder fullyconnected feedforward network consisting three hidden layers dimension relu neuron. obvious parameter complexity tagnet. also tuned denoted be-eml bemml respectively. intend verify important argument proposed model beneﬁts task-speciﬁc tagnet architecture rather large learning capacity generic deep models. compare proposed models closest shallow competitors i.e. joint optimization methods graph-regularized sparse coding discriminative clustering re-implement work using losses denoted sceml sc-mml since authors already revealed sc-mml outperforms classical methods graph methods compare again. figure accuracy plots tagneteml/tagnet-mml mnist starting initialization tested every iterations. accuracy sc-eml/sc-mml also plotted baselines. although paper evaluates proposed method using image datasets methodology limited image subjects. apply widely-used measures evaluate clustering performances accuracy normalized mutual information follow convention many clustering work distinguish training testing. train models available samples dataset reporting clustering performances testing results. results averaged independent runs. experiment settings proposed networks implemented using cuda-convnet package network takes stages default. apply constant learning rate momentum trainable layers. batch size particular encode graph regularization prior model traincompare results reported performances multipie. revealed full comparison results table proposed task-speciﬁc deep architectures outperform noticeable margin. underlying domain expertise guides data-driven training principled way. contrast general-architecture baseline encoders appear produce much worse results. furthermore evident proposed end-to-end optimized models outperform non-joint counterparts. example mnist datasettagnetmml surpasses nj-tagnet-mml around accuracy nmi. comparing tagnet-eml/tagnet-mml sc-eml/sc-mml draw promising conclusion adopting parameterized deep architecture allows larger feature learning capacity compared conventional sparse coding. although similar points well made many ﬁelds interested closer look two. fig. plots clustering accuracy curves tagnet-eml/tagnet-mml mnist dataset along iteration numbers. model well initialized beginning clustering accuracy computed every iterations. ﬁrst clustering performances deep models even slightly worse sparse-coding methods mainly since initialization tagnet hinges truncated approximated graph-regularized sparse coding. small number iterations performance deep models surpass sparse coding ones continue rising monotonically reaching higher plateau. effects graph regularization graph regularization term imposes stronger smoothness constraints sparse codes larger also happens tagnet. investigate clustering performances tagnet-eml/tagnet-mml inﬂuenced various values. fig. observe identical general tendency three datasets. increases accuracy/nmi result ﬁrst rise decrease peak appearing interpretation local manifold information sufﬁciently encoded small ﬁne-tuned losses). hand large sparse codes over-smoothened reduced discriminative ability. note similar phenomenons noteworthy observe graph regularization behaves differently three them. notice coil dataset sensitive choice increasing leads improvement terms accuracy nmi. veriﬁes signiﬁcance graph regularization trying samples limited mnist dataset models obtain gain accuracy tuning however unlike coil almost always favors larger model performance mnist dataset tends saturated even signiﬁcantly hampered continues rising multipie dataset witnesses moderate improvements around measurements. sensitive two. potentially might complex variability original images makes graph unreliable estimating underlying manifold geometry. suspect sophisticated graphs help alleviate problem explore future. scalability robustness mnist dataset re-conduct clustering experiments cluster number ranging using tagneteml/tagnet-mml. fig. shows clustering accuracy change varying number clusters. clustering performance transits smoothly robustly task scale changes. examine proposed models’ robustness noise various gaussian noise whose standard deviation ranges re-train mnist model. fig. indicates tagnet-eml tagnetmml certain robustness noise. less even little visible performance degradation. tagnet-mml constantly outperforms tagnet-eml experiments interesting observe fig. latter slightly robust noise former. perhaps owing probability-driven loss form allows ﬂexibility. hierarchical clustering multipie observed multipie challenging basic identity clustering task. however comes several attributes pose expression illumination could assistance proposed dtagnet framework. section apply similar setting multipie subset setting pose clustering stage auxiliary task expression clustering stage auxiliary task. target might curious that matters performance boost deeply task-speciﬁc architecture brings extra discriminative feature learning proper design auxiliary tasks capture intrinsic data structure characterized attributes? answer important question vary target cluster number either reconduct experiments. table reveals auxiliary tasks even without striaghtforward task-speciﬁc interpretation still help gain better performances. comprehensible simply promote discriminative feature learning low-to-high coarse-to-ﬁne scheme. fact complementary observation conclusion found classiﬁcation hand least speciﬁc case target cluster numbers auxiliary tasks closer ground-truth models seem achieve training dtagnet-eml/dtagnet-mml follows aforementioned process except considering extra back-propagated gradients task stage then test separately targeted task. dtagnet auxiliary task also jointly optimized intermediate feature differentiate methodology substantially thus surprise table auxiliary task obtains much improved performances notably performances overall identity clustering task witness impressive boost around accuracy. also test dtagnet-eml/dtagnet-mml kept. experiments verify adding auxiliary tasks gradually overall task best performances. conjecture properly matched every hidden representation layer fact suited clustering attributes corresponding layer interest. whole model resembled problem sharing low-level feature ﬁlters among several relevant high-level tasks convolutional networks distinct context. hence conclude that deeply-supervised fashion shows helpful deep clustering models even explicit attributes constructing practically meaningful hierarchical clustering problem. however preferable exploit attributes available lead superior performances clearly interpretable models. learned intermediate features potentially utilized multi-task learning conclusion paper present deep learning-based clustering framework. trained features taskspeciﬁc deep architecture inspired sparse coding domain expertise optimized clusteringoriented losses. well-designed architecture leads effective initialization training signiﬁcantly outperforms generic architectures parameter complexity. model could interpreted enhanced introducing auxiliary clustering losses intermediate features. extensive experiments verify effectiveness robustness proposed models.", "year": 2015}