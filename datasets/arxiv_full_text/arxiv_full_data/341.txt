{"title": "Activation Maximization Generative Adversarial Nets", "tag": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "abstract": "Class labels have been empirically shown useful in improving the sample quality of generative adversarial nets (GANs). In this paper, we mathematically study the properties of the current variants of GANs that make use of class label information. With class aware gradient and cross-entropy decomposition, we reveal how class labels and associated losses influence GAN's training. Based on that, we propose Activation Maximization Generative Adversarial Networks (AM-GAN) as an advanced solution. Comprehensive experiments have been conducted to validate our analysis and evaluate the effectiveness of our solution, where AM-GAN outperforms other strong baselines and achieves state-of-the-art Inception Score (8.91) on CIFAR-10. In addition, we demonstrate that, with the Inception ImageNet classifier, Inception Score mainly tracks the diversity of the generator, and there is, however, no reliable evidence that it can reflect the true sample quality. We thus propose a new metric, called AM Score, to provide more accurate estimation on the sample quality. Our proposed model also outperforms the baseline methods in the new metric.", "text": "class labels empirically shown useful improving sample quality generative adversarial nets paper mathematically study properties current variants gans make class label information. class aware gradient cross-entropy decomposition reveal class labels associated losses inﬂuence gan’s training. based that propose activation maximization generative adversarial networks advanced solution. comprehensive experiments conducted validate analysis evaluate effectiveness solution am-gan outperforms strong baselines achieves state-of-the-art inception score cifar-. addition demonstrate that inception imagenet classiﬁer inception score mainly tracks diversity generator however reliable evidence reﬂect true sample quality. thus propose metric called score provide accurate estimation sample quality. proposed model also outperforms baseline methods metric. generative adversarial nets learning generative models recently shown promising results various challenging tasks realistic image generation conditional image generation image manipulation text generation despite great success still challenging current models produce convincing samples trained datasets high variability even image generation resolution e.g. cifar-. meanwhile people empirically found taking advantages class labels signiﬁcantly improve sample quality. three typical models make label information catgan builds discriminator multi-class classiﬁer; labelgan extends discriminator extra class generated samples; ac-gan jointly trains real-fake discriminator auxiliary classiﬁer speciﬁc real classes. taking class labels account models show improved generation quality stability. however mechanisms behind fully explored paper mathematically study models consideration class labels. derive gradient generator’s loss w.r.t. class logits discriminator named class-aware gradient labelgan show gradient tends guide generated sample towards speciﬁc real classes. moreover show ac-gan viewed model hierarchical class discriminator. based speciﬁcally argue model explicit target class would provide clearer gradient guidance generator implicit target class model like comparing show introducing speciﬁc real class logits replacing overall real class logit discriminator usually works better simply training auxiliary classiﬁer. argue that adversarial training missing auxiliary classiﬁer would make model likely suffer mode collapse produce quality samples. also experimentally predeﬁned label tends result intra-class mode collapse correspondingly propose dynamic labeling solution. proposed model named activation maximization generative adversarial networks empirically study effectiveness am-gan controlled experiments results consistent analysis note that am-gan achieves state-of-the-art inception score cifar-. addition experiments commonly used metric needs investigation. paper conduct study widely-used evaluation metric inception score extended metrics. show that inception model inception score mainly tracks diversity generator reliable evidence measure true sample quality. thus propose metric called score provide accurate estimation sample quality compensation. terms score proposed method also outperforms strong baseline methods. rest paper organized follows. section introduce notations formulate labelgan ac-gan∗ baselines. derive class-aware gradient labelgan section reveal class labels help training. section reveal overlaid-gradient problem labelgan propose am-gan solution also analyze properties am-gan build connections related work. section introduce several important extensions including dynamic labeling alternative predeﬁned labeling activation maximization view technique enhancing ac-gan∗. study inception score section accordingly propose metric score. section empirically study am-gan compare baseline models different metrics. finally conclude paper discuss future work section besides extending original two-class discriminator discussed section odena proposed alternative approach i.e. ac-gan incorporate class label information introduces auxiliary classiﬁer real classes original framework. core idea unchanged deﬁne variant ac-gan following refer ac-gan∗ outputs binary discriminator vanilla vectorizing operator similar deﬁned classes probability distribution real classes given auxiliary classiﬁer. ac-gan sample coupled target class loss auxiliary classiﬁer w.r.t. added generator leverage class label information. refer losses auxiliary classiﬁer i.e. auxiliary classiﬁer losses. formulation modiﬁed version original ac-gan. speciﬁcally omit auxiliary classiﬁer loss encourages auxiliary classiﬁer classify fake sample target class discussions provided section note also adopt log) loss generator. section introduce class-aware gradient i.e. gradient generator’s loss w.r.t. class logits discriminator. analyzing class-aware gradient labelgan gradient tends reﬁne sample towards classes sheds light class label information helps generator improve generation quality. delving details ﬁrst introduce following lemma gradient properties cross-entropy loss make analysis clearer. lemma logits vector softmax function current softmax probability distribution denote target probability distribution figure illustration overlaid-gradient problem. classes encouraged time combined gradient direct none classes. could addressed assigning generated sample speciﬁc target class instead overall real class. formulation overall gradient w.r.t. generated example vanilla gradient real classes distributed speciﬁc real class logit according current probability ratio such gradient naturally takes label information consideration generated sample higher probability certain class lead larger step towards direction increasing corresponding conﬁdence class. hence individually gradient discriminator sample tends reﬁne towards classes probabilistic sense. sample labelgan optimized real classes rather simply real vanilla gan. thus regard labelgan implicit target class model. reﬁning generated sample towards speciﬁc classes would help improve sample quality. recall similar inspirations related work. denton showed result could signiﬁcantly better trained separated classes. ac-gan introduces extra loss forces sample class achieves better result. labelgan generator gets gradients speciﬁc real class logits discriminator tends reﬁne sample towards classes. however labelgan actually suffers overlaid-gradient problem real class logits encouraged time. though tends make sample classes training gradient sample weighted averaging multiple label predictors. illustrated figure averaged gradient towards none classes. multi-exclusive classes setting valid sample classiﬁed classes discriminator high conﬁdence. resolve problem explicitly assign generated sample single speciﬁc class target. deﬁnition section model aforementioned formulation named activation maximization generative adversarial networks paper. interpretation towards naming section difference am-gan labelgan lies generator’s loss function. sample am-gan speciﬁc target class resolves overlaid-gradient problem. figure am-gan v.s. ac-gan∗ am-gan viewed combination labelgan auxiliary classiﬁer ac-gan∗ combination vanilla auxiliary classiﬁer. am-gan naturally conduct adversarial training among classes ac-gan∗ adversarial training conducted real-fake level missing auxiliary classiﬁer. labelgan am-gan models classes. introduce following cross-entropy decomposition lemma build connections models classes k-classes models lemma given similar analysis adapted ﬁrst term discriminator. note equals one. interestingly decomposing am-gan losses am-gan viewed combination labelgan auxiliary classiﬁer decomposition perspective disparate am-gan ac-gan combination vanilla auxiliary classiﬁer. auxiliary classiﬁer loss also viewed cross-entropy version generator loss catgan generator catgan directly optimizes entropy make sample high conﬁdence classes am-gan achieves ﬁrst term decomposed loss terms cross-entropy given target distribution. am-gan combination cross-entropy version catgan labelgan. extend discussion am-gan catgan appendix class label turns formulation ac-gan∗ viewed hierarchical classes model consists two-class discriminator k-class auxiliary classiﬁer illustrated figure conversely am-gan non-hierarchical model. classes stay level discriminator am-gan. hierarchical model ac-gan∗ adversarial training conducted real-fake twoclass level misses auxiliary classiﬁer. adversarial training theoretical guarantee global convergence pdata. taking original formulation instance generated samples collapse certain point i.e. pdata must exit another point pdata). given optimal pg+pdata collapsed point relatively lower score. existence higher score points maximizing generator’s expected score theory strength recover mode-collapsed state. practice pdata usually disjoint nevertheless general behaviors stay same samples collapse certain point likely relatively lower score adversarial network. without adversarial training auxiliary classiﬁer mode-collapsed generator would penalties auxiliary classiﬁer loss. experiments ac-gan likely mode-collapsed empirically found reducing weight auxiliary classiﬁer losses would help. section introduce extra adversarial training auxiliary classiﬁer improve ac-gan∗’s training stability sample-quality experiments. contrary am-gan non-hierarchical model naturally conduct adversarial training among class logits. section simply assume generated sample target class. possible solution like ac-gan predeﬁning sample class label substantially results conditional gan. actually could assign sample target class according current probability estimated discriminator. natural choice could class maximal probability currently argmaxi∈{...k} generated sample name dynamic labeling. according experiments dynamic labeling brings important improvements am-gan applicable models require target class generated sample e.g. ac-gan alternative predeﬁned labeling. experimentally models pre-assigned class label tend encounter intra-class mode collapse. addition dynamic labeling model remains generating pure random noises potential beneﬁts e.g. making smooth interpolation across classes latent space practicable. training viewed adversarial activation maximization process. speciﬁc generator trained perform activation maximization generated sample neuron represents probability target class discriminator trained distinguish generated samples prevents getting desired high activation. worth mentioning sample maximizes activation neuron necessarily high quality. traditionally people introduce various priors counter phenomenon adversarial process training detect unrealistic samples thus ensures high-activation achieved high-quality samples strongly confuse discriminator. experimentally ac-gan easily mode collapsed relatively weight auxiliary classiﬁer term generator’s loss function would help. section attribute mode collapse miss adversarial training auxiliary classiﬁer. adversarial activation maximization view without adversarial training auxiliary classiﬁer loss requires high activation certain class cannot ensure sample quality. ac-gan vanilla loss plays role ensuring sample quality avoiding mode collapse. introduce extra loss auxiliary classiﬁer ac-gan∗ enforce adversarial training experimentally consistently improve performance represents uniform distribution spirit catgan recall omit auxiliary classiﬁer loss experiments e∼g] improve ac-gan∗’s stability make less likely mode collapse also leads worse inception score. report detailed results section understanding phenomenon that encouraging auxiliary classiﬁer also classify fake samples target classes actually reduces auxiliary classiﬁer’s ability providing gradient guidance towards real classes thus also alleviates conﬂict loss auxiliary classiﬁer loss. difﬁculties generative models evaluation methodology section conduct mathematical empirical analysis widely-used evaluation metric inception score relevant metrics. show inception score mainly works diversity measurement propose score compensation inception score estimating generated sample quality. recently proposed metric evaluating performance generative models inception score found well correlated human evaluation publiclyavailable inception model pre-trained imagenet introduced. applying inception model generated sample getting corresponding class probability distribution inception score calculated common understanding inception score works lies high score ﬁrst term indicates generated samples high diversity high score second term −ex)] indicates individual sample high quality however taking cifar- illustration data evenly distributed classes inception model trained imagenet presented figure makes inception score problematic view decomposed scores i.e. −ex)]. would whether higher indicates better mode coverage whether smaller indicates better sample quality. figure training curves inception score decomposed terms. inception score i.e. ex)]); ex)]. common understanding inception score that value measures diversity generated samples expected increase training process. however usually tends decrease practice illustrated figure statistics cifar- training images. imagenet classes; distribution imagenet classiﬁer class; distribution cifar- classiﬁer class. inception model value score cifar- training data variant means even real data would still strongly prefer samples others. classiﬁer pre-trained cifar- values cifar- training data thus used indicator sample quality. experimentally that figure value usually going training process however expected increase. delve detail speciﬁc sample training data value score also variant illustrated figure means even real data would still strongly prefer samples others. operator inception score large variance value aggravate phenomenon. also observe preference class level figure e.g. ex)]=. trucks ex)]=. birds. seems imagenet classiﬁer indicators inception score cannot work correctly. next show inception score actually works diversity measurement. since individual indicators strongly correlated back inception score’s original formulation g)]. form could interpret inception score requires sample’s distribution highly different overall distribution generator indicates good diversity generated samples. empirically observed mode-collapsed generator usually gets inception score. extreme case assuming generated samples collapse single point would minimal inception score result zero. simulate mode collapse complicated case design synthetic experiments following given points xn−} point adopting distribution representing class vectorization operator length deﬁned section randomly drop points evaluate draw curve. showed figure increases value monotonically increases general means well capture mode dropping diversity generated distributions. figure mode dropping analysis inception score. uniform density classes; gaussian density classes. value monotonically increases general number kept classes increases illustrates inception score able capture mode dropping diversity generated distributions. error indicates values random dropping. remaining question whether good mode coverage sample diversity mean high quality generated samples. analysis evidence. possible explanation that practice sample diversity usually well correlated sample quality. note point multiple variants missing generated cannot detected score. means accordingly pretrained classiﬁer score cannot detect intra-class level mode collapse. also explains inception network imagenet could good candidate cifar-. exploring optimal challenge problem shall leave future work. however evidence using inception network trained imagenet accurately measure sample quality shown section compensate inception score propose introduce extra assessment using accordingly pretrained classiﬁer. accordingly pretrained classiﬁer real samples share similar samples hold scores less showed figure demonstrates classiﬁer used indicator sample quality. entropy term actually problematic training data evenly distributed classes argmin uniform distribution. take ¯ctrain account replace divergence ¯ctrain requires close ¯ctrain sample entropy minimal value score zero smaller value better. sample training curve score showed figure indicators score work expected. table inception score score results. models column share network structures hyper-parameters. applied dynamic predeﬁned labeling models require target classes. table maximum value mean ms-ssim various models classes cifar-. high-value indicates obvious intra-class mode collapse. please refer figure appendix visual results. empirically validate analysis effectiveness proposed method conduct experiments image benchmark datasets including cifar- tiny-imagenet comprises classes training images class. evaluation several metrics used throughout experiments including inception score imagenet classiﬁer score corresponding pretrained classiﬁer dataset densenet model. also follow odena mean ms-ssim randomly chosen pairs images within given class coarse detector intra-class mode collapse. modiﬁed dcgan structure listed appendix used experiments. visual results various models provided appendix considering page limit figure etc. repeatable experiment code published research. ﬁrst question whether training auxiliary classiﬁer without introducing correlated losses generator would help improve sample quality. words generator loss ac-gan∗ setting. shown table improves gan’s sample quality improvement limited comparing methods. indicates introduction correlated loss plays essential role remarkable improvement training. usage predeﬁned label would make model transform conditional version substantially disparate generating samples pure random noises. experiment dynamic labeling ac-gan∗ ac-gan∗+ am-gan seek fair comparison among different discriminator models including labelgan gan. keep network structure hyper-parameters different models difference lies output layer discriminator i.e. number class logits necessarily different across models. shown table ac-gan∗ achieves improved sample quality vanilla sustains mode collapse indicated value ms-ssim table introducing adversarial training auxiliary classiﬁer ac-gan∗+ outperforms ac-gan∗. implicit target class model labelgan suffers overlaid-gradient problem achieves relatively higher sample entropy score comparing explicit target class model am-gan ac-gan∗+ table proposed am-gan model reaches best scores baselines. also test ac-gan∗ decreased weight auxiliary classiﬁer losses generator achieves inception score score ms-ssim. ms-ssim indicates obvious mode collapse also conform analysis. am-gan achieves inception score previous experiments signiﬁcantly outperforms baseline models implementation reported scores table enhancing discriminator ﬁlters layer am-gan also outperforms orthogonal work enhances class label information class splitting. result am-gan achieves state-of-the-art inception score cifar-. training process important ensure balance generator discriminator. generator’s network structures switching dynamic labeling class condition hard hold good balance generator discriminator avoid initial intra-class mode collapse discriminator need powerful; however usually turns discriminator powerful provide suitable gradients generator results poor sample quality. nevertheless suitable discriminator conduct comparisons results found table general conclusion similar above ac-gan∗+ still outperforms ac-gan∗ am-gan reaches best performance. it’s worth noticing ac-gan∗ suffer mode collapse setting. class conditional version although ﬁne-tuned parameters inception score still relatively low. explanation could that class conditional version sample diversity still tends decrease even relatively powerful discriminator. slight intra-class mode collapse per-sample-quality tends improve results lower score. supplementary evidence around labelgan need explicit labels model experiment settings. please note inception score score worse conditional version. difference discriminator becomes powerful extended layer attests balance generator discriminator crucial. that without concern intra-class mode collapse using dynamic labeling makes balance generator discriminator much easier. loss note report results modiﬁed version ac-gan i.e. ac-gan∗ table take omitted loss back ac-gan∗ leads original ac-gan turns achieve worse results inception score score cifar- though dismisses mode collapse. speciﬁcally dynamic labeling setting inception score decreases score increases predeﬁned class setting inception score decreases score increases performance drop might different network architectures hyper-parameters ac-gan still fail achieve report inception score i.e. cifar- using reported hyper-parameters original paper. since publicize code suppose might unreported details result performance gap. would leave studies future work. plot training curve terms inception score score figure inception score score evaluated number samples salimans comparing inception score score stable general. samples inception score would stable however evaluation inception score relatively costly. better alternative inception model could help solve problem. ac-gan∗’s curves appear stronger jitter relative others. might relate counteract auxiliary classiﬁer loss loss generator. another observation am-gan terms inception score comparable labelgan ac-gan∗+ beginning terms score quite distinguishable other. cifar- experiments results consistent analysis proposed method outperforms strong baselines. demonstrate conclusions generalized experiments another dataset tiny-imagenet. tiny-imagenet consists classes fewer samples class cifar- challenging. downsize tiny-imagenet samples simply leverage network structure used cifar- experiment result showed also table comparison am-gan still outperforms methods remarkably. ac-gan∗+ gains better performance ac-gan∗. paper analyze current models incorporate class label information. analysis shows that labelgan works implicit target class model however suffers overlaidgradient problem meantime explicit target class would solve problem. demonstrate introducing class logits non-hierarchical i.e. replacing overall real class logit discriminator speciﬁc real class logits usually works better simply supplementing auxiliary classiﬁer provide activation maximization view training highlight importance adversarial training. addition according experiments predeﬁned labeling tends lead intra-class mode collapsed propose dynamic labeling alternative. extensive experiments benchmarking datasets validate analysis demonstrate proposed am-gan’s superior performance strong baselines. moreover delve deep widelyused evaluation metric inception score reveal mainly works diversity measurement. also propose score compensation accurately estimate sample quality. paper focus generator sample quality related work focuses discriminator semi-supervised learning. future work would like conduct empirical studies discriminator learning semi-supervised learning. extend am-gan unlabeled data appendix unsupervised semi-supervised accessible framework am-gan. classiﬁer-based evaluation metric might encounter problem related adversarial samples requires study. combining am-gan integral probability metric based models wasserstein could also promising direction since orthogonal work. denton emily chintala soumith fergus deep generative image models using laplacian pyramid adversarial networks. advances neural information processing systems heusel martin ramsauer hubert unterthiner thomas nessler bernhard klambauer günter hochreiter sepp. gans trained time-scale update rule converge nash equilibrium. arxiv preprint arxiv. nguyen dosovitskiy alexey yosinski jason brox thomas clune jeff. synthesizing preferred inputs neurons neural networks deep generator networks. advances neural information processing systems nguyen yosinski jason bengio yoshua dosovitskiy alexey clune jeff. plug play generative networks conditional iterative generation images latent space. arxiv preprint arxiv. salimans goodfellow zaremba wojciech cheung vicki radford alec chen improved techniques training gans. advances neural information processing systems szegedy christian vanhoucke vincent ioffe sergey shlens wojna zbigniew. rethinking inception architecture computer vision. proceedings ieee conference computer vision pattern recognition wang zhou simoncelli eero bovik alan multiscale structural similarity image quality assessment. signals systems computers conference record thirty-seventh asilomar conference volume ieee zhang hongsheng zhang shaoting huang xiaolei wang xiaogang metaxas dimitris. stackgan text photo-realistic image synthesis stacked generative adversarial networks. arxiv preprint arxiv. jun-yan krähenbühl philipp shechtman efros alexei generative visual manipulation natural image manifold. european conference computer vision springer label smoothing avoiding extreme logits value showed good regularization general version label smoothing could modifying target probability discriminator) salimans proposed one-side label smoothing. apply label smoothing real samples reasoning one-side label smoothing applying label smoothing fake samples lead fake mode data distribution obscure. next show exact problems applying label smoothing fake samples along log) generator loss view gradient w.r.t. class logit i.e. class-aware gradient also show problem exist using log) generator loss. without label smooth log) always∗ preserves gradient direction log) though giving difference gradient scale. must note non-zero gradient mean gradient efﬁcient valid. both-side label smoothed version strong connection least-square fake logit ﬁxed zero discriminator maps real real logit maps fake real logit generator contrast tries fake sample gradient logit also similar. auxiliary classiﬁer loss am-gan also viewed cross-entropy version catgan generator catgan directly optimizes entropy make sample class am-gan achieves ﬁrst term decomposed loss terms cross-entropy given target distribution. am-gan cross-entropy version catgan combined labelgan introducing additional fake class. pref reference label distribution prediction unsupervised data. example pref could uniform distribution requires unlabeled data make candidate class logits. loss optionally added semi-supervised setting pref could deﬁned predicted label distribution labeled training data ex∼pdata]. recently proposed metric evaluating performance generative models inceptionscore found well correlated human evaluation pre-trained publicly-available inception model introduced. applying inception model generated sample getting corresponding class probability distribution inception score calculated overall class distribution training data ¯ctrain added reference. show following that fact mode score inception score equivalent. lemma class probability distribution sample denote another probability distribution *layer used class condition experiments optimizer adam beta=. beta=.; batch size=. learning rate exponential decay stair initial learning rate weight normalization weight", "year": 2017}