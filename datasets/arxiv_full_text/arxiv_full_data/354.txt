{"title": "XFlow: 1D-2D Cross-modal Deep Neural Networks for Audiovisual  Classification", "tag": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We propose two multimodal deep learning architectures that allow for cross-modal dataflow (XFlow) between the feature extractors, thereby extracting more interpretable features and obtaining a better representation than through unimodal learning, for the same amount of training data. These models can usefully exploit correlations between audio and visual data, which have a different dimensionality and are therefore nontrivially exchangeable. Our work improves on existing multimodal deep learning metholodogies in two essential ways: (1) it presents a novel method for performing cross-modality (before features are learned from individual modalities) and (2) extends the previously proposed cross-connections, which only transfer information between streams that process compatible data. Both cross-modal architectures outperformed their baselines (by up to 7.5%) when evaluated on the AVletters dataset.", "text": "architecture process ﬁxed-size inputs perform averaging video frames corresponding mfcc sets examples dataset. since length video example vary considerably person person examples undergo averaging large window size. results loss information changes consecutive frames expected would hurt performance model. indeed architecture described subsection ii-b processes variablelength examples capable accurate predictions. primary role cross-connections perform information exchange features individual modalities learned fundamental incompatibility exists data—there trivially interpretable abstract— propose multimodal deep learning architectures allow cross-modal dataﬂow feature extractors thereby extracting interpretable features obtaining better representation unimodal learning amount training data. models usefully exploit correlations audio visual data different dimensionality therefore nontrivially exchangeable. work improves existing multimodal deep learning metholodogies essential ways presents novel method performing cross-modality extends previously proposed cross-connections transfer information streams process compatible data. cross-modal architectures outperformed baselines evaluated avletters dataset. interesting extension unimodal learning consists deep models fuse several modalities thereby learn shared representation outperforming previous architectures discriminative tasks. however cross-modality existing models occurs features learned thereby preventing unimodal feature extractors exploiting information contained within modalities. work presented paper focused enabling information modalities exchanged extracting interpretable features making possible exploit correlations several types data directly. information exchange occur data varying dimensionality thus poses highly nontrivial problem. idea behind method correlations different kinds data learn better representation would result combining independent unimodal feature extractors given amount training data. illustrated figure ﬁrst multimodal architecture takes input tuple outputs probability distribution possible classes example belongs ﬁrst element represents visual modality processed convolutional neural network whereas max-pool} block fully-connected layers processing audio data vice versa. therefore design complex types cross-connections would enable data exchanged sensible manner allow useful interpretations transfers. passed convolutional ﬂattened processed fully-connected layer. finally concatenate output latter output corresponding fully-connected layer directly outputs cross-connections perform inverse operation output fully-connected layer passed another layer type number features matches dimensionality required deconvolution operation. apply latter reshaped data concatenate result output residual learning purpose making internal layers neural network represent data accurately. cross-connection design allows straightforwardly including residual cross-modal connections allow input modality directly interact another modality’s intermediate representations. effectively potential correct unwanted effects stream’s intermediate transformations might caused. figure also illustrates residual connections constructed similar manner cross-connections. mathematically cross-connection residuals reshape∗ kres stream depth concatenate reshape∗ kxcon learnable weights learnable {cnn× mlp}–lstm cnn× model namely tuples form however fundamental difference lies fact video frame/mfccs pair provided separately input pre-concatenation streams. brings forward crucial advantage average data across frames keeping temporal structure intact maintaining richer source features modalities. fig. {cnn× mlp}–lstm macro-scale sequential processing across time steps. rectangle figure input modalities denoted \u0014ximg xmfcc output lstm layer time shown figure feature extractor single frame weight-shared across frames allows process input sequences arbitrary lengths. features extracted modalities frame gets sequence illustrated figure layer produces features entire example ﬁnally classiﬁed softmax layer. additionally crossresidual connections designed manner pool} block removed number kernels remaining layers halved. underlying motivation choice arises features longer extracted averaged block corresponding entire video rather individual frame thereby heavily sparsifying available information single input. outputs zero upon xavier initialisation. enable network beneﬁt transmitted data applied general prelu activation function allows data leak negative input space. folds assess performance classiﬁer. folds corresponds different person dataset seen extension usual leave-one-out crossvalidation approach fold corresponds example. allowed examine well models behave realistic audiovisual recognition setting— train classiﬁer data collected group people expect model able correctly identify information exposed person. fully-connected layer stream merge layer ﬁnal fully-connected layer. chose larger value case increased likelihood overﬁtting fully-connected layers number parameters much larger convolutional layers. table classiﬁcation accuracy avletters dataset mentioned architectures. p-values corresponding statistically signiﬁcant results underlined following table. kinds information. residual crossconnections previously used process modalities require intrinsic transformations. consequently main challenge building variety cross-modal connections lied fundamental incompatibility data types exchanged. novel cross-modality enabled architectures favourably exploit correlations modalities outperforming baselines avletters dataset. research presented paper could extended modalities potentially processing hierarchical manner. future directions also include investigating representations learned cross-modal connections construction higherquality dataset would suffer over-processing alignment issues. zhang delving deep rectiﬁers surpassing human-level performance imagenet classiﬁcation proceedings ieee international conference computer vision achieved edge model without crossconnections p-value showing statistical signiﬁcance. illustrates quality issues avletters—along averaging results loss information discrepancy visual audio data pre-processing required average mfcc sets video frames time windows different lengths likely resulted misalignment visual audio information. using newly developed crossresidual connections transform representations vice versa designed novel deep learning architectures processing audiovisual data could easily applied", "year": 2017}