{"title": "The Statistical Recurrent Unit", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Sophisticated gated recurrent neural network architectures like LSTMs and GRUs have been shown to be highly effective in a myriad of applications. We develop an un-gated unit, the statistical recurrent unit (SRU), that is able to learn long term dependencies in data by only keeping moving averages of statistics. The SRU's architecture is simple, un-gated, and contains a comparable number of parameters to LSTMs; yet, SRUs perform favorably to more sophisticated LSTM and GRU alternatives, often outperforming one or both in various tasks. We show the efficacy of SRUs as compared to LSTMs and GRUs in an unbiased manner by optimizing respective architectures' hyperparameters in a Bayesian optimization scheme for both synthetic and real-world tasks.", "text": "sophisticated gated recurrent neural network architectures like lstms grus shown highly effective myriad applications. develop un-gated unit statistical recurrent unit able learn long term dependencies data keeping moving averages statistics. sru’s architecture simple un-gated contains comparable number parameters lstms; srus perform favorably sophisticated lstm alternatives often outperforming various tasks. show efﬁcacy srus compared lstms grus unbiased manner optimizing respective architectures’ hyperparameters bayesian optimization scheme synthetic realworld tasks. analysis sequential data long staple machine learning. domain areas like natural language speech music video processing recently garnered much attention. study sequences broad extended general functional analysis recent success neural network based models especially recurrent architectures. recurrent networks dynamical systems represent time recursively. example simple recurrent unit contains hidden state depends previous hidden state. however training networks observed difﬁcult practice exploding vanishing gradients propagating error gradients time exploding gradients mitigated techniques like gradient clipping normalization vanishing gradients harder deal with. result sophisticated gated architectures like long-short term memory networks gated recurrent units networks developed. gated architectures contain memory cells along gates control much decay time thereby aiding networks’ ability learn long term dependencies sequences. notwithstanding still challenges capturing long term dependencies gated architectures paper present simple un-gated architecture statistical recurrent unit often outperforms complicated alternatives. although keeps simple moving averages summary statistics novel architecture makes adept previous gated units capturing long term information sequences comparing across different windows time. instance unlike traditional recurrent units obtain multitude viewpoints past simple linear combinations averages. shall illustrate efﬁcacy using real-world synthetic sequential data tasks. structure paper follows ﬁrst detail architecture well provide several intuitions insights design; after describe experiments comparing popular gated alternatives perform dissective study gaining understanding unit exploring various hyper-parameters affect performance; ﬁnally discuss conclusions study. maintains long term sequential dependencies rather intuitive fashion–through summary statistics. name implies statisticians often employ summary statistics trying represent dataset. quite naturally then look algorithm learns represent data seen previously much vein neural statistician averaging statistics sequential points lose valuable temporal information. maintains sequential information ways ﬁrst generate recurrent statistics depend context previously seen data; second generate moving averages several scales allowing model distinguish type data seen different points past. expound methods creating temporally-aware statistics below. shall statistical design yields powerful simple model able analyze sequential data create summary statistics learning sequences. furthermore relus exponential moving averages able overcome vanishing gradient issues common many recurrent units. consider input sequence real valued points seen second table compute vector statistics point. here vector independent points average produce summary statisvectors tics sequence. approach amounts treating sequence i.i.d. points drawn form distribution marginalizing time. clearly lose temporal information useful many sequence related tasks. interesting note global average pooling operations gained recent traction convolutional networks analogously i.i.d. statistic approach global averaging lose spatial information high-level summary statistics provide effective representation. still marginalizing time provide robust approach sequence tasks thus consider following methods producing statistics. first provide temporal information whilst still utilizing averages recurrent statistics also depend values previous points compute statistics point function also function previous statistics etc.) records input index. averaged statistics i.e. complete sequence. recurrent statistics undoubtedly suffer curse dimensionality. hence consider restrictive model recurrent statistics expound second provide even temporal information considering summary statistics multiple scales. shed light dynamics statistics time using several weights summary statistics. simple hypothetical example consider taking multiple means across separate time windows approach illustrate summary statistics evolve time. exponential moving averages compute means; hence consider multiple weights taking exponential means various scales shown last table later show multi-scaled approach capable combinatorial number viewpoints past statistics simple linear combinations. function producing statistics given current point previous statistics constant initial vector convention. note general standpoint given ﬂexible model enough dimensions recurrent summary statistics like perfectly encode ones sequence. take instance followdiscussed broad terms create temporally-aware summary statistics multi-scaled recurrent statistics. below cover speciﬁcally creates uses summary statistics sequences. design deliberately chosen allow long term dependencies learned. better elucidate design intuition take brief excursion another statistics machine learning representation data mean embeddings distributions core concept mmes embed thereby represent distribution statistics distribution given positive semideﬁnite kernel numerous works shown success representing distributions sets mmes interpretation design srus modifying mme’s sequences. course applying mmes directly sequences simply ignore non-i.i.d. nature sequences treat points comprising set. however loses important sequential information previously mentioned. discuss speciﬁc modiﬁcations make traditional mmes beneﬁts yield. first note clear analogue mean embedding moving average moving averages clearly serving summary statistics previously seen data. however statistics averaging comprised apriori rkhs features typical mmes rather learned non-linear features. beneﬁt using data-driven statistics interpreted using linear kernel learned features. second recall typical mmes statistics depend single point aforementioned i.i.d. data however loses sequential information averaged. instead wish assign statistics depend data seen since provides context one’s current point sequence. instance want statistic keeps track difference current point mean previous data. provide context based previous data making statistics considered time wise non-linearity take relu max. operates exponential moving averages kept various scales moving averages recurrent statistics dependent current input also features averages moving averages concatenated used create output upwards network. figure graphical representation sru. solid lines indicate dependence current value node. dashed lines indicate dependence previous value node. current point well summary previous data used make statistics turn used moving averages ﬁnally output feed-forward rest network. practiced noted sufﬁces worth noting exponential averages inputs considered previously however approach performs moving average linear features depends current observation fairly inﬂexible. furthermore work considers scale feature limiting views available statistic one. relus recurrent units also recently explored however statistics kept limited simple initialized special manner. third multi-scaled moving averages statistics gives simple powerful rich view past data unique recurrent unit. short keeping moving averages different scales able uncover differences statistics various times past. note unroll moving averages thus smaller weighs current statistics older statistics; hence concatenated vector provides multi-scale view statistics time instance keeping statistics short long terms pasts already yields information evolution sequence time. figure unroll moving average updates visualize different emphasis past varying statistics plot values weights moving averages points past across rows. alpha values closer focus recent past values close maintain emphasis distant past well. interesting useful property keeping multiple scales statistic obtain combinatorial number viewpoints past simple linear combinations ones statistics. instance properly chosen wjµ−wkµ provides aggregate statistics past course complicated linear combinations performed obtain richer viewpoints comprised multiple windows. furthermore using linear projection statistics able compute output features combined viewpoints several statistics. kind multi-viewpoint perspective previously seen data difﬁcult produce traditional gated recurrent units since must encode sequence currently store activation separate nodes figure visualize power taking linear combinations providing different viewpoints past data. show effective weights would used weighing statistics considers .−µ; equivalent considering statistics distant past. similarly show effective weights taking .−µ− .−µ− rows respectively. linear combinations amount considering viewpoints concentrated various points past. lastly worth noting complicated linear combinations lead even richer views previous statistics; instance show .−µ−.−µ concentrates statistics distant recent past de-emphasizes statistics data less recent past. viewpoint future use. srus hand need take simple linear combinations capture various viewpoints past. example shown above statistics distant past available simple subtraction moving averages windowed view would require gated unit learn stop averaging certain point sequence corresponding statistic would yield information outside window. contrast statistic kept provides combinatorial number varying perspectives past linear combinations multi-scaled nature. previously mentioned shown vanishing gradients make learning recurrent units difﬁcult inability propagate error gradients time. notwithstanding simple un-gated structure features several safeguards vanishing gradients. first units statistics comprised relus. relus observed easier train general deep networks success recurrent units intuitively relus allow propagation error positive inputs without saturation vanishing gradients traditional sigmoid units. ability relus makes especially adept learning long term dependencies time. furthermore explicit moving average statistics allows longer term learning. consider following derivative error signal w.r.t. element compared performance popular gated recurrent units lstm unit. experiments performed tensorflow used standard implementations grucell basiclstmcell grus lstms respectively. order perform fair unbiased comparison recurrent units hyper-parameters greatly affect performance used hyperopt bayesian optimization package. believe approach gives algorithm fair shot succeed without injecting biases experimenters imposing gross restrictions architectures considered. experiments used optimization using gradient clipping norm algorithms. unless otherwise speciﬁed trials performed search following hyper-parameters validation initial learning rate initial learning rate used range decay multiplier multiply learning rate every iterations range three dropout keep rate percent output units kept dropout range four units number units recurrent unit addition following parameters searched stats dimensionality summary dims dimensionality first provide evidence traditional gated units difﬁculties capturing type multi-scale recurrent statistic based dependencies offers. show relative inefﬁciency traditional gated units learning long term dependencies statistics considering synthetic data ground truth sru. iid∼ begin sequences results projection generate total points sequence training sequences validation sequences testing sequences. ground truth statistical recurrent unit three statistics positive part inputs negative part inputs internal statistic denote {αi} moving averages using respective statistic. internal statistic used updating statistics updated ground truth constructed generated training validation testing sequences. seen figure sequences follow simple pattern start negative values quickly pushed zero positive values follow parabolic line hitting zero point slope downward depending initial values. simple clear trained recurrent units must able hold long-term information since sequences converge point future behaviour depends initial values. look minimize mean squared errors loss consider sequence |xt+ output network conducted trials bayesian optimization described obtained following results table long-term statistical relationships captured indeed different traditional recurrent units. previously mentioned able obtain multitude different views statistics task traditional units achieve less efﬁciently since must devote whole memory cell viewpoint statistic. show below able outperform traditional gated units long term problems even real data generated model class. next explore ability recurrent units longterm dependencies ones data synthetic task using real dataset. observed lstms perform poorly classifying long pixel-by-pixel sequence mnist digits synthetic task gray-scale mnist digit image ﬂattened observed sequence task based output observed after feeding network classify digit corresponding image hence project output recurrent unit dimensions softmax activation. report bayesian optimized results table resource constraints trial consisted training iterations. able outperform grus lstms. given long length dependencies pixel sequences experiment surprising srus’ abilities capture long-term dependencies aiding achieve much lower error. next study behavior statistical recurrent unit dissective study vary several parameters consider variants base model with stats=; dims=; units=. keep parameters initial learning rate decay ﬁxed optimal values found unless learning case also learning rates need multi-scaled recurrent statistics. recall designed statistics used expressly capture long term time dependencies sequences. recurrent statistics i.e. statistics themselves depend previous points’ statistics multiscaled averages. show timedependent design choices vital capturing long term dependencies data. furthermore show relu statistics lends better learning. explored impact time-dependent statistics learning ﬁrst considering naive i.i.d. summary statistics sequences. achieved using dims= {.}. past-dependent context used statistics i.e. used i.i.d.-type statistics typical unordered sets. furthermore single scale near means points’ statistics weighted nearly identically regardless index. optimized using recurrent statistics single scale using recurrent statistics single scale using recurrent statistics multiple scales report errors table predictably cannot learn simply keeping i.i.d. type statistics pixel values single scale. furthermore using recurrent statistics enough. interesting note however keeping i.i.d. statistics multiple scales able predict digits next explored effects scales keep statistics varying considering table additional longer scales learning dataset. surprising given long term nature pixel sequences. lastly considered non-relu statistics changing element-wise non-linearity hyperbolic tangent tanh. postulated relus would help learning since observed better handle problem vanishing gradients. evidence swapping relus hyperbolic tangent units srus error rate using hyperbolic tangent units. although previous uses relus required careful initialization srus able relus better learning without special considerations. dimension recurrent summary. next explore affect varying number dimensions used recurrent summary statistics consider dims previously discussed provides context based past data produce noni.i.d. statistics moves along sequences. would expect dimensionality limit information past values small hinder performance. also interesting enough dimensions diminishing returns adding more. number statistics outputs. finally vary number statistics stats outputs units. interestingly seems robust number outputs propagated network. however performance considerably affected number statistics considered. henceforth consider real data sequence learning tasks. first used polyphonic music datasets boulanger-lewandowski time-step binary vector representing notes played respective time-step. since required predict binary vectors used element-wise sigmoid i.e. binary vector notes modeled output feeding recurrent network. following experiment modeled frequency cepstrum coefﬁcients dataset nearly scraped sound clips electronica-genre songs. mfccs perceptually based spectral features positioned logarithmically scale approximates human auditory system’s response looked model real-valued coefﬁcients using recurrent units modeling projection output recurrent unit next consider weather data prediction using north america regional reanalysis project. dataset provides long-term consistent climate data regional scale north american domain. period reanalyses october present analyses made times daily take input sequences year-long sequences weather variables location year i.e. input sequence length sequence weather variables given lat/lon coordinate. considered following variables presm pressure tcdc total cloud cover relative humidity tmpsfc surface temperature snod snow depth surface ugrdm component wind ground; vgrdm component wind ground. variables standardized figure example sequences. results using training location sequences validation testing instances. again look model next point sequence projection output recurrent unit feeding previous points. table srus lstms perform nearly identically; perhaps cyclical nature climate data beneﬁcial gated units. finally look predict positions national basketball association players based previous court positions play. optical tracking data project provided stats sportvu product obtained data composed coordinates players ball. minimize squared norm errors predictions. observed large margin improvement srus gated architectures table reminiscent synthetic data experiment suggests dataset contains long term dependencies able exploit. believe summary statistics under-explored modern recurrent units. although recent studies convolutional networks considered global average pooling essentially using high-level summary statistics represent images little exploration summary statistics modern recurrent networks. introduce statistical recurrent unit novel architecture seeks capture long term dependencies data using simple moving averages rectiﬁed-linear units. motivated success mean-map embeddings representing unordered datasets interpreted alteration mmes sequential data. main modiﬁcations follows ﬁrst uses data-driven statistics unlike typical mmes rkhs features a-priori selected class kernels; second srus recurrent statistics dependent current point previous points’ statistics condensation kept moving averages; third keep moving averages various scales. provide evidence combination modiﬁcations yield much better results isolation. resulting recurrent unit especially adept capturing long term dependencies data readily access combinatorial number viewpoints past windows simple linear combinations. abadi mart´ın barham paul chen jianmin chen zhifeng davis andy dean jeffrey devin matthieu ghemawat sanjay irving geoffrey isard michael tensorﬂow system large-scale machine proceedings usenix sympolearning. sium operating systems design implementation savannah georgia bergstra james komer brent eliasmith chris yamins david hyperopt python library model selection hyperparameter optimization. computational science discovery boulanger-lewandowski nicolas bengio yoshua vincent pascal. modeling temporal dependencies high-dimensional sequences application polyphonic arxiv preprint music generation transcription. arxiv. kyunghyun merri¨enboer bart bahdanau dzmitry bengio yoshua. properties neural machine translation encoder-decoder approaches. arxiv preprint arxiv. chung junyoung gulcehre caglar kyunghyun bengio yoshua. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. donahue jeffrey anne hendricks lisa guadarrama sergio rohrbach marcus venugopalan subhashini saenko kate darrell trevor. long-term recurrent convolutional networks visual recognition proceedings ieee conference description. computer vision pattern recognition graves alex mohamed abdel-rahman hinton geoffrey. speech recognition deep recurrent neural networks. acoustics speech signal processing ieee international conference ieee iandola forrest song moskewicz matthew ashraf khalid dally william keutzer kurt. squeezenet alexnet-level accuracy fewer arxiv preprint parameters and¡ model size. arxiv. jarrett kevin kavukcuoglu koray lecun yann best multi-stage architecture object recognition? computer vision ieee international conference ieee mikolov tomas joulin armand chopra sumit mathieu michael ranzato marc’aurelio. learning longer memory recurrent neural networks. arxiv preprint arxiv. muandet krikamol fukumizu kenji sriperumbudur bharath sch¨olkopf bernhard. kernel mean embedding distributions review beyonds. arxiv preprint arxiv. nair vinod hinton geoffrey rectiﬁed linear units improve restricted boltzmann machines. proceedings international conference machine learning smola alex gretton arthur song sch¨olkopf bernhard. hilbert space embedding distributions. international conference algorithmic learning theory springer vinyals oriol kaiser łukasz terry petrov slav sutskever ilya hinton geoffrey. grammar advances neural information foreign language. processing systems", "year": 2017}