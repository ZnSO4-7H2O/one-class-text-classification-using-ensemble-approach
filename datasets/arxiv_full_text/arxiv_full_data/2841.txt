{"title": "Robust Maximization of Non-Submodular Objectives", "tag": ["stat.ML", "cs.AI", "cs.DS", "cs.LG"], "abstract": "We study the problem of maximizing a monotone set function subject to a cardinality constraint $k$ in the setting where some number of elements $\\tau$ is deleted from the returned set. The focus of this work is on the worst-case adversarial setting. While there exist constant-factor guarantees when the function is submodular, there are no guarantees for non-submodular objectives. In this work, we present a new algorithm Oblivious-Greedy and prove the first constant-factor approximation guarantees for a wider class of non-submodular objectives. The obtained theoretical bounds are the first constant-factor bounds that also hold in the linear regime, i.e. when the number of deletions $\\tau$ is linear in $k$. Our bounds depend on established parameters such as the submodularity ratio and some novel ones such as the inverse curvature. We bound these parameters for two important objectives including support selection and variance reduction. Finally, we numerically demonstrate the robust performance of Oblivious-Greedy for these two objectives on various datasets.", "text": "study problem maximizing monotone function subject cardinality constraint setting number elements deleted returned set. focus work worstcase adversarial setting. exist constant-factor guarantees function submodular guarantees non-submodular objectives. work present algorithm obliviousgreedy prove ﬁrst constant-factor approximation guarantees wider class non-submodular objectives. obtained theoretical bounds ﬁrst constantfactor bounds also hold linear regime i.e. number deletions linear bounds depend established parameters submodularity ratio novel ones inverse curvature. bound parameters important objectives including support selection variance reduction. finally numerically demonstrate robust performance oblivious-greedy objectives various datasets. example important problem machine learning feature selection goal extract subset features informative w.r.t. given task tasks great importance select features exhibit robustness deletions. particularly important domains non-stationary feature distributions input sensor failures another important example optimization unknown function point evaluations require performing costly experiments. experiments fail protecting worst-case failures becomes important. where size subset removed solution objective function exhibits submodularity natural notion diminishing returns constant factor approximation guarantee obtained robust problem however many applications mentioned feature selection problem objective funcmany cases greedy algorithm performs well empirically even objective function deviates submodular. important class objectives γ-weakly submodular functions. simply submodularity ratio quantity characterizes close function submodular. ﬁrst introduced shown functions approximation ratio greedy problem degrades slowly authors obtain approximation guarantee objective submodular greedy algorithm perform arbitrarily badly applied problem submodular version problem ﬁrst introduced krause ﬁrst eﬃcient algorithm constant factor guarantees obtained orlin bogunovic authors introduce pro-greedy algorithm attains .-guarantee allows greater robustness i.e. allowed number removed elements clear obtained guarantees generalize non-submodular functions. submodular setting curvature-dependent constant factor approximation obtained holds number removals. auxiliary function subject cardinality constraint setting variants used various applications example sparse approximation feature selection sparse recovery sparse m-estimation column subset selection problems important result deletion robust submodular maximization streaming setting considered versions robust submodular optimization problems also studied. goal select elements robust worst possible objective given ﬁnite monotone submodular functions. problem diﬀerent types constraints considered studied domain inﬂuence maximization robust version budget allocation problem considered authors study problem maximizing monotone submodular function adversarial noise. conclude section noting recently couple diﬀerent works studied robust submodular problems tion problem wider class monotone non-submodular functions. present algorithm oblivious-greedy prove ﬁrst constant factor approximation guarantees problem function submodular mild conditions recover approximation guarantees obtained previous works parameters characterize function. used previous works e.g. submodularity ratio novel inverse curvature. prove interesting relations parameters obtain theoretical bounds important applications support selection variance reduction objective used batch bayesian optimization. allows obtain ﬁrst robust guarantees important objectives. states weakly submodular submodularity ratio consequently result enlarges number problems greedy comes guarantees. work consider robust version problem goal protect worst-case adversarial deletions features. greedy guarantee. diﬀerent works studied performance greedy algorithm problem objective γ-weakly submodular. analysis going make following important result items constructs sets ﬁrst constructed oblivious selection i.e. ⌈βτ⌉ items individually highest objective values selected. here input parameter together determines size provide information parameter next section. second size obtained running greedy algorithm remaining items finally algorithm outputs size problem weakly submodular objective greedy algorithm achieves constant factor approximation oblivious selection achieves -approximation harder problem greedy fail arbitrarily badly interestingly enough combination algorithms reﬂected oblivious-greedy leads constant factor approximation problem quantity interest section remaining utility adversarial removal elements size returned size chosen mine⊂s|e|≤τ denote optimal solution size goal s)). omitted proofs section found supplementary material. intermediate results. stating main result provide three lower bounds returned denote elements removed i.e. similarly ﬁrst lemma borrowed states least constant fraction utility elements obtained greedily second stage. additionally submodular parameters obtained bound ﬁxed except superadditivity ratio take value approximation factor improves greater i.e. closer function superadditive. hand supermodular approximation factor improves larger smaller observation approximation factor depends instead inverse curvature asymptotic approximation ratio slightly worse compared theorem however depending considered application might signiﬁcantly harder provide bounds inverse curvature bipartite subadditivty ratio hence cases formulation might suitable. main result. obtain main result examining maximum obtained lower bounds lemma note three obtained lower bounds depend lemma beneﬁt small opposite true lemma examining latter observe lemma beneﬁt small opposite true lemma carefully balancing cases arrive main result. section consider important real-world applications deletion robust optimization interest. show parameters used statement main theoretical result explicitly characterized implies obtained guarantees applicable. diﬀerent acquisition functions proposed evaluate utility candidate points next evaluations unknown function recently variance reduction objective used acquisition function unknown function evaluated points maximally reduce variance posterior distribution given points represent potential maximizers. formalize follows. function i.e. gp). {e··· e|s|} denote points r|s|×d denote corresponding data matrix observations respectively. posterior distribution given points observations posterior variance given ﬁrst consider recent results connect submodularity concavity order obtain bounds robust support selection general concave functions make theoretical bounds obtained oblivious-greedy corollary batch bayesian optimization goal optimize unknown non-convex function costly concurrent function evaluations often concurrent evaluations correspond running expensive batch experiments. case experiments fail beneﬁcial select experiments robust way. i.i.d. standard gaussian variance training data points additional points used testing. generate -sparse regression vector selecting random entries compare performance oblivious-greedy robust algorithms against oblivious pro-greedy greedy-type algorithms greedy stochastic-greedy random-greedy orthogonal-matching-pursuit. require asymptotic results hold found results shown fig. since pro-greedy make sense regime relatively small plots show performance feasible values observed oblivious-greedy achieves best performance among methods terms training error test score. also greedy-type algorithms become less robust larger values random greedy adversaries order introduce randomness removal process consider random greedy iteratively selects random element elements whose marginal gains highest terms reducing objective results. figure performance three algorithms shown ﬁxed diﬀerent ﬁgures correspond diﬀerent values. observe greedy outperforms oblivious values oblivious clearly outperforms greedy presented values oblivious-greedy outperforms greedy oblivious selection. larger values correlation points becomes small consequently objective cases three algorithms perform values. similarly. figure show performance three algorithms decreases number removals increases. number removals small greedy algorithm perform similarly number removals increases performance greedy drops rapidly. presented algorithm obliviousgreedy achieves constant-factor approximation guarantees robust maximization monotone non-submodular objectives. theoretical guaranresolves important question posed also obtained ﬁrst robust guarantees support selection variance reduction objectives. various experiments demonstrated robust performance oblivious-greedy showing outperforms oblivious selection greedy hence achieves best worlds. points training additional points testing. label i-th data point /β)) otherwise. results shown fig. observe oblivious-greedy outperforms methods terms achieved objective value generalization error. also note performance greedy decays signiﬁcantly increases. mnist consider -class logistic regression task mnist dataset. experiment oblivious-greedy sample images digit training phase images testing. results shown fig. observed oblivious-greedy distinctive advantage greedy oblivious increases performance greedy decays signiﬁcantly robust oblivious starts outperform autoregressive process function values points generated /-m´atern kernel lengthscale output variance samples function corrupted gaussian noise objective function used variance reduction finally half points randomly chosen selected half used selection process. algorithm. authors would like thank jonathan scarlett slobodan mitrovi´c useful discussions. work done jz’s summer internship lions epfl. vc’s work supported part european research council european union’s horizon research innovation program part swiss national science foundation project part nccr marvel funded swiss national science foundation.", "year": 2018}