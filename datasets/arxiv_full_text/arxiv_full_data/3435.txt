{"title": "CERN: Confidence-Energy Recurrent Network for Group Activity Recognition", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "This work is about recognizing human activities occurring in videos at distinct semantic levels, including individual actions, interactions, and group activities. The recognition is realized using a two-level hierarchy of Long Short-Term Memory (LSTM) networks, forming a feed-forward deep architecture, which can be trained end-to-end. In comparison with existing architectures of LSTMs, we make two key contributions giving the name to our approach as Confidence-Energy Recurrent Network -- CERN. First, instead of using the common softmax layer for prediction, we specify a novel energy layer (EL) for estimating the energy of our predictions. Second, rather than finding the common minimum-energy class assignment, which may be numerically unstable under uncertainty, we specify that the EL additionally computes the p-values of the solutions, and in this way estimates the most confident energy minimum. The evaluation on the Collective Activity and Volleyball datasets demonstrates: (i) advantages of our two contributions relative to the common softmax and energy-minimization formulations and (ii) a superior performance relative to the state-of-the-art approaches.", "text": "figure cern represents two-level hierarchy lstms grounded onto human trajectories lstms predict individual actions {yi} human interactions {yij} event class given video. cern outputs optimal conﬁguration lstm predictions jointly minimizes energy predictions maximizes conﬁdence addressing brittleness cascaded predictions uncertainty. realized extending two-level hierarchy additional energy layer trained end-to-end fashion. extracting deep visual representations predicting individual actions respective human trajectories. outputs bottom lstms forwarded higher-level lstm predicting events. predictions made feed-forward using softmax layer lstm. hierarchy lstms trained end-to-end using backpropagation-through-time cross-entropy loss. motivated success approaches start similar two-level hierarchy lstms recognizing individual actions interactions events. extend hierarchy producing reliable accurate predictions face uncertainty visual input. ideally aforementioned cascade learned overcome uncertainty given domain however empirical evaluation suggests existing benchmark datasets volleyball dataset relatively small robust training work recognizing human activities occurring videos distinct semantic levels including individual actions interactions group activities. recognition realized using two-level hierarchy long shortterm memory networks forming feed-forward deep architecture trained end-to-end. comparison existing architectures lstms make contributions giving name approach conﬁdence-energy recurrent network cern. first instead using common softmax layer prediction specify novel energy layer estimating energy predictions. second rather ﬁnding common minimum-energy class assignment numerically unstable uncertainty specify additionally computes p-values solutions estimates conﬁdent energy minimum. evaluation collective activity volleyball datasets demonstrates advantages contributions relative common softmax energy-minimization formulations superior performance relative state-of-the-art approaches. paper addresses activity recognition videos showing group activity event arising whole number individual actions human interactions goal recognize events interactions individual actions settings training examples classes annotated. ground truth annotations interactions provided training data pursue recognition events actions. recent deep architectures representing multilevel cascade long short-term memory networks shown great promise recognizing video events. approaches lstms bottom layer grounded onto individual human trajectories initially obtained tracking. lstms aimed lstms cascade. hence cases seen training data observe feedforwarding predictions typically brittle errors made bottom level directly propagated higher level. address challenge augment training set. practical collecting annotating group activities usually difﬁcult. ﬁrst contribution aimed mitigating brittleness direct cascading predictions previous work. specify energy function capturing dependencies lstm predictions within cern enable recognition energy minimization. speciﬁcally extend aforementioned two-layer hierarchy lstms additional energy layer estimating energy predictions. replaces common softmax layer output lstms. importantly extension allows robust energy-based end-to-end training layer lstms cern. second contribution aimed improving numerical stability cern’s predictions perturbations input resolving ambiguous cases multiple similar-valued local minima. instead directly minimizing energy consider reliable solutions illustrated fig. reliability conﬁdence solutions formalized using classical tool statistical hypothesis test namely p-values corresponding lstm’s hypotheses thus seek conﬁdent solutions regularizing energy minimization constraints p-values. effectively amounts joint maximization conﬁdence minimization energy cern outputs. therefore specify estimate minimum energy certain conﬁdence constraints rather energy. also energy regularized p-values robust deep learning. speciﬁcally formulate energy-based loss accounts energy also pvalues cern predictions training data. evaluation collective activity volleyball datasets demonstrates advantages contributions compared common softmax energy-based formulations superior performance relative state-of-the-art methods. following sec. reviews prior work sec. speciﬁes cern sec. formulate energy conﬁdence sec. describes energy layer sec. speciﬁes learning ﬁnally sec. presents results. figure imaginary illustration solution space circle represents candidate solution. colors sizes circles indicate energy conﬁdence computed energy layer cern. candidate solution minimum energy seems numerically unstable small perturbations input. joint maximization conﬁdence minimization energy gives different conﬁdent solution conﬁdence speciﬁed terms p-values energy potentials. formulate energy-based loss end-to-end learning cern. loss accounts energy p-values. group activity recognition. group activity recognition often requires explicit representation spatiotemporal structures group activities deﬁned terms individual actions pairwise interactions. previous work typically used graphical models and-or grammar models learn structures grounded handcrafted features. recent methods learn graphical model typically using recurrent neural networks also work group activity recognition demonstrated many advantages using deep architectures rnns mentioned nondeep approaches. approach extends work replacing rnn’s softmax layer energy layer specifying energy-based model takes account p-values network’s predictions. energy-based learning. energy-based formulations inference learning common non-deep group activity recognition seldom used deep architectures. recently approaches tried learn energy-based model using deep neural networks demonstrated energy-based objectives great potential improving performance structured predictions especially training data limited. approach extends work regularizing energy-based objective additionally accounts conﬁdence predictions. figure specify evaluate versions cern. cern deep architecture lstms grounded cnns video frames bottom. lstms forward class predictions energy layer top. cern- lstms bottom level compute distributions individual action classes distributions interaction classes cern- additional lstm computing distribution event classes. takes lstm outputs infers energy minimum maximum conﬁdence. ﬁgure shows cern- cern- give different results group activity crossing. cern- wrongly predicts walking. cern- typically yields better results group activities deﬁned individual actions. proach additionally estimates regularizes inference p-values. p-values speciﬁed within framework conformal prediction allows selection reliable numerically stable predictions. teraction classes lstm outputs forwarded energy layer cern computing energy finally cern outputs structured prediction whose energy high conﬁdence shown fig. specify evaluate versions cern. cern- uses lstms predicting individual actions interactions whereas event class predicted cern- additional event lstm takes features maxpooled outputs node edge lstms computes distribution event classes cern- takes three types class distributions input specifically }i∈v {ψe}∈e predicts optimal class assignment following specify p-val. recognizing events interactions individual actions deep architecture lstms called cern shown fig. cern similar deep networks presented viewed graph nodes corresponding individual human trajectories edges corresponding pairs human trajectories. human trajectories extracted using offthe-shelf tracker also {··· denotes event class union individual action classes human interaction classes {yij associated nodes edges. cern assign lstm every node edge node lstms share weights edge lstms also weights. lstms convolutional neural networks compute deep features corresponding human trajectories output softmax distributions individual action classes softmax distributions human indicator denotes human trajectories training videos ground truth labels belonging ground truth event class lstm prediction reliable i.e. high p-value many training examples class larger nonconformity measures. better understand relationship nonconformity measure p-value consider simple case illustrated fig. ﬁgure plots distributions nonconformity measures action classes training examples suppose observe instance whose softmax output indicates action class higher probability true label i.e. curves however softmax output likely wrong. because fig. p-values since majority training examples class label larger nonconformity measures hence class conﬁdent solution. denotes pairs human trajectories training videos ground truth labels belonging ground truth event class lstm prediction high p-value many training examples larger nonconformity measures. figure simple illustration relationship nonconformity measure individual actions p-value ratio dashed region whole area curve indicates p-value. clearly given instance action class larger softmax output action class higher conﬁdence. training videos showing event cyij parameters denotes softmax output corresponding node lstm denotes softmax output corresponding edge lstm denote visual cues extracted respective human trajectories several well-studied ways deﬁne pvalues paper follow framework conformal prediction conformal prediction uses nonconformity measure estimate extent prediction different system’s predictions made training. hence provides formalism estimate conﬁdence predictions based past experience training data. below deﬁne nonconformity measure used compute p-values lstms’ predictions individual actions interactions events. nonconformity measure p-values figure takes softmax outputs lstms along estimated p-values input outputs solution jointly minimizes energy maximizes p-value fisher’s combined hypothesis test. deﬁne statistical signiﬁcance hypothesis among hypotheses need combine p-values predictions assigned nodes edges event rigorously specifying p-value compound statistical test p-val consisting multiple hypotheses follow fisher’s combined hypothesis test fisher’s theory states independent hypothesis tests whose p-values p··· characterized test statistic proved follow probastatistic bility distribution degrees freedom. follows minimization statistic yield maximum p-value characterizing fisher’s combined hypothesis test. extend deep architecture lstms additional energy layer aimed jointly minimizing energy given maximizing p-value fisher’s combined hypothesis test given cern- optimization problem expressed parameters impose lowerbound constraints p-values. recall according fisher’s theory combined hypothesis test decreasing constraint parameters enforce higher p-values solution. argminyc=ci violated case. alternatively loss replaced energy-based loss functions also considered treat latent variables simplicity thus consider accuracy however include comparison corresponding ground truth label loss function. usually difﬁcult violated case. however points inference violated case require global minimum solution since normalization term modeled energy-based model simply output node edge lstms. practice ﬁrst train network using common losses cross-entropy learn representation excluding namely input layer softmax layers. p-value training instance computed removing training sets finally train weights minimizing loss. implementation details. stack node lstms edge lstms vgg- model without layer. vgg- pre-trained imagenet ﬁne-tuned lstms jointly. train layer cern ﬁxing weights cnns bottom layer lstms. batch size joint training bottom lstms vgg- training converges within iterations. event lstm trained using iterations batch size mini-batch gradient descent rmsprop learning rate ranging keras theano backend implement cern training testing single nvidia titan gpu. fair comparison tracker implementation speciﬁcally tracker dlib library cropped image sequences persons pairs persons used inputs node lstms edge lstms respectively. compare approach state-of-the-art methods addition evaluate following reasonable baselines. baselines -layer lstms test network -layer lstms similar baselines full models compute potentials p-values. energy layer cern- p-values baseline represents cern- network however pvalues computed used regularizing energy minimization. hence event class prediction comes standard energy minimization. cern- p-values similar estimate p-values cern-. collective activity dataset consists videos annotated activity categories individual action labels pairwise interaction labels interaction labels provided extended annotation dataset ﬁrst train node lstms edge lstms time steps nodes. then concatenate outputs types lstms bottom layer cern along vgg- features pass concatenation bidirectional event lstm nodes time steps layer cern. concatenation passed pooling layer fully-connected layer output dimension comparison baselines following performance metrics multi-class classiﬁcation accuracy mean per-class accuracy split training testing sets tab. summarizes performance methods recognizing group activities. note tab. deep neural nets. seen energy layer signiﬁcantly boosts accuracy outperforming state-of-the-art large margin. even bottom layer lstms cern- still outperforms -layer lstms thanks without baseline yields lower accuracy even additional lstms interactions. accuracies recognizing individual actions interactions collective activity dataset using node lstms edge lstms respectively. note cern- cern- share node edge lstms. evaluating numerical stability predicting group activity classes cern- corrupt human trajectories testing data control amount corruption corruption probability. instance corfigure performance decrease group activity recognition varying percentage corruption human trajectories collective activity dataset. compare -layer lstms cern p-values cern- using corrupted trajectories input. ruption probability corrupt bounding person every video frame chance. bounding selected randomly shift horizontal vertical displacement ranging original bounding box’s width height respectively. fig. shows cern- consistently experiences lower degradation performance compared baselines without p-values. indicates incorporating p-values energy model indeed beneﬁts inference stability. beneﬁt becomes signiﬁcant amount corruption input data increases. fig. shows example crossing activity. seen although cern- share individual action labels majority people assigned incorrect action labels cern- still correctly recognize activity. volleyball dataset consists videos annotated frames. actions labels waiting setting digging failing spiking blocking jumping moving standing; group activity classes include right right spike right pass right winpoint left winpoint left pass left spike left set. interactions annotated dataset recognize interactions remove edge lstms. figure decrease group activity recognition accuracy different input distortion percentages volleyball dataset cern- compared -layer lstms cern- p-values event lstm cern- bidirectional lstm nodes time steps. pooling types pooling output node lstms dividing players groups ﬁrst pooling group separately. test types pooling approach rule effect pooling type comparison. cern- pooling layer thus categorized group style. recognition accuracy individual actions using node lstms accuracies recognizing group activities summarized tab. cleary regularized energy minimization increases accuracy compared conventional energy minimization cern- outperforms state-of-the-art using either pooling types. cern- achieve accuracy comparable cern- volleyball dataset. mainly cern- reasons group activity based individual actions provide sufﬁcient information recognizing complex group activities sports videos. cern- overcomes problem adding event lstm. figure qualitative results collective activity dataset. left right show inference results cern- ground truth labels respectively. colors bounding boxes indicate individual action labels interaction labels shown simplicity. figure qualitative results volleyball dataset results results cern- ground truth labels colors bounding boxes indicate individual action labels numbers frame ids. qualitative results right pass activity depicted fig. demonstrates advantage inference based regularized energy compared softmax output deep recurrent networks action predictions accurate. addressed problem recognizing group activities human interactions individual actions novel deep architecture called conﬁdence-energy recurrent network cern extends existing two-level hierarchy lstms additionally incorporating conﬁdence measure energy-based model toward improving reliability numerical stability inference. inference formulated joint minimization energy maximization conﬁdence measure predictions made lstms. realized differentiable energy layer computes energy regularized p-value fisher’s combined statistical test. deﬁned energy-based loss terms regularized energy learning end-to-end. cern evaluated collective activity dataset volleyball dataset. comparison previous approaches predict group activities feed-forward manner using deep recurrent networks cern gives superior performance also gives numerically stable solutions uncertainty. collective activities simpler variant cern- gives accurate predictions strong baseline representing two-level hierarchy lstms softmax outputs taken predictions. variant cern- increases complexity yields better accuracy challenging group activities merely individual actions complex whole.", "year": 2017}