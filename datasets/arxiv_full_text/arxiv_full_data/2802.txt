{"title": "Faster Algorithms for Large-scale Machine Learning using Simple Sampling  Techniques", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Now a days, the major challenge in machine learning is the `Big~Data' challenge. The big data problems due to large number of data points or large number of features in each data point, or both, the training of models have become very slow. The training time has two major components: Time to access the data and time to process (learn from) the data. In this paper, we have proposed one possible solution to handle the big data problems in machine learning. The idea is to reduce the training time through reducing data access time by proposing systematic sampling and cyclic/sequential sampling to select mini-batches from the dataset. To prove the effectiveness of proposed sampling techniques, we have used Empirical Risk Minimization, which is commonly used machine learning problem, for strongly convex and smooth case. The problem has been solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each using two step determination techniques, namely, constant step size and backtracking line search method. Theoretical results prove the same convergence for systematic sampling, cyclic sampling and the widely used random sampling technique, in expectation. Experimental results with bench marked datasets prove the efficacy of the proposed sampling techniques.", "text": "slow training models. days major challenge develop efﬁcient scalable learning algorithms dealing data problems training time models major components training time time access data time process data. known processing data running program ﬁrst brought memory precisely hard disk. time taken brining data program hard disk memory called access time. access time further three components namely seek time rotational latency time transfer time moreover data read content wise rather block-wise block multiple sectors. interesting note data stored contiguous memory close proximity lesser data access time compared data dispersed away other. lesser seek latency transfer times. case seek latency times moving parts based direct access mechanism transfer time still plays role. since data read/written block-wise contiguous data access would transfer times dispersed data access would require number transfer times. moreover cache memory strategies also favor contiguous memory access make faster compared dispersed data access. thus contiguous data access time faster dispersed data access cases data stored hdd. difference access time would prominent hdd. second component training time processing time time taken process data solve model parameters. mini-batching iterative learning algorithms accessing processing data intermixed occurs quite frequently. data days major challenge machine learning ‘big data’ challenge. data problems large number data points large number features data point both training models become slow. training time major components time access data time process data. paper proposed possible solution handle data problems machine learning. idea reduce training time reducing data access time proposing systematic sampling cyclic/sequential sampling select mini-batches dataset. prove effectiveness proposed sampling techniques used empirical risk minimization commonly used machine learning problem strongly convex smooth case. problem solved using saga svrg saag-ii mbsgd using step determination techniques namely constant step size backtracking line search method. theoretical results prove convergence systematic sampling cyclic sampling widely used random sampling technique expectation. experimental results bench marked datasets prove efﬁcacy proposed sampling techniques. technological developments availability number data sources evolved data ‘big data’ meaning ‘big’ ‘big data’ continuously changing increasing size datasets. data multiple aspects like volume velocity variety veracity etc. aspect namely volume i.e. size datasets posed challenge machine learners train models large datasets. problems called large-scale data problems large number data points large number features data point both lead access time dependent sampling technique processing time dependent method used solve problem. till date improve training time focus given improve processing time using different methods. training time dependent data accessing time data processing time noted that generally data access time high compared data processing time. thus efforts equally done improve data accessing time. paper tried reduce training time data problems reducing data access time using systematic sampling technique cyclic sampling select mini-batches data points dataset. selected mini-batch data points. iteration complexity solving reduced subproblem |bj| size mini-batch independent since easier solve problem widely used approach handle large-scale problems paper uses reduced subproblem approach. data challenge major challenge machine learning dealing data problems stochastic approximation approaches widely used take data points form reduced subproblem iteration. helps reducing computational complexity iteration thereby helps solving large-scale problems. stochastic noise like method stochastic approximation affects solution problem. stochastic noise reduced using different techniques like mini-batching decreasing step sizes importance sampling variance reduction techniques discussed selecting data point mini-batch data points dataset random sampling widely used sampling technique importance sampling adaptive sampling importance sampling non-uniform sampling technique uses properties data ﬁnding probabilities selection data points iterative process leading acceleration training process. importance sampling involves overhead calculating probabilities average computational complexity empirical risk minimization problem commonly used machine learning statistics typically consists loss function paper l-regularized regularizing term. problem assumptions strong convexity smoothness used demonstrating effectiveness proposed sampling techniques. training data model loss functions parameter vector regularization coefﬁcient l-regularized problem given paper experimentation uses logistic loss function. problem represented large-scale data problem iteration complexity learning algorithms traditional methods like gradient descent high. because problems large number data points large number features both iteration updates variables data points. high computational complexity iteration would expensive even infeasible single machine process single iteration learning algorithm. stochastic approximation approach widely used handling cases uses data point mini-batch data points iteration importance sampling could computational complexity iteration cases like implemented dynamically. stratiﬁed sampling technique divides dataset clusters similar data points mini-batch data points adaptive sampling information classes i.e. data-labels used selecting data points; technique gives good results problems large number classes. random sampling widely used mini-batching large-scale learning problems known importance sampling technique mini-batching. paper focused simple sampling techniques involve extra overhead effective dealing large-scale learning problems. simple sampling techniques namely systematic sampling cyclic/sequential sampling techniques proposed selecting mini-batches. best knowledge systematic sampling used machine learning selecting data points. ﬁrst introduce systematic sampling machine learning selection mini-batches data points dataset. this cyclic sampling used coordinate descent block coordinate descent methods selecting coordinate block coordinates respectively best knowledge cyclic sampling used selecting mini-batches. sampling techniques simple effective easy implement. proposed sampling techniques reduce training time models reducing data access time. this reducing training time models focus mainly given reducing processing time paper focused reducing data access time changing sampling techniques. novel systematic sampling sequential/cyclic sampling techniques proposed selecting mini-batches data points dataset solving large-scale learning problems. proposed techniques focus reducing training time learning algorithms reducing data access time. experimental results prove efﬁcacy systematic sampling cyclic sampling reducing training time. results proved using different methods using step determination techniques namely constant step size backtracking line search methods. sampling selecting mini-batch data points whole dataset. iterative approximation methods used solving problem sampling iteration/epoch. convergence learning algorithm depends upon type sampling used since sampling controls things data access time diversity data. general consecutive data points used reduce data access time reduces training time selected data points might diverse affects convergence learning algorithm. hand data used different locations data access time increases training time selected data points might diverse improve convergence. thus sampling signiﬁcant role learning algorithms. three sampling techniques namely random sampling cyclic sampling systematic sampling discussed used learning algorithm simple easy implement involve extra overhead thus effective handling large-scale problems. random sampling random sampling types replacement without replacement. replacement ﬁrst selects data point randomly whole dataset data point equal probability selection then similarly second data point selected randomly whole dataset previously selected point equal probability selection select data points. without replacement ﬁrst selects data point randomly whole dataset data point equal probability selection then similarly second data point selected randomly remaining points without considering previously selected point select data points. cyclic/sequential sampling first mini-batch selected taking ﬁrst points. second minibatch selected taking next points data points covered. start ﬁrst data point. madow madow randomly selects ﬁrst point selects remaining points according ﬁxed pattern e.g. randomly selects data point selects data points mini-batch positive integer. simplicity taken. given example .... size mini-batch selected four mini-batches selected/drawn using different sampling techniques follows mini-batches selected using random sampling replacement mini-batches selected using random sampling without replacement mini-batches selected using cyclic sampling mini-batches selected using systematic sampling clear examples random sampling replacement points selected randomly repetition inside mini-batch within mini-batches. cyclic sampling simplest non-probabilistic sampling selects points sequential manner. systematic sampling ﬁrst point selected randomly remaining points selected back back idea replacement without replacement applied between mini-batches sampling without replacement within mini-batches demonstrated. interesting note data points minibatch dispersed different sectors disk every data point needs seek time latency time. since data read block-wise content-wise possible data point present different block thus needs transfer time also. seek time needed mini-batch starts ﬁrst data point moves till seek time needed mini-batch ﬁrst element determined randomly rest points mini-batch stored contiguous memory locations. seek time least transfer time almost equal less compared generally data point needs separate transfer time case needs many transfer times number blocks required mini-batch data points. thus overall access time access mini-batch data points minimum maximum observed gives best solution compared given number epochs access time most. hand access time least given number epoch convergence slowest. trade-off reducing data access time convergence learning algorithm. balances trade-off since best techniques like data points stored contiguous memory locations like randomness draws ﬁrst point randomly. overall methods using converges faster compared methods using discussed sec. general learning algorithm systematic sampling solve large-scale problems given algorithm similar learning algorithms obtained cyclic random sampling techniques using corresponding sampling technique selecting mini-batch step algorithm algorithm starts initial solution. divides dataset mini-batches using systematic sampling selecting mini-batches. inside inner loop takes mini-batch formulates subproblem solves sub-problem thus formed. process repeated sub-problems mini-batches solved. solution process repeated given number epochs stopping criteria used algorithm. step algorithm different solvers used update solution like saga saag-ii mbsgd solvers discussed section convergence proof learning algorithms provided using simplest solver mbsgd method constant step size simplicity proofs focus study sampling techniques solvers. l-regularized problem solved following assumptions demonstrate efﬁcacy proposed sampling techniques. assumed regularization term hidden inside loss function term notational convenience otherwise needs write separate gradients regularization term. theorem suppose function given under assumptions constant step size taking solver mbsgd algorithm converges linearly expectation cyclic systematic sequential sampling techniques. experiments performed using methods namely saga saag-ii svrg mbsgd bench marked datasets presented table method mini-batches size data points techniques step size namely constant step size method backtracking line search method predeﬁned number epochs. constant step size method uses lipschitz constant takes step size methods. backtracking line search performed approximately using selected mini-batch data points performing backtracking line search whole dataset could hurt convergence learning algorithm large-scale problems taking huge time. dataset method runs times dataset three sampling techniques compared different thus settings used compare results three sampling techniques. training time depends conﬁguration machine experiments performed noted experiments conducted macbook sampling technique i.e. random sampling cyclic sampling systematic sampling algorithmic structure used difference selecting mini-batch sampling technique. sampling technique dataset divided predeﬁned number mini-batches algorithm simplicity dataset divided equal sized minibatches except last mini-batch might data points less equal mini-batches. epoch array size equal number data points dataset taken array contains randomized indexes data points. select minibatches array contents equal mini-batch size till array selected sequentially. epoch array size equal number data points dataset taken containing indexes data points sorted order. select mini-batches array contents equal mini-batch size till array selected sequentially. epoch array size equal number mini-batches taken array contains randomized indexes mini-batches. select mini-batch array element selected sequence. array element gives ﬁrst index data point selected mini-batch. data points selected sequentially starting index mini-batch equal size mini-batch till last data point dataset. experimental results plot difference objective function optimum value training time three sampling techniques namely random sampling cyclic/sequential sampling systematic sampling represented figs.–. save space results different samplings constant step size backtracking line search methods plotted ﬁgure clear ﬁgures different settings dataset compare prove methods converges faster general gives best results intuition sometimes produces better results since quite similar. results constant step size backtracking line search methods show similar results. larger datasets like susy higgs show clear advantage smaller dataset thus prove efﬁcacy/suitability large-scale learning problems. results like higgs dataset method learning algorithms sampling techniques converge quickly value careful examination reveals converges earlier please look supplementary material tabular representation experiments represents couple tables comparing training time objective function value given number epochs. paper novel systematic sampling cyclic sampling techniques proposed solving largescale learning problems improving training time reducing data access time. methods convergence expectation random cyclic systematic sampling techniques cyclic systematic sampling training times faster widely used random sampling technique expense fractionally small difference minimized objective function value given number epochs. thus systematic sampling technique suitable solving large-scale problem accuracy solution. random shufﬂing data used data learning algorithms systematic cyclic sampling improve results cases similar data points grouped together. sampling techniques extended parallel distributed learning algorithms.", "year": 2018}