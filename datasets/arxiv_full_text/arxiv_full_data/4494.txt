{"title": "Adaptive Online Prediction by Following the Perturbed Leader", "tag": ["cs.AI", "cs.LG", "I.2.6; G.3"], "abstract": "When applying aggregating strategies to Prediction with Expert Advice, the learning rate must be adaptively tuned. The natural choice of sqrt(complexity/current loss) renders the analysis of Weighted Majority derivatives quite complicated. In particular, for arbitrary weights there have been no results proven so far. The analysis of the alternative \"Follow the Perturbed Leader\" (FPL) algorithm from Kalai & Vempala (2003) (based on Hannan's algorithm) is easier. We derive loss bounds for adaptive learning rate and both finite expert classes with uniform weights and countable expert classes with arbitrary weights. For the former setup, our loss bounds match the best known results so far, while for the latter our results are new.", "text": "tives quite complicated. particular arbitrary weights results proven far. analysis alternative follow perturbed leader algorithm kalai vempala based hannan’s algorithm easier. derive loss bounds adaptive learning rate ﬁnite expert classes uniform weights countable expert classes arbitrary weights. former setup loss bounds match best known results latter results new. prediction expert advice follow perturbed leader general weights adaptive learning rate adaptive adversary hierarchy experts expected high probability bounds general alphabet loss online sequential prediction. introduction setup notation ifpl bounded best expert hindsight feasible bounded infeasible combination bounds choices hierarchy experts lower bound adaptive adversary miscellaneous discussion open problems prediction expert advice considers ensemble sequential predictors master algorithm constructed based historical performance predictors. goal master algorithm perform nearly well best expert class sequence outcomes. achieved making predictions close better experts. theory rapidly developed recent past. starting weighted majority algorithm aggregating strategy vast variety diﬀerent algorithms variants published. parameter algorithms learning rate. parameter ﬁxed early algorithms established so-called doubling trick make learning rate coarsely adaptive. little later incrementally adaptive algorithms developed others. section compare results works detail. unfortunately loss bound proofs incrementally adaptive variants quite complex technical despite typically simple elegant proofs static learning rate. complex growing proof techniques also another consequence original algorithm assertions proven countable classes experts arbitrary weights modern variants usually restrict ﬁnite classes uniform weights discussion section). might suﬃcient many practical purposes prevents application general classes predictors. examples extrapolating data points help polynomial degree d=... –or– class computable predictors. furthermore authors concentrated predicting binary sequences often losses less common. nevertheless easy abstract completely predictions consider resulting losses only. instead predicting according weighted majority time step chooses single expert diﬀerent general approach achieve similar results follow perturbed leader principle dates back early called hannan’s algorithm kalai vempala published simpler proof main result hannan also succeeded improve bound modifying distribution perturbation. resulting algorithm performance guarantees wm-type algorithms ﬁxed learning expert hindsight) shown countable expert classes arbitrary weights adaptive learning rate arbitrary losses. regarding adaptive learning rate obtain proofs simpler elegant corresponding algorithms. further prove ﬁrst loss bounds usually refers online worst case setting experts deliver sequential predictions time range ...t given. time know actual predictions past losses. goal give prediction overall loss steps much worse best expert’s loss sequence outcomes. prediction deterministic adversary could choose sequence provokes maximal loss. randomize predictions. consequently prediction strategy expected loss sequence small. paper structured follows. section give basic deﬁnitions. consider general online decision problems ﬁnite-dimensional spaces focus online prediction tasks based countable number experts. like exploit infeasible predictor analysis. sections derive main analysis tools. section generalize upper bound ifpl arbitrary weights. main diﬃculty faced appropriately distribute weights various terms. corresponding lower bound open problem. section exploit restricted setup signiﬁcantly improve allowing bounds logarithmic rather linear number experts. upper lower bounds ifpl combined derive various regret bounds section bounds static dynamic learning rate terms sequence length follow straight-forwardly. proof main bound terms loss much elegant analysis previous comparable results. section proposes novel hierarchical procedure improve bounds non-uniform weights. section lower bound established. section consider case independent randomization seriously. particular show derived bounds also hold adaptive adversary. section treats additional issues including bounds high probability computational aspects deterministic predictors absolute loss. finally section discuss results compare references state open problems. sequential predictions yt∈y times .... time step access predictions ≤i≤n experts {e...en} size expert pool ∪{∞}. convenient notation ﬁnite countably inﬁnite expert pool. made prediction make observation xt∈x loss revealed expert’s prediction. preﬁx code words instance +lni complexity deﬁnes weight means e−ki vice versa. following talk complexities rather weights. ﬁnite usually sets case uniform complexities/weights. experts countably inﬁnite uniform complexities possible. vector complexities denoted ≤i≤n. time expert suﬀers loss t)≤i≤n vector losses time +...+st− total past loss vector smin loss best expert hindsight usually know advance time forward identiﬁed space unit vectors ={ei since decision consists selecting single expert states identiﬁed space. main focus i.e. selecting expert performed best past predict according expert approach fails reasons. first minimum solve ﬁrst problem penalizing expert complexity i.e. predicting according expert approach solves second problem adding expert’s loss random perturbation. choose perturbation negative exponentially distributed either independent time step beginning time former choice preferable order protect adaptive adversary generates order bounds high probability main analysis however latter choice convenient. linearity expectations possibilities equivalent dealing expected losses henceforth assume without loss generality initial perturbation depends learning rate give choices section established main tools analysis. expected loss time ◦st]. idea analysis intermediate predictor ifpl ifpl predicts according thus knowledge ◦st] denote expected loss ifpl time losses ifpl upper-bounded section lower-bounded section note deﬁnition algorithm deviates uses exponentially distributed perturbation similar fpl∗ one-sided non-stationary learning rate like hannan’s algorithm. notes. observe stated algorithm regardless actual predictions experts possible observations losses relevant. note also expert implement highly complicated strategy depending past outcomes despite trivializing identiﬁcation constant unit vector. complex expert’s behavior summarized hidden state vector =loss≤i≤n. results therefore apply arbitrary prediction observation spaces arbitrary bounded loss functions. contrast major part work developed binary alphabet absolute loss only. finally note setup allows losses generated adversary tries maximize regret knows algorithm experts’ past predictions/losses. adversary also access fpl’s past decisions must independent randomization time step order achieve good regret bounds. second last term even negative hence small last term small case function q∈irn random perturbation continuous function mean variance small variance large intermediate variance makes smin mini{si s+...+st−= state/loss summary argmind∈d{d ◦s}= best decision total time=step current time=step. penalization complexity expert irn= random vector independent exponentially distributed components. argmini∈e{si loss several pages next result establishes similar bound instead using expected value best loss smin used. computational advantages since smin immediately available needs evaluated logarithmic estimate second third bound unnecessarily rough convenience only. therefore coeﬃcient log-term ﬁnal bound theorem reduced without much eﬀort. plugging estimates back yields case complexity longer square root. although already implies hannan consistency i.e. average round regret tends zero desirable likely hold. able derive improved bounds modiﬁcation. consider two-level hierarchy experts. first consider subclass experts complexity regard fplk experts form fpl. class meta experts contains complexity expert allows derive good bounds. following quantities referring complexity class regard fplk meta experts deﬁne meta state fplk meta expert suﬀers loss would also work.) hence setting expert setting deﬁne +lnk sums normal state initial versus independent randomization. assumed perturbations sampled time already indicated expectation equivalent generating perturbation time step i.e. theorems remain valid case. former choice favorable analysis latter advantages. first repeated sampling perturbations guarantees better bounds high probability second losses generated adaptive adversary access fpl’s past decisions time ﬁgure initial random perturbation force large loss. show bounds remain valid even case adaptive adversary independent randomization used. oblivious versus adaptive adversary. recall protocol expert made prediction combined form predic observe loss revealed fpl’s expert’s tion yfpl prediction. independent randomization yfpl oblivious adversary recursively inserting eliminating experts strong statement holds full observation game i.e. time step learn losses. partial observation games bandit case actual action depend past action means case past observation assertion longer holds. adaptive adversary analyzed shown finally yifpl additionally depend reduced dependencies hence ifpl bounds also hold adaptive adversary. randomizing independently described previous section actual loss expected loss before. advantage independent randomization much better high-probability bound. exploit chernoﬀ-hoeﬀding bound need compute expectations explicitly. given need compute order update actual note prediction fpl. +k/ηt following representation compute integral. inﬁnite last approximated dominant contributions. alternatively modify algorithm considering ﬁnite pool experts time step; next paragraph. expectation also approximated sampling several times. finitized expert pool. case inﬁnite expert class compute minimum inﬁnite time step directly feasible. possibility address choose experts ﬁnite pool time step. case algorithm also discussed obtain behavior introducing entering time expert. expert considered bounds leads theorem corollary additional additional ﬁnal bounds since must regret best expert hindsight already entered game best expert hindsight all. selecting implies bounds entering times similar ones derived here. details proofs construction found deterministic prediction absolute loss. another second last paragraph following decision space make deterministic decision d=wt∈∆ time bounds holding sure instead selecting probability |xt−yi observation predictions master algorithm predicting chosen deterministic prediction space loss-function loss convex. xtyi absolute loss |xt−pt| master deterministically predicting actually coincides pt-expected loss master predicting probability hence regret bound absolute loss also implies regret loss. sense price pays easy analysis adaptive learning rate. evidence diﬀerence really exists proof artifact. special loss functions bounds sometimes improved e.g. leading constant static case loss structure algorithm however questionable corresponding bounds hold there. general weights. several dynamic bounds uniform weights previous result non-uniform weights know gives dynamic bound ℓgentile norm algorithm absolute loss. comparable bound rapidly seems analysis experts algorithms including weighted majority variants gets complicated general weights together adaptive learning rate choice learning rate must account weight best expert loss. quantities known advance diﬀerent impact learning rate increasing current loss estimate always decreases optimal learning rate expert higher complexity would larger. hand analyses known prediction. {νi} class probability distributions sequences assume true sequence sampled µ∈{νi} complexity known bayes optimal predictor based e−kνi -weighted mixture νi’s expected total loss √lµkµ while hedge sample expert without knowing prediction need know experts’ predictions. note also many loss-functions like quadratic loss ﬁnite regret achieved thm.] using obtained bound except leading order constant sequence independently assumption generated another indication bound leading constant could hold. detailed comparison bayes bounds bounds.", "year": 2005}