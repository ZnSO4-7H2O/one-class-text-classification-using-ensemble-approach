{"title": "Modelling Data Dispersion Degree in Automatic Robust Estimation for  Multivariate Gaussian Mixture Models with an Application to Noisy Speech  Processing", "tag": ["cs.CL", "cs.LG", "stat.ML"], "abstract": "The trimming scheme with a prefixed cutoff portion is known as a method of improving the robustness of statistical models such as multivariate Gaussian mixture models (MG- MMs) in small scale tests by alleviating the impacts of outliers. However, when this method is applied to real- world data, such as noisy speech processing, it is hard to know the optimal cut-off portion to remove the outliers and sometimes removes useful data samples as well. In this paper, we propose a new method based on measuring the dispersion degree (DD) of the training data to avoid this problem, so as to realise automatic robust estimation for MGMMs. The DD model is studied by using two different measures. For each one, we theoretically prove that the DD of the data samples in a context of MGMMs approximately obeys a specific (chi or chi-square) distribution. The proposed method is evaluated on a real-world application with a moderately-sized speaker recognition task. Experiments show that the proposed method can significantly improve the robustness of the conventional training method of GMMs for speaker recognition.", "text": "dalei haiqing department electrical computer engineering concordia university maisonneuve blvd. west montreal canada *daleiwugmail.com; haiqingwugmail.com standard training method gmms maximum likelihood estimation expectation maximisation algorithm though proved effective method still lacks robustness training process. instance robust gross outliers cannot compensate impacts out-liers contained training corpus. well known outliers often widely exist training population clean data often either contaminated noise interfered objects claimed data. out-liers training population distract parameters trained models inappropriate locations therefore break models result poor perform-ance recognition system. order solve problem partial trimming scheme introduced cuesta improve robustness statistical models k-means gmms. method prefixed proportion data samples removed training corpus rest data used model training. method found robust gross outliers applied small scale data examples. however method used real-world appli-cations noisy acoustic signal processing hard know optimal cutoff proportion used since know faction data taken away overall training population. bigger smaller removal proportion would result either removing much useful information effect remov-ing outliers. attack issue propose method using dispersion degree model trimming scheme prefixed cutoff portion known method improving robustness statistical models multivariate gaussian mixture models small scale tests alleviating impacts outliers. however method applied realworld data noisy speech processing hard know optimal cut-off portion remove outliers sometimes removes useful data samples well. paper propose method based measuring dispersion degree training data avoid problem realise automatic robust estimation mgmms. model studied using different measures. theoretically prove data samples context mgmms approximately obeys specific distribution. proposed method evaluated real-world application task. experiments proposed method significantly improve robustness conventional training method gmms speaker recognition. statistical models gaussian mixture models hidden markov models many signal processing domains include instance acoustical noise reduction image recognition speech/speaker recognition etc. paper study robust modelling issue regarding gmms. issue important since gmms often used fundamental components build complicated models hmms. thus method contributions proposed method threefold first suggest trimmed k-means algorithm conventional k-means approach initialise parameters gmms. showed paper appropriate initial values model parameters crucial robust training gmms. second propose dispersion degree training data samples selection criterion automatic outlier removal. theoretically prove dispersion degree approximately obeys certain distribution depending measure uses. refer method automatic robust estimation trimming scheme gaussian mixture models hereafter. third evaluate proposed method realworld application moderately-sized speaker recognition task. experimental results show proposed method significantly improve robustness conventional training algorithm gmms making robust gross outliers. rest paper organised follows section present framework proposed are-trim training algorithm gmms. section present trimmed k-means clustering algorithm compare conventional k-means. section propose dispersion degree model based distance metrics armtrim. carry experiments evaluate aretrim section finally conclude paper findings section proposed are-trim algorithm essentially includes several modifications conventional training method gmms. conventional training algorithm norm-ally consists steps k-means clustering algorithm model parameter initialisation algorithm parameter fine training illustrated fig. well known appropriate model parameters crucially affect final performance gmms inapp-ropriate initial values could cause fine training second step trapped local optimum often globally optimal. therefore interesting study robust clustering algorithms help train better model parameters. are-trim focused modifying first stage conventional training algorithm i.e. model parameter initialisation algorithm keeping fine training step untouched. modification consists steps substituting conventional k-means clustering algorithm robust trimmed k-means clustering algorithm. section shall give details concerning reason clustering algorithm robust conventional kmeans clustering algorithm. applying dispersion degree model identify outliers automatically trim training population. procedure extra step required govern monitor training process overall procedure automatic robust therefore referred automatic robust estimation overall are-trim procedure illustrated fig. next section shall explain trimmed k-means clustering algorithm robust conventional k-means clustering method present theorems properties related trimmed kmeans clustering algorithm. first step are-trim replaces conventional k-means clustering algorithm trimmed kmeans clustering algorithm initialise model gmms. good property parameters trimmed k-means clustering robustness algorithm important robustness are-trim. hence section shall present properties trimmed k-means clustering algorithm well clarify robust gross outliers conventional kmeans clustering algorithm. trimmed k-means clustering extended version conventional k-means clustering algorithm using impartial trimming scheme remove part data training population. based concept trimmed sets. first define given training data trimmed defined subset trimming percent data full data terms parameter clustering algorithm specifically defined follows clusters objects partitioned based criter-ion minimising intra-class covariance i.e. existence consistency trimmed k-means algorithm proved cuesta-albertos -dimensional states that given random vari-able space trimming factor continuous nondecreasing metric function exists trimmed k-means furthermore also proved dimensional random variable space probability measure sequence em-pirical trimmed k-means converges surely probability unique trimmed k-means robustness trimmed k-means algorithm theoretically identified using three methods influence function breakdown point qualita-tive robustness results surely show trimmed k-means theoretically robust conventional kmeans. next shall overview results briefly. metric measure robustness statistical estimator providing rich quantitative implementation minimum value intra-class covariance obtained many methods checking change intra-class covariance successive iterations less given threshold. theory robust statistics laid solid theoretic foundations robustness trimmed kmeans turns provide strong supports advantages proposed are-trim conventional training algorithm. robustness trimmed k-means illustrated fig. fig. clusters assumed represent groups data data clustered around clusters fact outlier point referred breakdown point i.e. case classic k-means surely breaks existence generated. therefore clusters however trimmed k-means algorithm break appropriate trimm-ing still value right clusters able sought. illustrates robustness trimmed k-means. limit exists. definition derivative provides measure model good properties derived if’s esimator expected extend measure uses smallest fraction corrupted observations needed break estimator xtnε besides measures qualitative robust-ness another method measure robustness estimator. proposed hampel defined equicontinuity condition i.e. given real distribution sequence continu-ous estimators summarised follows k-means bounded bounded penalty clustering procedure whereas bounded trimmed k-means wider range functions practically vanishes outside cluster. previous work fixed fraction data trimmed full training order realise training trimmed k-means. however real applications e.g. acoustic noise reduction speaker recognition tasks fixed cut-off data strategy able successfully trim essential outliers test data often fixed number outliers. paper propose issue solved using model data dispersion degree. given data random variable represent dispersion degree data terms multi-class prove certain distance metric random variable approximately ob-eys certain distribution size data approaches infinity i.e. +∞→t this first theorem respect application euclidean distance below. denote multi-class wh-ere represents gaussian compo-nent elements data sample assigned optimal class bayesian rule i.e. selecting maximum conditional probability data point k-th gaussian model let’s also assume random variable represent dispersion degree data terms multiclass includes dispersion degree sample data derived based continuous euclidean distance defined i.e. start definition data dispersion degree simplest case i.e.the dispersion degree data point terms cluster. cluster mean vector inverse covariance matrix given data point dispersion degree data point deviates cluster defined certain distance concept dispersion degree data point terms cluster easily generalised multi-class dis-persion degree cx|| data point terms multi-class defined dispersion degree data point terms optimal class data point belongs sense bayesian decision rule i.e. according essential training e.g. assumed training data point modelled normal distribution component thus naturally sense bayesian decision rule. thus i.e. remarks theorem shows dispersion degree data terms euclidean distance approxi-mately obeys distribution. however degrees freedom large distribution approxi-mated normal result es-pecially useful distribution real applications easy computation large. following theorem holds theorems dispersion degree data terms multi-class therefore modelled either distribution chi-square distribution depending distance measure applied theoretical results. practice normal distribution often used instead fast computation convenient manipulation approximate chi-square distribution especially number degrees freedom distribution distribution known approximated using theorem distribution following result. clarifying dispersion degree essentially obeys certain distribution apply model automatic outlier removal are-trim using following definition definition given dispersion degree model data threshold data point identified outlier threshold conditional cumulative probability dispersion degree data point conditioned dispersion degree model larger threshold i.e. definition proper value selected threshold outliers automatically identified thus removed proposed aretrim training scheme. detailed algorithm formulated follows including main parts i.e. estimation identification pro-cesses remarks theory notice theorem reply using bayesian decision rule partition process. well known pattern recognition domain bayesian decision rule optional classifier sense minimising decision risk squared form hence quality recog-nition process able satisfy requirements pattern recognition applications. next section shall carry experiments show procedure effective. previous sections discussed aretrim algorithm theoretical viewpoint. section shall show effectiveness applying real signal processing application. proposed training approach aims improving robustness classic gmms adopting automatic trimmed k-means training techniques. thus theoretically speaking application using gmms speaker /speech recognition acoustical noise reduction used evaluate effectiveness proposed method. without loss genera-lisation select moderately-sized speaker recognition task evaluation widely accepted effective method speaker recognition. mfcc features obtained using used windows shift pre-emphasis factor hamming window scaled feature bands. mfcc coefficients used except database neither cepstral mean subtraction time difference features increase perform-ance used. apart these extra processing measure employed. also gmms gaussians trained model speaker tasks. gaussians diagonal covariance matrices well-known diagonal converiance matrices produce similar results full converiance matrices also standard method based algorithm used train gmms efficiency wide application speech processing. trimmed k-means step random selection points used initialise centroids clusters. first present experiment testing dispersion degree model using euclidean distance. expe-riment different values threshold according definition used trim outliers existing training data. represents outlier pruned means maximum number whereas outliers identified trimmed off. proposed method tested development fig. that proposed method improve system performance development threshold accuracy are-trim threshold values develop-ment test higher conventional training method. shows effectiveness proposed method. values threshold small; otherwise remove much meaningful points actually outliers. result unpredictable system performance showed fig. give averaged proportions trimmed samples corresponding value threshold averaged\" proportions obtained across number training speakers number iterations. iteration training trimmed k-means applied using trimmed threshold figure averaged trimming proportions increase values trimming threshold move away practice suggest range used select optimal value reasonable probability range identifying outliers. range valid data highly possibly trimmed off. also partly shown fig. roughly corresponding data removed. trend performance changes development test shows similar increase peak values obtained outlier removal improvements development test set. outlier re-moval system performance varies development test set. implies outliers training data effectively removed robust models obtained. however data removed taking values beyond useful data removed well. hence system performance demonstrated threshold-varying characteristic depending part data removed. therefore suggest practice used suitable value threshold selected. experiment choose optimum based development set. euclidean distance model dispersion degree take consideration covariance data cluster consider distances centroid. however covariance cluster quite different largely affect distribution data dispersion degrees. thus experiment evaluate mahalanobis distance automatic trimming measure are-trim. fig. development test trimmed training scheme significantly improve robustness system performance. development trimming factor accuracy test improved shows automatic trimming mahalanobis distance applied. finally proposed training scheme evaluated realistic application medium-size speaker identification task. experiments showed proposed method significantly improve robustness gaussian mixture models. next shall theorem derive distrieq. used proof theorem bution simplicity presentation drop larger obtained using euclidean distance suggest mahalanobis distance better metric model dispersion degrees consideration covariance factor modelling. paper proposed automatic trimming estimation algorithm conventional gaussian mixture models. trimming scheme consists several novel contributions improve robustness gaussian mixture model training effectively removing outlier interference. first modified lloyd’s algorithm proposed realise trimmed k-means clustering algorithm used parameter initialisation gaussian mixture models. secondly data dispersion degree proposed used automatically identifying outliers. thirdly theore-tically proved data dispersion degree context training approximately obeys certain distribution terms either eu-clidean fisher doddingtion goudie-marshall darpa speech recognition research database proceedings darpa specifications status\" workshop speech recognition february tang zhang chi-square distribution non-negative approximation definite quadratic forms non-central normal variables\" comput-ational statistics data analysis morris koreman trained classify small number speakers generates discriminative features improved speaker recognition\" proceedings ieee int. carnahan conference security technology reynolds zissman quatieri leary telephone carlson effect transmission degradations speaker recognition performance\" proceed-ings international conference acoustics speech signal processing jiang normalisation transformation techniques speaker recognition\" speech recognition techniques technology appli-cations kordic springer press dalei wuharbin computer m.sc. engineering university degree science computer tsinghua university china ph.d. degree computational linguistics saarland university saarbruecken saar-land germany worked post-doctoral researcher department computer science engineering york university. since working postdoctoral researcher department electrical com-puter engineering concordia university. research interest focuses automatic speech recognition automatic speaker recognition machine learning algorithms speech enhance-ment.", "year": 2014}