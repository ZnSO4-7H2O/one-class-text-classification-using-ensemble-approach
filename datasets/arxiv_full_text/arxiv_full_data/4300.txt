{"title": "BSVM: A Banded Suport Vector Machine", "tag": ["stat.ML", "cs.CV"], "abstract": "We describe a novel binary classification technique called Banded SVM (B-SVM). In the standard C-SVM formulation of Cortes et al. (1995), the decision rule is encouraged to lie in the interval [1, \\infty]. The new B-SVM objective function contains a penalty term that encourages the decision rule to lie in a user specified range [\\rho_1, \\rho_2]. In addition to the standard set of support vectors (SVs) near the class boundaries, B-SVM results in a second set of SVs in the interior of each class.", "text": "standard c-svm like penalty function penalizes b-svm replaces constant c-svm. novel b-svm penalty function. function penalizes total penalty function b-svm. total penalty choosing impose milder penalty values figure shows classiﬁcation obtained example data using c-svm bsvm. blue points correspond class respectively. cyan indicated orange. yellow squares correspond support points cyan squares correspond support points green squares correspond support points sparsity solution controlled case c-svm figure shows decision rule c-svm b-svm note bsvm second penalty term values interval heat decision rule c-svm heat decision rule b-svm. c-svm values decision rule unbalanced class central cluster located class gets much smaller values c-svm rest class b-svm however clusters class including centered similar values. result second penalty term b-svm objective function. figure shows fraction points classiﬁed correctly c-svm b-svm function decision rule threshold. x-axis shows decision rule threshold percentage maximum absolute value decision function training points. y-axis shows overall classiﬁcation accuracy sensitivity c-svm b-svm. b-svm objective function contains penalty term encourages decision rule user speciﬁed range addition standard support vectors near class boundaries b-svm results second interior class. scalars functions denoted non-bold font vectors vector functions denoted bold font using lower case letters matrices denoted bold font using upper case letters transpose matrix motivate development b-svm following way. suppose vector comes arbitrary probability distribution mean ﬁnite co-variance consider linear decision rule easy mean covariance cov] chebyshev’s inequality exists high probability band around expected comes hence every probability distribution vectors class class ﬁnite cosince non-linear decision rules c-svm simply linear decision rules operating high dimensional space kernel trick b-svm band formation argument holds non-linear decision rules well. vectors data labels first consider linear case afterwards transform general case kernel trick. vector scalar parameters linear decision rule separating class derive b-svm dual problem order maximize lower bound b-svm primal objective function equation dual problem simpler solve compared primal form proceed follows figure standard c-svm like penalty function penalizes b-svm replaces constant c-svm. novel b-svm penalty function. function penalizes total penalty function b-svm. total penalty choosing impose milder penalty values next solve primal variables terms dual variables minimizing respect primal variables. since lagrangian convex function primal variables unique global minimum obtained using ﬁrst order karush kuhn tucker conditions given non-linear vector function takes inputs high dimensional space. recover kernel b-svm linear b-svm data-label pairs instead original pairs practice need explicitly products kernel matrix elements calculation requires solution concave maximization problem elements chosen using suitable kernel accomplished using sequential minimal optimization type active technique projected conjugate gradient technique indicated cyan class membership indicated orange. yellow squares correspond support points cyan squares correspond support points green squares correspond support points figure figure shows decision rule c-svm b-svm note b-svm second penalty term interval heat decision rule c-svm heat decision rule b-svm. c-svm values decision rule unbalanced class central cluster located class gets much smaller values c-svm rest class b-svm however clusters class including centered similar values. result second penalty term b-svm objective function. figure figure shows fraction points classiﬁed correctly c-svm b-svm function decision rule threshold. x-axis shows decision rule threshold percentage maximum absolute value decision function training points. y-axis shows overall classiﬁcation accuracy sensitivity c-svm b-svm. figures show decision rule values training points c-svm b-svm. recall c-svm enforce upper limit whereas b-svm attempts encourage seen figure b-svm successful limiting absolute value figures show heat decision rule c-svm b-svm respectively evaluated grid containing training points. seen that work considered binary classiﬁcation problem feature vectors individual classes ﬁnite co-variance. showed b-svm natural generalization c-svm situation. turns b-svm dual maximization problem retains concavity property c-svm counterpart c-svm turns special case b-svm types arise b-svm α-svs similar standard c-svm θ-svs arise novel b-svm objective function penalty b-svm decision rule balanced c-svm decision rule since assigns values summary b-svm used enforce balanced decision rules binary classiﬁcation. anticipated c-svm leave error bounds bias free case given jaakkola haussler continue hold similar form bias free b-svm well.", "year": 2011}