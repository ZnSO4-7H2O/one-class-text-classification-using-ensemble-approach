{"title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region  Embeddings", "tag": ["stat.ML", "cs.CL", "cs.LG"], "abstract": "One-hot CNN (convolutional neural network) has been shown to be effective for text categorization (Johnson & Zhang, 2015). We view it as a special case of a general framework which jointly trains a linear model with a non-linear feature generator consisting of `text region embedding + pooling'. Under this framework, we explore a more sophisticated region embedding method using Long Short-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly large) sizes, whereas the region size needs to be fixed in a CNN. We seek effective and efficient use of LSTM for this purpose in the supervised and semi-supervised settings. The best results were obtained by combining region embeddings in the form of LSTM and convolution layers trained on unlabeled data. The results indicate that on this task, embeddings of text regions, which can convey complex concepts, are more useful than embeddings of single words in isolation. We report performances exceeding the previous best results on four benchmark datasets.", "text": "one-hot shown effective text categorization view special case general framework jointly trains linear model non-linear feature generator consisting ‘text region embedding pooling’. framework explore sophisticated region embedding method using long short-term memory lstm embed text regions variable sizes whereas region size needs ﬁxed cnn. seek effective efﬁcient lstm purpose supervised semi-supervised settings. best results obtained combining region embeddings form lstm convolution layers trained unlabeled data. results indicate task embeddings text regions convey complex concepts useful embeddings single words isolation. report performances exceeding previous best results four benchmark datasets. text categorization task assigning labels documents written natural language numerous real-world applications including sentiment analysis well traditional topic assignment tasks. state-of-the methods text categorization long linear predictors either bag-ofword bag-of-n-gram vectors input e.g. this however changed recently. non-linear methods make effective word order shown produce accurate predictors traditional bow-based linear models e.g. particular ﬁrst focus one-hot proposed convolutional neural network feedforward neural network convolution layers interleaved pooling layers originally developed image processing. convolution layer small region data every location converted low-dimensional vector information relevant task preserved loosely term ‘embedding’. embedding function shared among locations useful features detected irrespective locations. simplest form onehot works follows. document represented sequence one-hot vectors convolution layer converts small regions document low-dimensional vectors every location pooling layer aggregates region embedding results document vector taking componentwise maximum average; layer classiﬁes document vector linear model onehot semi-supervised extension shown superior number previous methods. work consider general framework jointly trains feature generator linear model feature generator consists ‘region embedding pooling’. speciﬁc region embedding function one-hot takes simple form considering simplicity method works surprisingly well region size appropriately set. however also potential shortcomings. region size must ﬁxed optimal size relevant regions vary. practically region size cannot large number parameters learned depends proposed variations alleviate issues. example bow-input variation allows vector region. enables larger region expense losing word order region limited. work build general framework ‘region embedding pooling’ explore sophisticated region embedding long short-term memory seeking overcome shortcomings above supervised semi-supervised settings. lstm recurrent neural network. typical applications text lstm takes words sequence one; i.e. time takes input t-th word output time therefore output time step regarded embedding sequence words seen designed enable learning dependencies larger time lags feasible traditional recurrent networks. lstm used embed text regions variable sizes. pursue best lstm purpose compare resulting model previous best methods including one-hot previous lstm. strategy simplify model much possible including elimination word embedding layer routinely used produce input lstm. ﬁndings threefold. first supervised setting simpliﬁcation strategy leads higher accuracy faster training previous lstm. second accuracy improved training lstms unlabeled data learning useful region embeddings using produce additional input. third lstm models one-hot strongly outperform methods including previous lstm. best results obtained combining types region embeddings trained unlabeled data indicating strengths complementary. overall results show text categorization embeddings text regions convey higher-level concepts single words isolation useful useful region embeddings learned without going word embedding learning. report performances exceeding previous best results four benchmark datasets. code experimental details available http//riejohnson.com/cnn download.html. text lstm used labeling generating words. also used representing short sentences mostly sentiment analysis rely syntactic parse trees; e.g. unlike studies work well focuses classifying general full-length documents without special linguistic knowledge. similarly applied lstm categorizing general full-length documents. therefore empirical comparisons focus reported state results. ﬁrst introduce general lstm formulation brieﬂy describe dl’s model illustrates challenges using lstms task. lower layer time step would example size vocabulary input one-hot vector representing word dimensionality word vector lower layer word embedding layer. lstm units dimensionality weight matrices bias vectors need trained rq×d rq×q types centerpiece lstm memory cells designed counteract risk vanishing/exploding gradients thus enabling learning dependencies larger time lags feasible traditional recurrent networks. forget gate resetting memory cells. input gate output gate control input output memory cells. word-vector lstm dl’s application lstm text categorization straightforward. illustrated figure document output lstm layer output last time step represents whole document like many studies lstm text words ﬁrst converted low-dimensional dense word vectors word embedding layer; therefore call word-vector lstm wv-lstm. observed wv-lstm underperformed linear predictors training unstable. attributed fact documents long. addition found training testing wv-lstm time/resource consuming. perspective using epoch wv-lstm training takes nearly times longer one-hot training even though achieves poorer accuracy sequential nature lstm i.e. computation time requires output time whereas modern computation depends parallelization speedup. documents mini-batch processed parallel variability document lengths reduces degree parallelization. shown training becomes stable accuracy improves drastically lstm word embedding layer jointly pre-trained either language model learning objective autoencoder objective suggested making mini-batch consist sequences similar lengths found tasks strategy slows convergence presumably hampering stochastic nature sgd. section focuses end-to-end supervised setting additional data additional algorithm general strategy simplify model much possible. start elimination word embedding layer one-hot vectors directly lstm call one-hot lstm short. facts word embedding linear operation written one-hot vector columns word vectors. therefore replacing lstm weights removing word embedding layer word-vector lstm turned one-hot lstm without changing model behavior. thus word-vector lstm expressive one-hot lstm; rather merit training word embedding layer would imposing restrictions achieve good prior/regularization effects. end-to-end supervised setting word embedding matrix would need initialized randomly trained part model. preliminary experiments under framework unable improve accuracy one-hot lstm inclusion randomly initialized word embedding layer; i.e. random vectors failed provide good prior effects. instead demerits evident meta-parameters tune poor accuracy lowdimensional word vectors slow training/testing high-dimensional word vectors dense. word embedding appropriately pre-trained unlabeled data inclusion form semi-supervised learning could useful. show later however type approach falls behind approach learning region embeddings training one-hot lstm unlabeled data. altogether elimination word embedding layer found useful; thus base work one-hot lstm. pooling simplifying sub-problems framework ‘region embedding pooling’ simpliﬁcation effect follows. wv-lstm sub-problem lstm needs solve represent entire document vector make easy changing detecting regions text relevant task representing vectors illustrated figure lstm layer emit vectors time step pooling aggregate document vector. wv-lstm lstm remember relevant information gets document even relevant information observed words away. task lstm easier allowed forget things forget gate focus representing concepts conveyed smaller segments phrases sentences. related architecture appears deep learning tutorials though uses word embedding. another related work combined pooling non-lstm recurrent networks word embedding. chopping speeding training addition simplifying sub-problem pooling merit enabling faster training chopping. since goal lstm embedding text regions instead documents longer crucial document beginning sequentially. time training chop document segments ﬁxed length sufﬁciently long process segments mini batch parallel segments individual documents. perform testing without chopping. train lstm approximations sequences speed test real sequences better accuracy. risk chopping important phrases easily avoided segments slightly overlap. however found gains overlapping segments tend small experiments reported done without overlapping. removing input/output gates found lstm followed pooling presence input output gates typically improve accuracy removing nearly halves time memory required training testing. intuitive particular pooling make output gate unnecessary; role output gate prevent undesirable information entering output irrelevant information ﬁltered max-pooling. without input output gates lstm formulation simpliﬁed table training time error rates lstms elec. chop chopping size. time seconds epoch training tesla error classiﬁcation error rates test data. wv-lstmp word-vector lstm pooling. ohlstmp one-hot lstm pooling. oh-lstmp one-hot bidirectional lstm pooling. memory required training make practical layer lstm going opposite direction accuracy improvement. shown figure concatenate output forward lstm backward lstm referred bidirectional lstm literature. resulting model one-hot bidirectional lstm pooling abbreviate oh-lstmp. table shows much accuracy and/or training speed improved elimination word embedding layer pooling chopping removing input/output gates adding backward lstm. data converted lower-case letters. neural network experiments vocabulary reduced frequent words training data reduce computational burden; square loss minimized dropout applied input layer; weights initialized gaussian distribution average/maximum length table data. avg/max documents training/test data. imdb elec sentiment classiﬁcation movie reviews amazon electronics product reviews respectively. topic categorization reuters news articles newsgroup messages respectively. hyper parameters learning rates chosen based performance development data held-out portion training data training redone using training data chosen parameters. used pooling method used parameterizes number pooling regions pooling done non-overlapping regions equal size resulting vectors concatenated make vector document. pooling settings chosen based performance development data max-pooling imdb elec average-pooling rcv; max-pooling chosen. table error rates supervised results without pretraining. oh-cnn results respectively; wv-lstm results imdb others experimental results work. table shows error rates obtained without additional unlabeled data pre-training sort. meaningful comparison table shows neural networks comparable dimensionality embeddings onehot convolution layer feature maps bidirectional lstms units each. words convolution layer produces -dimensional vector location lstm direction emits -dimensional vector time step. exception wv-lstm equipped lstm units word embedding layer dimensions; states without pre-training addition lstm units broke training. complex larger one-hot reviewed later. review non-lstm baseline methods. last table shows best one-hot results within constraints above. obtained bow-cnn vector region) region size seq-cnn region size others. table three four datasets oh-lstmp outperforms cnn. however underperforms both. conjecture strict word order useful rcv. point also observed performances. n-gram better bag-of-word bow-cnn outperforms seq-cnn. bags words window every location useful words strict order. presumably former easily cover variability expressions indicative topics. thus lstm ability words bags loses bow-cnn. one-hot one-hot lstm lstm embed regions variable sizes whereas requires region size ﬁxed. attribute fact small improvements oh-lstmp oh-cnn table however shortcoming alleviated multiple convolution layers distinct region sizes. show table one-hot cnns layers different region sizes rival oh-lstmp. although models larger table training/testing still faster lstm models simplicity region embeddings. comparison strength lstm embed larger regions appears contributor here. amount training data sufﬁcient enough learn relevance longer word sequences. overall one-hot works consider following views words already seen document next words task tv-embedding learning predict view- based view-. train one-hot lstms directions figure unlabeled data. purpose input output gates well forget gate found useful. theory tv-embedding says region embeddings obtained useful task interest views related concepts relevant task. reduce undesirable relations views syntactic relations performed vocabulary control remove function words vocabulary target view found useful also lstm. output tv-embedding indexed time step tv-embeddings contains lstms going forward backward figure although possible ﬁne-tune tv-embeddings labeled data simplicity faster training ﬁxed experiments. easy expanded tv-embeddings form lstm also tv-embeddings form convolution layers obtained jzb. similarly possible lstm tv-embeddings produce additional input cnn. lstm tv-embeddings tv-embeddings region embeddings formulations different other; therefore expect complement bring performance improvements combined. empirically conﬁrm conjectures experiments below. note able naturally combine several tv-embeddings strength comparison previous best results previous best performance obtained pre-training wv-lstm units labeled training data. oh-lstmp achieved better. previous best results datasets unlabeled data review semi-supervised results. exploit unlabeled data additional resource non-linear extension two-view feature learning whose linear version appeared earlier work used learn unlabeled data region embedding embodied convolution layer. work learn region embedding embodied one-hot lstm. start brief review non-linear two-view feature learning. rough sketch follows. consider views input. embedding called tv-embedding embedded view good original view purpose predicting view. views labels related another hidden states tv-embedded view good original view purpose classiﬁcation. embedding useful provided dimensionality much lower original view. applied idea regarding text regions embedded convolution layer view surrounding context view training tv-embedding unlabeled data. obtained tv-embeddings used produce additional input supervised region embedding one-hot resulting higher accuracy. used imdb elec semi-supervised experiments; excluded absence standard unlabeled data. table summarizes unlabeled data. experiment lstm tv-embeddings trained lstms units unlabeled data. training objective predict next words others. similar minimized weighted square goes time steps represents next words vector model output; achieve negative sampling effect speed-up; vocabulary control performed reducing undesirable relations views sets vocabulary target frequent words excluding function words details followed supervised experiments. semi-supervised one-hot bidirectional lstm pooling table used lstm tv-embeddings trained unlabeled data described above one-hot lstms directions compared supervised oh-lstmp clear performance improvements obtained datasets thus conﬁrming effectiveness approach. review semi-supervised performance wv-lstms model consisted word embedding layer dimensions lstm layer units hidden units lstm layer; word embedding layer lstm pre-trained unlabeled data ﬁne-tuned labeled data; pre-training used either language model objective autoencoder objective. error rate imdb elec best effort perform pre-training language model objective. used conﬁguration elec however classes hidden units turned changed although preprevious studies lstm text often convert words pre-trained word vectors wordvec popular choice purpose. therefore tested wv-lstmp whose difference oh-lstmp input lstm layers pre-trained word vectors. word vectors optionally updated training. types word vectors tested. google news word vectors trained wordvec huge news corpus provided publicly. tasks wv-lstmp using google news vectors performed relatively poorly. wordvec trained domain unlabeled data better results observed scaled word vectors appropriately still underperformed models region tv-embeddings used domain unlabeled data. attribute superiority models tv-embeddings fact learn unlabeled data embeddings text regions convey higher-level concepts single words isolation. review performance one-hot -dim tv-embedding comparable lstm -dim lstm tv-embeddings terms dimensionality tv-embeddings. lstm rivals outperforms imdb/elec underperforms rcv. increasing dimensionality lstm tvembeddings obtain still reach cnn. discussed earlier attribute superiority one-hot unique representing parts documents input. table comparison previous best results. error rates unlabeled data used? co-tr. optimized co-training using oh-cnn base learner parameters optimized test data; demonstrates difﬁculty exploiting unlabeled data tasks. setting oh-lstmp takes additional input embeddings lstm tv-embeddings used table three tv-embeddings obtained three distinct combinations training objectives input representations publicly provided. tv-embeddings trained applied text regions size every location taking input imdb/elec rcv. connect tv-embeddings lstm aligning centers regions former lstm time steps; e.g. tv-embedding result ﬁrst words passed lstm time step third word. second setting trained one-hot types tv-embeddings replacing output rows table show results types models. comparison also show results lstm lstm tv-embeddings tv-embeddings effects combination compare compare row. example adding tv-embeddings lstm error rate imdb improved adding lstm tv-embeddings error rate improved results indicate that expected lstm tv-embeddings tv-embeddings complement improve performance combined. previous best results literature shown table results previous semi-supervised models found clearly underperform semi-supervised one-hot table best supervised results imdb/elec ﬁrst obtained integrating document embedding layer one-hot cnn. many previous results imdb found except bi-gram nbsvm paragraph vectors considered large improvements. shown last table model improved also elec best models exceeded previous best results. within general framework ‘region embedding pooling’ text categorization explored region embeddings one-hot lstm. region embedding onehot lstm rivaled outperformed state-of-the one-hot proving effectiveness. also found models either types region embedding strongly outperformed methods including previous lstm. best results obtained combining types region embedding trained unlabeled data suggesting strengths complementary. result reported substantial improvements previous best results benchmark datasets. high level results indicate following. first task embeddings text regions convey higher-level concepts useful embeddings single words isolation. second useful region embeddings learned working one-hot vectors directly either labeled data unlabeled data. finally promising future direction might seek framework region embedding methods complementary beneﬁts. kyunghyun merri¨enboer bart gulcehre caglar bahdanau dzmitry bougares fethi schwenk holger bengio yoshua. learning phrase representations using encoder-decoder statistical machine translation. proceedings emnlp hinton geoffrey srivastava nitish krizhevsky alex sutskever ilya salakhutdinov ruslan improving neural networks preventing co-adaptation feature detectors. arxiv. tieleman tijman hinton geoffrey. lecture .rmsprop divide gradient running average recent magnitude. coursera neural networks machine learning phong zuidema willem. compositional distributional semantics long short-term memory. proceedings fourth joint conference lexical computational semantics", "year": 2016}