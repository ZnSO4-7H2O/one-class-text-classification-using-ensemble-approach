{"title": "Unsupervised Basis Function Adaptation for Reinforcement Learning", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "When using reinforcement learning (RL) algorithms to evaluate a policy it is common, given a large state space, to introduce some form of approximation architecture for the value function (VF). The exact form of this architecture can have a significant effect on the accuracy of the VF estimate, however, and determining a suitable approximation architecture can often be a highly complex task. Consequently there is a large amount of interest in the potential for allowing RL algorithms to adaptively generate (i.e. to learn) approximation architectures.  We investigate a method of adapting approximation architectures which uses feedback regarding the frequency with which an agent has visited certain states to guide which areas of the state space to approximate with greater detail. We introduce an algorithm based upon this idea which adapts a state aggregation approximation architecture on-line.  Assuming $S$ states, we demonstrate theoretically that - provided the following relatively non-restrictive assumptions are satisfied: (a) the number of cells $X$ in the state aggregation architecture is of order $\\sqrt{S}\\ln{S}\\log_2{S}$ or greater, (b) the policy and transition function are close to deterministic, and (c) the prior for the transition function is uniformly distributed - our algorithm can guarantee, assuming we use an appropriate scoring function to measure VF error, error which is arbitrarily close to zero as $S$ becomes large. It is able to do this despite having only $O(X\\log_2{S})$ space complexity (and negligible time complexity). We conclude by generating a set of empirical results which support the theoretical results.", "text": "abstract using reinforcement learning algorithms evaluate policy common given large state space introduce form approximation architecture value function exact form architecture signiﬁcant eﬀect accuracy estimate however determining suitable approximation architecture often highly complex task. consequently large amount interest potential allowing algorithms adaptively generate approximation architectures. investigate method adapting approximation architectures uses feedback regarding frequency agent visited certain states guide areas state space approximate greater detail. introduce algorithm based upon idea adapts state aggregation approximation architecture on-line. assuming states demonstrate theoretically provided following relatively non-restrictive assumptions satisﬁed number cells state aggregation architecture order greater policy transition function close deterministic prior transition function uniformly distributed algorithm guarantee assuming appropriate scoring function measure error error arbitrarily close zero becomes large. able despite space complexity conclude generating empirical results support theoretical results. commonly used reinforcement learning algorithms store estimate what’s known value function corresponds particular policy mapping state-action pair real value reﬂects amount reward agent obtain starting state-action pair following policy question order algorithm perform well important estimate accurate possible since estimate governs algorithm update policy. traditional algorithms q-learning generate exact estimates dealing small state action spaces. however environments complex applying algorithms directly becomes computationally demanding. result common introduce form architecture approximate example parametrised functions issue introducing approximation however accuracy algorithm’s estimate highly dependent upon exact form architecture chosen accordingly number authors explored possibility allowing approximation architecture learned agent rather pre-set manually designer bu¸soniu overview. might hope achieve employing approach create algorithm still relatively computational demands time increased ﬂexibility allowing apply algorithm wider problems without needing invest time suitably adapt case. assume approximation architecture adapted linear methods known basis function adaptation. simple perhaps under-explored method basis function adaptation involves using estimate frequency agent visited certain states determine states accurately represent. methods unsupervised sense direct reference reward estimate made. concept using visit frequencies unsupervised manner completely however remains relatively unexplored compared methods seek measure error estimate explicitly error feedback demonstrate however unsupervised methods distinct advantages estimates visit frequencies cheap calculate store accurate estimates visit frequencies generated fewer samples accurate estimates example temporal diﬀerences suitable conditions applying appropriate scoring function methods fact generate accurate estimates guaranteeing certain cases scores arbitrarily close zero. overarching objective article closely examine unsupervised methods seek quantify possible advantages. point perhaps surprising forms substance article’s main result. ﬁxed policy stationary distribution describing probability state. suppose policy transition function given problem close deterministic prior transition function uniformly distributed show that conditions agent follows arbitrary policy will average tend spend almost time small subset state space. indeed state space size theoretical upper bound average size subset provided enough basis functions individually represent states subset unsupervised basis function adaptation methods ensure arbitrarily well estimated subset. scoring function apply weighted probability visiting state guarantee arbitrarily score. implication that circumstances unsupervised methods perform least well complex methods compared methods cheaply whilst conditions encompass many important general problems also explore potential generalise condition encompass larger possible priors. also explore ideas experimentally. experimental results suggest techniques provide powerful advantage many real world settings. realistic parameter settings techniques reduce error amount range cases experimental results also suggest assumptions required theory relaxed. noted above unsupervised techniques present relatively unexplored. menache provided brief evaluation unsupervised algorithm setting policy evaluation. berstein shimkin examined algorithm kernels progressively split based visit frequency kernel. algorithm includes policy updates incorporate knowledge uncertainty estimate. algorithm propose works conjunction state aggregation approximation architecture employing form cell-splitting give state space regions less resolution. bears similarities number approaches examined literature moore atkeson provide early algorithm based updating state aggregation architecture whilst whiteson examined basis function construction method involving cell splitting. paper actually found unsupervised method performed unfavourably compared alternative approaches proposed. however environment used test algorithm satisfy stated assumptions. remainder section outline formal framework using. section details algorithm performs unsupervised basis function adaptation based state aggregation. section outline main theoretical results. finally section empirical results designed support extend results section always assume agent’s policy ﬁxed reward function mapping state-action pair real number agent state takes action receive reward reward function assumed assuming able store explicit real value state-action pair traditional algorithms provide means estimating estimates converge correct value becomes large cases large though impossible store real values. hence dealing cases common employ approximation. approximate however even underlying algorithm still converges generate estimate longer rely estimate arbitrarily close true value form approximation parametrised value function approximation involves generating approximation using parametrised functions. goal algorithm becomes value parameters estimate near true value possible. approximate denoted assuming approximating state space action space value parametrised matrix dimension approximation architecture linear expressed form column ﬁxed vector dimension pair distinct vectors dimension given called basis functions. common assume case distinct basis functions want design algorithm adapt approximation architecture need means assess well this. scoring function used assess accuracy estimate many basis function adaptation algorithms scoring function form feedback help guide basis functions updated. cases important score something measured computationally. vector probability state given stationary distribution associated note true appears value however unknown. therefore another commonly used scoring function uses bellman operator obtain approximation mse. scoring function denote ﬁrst that whilst applying weighting appears natural scoring function necessarily need weighted circumstances appropriate measure accuracy estimate would example weight every state equally acknowledge limitation analysis section second that investigating unsupervised basis function adaptation methods implicitly making comparison methods basis function adaptation explicit scores source feedback algorithm uses scoring function feedback best evaluated terms well minimises particular scoring function. scoring function attractive choice provide feedback supervised methods since weighting error probability visiting state possible generate estimate score without knowing probability visiting state. fact feedback based scoring function weighted would implicitly require normalising score state. turn would require estimate implies distinct values need recorded. hence unsupervised methods perform comparatively well terms minimising probability weighted scoring functions great importance comparing methods supervised alternatives. noted introduction circumstances exist agent will average tend spend almost time relatively small subset state space. examine circumstances closely section underlying idea pasa make representation detailed possible relatively small subset split cell largest value recalculate cells continue fashion cells. could rerun splitting process every iteration prefer discrete intervals. provided large small states resulting cells tend detailed representation areas state space high stationary probability. progressively split base cells. estimate probability visiting resulting cells subtracting estimates another. consequence become apparent pasa converges point algorithm described paragraph above whilst requiring space complexity. trade take longer amount time converge although practical terms diﬀerence would appear marginal. setting pasa works detail worthwhile highlighting aspect algorithm. note many methods examined literature involve termed basis function construction basis functions determined initial process once-and-forall occurs prior agent beginning function normal examples include munos moore whiteson methods work example progressively adding basis functions criteria satisﬁed. pasa algorithm falls alternative class methods termed basis function optimisation assumption methods ﬁxed number basis functions progressively updated throughout whole period agent functions. approach advantage ﬂexible basis functions adapt policy changes indeed changes environment. approximately size. using deﬁne partition splitting cell original partition. leave half cell index give half index taking partition create partition splitting cell. continuing fashion partition containing cells need additional mechanisms allow temporary copy made call also store dimensional boolean vector entry zero start sequence. keeps track whether particular cell state don’t want process outlined algorithm note algorithm calls procedure split cells. procedure simply updates given latest value also calls convert procedure converts mapping mapping procedures computationally straightforward. diagram illustrating main steps figure pasa requires modest increase computational resources compared ﬁxed state aggregation. relation time complexity updated parallel algorithm’s update vector updated intervals practice large allows time converge. mapping state cell order time complexity algorithm using pasa compared minimum equally sized cells. hence pasa involves eﬀectively increase time complexity. pasa involve additional space complexity respect storing vector must store real values. also store overall space complexity becomes although must also stored ﬁxed state aggregation. component space complexity introduction pasa regarding sampling eﬃciency points made. ﬁrst that since visit frequencies depend individual action reward subsequent trajectory estimated quickly much quickly than example temporal diﬀerences. second point arises compare pasa methods based explicitly estimating error pasa converged estimate needs converge once. contrast estimate used update basis functions value must principle generated time update basis functions. could serious consequences time required process converge take long time generate accurate estimate particular basis functions. particularly near large. moreover special case scoring function weights actions equally large heavily favours certain actions accurate estimate bellman error require even larger number samples since rarely taken actions high amount variance. relation convergence properties pasa considering situation updated provided continues change pasa converge instead continuously update however ﬁxed pasa converge particular sense describe. outline pasa assumed single ﬁxed step-size parameter proof easier suppose distinct ﬁxed step-size parameter element diﬀerent value remainder section understood referring vector step-size parameters. function becomes ε-ﬁxed provided that value remain satisfying probability least time scaled equivalent t/ηk will associated deterministic diﬀerence converges weakly distribution ornstein-ulenbeck process normal stationary hence distribution scaling factor fact bounded unit interval select scalar requirement satisﬁed provided following reason. suppose remains within interval size deﬁne imax maxi{u) imax have first note ηb+i that assuming {ρj}i remains ﬁxed ¯ub+i remain interval size iterations probability least iterations elapsed assumption {ηj}b+i− {ρj}i remain interval size iterations probability least cases provided iterations elapsed. accordingly choose min{hi+ choose τi+. hence remain interval size iterations probability least iterations elapsed. choose claim holds. hence choose vector becomes ε-ﬁxed holds taken care allow vector remain ﬁxed function practical applications ﬁxed step-sizes allow agent continue adapt response example changes environment ﬁxed step-sizes experiments below. whilst experiments section single step-size parameter details proof point merit using vector step-size parameters part sophisticated implementation ideas underlying pasa main theoretical results. idea that many important circumstances following ﬁxed policy agent tendency spend nearly time small subset state space. property advantage. means focussing small area eliminate terms signiﬁcantly contribute expected squared error. trick quantify tendency. fact adopted cell splitting approach critical importance easily permits create cells contain single state momentarily putting aside assumptions make following observation. deterministic pick starting state agent create path state space eventually revisit previously visited state enter cycle. call states path call states cycle denote respectively. course cycle terminate path cycle created call states second path cycle continue manner sets cs}. call union sets denote number states call number sets empty. also used fact poisson distribution parameter will becomes suﬃciently large well approximated normal distribution mean standard deviation hence replace ﬁrst second third equality second ﬁrst moment respectively note expectation also derived solution birthday problem solution birthday problem gives expectation since cycle length equal probability conditioned total path length divide average note deterministic transition matrix generated guaranteed irreducible aperiodic case exist. technicality state distribution periodic still restricted small number states violate conclusions. pasa algorithm provided large enough provided subset state space suﬃciently high probability suﬃciently small majority states high probability represented individually ﬁnal basis functions. fact conjunction results we’ve generated regarding distribution demonstrate pasa generate basis functions resulting estimate arbitrarily error prove almost identical fashion that assumptions redeﬁned actions weighted equally i.e. equation factor removed similarly arbitrarily close zero arbitrarily high probability. bound provided represents signiﬁcant reduction complexity starts take size comparable many real world problems could make diﬀerence problem tractable intractable. also seems likely bound theorem improved upon conditions commonly encountered practice particular taken reﬂect greedy policy. condition might happens longer generated randomly rather example according process policy iteration. intuitively plausible typically eﬀect reducing oﬀer general proof. however note that example states reward determine shorter path average random path pair states denote average path length reward states determined agent updating policy increase reward. reasoning we’ve stated increase bounded value increases. apply reasoning random policy case conclude states visited policy average states falling unique cell increases much slowly true pasa increased. alterations made pasa algorithm remove factor theorem however alternative algorithm complex describe unlikely perform noticeably better practical setting. interest prior inﬂuences values moments calculated lemmas assume deterministic transition function ﬁxed deterministic policy discussion around uniform priors also able assume sequence states setting need assume sequence generated according speciﬁc probability distribution. since discussion below generally assume prior distribution transition probabilities identical diﬀerent actions able continue assume arbitrary. then given arbitrary policy assuming sequence starting states distributed uniformly random minimised prior uniform. using fact theorem extended priors continue assume arbitrary policy starting states selected uniformly random consider general class priors using result karlin rinott result used demonstrate that priors satisfy following three conditions results assume degree similarity transition prior probabilities state. perhaps interesting potential generalisation follows. deﬁne balanced prior deterministic transition function prior random vector independently distributed pr|si pr|si pr|si essence prior probability transitioning state transitioning reverse direction sort prior would reﬂective many real world problems incorporate notion geometric space distances navigating around grid represents important generalisation. diﬀerence uniform prior expectation states close one-another states apart. however similar uniform prior inherent creating cycles larger expected value uniform case. hard conceive examples signiﬁcantly reduced particular balanced prior compared uniform case. would furthermore appear plausible that amongst balanced priors would maximised uniform prior. indeed investigation using numerical optimisation techniques demonstrates case even ﬁxed arbitrary starting state selected relative balanced transition probabilities. techniques used generalisations stated cannot used prove similar result balanced priors. conjecture based numerical analysis earlier stated results follow cases stronger statement maximised uniform prior conclusions regarding moments follow. case balanced priors strong result hold seen large taking comparing uniform prior prior transitions outside single ﬁxed cycle covering states probability zero prior notwithstanding formal result equivalent theorem unavailable balanced priors selecting suitable parameter values still able exploit apparent tendency agent spend majority time small subset state space. main diﬀerence small subset change slowly time. situation become altered favour policy iteration introduced policies longer random rather target states reward. experimental results focus transition functions drawn uniform distribution however research could help indicate extent pasa generate estimate error subject slightly altered dynamics introduced balanced priors. main objectives section test empirically tendency agent spend majority time small subset state space conduct experimental comparison pasa ﬁxed state aggregation demonstrate that average pasa help generate estimates lower figure report outcome sequence independent trials calculated explicitly order test theoretical bounds. results demonstrate estimates accurate bound lemma generous. figure also equivalent tests square grid world environment helps reinforce discussion around balanced transition function priors total probability stationary state distribution figure demonstrates that held constant increased start appears linear increase function seems likely leakage states outside becomes smaller remains same leakage become pronounced. whilst doesn’t invalidate underlying principles practical setting something remain aware even reasonably high e.g. require high value e.g. reasonably small number states required obtain total probability e.g. around states implying general comparatively low. ﬁnal comment whilst need theoretically guarantee capturing every high-probability state individually factor likely essential practical setting approaches large function values approach value reward function discounted reward heavily weighted future randomness agent’s trajectory future reward heavily impacted individual decision. hence conditions ﬁxed state aggregation also tend towards arbitrarily values. furthermore given state aggregation architecture number high probability states cell large average value state-action cell also approach value second point important calculating since cells large estimate tend fig. agent spends majority time small number states. chart explicitly calculated average values independent randomly generated transition functions. chart uniform transition function prior. included error bars reﬂect conﬁdence intervals chart grid-world environment fig. agent spends majority time small number states. states ordered terms stationary probability determine average conﬁdence intervals included. charts left right respectively. reasons taken care close used compare pasa ﬁxed state aggregation. since accurate measure serve strengthen results. downside that since exact estimate impossible calculate large perform comparison relatively values theoretical insights infer similar performance diﬀerences exist large cases reported square root algorithms comparing pasa combined sarsa algorithm sarsa algorithm ﬁxed state aggregation cells sized equally possible. shown sarsa generate estimate minimal given ﬁxed policy ﬁxed linear approximation architecture tested algorithms sequences randomly generated environments states particular algorithm eight separate experiments randomly generated trials each. cases algorithms tested identical environments. value listed equal respectively guided part figure comments results experiments shown table values parameters used follows respect sampling compared estimate state visited exact policy experiment calculated advance exactly solving system equations involving using least squares. quite large number iterations used trial part accommodate complex environment also provide clear evidence estimates stabilised. long trials necessary signiﬁcant diﬀerence algorithms minimising number iterations required strong difference pasa ﬁxed state aggregation would depend optimising parameters like subject choices since theoretic results assume example made arbitrarily practical setting won’t falling zero. however table demonstrates that computational cost able signiﬁcantly reduce ﬁxed state aggregation case. table shows error reduced around cent less complex case much approximately cent. noted above eﬀect pasa becomes slightly less pronounced approaches xeon .ghz algorithm variants although includes time calculate beforehand exact solution setting approximately half appears reliable ensuring large number high probability states fall singleton cell. smaller values tend exhaust many cells seeking isolate individual states whereas greater values mean pasa little control ﬁnal basis functions. fig. comparative pasa versus ﬁxed state aggregation function line average independent trials. averaged blocks thousand iterations. becomes larger pasa tend take longer decrease mapping requires time converge comparatively large). figure shows that increases pasa make algorithm take slightly longer generate however longer term signiﬁcantly reduced. optimising parameters even value might expected increase disparity pasa ﬁxed state aggregation received much attention oversight. seen many circumstances reﬂecting real-world problems unsupervised methods eﬀective creating approximation architecture thereby help generate accurate estimates distinguishes unsupervised methods complex alternatives however simplicity. seen algorithm pasa carries minimal additional costs compared algorithm supports. eﬀectiveness combined cost which view make techniques promising candidate research. setting policy improvement advantages oﬀered unsupervised methods potential particularly important. policy update requires estimate however policy update change requiring estimate. accurately eﬃciently estimating policy critical. initial experimentation suggests pasa algorithm signiﬁcant impact algorithm performance policy updates introduced. perhaps come surprise given reduced error shown section pasa algorithm represents relatively naive simplistic application idea unsupervised basis function adaptation appears likely number improvements could made algorithm order optimise performance. theoretical perspective assumption around scoring functions weighted crucial. nature estimate generated pasa associated algorithm well estimated states frequently visited existing policy. however results poorer estimates value deviating current policy. thus even though expected error immediately follow algorithm optimise policy standard policy iteration ultimately however theoretical implications improved estimate context policy iteration complex would need subject research.", "year": 2017}