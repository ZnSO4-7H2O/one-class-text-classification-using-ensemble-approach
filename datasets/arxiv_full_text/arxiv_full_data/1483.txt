{"title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence  Models", "tag": ["cs.AI", "cs.CL", "cs.CV"], "abstract": "Neural sequence models are widely used to model time-series data in many fields. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-$B$ candidates -- resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose \\emph{Diverse Beam Search} (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space -- implying that DBS is a \\emph{better search algorithm}. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.", "text": "ashwin vijayakumar michael cogswell ramprasath selvaraju qing stefan david crandall dhruv batra {ashwinkvcogswellramsunqingsteflee}vt.edu djcranindiana.edu dbatravt.edu neural sequence models widely used model time-series data. equally ubiquitous usage beam search approximate inference algorithm decode output sequences models. explores search space greedy left-right fashion retaining top-b candidates resulting sequences differ slightly other. producing lists nearly identical sequences computationally wasteful also typically fails capture inherent ambiguity complex tasks. overcome problem propose diverse beam search alternative decodes list diverse outputs optimizing diversity-augmented objective. observe method ﬁnds better top- solutions controlling exploration exploitation search space implying better search algorithm. moreover gains achieved minimal computational memory overhead compared beam search. demonstrate broad applicability method present results image captioning machine translation visual question generation using standard quantitative metrics qualitative human studies. method consistently outperforms previously proposed techniques diverse decoding neural sequence models. last years recurrent neural networks long short-term memory networks generally neural sequence models become standard choice modeling time-series data wide range applications speech recognition machine translation conversation modeling image video captioning visual question answering based sequence generation architectures model conditional probability output sequence given input output tokens ﬁnite vocabulary inference rnns. maximum posteriori inference rnns task ﬁnding likely output sequence given input. since number possible sequences grows |v|t exact inference np-hard approximate inference algorithms like beam search commonly employed. heuristic graph-search algorithm maintains top-scoring partial sequences expanded greedy left-to-right fashion. fig. shows sample search tree. lack diversity despite widespread usage long understood solutions decoded generic lacking diversity notice captions near-duplicates similar shared paths search tree minor variations end. contrast captions signiﬁcantly diverse similar inter-human variability describing images. jurafsky illustrate this comparison captions provided humans shown fig. behavior disadvantageous many reasons highlight three crucial ones here loss-evaluation mismatch i.e. improvements posterior-probabilities necessarily common practice deliberately throttle become poorer optimization algorithm using reduced beam widths. treatment optimization algorithm hyper-parameter intellectually dissatisfying also signiﬁcant practical side-effect leads decoding largely bland generic safe outputs e.g. always saying don’t know conversation models iii) importantly lack diversity decoded solutions fundamentally crippling problems signiﬁcant ambiguity e.g. multiple ways describing image responding conversation correct important capture ambiguity ﬁnding several diverse plausible hypotheses. overview contributions. address shortcomings propose diverse beam search general framework decode list diverse sequences used alternative high level decodes diverse lists dividing beam budget groups enforcing diversity groups beams. drawing recent work probabilistic graphical models literature diverse m-best inference optimize objective consists terms sequence likelihood model dissimilarity term encourages beams across groups differ. diversity-augmented model score optimized doubly greedy manner greedily optimizing along time groups summarize primary technical contribution diverse beam search doubly greedy approximate inference algorithm decoding diverse sequences. method consistently outperforms comparable terms run-time memory requirements. report results image captioning machine translation visual question generation demonstrate broad applicability dbs. results improvements oracle task-speciﬁc diversity-related metrics baselines. conduct human studies evaluate role diversity human preferences image captions. also analyze parameters show robust wide range values. finally also show method general enough incorporate various forms dissimilarity term. overall algorithm simple implement consistently outperforms wide range domains without sacriﬁcing efﬁciency. implementation available https//github. begin refresher describing generalization diverse beam search. notational convenience denote natural numbers index ﬁrst elements vector decoding problem. rnns trained estimate likelihood sequences tokens ﬁnite dictionary given input updates internal state estimates conditional probability distribution next output given input previous output tokens. denote logarithm conditional probability distribution tokens time simplify notation index single variable clear depends previous outputs context. log-probability partial solution decoding problem task ﬁnding sequence output conditioned previous outputs decoding optimal length-t sequence setting viewed inference -order markov chain nodes corresponding output tokens. size largest factor graph grow |v|t also requires wastefully forwarding repeatedly compute entries factors. thus approximate algorithms employed. beam search. prevalent method approximate decoding stores top-b highly scoring candidates time step; known beam width. denote solutions held start time yb}. time step considers possible single token extensions beams given selects likely extensions. formally step objective trivially maximized sorting members log-probabilities selecting top-b. process repeated time likely sequence selected ranking beams based log-probabilities. method allows multiple sequences explored parallel completions tend stem single highly valued beam resulting outputs typically minor perturbations single sequence. overcome shortcoming consider augmenting objective dissimilarity term measures diversity candidate sequences. jointly optimizing candidates time step intractable number possible solutions grows |v|b avoid joint optimization problem divide beam budget groups greedily optimize group using beam search holding previous groups ﬁxed. doubly greedy approximation along time across groups turns function current group’s possible extensions. detail speciﬁcs approach section. diverse beam search. beams time partitioned disjoint without loss generality consider equal partition non-empty subsets group contains groups. beam search applied group produce solutions; however group would produce identical outputs. figure diverse beam search operates left-to-right time bottom groups. diversity groups combined joint log-probabilities allowing continuations found efﬁciently. resulting outputs diverse standard approaches. extend beams group exact form vary discussion choice dealt section optimize group previous groups ﬁxed extending group time amounts standard using dissimilarity augmented log-probabilities written approach call diverse beam search detailed algorithm example shown figure decoding image-captions. example group performs smaller diversity-augmented size snapshot shown group stepped forward diversity augmented score words dictionary computed conditioned previous groups. score words adjusted similarity previously chosen words ‘birds’ ‘the’ ‘an’ optimal continuations found standard summary works doubly greedy manner enabling incorporate diversity beam search. moreover ﬁrst group conditioned groups optimization method guaranteed least good beam search size b/g. diverse m-best lists. task generating diverse structured outputs probabilistic models studied extensively batra formalized task markov random fields divmbest problem presented greedy approach solves outputs iteratively conditioning previous solutions induce diversity. kirillov show solutions found jointly certain kinds energy functions. techniques developed kirillov directly applicable decoding rnns satisfy assumptions made. related proposed approach gimpel apply divmbest approach machine translation using beam search black-box inference algorithm. obtain diverse solutions beam searches arbitrary size sequentially performed retaining top-scoring candidate using update diversity term. approach extremely wasteful iteration solution returned beam search kept. consequently iterative method time consuming poorly suited batch processing producing large number solutions. algorithm avoids shortcomings integrating diversity within beams discarded. running multiple beam searches parallel staggered time offsets obtain large time savings making method comparable classical potential advantage method complex diversity measures sentence-level incorporated. however observed empirically initial words tend signiﬁcantly impact diversity resultant sequences suggesting later words contribute signiﬁcantly diverse inference. context work closely related jurafsky propose diversiﬁcation heuristic overcome shortcomings gimpel discourages sequences sharing common roots implicitly resulting diverse lists. introducing diversity modiﬁed objective rather heuristic provides easier generalization incorporate different notions diversity control exploration-exploitation trade-off detailed section furthermore outperforms method. novel decoding objective maximizes mutual information inputs predicted outputs penalize decoding generic input independent sequences. achieved training additional target language model. although work share goals techniques developed disjoint complementary develops model modiﬁed inference algorithm applied model applicable. combination complementary techniques left interesting future work. ﬁrst explain baselines evaluation metrics used paper. next proceed analysis effects parameters. further report results image-captioning machine translation visual question generation. although results reported tasks noted task-agnostic algorithm replace decode diverse solutions. baselines. compare beam search following existing methods jurafsky work modiﬁes introducing intra-sibling rank. partial solution continuations sorted assigned intra-sibling ranks order decreasing log-probabilities log-probability extenstion reduced proportion rank continuations re-sorted modiﬁed log-probabilities select top-b diverse beam extensions. works secondary mechanisms re-rankers pick single solution generated lists. interested evaluating quality generated lists isolating gains diverse decoding implement re-rankers. instead simply sort list based log-probability. compare implementations methods none publicly available. evaluation metrics. evaluate performance generated lists using following metrics quantify complementary details oracle accuracy oracle top-k accuracy task-speciﬁc metric like bleu maximum value metric list potential solutions. upper bound diversity statistics count number distinct n-grams present list generated outputs. similar divide counts total number words generated bias long sentences. simultaneous improvements metrics indicate output lists increased diversity without sacriﬁcing ﬂuency correctness respect target tasks. human preference studies compare image captions produced also compare methods. finally discuss role diversity relating intrinsic details contained images. section study impact number groups strength diversity penalty various forms diversity functions language models. discussion experimental details included supplementary materials. number groups setting allows maximum exploration space setting reduces method resulting increased exploitation search-space around -best decoding. thus increasing number groups enables explore various modes model. empirically maximum exploration correlates improved oracle accuracy hence report results unless mentioned otherwise. diversity strength diversity strength speciﬁes trade-off joint logprobability diversity terms. expected higher value produces diverse list; however excessively high values overpower model probability result grammatically incorrect outputs. performing grid search validation experiments. wide range values work well tasks datasets. choice diversity function section deﬁned function partial solutions outputs vector similarity scores potential beam completions. assuming previous groups inﬂuece completion current group independently simplify dissimilarity term summing group’s contributions factorized term take various forms ranging simple hamming cumulative diversity. sequences diverged sufﬁciently seems unnecessary perhaps harmful restrict cannot words time. encode ‘backing-off’ diversity penalty introduce cumulative diversity keeps count identical words used every time step indicative overall dissimilarity. speciﬁcally ])/γ} temperature parameter control∆ indicator function. n-gram diversity. current group penalized producing n-grams previous groups regardless alignment time similar gimpel proportional number times n-gram candidate occurred previous groups. unlike hamming diversity n-grams capture higher order structures sequences. neural-embedding diversity. previous diversity functions discussed perform exact matches neural embeddings wordvec penalize semantically similar words like synonyms. incorporated previous diversity functions replacing hamming similarity soft version obtained computing cosine similarity wordvec representations. using n-gram diversity representation n-gram obtained summing vectors constituent words. various forms encode different notions diversity. hamming diversity ensures different words used different times circumvented small changes sequence alignment. n-gram diversity captures higher order statistics ignores sentence alignment. neural-embedding based encodings seen semantic blurring either hamming n-gram metrics wordvec representation similarity propagating diversity penalties exact matches also close synonyms. using functions help outperform tasks examine; hamming diversity achieves best oracle performance despite simplicity. comparison performance functions image-captioning provided supplementary. dataset models. evaluate datasets coco pascal-s public splits karpathy fei-fei coco. pascal-s used testing save validation images used tune hyperparameters. train captioning model using neuraltalk code repository. results. observed table outperforms jurafsky datasets. observe gains pascal-s pronounced coco. suggests diverse predictions especially advantageous mismatch training testing sets making better inference strategy real-world applications. table also shows number distinct n-grams produced different techniques. method produces signiﬁcantly distinct n-grams compared also note method tends produce slightly longer captions compared beam search average. moreover pascal-s test split observe ﬁnds likely top- solutions average obtains maximum log-probability beam width. performance guaranteed better size experimental evidence suggests using replacement leads better least comparable performance. table oracle accuracy distinct n-grams coco pascal-s datasets image captioning although report cider observe similar trends standard metrics. human studies. evaluate human preference captions generated perform human study amazon mechanical turk using images pascals. image standard captions shown different users. asked which robots understands image better displaying intelligent human-like behavior? forced-choice test captions preferred time. diversity always needed? results show diversity outputs important systems interact consumers diversity always beneﬁcial? images many objects described multiple ways true objects notion studied ionescu deﬁnes difﬁculty score human response time solving visual search task. pascals dataset observe positive correlation difﬁculty scores humans preferring generally preferred humans difﬁcult images seems strong preference easier images. details human study correlation experiments provided supplementary. dataset models. english-french parallel data europarl corpus training set. report results news-test- news-test- newstest tune parameters. train encoder-decoder architecture proposed bahdanau using dlmt-tutorial code repository. encoder consists bi-directional recurrent network attention. sentence level bleu scores compute oracle metrics report distinct n-grams similar image-captioning. table consistently outperforms standard baselines respect metrics. table quantitative results en-fr machine translation newstest- dataset although report bleu- values similar trends hold lower bleu metrics well. also report results another novel task visual question generation dataset train model similar image captioning. instead captions training consists questions image. requires model reason multiple problems central vision like position color object relationships objects natural language. similarly learning right questions pertinent image also requires model reason ﬁner aspects making question generation interesting task. using beam search sample outputs results similarly worded questions brings details captured model modes. promotes diverse questions different types deﬁned antol observe number question types generated image increases employing beam search commonly used approximate inference algorithm decode sequences rnns; however suffers lack diversity. producing multiple highly similar generic outputs wasteful terms computation also detrimental tasks inherent ambiguity like image captioning. work presented diverse beam search describes beam search optimization problem augments objective diversity term. result ‘doubly greedy’ approximate algorithm produces diverse decodings using time resources beam search. method consistently outperforms beam search baselines across experiments without extra computation task-speciﬁc overhead. task-agnostic applied case used making applicable multiple domains. implementation made publicly available. references stanislaw antol aishwarya agrawal jiasen margaret mitchell dhruv batra lawrence zitnick devi parikh. visual question answering. proceedings ieee conference computer vision pattern recognition dzmitry bahdanau kyunghyun yoshua bengio. neural machine translation jointly learning align translate. proceedings international conference learning representations dhruv batra payman yadollahpour abner guzman-rivera gregory shakhnarovich. diverse m-best solutions markov random fields. proceedings european conference computer vision francis ferraro ishan mostafazadeh nasrinand misra aishwarya agrawal jacob devlin ross girshick xiadong pushmeet kohli dhruv batra lawrence zitnick. visual storytelling. proceedings conference north american chapter association computational linguistics human language technologies jenny rose finkel christopher manning andrew solving problem cascading proceedings errors approximate bayesian inference linguistic annotation pipelines. conference empirical methods natural language processing gimpel batra dyer shakhnarovich. systematic exploration diversity machine translation. proceedings conference empirical methods natural language processing radu tudor ionescu bogdan alexe marius leordeanu marius popescu papadopoulos vittorio ferrari. hard estimating difﬁculty visual search image. proceedings ieee conference computer vision pattern recognition andrej karpathy fei-fei. deep visual-semantic alignments generating image descriptions. proceedings ieee conference computer vision pattern recognition alexander kirillov bogdan savchynskyy dmitrij schlesinger dmitry vetrov carsten rother. inferring m-best diverse labelings single one. proceedings ieee conference computer vision pattern recognition jiwei michel galley chris brockett jianfeng bill dolan. diversity-promoting objective function neural conversation models. proceedings conference north american chapter association computational linguistics human language technologies tomas mikolov ilya sutskever chen greg corrado jeff dean. distributed representations words phrases compositionality. advances neural information processing systems nasrin mostafazadeh ishan misra jacob devlin margaret mitchell xiaodong lucy vanderwende. generating natural questions image. proceedings annual meeting association computational linguistics adarsh prasad stefanie jegelka dhruv batra. submodular meets structured finding diverse subsets exponentially-large structured item sets. advances neural information processing systems subhashini venugopalan marcus rohrbach jeffrey donahue raymond mooney trevor darrell kate saenko. sequence sequence-video text. proceedings ieee conference computer vision pattern recognition oriol vinyals alexander toshev samy bengio dumitru erhan. show tell neural image caption generator. proceedings ieee conference computer vision pattern recognition diversity strength. noted section method robust wide range values diversity strength fig. shows grid search image-captioning pascal-s dataset. choice diversity function. fig. shows oracle performace various forms diversity function described section observe hamming diversity surprisingly performs best. forms perform comparably outperforming figure fig. shows results grid search diversity strength parameter validation split pascal dataset. observe robust wide range values. fig. compares performance multiple forms diversity function naïve diversity performs best forms comparable better image-captioning conduct human preference study captions explained section screen shot interface used collect human preferences captions generated using presented fig. lists shufﬂed guard task gamed turker. mentioned section observe difﬁculty score image human preference captions positively correlated. dataset contains images less difﬁculty analyze correlation dividing data three bins. report images captions preferred majority vote table difﬁculty scores consisting mostly iconic images might expect would preferred often chance. however mismatch statistics training testing data results better performance dbs. examples case provided fig. general qualitative examples provided fig. figure images difﬁculty score captions preferred show ﬁrst ﬁgure. however observe captions perform better mismatch statistics testing training sets. interesting captions colored blue readability.", "year": 2016}