{"title": "Deep Convolutional Framelets: A General Deep Learning Framework for  Inverse Problems", "tag": ["stat.ML", "cs.CV", "cs.IT", "cs.LG", "math.IT"], "abstract": "Recently, deep learning approaches with various network architectures have achieved significant performance improvement over existing iterative reconstruction methods in various imaging problems. However, it is still unclear why these deep learning architectures work for specific inverse problems. To address these issues, here we show that the long-searched-for missing link is the convolution framelets for representing a signal by convolving local and non-local bases. The convolution framelets was originally developed to generalize the theory of low-rank Hankel matrix approaches for inverse problems, and this paper further extends the idea so that we can obtain a deep neural network using multilayer convolution framelets with perfect reconstruction (PR) under rectilinear linear unit nonlinearity (ReLU). Our analysis also shows that the popular deep network components such as residual block, redundant filter channels, and concatenated ReLU (CReLU) do indeed help to achieve the PR, while the pooling and unpooling layers should be augmented with high-pass branches to meet the PR condition. Moreover, by changing the number of filter channels and bias, we can control the shrinkage behaviors of the neural network. This discovery leads us to propose a novel theory for deep convolutional framelets neural network. Using numerical experiments with various inverse problems, we demonstrated that our deep convolution framelets network shows consistent improvement over existing deep architectures.This discovery suggests that the success of deep learning is not from a magical power of a black-box, but rather comes from the power of a novel signal representation using non-local basis combined with data-driven local basis, which is indeed a natural extension of classical signal processing theory.", "text": "abstract. recently deep learning approaches various network architectures achieved signiﬁcant performance improvement existing iterative reconstruction methods various imaging problems. however still unclear deep learning architectures work speciﬁc inverse problems. moreover contrast usual evolution signal processing theory around classical theories link deep learning classical signal processing approaches wavelets non-local processing compressed sensing well understood. address issues show long-searched-for missing link convolution framelets representing signal convolving local non-local bases. convolution framelets originally developed generalize theory low-rank hankel matrix approaches inverse problems paper extends idea obtain deep neural network using multilayer convolution framelets perfect reconstruction rectilinear linear unit nonlinearity analysis also shows popular deep network components residual block redundant ﬁlter channels concatenated relu indeed help achieve pooling unpooling layers augmented high-pass branches meet condition. moreover changing number ﬁlter channels bias control shrinkage behaviors neural network. discovery reveals limitations many existing deep learning architectures inverse problems leads propose novel theory deep convolutional framelets neural network. using numerical experiments various inverse problems demonstrated deep convolution framelets network shows consistent improvement existing deep architectures. discovery suggests success deep learning magical power black-box rather comes power novel signal representation using non-local basis combined data-driven local basis indeed natural extension classical signal processing theory. introduction. deep learning approaches achieved tremendous success classiﬁcation problems well low-level computer vision problems segmentation denoising super-resolution etc. theoretical origin success investigated exponential expressivity given network complexity rademacher complexity often attributed success. deep network also known learn high-level abstractions/features data similar visual processing human brain using multiple layers neurons non-linearity inspired success deep learning low-level computer vision several machine learning approaches recently proposed image reconstruction problems. x-ray computed tomography kang provided ﬁrst systematic study deep convolutional neural network low-dose showed deep using directional wavelets eﬃcient removing low-dose related noises. unlike low-dose artifacts reduced tube currents streaking artifacts originated sparse projection views show globalized artifacts diﬃcult remove using conventional denoising cnns independently proposed residual learning using u-net remove global streaking artifacts caused sparse projection views. wang ﬁrst apply deep learning compressed sensing trained deep neural network downsampled reconstruction images learn fully sampled reconstruction. then used deep learning result either initialization regularization term classical approaches. multilayer percepmedicine grant national institute biomedical imaging bioengineering providing low-dose grand challenge data set. work supported national research foundation korea grant number nrf-rab nrf-maa nrf-mca. tron developed accelerated parallel deep network architecture using unfolded iterative compressed sensing algorithm also proposed instead using handcrafted regularizers authors tried learn optimal regularizers. domain adaptation sparse view network projection reconstruction also proposed pioneering works consistently demonstrated impressive reconstruction performances often superior existing iterative approaches. however observed impressive empirical results image reconstruction problems unanswered questions encounter. example best knowledge complete answers following questions critical network design role ﬁlter channels convolutional layers networks need fully connected layers whereas others role nonlinearity rectiﬁed linear unit need pooling unpooling architectures role by-pass connection residual network many layers need furthermore troubling issue signal processing community link classical signal processing theory still fully understood. example wavelets extensively investigated eﬃcient signal representation theory many image processing applications exploiting energy compaction property wavelet bases. compressed sensing theory extended idea demonstrate accurate recovery possible undersampled data signal sparse frames sensing matrix incoherent. non-local image processing techniques non-local means also demonstrated impressive performance many image processing applications. link algorithms extensively studied last years using various mathematical tools harmonic analysis convex optimization etc. however recent years witnessed blind application deep learning toolboxes sometimes provides even better performance mathematics-driven classical signal processing approaches. imply dark signal processing opportunity therefore main goal paper address open questions. fact paper attempt address issues. instance papyan showed relu nonlinearity employed forward pass network interpreted deep sparse coding algorithm. wiatowski discusses importance pooling networks proving leads translation invariance. moreover several works including provided explanations residual networks. interpretation deep network terms unfolded sparse recovery another prevailing view research community however interpretation still give answers several questions example need multichannel ﬁlters paper therefore depart existing views propose interpretation deep network novel signal representation scheme. fact signal representation theory wavelets frames active areas researches many years mallat bruna proposed wavelet scattering network translation invariant deformation-robust image representation. however approach learning components existing deep learning networks. then missing here? important contributions work show geometry deep learning revealed lifting signal high dimensional space using hankel structured matrix. speciﬁcally many types input signals occur signal processing factored left right bases well sparse matrix energy compaction properties lifted hankel structure matrix. results frame representation signal using left right bases referred non-local local base matrices respectively. origin nomenclature become clear later. novel contributions realization non-local base determines network architecture pooling/unpooling local basis allows network learn convolutional ﬁlters. speciﬁcally applicationspeciﬁc domain knowledge leads better choice non-local basis learn local basis maximize performance. fact idea exploiting bases so-called convolution framelets originally proposed however aforementioned close link deep neural network revealed importantly demonstrate ﬁrst time convolution framelet representation equivalently represented encoder-decoder convolution layer multilayer convolution framelet expansion also feasible relaxing conditions furthermore derive perfect reconstruction condition rectiﬁed linear unit mysterious role redundant multichannel ﬁlters easily understood important tool meet condition. moreover augmenting local ﬁlters paired ﬁlters opposite phase relu nonlinearity disappears deep convolutional framelet becomes linear signal representation. however order deep network satisfy condition number channels increase exponentially along layer diﬃcult achieve practice. interestingly show insuﬃcient number ﬁlter channels results shrinkage behavior rank approximation extended hankel matrix shrinkage behavior exploited maximize network performance. finally overcome limitation pooling unpooling layers introduce multi-resolution analysis convolution framelets using wavelet non-local basis generalized pooling/unpooling. call class deep network using convolution framelets deep convolutional framelets. notations. matrix denotes range space refers null space denotes projection range space whereas denotes projection orthogonal complement notation denotes n-dimensional vector identity matrix referred in×n. given matrix rm×n notation refers generalized inverse. superscript denotes hermitian transpose. mainly interested real valued cases equivalent transpose inner product matrix space deﬁned trb) rn×m. matrix denotes frobenius norm. given matrix rn×m denotes j-th column elements sub-matrix rd×q refers j-th column vector referred ﬂipped version vector i.e. indices reversed. similarly given matrix rd×q notation rd×q mathematics hankel matrix. since hankel structured matrix component theory section discusses various properties hankel matrix extensively used throughout paper. hankel matrix representation convolution. hankel matrices arise repeatedly many diﬀerent contexts signal processing control theory system identiﬁcation harmonic retrieval array signal processing subspace-based channel identiﬁcation etc. hankel matrix also obtained convolution operation particular interest paper. here avoid special treatment boundary condition theory mainly derived using circular convolution. non-local basis matrix encoder non-local basis matrix decoder local basis matrix encoder local basis matrix decoder encoder decoder biases i-th non-local basis ﬁlter encoder i-th non-local basis ﬁlter decoder i-th local basis ﬁlter encoder i-th local basis ﬁlter decoder convolutional framelet coeﬃcients encoder convolutional framelet coeﬃcients decoder input dimension convolutional ﬁlter length number input channels number output channels single channel input signal i.e. p-channel input signal i.e. rn×p hankel operator i.e. rn×d extended hankel operator i.e. hd|p rn×p rn×pd generalized inverse hankel operator i.e. rn×d generalized inverse extended hankel operator i.e. number input output channels respectively; dﬁlter convolves j-th channel input compute contribution i-th output channel. deﬁning mimo ﬁlter kernel follows extension multi-channel convolution operation image domain straight-forward since similar matrix vector operations also used. required change deﬁnition hankel matrices fig. convolutional operations hankel matrix representations. single-input single-output convolution simo convolution mimo convolution miso convolution fig. convolutional operation. ﬁrst layer ﬁlter input output channel numbers respectively ﬁlter dimension thus corresponding convolution operation denotes i-th input j-th convolutional neural networks unique multi-dimensional convolutions used. specifically generate output channels input channels channel output computed ﬁrst convolving ﬁlters pinput channel images applying weighted outnote many types image patches sparsely distributed fourier spectra. example shown fig. smoothly varying patch usually spectrum content low-frequency regions frequency regions spectral components. similar spectral domain sparsity observed texture patch shown fig. spectral components patch determined spectrum patterns. case abrupt transition along edge shown fig. spectral components mostly localized along axis. cases construct hankel matrix using corresponding image patch resulting hankel matrix low-ranked property extremely useful demonstrated many applications example idea used image denoising deconvolution modeling underlying intact signals low-rank hankel structure artifacts blur components easily removed. rate innovations thus low-rank hankel matrix provides important link sampling theory compressed sensing sparse recovery problem solved using measurement domain low-rank interpolation note low-rank hankel matrix algorithms usually performed patch-by-patch manner also remarkable similar current practice deep level computer vision applications network input usually given patch. later show coincidence; rather suggests important link low-rank hankel matrix approach cnn. hankel matrix decomposition convolution framelets. last least important property hankel matrix hankel matrix decomposition results framelet representation whose bases constructed convolution so-called local non-local bases rn×r rd×r denote left right singular vector bases matrices respectively; rr×r diagonal matrix whose diagonal components contains singular values. then multiplying left right hankel matrix last equality comes since number rows columns right-multiplied vector interacts locally neighborhood vector whereas left-multiplied vector global interaction entire n-elements vector. accordingly represents strength simultaneous global local interaction signal bases. thus call non-local local bases respectively. represents interaction non-local basis local basis using expansion coeﬃcients derived following signal expansion called convolution framelet expansion general non-local local bases sparse bases showed framelet coeﬃcients made suﬃciently sparse optimally learning given non-local basis therefore choice non-local bases factors determining eﬃciency framelet expansion. following several examples non-local bases note non-zero elements column haar basis level haar decomposition represent global interaction. however cascading haar basis interaction becomes global resulting multi-resolution decomposition input signal. moreover haar basis useful global basis sparsify piecewise constant signals. later show average pooling operation closely related haar basis. energy compaction property proven jpeg image compression standard. bases matrix fully populated dense matrix clearly represents global interaction. best knowledge basis never used deep could interesting direction research. signal. interestingly non-local basis quite often used cnns pooling layer. case believed local structure signal important local-bases trained maximally capture local correlation structure signal. learned basis extreme case speciﬁc knowledge signal basis size quickly becomes large image processing applications. example interested processing image required memory store learnable non-local basis becomes possible store estimate. however input patch size suﬃciently small another interesting direction research deep cnn. main contributions deep convolutional framelets neural networks. section main theoretical contribution show convolution framelets directly related deep neural network relax condition original convolution framelets allow multilayer implementation. multi-layer extension convolution framelets call deep convolutional framelet explain many important components deep learning. deep convolutional framelet expansion. original convolution framelets exploits advantages rank hankel matrix approaches using bases several limitations. first convolution framelet uses orthonormal basis. second signiﬁcance multi-layer implementation noticed. here discuss extension relax limitations. become clear basic building step toward deep convolutional framelets neural network. proposition rn×m rd×q denote nonlocal local bases matrices respectively. suppose furthermore rn×m rd×q denote dual bases matrices satisfy frame condition remark compared proposition propositions general since consider redundant non-orthonormal non-local local bases allowing relaxed conditions i.e. speciﬁc reason investigate existing cnns large number ﬁlter channels lower layers. redundant global basis also believed useful future research proposition derived considering extension. however since existing deep networks condition mainly focus special case rest paper. finally using propositions show convolution framelet expansion realized matched convolution layers striking similarity neural networks encoder-decoder structure main contribution summarized following theorem. remark note exists major diﬀerence encoder decoder layer convolutions. aside diﬀerence speciﬁc convolutional ﬁlters non-local basis matrix applied later ﬁltered signal case encoder whereas non-local basis matrix multiplied ﬁrst local ﬁltering applied decoder layer. fact similar pooling unpooling operations pooling performed ﬁltering unpooling applied ﬁltering. hence non-local basis generalization pooling/unpooling operations. existing cnns often incorporate bias estimation layer convolution. accordingly interested extending deep convolutional framelet expansion bias estimation. speciﬁcally bias estimation encoder decoder convolutions modiﬁed simple convolutional framelet expansion using including bias) powerful encoder-decoder architecture emerges inserting encoder-decoder pair encoder-decoder pair illustrated blue lines respectively fig. general l-layer implementation convolutional framelets recursively deﬁned. example ﬁrst layer here denotes ﬁlter length number input channels i-th layer respectively refers number output channels. speciﬁc number channels analyzed following section. properties deep convolutional framelets. section several important properties deep convolutional framelets explained detail. first perfect reconstruction conditions also analyzed fourier domain shown following proposition proof. appendix remark note equivalent perfect reconstruction conditions orthogonal biorthogonal wavelet decompositions respectively however without frame condition non-local bases simpliﬁcation true. another reason interested imposing frame condition non-local bases order connection classical wavelet theory intuitive explanation proposition cascaded application multi-channel ﬁlter banks equivalent applying combined ﬁlter bank generated combination ﬁlters stage output dimension ﬁlter bank increases multiplicatively. special case proposition derive following suﬃcient condition obtained choosing minimum number output channels layer. implies number channels increase exponentially respect layers shown fig. diﬃcult meet practice memory requirement. then natural question still prefer deep network shallow one. proposition provides answer question. proof. appendix remark proposition derived assuming identity matrix non-local basis. expect similar results general non-local basis satisﬁes frame condition introduction non-local basis makes analysis complicated non-commutative nature matrix multiplication convolution defer analysis future study. implies number minimum encoder-decoder layer dependent rank structure input signal need deeper network complicated signal consistent empirical ﬁndings. moreover shown fig. given intrinsic rank depth network also depends ﬁlter length. example intrinsic rank hankel matrix network depth respect whereas longer ﬁlter used. suppose suﬃcient number output channels speciﬁc layer signal content approximated. then proof proposition easily rankhd|p upper-bounded rank structure approximated signal moreover numbers channels suﬃcient layers rank gradually decrease along layers. theoretical prediction conﬁrmed later discussion using empirical data. currently widely used deep learning approaches. speciﬁcally relu elementij= rn×m relu operator provides wise operation matrix matrix non-negative part i.e. ij=. inserting relus llayer encoder-decoder architecture neural network bias deﬁned recall condition deep convolutional framelets derived without assuming nonlinearity. thus introduction relu appears counter-intuitive context interestingly spite relu nonlinearity following theorem shows condition satisﬁed ﬁlter channels bias opposite phase available. proof. appendix remark proposition predicts existence ﬁlter pairs opposite phase. amazingly theoretical prediction coincides empirical observation deep learning literature. example shang observed intriguing property ﬁlters lower layers form pairs exploit property network performance improvement authors proposed called concatenated relu network explicitly remark note inﬁnite number ﬁlters satisfying fact important requirement existence opposite phase ﬁlters satisfying frame condition rather speciﬁc ﬁlter coeﬃcients. suggest excellent generalization performance deep network even small training data set. easily satisﬁed lower layers deep convolutional framelets; however number ﬁlter channels grows exponentially according layers shown thus higher however network gets deeper layers cannot satisfy condition proposition address this residual useful. recall residual widely used image classiﬁcation well image reconstruction. speciﬁcally residual architecture shown fig. represented accordingly choosing singular vector least singular value minimize error approximating hd|p using refer proposition high rank approximation using residual net. role perfect reconstruction condition shrinkage behaviour network. investigated perfect recovery condition deep convolutional framelets. here ready explain useful inverse problems. denotes hilbert space interest. signal processing using frame basis satisfying extensively studied frame-based image processing important advantages frame-based algorithm proven convergence makes algorithm powerful. example consider signal denosing algorithm recover noiseless signal noisy measurement additive noise. then frame-based denoising algorithm recovers unknown signal substituting right side applies shrinkage operator framelet coeﬃcients eliminate small magnitude noise signals denotes shrinkage parameter. denoising softhardthresholding shrinkage operations widely used. thus make frame-based denoising algorithm successful frame energy compacting signals concentrated small number framelet coeﬃcients whereas noises spread across framelet coeﬃcients. non-local local basis satisfy frame condition layers satisfy condition thus could denoising inpainting applications. then shrinkage operator deep convolutional framelets? although could still similar softhardthresholding operator unique aspects deep convolutional framelets number ﬁlter channels control shrinkage behaviour. moreover optimal local basis learnt training data give best shrinkage behavior. speciﬁcally low-rank shrinkage behaviour emerges number output ﬁlter channels suﬃcient. results useful practice since low-rank approximation hankel matrix good reducing noises artifacts demonstrated image denoising artifact removal deconvolution denotes ground-truth signal interested ﬁnding rank-r approximation. then feasible solution hankel structured matrix singular value decomposition rn×r rd×r denote left right singular vector bases matrices respectively; rr×r diagonal matrix singular values. then matrices pairs rn×n rd×r satisfying deep convolutional framelets correspond generalized pooling unpooling chosen based domain knowledges interested estimating ﬁlters restrict search space furthermore denote space signals positive framelet coeﬃcient i.e. benc bdec denotes encoder decoder biases respectively. then main goal neural network training learn training data associated rank-r hankel matrices. speciﬁcally regression assuming solution rank-r hankel structured matrix. therefore using deep convolutional framelets insuﬃcient channels need explicit shrinkage operation. idea extended multi-layer deep convolutional framelet expansion follows deep learning denoising algorithm whereas ﬁrst iteration corresponds existing deep learning-based inpainting algorithm conﬁrms deep convolutional framelets general deep learning framework. later provide numerical experiments using image denosing inpainting applications respectively. closely related resnet fig. except ﬁnal level nonlinearity commonly used by-pass connection by-pass connection normally placed input output network. practice researchers empirically determine whether bypass connections trial error. minimizing cost thus deep convolutional framelet by-pass connection subtracting fact equivalent ﬁlter condition resnet spans minimum singular vector subspace. brief true underlying signal lower dimensional structure artifact annihlating ﬁlter relationship easier achieve thus neural network bypass connection achieves better performance. hand artifact lower dimensional structure neural network without by-pass connection better. coincides many empirical ﬁndings. example image denoising streaking artifact problems residual network works better since artifacts random complicated distribution. contrast remove cupping artifacts x-ray interior tomography problem without skipped connection better since cupping artifacts usually smoothly varying multi-resolution analysis deep convolutional framelets. deep convolutional framelets given non-local basis local convolution ﬁlters learnt give best shrinkage behaviour. thus non-local basis important design parameter controls performance. particular energy compaction property deep convolutional framelets signiﬁcantly aﬀected recall basis hankel matrix results best energy compaction property; however basis varies depending input signal type cannot basis various input data. therefore choose analytic non-local basis approximate basis result good energy compaction property. thus wavelet preferable choices piecewise continuous signals images speciﬁcally wavelet basis standard pooling unpooling networks used low-frequency path wavelet transform exists additional high-frequency paths wavelet transform. another important motivation multi-resolution analysis convolutional framelets exponentially large receptive ﬁeld. example fig. compares network depth-wise eﬀective receptive ﬁeld multi-resolutional network pooling baseline network without pooling layers. size convolutional ﬁlters eﬀective receptive ﬁeld enlarged network pooling layers. therefore multi-resolution analysis indeed derived supplement enlarged receptive ﬁeld pooling layers detailed processing using high-pass band convolutional framelets. limitation u-net. explain multi-resolution deep convolutional framelets ﬁrst discuss limitations popular multi-resolution deep learning architecture called u-net composed encoder decoder network skipped connection. u-net utilizes pooling unpooling shown fig. obtain exponentially large receptive basically low-pass ﬁltered signal detail signals lost. address limitation retain detail u-net by-pass connections concatenation layers shown fig. speciﬁcally combining low-pass by-pass connection augmented convolutional framelet coeﬃcients caug represented proposed multi-resolution analysis. address limitation u-net propose novel multi-resolution analysis using wavelet non-local basis. discussed before ﬁrst layer interested learning note operation corresponds local ﬁltering followed non-local basis matrix multiplication shown block fig. then ﬁrst layer following decomposition fig. proposed multi-resolution analysis deep convolutional framelets. here corresponds convolution operation; blue blocks correponds encoder decoder blocks respectively. example multi-resolution deep convolutional framelet decomposition length- local ﬁlters. again corresponds standard average pooling operation. note need lifting operation extended hankel matrix hankel blocks ﬁrst layers generates ﬁltered output needs convolved d-length ﬁlters second layer. multilayer implementation convolution framelets results interesting encoderdecoder deep network structure shown fig. blue blocks represent encoder decoder blocks respectively. addition table summarizes dimension l-th layer matrices. speciﬁcally relu encoder parts given follows fig. shows overall structure multi-resolution analysis convolutional framelets length- local ﬁlters used. note structure quite similar u-net structure except high pass ﬁlter pass. conﬁrms close relationship deep convolutional framelets deep neural networks. experimental results. section investigate various inverse problem applications deep convolutional framelets including image denoising sparse view reconstruction inpainting. particular focus novel multi-resolution deep convolutional framelets using haar wavelets. applications multi-resolution deep convolutional framelets extended structure architecture fig. modiﬁed. resulting architectures illustrated figs. u-net used reference networks shown fig. note u-net fig. exactly reconstruction network inverse problems concatenation. stage consists four convolution layers followed relu except ﬁnal stage last layer. ﬁnal stage composed convolution layers low-pass high-pass branch last layer convolution layer. figs. unpooling. thus pooling layer wavelet transform generates four subbands bands. then band processed using convolutional layers followed another waveletbased pooling layer. shown figs. channels doubled wavelet decomposition. therefore number convolution kernels increases ﬁrst layer ﬁnal stage. fig. additional ﬁlters high-pass branches highest layer whereas fig. skipped connection. reason include additional ﬁlter fig. verify improvement u-net additional ﬁlters highpass bands rather comes wavelet-based non-local basis. fair comparison u-net structure fig. proposed network figs. also concatenation layers stack subband signals applying ﬁltering. although figs. fig. appear similar exists fundamental diﬀerences additional high-pass connections. particular figs. exist skipped connections subbands whereas u-net structure fig. high pass ﬁltering bypass connection. accordingly explained section u-net satisfy frame condition emphasizing low-pass components. hand networks figs. satisfy frame condition conjecture high pass signals better recovered proposed network. numerical results following indeed provide empirical evidence claim. image denoising. nowadays deep cnn-based algorithms achieved great performance image denoising section therefore show proposed multiresolution deep convolutional framelet outperforms standard u-net denoising task. speciﬁcally proposed network networks trained learn noise pattern similar existing work then noise-free image obtained subtracting estimated noises. training validation diverse resolution images dataset used train proposed network fig. speciﬁcally images dataset used training validation respectively. noisy input images generated adding gaussian noise train network various noise patterns gaussian noise re-generated every epoch training. proposed network trained adam optimization momentum initial learning rate divided half every iterations reached around size patch mini-batch size used. network trained using epochs. proposed network implemented python using tensorflow library trained using geforce gaussian denoising network took days training. standard u-net structure fig. used baseline network comparison. addition red-net used another baseline network comparison. fair comparison red-net implemented using identical hyperparameters. speciﬁcally number size. networks trained conditions. evaluate trained network used peak signal-to-noise ratio structural similarity index calculated quantitative evaluation. psnr used measure quality reconstructed image deﬁned fig. u-net structure used used comparative studies. proposed multi-resolution deep convolutional framelets structures denoising in-painting experiments sparse-view reconstruction respectively. table shows quantitative comparison denoising performance. proposed network superior u-net red-net terms psnr test datasets gaussian noise speciﬁcally edge structures smoothed standard u-net red-net whereas edge structures quite accurately recovered proposed network shown fig. additional high-pass branches proposed network make image detail well recovered. results conﬁrm imposing frame condition non-local basis useful recovering high resolution image predicted theory. sparse-view reconstruction. x-ray potential risk radiation exposure main research thrust reduce radiation dose. among various approaches lowdose sparse-view recent proposal reduces radiation dose reducing number projection views however insuﬃcient projection views standard reconstruction using ﬁltered back-projection algorithm exhibits severe streaking artifacts globally distributed. accordingly researchers extensively employed compressed sensing approaches minimize total variation sparsity-inducing penalties data ﬁdelity therefore main goal experiment apply proposed network sparse view reconstruction outperforms existing approaches computational speed well reconstruction quality. address this network trained learn streaking artifacts suggested using network architecture fig. training data used patient data provided aapm dose grand challenge initial images reconstructed projection data. generate several sparse view images measurements re-generated radon operator matlab. data composed projection data views. artifact-free original images generated iradon operator matlab using projection views. input images streaking artifacts generated using iradon operator projection views respectively. sparse view images correspond donwsampling factor then network trained remove artifacts. among patient data eight patient data used training patient data used validation test conducted using remaining another patient data. corresponds baseline network comparison u-net structure fig. single resolution similar experimental set-up single resolution architecture u-net fig. except pooling unpooling used. networks trained similarly using data set. quantitative evaluation normalized mean square error peak signal-to-noise ratio proposed network trained stochastic gradient descent regularization parameter learning rate gradually reduced epoch. number epoch mini-batch data using image patch used size image patch network implemented using matconvnet toolbox table illustrates average psnr values reconstruction results various number projection views. high-pass branch network deep convolutional framelets produced consistently improved images quantitatively across view downsampling factors. moreover visual improvements proposed network remarkable. example fig. shows reconstruction results projection views. severe view downsampling reconstruction result fig. provides severely corrupted images signiﬁcant streaking artifacts. accordingly reconstruction results fig. compatible full view reconstruction results fig. particular signiﬁcant remaining streaking artifacts conventional architecture reduced using u-net shown fig. reconstruction result comparison views. number right corner represents nmse values arrow refers area noticeable diﬀerences. yellow boxes denote zoomed area. reconstruction results full projection views views. reconstruction results u-net proposed multi-resolution deep convolutional framelets. fig. however indicated arrow blurring artifacts visible fig. hand proposed network removes streaking blurring artifact shown fig. quantitative evaluation also showed proposed deep convolutional framelets minimum nmse values. reconstruction results larger number projection views fig. show reconstruction results projection views. algorithms signiﬁcantly improved compared view reconstruction. however reconstruction results single resolution fig. u-net fig. details disappeared. hand detailed structures well reconstructed proposed deep convolutional framelets shown fig. quantitative evaluation also showed proposed deep convolutional framelets minimum nmse values. zoomed area fig. also conﬁrmed ﬁndings. reconstruction result deep convolutional framelets provided realistic image whereas results somewhat blurry. experimental results clearly conﬁrmed proposed network quite universal sense used various artifact patterns. network structure retaining high-pass subbands automatically adapts resolutions even though various scale image artifacts present. image inpainting. image inpainting classical image processing problem whose goal estimate missing pixels image. image inpainting many scientiﬁc engineering applications. recently ﬁeld image inpainting dramatically changed advances cnn-based inpainting algorithms remarkable aspects approaches superior performance improvement existing methods spite ultra-fast time speed. despite stellar performance link deep learning classical inpainting fig. reconstruction result comparison views. number right corner represents nmse values arrow refers area noticeable diﬀerences. yellow boxes denote zoomed area. reconstruction results full projection views views. reconstruction results u-net proposed multi-resolution deep convolutional framelets. approaches remains poorly understood. section inspired classical frame-based inpainting algorithms show cnn-based image inpainting algorithm indeed ﬁrst iteration deep convolutional framelet inpainting inpainting performance improved multiple iterations inpainting image update steps using cnn. note resulting inpainting algorithm assumes form recursive neural network output used input another iteration. corresponding inference step based algorithm illustrated fig. building block proposed multi-resolution deep convolution framelets fig. used. performed inpainting experiments using randomly sub-sampled images. used divk dataset experiments. speciﬁcally images database used training images used validation. addition training data augmented conducting horizontal vertical ﬂipping rotation. inpainting task random sub-sampled images pixels images randomly removed images every epoch training. images rescaled values training adam optimization momentum used. learning rate generators divided half every iterations reached around size patch used mini-batch size training random missing images. since network perform inferences intermediate reconstruction images network trained respect intermediate results. therefore training procedure implemented using multiple intermediate results inputs shown fig. particular trained network according multiple stages. stage trained network using initial dataset dinit composed missing images corresponding label images. training network using dinit converged input data network training replaced proposed network implemented python using tensorflow library trained using geforce training time inpainting network randomly missing data days. shows typical image update proposed structure. iteration goes images gradually improved works image restoration network update. associated psnr graph fig. conﬁrms algorithm converges update steps. reference network comparison compared algorithm red-net fair also used proposed method. shown fig. proposed method restore details edges images much better red-net. addition psnr results table missing pixel images indicated proposed method outperformed red-net data comparable datasets. existing inpainting networks based feed-forward network theory deep convolutional framelets leads recursive neural network gradually improved image cnn-based image restoration making algorithm accurate various missing patterns. results conﬁrmed theoretical framework deep convolutional framelets promising designing deep learning algorithms inverse problems. discussions. also investigate whether theory answer current theoretical issues intriguing empirical ﬁndings machine learning community. amazingly theoretical framework gives many useful insights. rankness extended hankel matrix. theory showed deep convolutional framelet closely related hankel matrix decomposition multi-layer implementation convolutional framelet reﬁnes bases maximal energy compaction achieved using deep convolutional framelet expansion. addition previously shown insuﬃcient ﬁlter channels rank structure extended hankel matrix successive layers bounded previous layers. perspective suggests energy compaction happens across layers investigated singular value spectrum extended hankel matrix. here provide empirical evidence singular value spectrum extended hankel matrix compressed going convolutional layers. experiment used single-resolution encoder-decoder architecture shown fig. sake simplicity. hyperparameters dataset training introduced denoising experiments. since energy compaction occurs convolutional framelet coeﬃcients considered encoder part corresponding network ﬁrst module ﬁfth module. shown fig. singular value spectrum extended hankel matrix compressed going layer. conﬁrms conjecture closely related low-rank hankel matrix approximation. insights classiﬁcation networks. mathematical theory deep convolutional framelet derived inverse problems many important implications ﬁnding general deep learning networks. example conjecture classiﬁcation network corresponds encoder part deep convolutional framelets. speciﬁcally encoder part deep convolutional framelets works energy compaction classiﬁer attached encoder discriminate input signals based compressed energy distributions. similar classical classiﬁer design feature vector ﬁrst obtained dimensionality reduction algorithm support vector machine type classiﬁer used. accordingly role residual redundant channels etc. believed hold classiﬁer networks well. also important note rank structure hankel matrix determines energy distribution convolutional framelets coeﬃcients translation rotation invariant shown invariance property considered important property gives theoretical motivation mallat’s wavelet scattering network therefore important connection deep convolutional framelets wavelet scattering. however beyond scope current paper left future research. finite sample expressivity. another interesting observation perfect reconstruction directly related ﬁnite sample expressivity neural network recently appeared intriguing article providing empirical evidences traditional statistical learning theoretical approaches fail explain large neural networks generalize well practice explain this authors showed simple depth-two neural networks already perfect ﬁnite sample expressivity soon number parameters exceeds number data points conjecture perfect ﬁnite sample expressivity closely related perfect reconstruction condition saying ﬁnite sample size input reproduced perfectly using neural network. intriguing link condition ﬁnite sample expressivity needs investigation. relationship pyramidal residual network. another interesting aspect convolutional framelet analysis increases ﬁlter channels shown appear follow conventional implementation convolutional ﬁlter channels interesting article provides strong empirical evidence supporting theoretical prediction. recent paper pyramidal residual network authors gradually increase feature channels across layers. design proven eﬀective means improving generalization ability. coincides prediction order guarantee condition ﬁlter channel increase. suggests theoretical potential proposed deep convolutional framelets. revisit existing deep networks inverse problems. based theory deep convolutional framelets revisit existing deep learning algorithms inverse problems discuss pros cons. extending work kang proposed wavelet domain residual learning low-dose reconstruction shown fig. idea wavresnet apply directional wavelet transform ﬁrst neural network trained learn mapping noisy input wavelet coeﬃcients noiseless ones essence interpreted deep convolutional framelets nonlocal transform performed ﬁrst. remaining layers composed local ﬁlters residual blocks. thanks global transform using directional wavelets signal becomes compressed main source advantages compared simple cnn. another uniqueness wavresnet concatenation layer ends performs additional ﬁlters using intermediate results. layer performs signal boosting however lack pooling layers receptive ﬁeld size smaller multi-scale network shown fig. accordingly architecture better suited localized artifacts low-dose noise eﬀective removing globalized artifact patterns sparse view automated transform manifold approximation recently proposed neural network approach image reconstruction claimed general various imaging modalities etc. typical architecture given fig. architecture similar standard except ﬁrst layer nonlocal basis matrix learned fully connected layer. moreover original signal domain measurement domain local ﬁlters followed successive layer without additional fully connected layer inversion. theory learning based non-local transform optimally adapted signals believed advantageous standard cnns. however order fully connected layer nonlocal bases huge size network required. example order recover image number parameters fully connected layer shown fig. calculation required parameter numbers). thus attempts learn image size using automap required memory becomes neither possible store avoid overﬁtting learning. another reason prefer analytic non-local bases. however measurement size suﬃciently small approach automap interesting direction investigate. conclusions. paper propose general deep learning framework called deep convolutional framelets inverse problems. proposed network architectures obtained based fundamental theoretical advances achieved. first show deep learning closely related existing theory annihilating ﬁlter-based low-rank hankel matrix approaches convolution framelets. particular theory motivated observation signal lifted high dimensional hankel matrix usually results low-rank structure. furthermore lifted hankel matrix decomposed using non-local local bases. showed convolution framelet expansion equivalently represented encoder-decoder convolutional layer structure. extending idea furthermore also derived multi-layer convolution framelet expansion associated encoder-decoder network. furthermore investigated perfect reconstruction condition deep convolutional framelets. particular showed perfect reconstruction still possible framelet coeﬃcients processed relu. also proposed novel class deep network using multi-resolution convolutional framelets. discovery provided theoretical rationale many existing deep learning architectures components. addition theoretical framework address fundamental questions raised introduction. speciﬁcally showed convolutional ﬁlters work local bases number channels determined based perfect reconstruction condition. interestingly controlling number ﬁlter channels achieve low-rank based shrinkage behavior. showed relu disappear paired ﬁler channels opposite polarity available. another important novel theoretical contribution that thanks lifting hankel structured matrix show pooling un-pooling layers actually come non-local bases augmented high-pass branches meet frame condition. deep convolutional framelets also explain role by-pass connection. finally shown depth network determined considering intrinsic rank signal convolution ﬁlter length. numerical results also showed proposed deep convolutional framelets provide improved reconstruction performance various conditions. limitations current work analysis based deterministic framework mysteries deep learning probabilistic nature apply expectation complex high-dimensional image distributions. understand link deterministic probabilistic views deep learning important need explored future. prove mathematical induction. input signal need φhdψ obtain ﬁltered signal since rn×d dimension local basis matrix rd×q satisfy frame condition next assume true layer. then number input channel llayer ﬁltering operation represented φhd|p)ψ hd|p) rn×pd. thus guarantee number columns local basis least satisfy frame condition implies output channel number l-th layer concludes proof. mart´ın abadi ashish agarwal paul barham eugene brevdo zhifeng chen craig citro greg corrado andy davis jeffrey dean matthieu devin tensorﬂow large-scale machine learning heterogeneous distributed systems arxiv preprint arxiv. eirikur agustsson radu timofte ntire challenge single image super-resolution dataset study ieee conference computer vision pattern recognition workshops july antoni buades bartomeu coll morel non-local algorithm image denoising computer vision pattern recognition cvpr ieee computer society conference vol. ieee harold burger christian schuler stefan harmeling image denoising plain neural networks compete bmd? ieee conference computer vision pattern recognition ieee yunjin chen thomas pock learning optimized reaction diﬀusion processes eﬀective image restoration proceedings ieee conference computer vision pattern recognition kostadin dabov alessandro vladimir katkovnik karen egiazarian image denoising sparse transform-domain collaborative ﬁltering ieee transactions image processing donoho compressed sensing ieee trans. information theory david donoho compressed sensing ieee transactions information theory maryam fazel ting pong defeng paul tseng hankel matrix rank minimization applications system identiﬁcation realization siam journal matrix analysis applications hammernik knoll sodickson pock learning variational model compressed sensing reconstruction proceedings international society magnetic resonance medicine yoseob jaejun jong chul deep learning domain adaptation accelerated projection reconstruction magnetic resonance medicine available also arxiv preprint arxiv. kaiming xiangyu zhang shaoqing jian delving deep rectiﬁers surpassing humanlevel performance imagenet classiﬁcation proceedings ieee international conference computer vision yingbo tapan sarkar matrix pencil method estimating parameters exponentially damped/undamped sinusoids noise ieee transactions acoustics speech signal processing kyong hwan dongwook jong chul general framework compressed sensing parallel using annihilating ﬁlter based low-rank hankel matrix ieee trans. computational imaging kyong hwan ji-yong dongwook juyoung sung-hong park jong chul artifact correction using sparse+ low-rank decomposition annihilating ﬁlter-based hankel matrix magnetic resonance medicine yann lecun yoshua bengio geoffrey hinton deep learning nature dongwook kyong hwan eung yeop sung-hong park jong chul acceleration parameter mapping using annihilating ﬁlter-based rank hankel matrix magnetic resonance medicine juyoung kyong hwan jong chul reference-free single-pass nyquist ghost correction using annihilating ﬁlter-based rank hankel matrix magnetic resonance medicine st´ephane mallat wavelet tour signal processing academic press group invariant scattering communications pure applied mathematics xiaojiao chunhua shen yu-bin yang image restoration using deep convolutional encoderdecoder networks symmetric skip connections advances neural information processing systems junhong lina carlini michael unser suliana manley jong chul fast live cell imaging nanometer scale using annihilating ﬁlter-based low-rank hankel matrix approach spie optical engineering+ applications international society optics photonics v–v. deepak pathak philipp krahenbuhl jeff donahue trevor darrell alexei efros context encoders feature learning inpainting proceedings ieee conference computer vision pattern recognition poole subhaneil lahiri maithreyi raghu jascha sohl-dickstein surya ganguli exponential expressivity deep neural networks transient chaos advances neural information processing systems olaf ronneberger philipp fischer thomas brox u-net convolutional networks biomedical image segmentation international conference medical image computing computer-assisted intervention springer wenling shang kihyuk sohn diogo almeida honglak understanding improving convolutional neural networks concatenated rectiﬁed linear units international conference machine learning wenzhe jose caballero ferenc husz´ar johannes totz andrew aitken bishop daniel rueckert zehan wang real-time single image video super-resolution using eﬃcient subpixel convolutional neural network proceedings ieee conference computer vision pattern recognition shanshan wang zhenghang leslie ying peng shun feng liang dagan feng dong liang accelerating magnetic resonance imaging deep learning ieee international symposium biomedical imaging ieee", "year": 2017}