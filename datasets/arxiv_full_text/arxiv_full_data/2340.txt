{"title": "The threshold EM algorithm for parameter learning in bayesian network  with incomplete data", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Bayesian networks (BN) are used in a big range of applications but they have one issue concerning parameter learning. In real application, training data are always incomplete or some nodes are hidden. To deal with this problem many learning parameter algorithms are suggested foreground EM, Gibbs sampling and RBE algorithms. In order to limit the search space and escape from local maxima produced by executing EM algorithm, this paper presents a learning parameter algorithm that is a fusion of EM and RBE algorithms. This algorithm incorporates the range of a parameter into the EM algorithm. This range is calculated by the first step of RBE algorithm allowing a regularization of each parameter in bayesian network after the maximization step of the EM algorithm. The threshold EM algorithm is applied in brain tumor diagnosis and show some advantages and disadvantages over the EM algorithm.", "text": "abstract—bayesian networks used range applications issue concerning parameter learning. real application training data always incomplete nodes hidden. deal problem many learning parameter algorithms suggested foreground gibbs sampling algorithms. order limit search space escape local maxima produced executing algorithm paper presents learning parameter algorithm fusion algorithms. algorithm incorporates range parameter algorithm. range calculated first step algorithm allowing regularization parameter bayesian network maximization step algorithm. threshold algorithm applied brain tumor diagnosis show advantages disadvantages algorithm. assume structure known. parameter learning case divided categories. training data complete problem resolved statistic approach bayesian approach. real application find complete training data difficult various reasons. data incomplete classical approaches usually used determine parameters bayesian network include algorithm gibbs sampling disadvantages classical approaches. robust algorithm order regularize learning problem modifications needed reduce search space help escape local maxima. problems learning parameter bayesian network motivate modification existing parameter learning algorithm network structure known data incomplete. machine learning considered among essential tools making decisions solving problems affect uncertainty. science allows automation methods helps expert take effective decision several areas. work functional means artificial intelligence combines concepts learning reasoning problemsolving. recent years bayesian networks become important tools modeling uncertain knowledge. used various applications information retrieval data fusion bioinformatics classification medical diagnostics bayesian network defined variables represent actors problem edge represent conditional independence variables. called parent noted node conditionally independent nodes given parents. conditional distribution nodes described bayesian networks graphical models apply concepts daily life modeling given problem causal structure graph indicating independence different actors problem using qualitative state form conditional probability tables. clarity semantics comprehensibility humans major advantages using bayesian networks modeling applications. offer possibility causal interpretation models learning. i=…n represents range variables k=…ri describes possible states taken j=...qi ranges possible parent configurations node process learning parameters bayesian network discussed many papers. goal parameter learning find probable explain data. training data x…xn} consists instances bayesian network nodes. parameter learning quantified log-likelihood function denoted data complete following equations international journal advanced computer science applications vol. iteration ensures likelihood function increases eventually converges local maximum. cons multiple nodes admitting large number missing data method learning method converges quickly local maximum. first step algorithm starts depending arbitrary quantities missing data. second steps consist employing expectation entries maximizing respect unknown parameters. results second step used arbitrary quantities next expectation step. algorithm converges difference successive estimates smaller fixed threshold number iterations bigger fixed maximum iteration. case variables observed simplest method used statistical estimate. estimats probability event frequency occurrence event database. approach gives nijk number events database variable state parents configuration principle somewhat different bayesian estimation find parameters likely knowing data observed. using dirichlet distribution priori parameters written second algorithm gibbs sampling introduced heckerman. gibbs sampling described general method probabilistic inference. applied type graphical models whether arcs directed whether variables discrete continuous. gibbs sampling special case mcmc generates string samples accepting rejecting interesting points. words gibbs sampling consists completing sample inferring missing data available information. learning parameters gibbs sampling method converges slowly solution number hidden variables large. third algorithm robust bayesian estimator it’s composed steps bound collapse first step consists calculating lower bound upper bound parameter bayesian network. second step applications databases often incomplete. variables observed partially never. classical approaches gibbs sampling algorithms. algorithms approximate except determinate bound upper bound parameter bayesian network. considered procedure runs data recorded observations variables allows bound conditional probability variable procedure begins identifying virtual frequencies following international journal advanced computer science applications vol. estimate based valid information database. description execution phase articulated parameter bayesian network using algorithm approximate. addition bound step algorithm gives lower bound upper bound parameter network defined work consists performing optimization bayesain network parameter using algorithm verifying bound step algorithm. that threshold algorithm consists verifying constrain mentionned equation maximized maximum number observations characteristics virtual frequencies defined zero called dirichlet distribution parameters αijk. define lower bound interval detailed example mentioned shows equations calculating conditional probabilities determining minimum maximum bounds interval. phase determining minijk maxijk depends frequency observed data database virtual frequencies calculated completing records. major advantage method independence distribution missing data without trying infer. find best parameters method second phase necessary. estimates parameters using convex combination distribution calculated given node. convex combination determined either external knowledge missing data dynamic calculating parameters used like input next step threshold algorithm. principle repeated convergence. stopping points algorithm. third step used force solution bounds calculating bound step algorithm. worst case solution moving toward directions reducing violations constraint mentionned equation section compare algorithm algorithm. apply work brain tumor diagnosis. bayesian network toolbooxs murphy test algorithm. bayesian network shown created experiments. then instances collected real diagnosis mention variables instanced. dataset learn bayesian network parameters composed instances node tacked real cases collected specialist brain tumor diagnosis. nodes discrete takes values. percentage missing data dataset equal majority missing data intermediate nodes bayesian network. causes missing data quality images doctor forgot mention details report. quation allows give performance parameters calculated bayesian network. log-likelihood gives parameters best describe training set. value updated iteration algorithm. show graphic functions already same. log-likelihood th_em algorithm lower log-likelihood algorithm. test applied starting points algorithms. convergence algorithm quickly algorithm. result shown cases change starting points algorithms addition probability distribution node modified. probability bounds calculating first step algorithm error rate become smaller. advantage algorithm consists absence zero probability probability distribution. node number. node parents configuration number. number state node nijk number cases node state parents configuration θijk parameter value node state ists parents configuration convex combination bounds calculated first step algorithm external information parameters bayesian networks. task becomes difficult complex structure. proposed method deletes information conditional probability tables bayesian network. real application training data bayesian network always incomplete nodes hidden. many learning parameter algorithms suggested foreground gibbs sampling algorithms. order limit search space escape local maxima produced executing algorithm paper presents learning parameter algorithm fusion algorithms. algorithm incorporates range parameter algorithm. threshold algorithm applied brain tumor diagnosis show advantages disadvantages algorithm jensen introduction bayesian network. press active dynamic information fusion facial expression understanding image sequence. ieee transactions pattern analysis machine intelligence vol. jayech mahjoub approach using bayesian network improve content based image classification systems ijcsi international journal computer science issues vol. issue november jayech mahjoub \"clustering bayesian network improve content based image classification systems\" international journal advanced computer science applicationsspecial issue image processing analysis", "year": 2012}