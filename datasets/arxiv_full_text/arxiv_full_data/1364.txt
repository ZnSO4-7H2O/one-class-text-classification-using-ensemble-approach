{"title": "Delving Deeper into Convolutional Networks for Learning Video  Representations", "tag": ["cs.CV", "cs.LG", "cs.NE"], "abstract": "We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call \"percepts\" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations.  We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.", "text": "propose approach learn spatio-temporal features videos intermediate visual representations call percepts using gated-recurrent-unit recurrent networks method relies percepts extracted levels deep convolutional network trained large imagenet dataset. high-level percepts contain highly discriminative information tend low-spatial resolution. low-level percepts hand preserve higher spatial resolution model ﬁner motion patterns. using low-level percepts however lead high-dimensionality video representations. mitigate effect control number parameters introduce variant model leverages convolution operations enforce sparse connectivity model units share parameters across input spatial locations. empirically validate approach human action recognition video captioning tasks. particular achieve results equivalent state-of-art youtubetext dataset using simpler caption-decoder model without extra features. video analysis understanding represents major challenge computer vision machine learning research. previous work traditionally relied hand-crafted task-speciﬁc representations growing interest designing general video representations could help solve tasks video understanding human action recognition video retrieval video captionning two-dimensional convolutional neural networks exhibited state-of-art performance still image tasks classiﬁcation detection however models discard temporal information shown provide important cues videos hand recurrent neural networks demonstrated ability understand temporal sequences various learning tasks speech recognition machine translation consequently recurrent convolution networks leverage recurrence convolution recently introduced learning video representation. approaches typically extract visual percepts applying video frames feed activations order characterize video temporal variation. previous works rcns tended focus high-level visual percepts extracted top-layers. cnns however hierarchically build-up spatial invariance pooling layers figure highlights. cnns tends discard local information layers frame-to-frame temporal variation known smooth. motion video patches tend restricted local neighborhood reason argue current architectures well suited capturing motion information. instead likely focus global appearance changes shot transitions. address issue introduce novel architecture applies solely top-layer also intermediate convolutional layers. convolutional figure visualization convolutional maps successive frames video. hierarchy observe convolutional maps stable time thus discard variation short temporal windows. applying directly intermediate convolutional maps however inevitably results drastic number parameters characterizing input-to-hidden transformation convolutional maps size. hand convolutional maps preserve frame spatial topology. propose leverage topology introducing sparsity locality units reduce memory requirement. extend gru-rnn model replace fully-connected linear product operation convolution. gru-extension therefore encodes locality temporal smoothness prior videos directly model structure. evaluate solution human action recognition soomro well youtubetext video captioning dataset chen dolan experiments show leveraging percepts multiple resolutions model temporal variation improves performance baseline model respective gains action recognition video captioning. section review gated-recurrent-unit networks particular type rnn. model applied sequence inputs variable lengths. deﬁnes recurrent hidden state whose activation time dependent previous time. speciﬁcally given sequence hidden state time deﬁned nonlinear activation function. rnns known difﬁcult train exploding vanishing gradient effect however variants rnns long short term memory gated recurrent units empirically demonstrated ability model long-term temporal dependency various task machine translation image/video caption generation. paper mainly focus networks shown similar performance lstms lower memory requirement figure high-level visualization model. approach leverages convolutional maps different layers pretrained-convnet. given input convolutional gru-rnn different time-step. bottom-up connections optionally added layers form stack-gru-rcn. element-wise multiplication. update gate decides degree unit updates activation content. reset gate. sigmoid function. unit close reset gate forgets previously computed state makes unit reading ﬁrst symbol input sequence. candidate activation computed similarly traditional recurrent unit rnn. section delves main contributions work. leveraging visual percepts different convolutional levels order capture temporal patterns occur different spatial resolution. ﬁrst architecture propose apply rnns independently convolutional t−). hidden representation map. deﬁne rnns ﬁnal time step classiﬁcation layer case action recognition text-decoder caption generation. implement recurrent function propose leverage gated recurrent units grus originally introduced machine translation. model input hiddenstate hidden hidden transitions using fully connected units. however convolutional inputs tensors applying directly lead drastic number parameters. input convolutional spatial size number channels. applying directly would require input-to-hidden parameters size dimensionality hidden representation. fully-connected grus take advantage underlying structure convolutional maps. indeed convolutional maps extracted images composed patterns strong local correlation repeated different spatial locations. addition videos smooth temporal variation time i.e. motion associated given patch successive frames restricted local spatial neighborhood. embed prior model structure replace fully-connected units convolution operations. therefore obtain recurrent units sparse connectivity share parameters across different input spatial locations denotes convolution operation. formulation model parameters d-convolutional kernels. model results hidden recurrent representation feature vector deﬁned preserves spatial topology location ensure spatial size hidden representation remains ﬁxed time zero-padding recurrent convolutions. size k×k×ox×oh using convolution parameters convolutional kernel spatial size chosen signiﬁcantly lower convolutional size candidate hidden representation activation gate reset gate deﬁned based local neigborhood size location input data previous hidden-state ht−. addition size receptive ﬁeld associated increases previous presentation t−... back time. model therefore capable characterizing spatio-temporal patterns high spatial variation time. gru-rcn layer applies d-convolutions time-step assume simplicity input-to-hidden hidden-tohidden convolutions kernel size perserve input dimension gru-rcn requires multiplications. gru-rcn sparse connectivity therefore saves computation compared fully-connected would require computations. memorywise gru-rcn needs store parameters convolutions kernels leading parameters. second architecture investigate importance bottom-up connection across rnns. gru-rcn applies layer-wise gru-rnn independent fashion stacked gru-rcn preconditions gru-rnn output previous gru-rnn current previous hidden representation given extratime step input convolutional units deep learning approaches recently used learn video representations produced state-of-art results karpathy tran proposed learn video representations leveraging large training datasets sport million. however unlike image classiﬁcation cnns yield large improvement traditional methods highlighting difﬁculty learning video representations even large training dataset. simonyan zisserman introduced two-stream framework train cnns independently optical inputs. stream focuses motion information stream leverage pre-trained image datasets. based stream representation wang extracted deep feature conducted trajectory constrained pooling aggregate convolutional feature video representations. models also used encode temporal information learning video representations conjonction cnns. donahue applied two-stream framework srivastava proposed addition investigate beneﬁt learning video representation unsupervised manner. previous works topic tended focus high-level visual percepts. contrast approach proposes leverage visual percepts extracted different layers d-cnn. recently also proposed leverage convolutional units inside network. however focus different task different model based lstm. addition applied model directly pixels. here recurrent convolutional units pre-trained convolutional maps extract temporal pattern visual percepts different spatial sizes. section presents empirical evaluation proposed gru-rcn stacked gru-rcn architectures. conduct experimentations different tasks human action recognition video caption generation. evaluate approach dataset soomro dataset action classes spanning youtube videos clips. videos composing dataset subject large camera motion viewpoint change cluttered backgrounds. report results dataset ﬁrst split commonly used split literature. perform proper hyperparameter seach videos ucf-thumos validation split jiang validation set. experiment consider representations videos inputs. extract visual percept using vgg- cnns consider either inputs. vgg- cnns pretrained imagenet ﬁne-tuned ucf- dataset following protocol wang extract convolution maps pool pool pool pool layers fully-connected layer features maps given inputs models. design evaluate three architectures action recognition. ﬁrst architecture gru-rcn apply convolutional gru-rnns independently convolutional map. convolution gru-rcn zero-padded convolutions preserves spatial dimension inputs number channels respective gru-rnn hidden-representations operation obtain hidden-representations time step. apply average pooling hidden-representations last time-step reduce spatial dimension feed representations classiﬁers composed linear layer softmax nonlineary. classiﬁer therefore focuses hidden-representation extracted convolutional speciﬁc layer. classiﬁer outputs averaged ﬁnal decision. dropout ratio applied input classiﬁers. second architecture stacked gru-rcn investigate usefulness bottom-up connections. stacked gru-rcn uses base architecture gru-rcn consisting convolutional gru-rnns channels respectively. however convolutional gru-rnn preconditioned hidden-representation gru-rnn applied previous convolution-map outputs. apply max-pooling hidden representations gru-rnn layers compatibility spatial dimensions. previous architecture gru-rnn hidden-representation last time step pooled given input classiﬁer. finally bi-directional gru-rcn investigate importance reverse temporal information. given convolutional maps extracted layer gru-rcn twice considering inputs sequential reverse temporal order. concatenate last hiddenrepresentations foward gru-rcn backward gru-rcn give resulting vector classiﬁer. follow training procedure introduced two-stream framework simonyan zisserman iteration batch videos sampled randomly training set. perform scale-augmentation randomly sample cropping width height temporal cropping size resize cropped volume estimate model parameters maximizing model log-likelihood training video-action pairs function takes crop random. adam kingma gradient computed backpropagation algorithm. perform early stopping choose parameters maximize log-probability validation set. also follow evaluation protocol two-stream framework simonyan zisserman test time sample equally spaced video sub-volumes temporal size frames. selected sub-volumes obtain inputs model i.e. corners center horizontal ﬂipping. ﬁnal prediction score obtained averaging across sampled sub-volumes cropped regions. compare approach different baselines vgg- vgg- rnn. vgg- spatial stream described wang take vgg- model pretrained image-net ﬁne-tune ucf- dataset. vgg- baseline applied using fully-connected gated-recurrent units top-of vgg-. takes input vgg- fully-connected representation fc-. following gru-rcn top-layer vgg- hiddenrepresentation dimensionality ﬁrst column table focuses inputs. ﬁrst report results different gru-rcn variants compare baselines vgg- vgg- rnn. gru-rcn variants outperform baselines showing beneﬁt delving deeper order learn video representation. notice vgg- slightly improve baseline result conﬁrms top-layer tends discard temporal variation short temporal windows. stacked-gru performs signiﬁcantly lower grurcn bi-directional gru-rcn. argue bottom-up connection increasing depth model combined lack training data make stacked-gru learning difﬁcult. bi-directional gru-rcn performs best among gru-rcn variant accuracy showing advantage modeling temporal information sequential reverse order. bi-directional gru-rcn obtains gain term performances relatively baselines focus vgg- layer. table also reports results state-of-art approaches using inputs. tran obtains best performance ucf- however noted trained million videos. approaches videos training learning temporal pattern. bi-directional gru-rcn compare favorably recurrent convolution network conﬁrming beneﬁt using different layers model temporal variation. table also evaluates gru-rcn model applied inputs. vgg- baseline actually decreases performance compared vgg- baseline. hand gru-rcn outperforms vgg- baseline achieving improvement less important stream noted stream vgg- applied consecutive inputs extract visual percepts therefore already captures motion information. finally investigate combination streams. following wang weighted linear combination prediction scores weight stream temporal stream. fusion vgg- model baseline achieve accuracy combining bi-directional gru-rcn gru-rcn achieves performance gain baseline reaching model part wang obtain state-of-art results using streams obtains also evaluate representation video captioning task using youtubetext video corpus chen dolan dataset video clips multiple natural language descriptions video clip. dataset open-domain covers wide range topics sports animals music movie clips. following split dataset training video clips validation clips test consisting remaining clips. perform video captioning so-called encoder-decoder framework framework encoder maps input videos abstract representations precondition caption-generating decoder. encoder compare vgg- bi-directional gru-rcn. models ﬁne-tuned ucf- dataset therefore focus detecting actions. extract abstract representation video sample equally-space segments. using encoder provide layer activations segment’s ﬁrst frame input text-decoder. gru-rcn apply model segment’s ﬁrst frames. concatenate gru-rcn hidden-representation last time step. concatenated vector given input text decoder. shown characterizing entities addition action important caption-generation task also encoder szegedy pretrained imagenet focuses detecting static visual object categories. model vgg- encoder bi-directional gru-rcn encoder googlenet encoder googlenet bi-directional gru-rcn encoder googlenet bi-directional gru-rcn encoder googlenet bi-directional gru-rcn encoder googlenet bi-directional gru-rcn encoder googlenet hrne p-rnn p-rnn soft-attention venugopalan venugopalan extra data thomason thomason table performance different variants model youtubetext video captioning. representations obtained proposed architecture combined decoders offer signiﬁcant performance boost reaching performance state-ofthe-art models. training video-description pairs description words long. used adadelta zeiler optimized hyperparameters using random search maximize log-probability validation set. table reports performance proposed method using three automatic evaluation metrics. bleu papineni meteor denkowski lavie cider vedantam evaluation script prepared introduced chen models early-stopped based negative-log-likelihood validation set. select model performs best validation according metric consideration. ﬁrst lines table compare performances vgg- bi-directional grurcn encoder. results clearly show superiority bi-directional gru-rcn encoder outperforms vgg- encoder three metrics. particular gru-rcn encoder obtains performance gain compared vgg- encoder according bleu metric. combining gru-rcn encoder focuses action googlenet encoder captures visual entities improve performances. googlenet bi-directional gru-rcn approach signiﬁcantly outperforms soft-attention relies googlenet cuboids-based d-cnn encoder conjunction similar soft-attention decoder. result indicates approach able offer effective representations. according bleu metric also outperform approaches using complex decoder schemes spatial temporal attention decoder hierarchical decoder approach without need using cd-encoder requires training large-scale video dataset. work address challenging problem learning discriminative abstract representations videos. identify underscore importance modeling temporal variation visual percepts different spatial resolutions. high-level percepts contain highly discriminative information tend low-spatial resolution. low-level percepts hand preserve higher spatial resolution model ﬁner motion patterns. introduce novel recurrent convolutional network architecture leverages convolutional maps levels deep convolutional network trained imagenet dataset take advantage percepts different spatial resolutions. empirically validated approach human action recognition video captioning tasks using ucf- youtubetext datasets. experiments demonstrate leveraging percepts multiple resolutions model temporal variation improve baseline model respective gain action recognition video captions tasks using inputs. particular achieve results comparable state-of-art youtubetext using simpler text-decoder model without extra features. authors would like acknowledge support following agencies research funding computing support nserc calcul qu´ebec compute canada canada research chairs cifar. would also like thank developers theano developing powerful tool scientiﬁc computing. bastien fr´ed´eric lamblin pascal pascanu razvan bergstra james goodfellow bergeron arnaud bouchard nicolas bengio yoshua. theano features speed improvements. deep learning unsupervised feature learning nips workshop bergstra james breuleux olivier bastien fr´ed´eric lamblin pascal pascanu razvan desjardins guillaume turian joseph warde-farley david bengio yoshua. theano math expression compiler. proceedings python scientiﬁc computing conference chen david dolan william collecting highly parallel data paraphrase evaluation. proceedings annual meeting association computational linguistics human language technologies-volume association computational linguistics chen xinlei fang tsung-yi vedantam ramakrishna gupta saurabh dollar piotr zitnick lawrence. microsoft coco captions data collection evaluation server. arxiv merri¨enboer gulcehre bahdanau bougares schwenk bengio learning phrase representations using encoder-decoder statistical machine translation. chung junyoung gulcehre caglar kyunghyun bengio yoshua. empirical evaluation gated recurrent neural networks sequence modeling. arxiv preprint arxiv. donahue hendricks guadarrama rohrbach venugopalan saenko darrell long-term recurrent convolutional networks visual recognition description. arxiv preprint arxiv. yue-hei hausknecht matthew vijayanarasimhan sudheendra vinyals oriol monga rajat toderici george. beyond short snippets deep networks video classiﬁcation. arxiv preprint arxiv. pingbo zhongwen yang zhuang yueting. hierarchical recurrent neural encoder video representation application captioning. arxiv preprint arxiv. xingjian chen zhourong wang yeung dit-yan wong wai-kin wangchun. convolutional lstm network machine learning approach precipitation nowcasting. arxiv preprint arxiv. szegedy christian yangqing sermanet pierre reed scott anguelov dragomir erhan dumitru vanhoucke vincent rabinovich andrew. going deeper convolutions. arxiv preprint arxiv. thomason jesse venugopalan subhashini guadarrama sergio saenko kate mooney raymond. integrating language vision generate natural language descriptions videos wild. coling venugopalan subhashini huijuan donahue jeff rohrbach marcus mooney raymond saenko kate. translating videos natural language using deep recurrent neural networks. naacl torabi atousa kyunghyun ballas nicolas christopher larochelle hugo courville aaron. describing videos exploiting temporal structure. computer vision ieee international conference ieee", "year": 2015}