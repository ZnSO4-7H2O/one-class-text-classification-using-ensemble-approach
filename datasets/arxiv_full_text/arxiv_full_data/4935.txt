{"title": "Overcoming Catastrophic Forgetting by Incremental Moment Matching", "tag": ["cs.LG", "cs.AI"], "abstract": "Catastrophic forgetting is a problem of neural networks that loses the information of the first task after training the second task. Here, we propose a method, i.e. incremental moment matching (IMM), to resolve this problem. IMM incrementally matches the moment of the posterior distribution of the neural network which is trained on the first and the second task, respectively. To make the search space of posterior parameter smooth, the IMM procedure is complemented by various transfer learning techniques including weight transfer, L2-norm of the old and the new parameter, and a variant of dropout with the old parameter. We analyze our approach on a variety of datasets including the MNIST, CIFAR-10, Caltech-UCSD-Birds, and Lifelog datasets. The experimental results show that IMM achieves state-of-the-art performance by balancing the information between an old and a new network.", "text": "catastrophic forgetting problem neural networks loses information ﬁrst task training second task. here propose method i.e. incremental moment matching resolve problem. incrementally matches moment posterior distribution neural network trained ﬁrst second task respectively. make search space posterior parameter smooth procedure complemented various transfer learning techniques including weight transfer l-norm parameter variant dropout parameter. analyze approach variety datasets including mnist cifar- caltech-ucsdbirds lifelog datasets. experimental results show achieves state-of-the-art performance balancing information network. catastrophic forgetting fundamental challenge artiﬁcial general intelligence based neural networks. models stochastic gradient descent often forget information previous tasks trained task online multi-task learning handles problems described continual learning. classic problem resurfaced renaissance deep learning research recently concept applying regularization function network trained task learning task received much attention. approach interpreted approximation sequential bayesian representative examples regularization approach include learning without forgetting elastic weight consolidation algorithms succeeded experiments assumption regularization function problem. here propose incremental moment matching resolve catastrophic forgetting problem. uses framework bayesian neural networks implies uncertainty introduced parameters neural networks posterior distribution calculated dimension random variable posterior distribution number parameters neural networks. approximates mixture gaussian posterior component representing parameters single task gaussian distribution combined task. merge posteriors introduce novel methods moment matching. mean-imm simply averages parameters networks tasks minimization average kl-divergence approximated posterior distribution combined task figure geometric illustration incremental moment matching mean-imm simply averages parameters neural networks whereas mode-imm tries maximum mixture gaussian posteriors. make reasonable search space loss function posterior means reasonably smooth convex-like. satisﬁes condition smooth convex-like path propose applying various transfer techniques procedure. gaussian posterior single task mode-imm merges parameters networks using laplacian approximation approximate mode mixture gaussian posteriors represent parameters networks. general naïve assume ﬁnal posterior distribution whole task gaussian. make work search space loss function posterior means needs smooth convex-like. words high cost barriers means networks task. make assumption gaussian distribution neural network reasonable applied three main transfer learning techniques procedure weight transfer l-norm parameters newly proposed variant dropout using parameters. whole procedure illustrated figure major approaches preventing catastrophic forgetting ensemble neural networks. task arrives algorithm makes network shares representation tasks however approach complexity issue especially inference number networks increases number tasks need learned increases. another approach studies methods using implicit distributed storage information typical stochastic gradient descent learning. methods idea dropout maxout neural module distributively store information task making large capacity neural network unfortunately studies following approach limited success failed preserve performance task extreme change environment occurred alternatively fernando proposed pathnet extends idea ensemble approach parameter reuse within single network. pathnet neural network twenty modules layer three four modules picked task layer evolutionary approach. method alleviates complexity issue ensemble approach continual learning plausible way. approach regularization term also received attention. learning without forgetting example approach uses pseudo-training data task learning task puts training data task network uses output pseudo-labels pseudo-training data. optimizing pseudotraining data task real data task attempts prevent catastrophic forgetting. framework promising properties pseudo training similar ideal training set. elastic weight consolidation another example approach uses sequential bayesian estimation update neural networks continual learning posterior distribution trained previous task used update prior distribution. prior used learning posterior distribution task bayesian manner. assumes covariance matrix posterior diagonal correlations nodes. though assumption fragile performs well domains. monumental recent work uses sequential bayesian continual learning neural networks. however updating parameter complex hierarchical models sequential bayesian estimation sequential bayes used learn topic models stream data broderick huang applied sequential bayesian adapt deep neural network speciﬁc user speech recognition domain assigned layer user adaptation applied estimation single layer. similar method bayesian moment matching used sum-product networks kind deep hierarchical probabilistic model though sum-product networks usually scalable large datasets online learning method useful achieves similar performance batch learner. method using moment matching focuses continual learning deals signiﬁcantly different statistics tasks unlike previous method. incremental moment matching moments posterior distributions matched incremental way. work gaussian distribution approximate posterior distribution parameters. given sequential tasks want optimal parameter gaussian approximation function posterior parameter task denotes approximation true posterior distribution whole task denotes approximation true posterior distribution training dataset task. denotes vectorized parameter neural network. dimension dimension respectively dimension example multi-layer perceptrons number nodes including bias terms. next explain proposed moment matching algorithms continual learning modern deep neural networks. algorithms generate different moments gaussian different objective functions dataset. optimal solution local kl-distance. notice covariance information needed mean-imm since calculating require series sufﬁcient perform task. idea mean-imm commonly used shallow networks however contribution paper discover mean-imm applied modern deep neural networks show performs better transfer techniques. future works include measures merge networks including kl-divergence bemode-imm variant mean-imm uses covariance information posterior gaussian distribution. general weighted average mean vectors gaussian distributions mode mog. discriminative learning maximum distribution primary interest. according lindsay modes clusters appendix details. motivated description mode-imm approximate laplacian approximation logarithm function expressed taylor expansion using laplacian approximation approximated follows equation term inverted practice identity matrix small constant here assume diagonal covariance matrices means correlation among parameters. diagonal assumption useful since decreases number parameters covariance matrix dimension parameters covariance inverse fisher information matrix following main idea approximation square gradients parameters good indicator precision inverse variance. fisher information matrix task deﬁned general loss function neural networks convex. consider shufﬂing nodes weights neural network preserves original performance. parameters neural networks initialized independently averaged might perform poorly high cost barriers parameters neural networks however show various transfer learning techniques used ease problem make assumption gaussian distribution neural networks reasonable. section introduce three practical techniques including weight-transfer l-transfer drop-transfer. weight-transfer initialize parameters task parameters previous task experiments weight-transfer critical continual learning performance. reason experiments paper weight-transfer technique default. weight-transfer technique motivated geometrical property neural networks discovered previous work found straight path initial point solution without high cost barrier various types neural networks datasets. discovery suggests weight-transfer previous task task makes smooth loss figure experimental results visualizing effect weight-transfer. geometric property parameter space neural network analyzed. brighter better. vectorized parameters trained networks randomly selected subsets cifar- dataset. ﬁgure shows better solutions three locally optimized parameters. surface solutions tasks optimal solution tasks lies interpolated point solutions. empirically validate concept weight-transfer linear path analysis proposed goodfellow randomly chose instances training dataset cifar- divided three subsets instances each. three subsets used sequential training models parameterized respectively. here initialized initialized weight-transfer. analysis loss accuracy evaluated series points varying figure loss surface model online subset nearly solution convex. ﬁgure shows parameter mean-imm performs better reference points however initialized convex-like shape disappears since high cost barrier loss function l-transfer variant l-regularization. l-transfer interpreted special case prior distribution gaussian covariance matrix. l-transfer regularization term distance added following objective function ﬁnding hyperparameter concept l-transfer commonly used transfer learning continual learning large unlike previous usage large small procedure. words ﬁrst trained equation small merged imm. since want make loss surface smooth minimize distance convex optimization l-regularizer makes convex function strictly convex. similarly hope l-transfer small help convexlike loss space drop-transfer novel method devised paper. drop-transfer variant dropout zero point dropout procedure. training phase following ˆµki used weight vector corresponding node table averaged accuracies disjoint mnist sequential tasks shufﬂed mnist three sequential tasks untuned setting refers natural hyperparameter equation algorithm whereas tuned setting refers using heuristic hand-tuned hyperparameters. hyperparam denotes main hyperparameter algorithm. transfer tuned. numbers parentheses refer standard deviation. every uses weight-transfer. disjoint mnist experiment l-transfer drop-transfer mean-imm mode-imm l-transfer mean-imm l-transfer mode-imm drop-transfer mean-imm drop-transfer mode-imm drop-transfer mean-imm drop-transfer mode-imm shufﬂed mnist experiment l-transfer drop-transfer mean-imm mode-imm l-transfer mean-imm l-transfer mode-imm drop-transfer mean-imm drop-transfer mode-imm drop-transfer mean-imm drop-transfer mode-imm studies interpreted dropout exponential ensemble weak learners. perspective since marginalization output distribution whole weak learner intractable parameters multiplied inverse dropout rate used test phase procedure. words parameters weak learners effect simply averaged oversampled learners dropout. process drop-transfer continual learning setting hypothesize dropout process makes averaged point arbitrary sampled points using equation good estimator. investigated search space loss function trained mnist handwritten digit recognition dataset without dropout regularization supplement evidence described hypothesis. dropout regularization makes accuracy sampled point dropout distribution average point sampled parameters respectively. case without dropout space arbitrary samples empirically convex second-order equation. based experiment expect search space loss function modern neural networks easily nearly convex also regularizers dropout make search space smooth point search space good accuracy continual learning. evaluate approach four experiments whose settings intensively used previous works details experimental results appendix source code experiments available github repository. disjoint mnist experiment. ﬁrst experiment disjoint mnist experiment experiment mnist dataset divided datasets ﬁrst dataset consists digits second dataset consists remaining digits task figure test accuracies various transfer techniques disjoint mnist. ltransfer drop-transfer boost performance make optimal value larger however drop-transfer tends make accuracy curve smooth l-transfer does. class joint categorization unlike setting previous work considers independent tasks -class categorization. inference decide whether instance comes ﬁrst second task task difﬁcult task previous work. evaluate models untuned setting tuned setting. untuned setting refers natural hyperparameter equation algorithm. tuned setting refers using heuristic hand-tuned hyperparameters. consider tuned hyperparameter setting often used previous works continual learning difﬁcult deﬁne validation setting. example model needs learn task learning task learning rate early stopping without validation arbitrary hyperparameter balancing used discover hyperparameters tuned setting oracle performance algorithm also show exist paths consisting point performs reasonably tasks. hyperparam table denotes hyperparameter mainly searched tuned setting. table figure shows experimental results disjoint mnist experiment. experimental setting usual sgd-based optimizers always perform less biases output layer task always pushed large negative values implies task difﬁcult. figure also shows mode-imm robust optimal mean-imm larger disjoint mnist experiment. shufﬂed mnist experiment. second experiment shufﬂed mnist experiment three sequential tasks. experiment ﬁrst dataset original mnist dataset. however second dataset input pixels images shufﬂed ﬁxed random permutation. previous work reaches performance level batch learner argued overcomes catastrophic forgetting domains. experimental details similar disjoint mnist experiment except models allowed dropout regularization. experiment ﬁrst dataset original mnist dataset. however second third dataset input pixels images shufﬂed ﬁxed random permutation respectively. therefore difﬁculty three datasets same though different solution required dataset. table figure shows experimental results shufﬂed mnist experiment. notice accuracy drop-transfer alone l-transfer drop-transfer alone results competitive without dropout whose performance around imagenet dataset. third experiment imagenetcub experiment continual learning problem imagenet dataset caltech-ucsd birds-- ﬁnegrained classiﬁcation dataset numbers classes imagenet dataset around numbers training instances respectively. imagenetcub experiment last-layer separated imagenet task. structure alexnet used trained model imagenet experiment match moments last-layer ﬁne-tuning model model mean-imm modeimm. figure shows mean-imm moderately balances performance tasks networks. however balanced hyperparameter mode-imm think scale fisher matrix different imagenet task. since number training data tasks different mean square gradient deﬁnition tends different. implies assumption mode-imm always hold heterogeneous tasks. appendix information including learning methods different class output layer different scale dataset used. results exceed previous state-of-the-art performance whose model also lwf. because previous works model initialized last-layer ﬁnetuning model directly original alexnet. case performance loss task decreased also performance gain task decreased. accuracies mean-imm imagenet task task respectively. gains compared previous state-of-the-art case mean-imm mode-imm accuracies respectively. lifelog dataset. lastly evaluate proposed methods lifelog dataset lifelog dataset consists instances egocentric video stream data collected days three participants using google glass three class categories location sub-location activity labeled frame video. lifelog dataset class distribution changes continuously classes appear passes. table shows mean-imm mode-imm competitive dual-memory architecture previous state-of-the-art ensemble model even though uses single network. shift optimal hyperparameter imm. tuned setting shows often exists makes performance mean-imm close mode-imm. however untuned hyperparameter setting mean-imm performs worse transfer techniques applied. bayesian interpretation assumes training k-th network mainly affected k-th task rarely affected information previous tasks. however transfer techniques break assumption; thus optimal shifted larger fortunately mode-imm works robustly mean-imm transfer techniques applied. figure illustrates change test accuracy curve corresponding applied transfer techniques following shift optimal mean-imm mode-imm. bayesian approach continual learning. kirkpatrick interpreted fisher matrix weight importance explaining model. shufﬂed mnist experiment since large number pixels always value zero corresponding elements fisher matrix also zero. therefore work allowing weights change used previous tasks. hand mode-imm also works selectively balancing weights using variance information. however assumptions weight importance always hold especially disjoint mnist experiment. important weight disjoint mnist experiment bias term output layer. nevertheless bias parts fisher matrix guaranteed highest value used balance class distribution ﬁrst second task. believe using diagonal covariance matrix bayesian neural networks naïve general failed disjoint mnist experiment. think could alleviated future work using complex prior matrix gaussian distribution considering correlations nodes network balancing information task. procedure produces neural network without performance loss task better ﬁnal solution terms performance task. furthermore easily weigh importance tasks models real time. example easily changed solution meanαtµt actual service situations companies importance task frequently changes real time handle problem. property differentiates continual learning methods using regularization approach including ewc. contributions four folds. first applied mean-imm continual learning modern deep neural networks. mean-imm makes competitive results comparative models balances information network. also interpreted success bayesian framework gaussian posterior. second extended mean-imm mode-imm interpretation mode-ﬁnding mixture gaussian posterior. mode-imm outperforms mean-imm comparative models various datasets. third introduced drop-transfer novel method proposed paper. experimental results showed drop-transfer alone performs well similar without dropout domain rarely forgets. fourth applied various transfer techniques procedure make assumption gaussian distribution reasonable. argued search space loss function among neural networks easily nearly convex also regularizers dropout make search space smooth point search space good accuracy. experimental results showed applying transfer techniques often boost performance imm. overall made state-of-theart performance various datasets continual learning explored geometrical properties bayesian perspective deep neural networks. authors would like thank jiseob min-oh donghyun kwak insu jeon christina baek heidi tessmer helpful comments editing. work supported naver corp. partly korean government byoung-tak zhang corresponding author.", "year": 2017}