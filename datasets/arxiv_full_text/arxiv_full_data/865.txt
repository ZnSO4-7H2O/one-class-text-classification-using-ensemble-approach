{"title": "Knowledge Transfer Pre-training", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Pre-training is crucial for learning deep neural networks. Most of existing pre-training methods train simple models (e.g., restricted Boltzmann machines) and then stack them layer by layer to form the deep structure. This layer-wise pre-training has found strong theoretical foundation and broad empirical support. However, it is not easy to employ such method to pre-train models without a clear multi-layer structure,e.g., recurrent neural networks (RNNs). This paper presents a new pre-training approach based on knowledge transfer learning. In contrast to the layer-wise approach which trains model components incrementally, the new approach trains the entire model as a whole but with an easier objective function. This is achieved by utilizing soft targets produced by a prior trained model (teacher model). Compared to the conventional layer-wise methods, this new method does not care about the model structure, so can be used to pre-train very complex models. Experiments on a speech recognition task demonstrated that with this approach, complex RNNs can be well trained with a weaker deep neural network (DNN) model. Furthermore, the new method can be combined with conventional layer-wise pre-training to deliver additional gains.", "text": "learning deep neural networks. existing pre-training methods train simple models stack layer layer form deep structure. layerwise pre-training found strong theoretical foundation broad empirical support. however easy employ method pre-train models without clear multi-layer structure e.g. recurrent neural networks paper presents pre-training approach based knowledge transfer learning. contrast layer-wise approach trains model components incrementally approach trains entire model whole easier objective function. achieved utilizing soft targets produced prior trained model compared conventional layerwise methods method care model structure used pre-train complex models. experiments speech recognition task demonstrated approach complex rnns well trained weaker deep neural network model. furthermore method combined conventional layer-wise pretraining deliver additional gains. deep learning gained signiﬁcant success wide range applications example automatic speech recognition typical deep models used include deep neural networks recurrent neural networks success deep models largely attributed various pre-training approaches alleviate under-ﬁtting over-ﬁtting problem hindered development complex neural models long time. well-known pre-training methods layer-wise train simple models stack layer layer form deep structure. pre-training mostly unsupervised usually followed ﬁnetuning step reﬁnes model supervised fashion. popular pre-training approaches based restricted boltzmann machines auto-associators respectively. basic idea layer-wise pre-training divide hard deep learning task easier tasks training simpler shallow models. theoretical analysis role deep model training presented bengio zhiyuan tang dong wang yiqiao zhiyong zhang center speech language technology research institute information technology tsinghua university division technical innovation development tsinghua national laboratory information science technology. zhiyuan tang also chengdu institute computer applications chinese academy sciences university chinese academy sciences. yiqiao also school information communication engineering beijing university posts telecommunications. emails tangzycslt.riit.tsinghua.edu.cn; wangdongmails.tsinghua.edu.cn; {panyq zhangzy}cslt.riit.tsinghua.edu.cn. empirical analysis provided erhan studies show layer-wise pre-training plays role regularization locates model ‘good’ place parameter space succeeding supervised training easy good local minimum. recently effectiveness layer-wise pre-training proved paul using group theory colleagues reported layer-wise discriminative pretraining obtain similar performance layer-wise unsupervised pre-training widely accepted layer-wise pre-training makes contribution ways discover hierarchical patterns invariant high-level feature obtained; initialize deep models health state supervised training conducted effectively. although theoretically sound empirically effective layer-wise pre-training limited multi-layer models. models without clear layer-wise structure easily pre-trained existing methods. example model involve clear layer-wise structure model complicated hidden-hidden connections. pre-train model either ad-hoc treatment required special pre-training model needs designing. example vinyals proposed two-stage approach ﬁrst stage hidden-hidden connections forward paths trained second stage entire network optimized. approach obviously suboptimal since recurrent path pre-trained together forward path. pasa colleagues constructed linear autoencoder sequential data pre-train model. linear autoencoder model exactly matches structure parameters jointly pretrained. following idea boulanger-lewandowski proposed recurrent temporal model match structure. task-speciﬁc pre-training models need speciﬁcally designed certainly ideal. moreover target model complex e.g. cross-layer connections would difﬁcult design appropriate pretraining model training model unsupervised learning often prohibit task. paper presents simple powerful pre-training approach based knowledge transfer largely motivated logit matching approach dark knowledge distiller model hinton colleagues basic idea well-trained model involves rich knowledge used guide training models. hinton’s work idea applied learn simple models complex models model ensembles applied idea train small dnns large complex show paper knowledge transfer general approach used different way. instead learning simple models complex models used pre-train complex models using simpler model. speciﬁcally possible train simple model model teacher guide training complex model normally difﬁcult accomplish. teacher model might rather weak sufﬁcient direct child model teacher model helps child model reach reasonable place parameter space child model learn ﬁnally ﬁnds good local optimum delivering performance even better teacher model. weak teacher strategy rather different idea logit matching dark knowledge distillation proposed teacher model plays role ‘supervisor’ instead ‘teacher’ teaching process essentially pre-training. self-learning child model correspondingly ﬁne-tuning. fact teaching processing dark knowledge distillation teacher model ﬁrstly trained used generate targets training data. targets actually posterior probabilities ‘soft’ compared original onehot ‘hard’ targets. soft targets used train child model. using soft targets leads smoother objective function makes pre-training much easier task training original hard targets. experiments task aurora database demonstrated three interesting ﬁndings knowledge transfer pre-training used train rnns challenging conventional methods; pre-training weak teacher model; combining knowledge transfer pre-training conventional pre-training delivers additional gains. reset paper organized follows. section brieﬂy discusses related works presents knowledge transfer pre-training. section presents experiments paper concluded section study directly motivated work dark knowledge distillation hinton important distinction simple models teach complex models. teacher model work fact knows much sufﬁcient provide rough guide important train complex models highly deep dnns multilayer rnns. precisely existing methods teacher model knowledge source method uses teacher model pre-training. work also related fitnets approach proposed romero teacher model used supervise learning another network different structure e.g. deeper fatter. particularly learned hidden layers instead output layers advantage transferring hierarchical knowledge child models. approach focuses learning output layer consider internal structure teacher model truly ‘blind learning’. offers ﬂexibility preanother related work hmm-based pre-training approach recently proposed pasa colleagues work authors train model trained model generate training data. generated data used pre-train models. approach shares idea knowledge transfer pre-training work. main difference knowledge transfer pasa’s approach based randomly sampled data essentially simulates joint distribution data target labels; whereas approach based targets predicted teacher model simulates conditional distribution targets given data. idea well-trained model used teacher help training models proposed hinton basic assumption teacher model learns rich knowledge training data knowledge used guide training child models simple hence unable learn many details without teacher’s guide. distill knowledge teacher model logit matching approach proposed teaches child model encouraging logits close generated teacher model terms square error dark knowledge distiller model proposed hinton encourages output child model close teacher model terms cross entropy. knowledge transfer idea applied learn simple models complex models simple model approach performance complex model focus dark knowledge distiller model rather logit matching showed better performance experiments. model uses well-trained teacher model predict targets training samples targets used train child model. predicted targets actually posterior probabilities targets associated output soft class identities targets deterministic original one-hot hard targets. make targets softer temperature introduced scale logits. formulated indexes target classes. argued hinton larger allows information non-targets distilled. original proposal knowledge transfer used train simple models complex model goal achieve light-weighted model approach performance complex model. argue paper knowledge transfer general method used pre-train complex models simple model. training original hard targets. intuitively soft targets offer probabilistic class labels deterministic hard targets. matches real situation uncertainty always exists classiﬁcation tasks. example speech recognition often difﬁcult identify phone class frame effect co-articulation. moreover uncertainty associated soft targets blurs decision boundary correct incorrect targets. smoothness associated soft targets stated argued soft targets result less variant gradients training samples. equal objective function smooth. smooth objective function certainly much easier optimize case targets extremely soft objective function becomes optimization trivial. ease training soft targets used simplify training complex models. generally speaking complex models involve large amount parameters complex dependencies among variables leads twisted objective functions hard optimize solve problem conventional layer-wise pre-training breaks complex model simpler models easily trained individually stack back form complex model. smoothness objective functions offered knowledge transfer learning form soft targets provides different simplify complex model training instead breaking complex model simple models replace twisted objective function smoother using soft targets training model. approach difﬁculty complex model training greatly reduced optimization conducted entire model instead single layer layer-wise pre-training. long smoothed objective function possesses similar trend original objective function gradients training smoothed function would result good initialization model parameters. targets ultimate goal model training ﬁne-tuning step required reﬁne model original hard targets. sense knowledge transfer learning pre-training step initializes model parameters ﬁnetuning good starting point reach better local minimum compared training hard targets beginning. knowledge transfer pre-training related curriculum training approach discussed training samples easy learn ﬁrstly selected model training difﬁcult samples selected later model strong enough. knowledge transfer pre-training soft targets regarded easy samples ﬁrstly used hard targets difﬁcult samples used later highlight knowledge transfer pre-training teacher model necessarily strong. goal pre-training provide good initialization ﬁne-tuning instead knowledge transfer model another model reasonable quality sufﬁcient teacher although intelligent teachers generally welcome. conﬁrmed advantage layer-wise pre-training discover hierarchical patterns input signal unsupervised learning. hierarchical patterns discovering desirable several reasons consistent information processing strategy human brains invariant high-level features robust noise corruption. potential problem layer-wise pre-training however patterns learned unsupervised fashion means purely derived statistics without considering task hand. example speech recognition less frequently occurred patterns rare consonant phones difﬁcult discover however important recognition tasks. knowledge transfer pre-training hand purely supervised high greedy towards target task. additionally approach pre-trains entire model tends fast. finally pre-train models without clear multi-layer structures. disadvantage functional mimic teacher model without considering internal structure teacher model. therefore neither discover hierarchical patterns learn teacher model. interesting idea combine different types pretraining methods. example layer-wise pretraining discover hierarchical patterns knowledge transfer pre-training promote patterns important task. simple approach investigated paper employ pre-training knowledge transfer pre-training sequentially advantages methods leveraged. proposed knowledge transfer pre-training applied train acoustic models systems. ﬁrst experiment knowledge transfer pre-training used train rnns teacher model. second experiment knowledge transfer pre-training compared pretraining layer-by-layer supervised pre-training combination knowledge transfer pre-training pretraining also investigated. experiments conducted aurora database noisy conditions data proﬁle largely standard utterances model training utterances development utterances testing. kaldi toolkit used conduct model training performance evaluation process largely follows aurora recipe gpu-based training. speciﬁcally training starts constructing system based gaussian mixture models standard -dimensional mfcc features plus ﬁrstsecond-order derivatives. system trained alignment provided system. feature used system dimensional fbanks. symmetric -frame window applied concatenate neighboring frames transform used reduce feature dimension forms input. architecture involves hidden layers layer consists units. output layer composed units equal total number gaussian mixtures system. cross entropy used training criterion stochastic gradient descendent algorithm employed perform training. train acoustic models model baseline system used teacher model. based lstm structure input features -dimensional fbanks output units correspond gaussian mixtures model. momentum empirically starting learning rate default. experimental results reported table performance evaluated terms criteria frame accuracy word error rate related training criterion important speech recognition. table reported training cross validation reported test set. table ‘rnn baseline trained hard targets directly. ‘rnn denotes systems knowledge transfer pre-training ‘rnn denotes systems knowledge transfer pre-training ﬁnetuning. settings temperature evaluated performance lstm layers reported respectively. problem largely solved knowledge transfer pre-training. seen table pre-training only systems obtain equal even better performance comparison baseline means knowledge learned helps models move local minima caused complex objective function. paying attention results seen pre-training improve training better better wers test obtained. indicates pre-training leads models generalizable respect datasets evaluation metrics. ﬁnetuning hard targets performances systems signiﬁcantly improved. additionally found larger leads worse training datasets better wers test dataset. indicates knowledge transfer pre-training contributes delivering generalizable model instead optimized model. comparing rnns involve lstm layers found layers lstms deliver better performance. note layers lstms rather complex structure pre-training layer-wise unsupervised models rather difﬁcult. knowledge transfer pre-training rather simple. results observed baseline beat baseline terms although much effort devoted calibrate training process including various trials different learning rates momentum values. consistent results published kaldi recipe. note mean rnns inferior dnns. results clear models better terms frame accuracy. unfortunately advantage propagated results test set. additionally results shown interpreted rnns suitable another interesting investigation weak teacher model conduct pre-training. hidden layer units trained used teacher model. model much weaker baseline involves hidden layers. results presented table show even weak model pre-training works fairly well although well original strong teacher model. results conﬁrm conjecture teacher model necessarily strong. principle role teacher teach details student correct direction student learn itself. compared model much simpler. extensive research recent years training dnns problem more. example speech recognition training model layers rather simple even without pre-training techniques experiment apply various pre-training methods train models. goal demonstrate necessity pre-training better comparison baseline involves hidden layers layer involves units more employed input feature. setting makes training little difﬁcult less hidden units need discriminative input larger feature vectors. original baseline pre-training methods didn’t show much help particularly layer-wise methods. rbmbased pre-training layer-by-layer discriminative pretraining proposed knowledge transfer pretraining. knowledge transfer pre-training teacher models tested baseline involves hidden layers simpler model hidden layer note knowledge transfer pre-training classiﬁcation layer needs re-initialized randomly pre-training otherwise would difﬁcult ﬁnetuning achieve reasonable improvement. results reported table iii. observed pre-training knowledge transfer pretraining achieve signiﬁcant performance improvement performance rather similar weak one-layer teacher model result slightly worse four-layer still rather good. layer-by-layer discriminative pre-training show much contribution experiment. results demonstrate knowledge transfer pre-training works least well state-of-theart layer-wise pre-training methods. approach knowledge transfer approach combined approach conducts layer-by-layer unsupervised pre-training knowledge transfer approach conducts supervised pre-training entire network. fine tuning ﬁnally conducted achieve best model. results shown table well. seen combination leads best performance obtain task. demonstrates pre-training methods complementary combination leverage respective advantage. pre-training methods initialize complex network stacking simple models layer layer knowledge transfer pre-training conducts initialization smooth objective function. supervised pre-training taskoriented entire-network pre-training faster. experimental results task demonstrated pre-training approach effectively help training complex models even weak teacher model. example model successfully used pretrain model. compared pre-training layer-by-layer discriminative pre-training approach leads comparable even better performance. additionally pre-training knowledge transfer pre-training combined lead additional performance gains experiments. future work involves studying knowledge transfer heterogeneous models e.g. probabilistic models neural models. dahl deng acero large vocabulary continuous speech recognition context-dependent dbn-hmms proceedings ieee international conference acoustics speech signal processing hinton deng dahl a.-r. mohamed jaitly senior vanhoucke nguyen sainath deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine vol. dahl deng acero context-dependent pretrained deep neural networks large-vocabulary speech recognition ieee transactions audio speech language processing vol. senior beaufays long short-term memory recurrent neural network architectures large scale acoustic modeling proceedings annual conference international speech communication association weng watanabe b.-h. juang recurrent deep neural networks robust speech recognition proceedings ieee international conference acoustics speech signal processing erhan bengio courville p.-a. manzagol vincent bengio unsupervised pre-training help deep learning? journal machine learning research vol. paul venkatasubramanian unsupervised deep deng dahl roles pre-training ﬁne-tuning context-dependent dbn-hmms real-world speech recognition proceedings nips workshop deep learning unsupervised feature learning seide chen feature engineering contextdependent deep neural networks conversational speech transcription ieee workshop automatic speech recognition understanding vinyals ravuri povey revisiting recurrent neural networks robust proceedings ieee international conference acoustics speech signal processing ieee boulanger-lewandowski bengio vincent modeling temporal dependencies high-dimensional sequences application polyphonic music generation transcription arxiv preprint arxiv. j.-t. huang gong learning smallproceedings size output-distribution-based criteria annual conference international speech communication association september available http//research.microsoft.com/apps/pubs/default.aspx?id=", "year": 2015}