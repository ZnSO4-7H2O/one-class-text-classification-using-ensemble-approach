{"title": "To Drop or Not to Drop: Robustness, Consistency and Differential Privacy  Properties of Dropout", "tag": ["cs.LG", "cs.NE", "stat.ML"], "abstract": "Training deep belief networks (DBNs) requires optimizing a non-convex function with an extremely large number of parameters. Naturally, existing gradient descent (GD) based methods are prone to arbitrarily poor local minima. In this paper, we rigorously show that such local minima can be avoided (upto an approximation error) by using the dropout technique, a widely used heuristic in this domain. In particular, we show that by randomly dropping a few nodes of a one-hidden layer neural network, the training objective function, up to a certain approximation error, decreases by a multiplicative factor.  On the flip side, we show that for training convex empirical risk minimizers (ERM), dropout in fact acts as a \"stabilizer\" or regularizer. That is, a simple dropout based GD method for convex ERMs is stable in the face of arbitrary changes to any one of the training points. Using the above assertion, we show that dropout provides fast rates for generalization error in learning (convex) generalized linear models (GLM). Moreover, using the above mentioned stability properties of dropout, we design dropout based differentially private algorithms for solving ERMs. The learned GLM thus, preserves privacy of each of the individual training points while providing accurate predictions for new test points. Finally, we empirically validate our stability assertions for dropout in the context of convex ERMs and show that surprisingly, dropout significantly outperforms (in terms of prediction accuracy) the L2 regularization based methods for several benchmark datasets.", "text": "training deep belief networks requires optimizing non-convex function extremely large number parameters. naturally existing gradient descent based methods prone arbitrarily poor local minima. paper rigorously show local minima avoided using dropout technique widely used heuristic domain. particular show randomly dropping nodes one-hidden layer neural network training objective function certain approximation error decreases multiplicative factor. side show training convex empirical risk minimizers dropout fact acts stabilizer regularizer. simple dropout based method convex erms stable face arbitrary changes training points. using assertion show dropout provides fast rates generalization error learning generalized linear models moreover using mentioned stability properties dropout design dropout based differentially private algorithms solving erms. learned thus preserves privacy individual training points providing accurate predictions test points. finally empirically validate stability assertions dropout context convex erms show surprisingly dropout signiﬁcantly outperforms regularization based methods several benchmark datasets. recently deep belief networks used design state-of-the-art systems several important learning applications. important reason success dbns model complex prediction functions using large number parameters linked non-linear gating functions. however also makes training models extremely challenging task. since potentially large number local minimas space parameters standard gradient descent style method prone getting stuck local minimum might arbitrarily global optimum. popular heuristic avoid local minima dropout perturbs objective function randomly dropping several nodes dbn. recently work understand heuristic certain limited convex settings baldi sadowski wager however general heuristic well understood especially context dbns. work ﬁrst seek understand conditions dropout helps training dbns. show fairly general one-hidden layer neural networks dropout indeed helps avoid local minima/stationary points. prove following holds least constant probability dropout decreases objective function value multiplicative factor long objective function value close optimal value best knowledge ﬁrst result explains performance dropout training neural networks. recently seminal work andoni showed rigorously gradient descent based method neural networks used learn low-degree polynomials. however method analyzes complex perturbation gradient descent apply dropout. moreover results apply signiﬁcantly general problem setting ones considered andoni section details. excess risk bounds dropout. additionally also study dropout heuristic relatively easier setting convex empirical risk minimization gradient descent methods known converge global optimum. contrast mentioned instability result convex setting result indicates dropout heuristic leads stability optimum. hints dichotomy dropout makes global optimum stable de-stabilizing local optima. particular study excess error incurred dropout method applied convex problem. show that expectation dropout solves problem similar weighted l-regularized exhibits fast excess risk rates comparison recent works analyze dropout style problems baldi sadowski wager study general problem convex generalized linear model provide precise generalized error bounds same. section details. private learning using dropout. privacy looming concern several large scale machine learning applications access potentially sensitive data dwork differential privacy dwork cryptographically strong notion statistical data privacy. extremely effective protecting privacy learning applications chaudhuri duchi song jain thakurta mentioned above convex erms dropout shown stable w.r.t. changing entries training data. using insight design dropout based differential private algorithm convex erms algorithm requires that expectation randomness dropout minimum eigenvalue hessian given convex function lower bounded. stark contrast existing differentially private learning algorithms. methods either need strongly convex regularization assume given strongly convex. experimental evaluation dropout. finally empirically validate stability regularization assertion dropout convex setting. particular focus stability dropout w.r.t. removal training data i.e. stability. study random adversarial removal data samples. interestingly recent works szegedy maaten provide complementary experiments study dropout adversarial removal training data szegedy studies adversarial perturbation test inputs maaten considers corrupted features. experiments indicate dropout engenders stability accuracy regularization. moreover perhaps surprisingly dropout yields accurate classiﬁer popular regularization several datasets. example atheist dataset repository dropout based logistic regression almost accurate regularized logistic regression. paper organization present analysis dropout training neural networks section then section presents excess risk bounds dropout applied convex problem. section show dropout applied convex erms leads stable solutions used guarantee differential privacy algorithm. finally present empirical results section section provide rigorous guarantees training certain class neural networks using dropout heuristic. particular show dropout ensures constant probability gradient descent stuck local optimum. fact certain assumptions show function estimation error actually reduces multiplicative factor dropout. andoni also study robustness properties local optima encountered gradient descent procedure training neural networks. however proof applies complex perturbation gradient descent approximating low-degree polynomials. problem setting. ﬁrst describe exact problem setting study. space input feature vectors ﬁxed distribution deﬁned ﬁxed function goal approximate function neural network given estimated function error measured neural network architecture. consider one-hidden layer neural network architecture nodes link function hidden layer node simplicity assume coefﬁcients ﬁxed goal learn parameters training data given learning task {)}x∼d. note andoni also studies architecture link functions assumed low-degree polynomials. dropout heuristic. describe dropout algorithm problem. t-th step sample data point perform gradient descent learning rate analyze effectiveness dropout heuristic function approximation. would like stress objective section demonstrate instability local-minima function approximation w.r.t. dropout perturbation. entails gradient descent procedure stuck local minima/stationary point dropout heuristic helps local minima fact reduces estimation error signiﬁcantly. however exposition guarantee dropout algorithm reaches global minimum. ensures using dropout local minimum. true polynomial represents i-th node’s link function neural network. α··· represent weights output layer neural network αmin ﬁxed distribution training examples drawn. probability least dropout dropped neural network satisﬁes following. high level theorem shows estimation error large enough following holds least constant probability dropout based perturbation signiﬁcantly lesser estimation error itself. next section apply results problem learning low-degree polynomials compare guarantees andoni proof theorem error polynomial approximation error polynomial approximation following identity. last step notational purposes. work andoni studied problem learning degree-d polynomials using polynomial neural networks described above. section provide comparative analysis andoni theorem above. approach andoni different approach ways analysis andoni through perturbation complex consider additive perturbation weights opposed multiplicative perturbation nodes exhibited dropout. order make results comparable assume node deal complex numbers bounds modulus.) andoni show error brought notice bound independent dimensionality degree polynomial terms rate convergence andoni ensures error reduces factor case another advantage theorem oblivious data distribution opposed results andoni explicitly require either uniform gaussian. previous section dropout helps come local minimum encountered gradient descent. section show generalized linear models dropout gradient descent provides excess risk bound number training data samples. problem setting. ﬁrst describe exact problem setting study. ﬁxed unknown distribution data domain input feature domain target output domain. loss real-valued convex function deﬁned population excess risk model deﬁned dropout heuristic. describe dropout based algorithm used minimize excess risk high-level standard stochastic gradient descent algorithm. however stepa random α-fraction coordinates parameter vector updated. data point generated stochastic gradient descent perturbed obtain i-th coordinate given bixi. perturbed used update parameter vector section assume sampling probablity algorithm exact dropout algorithm analyze. also analyze stylized variant dropout effectively captured standard regularized empirical risk minimization setup. analyses hinge observation even though loss functions strongly convex general dropout variants loss functions strongly convex expectation enable derive excess risk cases. recall non-strongly convex loss functions general lower bound excess risk loss function -strongly convex theorem provide excess risk guarantee dropout heuristic. theorem ﬁxed convex assumption true data domain loss {··· i.i.d. samples drawn risk randomness algorithm deﬁned learning rate proof theorem provided section observe assumed constants excess risk bound theorem second note bound dropout risk deﬁned special case linear regression dropout-based risk true risk plus regularization. hence case using standard arguments shalev-shwartz excess risk rate population risk deﬁned however loss functions clear close dropout based risk population risk. lemma drawn uniformly least squares loss function i.e. then note. notice even full rank ···) still obtain excess risk dropout loss. recall general non-strongly convex loss functions best excess risk hope demonstrating strong experimental advantage dropping nodes training deep neural networks series works providing strong theoretical understanding dropout heuristic baldi sadowski wager wang erven helmbold long wager mcallester maaten high-level conclusion works dropout behaves regularizer particular regularizer underlying optimization problem convex. terms rates convergence work wager provide asymptotic consistency dropout heuristic w.r.t. convex models. show asymptotically dropout behaves adaptive l-regularizer. work wager provide precise rate convergence excess risk data assumed coming possion generaive model underlying optimization task topic modeling. classic problem linear optimization polytope dropout recovers essentially bound follow perturbed leader kalai vempala bypassing issue tuning regularization parameter. work extend line work providing precise rate convergence dropout heuristic arbitrary generalized linear models essence providing analysis close fourth open problem raised work erven posed problem determining generalization error bound glms. surprising aspect result rate convergence algorithms received signiﬁcant attention chaudhuri monteleoni dwork chaudhuri jain kifer duchi song jain thakurta bassily extend line research show dropout allows exploit properties data ensure robustness hence differential privacy. differential privacy cryptographically strong notion de-facto standard statistical data privacy. ensures privacy individual entries data even presence arbitrary auxiliary information dwork deﬁnition -differential privacy dwork pairs neighboring data sets differing exactly entry algorithm -differentially private measurable sets range space following holds background. intuitive level differential privacy ensures measure induced space possible outputs randomized algorithm depend much presence absence data entry. intuition immediate consequences underlying training data contains potentially sensitive information ensures adversary learns almost information individual independent his/her presence absence data hence protecting his/her privacy since output depend much data entry algorithm cannot over-ﬁt hence provably good generalization error. formalizations implications found dwork bassily property learning algorithm over-ﬁt training data known necessary sufﬁcient learning algorithm generalize shalev-shwartz poggio bousquet elisseeff following provide stylized example dropout ensures differential privacy. appendix provide detailed approach extending example arbitrary generalized linear models section analyze stylized example linear loss functions simplex. idea ﬁrst show given data dropout algorithm satisﬁes differential privacy condition data differing entry later standard technique called proposetest-release framework dwork convert differential privacy guarantee. data domain i.i.d. samples distribution loss function constraint p-dimensional simplex. b··· i.i.d. uniform samples dropout optimization problem linear case deﬁned below. i-th coordinate given bixi. high level theorem states changing data entry training data changes induced probability measure possible outputs factor additive slack exponentially small number data samples w.l.o.g. assume neighboring data differs also xibi. clearly following show measures induced random variable data sets multiplicative closeness. analysis part closely relates differential privacy guarantee binomial distribution dwork given number non-zeroes j-th coordinate xi’s excluding therefore following ratio upper bounded chernoff bound long event happens probability least exp). lower tail binomial distribution analogous argument provides bound. following notice individually coordinates satisfy multiplicative closeness satisfy analogous closeness measure closeness measure fact within property follows using bhaskar concludes proof. figure stability analysis logistic regression atheist data set. experiments repeated times means plotted. show random removal training examples error marginal error vary shows adversarial removal. notice theorem independent data distribution direct implications differential privacy. show using propose-test-release framework dwork smith thakurta dropout heuristic provides differentially private algorithm. propose-test-release framework. notice pair data sets differing entry theorem differs using standard laplace mechanism differential privacy dwork show satisﬁes -differential privacy random variable sampled laplace distribution scaling parameter hand check condition true output output otherwise. theorem ensures framework -differentially private. theorem propose-test-release framework along dropout -differentially private optimizing linear functions simplex /nω. theorem direct consequence theorem thakurta using tail property laplace distribu w.p. least framework tion show long theorem least outputs exactly. current exposition framework tuned problem optimizing linear functions simplex much general treatment provided appendix performance learning algorithm function fraction training examples removed. measure captures dependent algorithm particular subset training data. show results glms well deep belief networks compare following baseline methods unregularized models l-regularized glm’s. describe experimental setup results model classes below. stability dropout logistic regression introduce perturbations forms random removal training examples adversarially remove training examples. random removal training examples given train model randomly selected fraction training data. report test error difference mean test error absolute difference test error baseline error refer difference marginal error. present results benchmark atheist dataset newsgroup corpus; total number examples dimensionality data training remaining testing dropout rate measure error terms fraction misclassiﬁed examples. figure shows results different values training logistic regression model regularization regularization variants dropout standard dropout hinton deterministic dropout outlined wang manning observe dropout variants exhibit stability unregularized regularized versions. moreover deterministic dropout stable standard dropout. notice that even though dropout consistently lower test error methods effectiveness diminishes increasing hypothesize decreasing amount training data regularization provided dropout also decreases adversarial removal training examples given training set. θfull model learned complete given value remove samples minimum θfull|. rest experiment remains random removal setting. figure shows test error marginal error different regularization methods w.r.t. adversarial setting. random removal dropout continues least good regularization methods studied. however observe dropout’s advantage decreases rapidly methods tend perform similarly. stability linear regression next apply methods linear regression using boston housing dataset bache lichman experiments. examples training rest testing. figure shows marginal error dropout less methods values interestingly small values dropout performs worse regularization although performs better higher values. dropout rate measure mean squared error. stability deep belief networks theoretical stability guarantees hold generalized linear models experiments indicate extend deep belief networks too. posit dropout algorithm dbn’s operates locally convex region stability properties hold. mnist data experiments. experiments data sets appendix mnist dataset contains examples training testing. training data network four layers. units layer respectively. error measure misclassiﬁcations. previous experiments measure stability randomly removing training examples. figure test error marginal error dropout well standard algorithm applied dbns. similar", "year": 2015}