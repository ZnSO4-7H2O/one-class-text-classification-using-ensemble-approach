{"title": "When coding meets ranking: A joint framework based on local learning", "tag": ["cs.CV", "cs.LG", "stat.ML"], "abstract": "Sparse coding, which represents a data point as a sparse reconstruction code with regard to a dictionary, has been a popular data representation method. Meanwhile, in database retrieval problems, learning the ranking scores from data points plays an important role. Up to now, these two problems have always been considered separately, assuming that data coding and ranking are two independent and irrelevant problems. However, is there any internal relationship between sparse coding and ranking score learning? If yes, how to explore and make use of this internal relationship? In this paper, we try to answer these questions by developing the first joint sparse coding and ranking score learning algorithm. To explore the local distribution in the sparse code space, and also to bridge coding and ranking problems, we assume that in the neighborhood of each data point, the ranking scores can be approximated from the corresponding sparse codes by a local linear function. By considering the local approximation error of ranking scores, the reconstruction error and sparsity of sparse coding, and the query information provided by the user, we construct a unified objective function for learning of sparse codes, the dictionary and ranking scores. We further develop an iterative algorithm to solve this optimization problem.", "text": "abstract sparse coding represents data point sparse reconstruction code regard dictionary popular data representation method. meanwhile database retrieval problems learning ranking scores data points plays important role. problems always considered separately assuming data coding ranking independent irrelevant problems. however internal relationship sparse coding ranking score learning? explore make internal relationship? paper answer questions developing ﬁrst joint sparse coding ranking score learning algorithm. explore local distribution sparse code space also bridge coding ranking problems assume neighborhood data point ranking scores approximated corresponding sparse codes local linear function. considering local approximation error ranking scores reconstruction error sparsity sparse coding query information provided user construct uniﬁed objective function learning sparse codes dictionary ranking scores. develop iterative algorithm solve optimization problem. jing-yan wang xuefeng king abdullah university science technology computational bioscience research center computer electrical mathematical sciences engineering division thuwal saudi arabia e-mail xin.gaokaust.edu.sa correspondence addressed gao. e-mail xin.gaokaust.edu.sa. tel. +--. sparse coding popular data representation method tries reconstruct given data point linear combination basic elements dictionary referred codewords. linear combination coeﬃcients imposed sparse e.g. combination coeﬃcients zeros. linear combination coeﬃcient vector data point used representation call sparse code sparsity. ability explore latent part-based nature data widely used represent data pattern classiﬁcation image understanding database retrieval problems. many sparse coding algorithms proposed learn dictionary sparse codes retrieval problems data points usually ranked according similarity measures queries. similarity measures referred ranking scores. recently methods learn ranking scores data points proposed showed power retrieval problems considering query information provided users distribution data points eﬃcient algorithms developed learn ranking scores possible sparse coding ranking score learning techniques boost performance nearest neighbor searching. ﬁrstly data points sparse codes using sparse coding algorithm learn ranking scores sparse code space. however strategy uses sparse coding ranking methods independently assumes irrelevant problems. paper following questions sparse coding ranking score learning answer questions propose learn sparse codes ranking scores jointly explore internal relationship. actually mairal proposed learn sparse codes dictionary classiﬁer jointly explore internal relationship sparse coding classiﬁcation. however existing work considering sparse coding ranking problems simultaneously. propose perform sparse coding data points query information provided user regularize learning ranking scores. importantly bridge learning sparse codes ranking scores also utilize local distribution data points assume local neighborhood data point ranking scores approximated sparse codes using local linear function. considering reconstruction error sparsity sparse coding problem local approximation error complexity local ranking score approximation query information regularization problems simultaneously construct uniﬁed objective function learning sparse codes dictionary ranking scores. optimizing objective function sparse codes ranking scores regularize learning other thus internal relationship explored. iterative algorithm developed optimize objective function regard sparse codes dictionary ranking scores using alternate optimization strategy. rest parts paper organized follows section brieﬂy introduce related works sparse coding ranking score learning. sections introduce proposed joint sparse coding ranking score learning method. section show performance proposed algorithm nearest neighbor retrieval problems using benchmark data sets. section paper concluded future work. since method based sparse coding ranking score learning local learning give brief introduction relevant works. widely used sparse coding method proposed based iteratively solving ℓ-constrained least square problem ℓ-regularized least square problem. solutions achieve signiﬁcant speedup sparse coding. method ignores local manifold structure distribution data points. solve issue proposed laplacian sparse coding explore local manifold structure data presented nearest neighbor graph used regularize learning sparse codes. nearest neighbor graph constructed data points present local manifold structure learned sparse codes neighboring data points imposed close. learning rank problems zhou also used nearest neighbor graph regularize learning ranking scores. disadvantage using nearest neighbor graphs ranking performance usually sensitive graph parameters. solve problem yang proposed local regression global alignment algorithm local linear function predict ranking scores original data points neighborhood data point explore local manifold information. although local manifold information utilized improve learning sparse codes ranking scores still clear connection sparse codes ranking scores local manifold context. paper predict ranking scores sparse codes local neighborhood data point explore connection. assume data data points denoted {xi}n d-dimensional feature vector i-th data point. data point provided user named query remaining data points given database. indicate query data point deﬁne query indicator vector query otherwise. problem data retrieval return data points database similar query. ranking scores learned data points similarities query data points ranked according ranking scores ranked data points returned retrieval results. ranking scores data points organized ranking score vector ranking score i-th data point. learn ranking score represent data points sparse codes dictionary ﬁrst meanwhile learn ranking scores sparse codes query information. following problems considered construct uniﬁed objective function learn sparse codes ranking scores. dsik reconstruction error i-th data point measured squared ℓ-norm distance ksik ℓ-norm based sparsity measure sparse code tradeoﬀ parameter. solving problem data points represented corresponding sparse codes. sparse codes predict ranking scores. local ranking score learning unitize local structure sparse code space propose learn local linear function neighborhood data point approximate ranking scores. knearest neighboring data points denoted propose learn linear function approximate ranking scores fj|jxj data points neighborhood sparse codes sj|jxj approximation error ranking scores measured squared ℓ-norm kwik square ℓ-norm based regularization term used control complexity local linear function tradeoﬀ parameter. overall problem obtained summing local minimization problems data points query regularization unitize query information provided users also regularize learning ranking scores query indicator. data point query ranking score large since similar itself. thus deﬁne large value constant force ranking scores queries close following minimization problem obtained tradeoﬀ parameters. problem need solve dictionary corresponding sparse codes {si}n ranking scores {fi}n data points. learning sparse codes ranking scores uniﬁed single optimization problem thus learning regularized other. critical diﬀerence proposed method traditional independent sparse coding ranking score learning algorithms ignore inherent connection them. directly solving problem diﬃcult thus adapt alternate optimization strategy solve ranking scores sparse codes dictionary updated iterative algorithm. iteration solved others ﬁxed roles switched. iterations repeated maximum iteration number reached. deﬁned local objective local ranking score learning problem rewrite matrix form deﬁne local ranking score vector ranking score j-th nearest neighbor point similarly deﬁne local sparse code matrix rm×k sparse code j-th nearest neighbor point rewrite objective function composed local objective functions data points thus local objective function minimized. minimize local objective function partial derivative regard zero moreover consider summation local objective functions data points rewrite product nearest neighbor indicator matrix }n×k indicate data points element deﬁned indicated optimal solution also function sparse codes data points directly solving problem complicated choose em-like algorithm solve iteration ﬁrstly estimated using sparse codes solved previous iteration ﬁxed sparse codes {si}n updated. moreover also choose update sparse codes one. sparse code considered others {sj}jj=i ﬁxed. reduces problem shortage algorithm high computational complexity. calculate ranking vector single query dictionary sparse codes data points updated iteration. unacceptable on-line retrieval system especially database size large. solve problem propose two-step strategy including oﬀ-line learning procedure learn dictionary sparse codes data points database on-line ranking procedure learn sparse code query ranking score vector. oﬀ-line learning procedure database data points {xi}n knowing query. regularize learning dictionary sparse codes data points ranking randomly select presentative data points data treat queries. selected query denoted query want learn ranking vector consider multiple queries parameter vector linear function predict ranking scores query sparse codes query otherwise. solve problem adapt similar alternate optimization strategy method used solve sparse codes dictionary solved section section on-line ranking procedure given database data points query need calculate ranking score vector query. already sparse codes data points dictionary learned oﬀ-line procedure. thus need calculate sparse code query data point ﬁxing sparse codes remaining data points. extend consider additional query on-line retrieval procedure learn sparse code ranking scores fi|n+ otherwise. problem also solved alternate optimization strategy. iterative algorithm fi|n+ updated alternately. moreover also assume k-nearest neighbors within considering xn+. on-line retrieval procedure need search nearest neighbors leaving ni|n ﬁxed. actually solve ranking scores local regularization matrices li|n ﬁrst data points ones calculated oﬀ-line learning procedure ﬁxed need update ln+. solved ﬁrst local learning regularization terms ignored ni|n regularization needs considered. thus computations fi|n+ evaluate proposed algorithm conducted experiments benchmark data sets compared individual sparse coding ranking score learning algorithms well simple combinations. used yale face database usps handwritten digit database coil object image database glass identiﬁcation data climate model simulation crashes data ionosphere data statistical information data sets given table conduct retrieval experiments employed -fold cross validation. data split four folds randomly fold used query remaining three folds combined used database set. ﬁrst performed oﬀ-line learning procedure database performed on-line ranking procedure query query set. retrieval performance ranking measured receiver operating characteristic curve recall-precision curve. area curve also used single performance measure. compared joint sparse coding ranking score learning algorithm state-of-the-art sparse coding method lapsc state-of-the-art ranking score learning algorithm lrga simple combination i.e. using lapsc learn sparse codes using sparse codes lrga learn ranking. individual sparse coding ranking algorithms based manifold learning. note consider supervised sparse coding algorithms fair comparison since proposed algorithm unsupervised learning algorithm. curves compared methods given fig. ﬁgures proposed method clearly outperforms independent sparse coding algorithm ranking score learning algorithm simple combination diﬀerent data sets. plots curves proposed method closer top-left corner ﬁgures method recall-precision curves proposed method closer topright corner ﬁgures methods. indicates overall better retrieval performance. strong evidences advantage joint sparse coding ranking method independent sparse coding ranking methods. claim supported values curves table data sets proposed method achieves highest values. example data coil proposed method achieves value higher moreover interesting lrga outperforms lapsc cases incorporating lapsc lrga simple achieve signiﬁcant improvement lrga. example fig. recall-precision curve lrga signiﬁcantly closer top-right corner lapsc recall-precision curves lrga simple combination lrga+lapsc close. although lapsc lrga explore manifold structure data lapsc applies manifold regularization sparse code space lrga directly regularizes ranking scores manifold. means manifold learning representation space guarantee eﬀective ranking result space necessary perform local learning ranking score space like lrga. moreover performing lrga sparse code space provided lapsc also improve retrieval results improvement marginal. sparse coding ranking performed jointly proposed method signiﬁcant improvements achieved. means sparse coding potential improve performance ranking necessary explore inner relation them. three tradeoﬀ parameters objective function also interested sensitivity proposed method parameters plot values diﬀerent values parameters fig. parameter sensitivity analysis performed yale face database fig. tends increase increased indicating sparser representation achieve better performance. however seems performance stable large value given. fig. seen proposed algorithm internal relationship popular data representation method sparse coding important procedure nearest neighbor search problem ranking score learning? answer question paper assume relationship exists propose explore using local linear function approximate ranking scores sparse codes local neighborhood data point. uniﬁed objective function constructed based local learning ranking scores sparse codes also based sparse coding query information regularization problems. iteratively optimizing regard sparse codes dictionary ranking scores develop ﬁrst joint sparse coding ranking score learning algorithm. assumption holds expected joint method takes advantage internal relationship outperform independent sparse coding ranking algorithms ignore relationship. proposed algorithm demonstrates superior performance existing sparse coding algorithm ranking score learning algorithm simple combination. veriﬁes assumption reveals existence internal relationship sparse coding ranking score learning problems. future extend proposed method data ranking using distributed computing models moreover investigate representation methods ranking purpose besides sparse coding using bayesian networks data representation ranking score learning proposed model simple squared ℓ-norm distance measure loss ranking score learning. however test process performance measure. future also study minimize loss function directly corresponds instead squared ℓ-norm distance obtain optimal performance measure directly", "year": 2014}