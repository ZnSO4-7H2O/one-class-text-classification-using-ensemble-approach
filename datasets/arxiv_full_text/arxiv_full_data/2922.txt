{"title": "Stable Distribution Alignment Using the Dual of the Adversarial Distance", "tag": ["cs.LG", "cs.AI", "cs.CV"], "abstract": "Methods that align distributions by minimizing an adversarial distance between them have recently achieved impressive results. However, these approaches are difficult to optimize with gradient descent and they often do not converge well without careful hyperparameter tuning and proper initialization. We investigate whether turning the adversarial min-max problem into an optimization problem by replacing the maximization part with its dual improves the quality of the resulting alignment and explore its connections to Maximum Mean Discrepancy. Our empirical results suggest that using the dual formulation for the restricted family of linear discriminators results in a more stable convergence to a desirable solution when compared with the performance of a primal min-max GAN-like objective and an MMD objective under the same restrictions. We test our hypothesis on the problem of aligning two synthetic point clouds on a plane and on a real-image domain adaptation problem on digits. In both cases, the dual formulation yields an iterative procedure that gives more stable and monotonic improvement over time.", "text": "methods align distributions minimizing adversarial distance recently achieved impressive results. however approaches difﬁcult optimize gradient descent often converge well without careful hyperparameter tuning proper initialization. investigate whether turning adversarial min-max problem optimization problem replacing maximization part dual improves quality resulting alignment explore connections maximum mean discrepancy. empirical results suggest using dual formulation restricted family linear discriminators results stable convergence desirable solution compared performance primal min-max gan-like objective objective restrictions. test hypothesis problem aligning synthetic point clouds plane real-image domain adaptation problem digits. cases dual formulation yields iterative procedure gives stable monotonic improvement time. adversarial methods recently become popular choice learning distributions highdimensional data. idea learn parametric representation distribution aligning empirical distribution interest according distance given discriminative model. time discriminative model trained differentiate true artiﬁcially obtained samples. generative adversarial networks neural networks discriminate samples parameterize learned distribution achieved particularly impressive results many applications generative modeling images image super-resolution image-to-image translation adversarial matching empirical distributions also shown promise aligning train test data distributions scenarios involving domain shift however gans related models proved extremely difﬁcult optimize. widely reported training gans tricky process often diverges requires careful parameter initialization tuning. arjovsky bottou recently identiﬁed several theoretical problems loss functions used gans analyzed contribute instability saturation training. paper focus major barriers stable optimization adversarial methods namely min-max nature. adversarial methods seek match generated real distributions minimizing notion statistical distance often deﬁned maximal difference values certain test functions could differentiate distributions. speciﬁcally case gans distance usually considered equal likelihood best neural network classiﬁer discriminates distributions assigning \"real generated?\" labels input points. order align distributions minimize maximum likelihood w.r.t. parameters learned aligned distribution. unfortunately solving min-max problems using gradient descent inherently difﬁcult. simple example demonstrate different ﬂavors gradient descent unstable comes solving problems kind. address issue explore possibility replacing maximization part adversarial alignment problem dual minimization problem linear kernelized linear discriminators. resulting dual problem turns much easier solve gradient descent. moreover make connections formulation existing objectives maximum mean discrepancy show strongly related iteratively reweighted empirical estimator mmd. ﬁrst evaluate well dual method handle point alignment problem lowdimensional synthetic dataset. then compare performance analogous primal method real-image domain adaptation problem using street view house numbers mnist domain adaptation dataset pair. goal align feature distributions produced network datasets classiﬁer trained label digits svhn loose accuracy mnist domain shift. cases show proposed dual formulation adversarial distance often shows improvement time whereas using primal formulation results drifting objective values often converge solution. explore dual formulation adversarial alignment objective linear kernelized linear discriminators relate maximum mean discrepancy; demonstrate experimentally synthetic real datasets resulting objective apply idea domain adaptation scenario show stability reaching long line work unsupervised generative machine learning review beyond scope work. recently adversarial methods learning generative neural networks achieved popularity ability effectively model high dimensional data e.g. generating realistic looking images. include original generative adversarial networks follow work proposing improved formulations wasserstein gans conditional gans related ideas proposed unsupervised domain adaptation. neural domain adaptation methods seek improve performance classiﬁer network target distribution different original training distribution introducing additional objective minimizes difference representations learned source target data. models align feature representations across domains minimizing distance ﬁrst second order feature space statistics adversarial objectives used domain adaptation domain classiﬁer trained distinguish generated source target representations using standard minimax objective well alternative losses instead aligning distributions feature space several models perform generative alignment pixel-space e.g. using coupled gans conditional gans models adapt hallucinating training images might look like target test domain. related line work uses adversarial training translate images domain style different domain create artistic effects several recent works perform alignment unsupervised without aligned image pairs objectives distribution matching proposed literature including maximum mean discrepancy f-discrepancy others also used generative modeling single step iterative reweighting procedure similar instance reweighting methods theoretically empirically shown improve accuracy presence domain shift. example huang used sample reweighting minimized empirical populations plug instance-weights weighted classiﬁcation loss whereas gong choose points series independent auxiliary tasks iterative reweighting performed cases. general statistical distances used distribution alignment fall categories either f-divergences integral probability metrics differences expected values test function samples different distributions maximized function family work speciﬁcally consider logistic adversarial objective show useful optimize dual present relation adversarial dual objective another statistical distance test function reproducing kernel hilbert space start well-known motivating example simple min-max problem show that even basic case gradient descent might fail dramatically. consider simplest min-max problem unique solution ﬁnding saddle point hyperbolic surface. given function problem solve minx maxy unique solution suppose want apply gradient descent solve problem. intuitive analog gradient vector might consider using update rule deﬁned vector ﬁeld however given point vector tangent closed circular trajectory thus following trajectory would never lead true solution observe trajectory produced update rule applied problem fig. neither block coordinate descent various learning rate schedules signiﬁcantly improve performance gradient descent problem. figure gradient descent fails solve saddle point problem minx maxy line presents trajectory gradient descent vector ﬁeld used iteration. blue lines examples vectors vector ﬁeld. want mention huge body work using alternative descent schemes convex-concave saddle point optimization including limited different variants mirror descent nesterov’s dual averaging mirror prox authors aware successful attempts context adversarial distribution alignment. techniques developed solving continuous games ﬁctitious play successfully adopted salimans suppose given ﬁnite points sampled distribution ﬁnite points sampled distribution goal match aligning speciﬁcally learn matching function maps close possible minimizing empirical estimate statistical distance parameters matching function argminθ denote contexts dependence important. regular adversarial approach obtains distance function ﬁnding best classiﬁer parameters discriminates points points considers distance equal likelihood classiﬁer. higher likelihood separating means form hypothesis general often chosen linear classiﬁer multi-layer neural network work class linear classiﬁers speciﬁcally logistic regression primal dual formulations. solution also kernelized obtain nonlinear discriminators. dual variable corresponds weight point. higher weight means point contributing decision hyperplane. optimal value alpha attains upper bound. maximizes inner expression computed closed form obtained splitting summation blocks include samples example matrix consists pairwise similarities points equal product corresponding data points linear case. factor front cross term comes fact off-diagonal blocks quadratic form equal. constraint alpha sums comes splitting optimality conditions bias term. denote resulting objective expression gives tight upper bound likelihood discriminator. thus minimizing upper bound minimize likelihood itself original loss therefore minimize distance distributions note overall problem changed unconstrained saddle point problem smooth constrained minimization problem ultimately converges gradient descent properly chosen learning rate whereas descent iterations saddle point problem guaranteed converge all. resulting smooth optimization problem consists minimization improve classiﬁcation scores move points towards decision boundary. next section provides intuition behind resulting iterative procedure. section show dual formulation adversarial objective interesting relationship another popular alignment objective. integral probability metric distributions given function family deﬁned shown closed form solution corresponding closed form empirical estimator unit ball reproducing kernel hilbert space reproducing kernel commonly referred maximum mean discrepancy deﬁnition essentially distance means vectors embedded corresponding rkhs. resulting empirical estimator combines average inner outer similarities samples distributions goes zero number samples increases note sample weights constant equal across samples dual distance introduced becomes exactly empirical estimate plus constant entropic regularizer. thus adversarial logistic distance introduced viewed iteratively reweighted empirical estimator distance. intuitively means optimization procedure consists alternating minimization steps best sample weights assignment changing regularized weighted minimized ﬁxed minimize resulting weighted distance changing matching function makes resulting procedure similar iteratively reweighted least squares algorithm logistic regression. interesting observation turns high weights iterative procedure given mutually close subsets closeness measured terms maximum mean discrepancy. happen exactly support vectors corresponding optimal domain classiﬁer. therefore procedure described essentially brings sets support vectors optimal domain classiﬁer different domains closer together. note computational complexity single gradient step proposed method grows quadratically size dataset kernelization step. however batched implementation method performed outperformed primal methods probably inference modern neural networks requires many products batch size batch size multiplication negligibly cheap compared rest network modern highly parallel computing architectures. show formulation applied distribution alignment speciﬁc problem unsupervised domain adaptation. scenario train classiﬁer supervised fashion domain update perform well different domain without using labeled samples latter. common examples include adapting camera different image quality different weather conditions. rigorously assume exist distinct distributions source distribution target distribution assume observe ﬁnite number labeled samples source distribution ﬁnite number unlabeled samples target distribution goal labeling function hypothesis space minimizes target risk even though labels samples source. ben-david showed that mild restrictions probability distributions target risk upper-bounded three terms source risk complexity term involving dataset size vc-dimensionality discrepancy source target distributions. thus order make target risk closer source risk need minimize discrepancy distributions. deﬁne discrepancy supremum differences measures across events given σ-algebra supa∈σ estimation indicated expression hard practice therefore usually replaced computationally feasible statistical distances. total variation distributions kolmogorov-smirnov test figure convergence analysis synthetic point alignment problem. procedure adversarial alignment primal discriminator weight space points lying two-dimensional plane converge single solution oscillates time. middle contrast proposed dual minimization adversarial objective steadily converges optimum. bottom observe similar behavior kernelized version. even though accuracy learned discriminators drifts time cases distance covariance matrices means decreased cases; however cases marked \"dual\" iterative procedure converged single stable solution indeed corresponds desired point cloud alignment. figure illustration. note absolute values objectives comparable therefore shown plots above. approach applied directly scenario discrepancy replaced adversarial objective uses logistic regression domain classiﬁer. section consider instance problem main task classiﬁcation hypothesis space corresponds multi-layer neural networks. compare standard min-max formulation adversarial objective min-min formulation report accuracy resulting classiﬁer target domain. synthetic distribution matching ﬁrst test performance proposed approach synthetic point cloud matching problem. data consists clouds points two-dimensional plane goal match points cloud points other. restrictions transformation target point cloud includes possible transformations therefore parameterized point coordinates themselves coordinates updated gradient step. minimized logistic adversarial distance primal space solving corresponding min-max problem compared maximizing proposed negative adversarial distance given dual logistic classiﬁer corresponding kernelized logistic classiﬁer gaussian kernel. expected optimization distances given dual versions domain classiﬁers worked considerably better distance given linear classiﬁer primal form. speciﬁcally results primal case sensitive choice learning rate. general resulting decent iterations saddle point problem converge single solution whereas dual versions successfully converged solutions matched clouds points visually terms means covariances presented fig. figure trained point cloud matching task primal approach leads unstable solution makes decision boundary spin around data points almost aligned whereas linear kernel dual approaches lead stable solutions gradually assign probability belonging either points exactly desired behaviour. yellow blue points original point clouds points correspond positions yellow points transformation suggest intuitive explanation dual procedure might work better addition fact optimization problems inherently easier saddle point problems. descision boundary classiﬁer dual space deﬁned implicitly weighted average observed data points data points move decision boundary moves them. points move rapidly discriminator explicitly parametrizes decision boundary weights discriminator change drastically keep moved points leading overall instability training procedure. support hypothesis observed interesting patterns behavior linear primal discriminator point clouds become sufﬁciently aligned decision boundary starts \"spinning\" around clouds slightly pushing corresponding directions. contrast dual classiﬁers gradually converging solutions assigned point probability corresponding either domains. real-image unsupervised domain adaptation also evaluated performance proposed dual objective visual domain adaptation task. performed series experiments svhn-mnist digit classiﬁcation dataset pair unsupervised domain adaptation setup task classiﬁer trained svhn unlabeled samples mnist improve test accuracy latter. following tzeng used standard lenet base model outputs last layer softmax feature representations. trained source network perform well source dataset discriminator distinguish features computed source target networks. training source network source domain initialized target network source weights optimized make distributions source target feature representations less distinguishable discriminator perspective. technical details given supplementary section tested several primal objectives based adversarial discriminative domain adaptation improved wasserstein gan-based objective unit-norm gradient regularizer compared dual objective. eliminate inﬂuence particular discriminator examine stability objective structure restricted discriminator hypothesis space linear classiﬁers primal objectives cannot kernelized support multilayer discriminators. restriction limits power resulting discriminator thus leading scores lower reported state interested trends behaviour objectives rather absolute reached values. model varied learning rates regularization parameters experiment epochs examine behavior models long run. details choice hyperparameters given section unsupervised domain adaptation access target labels thus cannot perform validation stopping criteria. fact labeled target data available could used ﬁne-tuning source model rather unsupervised learning. therefore evaluate behavior models multiple training epochs would stable face uncertain stopping criteria practical domain adaptation scenarios. figure shows digit classiﬁcation accuracies obtained four models target mnist dataset. presents distribution accuracies different epochs bottom shows evolution individual runs. results average descent iterations dual objective converged satisfactory solutions considerably higher number learning rate hyperparameter combinations compared methods. model often stayed peak performance whereas methods often slowly deviated amount instability demonstrates important choose exactly right hyperparameters stopping criteria models. contrast dual objective clearly performs well majority learning rates. wgan often performs better adda experiences signiﬁcant oscillations. tried using different validation heuristics considering runs resulted signiﬁcant drop distance signiﬁcantly change trends exact dual exists logistic discriminator case duality solve inner problem closed form want stress paper presents general framework alignment extended classes functions. speciﬁcally rewrite quadratic form kernel logistic regression frobenius inner product kernel matrix symmetric rank alignment matrix kernel matrix speciﬁes distances points chooses pairs minimize total distance. problem reduces maximizing maximum agreement alignment similarity matrices turn might seen replacing adversity original problem cooperation dual maximization problem. paper rank matrix could choose different alignment matrix parameterizations corresponding regularizer would correspond neural network discriminator adversarial problem wasserstein distance earth mover’s distance form. resulting problem dual minimization existing adversarial distances exploits underlying principle iteratively-reweighted alignment matrix ﬁtting discussed paper. assert order understand basic properties resulting formulation in-depth discussion well-studied logistic case less important discussion involving complicated deep models deserves paper own. paper proposes stable cooperative problem reformulation rather adversarial objective many recent papers presented adversarial objective lead min-max problem. proposed using dual discriminator objective improve stability distribution alignment showed connection presented quantitative results alignment datasets unsupervised domain adaptation results real-image classiﬁcation datasets. results suggest proposed dual optimization objective indeed better suited learning gradient descent saddle point objective naturally arises original primal formulation adversarial alignment. attempts duality reformulate notions statistical distances adversarial settings computationally feasible minimization problems promising. figure distribution target test accuracies different epochs different objectives svhn-mnist domain adaptation. dashed line represents source accuracy therefore larger accuracy distribution mass right line better. results suggest dual objective leads minimal divergence optimal solution majority learning rates hyperparameter combinations. methods lower solution stability descending order improved wgan adda. bottom evolution target test accuracy epochs. dual objective clearly performs well majority learning rates. wgan often performs better adda experiences signiﬁcant oscillations. different validation heuristics considering runs resulted signiﬁcant drop distance signiﬁcantly change trends proportion runs outperformed source baseline epochs were dual wgan adda. peter green. iteratively reweighted least squares maximum likelihood estimation robust resistant alternatives. journal royal statistical society. series jiayuan huang arthur gretton karsten borgwardt bernhard schölkopf alex smola. correcting sample selection bias unlabeled data. advances neural information processing systems combined vectors inﬂated adding many combinations parts vectors similar in-between vectors; example models optimal sets parameters included ﬁrst model second would inﬂate adding nine combinations varying model-speciﬁc parameters regularization parameters domain adaptation experiments discriminators models trainable discriminator trained epochs prior beginning actual adaptation improved performance models. kernel density estimates figure obtained ﬁxed bandwidth linear boundary correction method. algorithms took greyscale images resized input. restrictions imposed adding error term restrictions hold adaptation. implement dual algorithm mini-batch fashion extracted slices correspond points processed batch iteration. search space built explained above learning rates weight decay updating target network discriminator follows", "year": 2017}