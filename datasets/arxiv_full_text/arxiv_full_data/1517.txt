{"title": "Learning Multi-Modal Word Representation Grounded in Visual Context", "tag": ["cs.CL", "cs.AI", "cs.CV"], "abstract": "Representing the semantics of words is a long-standing problem for the natural language processing community. Most methods compute word semantics given their textual context in large corpora. More recently, researchers attempted to integrate perceptual and visual features. Most of these works consider the visual appearance of objects to enhance word representations but they ignore the visual environment and context in which objects appear. We propose to unify text-based techniques with vision-based techniques by simultaneously leveraging textual and visual context to learn multimodal word embeddings. We explore various choices for what can serve as a visual context and present an end-to-end method to integrate visual context elements in a multimodal skip-gram model. We provide experiments and extensive analysis of the obtained results.", "text": "representing semantics words long-standing problem natural language processing community. methods compute word semantics given textual context large corpora. recently researchers attempted integrate perceptual visual features. works consider visual appearance objects enhance word representations ignore visual environment context objects appear. propose unify textbased techniques vision-based techniques simultaneously leveraging textual visual context learn multimodal word embeddings. explore various choices serve visual context present end-to-end method integrate visual context elements multimodal skip-gram model. provide experiments extensive analysis obtained results. figure illustration approach underlying research questions concerns visual part model integration visual part text model deals evaluation embeddings. representing word semantics long-standing problem conditions major applications automatic sentiment text analysis summarization distributional semantic models leverage large text corpora distributional hypothesis strong assumption states words occur similar contexts similar meanings produce ﬁxed-length vectorial representation words based co-occurrences text corpora. improve quality word representation information crucial. indeed psychological studies given pieces evidence meaning words grounded perception report bias said texts seen images. observations outline complementary roles images texts bring perspectives multimodal approaches bridging textual information visual ones improve natural language processing tasks besides worth mentioning representation learning models enhance word sequential joint fusion techniques however works ignore visual context objects. posit learning representations contexts different modalities component multimodal dsms. importance context illustrated simple example image apple black background color texture shape. context e.g. growing tree infer relative size apples respect tree leaves apples fruits grow trees. someone eating apple infer apples edible example understand exploiting visual surroundings context objects might useful grasp semantics words. figure overview early fusion middle fusion late fusion techniques. round-corner rectangles denote word embeddings. green related images blue text orange round-corner rectangles multimodal embeddings built textual visual resources. stands example evaluation task namely word similarity. work propose multimodal model learning word representation leveraging contexts different modalities namely texts images. contribution threefold propose experiment various deﬁnitions visual context never taken account best knowledge models; propose multimodal context-driven model jointly learn representations textual visual modalities modalities inﬂuence media-independent word embeddings strength model require aligned images text present thorough analysis obtained results determine inﬂuence visual modality learned multimodal embeddings experimenting word classiﬁcation tasks. learning word representation textual resources. distributional semantic models implicitly explicitly based factorization co-occurrence matrix compute representation words. well-known models glove wordvec based. latter words either predicted given context vice-versa cases representation learned words context. several modiﬁcations improvements proposed skip-gram model using gaussian embeddings account variance meaning words using extra information provided knowledge bases learning word representations textual visual resources. recent studies motivate construction general-purpose word embeddings language perceptual inputs images. precisely psychological studies reveal meaning words grounded perception moreover highlight complementarity language images. particular human reporting bias states frequency people refer things actions language correlate real world frequencies. people usually mention common things rather talk write surprising events. systematic bias respect real-world frequencies motivates researchers exploit visual information learn word representation leading multimodal approaches. sequential methods separately construct visual textual word representations combine using different techniques i.e. middle fusion late fusion. given separately learned representations modality middle fusion consists merging form multimodal vector several aggregation methods considered concatenation singular value decomposition canonical correlation analysis weighted gram matrix combination task-driven crossmodal mapping late fusion word representations computed modality. multimodal interactions occur downstream task done simple linear combination similarity scores respectively obtained textual visual data. sequential models cited above textual representations pre-trained glove wordvec embeddings visual embeddings built aggregation activations obtained pre-trained forwarded images. idea close humans learn grounded meaning semantics observed joint models require aligned texts images. example bayesian modeling approach based assumption text associated images generated using shared underlying latent topics ground word representations vision trying predict abstract scene associated given sentence. model follows early fusion strategy require aligned text images. closer work extensions wordvec skip-gram proposed. example base model assumption frequency appearance concrete concepts correlates likelihood experiencing world. perceptual information concrete concepts introduced model whenever concept encountered textual modality. representations concrete words trained predict surrounding words perceptual features feature-norms deﬁned describe objects features work later followed whose method designed natural images instead feature-norms constructed hand. force representation words images close visual representation. work exploit line research focuses exploiting visual context done best knowledge. using modeling visual contexts. several works presented visual modality constrain textual representation close visual representation object. strategy drawbacks. first asymmetry consideration modalities text deﬁnes semantic context word surrounding words images used visual information object. second fact context objects appear informative complementary textual inputs improve word representation. indeed fact supported several works propose middle fusion approach visual embedding built counting number visual words images. ﬁrst attempt apply distributional hypothesis images semantically similar objects tend occur similar environments images. experiments come conclusion appearance context informative semantics appearance object itself. comparison model work propose jointly learn embeddings visual textual context. observations former proposes latent dirichlet allocation model. latter uses count-based technique learn multimodal word embedding leveraging visual textual contexts. first build targetcontext count matrices text images using bag-of-visual words represent images. concatenate matrices perform rank reduction svd. split matrices consider fusion feature level scoring level. however count-base method learn representation contexts performs poorly semantic tasks moreover approach uses bags visual words representation images. addition identiﬁcation entities context rich spatial information present objects located image. propose intrinsic spatial information contexts dividing image bins considering visual words separately region. however comes learning representations words exploiting spatiality challenging still largely under-explored. nature sentence linear structure list tokens image spatially-organized quantiﬁable information skip-gram model choosing surrounding words context natural choice text however images clear used context learn semantically rich representations objects several multimodal fusion methods exist none models presented signiﬁcantly better others question know build multimodal framework obvious answer especially alignment texts images missing. evaluation tasks assess quality word embeddings inherently biased hard examine depth contribution brought visual modality contrast works learning multimodal word representations posit exploiting visual context enhances learned representation words. assumption makes consider images complex scenes containing many objects. indeed images single object give little information object used found contrary image showing object environment used interacting objects much informative thanks surrounding context. accordingly address following research questions also illustrated figure images used learn semantic representations objects? particular context capture semantic word/entity? note work consider entities subset words correspond objects images. high-level context image seen objects ...}. simple view gives high-level information environment objects occur. given entity image deﬁne objects appear image. then context surrounding object. deﬁne rm×d simple lookup table embeddings objects dimension representation space matrix. low-level context coarser level visual context elements seen image patches full image entity masked black pixels. call low-level context since directly uses pixel values surroundings entities. using low-level context interesting objects left unidentiﬁed images current models. however requires bigger complex model difﬁcult extract meaningful information pixel values. suggest possible choices select instance full image entity masked replacing values zeros; small image patch randomly chosen around entity. practice several choices ...}. image patch forwarded parametrized form activation vector cnnθ obtained last layer network. visual context vector formed projection dimension matrix rd×b. parameters learned enhancing context spatial information. dataset provides localization information entities wish annotations gives additional spatial information. example looking position image respect table hand person infer cups tables handed people. wish enhance visual contexts presented spatial information. consider methods model name visual spatiality compute vector representing visual relationships models integrate visual context element ﬁrst method considers low-level features corresponds spatial vector whose components relative positions axes bounding boxes ratio width height bounding boxes second method high-level features vector corresponds spatial vector whose components four indicator functions denoting whether context below beside above bigger entity following connaturally integrate visual model text-based model form multimodal dsm? evaluate examine contribution given visual modality ﬁnal word embeddings? present multimodal model leveraging visual textual contexts words order fulﬁll distributional hypothesis. ﬁrst formalize definition visual context propose experiments select appropriate visual context elements introduce multimodal joint model based skip-gram framework textual visual parts model share word embeddings updated textual visual inputs contexts modality speciﬁc. strength model relies fact require aligned data. since focus paper assume objects already detected images. representation learning visual contexts section formalize name visual contexts detail choice modeling propose. formalization. based original wordvec skipgram algorithm considers entities contexts translate follows distributional hypothesis images concrete model. case contexts visual contexts deﬁne latter. choice visual context elements need correspond list semantic entities instance visual context elements surrounding objects low-level features visual appearance also localization surrounding objects respect considered entity. entities embedding associated entity/word negative context sigmoid function. loss formulation close original skip-gram loss integrates learning shares parameters computation every context element. choice modeling. given entity propose different ways modeling instance visual context elements detail build parametrize scenarios baselines scenarios. evaluate different components model evaluate different scenarios. particular train model uses objects visual contexts model uses image patches model uses full images models spatial context information also evaluated denoted ﬁrst argument denotes visual context type second spatial context features third integration instance corresponds using image patches low-level visual features bilinear product. combinations models skip-gram textmodel trained evaluated multimodal word representations method explained section baselines baseline inspired state-of-theart model since visual features objects learn word representations contrast visual context features model. visual entity assume visual vector representing entity available. training along text-only skip-gram loss similarity between embedding entity visual appearance maximized max-margin framework lobject margin visual appearance negative object object kept ﬁxed visual information incorporated time entity encountered text. note model corresponds visual loss text-only skip-gram loss. evaluate visual context-driven multimodal representation learning model also evaluate skip-gram text model sequential model noted embeddings model concatenated embeddings obtained projected lower-dimensional space pca. serves comparison point joint approach sequential one. tasks similarly evaluate model three different semantic tasks namely word similarity relatedness feature norm prediction abstractness/concreteness prediction. task serves biased indicator quality embeddings. present evaluation benchmarks follows. word similarity relatedness benchmarks. semantic relatedness evaluates similarity degree word pairs. several benchmarks provide gold labels built integrated visual context embedding form spatially-informed visual context used skip-gram equations instead again variants considered linear combination visual context spatial vector i.e. denotes concatenation operator; bilinear interacθ r×d×d. tion model free parameters considers bilinear interaction spatial vector visual context present multimodal representation learning model integrates previously presented visual module textual skip-gram. main idea word embeddings shared across modalities context media-speciﬁc. contribution modality controlled linear combination modality-speciﬁc costs gives following global loss function crucial point model require aligned texts images train model extra pre-trained representations external datasets require entities identiﬁed images associated unique word vocabulary. besides justify joint model think important representations learned entities contexts. indeed entities embeddings affected modalities context representations change updated transitivity modalities shared embeddings. data large collection english texts dump wikipedia database cleaned gensim software provides million articles vocabulary million unique words. visual data visual genome dataset large image collection large number different objects rich complex scenes table results. columns left part table spearman correlations word similarity benchmarks columns right side f-scores feature-norm prediction task best results highlighted bold. simlex- semsim vissim spearman correlation computed list similarity scores given model gold labels. higher correlation semantic captured embeddings. word similarity benchmarks widely used intrinsic embedding evaluation biased sense good intrinsic evaluation scores imply useful embeddings downstream tasks shown feature norm prediction. task predicting features norms objects given word representation evaluate visual textual-based representations. consider task evaluate word embeddings setup evaluation. evaluation dataset extract mcrae dataset total characteristics grouped categories entities. linear classiﬁer trained -fold validation scores reported. abstractness concreteness prediction. norms give concreteness ratings english words. multimodal word representation wish know contains information used predict concreteness rating associated word. practice train kernel predict gold concreteness rating word embeddings. note task used evaluate multimodal representations since visual-based ones cover small vocabulary. layer). slice tensor shape corresponds activation region original image. negative examples entity models trained stochastic gradient descent learning rate mini-batches size regularized l-penalty respectively weighted scalars values hyperparameters found cross-validation evaluating visual context-driven semantic representations words. table reports results experiments discussing kind visual information useful. ﬁrst conclusion draw surroundings entities informative visual appearance objects evaluation word similarity benchmarks. indeed results word similarity task highlight model scenarios generally overpass baselines. instance results model pfull average higher baseline however feature-norm prediction task direct visual features objects better suited categories describe visually objects visual categories ‘encyclopedic’ ‘taste’ ‘sound’. measure complementarity features objects surroundings also evaluated ensemble model combines baseline model denotes summation loss functions embeddings shared. interestingly combining visual contexts direct features results model good average performance showing complementarity visual contexts visual entity representations. table experimental results word similarity evaluation benchmarks feature-norm prediction task concreteness prediction task concreteness measures coefﬁcients determination multiplied benchmarks w.r.t. feature-norm prediction task high low-level spatial features lead similar results. reinforces intuition visual context particularly spatial information promising learning word representation reducing human reporting bias affecting texts images. third conclusion draw high-level contexts yield better scores low-level contexts using low-level visual features challenging problem. however promising since cheap collect require context annotations contain rich information handled correctly. difﬁculty lies natural noise surroundings objects need visual modules automatically extract high-level information pixel values. rq/rq evaluating multimodal context-driven multimodal representation learning model analysis. table reports results embeddings initialized pre-trained embeddings obtained textbaseline. results highlight trained multimodal outperform text-only baseline evaluation tasks. instance shows average improvement in-line conclusions related works besides joint model compares favorably sequential model built embeddings obtained note relative improvement showing embeddings computed using multiple modalities beneﬁcial. like also evaluated ensemble model measure complementarity visual features multimodal model. again generally notice slight improvement opens perspectives formalizing leveraging visual information entities context. drawn analysis visual surroundings entities useful direct features evaluated tasks combination models shows complementarity approaches adding spatial term visual context signiﬁcantly increases performances ﬁnally higher-level contexts slightly easier lower-level contexts deeper insight learned embeddings explaining impact visual modality multimodal word representation. estimate correlation shift measured embedding concreteness degree word. result outlines correlation ρspearman showing visual concrete words embeddings changed visual abstract words. expected because visual part adds information visual entities. work proposed multimodal context-based approach learn word embeddings. extensive experiments line related work observed complementarity visual textual data learn word representations. importantly shown visual surroundings objects relative localization informative build word representations actually than complementary visual appearance objects exploited previous works. future work explore downstream tasks evaluate multimodal word embeddings might give ﬁner insights visual part model contributes learning representations. orthogonally focus contexts learned representations. particular would like aligned consistent multimodal representations learned weak supervision provided entities. also extend work learn relation representations objects based multimodal representations exploitation existing work partially supported chist-era project muster labex smart supported french state funds managed within investissements d’avenir programme reference anr--idex-. additionally thank guillem collell providing pretrained visual vectors needed evaluating baseline.", "year": 2017}