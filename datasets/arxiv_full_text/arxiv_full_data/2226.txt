{"title": "A Practically Competitive and Provably Consistent Algorithm for Uplift  Modeling", "tag": ["cs.LG", "cs.AI", "stat.ML"], "abstract": "Randomized experiments have been critical tools of decision making for decades. However, subjects can show significant heterogeneity in response to treatments in many important applications. Therefore it is not enough to simply know which treatment is optimal for the entire population. What we need is a model that correctly customize treatment assignment base on subject characteristics. The problem of constructing such models from randomized experiments data is known as Uplift Modeling in the literature. Many algorithms have been proposed for uplift modeling and some have generated promising results on various data sets. Yet little is known about the theoretical properties of these algorithms. In this paper, we propose a new tree-based ensemble algorithm for uplift modeling. Experiments show that our algorithm can achieve competitive results on both synthetic and industry-provided data. In addition, by properly tuning the \"node size\" parameter, our algorithm is proved to be consistent under mild regularity conditions. This is the first consistent algorithm for uplift modeling that we are aware of.", "text": "abstract—randomized experiments critical tools decision making decades. however subjects show signiﬁcant heterogeneity response treatments many important applications. therefore enough simply know treatment optimal entire population. need model correctly customize treatment assignment base subject characteristics. problem constructing models randomized experiments data known uplift modeling literature. many algorithms proposed uplift modeling generated promising results various data sets. little known theoretical properties algorithms. paper propose tree-based ensemble algorithm uplift modeling. experiments show algorithm achieve competitive results synthetic industry-provided data. addition properly tuning node size parameter algorithm proved consistent mild regularity conditions. ﬁrst consistent algorithm uplift modeling aware copyright notice paper accepted ieee international conference data mining. authors assigned institute electrical electronic engineers rights ieee copyright. article published proceedings icdm title shorten version paper. decision makers often face situation need identify alternatives candidate leads desirable outcome. example airline company sells priority boarding ancillary product needs select good price maximizes revenue. oftentimes passengers show signiﬁcant heterogeneity response prices answer price optimal depends circumstance. example revenue maximizing prices likely different route major cities route vacation destinations. luckily application conduct randomized experiments learn subject responses different scenarios. experiment subjects randomly assigned treatments following given probability distribution. characteristics subject assigned treatment response recorded. given randomized experiment data want construct generating mapping feature space ﬁnite labels uplift modeling confused classiﬁcation problems. fundamental difference comes fact data uplift modeling unlabeled. individual subject impossible know treatment optimal observe response assigned treatment none alternatives. poses unique challenges construction evaluation uplift models. research area related different uplift modeling study heterogeneous treatment effect uplift modeling aims identify optimal treatment among possibly many alternatives analysis heterogeneous treatment effect focus estimating difference expected response caused single treatment. distinction areas apparent look formulation. feature vector treatment. denote response distribution depends uplift modeling treatment take ﬁnite number values denoted objective obtain accurate estimator indicates treatment applied otherwise. clear heterogeneous treatment effect applicable single treatment deﬁnition subtraction ambiguous terms. similar arguments made difference uplift modeling subgroup analysis model built. given test example obtain predicted response treatment select correspondingly best treatment. main advantage approach require specialized algorithms. existing classiﬁcation/regression model incorporated scheme. disadvantage always perform well practice correctly identify optimal treatment learning algorithm need know well every treatment doing. however information treatments never provided learning algorithm scheme. discussion failure please section disappointed performance separate model approach researchers proposed number specialized algorithms uplift modeling. designed special case single treatment methods multiple treatments introduced tree-based algorithm described extended multiple treatment cases using weighted pairwise distributional divergence splitting criterion. multinomial logit formulation proposed treatments incorporated binary features. also explicitly include interaction terms treatments features. relevant work contextual treatment selection algorithm presented tree-based ensemble method. grows group trees random subsample original training data. step tree growing process random subset features drawn candidates exhaustive search conducted best splitting point. split evaluated increase expected response bring measured training data. aware ﬁrst uplift algorithm handle multiple treatments continuous response. lead signiﬁcant performance improvement applicable methods. drawback exhaustive search susceptibility outliers. splits likely placed adjacent extreme values. especially problematic uplift trees score split affected estimations treatments. outliers treatment inﬂuence choice split point. furthermore successive splits tend group together similar extreme values introducing bias estimation expected responses. solve problem above introduce modiﬁed version algorithm named unbiased contextual treatment selection difference separation partition feature space estimation leaf responses. growing tree ucts ﬁrst randomly splits training data subsets selecting tree splits estimating treatment-wise expected response leaf nodes. section demonstrate experimentally ucts competitive using synthetic industry provided data. another advantage two-sample approach makes consistency analysis tractable. section prove ucts achieve mean-square consistency mild regularity conditions properly tuning node size parameter. ﬁrst consistency result uplift modeling aware reminder section deﬁne notations used throughout paper. ucts algorithm described detail section section explain setup results numerical experiments. consistency analysis ucts presented section section ends paper brief summary. represents feature vector realization. subscripts used indicate speciﬁc features. example feature vector realization. denote d-dimensional feature space. different treatments encoded priority boarding example mentioned earlier airline wants customize price priority boarding maximize revenue would charactering information ﬂights origin-destination pair date time ﬂights etc.. would discrete candidate prices response would revenue passenger-segments. treatment selection rule mapping feature space space treatments goal uplift modeling based training data treatment selection rule expectation high possible. obvious maximum expected response achieved point-wise optimal treatment rule maxt=..k classiﬁcation regression trees combined ensembles prove among powerful machine learning methods almost predictably contextual treatment selection algorithm generates treebased ensembles also leads signiﬁcant performance improvement uplift modeling problems section describe modiﬁed version called unbiased contextual treatment selection eliminates estimation bias leaf responses using separate data sets partition generation leaf estimation. consider binary partition approach split creates branches tree. subset feature space associated current node. suppose candidate split divides left child-node right child-node allows select different treatments child nodes. added ﬂexibility brings increase expected response φl|x φr|x estimating conditional expectation requires care. need consider fact estimation done treatment. therefore fewer samples available. addition treatments equal probabilities randomized generates training set. experiment number samples treatment given user-deﬁned parameters split estimator deﬁned follows. split parent node initialize recursive deﬁnition estimation root node sample average. letting ˆyt) inherit parent node estimation enough samples allows tree grow full extend ensuring reliable estimation minority treatments. summarize score split computed order eliminate bias ucts uses separate data estimate leaf response partition generated. achieved randomly splitting training approximation estimation user-deﬁned parameter contains fraction examples sampled treatment. contains rest data. sets used generate tree structure using splitting criteria terminations conditions described above. nodes tree grown denote examples fall treatment empty conditional expected response treatment estimated sample average otherwise inherits estimation parent node. assume contains samples treatments least root node. deﬁnition estimations root node ﬁrst traverse level level nodes estimated. challenges testing uplift algorithms lack publicly available randomized experiments data. section ﬁrst simple two-dimensional data model illustrate behavioral difference ucts cts. then performance ucts tested larger data sets. ﬁrst -dimensional synthetic data set. second industry provided data pricing priority boarding ﬂights. data sets ones tested allows directly compare results. algorithm unbiased contextual treatment selection input training data fraction data used partition generation number trees ntree number features considered split mtry feature randomization factor minimum number samples required split split regularity factor tree-balance factor alpha draw round samples create approximation samples drawn proportionally treatment. estimation build tree step growing process coordinate drawn random probability mtry coordinates drawn random probability perform split largest among alpha-regular splits selected coordinate coordinates. output step nodes tree. estimate conditional expectation under treatment nodes described section ii-c. prediction given test point predicted expected response treatment average predictions trees. optimal treatment largest predicted expected response. fig. optimal treatment rule example section iii-a. vertical boundary middle plot located note vertical axis discrete variable illustrated like continuous simplicity. optimal treatment rule data model illustrated fig. vertical boundary middle located feature plotted like continuous variable could image. note that although optimal treatment assignment exhibits sharp change actual difference treatments changes smoothly zero middle. therefore algorithms likely difﬁculty identifying correct treatment around another characteristic example variance response grows quadratically susceptible extremes values ucts expect behaviors different large. fair comparison behaviors ucts must ﬁrst optimal parameters speciﬁcally split ucts split cts. parameters selected based performance models trained different training sets measured true data model. result training size samples treatment split ucts split cts. then training sets sampled decision boundary reconstructed algorithms chosen parameters plotted fig. decision boundary generated ucts much smoother training sets. especially case right side plot variance response high extreme values common. verify ucts sacriﬁcing performance smoothness compare results ucts models models generated training sets. expected response model estimated using true data model. average performance conﬁdence interval plotted fig. ucts fully competitive cts. example helpful understand behavioral difference ucts might complex enough represent real world scenarios. subsection consider -dimensional data model much complex response distribution. also data model used section allows compare results theirs. feature space ﬁfty-dimensional hyper-cube length features uniformly distributed feature space i.e. four different treatments response treatment deﬁned below. ﬁrst term mixture exponential functions deﬁned term treatments represents systematic dependence response features. second term treatment effect determines magnitude effect. third term zero-mean gaussian noise standard deviation fig. average expected response ucts models example section iii-a computed training sets. conﬁdence interval also shown plot. lower horizontal dash line indicates expected response ﬁxed single treatment upper optimal treatment rule. symmetry model expected response treatments estimated using monte carlo simulation samples. similarly expected response optimal treatment rule estimated performance ucts tested different training data sizes speciﬁcally samples treatment. size training sets test sets provided results data generate margin error. training model ntree mtry nreg alpha important parameter split selected validation results plotted fig. fig. averaged expected response different algorithms data model section iii-b. margin error computed results different training datasets. data size algorithms tested datasets. fig. treatment rule reconstructed ucts example section iii-a. plots generated training set. individual plot horizontal axis feature vertical axis feature labels ticks axes fig. omitted simplicity. methods separate model approach random forest k-nearest neighbor support vector regressor radial basis kernel adaboost ﬁgure ucts outperform separate model approaches training size greater training size almost achieved optimal performance. meanwhile margins error ucts overlap every training size. unreasonable comparable performance particular data model. mentioned introduction applications uplift modeling customized pricing. example apply uplift algorithms select price priority boarding airlines based ﬂight information. data provided major airlines europe. data half passengers receive default price half receives treatment price interestingly prices lead average revenue passenger overall. total features derived based information ﬂight reservation. origin station origin-destination pair departure weekday arrival weekday number days ﬂight booking departure ﬂight fare ﬂight fare passenger ﬂight fare passenger mile group size. performance ucts compared methods separate model approach random forest support vector machine adaboost k-nearest neighbors well uplift random forest method implemented cts. data randomly split training test ucts ntree mtry nreg alpha according results validation min_split details parameter tuning methods found appendix expected revenue algorithm plotted fig. beneﬁt applying specialized uplift algorithms apparent. best result separate model approach increase relative ﬁxed pricing. however ucts achieve astonishing increase. also plot modiﬁed uplift curves methods fig. horizontal axis indicates percentage population subject treatments vertical axis expected response given percentage. useful tool balancing gain customizing treatment assignment risk exposing subjects treatments. fig. ucts achieves higher expected response methods given percentage. knowing exist learning algorithm always performs better others regardless underlying data model hope demonstrated experiments section ucts competitive data sets. next section present distinct advantage ucts provable consistency. tree-based ensemble methods eluded theoretical analysis many years. since publication random forest become major analytical tool many areas application stable excellent performance. still open question whether algorithm consistent not. difﬁculties analysis come partly fact algorithm highly data-dependent partly randomization procedure. recent years several critical attempts making theory abbreviate lighten notation made clear dependence training data auxiliary randomness number trees given estimator treatment rule simply deﬁned lemma establishes connection consistency uplift problems regression problems. need ensure consistency simultaneously treatments need detail recursive partitioning algorithms. partition feature space generated recursive partitioning algorithm represented leaf nodes. given point feature space denote element contains suppose features distributed according density function expected fraction samples leaf node given training examples number examples paper consider case splits orthogonal splitting variables. therefore leaves rectangles diamj length along coordinate. rigorously describe theoretical results introduce following deﬁnitions. deﬁnition tree random-split tree every step tree-growing procedure marginalizing probability next split occurs along j-th feature bounded deﬁnition uplift tree -regular split leaves least fraction available training examples side split leaf node contains least training examples leaf node training examples treatment deﬁnition regularity different deﬁnition requires treatments least samples. need point latter likely ill-deﬁned small. consider example samples. samples treatment treatment regularity conditions listed satisﬁed choice uplift modeling similar situation. many algorithms proposed past decades achieved promising results various data sets. however best knowledge publication theoretical properties algorithms. order vacancy literature understand behavior algorithm section provide proof consistency proposed ucts algorithm. unlike theoretical studies random forest often concentrate simpliﬁed versions procedure proof ucts exactly described algorithm general framework uplift modeling that observing feature vector subject decision maker applies treatment subject observes response assume zero-mean random noise depend conditional expectation simply expectation taken obvious maximum expected response achieved point-wise optimal treatment rule maxt=..k given samples randomized experiment goal uplift algorithm construct treatment selection rule close possible. sense deﬁne consistency uplift algorithms following. ucts model consists collection randomized uplift trees estimator tree forest predicted value query point denoted independent random variables distributed generic random variable independent auxiliary random variable used subsample training data tree select splitting variables. averaging tree predictions gives predicted value forest increasing ease accessing analyzing large amount data comes possibility necessity personalization. uplift modeling proved important tool movement. algorithm presented paper addition competitive performance-wise ﬁlls vacancy literature provable consistency. also need hoeffding’s inequality binomial distribution. number success independently identically distributed bernoulli random variables success probability have lemma states leaf node -regular tree small probability measure. lemma leaf node -regular tree grown training examples satisﬁes following inequality lemma proves diameter leaf nodes random-split -regular tree shrinks dimensions number training examples grows. lemma leaf node randomsplit -regular tree grown training examples satisﬁes following inequality help lemma lemma proceed prove consistency ucts trees. intuition quite straightforward. properly tuning parameter split dimension leaf node vanishes well within-node variance response. addition estimate leaf node response arbitrary accuracy. features uniformly distributed d-dimensional unit hypercube i.e. assumption restrictive might seem. trees invariant monotone transformations distribution bounded support bounded nonzero density function rescaled without loss generality uniform distribution. internal node -regular tree child node. given number examples node number examples child node follows binomial distribution hoeffding’s inequality leaf node -regular tree. regularity know shallowest possible path root leaf created repeatedly splitting fraction training example termination conditions met. therefore number splits root leaf greater ln−. marginal probability split number made coordinate bounded splits coordinate stochastic lower bound again hoeffding’s inequality point clear long selected diminish properly therefore even single tree grown ucts consistent. easily establish consistency averaging ensemble following inequality given training examples auxiliary randomness denote partition feature space generated approximation random test data deﬁne i.e. contains data fall leaf also assigned treatment wager stefan susan athey. estimation inference heterogeneous treatment effects using random forests. journal american statistical association just-accepted chickering david maxwell david heckerman. decision theoretic approach targeted advertising. proceedings sixteenth conference uncertainty artiﬁcial intelligence. morgan kaufmann publishers inc. zhao xiao fang david simchi-levi. uplift modeling multiple treatments general response types. proceedings siam international conference data mining. society industrial applied mathematics", "year": 2017}