{"title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network  with Multimedia Pivot", "tag": ["cs.CL", "cs.AI", "cs.CV", "cs.MM"], "abstract": "We propose an approach to build a neural machine translation system with no supervised resources (i.e., no parallel corpora) using multimodal embedded representation over texts and images. Based on the assumption that text documents are often likely to be described with other multimedia information (e.g., images) somewhat related to the content, we try to indirectly estimate the relevance between two languages. Using multimedia as the \"pivot\", we project all modalities into one common hidden space where samples belonging to similar semantic concepts should come close to each other, whatever the observed space of each sample is. This modality-agnostic representation is the key to bridging the gap between different modalities. Putting a decoder on top of it, our network can flexibly draw the outputs from any input modality. Notably, in the testing phase, we need only source language texts as the input for translation. In experiments, we tested our method on two benchmarks to show that it can achieve reasonable translation performance. We compared and investigated several possible implementations and found that an end-to-end model that simultaneously optimized both rank loss in multimodal encoders and cross-entropy loss in decoders performed the best.", "text": "abstract propose approach build neural machine translation system supervised resources using multimodal embedded representation texts images. based assumption text documents often likely described multimedia information somewhat related content indirectly estimate relevance languages. using multimedia pivot project modalities common hidden space samples belonging similar semantic concepts come close other whatever observed space sample modality-agnostic representation bridging diﬀerent modalities. putting decoder network ﬂexibly draw outputs input modality. notably testing phase need source language texts input translation. experiments tested method benchmarks show achieve reasonable translation performance. compared investigated several possible implementations found end-to-end model simultaneously optimized rank loss multimodal encoders cross-entropy loss decoders performed best. machine translation important challenges natural language processing. irrespective traditional statistical machine translation modern neural machine translation methods data always mutually indispensable other. indeed success corpus-based mainly dependent quality scale available parallel corpora train systems. recent state-of-the-art systems shown translation surprisingly improved suﬃciently large-scale data high computational power hand prepare corpora remained problem. speciﬁc domains news patents wikipedia relatively high-quality multilingual translations made available content holders volunteer workers utilized researchers decades however general cases always possible collect suﬃcient amount parallel data generic documents monolingual. human cost preparing manual translation quite high particularly prohibitive minor language pairs resources severely limited. tackle situation parallel corpora available branch called pivot-based machine translation developed. idea pivot-based approach indirectly learn alignment source target languages help third modality although previous studies along line mainly based third language work propose novel general framework utilize arbitrary multimedia content pivot. nowadays easily abundant monolingual text documents rich multimedia content side information e.g. text photos videos posted social networking sites blogs. visual media expected less correlated counterpart texts following objective document. considering generally understand content images taken countries regardless language visual information universal representation ground diﬀerent languages. moreover recent years performance visual recognition dramatically improved owing huge success deep learning considered human level generic image recognition expect state-of-the-art visual recognition techniques mature enough accurately extract language-agnostic semantics images help improve natural language processing tasks. multimedia pivot-based machine translation established could possibly utilize abundant monolingual multimedia documents naturally provided users build high-performance open-domain systems. realize this propose neural network based method combining multimodal representation learning encoder-decoder models. note model align source encoder target decoder without source-to-target path training often utilized pseudo corpus based methods. moreover idea agnostic implementations encoder decoder networks. dealing limitations number good-quality parallel comparable corpora important issues cross-lingual learning. straightforward approach automatically mine parallel corpora typically noisy repositories. methods exploited bootstrap approach starting base translation systems whereas others utilized external meta information links coupling bilingual texts among them images also exploited cross-lingual document matching relatively early works. however methods simply rely reading near-duplicate detection images thus cannot identify similarities semantics fundamental limitation compared work. another line work train system non-parallel data help another modality indirect knowledge transfer called pivot-based machine translation. recent works focused existing popular language pivot creating direct parallel corpora minor language pairs practically diﬃcult major languages relatively often coupled language. source-to-target translation realized ﬁrst translating source language pivot language translating target language. nonetheless method still assumes source-pivot pivot-target parallel corpora available would require eﬀort human experts languages minor ones. moreover diﬃcult images pivot approach explicitly decoding image text well-established technique. therefore image-based pivots mainly used relatively easier tasks bilingual lexicon learning image similarity used criteria estimate relevance words attached images grounding natural language real-world representations always important topic computer vision would ﬁrst natural choice huge breakthrough convolutional neural networks visual recognition signiﬁcantly advanced terms accuracy ﬂexibility enabling development many brand-new technologies. amongst them image captioning automatically annotates description input image natural language become hottest topics recent years image captioning essentially interpreted translation image sentence drawn attention community well. recently research ﬁeld called multimodal machine translation proposed became subtask task images addition source languages inputs improve translation performance hopefully relaxing ambiguity alignment cannot solved texts only. feasibility approach demonstrated methods visual-based reranking results however task assumes images available part query testing phase thus objective setup entirely diﬀerent ours. non-language multimedia pivot need ﬂexible mechanism semantically align diﬀerent types data. idea derive common representation shared modalities. words whatever modality observed data belonging implicit concept mapped roughly point embedding space. classical standard method multimodal learning probably linear canonical correlation analysis successfully used image-language collaborations semantic image retrieval annotation well cross-lingual information retrieval recent methods based deep neural networks pairwise ranking loss shown signiﬁcantly improve multimodal embedding owing natural capability learning discriminative nearest-neighbor metrics stability gradient-based learning. successfully used image captioning within framework deep encoder-decoder model fig. models neural translation based pivot images training phase language encoders forced high correlations image encoder multimodal space decoder target language trained. testing phase translation realized simply feedforwarding multimodal space core zero-shot learning. further target sequence decoder multimodal representation compose end-to-end encoder-decoder model. theoretical viewpoint work line recently proposed methods form multi-stream encoder-decoder model. look models detail describe contribution section goal build translation model source language target language utilizing side information pivot. below call pair text description counterpart image document. training system suppose monolingual documents similarly also documents source language target language importantly overlap; share images all. obviously appear diﬀerent spaces share common visual space handled encoder. denote model divided roughly important components. ﬁrst component multimodal representation learning parameters encoders optimized mapped semantic space call multimodal space. good multimodal space obtained instances modalities roughly vector representation long tied together similar semantic concepts. second component build target language decoder multimodal space ﬁnal translation realized emphasized need texts input testing phase similar standard machine translation. figures illustrates approach. several options model topology training strategies thoroughly compared experiments. describe details following sections. hyperparameter margin similarity score function measures product. note outputs encoder unit normalized thus equal cosine similarity. denotes negative descriptions sampled mini-batch. probability model outputs ground truth word step two-way model closely related image-captioning model proposed except apply diﬀerent languages encoder decoder parts. viewed end-to-end fusion multimodal embedding image-captioning models. advantages three-way model two-way model many. first image-target alignment ignored two-way model images implicitly bind source target languages jointly enforcing high correlations three-way model. thus multimodal representation expected improved bridging languages. moreover simultaneously optimizing constraints would positive regularization eﬀect manner similar so-called multi-task learning. second unlike two-way model three-way model utilize images descriptions training decoders target language because mapped common representation. interpreted sort data augmentation expected improve robustness. loss reconstructing target descriptions follows investigate strategies training whole model. ﬁrst strategy two-step approach ﬁrst optimize encoder loss parameters encoders start optimizing decoder respect second strategy end-to-end approach jointly optimize encoder decoder losses. combined loss although work know ﬁrst attempt zero-resource machine translation using multimedia pivot theoretically close methods inspired model. topology network similar recently proposed many-to-one sequence-to-sequence model however model designed standard multi-task learning multimodal embedding layer like ours. therefore cannot align source encoder target decoder zero-shot situations. deal zero-shot problem firat incorporated synthetic parallel corpora explicitly include source-to-target path training. words approach zero-shot problem data-side approach model-side help multimodal embedding technique. rajendran used basically idea multimodal space implemented generalized neural encoders respectively. major diﬀerence models cross-modal decoders interest multimodal representation itself. show simultaneously optimizing decoders positive eﬀects decoding also learned representation itself. saha proposed end-to-end model multimodal embedding target decoder almost identical two-way model except multimodal fusion based correlation loss. described above three-way model including target encoder multimodal learning many advantages. fact signiﬁcantly improve multimodal representation decoding performance compared two-way model show experiments. study used publicly available multilingual image-description datasets. iapr-tc dataset images english german descriptions. original descriptions provided german english translations added professionals. recently published multik dataset speciﬁcally designed research multimodal machine translation. images english german descriptions image. extension flickrk image-caption dataset english german translations provided types bilingual annotations provided multik dataset. namely machine translation task multilingual image captioning task respectively. used former experiments followed oﬃcial training validation testing splits. preprocessing words converted lowercase tokenized using natural language toolkit appearing less times training splits replaced symbol. table summarizes statistics datasets experimental setup. randomly split data non-overlapping sets training validation testing. unnecessary modalities split ignored. notable direct english-german parallel data even validation sets. although current largest multi-lingual image description datasets know relatively small compared standard studies neural machine translation. note work beginning stage focus show feasibility zero-shot translation using multimedia pivot well investigate component model aﬀects relative improvements performance. choice encoders decoders modality within scope paper used standard neural models domain. visual encoder employed public vgg- network powerful widely used cnns pre-trained imagenet dataset used features layer vgg- another fully connected layers hidden units each tuned training. implementation used pre-computed features multik provided wmt’ multimodal machine translation task extrracted features iapr-tc using caﬀe language encoders decoders used recurrent neural networks long short-term memory used -dimensional word embedding -dimensional hidden units. note dimensions encoders equal coupled multimodal space. used adam optimizer mini-batch size training network stopped optimization validation loss longer improved. ﬁxed experiments. compose mini-batch language encoders decoders padded special null symbols align length sentences input batch standard practice seq-to-seq learning. image encoder visual representation example static -dimensional vector mini-batch simple matrix feature vectors batch examples rows order corresponding text-side mini-batch. training data randomly shuﬄed beginning epoch mini-batches order. evaluation mainly used standard corpus-level bleu metrics used multi-bleu.pl script moses toolkit compute bleu scores. also evaluated sentence-level bleu+ metrics modiﬁcation bleu smoothing terms higher-order n-grams making possible evaluate performance short sentences. note bleu score plain text corresponding bleu+ score parenthesis. table summarizes results baseline models. demonstrate performance could obtain supervised parallel corpus show scores sequence-to-sequence trained architectures changing number randomly sampled parallel data. separately evaluate eﬀectiveness multimodal space ﬁrst focused simple nearest-neighbor-based translation. namely query description source language retrieved nearest-neighbor training sample simply output description. experiment essentially measures retrieval performance appropriate evaluating multimodal representation itself. baseline implemented naive method based tfidf visual features. query ﬁrst retrieved similar document terms cosine similarity tfidf text features. then coupled image document retrieved nearest document terms distance features whose caption would output translation result. table shows results nearest-neighbor methods. with dec. represents multimodal space jointly trained decoder others indicate independently trained ones reference also noted performance randomly sampled description expected three-way model generally outperformed two-way model. interestingly observed performance improved directly retrieved descriptions target side. fact indicates descriptions projected multimodal space still represent useful information apparent images. hypothesize jointly optimizing multimodal embedding loss decoder loss result better multimodal space decoder learning good constraint multi-task learning framework. shown result with dec. models generally achieve better performance iapr-tc multik. reasonable independently trained multimodal space poor iapr-tc relatively good multik comparison tfidf feature baseline suggests. table shows detailed comparison approach diﬀerent conﬁgurations. comparing baselines best results roughly comparable sequence-to-sequence models number parallel sentences limited large monolingual ones. summarize ﬁndings below. comparison model topologies shows three-way models generally outperformed two-way counterparts. however images feed-forwarded training decoder diﬀerences performance subtle two-way model sometimes outperformed three-way model. attractive aspect three-way approach image description decoder training always provided best results. possibly utilize external monolingual corpora improve decoders would like investigate future work. training strategy end-to-end training generally achieved better results two-step approach diﬀerence large multik. iapr-tc results nearest-neighbor experiment suggest multimodal space relatively poor case jointly optimizing multimodal space decoder seemed signiﬁcantly improve performance. result also corresponds observation previous section. show loss curves english german translation task threeway models. figure show results iapr-tc multik respectively. note training loss cannot directly compared test loss based entirely diﬀerent criteria. nonetheless validation loss test loss converge similar timings making possible tune network properly. another observation decoder training seems overﬁtting earlier multik. could another reason end-to-end approach showed signiﬁcant improvement dataset. finally demonstrate qualitative results zero-shot translation table observed method actually translate many sentences correctly. besides successful ones also many interesting errors. many translations although overall description scene less relevant attributes numbers objects often missed. reasonable currently using single visual feature vector global cnns therefore diﬃcult align ﬁne-grained local information images correctly. tackle problem would promising integrate sophisticated object detection segmentation methods future. also observe number small grammatical errors possibly lack suﬃcient training data. expect problem mitigated utilizing external monolingual data target language. work tackled challenging task training system monolingual data containing multimedia side information. unlike many previous studies used multimedia simply addition texts inputs reinforce machine translation used parallel corpora training image inputs testing phase. system made possible training multimodal encoders share common modality-agnostic semantic representaein radfahrer einem gelben radtrikot kurzer schwarzer radhose einem graublauen helm f¨ahrt einem gelben rennrad linken straßenseite eines highways; dunkelh¨autiges m¨adchen langen schwarzen haaren einem blauen pullover steht einem braunen ufer vordergrund; dunkelblauer dahinter; weiße wolken einem blauen himmel hintergrund; tion using images pivot. compared several possible implementations showed feasibility approach. notably found three-way model particularly promising terms performance ﬂexibility handling various modality-speciﬁc data. although target paper fully unsupervised setup naturally include parallel data semi-supervised manner external monolingual text corpora target language enhance performance attractive direction future research. course experimental results also suggest long still signiﬁcant performance compared supervised sequence-to-sequence baselines. expect reduce expressive visual encoders powerful attention mechanisms multimodal learning methods remarkably improved recent years. moreover current method intrinsically limited domain texts grounded visual content always case generic documents. would like extend approach handle side information investigate automatically crawled noisy data important milestone realizing true zero-resource utilizing abundant multimedia monolingual documents web.", "year": 2016}