{"title": "Learning Robot Activities from First-Person Human Videos Using  Convolutional Future Regression", "tag": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "abstract": "We design a new approach that allows robot learning of new activities from unlabeled human example videos. Given videos of humans executing the same activity from a human's viewpoint (i.e., first-person videos), our objective is to make the robot learn the temporal structure of the activity as its future regression network, and learn to transfer such model for its own motor execution. We present a new deep learning model: We extend the state-of-the-art convolutional object detection network for the representation/estimation of human hands in training videos, and newly introduce the concept of using a fully convolutional network to regress (i.e., predict) the intermediate scene representation corresponding to the future frame (e.g., 1-2 seconds later). Combining these allows direct prediction of future locations of human hands and objects, which enables the robot to infer the motor control plan using our manipulation network. We experimentally confirm that our approach makes learning of robot activities from unlabeled human interaction videos possible, and demonstrate that our robot is able to execute the learned collaborative activities in real-time directly based on its camera input.", "text": "paper present cnn-based approach enables robot learning activities ‘human’ example videos. human activity videos attractive training resources require hardware professional software teaching robots even though might create difﬁculties like transferring learned human-based models actual robots. given videos humans executing activity human’s viewpoint objective make robot learn temporal structure activity future regression network learn transfer model motor execution. idea human’s ﬁrstperson video video humanoid robot expected obtain activity execution similar. providing ﬁrst-person human videos robot providing robot ‘visual memory’ performing activities previously. enables robot directly learn visual observation expected correct execution activity change viewpoint. previous works robot activity learning human videos extending previous concept ‘robot learning demonstration’ mostly done direct motor control data. however works focused learning grammar representations human activities modeling human activities sequence atomic actions approaches limited aspect activities always represented terms pre-deﬁned atomic actions users teach robot recognize atomic actions human activity videos providing labeled training data prevented robot learning activities scratch also limited human deﬁne atomic actions activity added. furthermore since trainable end-toend fashion robot somehow ﬁgure execute atomic actions usually done hand-coding motion. introduce robot activity learning model using fully convolutional network future representation regression. extend state-of-the-art convolutional object detection network representation human hand-object information video frame newly introduce concept using fully convolutional network regress intermediate scene representation change future frame objective make robot learn temporal structure activity future regression network learn transfer model motor execution. present deep learning model extend state-of-the-art convolutional object detection network representation/estimation human hands training videos newly introduce concept using fully convolutional network regress intermediate scene representation corresponding future frame combining allows direct prediction future locations human hands objects enables robot infer motor control plan using manipulation network. experimentally conﬁrm approach makes learning robot activities unlabeled human interaction videos possible demonstrate robot able execute learned collaborative activities real-time directly based camera input. important abilities humans able learn activities motor controls others’ behaviors. person watches others performing activity he/she learns visually predict future consequences motion activity also learns execute activity himself/herself. recently approaches taking advantage deep learning robot manipulation gaining increasing amount attention directly learning motor control policies given visual inputs convolutional neural networks particularly successful since able jointly learn image features optimized task based training data. ability models incorporating convolutional recurrent neural networks likely become major trend robotics like already happened computer vision machine learning. however although deep learning oriented approaches showed promising results learning video prediction actual motor control policy limited relatively simple actions object grasping pushing. large amount ‘robot’ data necessary direct training cnns rnns millions parameters. large number samples humans motor controlling robot necessary generating training data fig. overview perception component perception component consists fully convolutional neural networks ﬁrst network extended version state-of-the-art convolutional object detection network representation human hands estimation bounding boxes second network future regression network regress intermediate scene representation corresponding future frame. network require activity labels hand/object labels videos training. later). combining allows direct explicit prediction future hand locations allows robot feature-level prediction future representations also semantic-level prediction explicit future hand locations humans robots learned activity jointly performed network. future hand prediction results used manipulation network learns mapping hand locations image coordinate actual motor control. activity learning unsupervised approach aspect require activity labels hand/object labels activity videos. require hand-annotated training data learning hand representation network already exists public datasets purpose require labels future regression network. future regression network learned without supervision capturing changes hand-based representations training videos. addition importantly networks designed function real-time actual robot operation show capability experiments paper. demonstration since enables robots automatically learn task demonstration non-robotics expert important robotics. however limitations since approaches focused making robots learn motor control polices human data usually form direct control sequences obtained actual robots simulation softwares moreover often requires knowledge primitive actions teaching high-level tasks also previous works robot activity learning visual data extending previous concept lfd. works focused learning grammar representations human activities conventional third-person videos modeling human activities sequence atomic actions grammar representation composed atomic actions allows transfer human activity structure robots robot replication human activities possible usually hand-coded motion transfer human atomic actions robot atomic actions. however activity learning generally done fully supervised fashion human annotations approaches assumed reliable estimation semantic features videos human hands human body skeletons. studied approach directly learn object manipulation trajectories video prediction approach paper generate proper robot behaviors predicting ‘future’ visual representation. idea representation leads estimation future positions objects hands humans robots. visual prediction core components perception system. previous works prediction future frames computer vision community however limited attempt applying future predictions robotics systems since approaches general requires components interpreting predicted representation generate robot actions. works robot manipulation actually attempted. exists recent robotics work attempted applying visual prediction generating robot control actions study shows potential applying visual prediction robotic manipulation task; enables transferring visual perception robot manipulation component generating motor control commands without additional components interpret recognition results. however requires huge amount training data using actual physical robots make robot learn activities thus limited robot needs learn many activities. first-person videos first-person videos also called egocentric videos videos taken actor’s viewpoint. recognition human/robot activities ﬁrst-person videos actively studied particularly past years including recognition human actions wearable cameras human-robot interactions robot cameras however focused building discriminative video classiﬁers attempt learn ‘executable’ representations human activities transfer robots limited. main contribution paper enabling robot activity learning human interaction videos using newly proposed convolutional future regression. believe ﬁrst work present deep learning-based method learning human-robot interactions human-human videos. also believe ﬁrst paper take advantage human ‘ﬁrst-person videos’ robot activity learning. given sequence current frames goal predict future hand locations interactive objects front robot generate robot control commands moving robot’s hands predicted hand locations. employ components achieving goal. ﬁrst component perception component consists fully convolutional neural networks extended version single shot multibox detector create hand-based scene representation estimate bounding boxes future regression network model intermediate scene representation change future frames. second component manipulation component maps hand locations image coordinate actual motor control using fully connected layers. idea approach proposed perception component allows prediction future hand locations given current video input camera. future prediction learned based humans’ ﬁrstperson activity videos using training data assumption robot camera similar viewpoint human ﬁrst-person videos. allows robot directly predict ideal future hand locations activity inferring hand move activity executed successfully. next manipulation component generates actual robot control commands move robot’s hands predicted future locations. hand representation network ﬁrst construct network hand-based representation image scene extending object detection framework. extended inserting fully convolutional auto-encoder convolutional layers followed deconvolutional layers dimensionality reduction. allows approach abstract image lower dimensional intermediate representation. convolutional/deconvolutional layers kernels number ﬁlters convolutional layer green convolutional layers fig. correspond them. convolutional layers deconvolutional layers symmetric number ﬁlters pooling layer instead stride last convolutional layer dimensionality reduction. thus increase number ﬁlters last convolutional layer compensate loss information. function denotes feature extractor compressed intermediate visual representation indicates estimator uses compressed representation input locating hand boxes time formulation network predict hand locations time training. future regression network although hand representation network allows obtaining hand boxes ‘current’ frame objective ‘future’ hand locations ˆyt+∆ instead current locations ˆyt. fig. summarizes data perception component testing phase. given video frame time extract intermediate scene representation using feature extractor feed future regression network future scene representation ft+∆. next feed ˆft+∆ estimator ﬁnally obtain future position hands ˆyt+∆ time advantage formulation allows predict future hand locations considering implicit activity object context even without explicit detection objects scene. auto-encoder-based intermediate representation abstracts scene conﬁguration internally representing objects/hands currently scene fully convolutional future regressor takes advantage prediction. although perception component able predict future hand locations humans ﬁrst-person human activity videos insufﬁcient robot manipulation. here construct another regression network mapping predicted human hand locations image coordinate actual motor control commands. main assumption video frame robot’s camera similar viewpoint training data allowing take advantage learned model robot future hand prediction assuming manipulation component predicts future robot joint states given current robot joint states robot hand locations future hand locations telling robot’s hands move network formulated function manipulation component consists seven fully connected layers following number hidden units layer weights network obtained used perception networks indicates robot joint states time training episode represents robot hand locations time training episode fig. shows manipulation component generating robot control commands. formulate problem regression problem. main idea intermediate representation hand representation network abstracts hand-object information scene able take advantage infer future representation ˆft+∆. regression becomes possible simply plug-in predicted future representation ˆft+∆ remaining part hand network obtain ﬁnal future hand prediction results. therefore newly design network predicting intermediate scene representation corresponding future frame ˆft+∆ fully convolutional future regression network fig. seven convolutional layers kernels. addition layer kernels followed last layer kernel. trained weights regression network unlabeled ﬁrst-person human activity videos using following loss function future regression network intermediate scene representation intermediate layers hand network auto-encoder lower dimensionality. finally future scene representation ft+∆ hand network estimating hand boxes corresponding future frame future hand locations yt+∆. fig. robot manipulation component approach. generates robot control commands given current robot joint state current robot hand locations predicted future robot hand locations. combination perception component manipulation component provides real-time robotics system takes video frames input generates motor control commands activity execution. manipulation component replaced standard inverse kinematics neural network-based model generates natural movements considering desired location robot’s end-effectors well joint conﬁguration sequences egohands public dataset containing ﬁrst-person videos people interacting four types activities frames ground-truth hand labels. here added frames groundtruth annotations original dataset cover hand postures. dataset learn hand representation network trained locate hand boxes video frame. unlabeled human-human interaction videos collected total ﬁrst-person videos human-human collaboration scenarios video clip ranging seconds. dataset main dataset teaching task robot. contains types tasks person wearing camera cleaning objects table partner approaches table holding heavy person wearing camera pushing trivet table toward partner he/she approaching table holding cooking pan. videos unlabeled videos without activity/hand annotation trained convolutional regression network using dataset. image plane robot’s corresponding joint angles time recorded ﬁles making human operator move robot arms obtained robot joint conﬁguration sequences moving robot cover possible motion general human-robot interaction tasks. here assume robot supposed operate similar environment test phase. note recorded interaction scenario annotation regarding activity motion provided. used baxter research robot recording ﬁles baxter seven degrees-of-freedom contains variables arm. order estimate robot’s hand position image plane projected positions baxter’s grippers image plane recorded projected positions joint angles order provide quantitative comparisons compared perception component four different baselines hand-crafted representation uses hand-crafted state representation based explicit object hand detection. encodes relative distances interactive objects scenarios uses future hand location using neural network-based regression. speciﬁcally detects objects using kaze features hands using based hand detector computes relative distances objects hands building state representation dimensional vector. then built network fully connected layers trained using state representations interaction dataset use. hands uses hand locations future regression. predicts future hand locations solely based current hand locations withconsidering visual representations. order train baseline model extracted hand locations frames interaction videos using hand representation network made ﬁles store detected hand locations frame frame numbers. this trained another neural network model future hand location prediction using ﬁles seven fully connected layers number hidden units robot manipulation network. future annotations baseline uses original model trained based egohands dataset. instead training model infer current hand locations given input frame ﬁne-tuned model egohands dataset changing annotations dataset future locations hands instead making current hand locations. also used additionally frames ﬁnetuning since original egohands dataset insufﬁcient training. future annotations baseline also using original model trained model scratch. time changed annotations egohands dataset trained model. ﬁne-tuned model used future annotations baseline. ﬁrst evaluated perception component approach terms precision recall f-measure compared baselines. ﬁrst evaluation made approach predict bounding boxes human hands future frame given current image frame. measured intersection union ratio areas predicted ground truth hand locations. ratio greater predicted accepted true positive. experiment randomly split human-human interaction videos training testing sets videos used training sets remaining videos used testing sets total videos. table shows quantitative results future hand prediction. here plus-minus sign indicates standard deviation represents number frames used input regression network. frames able clearly observe approach signiﬁcantly outperforms including state-of-the-art object detector modiﬁed hand prediction. proposed network yielded best performance terms three metrics score f-measure. best performance tions hands. size image plane measured mean pixel distance ground truths predictions present frame. table shows mean pixel distance errors four types hands more conﬁrm approaches greatly outperform performance baselines. overall average distance high changes human hand shapes variations sufﬁcient terms generating robot motion. also compared accuracies methods considering right hand predictions since position right hand important robot manipulation locations types hands. because test scenarios robot’s activities focused right hand motion. table shows mean pixel distance ground truth predicted position right hand’. performances approaches superior baselines. examples visual predictions results illustrated fig. finally conducted user study evaluate success level robot activities performed based proposed approach human subjects. total participants recruited campus asked perform activities together robot. interactions participants asked complete questionnaire robot behaviors task. questionnaire statements scales express impression robot behaviors fig. examples visual prediction. ﬁrst example activity clearing table second example activity pushing trivet toward person holding cooking pan. ﬁrst shows input frames second shows future hand prediction results. third overlaid predictions future frames. boxes correspond predicted left hand’ locations blue boxes correspond right hand’ green boxes correspond opponent’s left hand cyan boxes correspond opponent’s right hand. frames captured every second. think robot cleared table make space task think robot passed trivet closer cooking task addition approach designed implemented following three baselines compared quantitative results base base control uses baseline future annotations perception component base manipulation network trained using robot activity ﬁles. base control network direct maps current hand locations image plane current seven joint angles robot without term base control uses future annotations perception component manipulation component generate motor commands. perception base control used perception component predict future hand locations base control network manipulation. cases ﬁnal control robot performed taking advantage baxter providing estimated future joint angle conﬁguration. result participant interacted robot total times random order. table shows results. results indicate participants evaluated robot approach performed better tasks. received higher average score compared baselines participants. examples real-time robot experiments human subjects illustrated fig. method operates slow real-time unoptimized code. takes frame using nvidia pascal titan able conduct real-time human-robot collaboration experiments using paper proposed robot activity learning model using fully convolutional network future representation regression. main idea make robot learn temporal structure human activity future regression network learn transfer model motor execution using manipulation network. show approach enables robot infer motor control commands based prediction future human hand locations real-time. experimental results conﬁrm approach predicts future locations human/robot hands reliably also able make fig. qualitative results real-time robot experiments. similar fig. examples clearing table pushing trivet toward person. example ﬁrst shows exact frames used inputs robot second shows robot human person viewpoint. frames captured every second. robots execute activities based predictions. paper focuses robot learning location-based hand movements handling dynamic hand posture changes remains future challenges. gupta eppner levine abbeel learning dexterous manipulation soft robotic hand human demonstrations ieee/rsj international conference intelligent robots systems kitani okabe sato sugimoto fast unsupervised ego-action learning ﬁrst-person sports videos ieee conference computer vision pattern recognition bambach crandall lending hand detecting hands recognizing activities complex egocentric interactions ieee international conference computer vision", "year": 2017}