{"title": "Averaged-DQN: Variance Reduction and Stabilization for Deep  Reinforcement Learning", "tag": ["cs.AI", "cs.LG", "stat.ML"], "abstract": "Instability and variability of Deep Reinforcement Learning (DRL) algorithms tend to adversely affect their performance. Averaged-DQN is a simple extension to the DQN algorithm, based on averaging previously learned Q-values estimates, which leads to a more stable training procedure and improved performance by reducing approximation error variance in the target values. To understand the effect of the algorithm, we examine the source of value function estimation errors and provide an analytical comparison within a simplified model. We further present experiments on the Arcade Learning Environment benchmark that demonstrate significantly improved stability and performance due to the proposed extension.", "text": "need expressive ﬂexible non-linear function approximation emerges. except successful attempts combination non-linear function approximation considered unstable shown diverge even simple domains recent deep q-network algorithm ﬁrst successfully combine powerful non-linear function approximation technique known deep neural network together q-learning algorithm. presented remarkably ﬂexible stable algorithm showing success majority games within arcade learning environment increased training stability breaking problem sequential supervised learning tasks. introduces concept target network uses experience replay buffer following work additional modiﬁcations extensions basic algorithm increased training stability. schaul suggested sophisticated sampling strategy. several works extended standard exploration techniques deal high-dimensional input mnih showed sampling could replaced asynchronous updates parallel environments wang suggested network architecture base advantage function decomposition work address issues arise combination q-learning function approximation. thrun schwartz ﬁrst investigate issues termed overestimation phenomena. operator q-learning lead overestimation state-action values presence noise. hasselt suggest double-dqn uses double q-learning estimator method solution problem. additionally hasselt showed q-learning overestimation occur practice instability variability deep reinforcement learning algorithms tend adversely affect performance. averaged-dqn simple extension algorithm based averaging previously learned q-values estimates leads stable training procedure improved performance reducing approximation error variance target values. understand effect algorithm examine source value function estimation errors provide analytical comparison within simpliﬁed model. present experiments arcade learning environment benchmark demonstrate signiﬁcantly improved stability performance proposed extension. reinforcement learning agent seeks optimal policy sequential decision making problem learning action optimal environment state. course time many algorithms introduced solving problems including q-learning sarsa policy gradient methods methods often analyzed setup linear function approximation convergence guaranteed mild assumptions practice real-world problems usually involve high-dimensional inputs forcing linear function approximation methods rely upon hand engineered features problem-speciﬁc state representation. problemspeciﬁc features diminish agent ﬂexibility work suggests different solution overestimation phenomena named averaged-dqn based averaging previously learned q-values estimates. averaging reduces target approximation error variance leads stability improved results. additionally provide experimental results selected games arcade learning environment. popular algorithms q-learning algorithm algorithm based simple value iteration update directly estimating optimal value function tabular q-learning assumes table contains action-value function estimates preform updates using following update rule number states large maintaining looktable possible state-action pairs values memory impractical. common solution issue function approximation parametrized deep networks present algorithm slightly different formulation algorithm iteration algorithm solves supervised learning problem approximate action-value function extension implementing function approximation form consider usual learning framework agent faced sequential decision making problem interaction environment takes place discrete time steps time agent observes state selects action results scalar reward transition next state consider inﬁnite horizon problems discounted cumulative reward ob−trt discount factor. goal agent optimal policy maximize expected discounted cumulative reward. value-based methods solving problems encode policies value functions denote expected discounted cumulative reward given state following policy speciﬁcally interested state-action value functions target values constructed using designated target-network expectation taken w.r.t. sample distribution experience transitions buffer loss minimized using stochastic gradient descent variant sampling mini-batches buffer. additionally requires exploration procedure interact environment number experience transitions added exploration note original implementation transitions added buffer simultaneously minimization loss using hyperparameters employed mnih experience transitions buffer replaced between target network parameter updates sampled minimization. averaged-dqn algorithm extension algorithm. averaged-dqn uses previously learned q-values estimates produce current action-value estimate averaged-dqn algorithm stabilizes training process reducing variance target approximation error elaborate section computational effort compared k-fold forward passes q-network minimizing loss number back-propagation updates remains dqn. output algorithm average last previously learned q-networks. figure averaged-dqn performance atari game breakout. bold lines averages seven independent learning trials. every frames performance test using \u0001-greedy policy frames conducted. shaded area presents standard deviation. averaged-dqn hyperparameters used taken mnih note recently-learned state-action value estimates likely better older ones therefore also considered recency-weighted average. practice weighted average scheme improve performance therefore presented here. next discuss various types errors arise combination q-learning function approximation algorithm effect training stability. refer dqn’s performance breakout game figure source learning curve variance dqn’s performance occasional sudden drop average score usually recovered next evaluation phase another phenomenon observed figure initially reaches steady state followed gradual deterioration performance. figure double-dqn averaged-dqn performance average value estimates atari game asterix. bold lines averages seven independent learning trials. shaded area presents standard deviation. every frames performance test using \u0001-greedy policy frames conducted. hyperparameters used taken mnih error learned relatae result several factors firstly sub-optimality inexact minimization. secondly limited representation power neural lastly generalization error unseen state-action pairs ﬁnite size buffer. q-learning overestimation phenomena ﬁrst investigated thrun schwartz work thrun random varischwartz considered able uniformly distributed interval expected overmax operator target estimation errors upper bounded intuition upper bound worst case values equal equality upper bound overestimation error different nature since presents positive bias cause asymptotically sub-optimal policies shown thrun schwartz later hasselt environment. note uniform bias action-value function cause change induced policy. unfortunately overestimation bias uneven bigger states q-values similar different actions states start long trajectory assume statistical model mentioned start section. consider unidirectional markov decision process figure agent starts state state terminal state reward state equal zero. example gives intuition behavior variance dqn. accumulated past iterations updates trajectory. accumulation errors results bigger variance associated adverse effect discussed section double q-learning implementation possible approach tackle overestimation problem replaces positive bias negative one. another possible remedy adverse effects error directly reduce variance proposed scheme figure repeated experiment presented hasselt experiment discussed hasselt example overestimation leads asymptotically sub-optimal policies. since averageddqn reduces variance experiment supports hypothesis main cause overestimation variance. analyse variance ﬁrst must assume statistical model similar thrun schwartz suppose random process furthermore cov. however obtain explicit comparison specialize model m-state unidirectional figure consider approaches variance reduction. ﬁrst averaged-dqn second term ensemble-dqn. start ensemble-dqn straightforward obtain variance reduction evaluate averaged-dqn adopt typical methodology agent performance measured training. refer reader liang discussion evaluation methods benchmark. hyperparameters used taken mnih presented completeness appendix code taken mcgill university rllab available online evaluated averaged-dqn algorithm three atari games arcade learning environment game breakout selected popularity relative ease reach steady state policy. contrast game seaquest selected relative complexity signiﬁcant improvement performance obtained variants wang finally game asterix presented hasselt example overestimation leads divergence. seen figure table three games increasing number averaged networks averaged-dqn results lower average values estimates better-preforming policies less variability runs independent learning trials. game asterix similarly hasselt divergence prevented averaging. overall results suggest practice averaged-dqn reduces variance leads smaller overestimation stabilized learning curves signiﬁcantly improved performance. gridworld problem common benchmark opposed gridworld smaller state space allows buffer contain possible state-action pairs. additionally allows optimal value function acmeaning averaged-dqn theoretically efﬁcient variance reduction ensemble-dqn least times better dqn. intuition averaged-dqn averages taes averages value estimates next states. figure shows averaged-dqn performance different number averaged networks three atari games. averaged-dqn reduced dqn. bold lines averaged seven independent learning trials. every frames performance test using \u0001-greedy policy frames conducted. shaded area presents standard deviation. bottom shows average value estimates three games. seen number averaged networks increased overestimation values reduced performance improves less variability observed. hyperparameters used taken mnih experiments used averaged-dqn ensemble-dqn buffer containing possible state-action pairs. network architecture used composed small fully connected neural network hidden layer neurons. minimization loss adam optimizer used mini-batches samples target network parameters update ﬁrst experiment minibatches second. experiment problem gridworld state space contains pairs points discrete grid }xy∈...). algorithm interacts environment pixel features one-hot feature })xy∈.... four actions corresponding steps compass direction reward state otherwise. consider discounted return problem discount factor figure seen increasing number averaged target networks leads reduced overestimation eventually. also averaged target networks seem reduces overshoot values leads smoother less inconsistent convergence. table columns present average performance averaged-dqn frames using \u0001-greedy policy frames. standard variation represents variability seven independent trials. average performance improved number averaged networks. human random performance taken mnih figure averaged-dqn average predicted value gridworld. increasing number averaged target networks leads faster convergence less overestimation bold lines averages independent learning trials shaded area presents standard deviation. ﬁgure abcd present averaged-dqn average overestimation. sequence overestimates values. note ensemble-dqn implemented experiments demanding computational effort empirical evidence already obtained simple gridworld domain. work presented averaged-dqn algorithm extension stabilizes training improves performance efﬁcient variance reduction. shown theory practice proposed scheme superior variance reduction compared straightforward computationally demanding approach ensemble-dqn demonstrated several games atari increasing number averaged target networks leads better polifigure averaged-dqn ensemble-dqn predicted value gridworld. averaging past learned value beneﬁcial learning parallel. bold lines averages independent learning trials shaded area presents standard deviation. cies reducing overestimation. averaged-dqn simple extension easily integrated variants schaul hasselt wang bellemare indeed would interest study added value averaging combined variants. also since averaged-dqn variance reduction effect learning curve systematic comparison different variants facilitated discussed future work dynamically learn many networks average best results. simple suggestion correlate number networks state td-error similarly schaul finally incorporating averaging techniques similar averageddqn within on-policy methods sarsa actorcritic methods stabilize algorithms. bellemare marc srinivasan sriram ostrovski georg schaul saxton david munos remi. unifying count-based exploration intrinsic motivation. arxiv preprint arxiv. boyan justin moore andrew generalization reinforcement learning safely approximating value function. advances neural information processing systems liang yitao machado marlos talvitie erik bowling michael. state control atari games proceedings using shallow reinforcement learning. international conference autonomous agents multiagent systems mnih volodymyr kavukcuoglu koray silver david graves alex antonoglou ioannis wierstra daan riedmiller martin. playing atari deep reinforcement learning. arxiv preprint arxiv. mnih volodymyr kavukcuoglu koray silver david rusu andrei veness joel bellemare marc graves alex riedmiller martin fidjeland andreas ostrovski georg human-level control deep reinforcement learning. nature mnih volodymyr badia adria puigdomenech mirza mehdi graves alex lillicrap timothy harley silver david kavukcuoglu koray. asynchronous methods deep reinforcement learning. arxiv preprint arxiv. riedmiller martin. neural ﬁtted iteration–ﬁrst experiences data efﬁcient neural reinforcement learning method. european conference machine learning springer sutton richard mcallester david singh satinder mansour yishay. policy gradient methods reinforcement learning function approximation. nips volume tang haoran rein houthooft davis foote adam stooke chen duan john schulman filip turck pieter abbeel. exploration study count-based exploration deep reinforcement learning. arxiv preprint arxiv. figure presents single learning trial compared averaged-dqn emphasizes source variability learning trials occasions drops average score within learning trial. suggested section effect related causing deviation steady state policy. figure averaged-dqn performance atari game breakout. bold lines single learning trials averaged-dqn algorithm. dashed lines present average independent learning trials. every frames performance test using \u0001-greedy policy frames conducted. shaded area presents standard deviation averaged-dqn hyperparameters used taken mnih selected three popular games atari experiment averaged-dqn. used exact setup proposed mnih experiments provide details completeness. implementation available https//bitbucket.org/oronanschel/ atari_release_averaged_dqn. network input tensor. contains concatenation last four observed frames. frame rescaled gray-scale. used three convolutional layers followed fully-connected hidden layer units. ﬁrst convolution layer convolves input ﬁlters size second layers size ﬁnal ﬁlters size activation unit layers rectiﬁer linear units fully connected layer output different q-values minimization loss function rmsprop used. discount factor optimizer learning rate steps target network updates training done frames. agent evaluated every steps size experience replay memory tuples. buffer gets sampled update network every steps mini batches size exploration policy used \u0001-greedy policy decreasing linearly steps.", "year": 2016}