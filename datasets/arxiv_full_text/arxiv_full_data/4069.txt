{"title": "Structured Sparse Method for Hyperspectral Unmixing", "tag": ["cs.CV", "cs.AI"], "abstract": "Hyperspectral Unmixing (HU) has received increasing attention in the past decades due to its ability of unveiling information latent in hyperspectral data. Unfortunately, most existing methods fail to take advantage of the spatial information in data. To overcome this limitation, we propose a Structured Sparse regularized Nonnegative Matrix Factorization (SS-NMF) method from the following two aspects. First, we incorporate a graph Laplacian to encode the manifold structures embedded in the hyperspectral data space. In this way, the highly similar neighboring pixels can be grouped together. Second, the lasso penalty is employed in SS-NMF for the fact that pixels in the same manifold structure are sparsely mixed by a common set of relevant bases. These two factors act as a new structured sparse constraint. With this constraint, our method can learn a compact space, where highly similar pixels are grouped to share correlated sparse representations. Experiments on real hyperspectral data sets with different noise levels demonstrate that our method outperforms the state-of-the-art methods significantly.", "text": "hyperspectral unmixing received increasing attention past decades ability unveiling information latent hyperspectral data. unfortunately existing methods fail take advantage spatial information data. overcome limitation propose structured sparse regularized nonnegative matrix factorization method following aspects. first incorporate graph laplacian encode manifold structures embedded hyperspectral data space. highly similar neighboring pixels grouped together. second lasso penalty employed ss-nmf fact pixels manifold structure sparsely mixed common relevant bases. factors structured sparse constraint. constraint method learn compact space highly similar pixels grouped share correlated sparse representations. experiments real hyperspectral data sets different noise levels demonstrate method outperforms state-of-the-art methods signiﬁcantly. hyperspectral imaging widely used many ﬁelds since provides ability record scene hundreds contiguous narrow spectral bands spatial resolution sensor spectra spatial neighboring substances inevitably merge together leading mixed pixels hyperspectral data. makes hyperspectral unmixing essential step hyperspectral image analysis. generally task decompose pixel spectrum constituent spectra corresponding percentages paper focus unsupervised problem endmembers abundances unknown making challenging problem. general linear methods roughly classiﬁed categories geometrical methods statistical ones geometrical methods assume hyperspectral pixels located within simplex whose vertices correspond endmembers. n-findr vertex component analysis typical geometrical methods. former treats vertices simplex maximum volume endmembers latter searches endmembers projection. methods simple fast. unfortunately require existence pure pixels endmember usually unavailable practice. moreover geometrical methods fail consider local structures latent hyperspectral data leading inaccurate estimation endmembers abundances. figure illustration priors. hyperspectral image. abundances targets presented proportions colors tree water soil road example mixed pixels yellow patch; color proportion pixel pointed arrow .red +.green road structure latent data. spectral curves pixels- similar road structure. accordingly number statistical methods proposed. nonnegative matrix factorization extensions typical statistical methods. unsupervised method tries nonnegative matrices approximate original matrix product. nonnegative constraint factor matrices valuable reasons. first endmembers corresponding abundances nonnegative makes physically suited problem. second nonnegative constraint allows additive subtractive combinations leading parts-based representation parts-based property makes representation results intuitive interpretable since psychological physiological evidences shown human brain works parts-based although enjoyed great success many applications face analysis documents clustering three weaknesses. first solution space large caused non-convex objective function. many extensions proposed reduce solution space adding various constraints framework non-smooth mvc-nmf mdmd-nmf gl-nmf second parts-based property usually strong enough problem resulting less expressive estimation endmembers. possible solution employ sparse constraint framework regularization parameter control weight parts-based property. third method consider similarities neighboring pixels leading structure information unused. paper propose effective method named structured sparse overcome three limitations nmf. motivation based priors. first smoothness local patches images many regions spectral curves pixels similar other. fig. illustrates example pixels road region similar visual appearance regions structures preserved unmixing. second pixels hyperspectral data mixed endmembers summary pixels structure sparsely mixed common relevant endmembers. obviously observed fig. proportions four colors represent abundances four endmembers blue green black seen colors fig. regional smooth sparsely mixed blue green black ink. pixels. highly related neighboring pixels grouped graph. accordingly graph constraint employed ss-nmf transfer structures unmixing results. second lasso penalty sparsely encode abundances pixel. ss-constraint learn abundance space abundances regional smooth sparse shown fig. experiments real hyperspectral data different noise levels illustrate ss-nmf outperforms state-of-the-art methods. remarks method propose meaningful ss-constraint apply framework. basic idea encourage highly similar pixels graph share correlated sparse abundances similar help ss-constraint ss-nmf overcomes three limitations problem. ss-nmf effective method problems. compare method several state-of-the-art methods hyperspectral data. quantitative qualitative performances show method outperforms state-of-the-art methods signiﬁcantly. remainder paper organized follows. section brieﬂy review linear mixture model nonnegative matrix factorization method. section presents formulation optimization algorithm convergence proof analyzes computation complexity ss-nmf method. extensive results well detailed comparisons analyses presented section followed conclusions section hyperspectral image column vectors corresponding pixels rl×n rk×n contains fractional abundances endmembers pixel k-th vector contains fractional abundances k-th endmember associated pixels. called k-th abundance map. rl×n residual term. problems intimates endmembers latent hyperspectral data space. addition pixels mixed parts endmembers. therefore good approximations achieved following conditions satisﬁed unveil intrinsic structures hyperspectral data whose rank learned endmembers expressive figure highly similar neighboring pixels could grouped local structures. hyperspectral image. local windows highly similar neighboring pixels grouped together. local structures combined highly similar pixels windows grouped together. window covers edge ‘tree’ ‘road’ sides. frobenius norm elements rl×k unknown nonnegative. although convex respect respectively non-convex variables together therefore unrealistic global minima. alternately proposed multiplicative update rules follows compared vector quantization method principal component analysis method nonnegative constraint factor matrices enables parts-based representation makes superior facial image analyses document representations mentioned before applies nonnegative constraint factor matrices leads parts-based representation original data. however fails take consideration sparse prior abundances manifold structures hidden hyperspectral data essential problem. section introduce ss-nmf method overcome limitations adopting structured sparse constraint. framework ss-nmf fundamental problem construct structures. manifold learning theory spectral graph theory provide good idea local structures learned encoding similarities pixels nearest neighbor graph. combining local overlapping structures graph structure contains full similarity information pixel pairs. typically ways deﬁne nearest neighbors considering feature distances spatial distances only. former well suited less geometrically intuitive databases treat image node yale database database latter adapted geometrically motivated databases treat pixel grids plane nodes hyperspectral images. local smoothness prior hyperspectral images spatially neighboring pixels likely enjoy similar features however prior hold image edges pixels sides dissimilar. motivates weighting method proposed ss-nmf would thoroughly introduced following paragraph. proposed weighting method considers spatial distance feature distance simultaneously constructing local structures. suppose given hyperspectral image rl×n pixels. view pixel node construct weighted graph nodes. weight nodes obtained spectral angle distance metric deﬁned follows neighbors collected conditions pixel nearest spatial distance i.e. local window nearest feature distance i.e. calculating similarities neighboring pixels local window among shown fig. result partly avoid spreading graph across dissimilar pixels. essential local window covers edges pixels sides belonging different targets. fig. illustrates example ‘road’ side edge ‘tree’ side. metric deﬁned suitable metric since essential estimation metric endmembers process build local weighted graph structure node center. highly similar neighboring pixels grouped together repeating procedures pixel construct weight matrix contains manifold structures latent hyperspectral data. goal transfer structures learned abundance space. could done ﬁrst measuring distance abundance vectors minimizing manifold structures original data space transferred learned abundance space. however considered fact manifold structures hyperspectral data dominated speciﬁc sets endmembers. lasso penalty abundance matrix adopted resulting structured sparse constraint regularization deﬁned provides lasso penalty structure constraint sparse abundance vectors related structure tend similar. example local structure corresponding sparse abundances tend similar. addition controls strongly share similar abundances. renders structured sparse constraint deﬁned regularization parameters control complexity model elements nonnegative unknown. present optimization method solve problem deﬁned following subsection. similar problem objective function non-convex together. iterative algorithm could reach local minima introduced subsection. considered nonnegative constraint objective function could reformulated based karush-kuhn-tucker conditions θlkmlk φknakn could obtain following equations letting partial derivatives equal zero multiplying sides eqs. respectively worthwhile point form solution algorithm solution diagonal matrix positive diagonal elements. uncertainty simple widely used method scale column unit -norm -norm). achieved algorithm ss-nmf input hyperspectral data number endmembers parameters threshold output endmember matrix rl×k initialize endmember matrix randomly selecting pixels newly selected pixel dissimilar pixels selected initialize random positive values scale column unit -norm. subsection demonstrate optimization problem non-increasing using updating rules iteration ﬁnally converges local minima ﬁnite iterations. since updating rule convergence proof provided focus proof updating rule common skill used expectation maximization algorithm adopted introducing auxiliary function exactly upper bound function. means could obtain updating rule minimizing function t-th iteration lemma function deﬁned auxiliary function proof i.e. satisﬁed speciﬁcally prove since constant term linear term gradient descent method widely used optimization algorithm local minimum objective function. subsection relationship updating rules deﬁned eqs. gradient descent method. problem deﬁned basic updating rules given parameters rl×k rk×n learning rates. problem conditions ensure physical meaningful local minima obtained updating rules first values relatively small local minima. second values could ensure nonnegative property iteration. kind choices determine learning rates follows number bands number endmembers number pixels hyperspectral image number pixels local window percentage nearest neighbors local window number iterations subsection compare computational complexity proposed ss-nmf algorithm. since ss-nmf algorithms solved iteration way. describe complexity analysis steps. first analyze computational complexity iteration; second number iteration steps considered. besides updating rules ss-nmf needs construct structure relationships pixel pairs. suppose totally iterations needed convergence result. total computational cost seen computational complexity ss-nmf methods need iterations. however show iterations needed ss-nmf less subsection iv-h. conclusion total computational complexity ss-nmf close nmf. usually notation used analyze complexity algorithm. clear precise arithmetic operations well complexity analysis notation iteration summarized table table lists parameters used complexity analysis. three kinds arithmetic operations iteration addition multiplication division. calculated ﬂoat-point precision. speciﬁcally tips essential results table order matrix multiplication important. order evaluate proposed method adopt performance metrics spectral angle distance root mean square error widely used used evaluate performance estimated endmembers angle distance estimated endmember corresponding ground truth. deﬁned follows denotes ground truth endmember corresponding estimated result. smaller corresponds better performance. root mean square error used evaluate performance estimated abundance maps. given number pixels hyperspectral image ground truth abundance denotes corresponding estimated result. smaller rmse corresponds better performance. following subsections abundance showed visible ways pseudo color gray scale shown figs. vertex component analysis classic geometrical method needs existence pure pixels endmember. different algorithms estimate endmembers abundances simultaneously estimate endmembers. abundances estimated solving constrained least square problem code algorithm obtained http//www.lx.it.pt/bioucas/code.htm. nonnegative matrix factorization typical statistical method. nonnegative constraint could seen special case sparsity constraint tends parts-based results. code method downloaded http//www.ee.columbia.edu/grindlay/code.html. sparsity-constrained nonnegative matrix factorization /-nmf short) state-of-the-art algorithm problem. proposed since code unavailable internet implement ourself. graph regularized nonnegative matrix factorization proposed good algorithm extract graph information latent data transfer dimension representation space matrix factorization process. code obtained http//www.cad.zju.edu.cn/home/dengcai/data/gnmf.html. local neighborhood weights regularized graph based method. main contribution method integrates spectral spatial information constructing weighted graph. since code implemented author lost realize ourself. endmember dissimilarity constrained different listed methods. method utilizes constraint endmember prior. constraint would highly possible edc-nmf result whose endmember spectra smooth different other. code implemented ourself. among eight algorithms geometrical method seven algorithms belong statistical ones. parameter methods. next subsection methods parameters -nmf /-nmf g-nmf w-nmf ss-nmf algorithms introduced. value closely related sparsity abundance matrix unfortunately unknown. adopt method estimate calculating sparsity value hyperspectral data based metric given intuitive thought value closely related level similarities pixel pairs neighborhood. thus value estimated steps. first randomly select local patches pixels respectively compute similarities central pixel neighboring ones. second averaging values. improve accuracy searching urban data available http//www.tec.army.mil/hypercube widely used hyperspectral data research recorded hyperspectral digital imagery collection experiment october whose location urban area copperas cove u.s. pixels image. pixel corresponding area observed wavelengths ranging spectral resolution bands removed bands remained data. four endmembers data asphalt includes road parking area roofs; grass covers areas green appearance fig. tree owning different spectral signature grass appears dark green mainly bottom areas fig. roof appears white center fig. figure rmses four abundance maps snrs urban data asphalt grass tree roof. symbol bottom-left subﬁgure indicates gaussian noise added manually. jasper ridge popular hyperspectral data introduced pixels pixel observed bands covering wavelengths ranging spectral resolution original hyperspectral data complex ground truth consider subimage pixels. ﬁrst pixel corresponds pixel original image. removing bands remain bands four endmembers latent data road soil water tree shown fig. ground truths data sets determined using method proposed three steps. first method proposed adopted determine endmember number. second endmember spectra manually chosen hyperspectral data. spectra enjoy high similarity reference hyperspectral spectra supplied usgs mineral spectral library etc. given endmembers corresponding abundances solving constraint convex optimization problem easily implemented using matlab optimization toolbox. ground truths listed data sets showed last column figs. figure sads four endmembers snrs jasper ridge data road soil water tree. symbol bottom-left subﬁgure indicates gaussian noise added manually. test robustness subsection evaluates inﬂuence noise performances methods. choose i.i.d. zero-mean white gaussian noise rather correlated noise reasons. first without prior correlated noise hard hyperspectral image. second zero-mean white gaussian noise widely used noise study hyperspectral data seven experiments carried respect seven levels gaussian noise experiment repeated times mean results well corresponding standard deviations provided here. evaluation organized parts quantitative results qualitative results. quantitative results analysis figs. tables show experiment results urban data set. figs. illustrate plots rmse versus seven levels gaussian noise respectively. results proposed ss-nmf algorithm consistently better comparable methods. especially ss-nmf achieves signiﬁcantly better performances figs. tables show detailed rmse performances corresponding standard deviations versus seven levels gaussian noise. values table average statistics plotted four subﬁgures fig. taking value marked bold northeast corner table example average four values ss-nmf algorithm noise level condition fig. similarly table figure rmses four abundance maps snrs jasper ridge data road soil water tree. symbol bottom-left subﬁgure indicates gaussian noise added manually. seven noise levels ss-nmf gets best comparable performances. comparing best algorithms proposed ss-nmf algorithm i.e. /-nmf edc-nmf rmse respectively ss-nmf achieves decrement shown table decrement rmse shown table moreover level noise varies snr=∞ snr= method stable one. prove structured sparse regularization well suited problem. comparing results obtained g-nmf w-nmf sparse regularized methods -nmf /-nmf achieve better comparable results. demonstrates sparse constraint important graph constraint estimating endmembers. mainly graph constraint helps abundance space similar pixels share similar abundances. hence leads smooth hazy abundances closely related less expressive inaccurate endmembers iterative updating rules introduced extensions -nmf /-nmf g-nmf outperform algorithm signiﬁcantly. reasons this based methods overcome requirement pure pixels essential method. parts-based property helps discover latent endmembers corresponding abundances. deviations versus seven levels noise summarized tables graphical plots shown figs. proposed ss-nmf consistently better comparable methods. particular figs. tables show ss-nmf outperform algorithms signiﬁcantly. comparing best algorithm proposed algorithm i.e. /-nmf -nmf rmse respectively ss-nmf achieves decrement table decrement rmse table qualitative results analysis order give intuitive comparison results illustrate abundance maps urban data figs. abundance maps japer ridge data figs. ways show abundance maps pseudo color gray scale taking last subﬁgure fig. example illustrate fractional abundances associated pixel plotting corresponding pixel using proportions blue green black given respectively instance pixel colored whereas colored equal proportions blue appear purple. figs. achieved way. gray scale straightforward. taking subﬁgures last column fig. example illustrate fractional abundances associated pixel plotting corresponding pixels four subﬁgures different gray scale appearances given respectively. instance pixel white ﬁrst subﬁgure black three subﬁgures. comparing results figs. ss-nmf achieves similar abundance maps according ground truths. addition colors results regional smooth sparsely mixed blue green black ink. demonstrate structured sparse constraint effective meaningful. abundance maps estimated sparse regularized methods -nmf /-nmf) contain noise ss-nmf g-nmf. reason graph structure constraint kind smooth constraint could urge learnt abundance maps smooth. subsection evaluate impact varying regularized parameters upon performances. nine experiments conducted respect nine different regularized parameters hyperspectral images degraded manually adding gaussian noise. regularized parameters vary following values ones help ss-nmf best results data set. obviously different respect data sets. reduce randomness results experiment repeated times mean results provided. fig. shows graphical performances urban data jasper ridge data. simplicity average performances provided hyperspectral image left side rmse right side. taking fig. example value left subﬁgure achieved performing average calculation sads estimated endmembers; value right subﬁgure obtained averaging rmses estimated abundance maps. regularized parameter leading graphical results plain. results fig. figure performance varying parameters urban data jasper ridge data. ﬁrst column shows sads second column displays rmses. symbol bottom-right subﬁgure indicates equal times values best parameter setting respectively. regularized parameters vary ﬁrst decrease. proposed ss-nmf gets good performance interval especially ss-nmf outperforms algorithms greatly suitable parameter setting seems -nmf /-nmf g-nmf robust varying regularized parameters ssnmf method. might reasons phenomenon. first results -nmf /-nmf g-nmf much worse method. varying regularized parameters cannot make algorithms much better much worse results different parameter conditions. second although ss-nmf method could achieve signiﬁcantly better results methods achieves results extreme parameter settings. many weighting methods. three common ones considering feature distance only spatial distance spatial feature distances simultaneously. second third methods well suited hyperspectral images. reason hyperspectral images enjoy geometrically intuitive property pixels located grids plane means spatial neighborhood inherent pixel. previous experiments employ third weighting method treats pixels among biggest sads local windows nearest neighbors. subsection performances second figure performances ss-nmf weighting methods urban data. spatial distance considered local graph structures constructed. spatial feature distances considered local structures constructed. column shows kind abundance maps respectively i.e. asphalt grass tree roof. figure performances ss-nmf weighting methods jasper ridge data. spatial distance considered local graph structures constructed. spatial feature distances considered local structures constructed. column shows kind abundance maps respectively i.e. road soil water tree. figs. show abundance maps obtained ss-nmf algorithm different weighting methods considering spatial distance considering spatial feature distances simultaneously. experiment conducted hyperspectral images degraded manually adding gaussian noise. figs. show results weighting method smooth hazy. reason considering spatial distance easily urges pixels dissimilar spectral signatures connected graph. accordingly minimizing objective function transfers graph constraints abundance space. besides part improper constraints would confuse optimization algorithm solving nonconvex problem resulting minima. objective function ss-nmf proved non-increasing updating rules subsection study convergence rate convergence time proposed algorithm. fig. illustrates convergence curves ss-nmf hyperspectral data sets urban data jasper ridge data set. subﬁgure x-axis shows number iterations y-axis displays energies objective functions. seen updating rules algorithms efﬁcient usually within iterations; ss-nmf converges faster converging within iterations. convergence time measured seconds ss-nmf summarized table vii. three rows. ﬁrst shows time constructing weighted graphs. since need part time constructing graphs zero method denoted table. second shows time iteration; total convergence time summarized last row. comparing results table ss-nmf method costs less time method iteration process. reason although costs less convergence time ss-nmf urban data fails continue superiority jasper ridge data. reason might size jasper ridge data much smaller urban data approximately smaller jasper ridge data urban data i.e. respectively. based observed properties hyperspectral data propose effective method problem imposing structured sparse constraint framework called ss-nmf short. ss-nmf effectively overcome three limitations problem. structured sparse constraint ss-nmf transfers manifold structures inherently embedded original data space learned abundance space also learns expressive accurate endmembers. experiments several hyperspectral data sets show method achieves better results state-of-the-art methods sense quantitative qualitative performances. besides method relatively robust different noise levels. figure abundance maps seven noise levels urban data. seven rows seven columns ﬁgure. displays results noise level. column column illustrates results algorithm. last column shows ground truths. subﬁgure proportions blue green black associated pixel represent fractional abundances asphalt tree grass roof corresponding pixel. ﬁgure best viewed color. keshava survey spectral unmixing algorithms lincoln laboratory journal vol. tits keersmaecker somers asner farifteh coppin hyperspectral shape-based unmixing improve intrainterclass variability forest agro-ecosystem monitoring isprs journal photogrammetry remote sensing vol. keshava mustard spectral unmixing ieee transactions signal processing vol. bioucas-dias hyperspectral unmixing overview geometrical statistical sparse regression-based approaches ieee journal logothetis sheinberg visual object recognition annual review neuroscience vol. zhang cheng learning spatially localized parts-based representation ieee international conference huck guillaume robust hyperspectral data unmixing spatial spectral regularized yuan manifold regularized sparse hyperspectral unmixing ieee transactions gersho gray vector quantization signal compression. norwell kluwer academic publishers jolliffe principal component analysis york springer belkin niyogi laplacian eigenmaps spectral techniques embedding clustering advances neural information chung spectral graph theory. american mathematical society dec. xiang zhang semi-supervised classiﬁcation local spline regression ieee transactions pattern analysis donoho compressed sensing ieee transactions information theory vol. dempster laird rubin maximum likelihood incomplete data algorithm journal royal cormen leiserson rivest stein introduction algorithms press heinz c.-i. chang fully constrained least squares linear spectral mixture analysis method material quantiﬁcation hyperspectral imagery ieee transactions geoscience remote sensing vol. hoyer non-negative sparse coding ieee workshop neural networks signal processing zhang zhang enhancing spectral unmixing local neighborhood weights ieee journal selected figure abundance maps snr= noise level urban data pseudo color gray scale. three rows second displays results yellow row. third shows absolute value difference estimated results ground truth. subﬁgure proportions blue green black associated pixel represent fractional abundances asphalt tree grass roof corresponding pixels. four rows seven columns shows abundance maps target. column column illustrates results algorithm. last column shows ground truths. ﬁgure best viewed color. figure abundance maps seven noise levels jasper ridge data. seven rows seven columns ﬁgure. displays results noise level. column column shows results method. last column illustrates ground truths. subﬁgure proportions blue green black associated pixel represent fractional abundances tree water soil road corresponding pixel. ﬁgure best viewed color. figure abundance maps snr= noise level japer ridge data pseudo color gray scale. rows second displays absolute value difference estimated results ground truth. subﬁgure proportions blue green black associated pixel represent fractional abundances tree water soil road corresponding pixel. four rows seven columns shows abundance maps target. column column illustrates results algorithm. last column shows ground truths. road ﬁrst soil second difﬁcult targets similarity spectral curves them.", "year": 2014}