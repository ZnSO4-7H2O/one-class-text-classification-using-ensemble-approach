{"title": "QMDP-Net: Deep Learning for Planning under Partial Observability", "tag": ["cs.AI", "cs.LG", "cs.NE", "stat.ML"], "abstract": "This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and \"transfer\" to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.", "text": "paper introduces qmdp-net neural network architecture planning partial observability. qmdp-net combines strengths model-free learning model-based planning. recurrent policy network represents policy parameterized tasks connecting model planning algorithm solves model thus embedding solution structure planning network learning architecture. qmdp-net fully differentiable allows end-to-end training. train qmdpnet different tasks generalize ones parameterized task transfer similar tasks beyond set. preliminary experiments qmdp-net showed strong performance several robotic tasks simulation. interestingly qmdp-net encodes qmdp algorithm sometimes outperforms qmdp algorithm experiments result end-to-end learning. decision-making uncertainty fundamental importance computationally hard especially partial observability partially observable world agent cannot determine state exactly based current observation; plan optimal actions must integrate information past history actions observations. fig. example. model-based approach formulate problem partially observable markov decision process solving pomdps exactly computationally intractable worst case approximate pomdp algorithms made dramatic progress solving large-scale pomdps however manually constructing pomdp models learning data remains difﬁcult. model-free approach directly search optimal solution within policy class. restrict policy class difﬁculty data computational efﬁciency. choose parameterized policy class. effectiveness policy search constrained priori choice. deep neural networks brought unprecedented success many domains provide distinct approach decision-making uncertainty. deep q-network consists convolutional neural network together fully connected layer successfully tackled many atari games complex visual input replacing postconvolutional fully connected layer recurrent lstm layer allows deal partial observaiblity however compared planning approach fails exploit underlying sequential nature decision-making. introduce qmdp-net neural network architecture planning partial observability. qmdp-net combines strengths model-free learning model-based planning. qmdp-net recurrent policy network represents policy connecting pomdp model algorithm solves model thus embedding solution structure planning network fig. robot learning navigate partially observable grid worlds. robot map. belief initial state know exact initial state. local observations ambiguous insufﬁcient determine exact state. policy trained expert demonstrations randomly generated environments generalizes environment. also transfers much larger real-life environment represented lidar learning architecture. speciﬁcally network uses qmdp simple fast approximate pomdp algorithm though sophisticated pomdp algorithms could used well. qmdp-net consists main network modules represents bayesian ﬁlter integrates history agent’s actions observations belief i.e. probabilistic estimate agent’s state. represents qmdp algorithm chooses action given current belief. modules differentiable allowing entire network trained end-to-end. train qmdp-net expert demonstrations randomly generated environments. trained policy generalizes environments also transfers complex environments preliminary experiments show qmdp-net outperformed state-of-the-art network architectures several robotic tasks simulation. successfully solved difﬁcult pomdps require reasoning many time steps well-known hallway domain interestingly qmdp-net encodes qmdp algorithm sometimes outperformed qmdp algorithm experiments result end-to-end learning. pomdp formally deﬁned tuple state action observation space respectively. state-transition function deﬁnes probability agent state taking action state observation function deﬁnes probability receiving observation taking action state reward function deﬁnes immediate reward taking action state partially observable world agent know exact state. maintains belief probability distribution agent starts initial belief updates belief time step bayesian ﬁlter normalizing constant. belief recursively integrates information entire past history decision making. pomdp planning seeks policy maximizes value i.e. expected total discounted reward learn policies decision making partially observable domains approach learn models solve models planning. alternative learn policies directly model learning usually end-to-end. policy learning end-to-end exploit model information effective generalization. proposed approach combines model-based model-free learning embedding model planning algorithm recurrent neural network represents policy training network end-to-end. rnns used earlier learning partially observable domains particular hausknecht stone extended convolutional neural network replacing post-convolutional fully connected layer recurrent lstm layer similarly mirowski considered learning navigate partially observable mazes. learned policy generalizes different goals ﬁxed environment. instead using generic lstm approach embeds algorithmic structure speciﬁc sequential decision making network architecture aims learn policy generalizes environments. idea embedding speciﬁc computation structures neural network architecture gaining attention recently. tamar implemented value iteration neural network called value iteration network solve markov decision processes fully observable domains agent knows exact state require ﬁltering okada addressed related problem path integral optimal control allows continuous states actions neither addresses issue partial observability drastically increases computational complexity decision making haarnoja jonschkowski brock developed end-to-end trainable bayesian ﬁlters probabilistic state estimation. silver introduced predictron value estimation markov reward processes deal decision making planning. shankar gupta addressed planning partial observability. former focuses learning model rather policy. learned model trained ﬁxed environment generalize ones. latter proposes network learning approach robot navigation unknown environment focus mapping. network architecture contains hierarchical extension planning thus deal partial observability planning. qmdp-net extends prior work network architectures planning bayesian ﬁltering. imposes pomdp model computation structure priors entire network architecture planning partial observability. want learn policy enables agent effectively diverse partially observable stochastic environments. consider example robot navigation domain fig. environments correspond different buildings. robot agent observe location directly estimates based noisy readings laser range ﬁnder. access building maps models dynamics sensors. buildings differ signiﬁcantly layouts underlying reasoning required effective navigation similar buildings. training robot buildings want place robot building navigate effectively speciﬁed goal. formally agent learns policy parameterized tasks partially observable stochastic environments parameter values. parameter value captures wide variety task characteristics vary within including environments goals agents. robot navigation example encodes environment goal belief robot’s initial state. assume tasks share state space action space observation space. agent prior models dynamics sensors task objectives. training tasks subset values agent learns policy solves given issue general representation policy without knowing speciﬁcs parametrization. introduce qmdp-net recurrent policy network. qmdp-net represents policy connecting parameterized pomdp model approximate pomdp algorithm embedding single differentiable neural network. embedding model allows policy generalize effectively. embedding algorithm allows train entire network end-to-end learn model compensates limitations approximate algorithm. embedded pomdp model shared state space action space observation space designed manually tasks state-transition observation reward functions learned data. appear perfect answer learning problem would fig. qmdp-net architecture. policy maps history actions observations action. qmdp-net imposes structure priors sequential decision making partial observability. embeds bayesian ﬁlter qmdp algorithm network. hidden state encodes belief pomdp planning. qmdp-net unfolded time. represent true underlying models dynamics observation reward task true embedded pomdp algorithm exact true general. agent learn alternative model mitigate approximate algorithm’s limitations obtain overall better policy. sense qmdp-net embeds pomdp model network architecture aims learn good policy rather correct model. qmdp-net consists modules encodes bayesian ﬁlter performs state estimation integrating past history agent actions observations belief. encodes qmdp simple fast approximate pomdp planner qmdp chooses agent’s actions solving corresponding fully observable markov decision process performing one-step look-ahead search values weighted belief. evaluate proposed network architecture imitation learning setting. train expert trajectories randomly chosen task parameter values test parameter values. expert trajectory consist sequence demonstrated actions observations agent access ground-truth states beliefs along trajectory training. deﬁne loss cross entropy predicted demonstrated action sequences rmsprop training. appendix details. implementation tensorﬂow available online http//github.com/adacompnus/qmdp-net. assume tasks parameterized share underlying state space action space observation space want learn qmdp-net policy conditioned parameters qmdp-net recurrent policy network. inputs qmdp-net action observation time step well task parameter output action time step qmdp-net encodes parameterized pomdp model qmdp algorithm selects actions solving model approximately. choose manually based prior knowledge speciﬁcally prior knowledge general model states actions observations abstractions real-world counterparts task. robot navigation example robot moves continuous space choose grid ﬁnite size. order reduce representational computational complexity. transition function observation function reward function conditioned learned data end-to-end training. work assume tasks simplify network architecture. words depend end-to-end training feasible qmdp-net encodes model associated algorithm single fully differentiable neural network. main idea embedding algorithm neural network represent linear operations matrix multiplication summation convolutional layers represent maximum operations max-pooling layers. provide details qmdp-net’s architecture consists modules ﬁlter planner. fig. qmdp-net consists modules. bayesian ﬁlter module incorporates current action observation belief. qmdp planner module selects action according current belief filter module. ﬁlter module implements bayesian ﬁlter. maps belief action observation next belief belief updated steps. ﬁrst accounts actions second observations observation received taking action normalization factor. implement bayesian ﬁlter transforming layers neural network. ease discussion consider grid navigation task agent know state observes neighboring cells. access task parameter encodes obstacles goal belief initial states. given task choose state space. belief tensor. implemented convolutional layer convolutional ﬁlters. denote convolutional layer kernel weights encode transition function output convolutional layer encodes updated belief taking actions need select belief corresponding last action taken agent directly index general cannot simple indexing. instead soft indexing. first encode actions actions learned function maps indexing vector along appropriate dimension i.e. incorporates observations observation model n×n×|o| tensor represents probability receiving observation state grid navigation task observations depend obstacle locations. condition task parameter function neural network mapping paper cnn. encodes observation probabilities observations need observation probabilities last observation general cannot index directly. instead soft indexing again. encode observations observations function mapping indexing vector distribution weight finally obtain updated belief multiplying element-wise normalizing states. setting initial belief task encoded initialize belief qmdp-net additional encoding function actions selected weighting values belief. implement value iteration using convolutional pooling layers grid navigation task n×n×|a| tensor. expressed pooling layer input output. convolution convolutional ﬁlters followed addition operation reward tensor. denote convolutional layer encode transition function similarly ﬁlter. rewards navigation task depend goal obstacles. condition rewards task parameter maps paper cnn. implement iterations bellman updates stacking layers representing times tied weights. iterations approximate values state-action pair. weight values belief obtain action values finally choose output action low-level policy function mapping action output at+. qmdp-net naturally extends higher dimensional discrete state spaces n-dimensional convolutions used restricted discrete space handle continuous tasks simultaneously learning discrete planning states actions observations main objective experiments understand beneﬁts structure priors learning neural-network policies. create several alternative network architectures gradually relaxing structure priors evaluate architectures simulated robot navigation manipulation tasks. tasks simpler than example atari games terms visual perception fact challenging sophisticated long-term reasoning required handle partial observability distant future rewards. since exact state robot unknown successful policy must reason many steps gather information improve state estimation partial noisy observations. also must reason trade-off cost information gathering reward distance future. compare qmdp-net number related alternative architectures. qmdp-net variants. untied qmdp-net relaxes constraints planning module untying weights representing state-transition function different layers. lstm qmdp-net replaces ﬁlter module generic lstm module. architectures embed pomdp structure priors all. cnn+lstm state-of-the-art deep connected lstm. similar drqn architecture proposed reinforcement learning partially observability basic recurrent neural network single fully-connected hidden layer. contains structure speciﬁc planning partial observability. experimental domain contains parameterized tasks parameters encode environment goal belief robot’s initial state. train policy generate random environments goals initial beliefs. construct ground-truth pomdp models generated data apply qmdp algorithm. qmdp algorithm successfully reaches goal retain resulting sequence action observations expert trajectory together corresponding environment goal initial belief. important note ground-truth pomdps used generating expert trajectories learning qmdp-net. fair comparison train networks using expert trajectories domain. perform basic search training parameters number layers number hidden units network architecture. brieﬂy describe experimental domains. appendix implementation details. grid-world navigation. robot navigates unknown building given ﬂoor goal. robot uncertain location. equipped lidar detects obstacles direct neighborhood. world uncertain robot fail execute desired actions possibly wheel slippage lidar produce false readings. implemented simpliﬁed version task discrete grid world task parameter represented image three channels. ﬁrst channel encodes obstacles environment second channel encodes goal last channel encodes belief robot’s initial state. robot’s state represents position grid. actions moving four canonical directions staying put. lidar observations compressed four binary values corresponding obstacles four neighboring cells. consider deterministic stochastic variant domain. stochastic variant adds action observation uncertainties. robot fails execute speciﬁed move action stays place probability observations faulty probability independently direction. trained policy using expert trajectories random environments trajectories environment. tested separate random environments. maze navigation. differential-drive robot navigates maze help know pose domain similar grid-world navigation signiﬁcant challenging. robot’s state contains position orientation. robot cannot move freely kinematic constraints. four actions move forward turn left turn right stay put. observations relative robot’s current orientation increased ambiguity makes difﬁcult localize robot especially initial state highly uncertain. finally successful trajectories mazes typically much longer randomly-generated grid worlds. trained expert trajectories randomly generated mazes tested ones. object grasping. robot gripper picks novel objects table using two-ﬁnger hand noisy touch sensors ﬁnger tips. gripper uses ﬁngers perform compliant motions maintaining contact object grasp object. knows shape object grasped maybe object database. however know pose relative object relies touch sensors localize itself. implemented simpliﬁed variant task modeled pomdp task parameter image three channels encoding object shape grasp point belief gripper’s initial pose. gripper four actions moving canonical direction unless touches object environment boundary. ﬁnger binary touch sensors resulting distinct observations. trained expert demonstration different objects randomly sampled poses object. tested previously unseen objects random poses. choosing qmdp-net components task given task need choose appropriate neural network representation speciﬁcally need choose representation functions provides opportunity incorporate domain knowledge principled way. example local spatially invariant connectivity structure choose convolutions small kernels represent experiments grid navigation n×n× maze navigation robot possible orientations. tasks except object grasping task represent components kernels depending task. enforce proper probability distributions using softmax sigmoid activations convolutional kernels respectively. finally small fully connected component one-hot encoding function single softmax layer identity function. adjust amount planning qmdp-net setting large allows propagating information distant states without affecting number parameters learn. however results deeper networks computationally expensive evaluate difﬁcult train. used depending problem size. able transfer policies larger environments increasing executing policy. experiments representation task parameter isomorphic chosen state space architecture restricted setting rely represent convolutions small kernels. experiments general class problems interesting direction future work. main results reported table additional results reported appendix domain report task success rate average number time steps task completion. comparing completion time meaningful success rates similar. qmdp-net successfully learns policies generalize environments. evaluated environments qmdp-net higher success rate faster completion time alternatives nearly domains. understand better performance difference speciﬁcally compared architectures ﬁxed environment navigation. initial state goal vary across task instances environment remains same. results last table qmdp-net alternatives comparable performance. even performs well. why? ﬁxed environment network learn features optimal policy directly e.g. going straight towards goal. contrast qmdp-net learns model planning i.e. generating near-optimal policy given arbitrary environment. pomdp structure priors improve performance learning complex policies. moving across table left right gradually relax pomdp structure priors network architecture. structure priors weaken overall performance. however strong priors sometimes over-constrain network result degraded performance. example found tying weights ﬁlter planner lead worse policies. represent underlying transition dynamics using different weights allows choose approximation thus greater ﬂexibility. shed light issue visualize learned pomdp model appendix qmdp-net learns incorrect useful models. planning partial observability intractable general must rely approximation algorithms. qmdp-net encodes pomdp model qmdp approximate pomdp algorithm solves model. train network end-to-end. provides opportunity learn incorrect useful model compensates limitation approximation algorithm similar reward shaping reinforcement learning indeed results show qmdp-net achieves higher success rate qmdp nearly tasks. particular qmdp-net performs well well-known hallway domain designed expose weakness qmdp resulting myopic planning horizon. planning algorithm qmdp-net qmdp qmdp-net learns effective model expert demonstrations. true even though qmdp generates expert data training. note expert data contain successful qmdp demonstrations. successful unsuccessful qmdp demonstrations used training qmdp-net perform better qmdp would expect. qmdp-net policies learned small environments transfer directly larger environments. learning policy large environments scratch often difﬁcult. scalable approach table performance comparison qmdp-net alternative architectures recurrent policy networks. success rate percentage. time average number time steps task completion. denote deterministic stochastic variants domain environment size n×n. would learn policy small environments transfer large environments repeating reasoning process. transfer learned qmdp-net policy simply expand planning module adding recurrent layers. speciﬁcally trained policy randomly generated grid worlds applied learned policy several real-life environments including intel freiburg using lidar maps robotics data repository results environments table additional results different settings buildings available appendix qmdp-net deep recurrent policy network embeds pomdp structure priors planning partial observability. generic neural networks learn direct mapping inputs outputs qmdp-net learns model solve planning task. network fully differentiable allows end-to-end training. experiments several simulated robotic tasks show learned qmdp-net policies successfully generalize environments transfer larger environments well. pomdp structure priors end-to-end training substantially improve performance learned policies. interestingly qmdp-net encodes qmdp algorithm planning learned qmdp-net policies sometimes outperform qmdp. many exciting directions future exploration. first major limitation current approach state space representation. value iteration algorithm used qmdp iterates entire state space well known suffer curse dimensionality. alleviate difﬁculty qmdp-net end-to-end training learn much smaller abstract state space representation planning. also incorporate hierarchical planning second qmdp makes strong approximations order reduce computational complexity. want explore possibility embedding sophisticated pomdp algorithms network architecture. algorithms provide stronger planning performance algorithmic sophistication increases difﬁculty learning. finally restricted work imitation learning. would exciting extend reinforcement learning. based earlier work indeed promising. acknowledgments thank leslie kaelbling tomás lozano-pérez insightful discussions helped improve understanding problem. work supported part singapore ministry education acrf grant moe-t-- national university singapore acrf grant r---. references abadi agarwal barham brevdo chen citro corrado davis dean devin tensorflow large-scale machine learning heterogeneous systems http//tensorflow.org/. bakker zhumatiy gruener schmidhuber. robot reinforcement-learns identify memorize important previous observations. international conference intelligent robots systems pages merriënboer gulcehre bahdanau bougares schwenk bengio. learning phrase representations using encoder-decoder statistical machine translation. arxiv preprint arxiv. jonschkowski brock. end-to-end learnable histogram ﬁlters. workshop deep learning action interaction nips http//www.robotics.tu-berlin.de/fileadmin/ fg/publikationen_pdf/jonschkowski--nips-ws.pdf. silver huang maddison guez sifre driessche schrittwieser antonoglou panneershelvam lanctot mastering game deep neural networks tree search. nature silver hasselt hessel schaul guez harley dulac-arnold reichert rabinowitz barreto predictron end-to-end learning planning. arxiv preprint https//arxiv.org/abs/.. xingjian chen wang d.-y. yeung w.-k. wong w.-c. woo. convolutional lstm network machine learning approach precipitation nowcasting. advances neural information processing systems pages provide results additional environments lidar navigation task. lidar maps obtained section details. intel corresponds intel research lab. freiburg corresponds freiburg building belgioioso corresponds belgioioso castle. corresponds western wing csail building. note size grid size environment. qmdp-net policy trained grid navigation domain randomly generated environments using execute learned qmdp-net policy different settings i.e. convolutional layers planner share kernel weights. report task success rate average number time steps task completion. conventional setting value iteration executed fully known increasing improves value function approximation improves policy return increased computation. qmdp-net increasing effects overall planning quality. estimation accuracy latent values increases reward information propagate distant states. hand learned latent model necessarily true underlying model overﬁtted setting training. therefore high degrade overall performance. found ktest ktrain signiﬁcantly improved success rates test cases. increasing ktest ktrain beneﬁcial intel belgioioso environments slightly decreased success rates freiburg environments. compare qmdp-net untied variant untied qmdp-net. cannot expand layers untied qmdp-net execution. consequence performance poor. note alternative architectures considered speciﬁc input size thus applicable. demonstrate incorrect model result better policies solved approximate qmdp algorithm. compute qmdp policies pomdp modiﬁed reward values evaluate policies using original rewards. deterministic maze navigation task qmdp poorly. attempt shape rewards manually. motivation break symmetry model implicitly encourage information gathering compensate one-step look-ahead approximation qmdp. modiﬁed increase cost stay actions times original value. modiﬁed increase cost stay action times original value cost turn right action times original value. another states opposite around. expectation next states lower value current thus policy chooses stay action robot gather information stuck place. results demonstrate planning incorrect model improve performance correct model. value function plot value function predicted qmdp-net stochastic grid navigation task. used iterations qmdp-net. would expect states close goal high values. plot execution learned qmdp-net policy internal belief propagation stochastic grid navigation task. ﬁrst fig. shows environment including goal unobserved pose robot second shows ground-truth beliefs reference. access ground-truth beliefs training except initial belief. third shows beliefs predicted qmdp-net. last shows difference ground-truth predicted beliefs. ﬁgure demonstrates qmdp-net able learn reasonable ﬁlter state estimation noisy environment. depicted example initial belief uniform approximately half state space highly uncertain initial belief observation noise robot stays place steps steps state estimation still highly uncertain mostly spread right goal. therefore moving left reasonable choice additional stay action belief distribution small enough robot starts moving towards goal plot learned ground-truth state-transition functions. columns table correspond actions. ﬁrst shows ground-truth transition function. second shows learned state-transition function ﬁlter. third shows learned state-transition function planner. represent underlying transition dynamics learned transition probabilities different ﬁlter planner. different weights allows module choose approximation thus provides greater ﬂexibility. actions model learned abstractions agent’s actions indeed planner learned transition probabilities action match transition probabilities learned rewards directly correspond rewards underlying task reasonable obstacles assigned negative rewards goal assigned positive reward. note learned reward values correspond reward taking action therefore interpreted together corresponding transition probabilities grid-world navigation implement grid navigation task randomly generated discrete grids cell probability obstacle. robot actions move four canonical directions stay put. observations four binary values corresponding obstacles four neighboring cells. consider deterministic variant stochastic variant stochastic variant robot fails execute action probability case stays place. observations faulty probability independently direction. since receive observations directions probability receiving correct observation vector task parameter n×n× image encodes information environment. ﬁrst channel encodes obstacles obstacles free space. second channel encodes goal goal otherwise. third channel encodes initial belief robot states pixel value corresponds probability robot corresponding state. construct ground-truth pomdp model obtain expert trajectories training. important note learning agent access ground-truth pomdp models. ground-truth model robot receives reward step reaching goal bumping obstacle. qmdp solve pomdp model execute qmdp policy obtain expert trajectories. random grids training. initial goal states sampled free space uniformly. exclude samples feasible path. initial belief uniform random fraction free space includes underlying initial state. speciﬁcally number non-zero values initial-belief sampled number free cells grid. grid generate expert trajectories different initial state initial belief goal. note access true beliefs ﬁrst step underlying states along trajectory. test environments generated separately equal conditions. declare failure steps without reaching goal. note expert policy sub-optimal fail reach goal. exclude samples training include test set. choose structure model qmdp-net match structure underlying convolutions. task. transition function ﬁlter planner represent transition function weights. apply softmax function kernel matrix values one. reward function convolutional layers. ﬁrst kernel ﬁlters relu activation. second kernel ﬁlters linear activation. observation model similar two-layer cnn. ﬁrst convolution kernel ﬁlters linear activation. second kernel ﬁlters linear activation. action mapping one-hot encoding function. observation mapping fully connected network hidden layer units tanh activation. output units softmax activation. low-level policy function single softmax layer. state space mapping function identity function. finally choose number iterations planner module grids size respectively. convolutions imply spatially invariant local. underlying task locality assumption holds spatial invariance transitions depend arrangement obstacles. nevertheless additional ﬂexibility model allows qmdp-net learn high-quality policies e.g. shaping rewards observation function. maze navigation task differential drive robot navigate given goal. generate random mazes grids using kruskal’s algorithm. state space dimensions third dimension represents possible orientations robot. goal conﬁguration invariant orientation. robot actions move forward turn left turn right stay put. initial belief chosen similar manner grid navigation case space. observations identical grid navigation relative robot’s orientation signiﬁcantly increases difﬁculty state estimation. stochastic variant motion observation noise identical grid navigation. training test data prepared identically well. mazes size respectively. model qmdp-net -dimensional state space size n×n× action space actions. components network chosen identically previous case except components operate tensors size n×n×. would possible convolutions treat third dimension channels image instead conventional convolutions. output last convolutional layer size n×n×nc grid navigation task size n×n×nc maze navigation task. necessary tensors transformed dimensional form n×n××nc max-pool softmax activation computed along last dimension. consider implementation grasping task based pomdp model proposed hsiao hsiao focused difﬁculty planning high uncertainty solved manually designed pomdps single objects. phrase problem learning task access model know objects advance. setting robot receives image target object feasible grasp point know pose relative object. learn policy object generalizes similar unseen objects. object gripper represented discrete grid. workspace grid gripper shape grid. gripper moves four canonical directions unless reaches boundaries workspace touching object. case stays place. gripper fails move probability gripper ﬁngers touch sensors ﬁnger. touch sensors indicate contact object reaching limits workspace. sensors produce incorrect reading probability independently sensor. trial object placed bottom workspace random location. initial gripper pose unknown; belief possible states uniform random fraction upper half workspace. local observations readings touch sensors. task parameter image three channels. ﬁrst channel encodes environment object; second channel encodes position target grasping point; third channel encodes initial belief gripper position. artiﬁcial objects different sizes grid cells. object least cell gripper grasp. training objects. generate expert trajectories object random conﬁguration. test learned policies objects random conﬁgurations each. expert trajectories obtained solving ground-truth pomdp model qmdp algorithm. ground-truth pomdp robot receives reward reaching grasp point every state. qmdp-net choose model note underlying task possible observations. network components chosen similarly grid navigation task ﬁrst convolution kernel increased account distant observations. number iterations hallway navigation problem proposed littman used benchmark problem pomdp planning speciﬁcally designed expose weakness qmdp algorithm resulting myopic planning horizon. qmdp-net embeds qmdp algorithm end-to-end training qmdp-net able learn model signiﬁcantly effective given qmdp algorithm. hallway particular instance maze problem involves complex dynamics high noise. details refer original problem deﬁnition train qmdp-net random grids generated similarly grid navigation case using transitions match hallway pomdp model. execute learned policy particularly difﬁcult instance problem embeds hallway layout grid. initial state uniform full state space. trial robot starts random underlying state. trial deemed unsuccessful steps. obtain real-world building layouts using laser data robotics data repository speciﬁcally slam maps preprocessed gray-scale images available online downscale images classify pixel free obstacle simple thresholding. resulting maps shown fig. execute policies simulation grid deﬁned preprocessed map. simulation employs dynamics grid navigation domain. initial state initial belief chosen identically grid navigation case. qmdp-net policy trained grid navigation task randomly generated environments. training qmdp-net. execute learned policy lidar maps. account larger grid size increase number iterations executing policy. compare qmdp-net variants remove pomdp priors embedded network also compare generic network architectures embed structural priors decision making also considered additional architectures comparison including networks convlstm cells. convlstm variant lstm fully connected layers replaced convolutions. architectures performed worse cnn+lstm task. untied qmdp-net. obtain untied qmdp-net untying kernel weights convolutional layers implement value iteration planner module qmdp-net. also remove softmax activation kernel weights. equivalent allowing different transition model iteration value iteration allowing transition probabilities one. principle untied qmdp-net represent policy qmdp-net additional ﬂexibility. however untied qmdp-net parameters learn increases. training difﬁculty increases parameters especially complex domains training small amount data. lstm qmdp-net. lstm qmdp-net replace ﬁlter module qmdp-net generic lstm network keep value iteration implementation planner. output lstm component belief estimate input planner module qmdp-net. ﬁrst process task parameter input image encoding environment goal cnn. separately process action observation input vectors two-layer fully connected component. processed inputs concatenated single vector input lstm layer. size lstm hidden state output chosen match number states grid e.g. grid. initialize hidden state lstm using appropriate channel input encodes initial belief. cnn+lstm. cnn+lstm state-of-the-art deep convolutional network lstm cells. similar structure drqn used learning play partially observable atari games reinforcement learning setting. note train networks imitation learning setting using expert trajectories using reinforcement learning comparison qmdp-net fair. cnn+lstm network structure encode decision making policy compared vanilla also tailored input representation. process image input component vector input fully connected network component. output fully connected component combined single vector lstm layer. rnn. considered architecture vanilla recurrent neural network hidden units tanh activation. step inputs transformed single concatenated vector. outputs obtained fully connected layer softmax activation. performed hyperparameter search number layers hidden units adjusted learning rate batch size alternative networks. particular trials deterministic grid navigation task. architecture chose best parametrization found. used parametrization tasks. train networks qmdp-net alternatives imitation learning setting. loss deﬁned cross-entropy predicted demonstrated actions along expert trajectories. receive supervision underlying ground-truth pomdp models. train networks backpropagation time mini-batches networks implemented tensorﬂow rmsprop optimizer decay rate momentum setting. learning rate qmdp-net alternative networks. limit number backpropagation steps qmdp-net untied variant; alternatives gave slightly better results. used combination early stopping patience exponential learning rate decay particular started decrease learning rate prediction error decrease consecutive epochs validation training data. performed iterations learning rate decay. perform multiple rounds training method described above. partially observable domains predictions increasingly difﬁcult along trajectory require multiple steps ﬁltering i.e. integrating information long sequence observations. therefore ﬁrst round training limit number steps along expert trajectories training qmdp-net alternatives. convergence perform second round training full length trajectories. number steps along expert trajectories training round used training rounds training qmdp-net untied variant. training alternative networks used gave better results. trained policies grid navigation task grid ﬁxed initial state goal vary. variant found setting degrades ﬁnal performance alternative networks. used single training round task.", "year": 2017}