{"title": "Feature Selection with Annealing for Computer Vision and Big Data  Learning", "tag": ["stat.ML", "cs.CV", "cs.LG", "math.ST", "stat.TH"], "abstract": "Many computer vision and medical imaging problems are faced with learning from large-scale datasets, with millions of observations and features. In this paper we propose a novel efficient learning scheme that tightens a sparsity constraint by gradually removing variables based on a criterion and a schedule. The attractive fact that the problem size keeps dropping throughout the iterations makes it particularly suitable for big data learning. Our approach applies generically to the optimization of any differentiable loss function, and finds applications in regression, classification and ranking. The resultant algorithms build variable screening into estimation and are extremely simple to implement. We provide theoretical guarantees of convergence and selection consistency. In addition, one dimensional piecewise linear response functions are used to account for nonlinearity and a second order prior is imposed on these functions to avoid overfitting. Experiments on real and synthetic data show that the proposed method compares very well with other state of the art methods in regression, classification and ranking while being computationally very efficient and scalable.", "text": "implementation ease. algorithms simple implement avoid over-ﬁtting ad-hoc designs. regularization parameters deﬁned ease tuning mind. real-world applications helpful algorithm customizable cost based computing resources. recently penalized methods received attention high-dimensional feature selection. solve class optimization problems sparsityinducing penalties scad statistical guarantee junk dimensions removed high probability optimization based algorithms scalable enough tuning penalty parameter could time consuming large datasets. methods cannot adaptively capture nonlinearity. boosting also used feature selection restricting weak learner constructed single variable. boosting algorithms progressive manner iteration weak learner added current model sake decreasing value certain loss function feature selected next boosting iteration strongly depends subset selected features current coefﬁcients. design examines essentially features boosting iteration since hundreds even thousands iterations usually required boosting fast enough data computation. also exist numerous ad-hoc procedures designed feature selection speciﬁc problems. although many ideas class methods motivating lack universal learning schemes simple implement adapt different situations. paper combine regularization technique sequential algorithm design bring forward novel feature selection scheme could suitable data learning theoretical guarantees. abstract—many computer vision medical imaging problems faced learning large-scale datasets millions observations features. paper propose novel efﬁcient learning scheme tightens sparsity constraint gradually removing variables based criterion schedule. attractive fact problem size keeps dropping throughout iterations makes particularly suitable data learning. approach applies generically optimization differentiable loss function ﬁnds applications regression classiﬁcation ranking. resultant algorithms build variable screening estimation extremely simple implement. provide theoretical guarantees convergence selection consistency. addition dimensional piecewise linear response functions used account nonlinearity second order prior imposed functions avoid overﬁtting. experiments real synthetic data show proposed method compares well state methods regression classiﬁcation ranking computationally efﬁcient scalable. index terms—feature selection supervised learning regression classiﬁcation ranking. introduction feature selection popular crucial technique speed computation obtain parsimonious models generalize well. many computer vision medical imaging problems require learning classiﬁers large amounts data millions features even observations. data pose great challenges feature selection. statistical guarantee. consideration inevitable noise contamination numerous nuisance dimensions datasets trustworthy learning approach must recover genuine signals high probability. universality. rather restricting speciﬁc problem universal learning scheme adapt different types problems including instance regression classiﬁcation ranking others. barbu gramajo department statistics florida state university tallahassee florida email abarbustat.fsu.edu yshestat.fsu.edu ggramajostat.fsu.edu. work supported part darpa msee grant darpa simplex n--c- grant dms-. fig. classiﬁcation comparison logitboost equally correlated data features observations variables selected. left training time. middle percent variables correctly detected right area curve. rather growing model adding variable time consider shrinkage estimation problem whole predictor space together annealing lessen greediness. attractive feature number variables removed model parameters updated time makes problem size keep dropping iteration process. worth mentioning learning scheme ad-hoc principle keep kill exact form theoretical guarantee optimality consistency. proposed feature selection approach handle large datasets without online idea successfully applied previously different areas signal processing reduced rank modeling network screening total amount data algorithm needs access training times size training orders magnitude faster penalization boosting algorithms. algorithm easily distributed grid processors even larger scale problems. experiments extensive synthetic real data provide empirical evidence proposed learning performance comparable better upto-date penalization boosting methods runs much efﬁciently large datasets. related works brieﬂy discuss related works grouped feature selection methods penalized loss algorithms boosting. feature selection methods. shares similarity recursive feature elimination procedure alternates training classiﬁer current feature removing percentage features based magnitude variable coefﬁcients. however approach following signiﬁcant differences viewed backward elimination method variable elimination built optimization process. although many methods variable removal model update algorithm design combining optimization update progressive killing unique best knowledge. principles enjoy theoretical guarantees convergence variable selection parameter consistency. exist feature selection methods mrmr parallel select features independent model built features. contrast method simultaneously selects features builds model selected features uniﬁed approach aimed minimizing loss function sparsity constraints. penalized loss algorithms sparsity inducing penalty scad optimize nondifferentiable objective loss function various ways. proposed method different penalized methods variable selection obtained imposing sparsity prior variables successive optimization reduction constrained loss function. sparsity parameter intuitive penalty parameters provides direct cardinality control obtained model. introduce undesired bias coefﬁcients. contrast bias introduced penalty certain sparsity level might large lead poor classiﬁcation performance common practice using penalty penalized model variable selection reﬁt unpenalized model selected variables afterwards. two-step procedure necessary approach proposed paper. another related class methods based stochastic gradient descent however still sparsity inducing penalty obtain feature selection makes difﬁcult optimize slow practice. present section evaluation implementation lags behind method computation time feature selection accuracy prediction power. boosting. would also interesting compare boosting. boosting algorithms adaboost logitboost floatboost robust logitboost cite optimize loss function greedy manner iterations iteration adding weak learner decreases loss most. modern versions lp-adaboost arcgv adaboost* lp-boost optimal adaboost coordinate ascent boosting approximate coordinate ascent boosting optimizing notion margin iteration. boosting regarded coordinate descent algorithm optimizing loss function margin based. boosting algorithms explicitly enforce sparsity used feature selection using weak learners depend single variable feature selected next boosting iteration depends features already selected current coefﬁcients. dependence structure makes difﬁcult obtain general theoretical variable selection guarantees boosting. approach introduced paper different boosting starts variables gradually removes variables according elimination schedule. indeed top-down design opposite boosting seems less greedy feature selection based experiments section perhaps importantly computation rank features step keeps dropping problem size total cost customized based computation resources. feature selection annealing algorithm training examples loss function deﬁned based examples. formulate feature selection problem constrained optimization number relevant features given parameter loss function differentiable respect constrained form facilitates parameter tuning comparison penalty parameters regularization parameter much intuitive easier specify. experiments section also demonstrate robustness choice long within large range. course nonconvex constraint optimization problem challenging solve especially large basic algorithm description ideas algorithm design using annealing plan lessen greediness reducing dimensionality gradually removing irrelevant variables facilitate computation. prototype algorithm summarized algorithm actually pretty simple. starts initial value parameter vector usually alternates basic steps step parameter updates towards minimizing loss gradient descent annealing schedule support coefﬁcient vector gradually tightened till reach step conducts adaptive screening resulting nonlinear operator increases difﬁculty theoretical analysis. perhaps surprisingly keep-or-kill rule simply based magnitude coefﬁcients involve information objective function contrast many ad-hoc backward elimination approaches. nicely theorem shows design always rigorous guarantee computational convergence statistical consistency. prototype algorithm extremely simple implement. importantly problem size thus complexity keep dropping owing removal process. annealing schedule nuisance features difﬁcult identify handled close optimal solution ‘apparent’ junk dimensions eliminated earlier stages save computational cost. figure gives demonstration removal convergence process classiﬁcation problem observations variables described section notice zeroed iteration. algorithm stabilizes quickly examples variants algorithm used optimization differentiable loss function subject sparsity constraint described examples given follows regression classiﬁcation ranking. regression. given training examples penalized squared-error loss fig. loss functions left losses interval right zoom interval classiﬁcation. used classiﬁcation feature selection. given training examples loss functions lorenz loss differentiable everywhere zero grows logarithmically respect properties make lorenz loss behave like loss sense correctly classiﬁed examples margin don’t contribute loss. moreover lorenz loss robust label noise logistic losses loss values misclassiﬁed examples margin much higher close margin. loss fig. number kept features iteration different schedules iter implementation details part provide empirical values algorithmic parameters implementation. first annealing schedule {me} slow enough works well terms estimation selection accuracy. fast decaying schedule could reduce computational cost signiﬁcantly. experience shows following inverse schedule parameter provides good balance efﬁciency accuracy max. gradient learning rate arbitrarily small provided number iterations large enough. course large coefﬁcients converge. used classiﬁcation regression. finally observe performance algorithm rather stable large range values parameters iter advantageous implementation parameter tuning. large scale implementation. algorithm parallelized large scale problems subdividing data matrix grid sub-blocks memory processing units. per-observation response vectors obtained row-wise reduction partial sums computed regression lime→∞ always exists; classiﬁcation overlap condition appendix conclusion holds. moreover limit point locally optimal solution minββ≤k suppose asymptotically limit scaled fisher information matrix exists i.e. design matter fact) true satisfy regression coefﬁcient classiﬁcation diag positive deﬁnite. number then exists slow enough schedule {me} sufﬁciently large consistent occurs estimator probability tending proof details given supplementary material. theorem holds generally smoothly penalized loss criteria. example true need overlap assumption classiﬁcation. convergence results regardless large reassuring computation. also imply implementation adopt universal choice stepsize iteration long properly small. moreover view need evaluate global minimum attain good accuracy cooling schedule slow enough. although coming adaptive schedule theoretically sound tempting current theoretical results seem give slow schedules. based empirical experience recommend using inverse function attain good balance accuracy efﬁciency. optimal cooling schedule left theoretical/empirical investigations future. convex works well practice together algorithm seen experiments. ranking. developed extension deal ranking problems. training instances true rankings observations pairs n}×{ criterion used compare instances generate true rankings example better equally good worse here training refers ﬁnding ranking function speciﬁed parameter vector agrees much possible true rankings rij. convergence consistency theorem investigate performance estimators regression classiﬁcation problems. ﬁrst case statistical assumption follows gaussian distribution latter situation following bernoulli distribution mean exp) denotes true coefﬁcient vector focus log-likelihood based loss subsection squared-error loss logistic loss respectively. clarity redeﬁne follows applications large rest subsection iter algorithm. value iteration {me} non-increasing annealing schedule satisfying sufﬁciently large values suppose either regression classiﬁcation. deﬁne rn×m design matrix theorem following convergence consistency results hold classiﬁcation regression respectively stands spectral norm design fig. piecewise linear response functions obtained detection problem using second order prior basis response vector coefﬁcient vector variable ﬁcient vector experiments ﬁrst present simulations synthetic data evaluate feature selection prediction performance compare state feature selection methods. present experiments datasets applications classiﬁcation face keypoint detection applications ranking motion segmentation. synthetic data experiments section focus classiﬁcation regressions problems. data simulations correlated predictors sampled multivariate normal δ|i−j| computational reasons large data experiment shown figure predictors correlated another described section classiﬁcation label data point stability algorithmic parameters experiment evaluate stability algorithm respect tuning parameters learning rate annealing rate number iterations iter. experiment conducted linearly separable data k=k∗=. piecewise linear learners piecewise linear learner piecewise function depends variable instance deﬁned based range variable predeﬁned number bins. length. value learner ﬁnds index returns recent works nonlinear additive models depend variables dimensional smooth functions. proved cubic b-splines optimize smoothness criterion functions. variable selection obtained group lasso penalty. similar model presented coordinate descent soft thresholding algorithm used optimizing group-penalized loss function. work differs works imposing constraints coefﬁcients instead biasing penalty. moreover optimization achieved novel gradual variable selection algorithm works well practice computationally efﬁcient. nonlinear response regularization. aside shrinkage penalty experiment second order prior three parameters large range values yield optimal prediction performance. robustness property contrast sensitivity issue penalty parameters like methods. greatly facilitates parameter tuning reduces ad-hocness. classiﬁcation experiments experiment compare variable selection prediction performance algorithm logitboost algorithm various sparsityinducing penalties popular literature. calling logitboost feature selection require weak learner depend variable. interior point method l-penalized logistic regression using implementation http//www.stanford.edu/∼boyd/l logreg/. obtain given number variables penalty coefﬁcient found using bisection method bisection procedure calls interior point training routine times found gives exactly nonzero coefﬁcients. unpenalized model ﬁtted selected variables.. elastic logistic loss penalty using stochastic gradient descent algorithm. used python implementation sklearn.linear_model.sgdclassifier epochs convergence bisection method ﬁnding appropriate penalty coefﬁcient. feature selection model reﬁt selected variables penalty quantile tisp using ﬁxed sparsity level thresholding iterations iterations selected variables convergence. logistic regression using scad penalty respectively. implementations evaluated ncvreg package based coordinate descent algorithm cvplogistic package based majorization-minimization coordinate descent algorithm cvplogistic package obtained better results reported paper. logitboost using univariate linear regressors weak learners. version learners trained boosting iteration best added classiﬁer. similar learners randomly selected trained iteration best added classiﬁer. tables shown all-variable detection rate average percent correctly detected variables obtained independent runs. average value }|/k∗ stringent criterion percentage times variables correctly found i.e. average area curve unseen data size training data average training times also shown tables algorithm detects true variables often obtains signiﬁcantly better numbers algorithms. time training time reduced three orders magnitude compared penalized methods tisp logitboost. penalized logistic regression needs times data obtain similar performance fsa. noisy data scad methods cannot always reach bayes error even large data. probably sometimes stuck weak local optimum. elastic based stochastic gradient descent behind terms variable selection competitive terms prediction small data sizes. penalized good prediction large data sizes better faster using times less data. observe given sufﬁcient training data algorithm always true variables learn model almost perfect prediction test data. ﬁndings accord theoretical guarantees convergence consistency theorem large data experiment subsection experiment even larger datasets equally correlated predictors. equally correlated case well known challenging feature selection literature. generating datasets trivial computationally efﬁcient ways delivers observations follows ﬁrst generate algorithms compared lorenz loss logitboost named section figure shown training times percent variables correctly detected area curve averaged runs. recall logitboost implementation works columns data matrix thus logitboost severely limited memory capacity could handle observations experiments. although data reloading appears option almost variables needed reloaded boosting iteration computation slow iterations. contrast re-load working variables number quickly drops affordable number starting features less iterations working features observations stored memory experiments. according figure could least times faster logitboost handle much larger datasets. time better detecting true variables better prediction logitboost. regression experiments similar classiﬁcation simulations observations sampled multivariate normal δ|i−j| given dependent variable obtained feature selection simulated annealing wrapper method linear regression uses simulated annealing select best variable -fold cross-validation training set. implemented using caret package iterations restarts. elastic built lasso function matlab mixing coefﬁcient model reﬁt selected variables least squares shrinkage penalty linear regression shrinkage gave best result. methods obtains better predictions terms root mean square error methods. methods need least times data obtain similar performance method. simulated annealing based method decent extremely slow. reputation data classifying websites malicious/non-malicious based feature vector size million. data organized days observations day. training contains ﬁrst days test day. reputation dataset compared conﬁdence-weighted online learning algorithm logistic regression stochastic gradient descent reported table linear lorenz loss obtains test error selecting features close conﬁdence-weighted algorithm error errors logistic regression stochastic gradient descent perform feature selection respectively datasets part feature selection challenge comparisons obtained challenge website. exception feature selection simulated annealing entry described section using parameters linear svm. note challenge website test results could obtained gisette dataset linear algorithm decent compared recursive feature elimination mcp-penalized method better parallel course many methods perform even better using wrapper methods non-linear mappings line neural networks data experiments. number error error method features train valid test reputation m=.·n train=·n test= reg-sgd gisette train=n valid=n test= fsa* bins bins parallel dexter train=n valid=n test= fsa* fsa* dexter data training observations features linear overﬁts. best test error achieved training training validation sets. again data small many methods including wrapper methods nonlinear mappings used better linear classiﬁer. training times shown table showing much slower fsa. face keypoint detection experiments feature selection method intended used computer vision present experiments detecting face keypoints color images. face keypoints centers nose sides mouth corners chin bottom ears represented points aflw. dataset used training testing aflw dataset images containing faces annotated points. them images found contain face image selected training images containing least annotated faces. visual inspection found faces annotated used test dataset aflwmf. images contain faces. feature pool. classiﬁers trained using feature pool consisting histograms oriented gradients features haar features extracted channels training examples. training examples points gaussian pyramid positives within pixel keypoint annotation images pyramid inter-eye distance pixel range. negatives points least keypoint annotation. total aflwt training images contain billion negatives. negatives used training classiﬁers negative mining procedure similar difference hard negatives added training iteration thus training negatives increased mining iteration. classiﬁers trained iterations mining hard negatives. classiﬁer size. separate classiﬁer trained keypoint evaluated. classiﬁers except svm-pl trained monolithic classiﬁer features weak learners. svm-pl classiﬁer trained features without feature selection detection criteria. following criteria used evaluating detection performance. visible face keypoint considered detected image detection found away images pyramid. detected point images pyramid false positive least away face part evaluated face image. results algorithms. compared following learning algorithms logitboost using univariate piecewise constant regressors weak learners. speed reasons learners selected random trained boosting iteration best added classiﬁer. figure shown precision-recall curves detecting nine keypoints aflwmf data. fsa-svm fsa-lorenz perform similarly slightly outperform logistic loss. three versions outperform logitboost greatly outperform piecewise linear features. time training algorithm times faster algorithm times faster full version trains weak learners boosting iteration. fig. precision-recall curves face keypoint detection test aflwmf containing images faces. bottom left right left/right center left/right nose left/right mouth corner left/right chin. methods outperform classiﬁcation regression-based detectors. however must point face alignment methods top-down methods rely face detected ﬁrst face detector case method trained faces. contrast point detectors bottom-up detectors trained faces directly detect keypoints without intermediary step ﬁnding face. involve d-model based face detector uses nine fsa-lorenz keypoint detectors detect face pose obtain curve denoted fsa-lor face. results obtained using top-down pruning step keeps keypoint detections within predicted locations pose. using topinformation obtain results comparable method slightly better supervised descent method tracked frames image sequence) number groups according common usually rigid motion. popular method sparse motion segmentation spectral clustering feature point trajectories projected lower dimensional space spectral clustering performed according afﬁnity measure. ranking using loss piecewise linear response functions used ranking number candidate motion segmentations obtained spectral clustering different parameters predict best one. segmentation features extracted measuring model ﬁtness cluster compactess. details candidate segmentations generated feature pool given supplementary material. rank based method motion segmentation evaluated hopkins dataset hopkins dataset contains sets feature point trajectories motions videos along corresponding ground truth segmentation. based content video type motion sequences categorized three main groups checkerboard trafﬁc articulated. figure shows sample frames three videos hopkins database feature points superimposed. checkerboard average median trafﬁc average median articulated average median average median checkerboard average median trafﬁc average median articulated average median average median sequences combined average median fold cross validation. hopkins dataset contains sequences videos. videos divided random subsets subset containing videos. hopkins sequences also divided subsets subset containing sequences corresponding subsets videos. reason separating videos ﬁrst sequences fairness. motion sequences subsets motion sequences possible segmentation motions subset motions. happens would unfair -motion sequence training -motion subset sequence testing. round cross validation select subsets sequences test form training remaining subsets. training apply obtained ranking function rank motion segmentations sequence. best ranked picked ﬁnal result calculate misclassiﬁcation rate. parameter settings. parameters fsa-rank method were number bins number selected features parameters iter compared fsa-rank method rankboost algorithm based features decision stumps weak rankers following parameters number thresholds decision stumps number boosting iterations misclassiﬁcation error ranking accuracy. table shown average misclassiﬁcation errors sequences training test set. methods compared randomized also difference misclassiﬁcation rate training test small fsa-rank especially -motion sequences. comparison average misclassiﬁcation rate motions test rankboost larger training misclassiﬁcation rates quite close method. probably small number features selected shrinkage prior together helped obtain small training error good generalization. compared uses ﬁxed measure select best segmentation method works better categories. moreover average misclassiﬁcation rates method motions motions almost half cumulative distributions shown figure motions method performs much better methods compared motions method comparable best nevertheless method outperforms rankboost situations. conclusion future work paper presented novel learning scheme feature selection high dimensional data applications. gradually identiﬁes removes irrelevant variables proceeds according annealing schedule. showed solves constrained optimization problem performance guarantee estimation selection. opposed penalized method proposed method runs much efﬁciently introduce undesired bias estimation. kills variables progressively based importance opposite model growing process boosting usually brings improvement variable selection prediction. algorithm suitable data computation simplicity ability reduce problem size throughout iteration. contrast boosting total amount data algorithm needs access training times size training makes amenable large scale problems. hence computation similar advantages online algorithm much accurate. approach applies generically many types problems including regression classiﬁcation ranking instance. extensive experiments synthetic data real data support competitive alternative many up-to-date feature selection methods. method challenging object detection problems. references agresti. categorical data analysis. wiley albert anderson. existence maximum likelihood estimates logistic regression models. biometrika bias estimation. biometrika bunea tsybakov wegkamp. sparsity oracle inequalities lasso. electronic journal statistics burden faires. numerical analysis. brooks/cole huang horowitz wei. variable selection nonparametric additive models. ann. stat. jiang huang. majorization minimization coordinate descent concave penalized generalized linear models. technical report koestinger wohlhart roth bischof. annotated facial landmarks wild large-scale real-world database facial landmark localization. first ieee international workshop benchmarking facial image analysis technologies landweber. iteration formula fredholm integral equations ﬁrst kind. amer. math. sparse online learning truncated gradient. journal machine learning research r¨atsch warmuth. maximizing margin boosting. computational learning theory pages ravikumar wasserman. spam sparse additive models. journal royal statistical society series rudin schapire daubechies. boosting based smooth margin. learning theory pages springer schapire. strength weak learnability. machine learning zhang. solving large scale linear prediction problems using stochastic gradient descent algorithms. icml page zhao model selection consistency lasso. journal adrian barbu adrian barbu received degree university bucharest romania ph.d. mathematics ohio state university ph.d. computer science ucla research scientist later project manager siemens corporate research working medical imaging. received thomas edison patent award coauthors work marginal space learning. joined statistics department florida state university ﬁrst assistant professor since associate professor. research interests computer vision machine learning medical imaging. yiyuan yiyuan received b.s. degree mathematics m.s. degree computer science peking university respectively received ph.d. degree statistics stanford university currently associate professor department statistics florida state university. research interests include high-dimensional statistics machine learning robust statistics statistics computing bioinformatics network science. liangjing ding liangjing ding received degree electrical engineering m.s. degree biomedical engineering university science technology china ph.d degree computational science florida state university research assistant supervision professor adrian barbu. research interests machine learning applications computer vision. gary gramajo received mathematics statistics ph.d statistics supervision professor adrian barbu florida state university. mcknight fellow statistician chick-ﬁl-a data analytics atlanta research interests machine learning applications challenging business problems. finally sequence deﬁned minγγ≤k /η). summary /η−x iterates yielded original algorithm notice used step equals eventually thus stay ﬁxed afterwards. lemma must decreasing remark. result implies implementation adopt universal step choice iteration long satisﬁes part prove strict convergence regardless size fact occur. without loss generality suppose thus inactive. unlike gaussian regression logistic regression ﬁnite maximum likelihood estimator even restricted small features class labels overlap predictor space literature avoid irregularities convergence consistency proofs proof. focus classiﬁcation case. proof regression similar facilitate derivation introduce necessary notation symbols. first deﬁne quantile thresholding variant hard-ridge thresholding given deﬁned largest components shrunk factor remaining components zero. case ties random breaking rule used. write given vector subset {··· denotes subvector components indexed similarly given matrix rn×m subset {··· denotes submatrix formed columns indexed deﬁne vector deﬁned componentwise. rn×m loss simplicity) β))). using previously deﬁned symbols easy calculate ∂f/∂β likewise fisher information matrix denoted explicitly calculated occur probability tending letting max|ξs∗ min|ξs∗| therefore step yields steps relevant predictors kept probability tending repeating argument stage gives consistency sufﬁciently large. proof gaussian case follows lines. part assume overlap condition landweber’s classical convergence result ranking motion segmentation major difﬁculty spectral clustering rigid motion lies dimensional space ﬁxed dimension. result several motions present video sequence hard determine best projection dimension spectral clustering. consequently segmentation methods propose project number spaces different dimensions best results according measure. however hard versatile measure consistently ﬁnds best dimension scenarios. moreover segmentation algorithms always parameters noise level separability afﬁnity measure need tuned according different scenarios. also hard expect exists parameters work well problems. furthermore many motion segmentation algorithms published recent years strength weaknesses. would practical importance segment sequence many different algorithms automatic select best segmentation. work address problem choosing best segmentation larger segmentations generated different algorithms algorithm different parameters. formalize ranking problem solve using supervised learning fsa-rank algorithm. segmentation spectral clustering candidate segmentation results generated velocity clustering algorithm brieﬂy describe make paper self-contained. existence guaranteed overlap assumption details). square matrix diagonal entries positive rank. orthogonal complement construct still satisﬁes without loss generality still write algebra last equality taylor expansion. however singular e.g. introduce converges. nicely construction thus ¯γ). indicates sequence strictly converges although larger limit point denoted satisﬁes local optimality easily follows. part construct two-stage cooling schedule show consistency. recall assumption solution central limit theorem indicates probability tending unique solution. stage accordingly squeezing operations take essential effect. bound estimation error write βo∞. central limit theorem ﬁrst term right hand side thus deﬁne min|β minj∈s∗ probability tending features segmentation. motion segmentation algorithm given sequence learned parameter vector used select best segmentation sequence. whole procedure described algorithm input measurement matrix rf×p whose columns point trajectories number clusters preprocessing build velocity measurement matrix given dmin dmax i-th column apply spectral clustering points using afﬁnity measure obtaining segmentation extract feature vector segmentation described above. compute ranking could easily calculated comparison ground truth segmentation. true rankings constructed based relative misclassiﬁcation errors segmentations. since test time segmentations belonging sequence compared contains pairs segmentations obtained sequence. number motions. range contains possible dimensions spaces containing mixed rigid motions. last spectral clustering applied obtain segmentation using angular afﬁnity removing possible repetitive segmentations around segmentations would generated sequence. method proposes error measure select best segmentation paper solves problem learning. likelihood prior based features motion segmentation described labeling types features characterize ranking motion segmentation likelihood features prior features. original trajectory vectors points obtained projection space dimension parameter least squares sense afﬁne subspaces points motion label denote label trajectory euclidean distance point plane total number trajectories. given k-nearest neighbor graph constructed using distance measure space given dimension distance could either euclidean distance angular distance deﬁned changing dimension number neighbors rankboost algorithm rankboost algorithm used paper baseline method compare performance learning ranking function. training instances. assume ground truth ranking given subset means ranked vice versa. rankboost searches ranking similar given ranking formalize goal distribution constructed max{ rij} learning algorithm tries ranking function minimizes weighted wrong orderings", "year": 2013}