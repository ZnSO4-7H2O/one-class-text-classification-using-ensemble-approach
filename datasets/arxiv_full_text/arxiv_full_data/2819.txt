{"title": "Applying Cooperative Machine Learning to Speed Up the Annotation of  Social Signals in Large Multi-modal Corpora", "tag": ["cs.HC", "cs.AI", "cs.LG", "stat.ML"], "abstract": "Scientific disciplines, such as Behavioural Psychology, Anthropology and recently Social Signal Processing are concerned with the systematic exploration of human behaviour. A typical work-flow includes the manual annotation (also called coding) of social signals in multi-modal corpora of considerable size. For the involved annotators this defines an exhausting and time-consuming task. In the article at hand we present a novel method and also provide the tools to speed up the coding procedure. To this end, we suggest and evaluate the use of Cooperative Machine Learning (CML) techniques to reduce manual labelling efforts by combining the power of computational capabilities and human intelligence. The proposed CML strategy starts with a small number of labelled instances and concentrates on predicting local parts first. Afterwards, a session-independent classification model is created to finish the remaining parts of the database. Confidence values are computed to guide the manual inspection and correction of the predictions. To bring the proposed approach into application we introduce NOVA - an open-source tool for collaborative and machine-aided annotations. In particular, it gives labellers immediate access to CML strategies and directly provides visual feedback on the results. Our experiments show that the proposed method has the potential to significantly reduce human labelling efforts.", "text": "scientiﬁc disciplines behavioural psychology anthropology recently social signal processing concerned systematic exploration human behaviour. typical work-ﬂow includes manual annotation social signals multi-modal corpora considerable size. involved annotators deﬁnes exhausting time-consuming task. article hand present novel method also provide tools speed coding procedure. suggest evaluate cooperative machine learning techniques reduce manual labelling eﬀorts combining power computational capabilities human intelligence. proposed strategy starts small number labelled instances concentrates predicting local parts ﬁrst. afterwards sessionindependent classiﬁcation model created ﬁnish remaining parts database. conﬁdence values computed guide manual inspection correction predictions. bring proposed approach application introduce nova open-source tool collaborative machine-aided annotations. particular gives labellers immediate access strategies directly provides visual feedback results. experiments show proposed method potential signiﬁcantly reduce human labelling eﬀorts. various research disciplines annotation social behaviours common task. process includes manually identifying relevant behaviour patterns audio-visual material assigning descriptive labels. generally speaking segments signals mapped onto discrete classes certain type gesture social situation emotional state person. social signal processing pentland subset events called social signals used augment spoken part message non-verbal information enable natural human-computer interaction vinciarelli automatically detect social signals sensory input machine learning applied. sensory input transformed compact relevant features classiﬁer trained manually labelled examples optimise learning function. trained classiﬁer used automatically predict labels unseen data. however since humans transmit non-verbal messages number channels complex interplay channels progress directly linked availability large well transcribed multi-modal databases rich human behaviour varying context diﬀerent environmental settings douglas-cowie common challenges creating datasets high degree naturalness required recording scenarios well recording scenario generalises settings number human raters needed reach consensus labels course sheer amount data. thinking many hours labelled data required clear gathering large amounts annotated training samples seems like infeasible task respect time cost eﬀort. unfortunately bulk databases collected hitherto contain either acted behaviour recorded limited number professional actors emo-db burkhardt isolated snapshots grimm proper training online recogniser however requires long continuous recordings collected preferably natural conditions douglas-cowie example semaine corpus mckeown composed sessions emotionally coloured free conversations. however like comparable corpora iemocap busso features audio-visual content only. given richness observable social expressions still growing need labelled data human interactions eerekoviae pantic vinciarelli least state-of-the algorithms deep neural networks proﬁt large amounts labelled training data even though exists vast resource data nowadays pervasive digital format relatively easy inexpensive collect public resources social media problem gathering relevant annotations still needs overcome. approach active learning algorithm interactively query user manually label certain data points. core idea extract informative instances pool unlabelled data based speciﬁc query strategy settles selected instances passed human annotators ﬁnally labelling model derived subset. this course reduces labelling eﬀort. addition positive side eﬀects. first speeds training since fewer instances processed. second helps improving maximum accuracy reaches coherent learning model work zhang takes idea step forward combines semisupervised learning techniques eﬃciently share labelling work human machine pre-existing classiﬁer used derive conﬁdence values unlabelled data thus human annotators involved instances predicted insuﬃcient conﬁdence. strategy allows performance existing classiﬁer improved minimising costly work human labelling. save labelling eﬀorts apply dynamic active learning choosing reliable raters ﬁrst zhang words algorithm also considers many annotators queried instance level. article subsume learning approaches eﬃciently combine human intelligence machine’s ability rapid computation term cooperative machine learning dong zhang figure illustrate approach creates loop machine learned model human annotators initial model trained used predict unseen data active learning module decides parts prediction subject manual revision human annotators afterwards initial model retrained using labelled data procedure repeated data annotated. actively incorporating human expert knowledge learning process becomes possible interactively guide improve automatic prediction. hence approach bears potential considerably manual eﬀorts. instance system quickly learn label simple behaviours already facilitates work load human annotators early stage. time could learn cope complex social signals well point able ﬁnish task completely automatic manner. iterative approach even help bridging quantitative qualitative coding still deﬁnes great challenge many ﬁelds social science chen paper hand examining extent proposed approach helps speeding annotation social signals. addition present tool allows researchers apply described techniques databases. main contributions work follows figure scheme depicts general idea behind cooperative machine learning initial model trained partially labelled data. initial model used automatically predict unseen data. labels conﬁdence selected manually revised. initial model retrained predicted revised data. aided labelling walk-through presented demonstrate collaborative annotating capability system. experiences users working tool reported discussed section interactive machine learning amershi fails olsen aims involve users actively creation models recognition tasks. approaches integrate automated data analysis interactive visualisation tools order enable users inspect data process features tune models. section focus approaches facilitate acquisition annotated data sets introduce novel methodology applying cooperative machine learning speed annotation social signals large multi-modal databases. common approach reduce human labelling eﬀort selection instances manual annotation based active learning techniques. basic idea forward instances prediction certainty high expected error reduction human annotators settles right thereby estimate informative ones. whole range options choose exist calculation ‘meaningful’ conﬁdence measures detecting novelty estimating degree model change data instance would cause trying track ‘scarce’ instances trying data instances rare terms expected label. sophisticated approaches aggregate results machine learning crowdsourcing processes increase eﬃciency labelling process. kamar made learned probabilistic models fuse results computational agents human labellers. showed allocate tasks coders order optimise crowdsourcing processes based expected utility. zhang developed agreement-based annotation technique dynamically determines many human annotators required label selected instance. technique considers individual rater reliability inter-rater agreement decide combination raters allocated instance. active learning shown great potential large variety areas including document mining tong koller multimedia retrieval wang activity recognition stikic emotion recognition zhang studies area focus gain obtained application speciﬁc active learning techniques. however little emphasis given question assist users application techniques creation corpora. beneﬁts integrating active learning annotation tasks demonstrated variety experiments annotation tools provide users access active learning techniques rare. recent developments audio image video annotation make active learning include camomile poignant ihearu-play hantke however systematic studies focusing potential beneﬁts active learning approach within annotation environment user’s point view performed rarely cheng bernstein pardo techniques enable systems learn human raters become widespread little attention paid usability challenges remaining tasks left end-users amershi rosenthal investigated kind information provided users order reduce annotation errors setting active learning. found contextual information predictions learning algorithms particular useful annotation activity data. contrast uncertainty information eﬀect accuracy labels indicated labellers classiﬁcation hard. amershi investigated empower users select samples training appropriate visualisation techniques. found representative overview best worst matching examples higher value high-certainty images conjecture high-certainty images provide much information learning processing similarity already labelled images. another paper amershi authors suggest interactive visualization technique assess model performance sorting samples according prediction scores. tool user directly inspect samples retrieve additional information annotate better performance tracking. tool allows users monitor performance individual samples model iteratively retrained. approaches supported users annotation selection samples training. alternative graphical user interfaces developed enable users create annotated examples training models. typically labels given instructions stimuli provided users evoke particular behaviours. example includes ssi/modelui wagner presents users graphical user interface allows test diﬀerent machine learning algorithms labelled data. labels acquired stimuli include textual instructions also images videos. however users determine kind stimuli data useful create tune models. summing said many studies experientially investigate potential novel techniques minimise human labour. addition studies actually label novel data rather test whether method could save eﬀort. also note prevailing choice merely active learning rather combination semi-supervised learning cooperative machine learning. relatively little attention paid however question make techniques available human labellers. high demand annotation tools integrate cooperative machine learning order reduce human eﬀort particular area social signal processing human raters typically disagree labels lotﬁan busso setting dynamic active cooperative strategies appear particularly promising learning target task also much possible raters reliability depending labels content labelled. likewise learned ‘whom trust when’ reduce annotation eﬀort requiring labels ‘right persons right time’. cooperative machine learning strategy propose two-fold one. divided session completion step information fraction single session used complete remaining part session session transfer step information labelled sessions used predict unlabelled sessions. understanding session deﬁnes single continuous self-contained recording. sessions database captured diﬀerent dates sites involving diﬀerent subjects. division motivated lack labelled data beginning annotation process usually allow build models robust enough generalise well unseen parts. especially true recording conditions involved subjects vary individual sessions. nevertheless already small fractions labelled data suﬃcient build models able make reliable predictions data resembles instances seen far. example data recorded subject comparable conditions something generally expect snapshots session. even models weak make reliable predictions whole dataset help speed early annotation process. following refer classiﬁer trained samples single session session-dependent classiﬁer. enough sessions completed session-independent model trained used accomplish remaining sessions. ensure quality recognition manual veriﬁcation outcome classiﬁcation might necessary. procedure accelerated rating predictions adding conﬁdence values predicted instances. instead reviewing everything annotators concentrate parts conﬁdence labels predicted high uncertainty. proposed strategy summarised follows session completion manually assign labels fraction session train session-dependent classiﬁer. apply complete remaining fraction. based conﬁdence values generated model query manual revision. session transfer take fully labelled sessions train session-independent classiﬁer. apply predict annotations remaining sessions. again based generated conﬁdence values decide parts require manual adjustment. classiﬁcation. depending corpus strategy applied necessarily best practise. instance dataset composed recordings short apply ﬁrst step adapt strategy initially complete recordings belonging subject. labelled data suﬃcient number individual subjects continue training subject-independent model apply remaining recordings. likewise described strategy across several databases too. case would concentrate individual databases ﬁrst afterwards obtain database-independent model label remaining databases. eﬃciently apply described strategy would like know sweet spot applying session completion session transfer step. hand apply early model becomes unstable predictions poor. hand annotate data necessary give away precious time. avoid described situations interested ﬁnding good trade-oﬀ machine performance human eﬀort. unfortunately cannot easily guess ideal moment hand task machine. amount training data required build robust model depends number factors homogeneity data discrimination ability extracted features number subjects classes least complexity recognition problem. alternatively instead trying determine sweet spot beforehand could iteratively test applicability strategy stop performance seems promising. therefore opted make described strategy integral part tool allows annotators visually examine results time individually decide whether labelling required not. however means time takes strategy becomes crucial factor. generally take longer seconds annotation process interrupted reach goal reuse much information possible. possibility apply classiﬁcation small sliding window rather simple classiﬁer. former means features extracted latter ensures fast training. following restrict complete discrete annotations deal multi-class problems. case step receive signal stream current session partly ﬁnished annotation composed labelled segments discrete start point. segments variable length gaps successive segments. applying following procedure predict segments unlabelled fraction session training assign frames overlap labelled segment least corresponding class. case several candidates keep dominant assign remaining frames rest class. figure visualisation cooperative machine learning strategy means step point last segment manual annotation deﬁnes training fraction ends prediction begins. labelled segments mapped onto frames empty frames assigned rest class. model build frames training fraction used predict frames prediction fraction. successive frames class label combined rest class removed segments conﬁdence highlighted. combine successive frames belonging class keep average probability combined frames conﬁdence. remove frames belong rest class. optionally apply thresholds remove small segments gaps. turn experiments examine practical eﬀect proposed cooperative machine learning strategies section means database including natural human-human interaction simulate situation detection system applied predict unlabelled fractions dataset. using original predicted parts corpus train ﬁnal detection model evaluate robustness eﬃciency approach. interaction database database cafaro corpus screenmediated face-to-face interactions features natural interactions human dyads expert-novice knowledge sharing context. session participant assumes role expert participant role novice. corpus created part aria-valuspa valstar project therefore strong focus medial face-to-face interactions expertnovice setting. figure shows users interaction. purpose noxi study interruption strategies. instance listener decides question comment speaker saying therefore starts attempt take speech turn. simplest detect situations looking spots voice participants overlapping. afterwards speaker change occurs assume interrupting party successfully took turn. otherwise treat failed attempt. however interposed utterance necessarily signal interrupt speaker. also expression approval interest denoted backchannels. likewise every speaker pause signals ﬂoor change instance speaker needs time think next. bridge pauses speakers usually utter ﬁller sound. hence correctly identify speaker interruptions separate backchannels ﬁllers speech parts. following present detection system trained automatically identify backchannels ﬁllers speech. first evaluate system following classic machine learning approach measure performance system. afterwards examine extent system able speed-up manual annotation process loop. agement communication expression nuances attitude intention yngve generally distinguish backchannels ﬁllers. english backchannels utterances uh-huh well short words yeah okay really?’ etc. purpose verbalise active listenership assessments well requests clariﬁcation sentence completions young generally actively support speaker keep ﬂoor. likewise ﬁllers sounds words speakers pause signal ﬁnished talking. listener hears ﬁller continues listening rather start talking. instance speakers interpose ﬁllers search word take pause think next. common ﬁller sounds english language clark like backchannels help continue conversations smoothly. addition backchannels ﬁllers laughter deﬁnes third group vocalisations often interposed both speaker listener without intention invoke turn change. laughter often sign amusement though also show scorn embarrassment ruch ekman previous work shown backchannels ﬁllers detected acoustic phonetic properties speech. edlund detect backchannels using k-nearest neighbors classiﬁer duration interspeaker relative loudness. system distinguishes backchannels vocabulary overall accuracy prasad bali disambiguate hindi word discourse particle also lexical equivalent three discourse functions questions backchannels agreement. extract features based pitch contour power duration apply k-means clustering. system backchannels correctly identiﬁed accuracy detection ﬁllers laughter events picked interspeech computational paralinguistics challenge schuller acoustic feature proposed organizers challenge derived energy cepstral voicing related low-level descriptors deltas. ﬁnal feature vector combines arithmetic mean standard deviation across current frame eight neighbouring frames. recognition accuracy reported area curve measure witten frank yielding unweighted average laughter classes ﬁller classes training support vector machines classiﬁer. adding phonetic features results improved another wagner number studies investigated hidden markov models modelling temporal variations diﬀerent laughter sounds. example tanaka campbell investigate four phonetic categories laughter based spectral features. approach single laughter categories recognised accuracy rate schuller test various classiﬁers using spectral features distinguish four kinds isolated non-verbal vocalisation experiment hmms achieve accuracy rate outperform hidden conditional random fields support vector machines scherer show performance laughter classiﬁers depends whether applied online oﬄine mode. svms gave best results oﬄine mode given segmentations. however online mode surpassed echo state networks hmms fail onoﬀsets laughter. particular challenge laughter detection overlapping laughter conversational speech. truong trouvain conducted study using four corpora order identify spectral features diﬀerentiate overlapping non-overlapping laughter. though experiments concentrate detection speech ﬁllers/ backchannels detection system generic possible. allow apply classiﬁcation problems too. also speed performance plays crucial role want interfere annotation process. following start describing proposed generic detection system. modularity capability fast online incremental processing rely opensmile audio feature extraction tool eyben however refrain using large statistical feature like compare assembles features brute-force combination level descriptors functionals schuller kind feature sets usually applied chunks several seconds length scenario however frame-based feature extracted small moving window reused across successive training steps. also keep mind especially beginning annotation process size training sets small. case smaller feature lower risk overﬁtting. mel-frequency cepstral coeﬀcients provide compact representation short-term power spectrum. long tradition speech recognition systems rabiner juang speaker veriﬁcation tasks ganchev also successfully applied ﬁeld social signal processing emotional speech recognition beritelli kishore satish neiberg schuller vogt andr´e laughter detection kennedy ellis knox mirghafori urbain tests calculate mel-frequency cepstral coeﬀcients ﬁrstsecond-order frame-to-frame diﬀerence according standard practice moving window frame step afterwards reduce stream frame step averaging always four frames. ensures sample rate feature stream consistent video frame rate though relevant current study handy want integrate visual features future. small enough detect start point voiced segments suﬃciently figure illustration feature extraction step. first four mfcc frames dimension averaged reduce sample rate signal afterwards neighbouring frames added frames left frames right. results ﬁnal feature vector size accurate. since length ﬁller events want detect longer optionally concatenate neighbouring frames sides current frame following denoted context size context size means current frame extended frames left frames right. increases number features factor classiﬁcation model linear support vector machine provided liblinear library large linear classiﬁcation since implementation kernels training time signiﬁcantly reduced even large input sets composed several thousand samples. multi-class classiﬁcation select l-regularised logistic regression solver bias term keep default values parameters. since expect unbalanced class distributions randomly remove samples match size class least number samples. finally features values scaled conﬁdence values scaled individual class scores established generic classiﬁcation system evaluate recognition performance noxi corpus pick sessions randomly split training including twothird sessions summing nearly audio data. remaining sessions form test overall duration almost tate voiced parts audio ﬁles. introduce machine bias none strategies described section applied. manual annotation accomplished three experienced annotators completing sessions. table lists applied annotation scheme. since labels assigned voiced sounds remaining parts implicitly deﬁne rest class silence. better audio quality head recordings. however turned close-talk recordings tended pick breathing sounds introduce additional breath class prevent false alarms silenced parts. backchannels ﬁllers laughter voiced sounds grunts coughs gathered single class denoted filler. speech segments neither backchannels ﬁllers labelled speech. example annotation shown figure asked raters measure long took annotate sessions. total spent little results average time minutes session. next split annotations frames length extract mfcc features results frames sample training samples class train linear model. results summarised table report classwise recognition accuracy unweighted average recall direct comparison interspeech social signals paralinguistic challenge also consider area curve measure. filler class shows results comparable schuller take prove detection system reasonable examined task. seen table increasing number concatenated frames positive eﬀect recognition accuracy especially filler class beneﬁts larger frame context explain fact ﬁllers usually short isolated speech episodes surrounded silence. figure notice saturating eﬀect frames. give impression system performs terms speed report measurements intel core i-k. tests extracting mfcc-based features context size frame step took minute mono audio sampled khz. extrapolated interaction requires less minutes extract features whole german subset. since features reused deﬁnes one-time eﬀort. training linear classiﬁer training took average frame-wise prediction test values suggest proposed detection system fast enough embedded annotation process without causing serious interruptions finally want know proposed detection system performs combination proposed strategies. section deﬁned sweet spot moment additional annotation eﬀorts longer improve stability classiﬁcation model. practically deﬁnes ideal point hand task machine. experientially determine sweet spot given problem incrementally inject information training process. following simulate procedure splitting original training parts assume sessions manually labelled whereas remaining sessions unlabelled derive three classiﬁers figure default condition classiﬁer evaluated training labelled sessions only. case unlabelled sessions predicted used retrain model. case predicted labels reviewed possibly corrected retraining takes place. threshold correct necessary. simulates case annotation process stopped point labelled fraction database used predict remaining parts. note case predicted labels included ﬁnal training step automatic selection strategies additional manual eﬀorts applied. simulates case parts prediction inspected assess additional manual eﬀort measure call inspection rate fraction frames conﬁdence correction rate fraction frames ﬁnally assigned diﬀerent label. table summarises performance test assume sessions original training labelled based results gain interesting insights. therefore assume classiﬁcation model maximum percent worse reference model trained sessions unweighted average recall least case deﬁnes conﬁdence threshold inspecting predicted labels. start labelled sessions. results obtained detection system described earlier using stacking context performance classiﬁer shows twelve sessions suﬃcient yield recognition accuracy. hence achieve goal stop labelling sessions skip last two. happens extend training predicted labels checking results sessions required achieve desired accuracy. fact extending training purely predicted data generally positive eﬀect recognition performance. although disappointing ﬁrst glance actually surprising. obviously cannot expect improve model unless inject knowledge case predictions without inspection. asked student revise test pointless unless point mistakes ﬁrst. hence manual eﬀorts needed here. indeed correcting frames conﬁdence yields already sessions. achieve actually review predicted frames. assume remaining sessions make approximately half frames corresponds full training total examine training data. mentioned earlier average time annotate session minutes. hence reckon saving approximately hours obviously signiﬁcantly speeds annotation process. predicted labels earlier receive stable classiﬁcation model. fact lift correction threshold observe yields already ﬁrst session. however achieved expense three times higher inspection rate means view almost hence better strategy complete couple sessions ﬁrst return apply smaller correction threshold afterwards leaving less data inspection results previous section encouraged integrate proposed cooperative machine learning approach annotation tool nova verbal annotator). give annotators possibility immediately inspect necessary correct predicted annotations. though earlier version tool existed extended achieve seamless integration collaborative annotation process. nova open-source downloaded http//github.com/hcmlab/nova. nova’s interface inspired existing annotation tools. instance eudico linguistic annotator wittenburg annotation video language kipp exmaralda schmidt tools oﬀer layer-based tiers insert time-anchored labelled segments discrete annotations. continuous annotations hand allow observer track content audiovisual stimulus time based continuous scale. tool allows labellers trace emotional content real-time dimensions feeltrace cowie descendant gtrace cowie allows user deﬁne dimensions scales. tools accomplish continuous descriptions carma girard darma girard wright interesting approach gathering crowd-sourced annotations ihearu-play hantke allows labelling audio material valence-arousal scale form browser-game. whereas tools restricted describe audiovisual data single user repovizz mayor integrated online system collaboratively annotate streams heterogeneous data datasets stored online database allowing users interact data remotely browser. though mentioned tools great help create annotations high level detail tools oﬀer none little automation. since labelling several hours interaction extremely time consuming task methods figure nova allows visualise various media signal types supports diﬀerent annotation schemes. down full-body videos along skeleton face tracking audio streams persons interaction. lower part several discrete continuous annotation tiers displayed. annotations edited static fraction recording interactively playback. automate coding process highly desirable. nova advanced features create collaborative annotations apply strategies support truly collaborative work-ﬂow several annotators machine database back-end provided store exchange combine annotation work. nova user interface designed special focus annotation long continuous recordings involving multiple modalities subjects. unlike annotation tools number media ﬁles displayed limited various types signals supported. further multiple types annotation schemes selected describe visualised content several statistics available process annotations created multiple coders. instance statistical measures cronbach’s cronbach cohen’s cohen applied identify interfigure integration nova database populated recordings human interaction. nova functions interface data provides database distribute accomplish annotation tasks among human annotators. times applied automatically complete unﬁnished fractions database session-dependent model trained partly annotated session applied complete pool annotated sessions used train session-independent model predict labels remaining sessions. cases conﬁdence values guide revision predicted segments following concentrate another feature nova tools speed annotation multi-modal corpora. figure shows nova mediator database several human machine annotators. steps described section supported interface. support collaborative annotation process nova maintains database back-end allows users load save annotations mongodb running central server. gives involved annotators possibility immediately commit changes follow annotation progress others. mongodb open-source cross-platform nosql database. chosen favour relational database simplicity fast read/write operations. design allows read write annotations manages relevant meta data corpus too. generally corpus represented single database including several collections collections figure overview nova’s database structure. annotations meta information subjects sessions etc. stored diﬀerent collections. nova includes necessary tools maintain populate database. soon several users collaborate common database becomes crucial implement adequate security policies. instance want prevent situation user accidentally overwrites annotation another user. therefore standard users edit delete annotations. however load annotations users. case annotation copied stored username. users privileged admin rights edit delete annotations users. also assign newly created annotations speciﬁc users. admin divide forthcoming annotation tasks among pool annotators. beside human annotators database also visited machine users. like human operator create access annotations. hence database also functions mediator human machine. control annotation progress introduced ‘isfinished’ signals annotation requires ﬁtting ﬁnished. second ‘islocked’ marks whether annotation editable not. nova provides instruments create populate database mongodb server scratch. gives users possibility apply tool corpora. time annotators schemes additional sessions added. speciﬁc knowledge databases required. best possible performance tasks related machine learning outsourced executed background process. framework open-source social signal interpretation framework. successfully applied couple recognition problems past lingenfelser urbain wagner since primarily designed build online recognition systems trained model directly used detect social cues real-time wagner applied stream signal values ﬁrst preemphasis ﬁlter mfcc features extracted sliding window frame step conﬁgure mfcc extraction separate option created however supports features sets too. instance allows scripts widely used opensmile toolkit eyben provides feature sets type signals. instance wrapper openface tool baltruˇsaitis available extract facial points action units video streams. here conﬁgured balance number class samples removing samples overrepresented classes scale features common interval. training model linear used. however also supports classiﬁcation models google’s neural network framework tensorflow popular theano library. conclude section walk-through demonstrates nova’s tools. assume database created populated several sessions audio recordings users. case work noxi database described apply annotation scheme used evaluation section want mark ﬁller breath events regular speech assigning labels breath filler speech. note number names classes deﬁned underlying annotation scheme easily adapted user labelling problem. ﬁrst step extract mfcc features german sessions noxi database. dialogue shown figure allows choose source stream feature extraction method optionally overwrite default frame step context sizes. extraction accelerated running several sessions parallel figure screenshot feature extraction dialogue. user chooses stream according feature extraction method feature extraction applied selected roles sessions. select input choose classiﬁcation model optionally left right context concatenate neighbouring feature frames afterwards trained model stored applied predict unlabelled data. predict annotations strategies section available. case session transfer dialogue similar figure shown. however time select previously trained model predict selected sessions. case session completion step annotation completed temporarily training model using labels available current tier. example completion shown figure screenshot shows labels conﬁdence highlighted pattern. crucial parts quickly found revised necessary. assess prediction accuracy model dialogue similar figure available. here pick trained model sessions want evaluation model applied predict labels selected sessions output compared existing annotations. result presented form confusion matrix shown figure confusion matrix provides information overall recognition performance well accuracies individual classes class pairs often confused. figure screenshot model training dialogue. user selects coding scheme role annotator sessions according annotation exists displayed stream selected deﬁne input learning step. finally model chosen training begins. figure visualisation partly ﬁnished annotation results tier automatically completed segments conﬁdence marked pattern. lower tier shows ﬁnal result manual correction. potential signiﬁcantly reduce human labelling eﬀort. however necessarily mean results gained simulation transferred human labellers without ado. hence following want discuss experiences users applying strategies nova tool introduced previous section. exact gain expect giving tool like nova hands human labellers? unfortunately general answer question probably exist. experiences show amount time save depends couple variables vary case case another. figure confusion matrix provides information recognition accuracy individual classes extent confused classes. instance speech frames often falsely classiﬁed ﬁllers vice versa. hence annotator attention classes revising predictions. probably largest uncertainty comes nature annotation problem ability applied machine learning techniques cope instance assume task labelling voiced parts audio. recordings background noise speech really prominent signal simple feature like loudness already allow train robust model samples generalizing well unseen data. case time saving tremendous. hand speech ﬁles noisy contain audible sounds possibly overlapping speech problem becomes immediately harder. consequence sophisticated feature needed manual labelling eﬀort required obtain robust model. consequence less time saved. even reckon case problems becomes hard train reliable model eﬀort manually revise prediction initial savings. possibility exchange features classiﬁers nova therefore essential precondition adapt problem hand best possible way. another point consider quality annotation desired. live false prediction? high precision mind high number false negatives? this course depends much purpose data labelled for. special social signals often lack ground truth. multiple raters employed agreement often turns low. makes specially diﬃcult estimate quality prediction. depends assessment user pleased automatic completion. here nova’s feature immediately visualize results important tool raters assess quality automatic predictions. finally comparing manual semi-manual annotations straight forward seem. observing automatic predictions observed onoﬀset labels often precise humans usually rather fuzzy likewise found short occurrences behaviour easily overlooked human labellers especially attention drops time. hence since machines show signs fatigue predictions often consistent throughout corpus compared humans. consequently applying strategies help saving time also lead accurate stable annotations. learn general applicability approach asked experienced annotators hired manual annotation noxi database redo sessions. task annotate speech ﬁller events. time however explicitly told make integrated tools. afterwards compared semi-automated annotation previous manual ones also asked subjective impressions. first reported positively surprised accuracy automatically generated labels. little manual eﬀorts required correct predictions given task. particular found cases detections even precise compared previous manual annotation. explain fact human brain naturally ﬁlters information perceives relevant current context. instance consciously hear short breathing sounds utterance since concentrate content spoken message. even force special attention certain events exhausting accurately label every occurrence. situation semi-automated annotation really machines contrast humans tired repeat task again. hence behaviour easy detect occurs frequently justify manual eﬀorts labelled automatically. course precise working method machine always desired eﬀect. annotators noticed sometimes incorrect labels introduced too. particular ﬁller events falsely detected beginning ending utterance. explain fact ﬁllers indeed words like yeah okay least similar sound classiﬁer learns label sounds ﬁllers surrounded silence case sentence boundaries. here often semantic context baur required decide whether word ﬁller part speech. situation automated approach likely fail. however even case still help speed annotation process since usually faster correct wrongly assigned label creating scratch inexperienced users cope nova asked students introductory lecture human-computer interaction solve annotation task questionnaire afterwards. firstly students divided four groups quick introduction nova given. stick recordings noxi database simpliﬁed annotation task classes speech laughter. asked students load sessions create empty annotation. annotating speech laughter chunks could session completion tool ﬁnish remaining part session. observing predicted labels could decide either manual labels repeat completion step revise predicted labels conﬁdence. finally provided manual annotation session asked compare semi-manual annotation. group successfully ﬁnished task less hour. questionnaire wanted know believe strengths weaknesses human versus machine coding. also asked open questions improve system. interestingly observed machine labels generally precise failed speciﬁc situations speech laughter occurred time. despite short time spent tool already reported loss concentration noted apply machines. regarding visual guidance revision prediction groups agreed highlighting labels conﬁdence helped correct annotations. however also interested opinion visualization information. currently labels adjustable conﬁdence threshold superimposed uniform pattern. binary decision advantage user quickly detect spots require actions. hand evident whether label weakly strongly accepted rejected. groups liked binary highlighting groups preferred detailed visualization using colour gradation. group also mentioned probabilities classes available better understanding prediction failed. investigations needed understand ﬁnely graduated representation several conﬁdence classes preferable. generally students reported diﬃculties using interface nova integrated tools helped complete task less time. also investigated approach performs respect modalities audio. applied openface baltruˇsaitis extract visual features videos noxi database result dimensional feature vector frame including facial landmarks action units gaze directions. based features facial behaviour learned. following experiment deﬁned task smile annotation. hence annotation scheme containing single label smile applied. employed experienced annotator working nova introduced tools. however time ﬁxed procedure interested seeing applied tools solve problem explorative process. therefore asked take notes experiences. would expect annotator started apply session completion step labelling smiles within ﬁrst minutes session. noted ﬁrst system able reliably predict smiles remaining session. therefore corrected another minutes predicted smiles removed predictions beyond point applied completion step again. procedure repeated prediction looked stable smiles conﬁdence revised. ﬁrst session ﬁnished trained model applied predict smiles second session prediction session looked reliable completed session revising labels conﬁdence. however subjects noted prediction stable enough decided apply session completion step instead. case completing session retrained model including labels. since robustness models improved session added training predictions accurate towards corpus. increasingly speeded coding process. total took less ﬁnish sessions previous experiment also shows possible detect moment safe hand labelling task machine noted time human annotator learns whether worth correcting predicted annotations instead adding labels ﬁrst letting machine complete session. especially earlier stages sometimes model trained subjects perform well enough unseen users. latter case better continue using session completion step. though automatic predict whether session completion session transfer used interface nova allows quickly explore options pick promising approach. point enough sessions available user apply following strategy assess quality prediction. train model subset completed sessions evaluate remaining ones. obtained confusion matrix provides feedback reliability labels. instance class often confused another worth review predictions class whereas labels predicted high conﬁdence safely skipped. generally visually reviewing predictions nova optimal work-ﬂow respect speciﬁc task. core idea behind cooperative machine learning create loop humans start solving task time machine learns automatically complete task. conventional approaches involves least parties end-user knowledge domain machine learning practitioner cope learning system. however make process rapid focused amershi demand control given enduser. tool combines traditional annotation interface functions applied requiring knowledge machine learning. found important give coders possibility individually decide labelling process. assess reliability automatic predictions immediate visual feedback provided gives annotators chance adapt strategies times. interactively guiding improving automatic predictions eﬃcient integration human expert knowledge rapid mechanical computation achieved. reported experiments show even end-users little background machine learning able beneﬁt described machine-aided techniques. also observed strategies potential speed coding also positive inﬂuence annotator’s coding style. preciseness machine-aided techniques introduce coding process level-of-detail improved time human eﬀorts reduced. here strategies guide attention annotator inspection predicted labels become crucial matter. mentioned rosenthal investigated kind information provided user minimise annotation errors. however studies concentrate single images whereas case deal continuous recordings. overload annotator many details decided uniformly highlight labels adjustable conﬁdence threshold. simulations section suggest approach helps signiﬁcantly reduce labelling eﬀorts. however exact gain depends highly nature complexity annotation problem applied machine learning techniques least expertise subjective attitude human coder. goal presented work foster application cooperative machine learning strategies speed annotation social signals large multi-modal databases. well described corpora rich human behaviour needed number disciplines social signal processing behavioural psychology. however populating captured user data adequate descriptions extremely exhausting time-consuming task. presented strategies tools distribute annotation tasks among multiple human raters automatically complete unﬁnished fractions database particular proposed two-fold strategy support manual coding process applied fresh database ﬁrst concentrates completing individual sessions. relatively small amount labels suﬃcient build session-dependent model though strong enough generalise well across whole database used derive local predictions. afterwards session-independent classiﬁcation model created ﬁnish remaining parts database. steps conﬁdence values created guide inspection predictions. prove usefulness approach presented results realistic use-case based database featuring natural interactions human dyads. experiments section picked task detecting ﬁllers speech. fillers important aims study turn taking interruption strategies. fast general audio detection system combination linear classiﬁcation model applied natural conversations yielding average recognition performance almost simulation proved labelling eﬀorts signiﬁcantly reduced using proposed system. applied combination revision instances conﬁdence value manual inspection reduced database. case corresponds saving approximately hours important bring proposed approach application. section introduced nova open-source tool collaborative machine-aided labelling. conventional annotation tools nova supports fully collaborative workﬂow allows distribute annotation tasks among multiple raters. discussed strategies integrated directly applied interface speed manual annotation. generalisability proposed detection system enable researchers adopt approach databases annotation tasks future. experiences diﬀerent groups users show approach also positively perceived end-user’s point view impressed system’s accuracy. feedback obtained made aware diﬀerent styles annotation adopted end-user machine. machine able annotate social signals much faster consistently humans human raters still bring better understanding application models trained eventually applied. furthermore human raters look behaviours labelled also reason context occur. presented results automated labelling process might inﬂuence human labellers positive manner. nevertheless aware risk machine-like style annotation might always result better systems. particular true social signals analysed raters usually disagree labels objective ground truth established. order beneﬁt complementary skills machines human raters annotation tools like nova needed smooth integration human intelligence resources. overall experiments demonstrated potential approach reducing human labour annotation process. future work focus question leverage complementary skills human machines. employment approach requires end-users incrementally inject information training process desired system behaviour achieved. workﬂow necessitates tight coordination machine human tasks. particular would desirable provide endusers guidelines hand annotation jobs machine. paper presented statistics evaluate injection knowledge human training process. future work also plan extent current workﬂow automatically generating recommendations order sessions database processed. poignant suggest hierarchical clustering select prototypical examples prioritise coding process. however straightforward adapt techniques continuous recordings. alternatively case make conﬁdence values generated label prediction. using average value following strategy conceivable every time session ﬁnished model built predict remaining sessions pick lowest score complete next. ensure manual eﬀorts spent data high potential improve learner next iteration.", "year": 2018}